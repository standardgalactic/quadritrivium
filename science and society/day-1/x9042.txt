really like you have to think about this

as like from one block to another and

sort of like being curious and seeing

like where this takes you so I'll start

with um some of of our earlier work uh

this was actually back in

2021 uh at the time we uh you know like

even the word gen didn't exist right so

this was uh quite novel and we were

already looking at like the great

progress in NLP and thinking like how

could this apply to uh to biology and in

particular to DNA sequences so DNA

sequences they're composed of four

letters a t CG and and the question is

like how could you use uh like NLP style

models and in this case we used bird

models to try to get going there so what

we decided was to uh actually uh use um

tokens of a length of six nucleotides

trying to find a good compromise between

like an ability to express the

information but also uh not use too much

compute power with too many tokens and

we started training models uh back from

2021 2022 uh actually in collaboration

with a few Partners including Nvidia we

trained on the Cambridge one uh the

superc computer of Nvidia in the UK also

Google gave us access to Cloud CPUs

allowed us to train at the time we were

a small startup and we didn't have those

capabilities but just to give you an

idea we started with the reference human

genome so uh so that was the starting

point and then we moved on to the

Thousand genome uh like data set again

open source data but allowing us to have

uh like sort of like more to play with

and then we tried also multi-species we

were like could we actually get more

information in case we want to go uh

beyond the human genome and does it make

sense so we tried a bunch of things and

I want to give you sort of a high level

sense of what were our results and so

the first thing is that the whole

concept of self-supervised learning as

you guess actually works very well for

genomics and as you train those models

