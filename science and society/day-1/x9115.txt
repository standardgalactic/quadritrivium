have good ways of representing

distributions of our video

frames um every attempt to do this uh

basically bumps into mathematical

interact

abilities um and so you could try to get

around the problem using you know um

statistics and and the math that was

invented by by physicists you know

vational inference and all that stuff

but in fact it's better to just throw

away the entire idea of doing

probabilistic modeling and just just say

I just want to learn this energy

function that tells me whether my output

is compatible with my input and I don't

care if this energy function is a

negative log of some

distribution um and so the reason we

need to do this of course is because we

cannot predict exactly what is going to

happen in the world there is a whole set

of possible things that may happen and

if we train a system to just predict one

frame it's not going to do a good job um

so the solution to that problem is an AR

a new architecture I call J embedding

predictive architecture or

jepa and that's because generative

architecture simply do not work for

producing videos you may have seen video

generation systems that produce pretty

amazing stuff there's a lot of hacks

that go be Beyond them behind them and

they don't really understand physics

um they don't need to they just need to

to predict pretty pictures they don't

need to actually have kind of accurate

model of the world okay so here's where

the JEA is the idea is that you run both

the observation and the output which is

the next observation into an encoder so

that the prediction does not consist in

predicting pixels but basically

predicting an abstract representations

of what goes on in the video video or

anything okay so let's compare those two

architectures on the left you have

generative

architectures you run X the observation

