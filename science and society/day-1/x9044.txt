were quite surprised at the time that

you know not many people were working on

these things because it actually works

really well and you know these models by

today's AI standard are not very large

we're talking about 500 million 1

billion parameter and works pretty well

and so this were these were our early

results and then we tried to see and at

instad we always have this passion for

research but trying to also see look at

applications it's like how does this

compare to a protein language models for

example uh like meta in particular has

developed at the time several several

models which which were quite exciting

from a protein language modeling point

of view and we were like could genomics

models actually bring value here and

when you think about it this is not an

obvious point right because you know if

you look at like um the amount of DNA

that is used in coding regions to

actually obtain proteins it's in the

order of one and a half% the rest is

basically sometimes of unknown value and

on top of it even if you're looking at

like coding regions in the genome they

are actually uh separated by uh Inon

regions which are not expressed and so

it's all over the place and finally

remember we're doing tokenization like

six nucleotides at a time it's unlikely

that these would match for example the

three nucleotides encoding a particular

amino acid like you need three you know

one codon is three nucleotides gets you

one amino acid so for all those reasons

it was not obvious that actually

genomics models would be very useful

property in language tasks but

surprisingly they are and sometimes they

beat even like models such as esm2 in

protein language space it depends really

on the task and we here we shared some

results in in in a recent in a recent

paper it depends on the task if the task

is too um dependent on the structure of

the protein or the particular particular

amino acids then perhaps a PLM a protein

