we need to train those systems and we

need to figure out how to train those

jeer architecture and it turns that to

not be completely trivial because you

need to train the the cost function in

this JEA architecture that measures the

the Divergence essentially between the

representation of Y and the predicted

representation of Y we need this to be

low on the training data but we need

also need it to be large outside the

training set okay so this is you know

this kind of energy function here that

has kind of uh Contours of equal equal

energy we we need to make sure the

energy is high outside of the manifold

of data and I only know two classes of

methods for this one set of method is

called contrastive it consists in having

data points which are those those blue

dark blue dots pushing the down the

energy of those and then generating you

know those flashing green dots and then

pushing the energy up the problem with

this type of method Contra method is

that they don't scale very well in high

dimension if you have too many

dimensions in your space of Y you're

going to going to need to push up in

lots of different places and um it it

doesn't work so well you need a lot of

contrastive samples for this to work

there's another set of method that I

call regularized method and what they do

is they use a regularizer on the energy

so as to minimize the volume of space

that can take low energy okay so that

leads to two

different types of learning procedure

one one learning procedure which is

contrastive you need to generate those

contrastive points and then push their

up to some L function and the other one

is some regularizer that is going to

sort of shrink wrap the the manifold of

data um so as to make sure that the

energy is higher outside so there's a

number of techniques to do this um I'll

describe just a just a handful and the

way um we we started testing them

