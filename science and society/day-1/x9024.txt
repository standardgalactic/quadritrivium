each other with different voices so I'll

skip that for sake of time and I'm going

to uh to jump uh and in order to

conclude in a few minutes

uh to uh this there we go so when you

look at what I described to you U is U

the fact that we what essentially put

together is what is called a multistream

um real time architecture with audio and

text which can actually that we use in a

certain way but we can train it for

other applications for instance we can

use it with a slight delay if the text

stream is delayed is a bit delayed then

you can use that for actual

transcription of speech so ASR um the

same architecture can also be used the

other way around if you start with a bit

of Advance with text and then you can

train the audio uh channel in order to

produce the synthesis of the speech then

it's TTS and you can even do that with

okay there we go and this is an example

where you see with the expressiveness

and it's actually the voice of of the of

the

Mushi dialogue AI but in this case it's

used in in the form of TTS this time I'm

not chatting but rather being controlled

by text I can express more than 70

emotions and speaking Styles like

Whispering or maybe I could sing and I

stop here and uh another thing is that

you can do that with multiple speakers

because we have two audio channels and

another example here of a dialogue which

is synthetic from uh a script do you

watch drama series if so which

one oh okay can you tell me about the

one one you

watch the reason why you didn't hear is

that it's it's stereo and now I just

have I guess

the one channel here let me just do that

because it's going to be important if I

can have my mouse can I have my mouse

oh oh God there we go yeah yeah yeah

that's cool um let's see where is sound

[Music]

there we

