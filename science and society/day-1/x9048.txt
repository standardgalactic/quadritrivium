multiple tasks so you can with this

model which we open source essentially

label many many species where perhaps

you have row DNA that is misunderstood

you could in one go get that information

and so that is the spirit of what we do

and finally I don't want to take too

much time I just say a few words on the

sort of exploration of multimodality

with language this was something I was

particularly personally quite intrigued

on I was like can language bring a

structuring property to all the work

we're doing because so far we've been

training models on nucleotide sequences

or amino acid sequences in protein space

but what about language could we do

something where actually could I speak

to the system could I ask the system

questions but and here I'm not trying to

do what classical NLP does which is

trained on the internet I want the model

to have Ultra accurate understanding of

biological sequences and understand

English at the same time and this was

not done when we started working on it

and so we tried what are the net uh sort

of like value of doing this well first

you make the model a lot more accessible

right everybody who is in a lab who's

not a computer scientist who's not a

machine learning engineer can suddenly

now use those models and sort of like

speak talk to the model and say hey can

you what can you see in this DNA

sequence for example or this RNA that's

kind of cool and the second thing which

is more on the research side of thing

was like could we actually make the

models better if I put in language and

that's a that was an open question at

the time when we started this work and

so what did we do we took open source

models uh here the Lama series and we

took a own nucleotide Transformer and

what we did was take the embeddings of

the nucleotide Transformer so the high

level representation of the nucleotide

Transformer and project them in the

space of embeddings of the language

