Moshi which is full duplex in the sense

that there is no speaker turns uh in

blue you have the audio of the speaker

which is compressed and goes into the

first uh uh audio stream in green the

audio stream of the AI which is then

decoded in order to to answer and then

we have added another text stream which

is processed in parallel and which

allows to capture more knowledge from

the starting point of the model which is

an actual text language model and then

with that we have a model which is able

to listen think and talk at the same

time even if talking means uh producing

silence and the theoretical latency is

160 milliseconds which is compatible

with the real time um dialogue all right

um so the uh the training stage I'm

going to skip for for for the details

for sake of time uh but just to to tell

you that at some point for the for fine

tuning the model we did need um actual

audio of people talking with separated

Source meaning each person on a

different Channel

and the first it's not very easy to find

the first data set that we found was

this Fisher data set of 2000 hours of

phone calls in the US recording in the

'90s and already with that we had the

Moshi version which was able to talk as

an American person in the90s on the

phone and this is the example South

Arizona oh brilliant so you're American

yes so you know about the US

right sorry so you know about the US

yes okay so who's the president right

now the President Bush oh

okay yeah I guess it was an interesting

time um so moving forward obviously that

was not the final thing then we wanted

to have a real modern dialogue and we

for that we needed a lot of this quality

dialogue um data and for and we we had

to do that with the synthetic data we

started by recording actual 200 in

studios dialogues of people and artist

having interaction and with some emotion

Etc including improvisations and

