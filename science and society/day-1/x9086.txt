they noticed that the accuracy

completely collapsed and uh they were

wondering why and started to analyze the

system by looking at where in the image

the system was paying attention and then

they noticed that the image was the

system was paying attention in

particular this area here uh I don't

know if you can see it but it's the the

shade of a little tube that the doctors

insert into the lung to let the liquid

flow out if if you have num more thorax

and of course having this tube in your

lung in the training s is correlated

with having a numo thorax very strongly

but at the same time it's not causal for

having a numo thorax so the correlation

breaks down if you get a go to a

different distribution because in this

case the test set was drawn from people

that have not yet been treated so this

points to the weakness of these systems

if they rely on correlations or most

specifically if they rely on the

correlations being stable uh across

shifts between training and

testing and this kind of problem occurs

everywhere in machine learning and I'm

going to show you a nice example that

two of my students found in language

models a nice visual example so they

they prompted I don't know if it was o1

or tp4 they prompted with this image

here and asked which of the which orange

circle is larger and they even asked you

to think step by

step and and maybe in this moment moment

I also want to acknowledge so this is

another former student who actually so

we all know this the the standard

reference for step by step he actually

developed this at open AI before before

it was published by Google but people at

open AI don't write papers so anyway so

they ask the system think step by step

and then the system says this is a

variant of the eing house illusion so

although the circle on the right appears

larger both orange circles are actually

the same size and it gives you an

