know playing with almost everybody is

playing with uh do not have the right

characteristics uh for for what we

want

um and the reason is uh they basically

um produce one token after the other

Auto regressively right so you have a

sequence of tokens which are subo units

but it doesn't matter what they are a

sequence of symbols and then you have a

predictor that is repeated over the

sequence that b basically take a window

of previous tokens and predict the next

token um and the way you train those

system is that you put the sequence at

the at the input and I really apologize

for this I'm going to

perhaps change the

resolution of the

screen so

that we don't have this constant

flashing

hang on just one second

okay not sure this is better but

hopefully all right um

so so the way those things are trained

is you take a sequence and you basically

train the system to just reproduce its

input on its output and because it has a

causal structure um it cannot cheat and

use a particular input to predict itself

it has to only look at the symbols that

are to the left of it that's called

causal

architecture um so that's very efficient

this is you know what people people call

a GPT general purpose Transformer but

you don't have to put Transformers in it

this could be anything it's just a caal

architecture and I'm afraid I haven't

fixed the flashing anyway um so the the

the way you train uh those systems uh

then you can use it to generate text by

just Auto aggressively producing a token

shifting it into the input and then

producing the second token shifting that

in ETC that's Auto prediction Not A New

Concept at all obviously um and there's

an issue with this which is that um the

U the that process is basically

