distillation so distillation is a

standard technology maybe some of you

have read the deeps paper that distill a

big model to a small model say it works

very well we did the same we started

with our big model we distilled it into

a smaller model and what we observe is

that in terms of performance you know

it's it's slightly lower but still very

good but we observe that it's better in

terms of robustness so we have different

ways to assess the robustness like

testing in you know TR funing it on a

one data set and testing on a different

hospital and so that's one you know one

of the surprise kind of surprises not

completely surprised but interesting uh

topic of research which is that uh

distillation seems to be away not only

to have a smaller model so cheaper model

but also more robust model in terms of

capacity to uh you know to be used in

different hospitals that may have some

specific Hospital batch

effects okay so I I'll now finish the

you know finish the talk by by

mentioning you know what what really is

the you know could be the the next

Frontier because here I say we and

others K mentioned DNA I mentioned a few

examples in proteins uh images cells so

clearly the field of foundation models

for biology has been quite active even

even though it's a small community in

taking data sets that exist pre-training

models and and developing specific

models uh specific use cases per

modality uh but tempting to go beyond

that you know because when you talk of a

of a molecule of a gene of a protein of

a cell of a tissue all these things are

connected right a cell is made of sorry

a tissue is made of cells a cell can be

characterized by the genes a gene is a

sequence of DNA Etc so it's kind of

obvious that uh a true computational

model of biology should be able to grow

across the case and actually many of the

questions we like the model to be useful

for require us to think across the case

