homomorphism um but I think uh gradually

running out of time so uh let me uh just

give you a few last thoughts so I think

these causal representations there are

two instantiations one is the AI

instantiation so a world model is

something that an intelligent agent

needs and I think we heard some talks

about this already today uh probably

also uh thinking and reasoning require

such World models and the ethologist

Conrad Lawrence one said that thinking

is nothing but acting in an imagin space

and the second instantiation

I would argue is in data science because

uh these kind of models and

representations they actually give you a

toolkit to compose uh different models

in a sensible way uh you could have some

models that are trained using machine

learning some mechanism that you learn

some mechan mechanisms that you simulate

some you might take from the medical

literature Etc and this tells you how to

compose them we once built such a system

for the covid-19 vaccination data from

Israel and and this allows you to do a

counterfactual exploration even for

scenarios that you haven't seen data for

so you you can you could ask questions

like what would have happened if we had

Israel had different use used the

different vaccine prioritization or if

the uh age risk profile of covid-19 had

be the one of the Spanish Flu instead

Etc so uh I promise I conclude with

maybe two thoughts about large language

models so uh

um one could Define and this is

something that Leon Bou and I have done

a perfect language model by saying

imagine we have a collection of all

sensible texts and uh we print a few

words on a tape let's call it a prompt

we then find all occurrences of exactly

these words in our collection we

randomly pick one of them we read the

next word in that book print it on the

tape and repeat so this is a purely

mechanistic system of course there's no

