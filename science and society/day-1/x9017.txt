this way the user speaks there is an

voice activity detection uh which

determines whether the the user speaking

and if so the speech it's transcribed

into uh uh into text with an automatic

speech recognition system it goes into

an llm language model and the answer

which is text is turned into audio with

a TTS which is text to speech generation

system so that's fine uh but it and it

works pretty well in many cases the one

of the difficulties is that it can take

a few seconds so it's it's the latency

is not so good for the interaction which

is not necessarily a big problem but

there are two more fundament

limitations one is the fact that you

lose the non-verbal aspect of the

expression of the user so the par

paralinguistic of the user the emotion

the accent in my case but also uh you

have to impose U spe speech turns

between the machine and the user which

make the experience very poor and really

and very limited so these are the things

that we wanted to uh to tackle in our

first project when we started the the

lab and regarding this last point which

is about the uh the fact that you have

you have to impose or not uh speaker

terms uh let me give a few more

explanation of that again sorry all the

the the writing in my slide is

completely uh uh messed up in this

version um so in blue you have the the

user is and if you have to impose and in

green the the AI if you have to impose

the um the turns uh then it means that

essentially you have the the same

experience as talking through a walki

talkie system so useful but not ideal

for interaction uh as what we we would

like IDE to get is something where like

when you talk on the phone or even face

to face uh there is no uh speaker turn

and the the speech can overlap and there

is back channeling etc etc um so that's

what we aim at um and maybe one one one

starting point for that is to see

whether we can uh owing to the Fantastic

