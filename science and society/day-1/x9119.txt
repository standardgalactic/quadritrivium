don't

maximize uh one way to do this is to

basically take the

um vectors representation vectors that

come out of the encoder over a batch of

samples um and make sure they contain

information how you can you do this you

can take that Matrix of representation

vectors and compute the product of that

matrix by it transpose you get a

covariance

matrix and you try to make that coar

Matrix equal to

Identity um

so there's a bad news with this which is

that this

basically approximates the information

content by making very strong

assumptions about the the nature of the

dependencies between the variables and

in fact it's an upper bound on

information content and we're pushing it

up crossing our fingers that the actual

information content which is below is

going to follow okay so it's slightly uh

uh irregular uh theoretically but but it

works all right so again uh you have a

matrix coming out of your encoder it's

got a number of samples um and each

Vector is a separate variable what we're

going to try to do is going to try to

make each variable individually uh

informative so we're going to try to

prevent the the variance of the variable

from going to zero force it to be one

for example and then we're going to

decorrelate the variables with each

other and that means Computing the co

Matrix of this Matrix is transpose

multiply by itself and then try to make

the resulting coar Matrix as close to

the identity uh Matrix as possible um

there are other methods that try to make

the samples uh orthogonal not the not

the variables um and those are

contrasting sample contrasting methods

um but they don't work in high dimension

and they require large

batches so we have a method of this type

called vag that means variance

