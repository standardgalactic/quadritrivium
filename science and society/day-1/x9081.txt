they see something that well aor is

below and B aor is above and you all

mostly sample or trying to things

that're close to it because you figure

out but actually this is not optimal you

know and this is what is on on this

picture TT from Chaba book because Chaba

wrote about it later in a book on on on

Bandits RL and so actually the best way

how to distinguish whether this is

better or this is better is actually not

sampling any of this thing is actually

sampling something orthogonal because

you already learned that this direction

is good but to distinguish which one is

this better you know it's better to

something called T so it's very uh cont

inuitive that sometimes you actually to

learn what decision is optimal you

actually sub suboptimal decision in a

very specific Dimension and this is what

Anan worked out and uh and you know this

is stuff that I can talk about because I

have only last last 30 seconds and so

you know and this is you know design the

that done on gr and so you can learn

more thata about paper ask us about the

details and just to conclude you know in

the first part I kind of showed how to

extract some metacognitive knowledge and

LM and use it to improve Network what is

the followup for that and this is

something you know that already working

on with an G also Peter Bartlet from

Berkeley and um and um and Deep Mind how

to actually train LMS better and how to

design automatically better to

training and so in the second part I I

tried to answer the question of of

Emanuel candes in the morning which

labels are more most informative for

this specific case and so here you know

here we in in this sneak peek two slides

we actively select you know what to ask

human so you know this are just like

maybe like 2% stuff that uh that we are

doing and you know there are some

architectural advances that we did both

in the in the in the network structure

and also in the in the test time

