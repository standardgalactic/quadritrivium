hierarchically so as to fulfill an

objective and systems that can

reason um and then systems that are

controllable and safe by Design not by

fine tuning which is the the case for

llms now the only way I know to build

systems of this type is to change the

type of of inference um that um

current AI systems perform so right now

the way an llm uh performs inferences by

running through a fixed number of layers

of a neural net a Transformer then

producing a token injecting that token

on the input and then running through a

fixed number of layers again and the

problem with this is that if you ask a

simple question or complex question and

you ask the system to answer by yes or

no like does 22 equal four yes or no or

does p equal NP yes or no is going to

spend the exact same amount of

computation to answer those two

questions so people are being kind of

cheating and telling the system well

explain you know the Chain of Thought

trick you you basically have the system

produce more tokens so that it's going

to spend more competition answering the

question but that's kind of a hack the

way um a lot of inference

in statistics for example that's going

to make Mike happy actually um the way

inference works is is not that way in uh

In classical AI in statistics uh in

structure prediction a lot of different

domains the way it works is that you

have a function that measures the degree

of compatibility or incompatibility

between your observation and a proposed

output and then the inference process

consist in finding the value of an

output that minimizes this

incompatibility measure okay let's call

it an energy function so you have an

energy function okay represented by the

square box here on the right um when it

doesn't disappear and and the system

just performs optimization for doing

inference now if the inference uh

problem is more difficult the system

