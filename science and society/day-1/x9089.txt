interventions and if we look at these

things we look at it also with this

perspective so our representation

uh certainly should have this kind of

element and this is something that we

don't have right now so this is

something that's about interventions and

about

causality and causality is a a field

that in the last decade or so has become

quite popular in machine learning people

are starting to think maybe it's at the

core of some of the problems that we are

struggling with in machine learning such

as problems of O and

generalization and uh there a beautiful

Theory underlying the field and I don't

have to don't have the time to go into

details uh but just a few basic basic

ideas so you have a set of observables

and uh unlike standards probability

Theory you assume they are lying on a on

a graph in the simplest case on a

directed aycc graph and uh and the

arrows of this graph represent direct

causations each variable is a function

of its parents in the graph so these are

the causes and then this is the effect

of the causes and an unknown noise term

and these noise terms are jointly

independent and this structure has

various interesting consequences one is

that they get a certain factorization of

the joint distribution into uh

conditionals of variables given their

parents so that's a sparse factorization

you only need to conditioned on the

parents um the other one is that these

random variables so the noises are

independent but since they're connected

through this graph the random variables

pick up a footprint of the graph

structure so there are certain

properties of the graph that you can see

from the OBS observational distribution

and these are captured by what's called

causal Mark of

conditioning and uh and people use these

uh observable Footprints conditional

Independence is testable um to test uh

