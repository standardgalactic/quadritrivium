also very happy that exactly there was a

work done with the collaboration with

the organizers of the conference Mike

and and

U and Eric and you know this a lot of

work was done uh but Anto I start with

some like initial uh you know initial

discussion in over several years

actually by Remy M who who is sitting

there uh on the like how do actually use

better way of labeling you know so

Emanuel CES in the morning like what is

the best way that humans can provide

labels to the and this is somehow first

answer to this problem and we very

recently you know started to also

collaborate with the French you know to

again support the French uh submit with

labeling company Kenny KY uh you know

please use their support a lot of French

companies do uh to uh to actually you

know evaluate uh those practically uh

and so a lot of work was done by by

antoan who's a student of Mike and Eric

all right also with withan and Alan and

P and Pa actually representing KY all

right so the original issue started

already you know I think this was

already at Gemini when we when I started

to think about it with actually David is

that a lot of times we pay this

companies to you know you know there's

several kinds of data like I was mostly

often in the past mostly focus on like

you know preference data for online rlf

and online naal learning right so you

you give humans two or 10 answer from

the same model and you ask them which

one you prefer you know and then you

either train reward model or you if

you're really really rich you know you

can directly do policy gradients on

these answers but a lot of these answers

is actually quite wasted uh because at

the beginning the model is very bad and

so all the answers are very bad and so

you let label you know you know you pay

for labeling noise at the end the mo you

know model is getting quite good and so

maybe the answer for a lot of questions

