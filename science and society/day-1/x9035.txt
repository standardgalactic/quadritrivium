easier for customers that we're starting

to use and give to our customers called

instruct lab so I'll just briefly tell

you what this is and I'm going to try I

don't have a time monitor here but I can

imagine we're behind time um but I'll

just keep going um and maybe you'll give

me a like a five minute Mark okay um so

how do we do it in traditional software

so um I mean anyone who's done computer

science know like you know or done any

software development or taken a course

even knows you write code right you

write a library that implements a kind

of functionality libraries tend to have

sort of you can name what each library

is for and what it does and uh you know

how would we do this in the llm case

well I mean a simple answer to that

question certainly not the only answer

to that question is that you can write

code that generates data right you can

write code that generates input output

pairs you can write data that produces

you know inputs and then some sort of

scoring rubric and basically by turning

this all into a coding exercise what we

can do is open the aperture to a much

wider group of people to really

participate in the creation process and

this is something that we're doing

internally with an IBM to make sure that

all of our llms are extremely well

tailored to our own internal use cases

and then as people add more and more

things then you know the models get just

get stronger and stronger and stronger

we have more flexibility and more more

highly tailored human curated data and

basically the the way this works is not

that different than what we call

inference scaling today it's some kind

of flow of you you generate something

potentially using a larger llm then you

decide whether it was correct or not and

you can sample and you can provide any

kind of flow there but basically you're

just iteratively refining something

until it's the best it can possibly be

those verifiers don't have to be llms

