several years ago um maybe five six

years ago was um to train them to learn

representations of images so you take

one image you corrupt it or transform it

in some ways and you run the original

image and the corrupted version in uh

identical encoders and you train a

predictor to predict the representation

of the original image from the corrupted

one once you are done training the

system you remove the predictor and you

use a representation at the output of

the encoder as input to a

simple like a linear classifier or

something of that type that you train

supervised uh so as to verify that the

representations that are learned are

good and this idea is very old it goes

back to the 1990s and things like we

used to call SII networks um and some

more recent work on on those joint

embedding architectures and then adding

the predictor is more is more

recent um so Sim clear which is from

from Google is a contrastive method

derived from s

Nets um but again the dimension is is

restricted so the regularized method uh

worked the following way way you try to

estimate have some sort of estimate of

the information content coming out of

the encoders and what you need to do is

prevent the encoder from collapsing

there's a trivial solution of training a

a Jeter architecture where the encoder

basically ignores the input produces a

constant output and now the prediction

error is zero all the time okay and

obviously that's a collapsed solution

that is uh not interesting so you need

the system you need to prevent the

system from from collapsing and with

which is the regularization method I was

talking about earlier and an indirect

way of doing this is maintain the

information content coming out of the

encoder okay so you're going to have a

training objective function which is a

negative information content if you want

because we minimize machine learning we

