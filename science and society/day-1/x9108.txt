formulate a problem you POS a problem to

an llm and if the problem is kind of a

standard puzzle the answer will be

regurgitated in just a few seconds if

you change the statement of the problem

a little bit the system will still

produce the same answer that it had

before because it has no real mental

model what goes on um in the in the

puzzle so how do um humans infants learn

how the world works and you know infants

accumulate a huge amount of background

knowledge about the world in the first

few months of life um Notions like

object permanence um solidity rigidity

natural categories of objects before

children understand language they do

understand the difference between the

table and the chair um that kind of

develops naturally and they understand

intuitive physics notion like gravity

inertia and things of that type around

the age of nine

months um so it takes a long time uh

observation mostly um until four months

because babies don't really have any

influence on the on the world before

that um but then uh through interactions

but the amount of interaction that's

that's required is ason

small

so if we want um AI system that can

reach eventually reach human level might

take a while um we call this Advanced

machine intelligence at meta we don't

like the term AGI artificial general

intelligence the reason being that human

intelligence is actually quite

specialized and so calling it AGI is

kind of a misnomer um so we call this

Ami we actually pronounce it Ami which

means friend in French

um so we need systems that um learn well

models from sensory input basically

mental models of how the world works

that you can manipulate in your mind

learning 2D physics um from video let's

say systems that have persistent memory

systems that can plan actions uh

possibly

