uh the answer is yes and the reason is

um okay there there's number of things

to say about this so first of all the

totality of all data centers in the

world uh the power consumption is

roughly about 3% of energy consumption

or electricity consumption actually not

all energy so it's not that big it's not

negligible but it's not that big and the

question is you know if we end up using

llms on a daily basis as I was

describing and billions of people using

this is it going to

um you know balloon to like some

significant portion of energy

consumption and the answer is no because

it's limited by economics I mean are

people ready to spend 2,000 EUR a year

to use an assistant which is what it

costs now if you want to use the top uh

uh systems probably not that many people

right um I was talking to um people in

India who are thinking of installing um

data centers to to serve AI assistant to

the Indian population the kind of budget

that we're talking about is about 10 to

100 times smaller than what it currently

cost and so they say you know if I want

to serve 800 million people in India the

cost have to come down by something like

20 or 30 and so there's a huge incentive

to do this there a huge incentive

because that's the only way uh you make

this economically feasible uh you can

recoup the cost of you know enormous

investments in uh infr structure uh

there's you know hundreds in each

company hundreds of Engineers working on

Hardware specialized chips to run

inference faster um there's number of

startups also that that do this um at

the next level people who work on you

know low level functions and compilers

to make sure things are the most

efficient possible then people who try

to kind of distill big models into small

ones and then people who have a mixture

of expert models so that you know you

use Simple models for simple questions

and then more complex one for more

