you flatten that so you you have in this

case four level of presentation you have

one after another one for four for the

four of them and then you have the next

frame Etc so you have eight you have two

time frames here and then you

decode you you predict progressively all

the level this way with your Transformer

uh which could be done the problem with

doing that is that you still have a lot

of computation to uh to do at each time

frame which is uh especially with this

size of Transformer 7 billion parameters

which is not very feasible instead what

we did is hierarchical so effectively

this hierarchy of four token which is

actually eight or 16 in the in the model

is is squashed into a single

representation which is used to predict

not the full stack itself but sorry an

intermediate contextual representation

which is a vector uh which then is used

with another small architecture a small

transformer of half billion parameters

which is then in charge of unfolding the

hierarchy of this acoustic token the

first one is by the way for the semantic

and the other are more for the acoustic

aspects and this is how it

goes and then you have your full pile

stack for this new time frame and it and

it continues and repeats and this thing

that we call the QR trans Transformer is

what is used in our model okay and now

the complexity is is is better and it's

amenable at least to Real Time

Performance which is what we We Want U

now we we have trained this on a large

amount of of audio and then you can

already use that for speech continuation

which the audio is not in the room by

the way so I don't know if somebody is

uh behind can put the audio in the room

or at some point we'll have to to do

something else I don't know

it's on my computer but maybe I have

a no

no okay let's try again so what you're

going to hear normally is a real audio

and then you will hear the same but

