um let me get a question in this way so

uh I think just like the AI for

sustainable development for for these 70

pillars from SGS if you're successful in

16 dimensions for the seven the 17th one

you you fail and then you fail for

sustainable development so I think this

is also true for AI safety then I we

wouldn't choose actually which one is

you know more severe uh uh that I I

think the way that we need to tackle

this problem is something like this that

actually um you you see maybe risk are

in different levels but now ai now maybe

they they're with some sort of self

reflexivity now and now and then the

model themselves they can do cognitive

escalation you don't know uh when that

the level of cognition are raising for

these AI models and then the AI models

can the level of risk can also raise um

in a self- automated way so choosing one

risk is not really rational so so let

let me say that the most dangerous one

after the inim report is that larger

language model got a very initial sense

of model of self with reflexive thinking

and this is the most dangerous thing

that we need to keep track of because

they can do self- automated cognit

cognitive escalation which makes risk

assessment at different levels very hard

uh so I think that that is the

scientific progress we need to focus on

yeah hi well it's a great pleasure to be

here uh from my perspective of course

all the risks are important and

interesting otherwise they wouldn't be

in the report but it is also the case

that they have a certain they have a

wide range of um probability of

occurring or evidence that they exist so

from my perspective I'm very interested

in studying and mitigating the risks

that we know 100% that they exist for

example violations of privacy for

example biases because we know that they

exist we know that they're impacting the

lives of potentially millions or

hundreds of millions of people and I

