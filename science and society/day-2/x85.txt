this is this is why we're using all of

the internet to train language models

nowadays because we think that bigger is

better and also the fact is is that the

more information you have synthesized in

your model the I mean the theory goes

the more you can access it right like if

you train your llm on the whole internet

then if whatever question you have you

can just ask it to that llm you don't

need 10 llms you only need one and so

the idea is that you know positively

speaking supposed to help access

information more quickly but on um on

the downside we are seeing these

material rebound effects of the fact

that we need more compute more gpus for

a longer time requires more mineral uh

uses more energy and creates more

E-Waste right and so there's a trade-off

because we're on one hand we we suppose

that more training more data Etc will

will will be a positive thing but it

comes at a cost and also um the energy

demands of data centers as I said have

been rising and so it's interesting

because even in terms of material

rebound effects we're seeing both the

positive and the negative and it's

really hard to quantify which one is is

is is winning or what's what's the

actual like uh relative cost of

each and now in terms of economic

rebounds effect the the famous jevans

Paradox which I don't know if you all

followed but like two weeks ago everyone

was talking about like all of blue sky

and Twitter everyone was like oh Jean's

Paradox je Paradox of AI um and so

jeevan's Paradox essentially is

considered a direct economic effect and

it says that when you improve the

efficiency of one product instead of uh

the it uh its production or its usage

decreasing which makes sense right it's

going to increase which is a paradox

because it's strange if you for example

uh jebin's Paradox was observed with

coal if you use coal more efficiently if

you can go further and further on the

