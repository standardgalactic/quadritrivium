of AI is is it's essentially impossible

to predict how the advances will you

know in AI will happen in in coming

years or decades and it's impossible to

predict what the policy will be so

there's a lot of uncertainty

at the same time as high severity and

when you're faced with such a situation

the like logical thing to do is to apply

What's called the precautionary

principle um so what does it say it says

that we just have to be cautious so one

way to think about it is um you know

we're driving in the fog and maybe it's

a montane Terrain so uh you know you

slow down you maybe try to improve your

technology to see through the fog right

so this is exactly uh what what Don was

talking about we need to uh make sure we

we develop the science so that we can

see the risks coming and you know slow

down when it's necessary take the left

turn when it's necessary uh this is what

the precautionary principle is is

telling us rather than just uh racing

ahead accelerating without knowing where

we're going which is just in my opinion

playing Uh Russian roulette with

everyone's

life and u fin question maybe for for

you professor and for Nua um you know

that in Europe we have the AA act you

know it well NOA and U there is always

this debate about you know having

regulation in order to have the the TR

tring framework for the development and

deployment of AI without hindering

Innovation um how do you see that for

for the top this very specific topic of

Risk Managers how can we do it yeah I

think this is very important uh to get

to know that everybody was talking about

you know making a balance between

development and safety this is not true

so recently we have a research actually

shows that if you tune the larg language

model to a safety Vector um uh space and

then actually you can you don't need to

change much and the large language model

just behave very safely with very low uh

