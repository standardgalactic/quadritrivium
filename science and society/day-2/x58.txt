it's it has so much stuff in there that

you wouldn't want to have in in uh in

data sets but it also has a lot of uh

copyrighted content a lot of you know

adult content etc etc so you know this

is what we're putting in our in our

language models uh David and I did study

about imet which is shown on top and

what we what we showed was that in the

categories of imet which is this really

iconic image data set that's used for

training AI models training and

evaluating AI models what we said what

we showed is that when we consulted with

experts in biology that some of the

categories were up to 98% incorrect

because they were created by people who

didn't know what a blackf footed fret

was right and and essentially these are

the data sets that are considered you

know perfect and gold standard and been

used for 10 years or more in order to

train and and deploy IM models and so

this whole idea of data and

representativity is is flawed at the

basis because what is even a

representative data set of of all

possible contexts right and then when it

comes to representativity you have to

think about evaluation and what are you

evaluating and and David alluded to this

as well he said you know essentially if

we're only only measuring accuracy then

we're ignoring all sorts of other uh

components that are really important

different metrics that are important and

uh I mean this can be summed up as you

can't improve what you what you can't

measure and I I truly believe in this

and yet uh the main leaderboards that

are used in AI in order to track

progress like papers with code only

focus on accuracy or F1 score or prec

Precision but they don't talk about

other criteria and we're not even

tracking these other criteria all we

have are these Crescendo data points

that oh yeah models are getting better

right but what are the data that what's

the data that we're using for evaluating

