these models and how how are they

getting better because for example if

the 2% Improvement in accuracy on imet

was was has come at a cost of a million

hours of GPU compute right like what are

we plotting over time are we plotting

the true costs or just the perceived

benefits and so there's all sorts of

real world constraints and when you talk

to people who are using AI in the real

world they're like you can't just put a

llama 400 billion parameter model in

production even if you think or know

that it's the best model because there

are real world constraints things like

efficiency of course and and money

because you have to pay for compute

things like robustness right how how

well does your model do with regards to

real world data because of course you

know if you have a chatbot that's

talking to people in real life the the

data is not going to be like the data

that you use to to evaluate your model

initially you have things like fairness

right that comes back to the equation

and and the data quality as well like

often if you want to deploy something in

production you have to make sure that

the data that was used to train the

model is similar to the data that is

going to come but if you don't have

access to that data how do you make that

comparison and so essentially there are

all these criteria all these metrics

that we're not even track at this point

and so um in terms of efficiency

particularly um I'm presenting a project

that I've been working on for the last

year around uh energy score ratings for

AI models and so the goal um here I

think energy score in uh in the US is

called an energy star but the the idea

is to do uh um evaluating um efficiency

and ranking and and comparing energy

consumption for different models for the

same set of tasks essentially so as you

as David said you know there's bees and

there's elephants but it doesn't make

sense to compare a model that's doing I

