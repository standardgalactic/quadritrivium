This is the first in a series of conversations on logically determined design and flow computing with null convention logic.
The materials of this conversation can be downloaded from carlfant.net.yt video.
And we begin with the first principle's basic structures and terminology.
The first principle is the null convention.
Given an element with two distinct states, such as an electronic element with high voltage and low voltage,
we assign one state to mean data and the other state to mean not data, which we will call null.
This is in contrast to the common practice of assigning both states a data meeting.
Next is the multi-rail convention.
With only one data state, the values of data variables have to be represented with multiple rails or multiple wires.
A binary variable, for instance, will be dual-rail encoded with two wires.
One meaning zero, the other meaning one.
Only one of which will be data at a time.
In the completeness convention, we define patterns of each state that represent completeness.
So there are patterns of data completeness and patterns of null completeness.
If you think of a dual-rail adder, it begins in an all-null state.
The inputs begin transitioning to data that propagate through the adder and the outputs begin transitioning to data.
When all of the outputs have transitioned to data, and that's one rail per output variable, that's a data completeness pattern.
When that occurs, we know that the add is complete and that we can allow null to propagate through the adder.
The inputs begin transitioning from data to null, they propagate through the circuit, the outputs begin transitioning from data to null,
and when all of the outputs have transitioned to null, that's a null completeness pattern.
We know that the circuit is completely set and ready to receive a new data set.
We transition monotonically between data completeness, null completeness, data completeness, null completeness, data completeness, and so on.
The most primitive completeness patterns are represented as thresholds in dual threshold logic functions with state holding behavior.
These are the function tables defining several of the functions.
This is a 2 of 2. Only when both inputs are data does it transition its output to data.
Only when both inputs are null does it transition its output to null.
If neither of those conditions are met, it doesn't transition its output.
So the green dash means that there is no transition. It holds the current state.
This defines a 3 of 3 function, this defines a 2 of 3, the 4 of 4, and a 3 of 4.
These are the graphic elements for each threshold function.
This is a 3 of 4, a 4 of 4, a 2 of 3, 3 of 3, and so on.
So this illustrates the behavior of a gate in the dual thresholds.
So we have a 3 of 5 gate here, and we begin in an all null state.
All the inputs are null and the output is null.
One of the inputs transitions to data. The output remains null.
A second input transitions to data, and the output remains null.
A third input transitions to data, and we meet our threshold. We meet our data completeness pattern.
And the output transitions to data.
Now, when we're in this state, we're waiting for the null inputs.
So one of the inputs transitions to null, but the output remains a data.
Another input transitions to null, and the output remains a data.
When the last input transitions to null, we've met the null completeness pattern and the output transitions to null.
And we're back here, and the inputs can start transitioning to data again.
We can define a set of dual threshold logic functions that cover all possible 1, 2, 3, and 4 input functions.
How this set of functions is derived, how it's used for synthesis, and how it's used for optimization,
is covered in LDD chapter 4 and will update in a future conversation.
For the moment, just realize that it exists.
The multi-rail convention is completely general. You can have as many rails as you like to represent as many meanings as you like, one rail per meaning.
In a processor, for instance, with 47 instructions, you might have a single variable with 47 rails.
Each rail represents an instruction, and when that instruction is decoded, that rail is set to data.
This is a movie of an NCL circuit with dual threshold functions.
It's a dual-rail circuit with six dual-rail inputs and three dual-rail outputs.
It starts in an all-null state, and we begin by presenting data, which is going to propagate through the circuit as yellow.
And as the data propagates, it goes to gates, and they mean thresholds, and it propagates on, and so forth.
We're going to watch this gate in particular.
It's a threshold to gate. One of its inputs is transitioned to data, and it hasn't met its threshold yet, so its output is still null.
So more inputs are going to arrive.
Now this gate, a second input is transitioned to data.
Now it's met its threshold, so it's going to transition its output to data, and that will propagate on.
Now if we back up a little bit, we'll notice that it doesn't matter in what order these inputs were transitioned at this gate.
This could have transitioned first, and this could have transitioned second. They could have transitioned simultaneously.
And when they transition doesn't matter. All that matters is whether the threshold was met.
Now this indifference to time reflects back to the input in that it doesn't matter when the inputs arrive, or in what order the inputs arrive.
When an input arrives, it will begin propagating through the circuit, presenting to gates and trying to meet thresholds.
Okay, so let's continue on with data propagation.
It goes through the circuit, so now we have all the inputs that propagated through all the outputs.
The outputs have all transitioned to data.
So the input data wave fronts have flown all the way through the circuit to the output, and we have a logical signal confidently telling us that we're done.
So how did we do that?
First of all, the circuit is a correct completeness circuit, and by that we mean that it's correct in the sense that it does the correct computation.
And it's a completeness circuit in the sense that it doesn't generate a complete output until it has a complete input.
Now how we specify and verify that a circuit is correct and complete is discussed in LDD chapter 4, and we'll discuss it in a future video conversation.
But this circuit is a correct completeness circuit.
Now given that, as the data flowed through the circuit, there were only transitions from null to data.
There were no transitions from data to null.
There were no incorrect transitions from null to data.
There were no spurious transitions from null to data, and there was no glitching. There were no glitches.
So as the data flowed through the circuit, only correct transitions to data occurred.
So when the output becomes data, we know that that's the correct resolution of the presented input.
And we can detect when the output becomes data, transitions from null, and we can confidently generate our done signal.
Now another thing to look at about this circuit is that considerably less than half of the gates transitioned in generating this correct output.
And a lot of the gates had input, but they just didn't meet their thresholds.
We'll talk about those in a bit with a null wave front.
So the done signal says we're done with the data wave front.
So the done signal goes back to the input of the circuit and allows a null wave front to propagate.
So now the null wave front begins propagating through.
And with one exception, which we'll talk about in a minute, the null wave front propagates with full logical determination, just like the data wave front did.
So if we look at our favorite gate here, one input transition to null, but the gate retains its output value of data.
Only when the null propagates to the other input and the gate has complete null, does it transition its output to null.
And there are no transitions from null to data.
There are only transitions from data to null.
There are no incorrect transitions.
There are no spurious transitions.
And again, there are no glitches.
So the null wave front is flowing with full logical determination, except for the orphan paths.
Now an orphan path is a branch of an otherwise effective path.
This is an effective path because it's observable at the output.
This is an ineffective path because it's not observable at the output.
And in particular, what's not observable is when this path transitions to null.
When this path transitions to null, we can observe it.
When this becomes null, we know that this path all the way back through here is null.
But we can't determine that this path became null by observation, so we have to make a timing assumption.
And the timing assumption is that this wire path propagates strictly faster than it takes this path to propagate through logic,
through the completeness, back, enable the input, and the next data wave front to reach this branch node.
And we can, during design, ensure that all the orphan paths are limited to a wire path, that they don't go through any gates.
So we can make that assumption a very valid assumption for most circuits.
So the null wave front propagates through.
The outputs all become null.
When the outputs become null, we know that the inputs are null.
The null is propagated all the way through.
The orphans are propagated and that we're done with a null wave front.
So the done signal becomes null, meaning we're done with a null wave front.
So the done signal can now go back to the input and say, we're done with a null.
We can take a new data path, data wave front.
So another data wave front flows through the circuit.
We detect that, the completeness of that, and another null wave front flows through the circuit,
followed by another data wave front, followed by another null wave front, and so on indefinitely.
So how does all this fit together and just what is the done signal doing?
We'll use a full adder circuit here as our example, composed of a half adder, a half adder, and an OR.
It's a much more efficient NCL full adder, but this circuit, everybody will understand how it works,
and it has enough structure to support a lot of discussion and a lot of examples.
It's a dual rail circuit, three dual rail inputs and two dual rail outputs.
This is the completeness, and this is our done signal.
So let's start in a null-null state again.
We have null completeness, and our done signal is null, and it goes back and it's inverted.
It's inverted from done to data, and the data is presented to this rank of two and two gates, which we will call an enable rank.
Now, when this becomes data, what that will do is allow a data wave front through the two of two gates.
Once the data wave front plays through the two of two gates, as long as this remains data,
these two of two gates will maintain the data wave front as it plays through the circuit.
Until we get data completeness out here, the done signal becomes data, and then it gets inverted to null,
and will allow a null wave front through.
Then when the null wave front comes along, the null wave front is allowed to play through,
and it's maintained by these two of two gates, as long as this signal remains null.
It's maintained by the two of two gates as the null wave front plays through the circuit.
When we get null completeness, this becomes null.
It's inverted to data, and will allow a data wave front through, and so on.
So what we've created here is a closed circuit with a single inverter.
And what we've created is a ring oscillator.
And it's a ring oscillator that oscillates between data completeness and null completeness,
and data completeness and null completeness.
And as a ring oscillator, it is a source of liveness entirely on its own terms.
It will continually strive to oscillate between data completeness and null completeness.
Now, our done signal is the signal that closed the oscillation, and created the oscillation with the inversion.
So now we won't call this done anymore.
We'll call this the closure signal.
So let's talk about the completenesses.
Each completeness follows an enable rank.
So the completenesses here are saying a little bit more than the fact that the circuit is done with its data processing.
It's also saying that the data wave front has been passed on to the next oscillation.
So we have a green oscillation here.
There's a completeness out here, and this is the closure for the green oscillation.
And when this is data, the data wave front will play through this, and will be stably maintained by this.
And what the completeness here says is that we're done with the data wave front.
The next oscillation has accepted the data wave front and is stably maintaining it.
And we no longer need to maintain that data wave front.
We can allow a null wave front in.
So the completenesses are indicating that the data wave front is also passed on to the next oscillation.
So what happens is that wave fronts are stably passed from oscillation to oscillation.
As the oscillations oscillate between data completeness and null completeness,
wave fronts of data and null are stably passed on from oscillation to oscillation.
Now we'll call this structure of the enable rank with the inverted closure followed by completeness a link.
And a link links oscillations.
Now the thing to notice at this point is that this is entirely in terms of logic.
There is no other form of expression.
There's no time reference. There's no controller.
There's no extra logical memory element.
It's entirely in terms of logic.
Since everything is in terms of logic, things can be optimized together.
For instance, links can be optimized into the combinational circuit.
And that's what we'll look at next.
So this is the circuit from the previous slide and this is where we're going.
Now we've detached these links from the input because we want to view these links as associated with the upstream oscillation.
In the same sense that these links are associated with this oscillation.
And we only want to talk about this circuit between this boundary and this boundary.
Now to perform the optimization, we take our closure and we run it into this rank of gates.
We change the threshold to 3.
So now that this rank of gates becomes an enable rank as well as a combinational logic rank,
we've eliminated the rank of gates that were the enable rank.
And we still have a link structure with the enable rank followed by the completeness.
Now if we were able to turn this piece of combinational logic into a link,
can we turn this into a link and can we turn this into a link?
And the answer is yes.
So this is the circuit from the previous slide and this is where we're going.
We take this closure and instead of closing with the input, we're going to close with this rank.
So we turn this rank into three of threes just like we did this rank to make it an enable rank.
We add completeness to make this a link.
We also add a buffer link here which will be discussed later in some other future conversation.
Now this completeness closes with this rank and we turn this rank into three of threes
and make that an enable rank and we add a completeness after that to create a link
and then the completeness of that link closes with the input.
Up here we had a single oscillation around the entire circuit.
Now the circuit consists of three oscillations.
Here's one oscillation.
Here's another oscillation.
And here is a third oscillation closing with the input.
So the next question is can we turn this rank, these gates, into links?
And the answer to that is also yes.
Again, this is the circuit from the previous slide and this is where we're going.
We're going to turn this into an enable rank.
So we have to add a gate here for completeness and upgrade the threshold of this.
And we'll take the blue closure now and run it into this rank.
So now this becomes a link and the completeness of this link now becomes the closure for this rank.
And the completeness for this goes to close with this which is these gates that we turned into an enable rank
and we added a two of two gate here.
And now this completeness closes with that and similarly this closes with that
and this completeness closes with that and now this completeness closes with the input.
So up here we had three oscillations.
Down here we have six oscillations.
One, two, three, four, five and the sixth oscillation closes with the input.
We've added a lot of gates to this circuit so we'll just do a little quick optimization.
These completeness gates here can be combined into a gate that's already defined in our set of functions, the THComp.
And we can also define a cell with this common circuit in it.
And we substitute these, the substitutions are shown down here.
The result of the substitutions are that we've reduced the circuit from 50 cells to 34 cells
and we've also reduced the oscillation period.
If you notice the oscillation period up here is one, two, three, four cells.
So its period is eight cells and the same oscillation down here is one, two, three cells.
So its period is six cells.
So where are we?
We're in a very unfamiliar place, but a very fascinating and worthwhile place.
We're in the world of network flow computing.
And what I want you to understand is that it's a very real place with very real possibilities.
What we've created here is a network of linked oscillations through which alternating waves of data and all spontaneously flow.
And we got to this through a series of fairly straightforward logical manipulations from this circuit.
It's a combinational circuit.
The links can be construed as registration stages around the circuit.
And this could be construed as a handshake between the registration stages around the circuit.
But as we did our logical manipulations, all of these structures started merging.
First, the link merged into the combinational logic.
And as we went on, the combinational logic merged into links.
So in the end, in this circuit down here, there is no single gate that is doing solely combinational logic.
Every gate in the circuit is doing flow control.
Most of the gates in the circuits are performing a memory function.
And many gates in the circuit are performing all three, flow control, memory function, and combinational logic.
But there's no structure you can point to and say that's the combinational circuit.
There's no structure you can point to and say that's the registration stage bounding the combinational circuit.
And there's no structure you can point to to say that's the handshake controlling the registration stages bounding the circuit.
The expressional elements that we're used to thinking in terms of combinational logic, registration, and control, such as clocks and handshakes,
all melt into integrated non-existence.
The component that remains evident throughout all the variations of logical structure is the linked oscillation.
That is why it's most effective in this realm of pure logical expression to think in terms of networks of linked oscillations.
There's no fundamental difference between these two circuits.
They both do the same combinational function.
They both have exactly the same input and output boundaries.
And they're both completely expressed in terms of logical relationships.
Both circuits are also just components in larger networks of linked oscillations.
As oscillations are linked to the inputs here and the outputs here,
this single oscillation becomes a component of a larger network of linked oscillations.
And as the inputs and outputs here are linked, this network of linked oscillations just grows.
So the two circuits are identical in that they are just components of a network of linked oscillations.
The difference between the two circuits is in the granularity of their oscillations and hence their throughput.
The finer-grained oscillations can accept inputs much more frequently than the coarser-grained oscillation.
If you look at the oscillation period here, it has 1, 2, 3, 4, 5, 6, 7, 8, 9.
There's one gate over here. There's 10 gates in the oscillation.
So there's 10 gate delays for the data oscillation and 10 gate delays for the null oscillation.
So the oscillation period is 20 gate delays.
What that means is that this circuit can accept a new data input every 20 gate delays.
Now the oscillations down here are 3 gate delays.
1, 2, 3. And all the oscillations are 3 gate delays.
1, 2, 3.
And so you double that and the oscillation period is 6 gate delays.
So what that means is that this circuit can accept a new data input every 6 gate delays.
And this will run almost three times, over three times faster than this circuit.
So the latency of the two circuits, it's about the same.
The latency for this circuit is one, two, three, four, five, six, seven gate delays.
So what that means is whenever a data input is presented,
seven gate delays later the output will appear.
The latency for this circuit is six gate delays, one, two, three, four, five, six.
So whenever the data input is presented, the output will appear six gate delays later.
But now this circuit will accept input every six gate delays,
and this circuit can accept an input only every 20 gate delays.
Did I mention that these circuits and networks of linked oscillations were expressed entirely
in terms of logical relationships?
There's no appeal to time, there's no clock, there's no delay lines,
there's no central anything, there's no global anything.
And there's no extra logical control.
These are logical structures behaving entirely on their own terms.
Future conversations will expand on the structures and behaviors of flow computing.
And we'll show how to build a risk processor as a network of linked oscillations.
Again, the materials of this conversation can be downloaded from curlfant.net slash ytvideo.
