And we'll take the blue closure now and run it into this rank.
So now this becomes a link and the completeness of this link now becomes the closure for this rank.
And the completeness for this goes to close with this which is these gates that we turned into an enable rank
and we added a two of two gate here.
And now this completeness closes with that and similarly this closes with that
and this completeness closes with that and now this completeness closes with the input.
So up here we had three oscillations.
Down here we have six oscillations.
One, two, three, four, five and the sixth oscillation closes with the input.
We've added a lot of gates to this circuit so we'll just do a little quick optimization.
These completeness gates here can be combined into a gate that's already defined in our set of functions, the THComp.
And we can also define a cell with this common circuit in it.
And we substitute these, the substitutions are shown down here.
The result of the substitutions are that we've reduced the circuit from 50 cells to 34 cells
and we've also reduced the oscillation period.
If you notice the oscillation period up here is one, two, three, four cells.
So its period is eight cells and the same oscillation down here is one, two, three cells.
So its period is six cells.
So where are we?
We're in a very unfamiliar place, but a very fascinating and worthwhile place.
We're in the world of network flow computing.
And what I want you to understand is that it's a very real place with very real possibilities.
What we've created here is a network of linked oscillations through which alternating waves of data and all spontaneously flow.
And we got to this through a series of fairly straightforward logical manipulations from this circuit.
It's a combinational circuit.
The links can be construed as registration stages around the circuit.
And this could be construed as a handshake between the registration stages around the circuit.
But as we did our logical manipulations, all of these structures started merging.
First, the link merged into the combinational logic.
And as we went on, the combinational logic merged into links.
So in the end, in this circuit down here, there is no single gate that is doing solely combinational logic.
Every gate in the circuit is doing flow control.
Most of the gates in the circuits are performing a memory function.
And many gates in the circuit are performing all three, flow control, memory function, and combinational logic.
But there's no structure you can point to and say that's the combinational circuit.
There's no structure you can point to and say that's the registration stage bounding the combinational circuit.
And there's no structure you can point to to say that's the handshake controlling the registration stages bounding the circuit.
The expressional elements that we're used to thinking in terms of combinational logic, registration, and control, such as clocks and handshakes,
all melt into integrated non-existence.
The component that remains evident throughout all the variations of logical structure is the linked oscillation.
That is why it's most effective in this realm of pure logical expression to think in terms of networks of linked oscillations.
There's no fundamental difference between these two circuits.
They both do the same combinational function.
They both have exactly the same input and output boundaries.
And they're both completely expressed in terms of logical relationships.
Both circuits are also just components in larger networks of linked oscillations.
As oscillations are linked to the inputs here and the outputs here,
this single oscillation becomes a component of a larger network of linked oscillations.
And as the inputs and outputs here are linked, this network of linked oscillations just grows.
So the two circuits are identical in that they are just components of a network of linked oscillations.
The difference between the two circuits is in the granularity of their oscillations and hence their throughput.
The finer-grained oscillations can accept inputs much more frequently than the coarser-grained oscillation.
If you look at the oscillation period here, it has 1, 2, 3, 4, 5, 6, 7, 8, 9.
There's one gate over here. There's 10 gates in the oscillation.
So there's 10 gate delays for the data oscillation and 10 gate delays for the null oscillation.
So the oscillation period is 20 gate delays.
What that means is that this circuit can accept a new data input every 20 gate delays.
Now the oscillations down here are 3 gate delays.
1, 2, 3. And all the oscillations are 3 gate delays.
1, 2, 3.
And so you double that and the oscillation period is 6 gate delays.
So what that means is that this circuit can accept a new data input every 6 gate delays.
And this will run almost three times, over three times faster than this circuit.
So the latency of the two circuits, it's about the same.
The latency for this circuit is one, two, three, four, five, six, seven gate delays.
So what that means is whenever a data input is presented,
seven gate delays later the output will appear.
The latency for this circuit is six gate delays, one, two, three, four, five, six.
So whenever the data input is presented, the output will appear six gate delays later.
But now this circuit will accept input every six gate delays,
and this circuit can accept an input only every 20 gate delays.
Did I mention that these circuits and networks of linked oscillations were expressed entirely
in terms of logical relationships?
There's no appeal to time, there's no clock, there's no delay lines,
there's no central anything, there's no global anything.
And there's no extra logical control.
These are logical structures behaving entirely on their own terms.
Future conversations will expand on the structures and behaviors of flow computing.
And we'll show how to build a risk processor as a network of linked oscillations.
Again, the materials of this conversation can be downloaded from curlfant.net slash ytvideo.
