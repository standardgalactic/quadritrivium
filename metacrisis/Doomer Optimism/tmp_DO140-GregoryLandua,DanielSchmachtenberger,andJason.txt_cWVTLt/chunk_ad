things that have to be thought through well in the failure case of those strategies in the process
of designing them. Totally. And the risk here, you know, to be frank is that the biggest challenge
we face is that thinking those through well is an anti signal to investment capital.
Because we're in the middle of this crazy race to the bottom where, you know, a VC wants you to
not think about those things and they want you to just like get like win the short term game over
and over and do that as effectively as possible. And if you're not doing that, it's an anti signal
to investment. There aren't really like the patient capital, philanthropic capital, you know,
where, where are the present moment resources going to come from to really sort of structure
and engage with those questions. We've done a good job of sort of like magically giving ourselves
space and time, although, you know, that doesn't last forever. And the amount of space and time
needed to think through and design, like go through design exercises, pilots be humble and like
expect to be wrong and sort of continue to set up the right sandboxes for this, you know, because
at the end of the day, I think I completely resonate with your intuition, Daniel, that, you
know, in order for this to work, we actually have to get down and redefine sort of like the monopoly
on private property and exiting rights. And that's really key to setting things up so that they work
well is, you know, really at the end of the day, I believe this is all an exercise in reembedding
markets into commons, right? Because that's how markets can be a force for coordination instead
of just sort of like this, you know, this race in which value extraction is optimized.
Embedding markets and commons, I think is a really good high level tag for a lot of things that needs
to happen. Jason, where are you at in this conversation that sort of emerged, you know,
and rapidly sped off as I was like, Oh, this is, you know, this is what we're working on exactly.
Well, I'm enjoying listening to this. I find this interesting. I mean, in the back of my mind,
what I'm thinking is, let's say that somehow your vision was achieved, Gregory, and we were able to
internalize all of the externalities. What would that mean about how society is structured? And
this goes back to my idea about kind of networked bioregionalism or cosmopolitan localism or, you
know, whatever kind of, you know, big kind of generality, you know, we want to think about. And
to me, it's still like, you know, if we're moving away from linear systems, and there's a lot more
recycling of minerals, we develop technology to do that, I still think that there's going to be more
focus and maybe I'm curious if you agree on local production of some things. I think food
is a big one. And so it's less efficient because you don't have the economies of scale. And so
it's an implicit kind of degrowth, which, you know, many people think needs to happen anyway.
Right. And but we still need global coordination. We need global tech, especially in the digital
tech sphere and mining of key minerals and things. And so there will still be global trade,
and there will still be global coordination. But I guess what is, you know, if all of this
goes according to plan, how is society structured differently? Like, do you have an idea about
that or we just have no way of knowing? I certainly have ideas. I'd love to hear
Daniel's comments on this. I feel like, you know, oftentimes Daniel in the you're so brilliant at
describing the meta crisis and sort of like walking through the sort of like logical sequences and
multipolar traps. But oftentimes, I find myself when listening to your conversations in this sort
of like, oh, I really want to know Daniel's opinion about like the world that potentially comes
next, the future vision and paths to get there. They're so okay. I appreciate you separating
those two things because they're really important. The paths to get there have to be inactable
and inactable at the progressively increasing scales that are needed if it is to become,
you know, widespread enough to address global issues. And that means has to be inactable within
the current cultural value systems, incentive landscapes, deterrent landscapes and everything
else, which don't look anything like what an idealized long term system might be. But an
idealized long term system cannot just be made to happen by global divine mandate or something.
You know, so a kind of libertarian type, like a Peter Thiel might ask the question if someone
is to describe what their more ideal long term system is, would say, tell me how your utopia
works and then tell me given how many people for religious reasons and for their political
ideologies and their vested interests don't want it and would do anything they could to prevent it,
tell me how much violence you're willing to enact to make your utopia happen and what the
results of inflicting that much violence culturally into those who do it are and is still utopia.
And so, you know, this is like John Wells' position, the enactment problem. So I think
we have to go back and forth between imagining, like understanding what the fundamental problems
are well enough to think about what solution sets that would be long term viable are,
which is the kind of future planning to think through how to reverse engineer,
but then also understand the current landscape of the world and what is inactable and what creates
externalities when you try to enact it to say what could take a step in the right direction,
also knowing that a heap of stuff will happen that we can't anticipate. So a step in the right
direction, reassess, try again, the forward engineering approach. And you kind of have to
go back and forward between those two approaches and forming each other. So we can talk about
some criteria of what a longer term viable set of global human systems must entail,
which doesn't necessarily say how do we enact them or what exactly they look like,
if that seems like an interesting starting place.
Yeah, and I really appreciate that. I mean, I think that's really an important reminder that
the reconnection of the ends and the means and the dynamic relationship between them and
how impossible it is to separate them. And maybe it's a distinction between a pro-topian approach
and a utopian approach or something like that. So yeah, I think that does a great framing. And
yeah, I mean, I think just diving in and kind of like weaving those back and forth a little bit
would be would be really beautiful. Yeah, I think the kind of classic libertarian capitalist argument
about why not to be utopian is that in the name of some kind of benefit of all the people Marxist
ideology, we got Stalin and Mao and etc. I don't think that is a holistically solid assessment
of the dynamics that happened. But thinking through the ideal world where you're willing to
cause whatever harm you need, hit learn in another case. This actually comes up for me today in terms
of my concerns about how much utilitarian ethics are proliferating as we're doing
planning ethics and machine learning so that the self-driving car can decide who to kill
in a scenario, the passenger or the person, the street or the whatever. And then as a result of
just kind of not only the coding of that, but rationalism in general, much easier to do some
kind of utilitarian ethics, which gets again into these commensuration issues. How many of this
nearly extinct species are worth? How many of these not that extinct, but maybe more sentient
species and whatever?
Question for you, Daniel. In that specific example, do you know if anyone is thinking about
what it would look like to allow a driver to program in their own choice about that? Like the
owner of the car or a company that owns a fleet or something to say, because people make those
choices maybe in split second decisions, but people do make that. That's a decision that
agency would usually sort of vest down to a person in the driver's seat who might choose
to take the risk on themselves instead of killing like a kid or something.
Yeah. So I mean, that's a very complicated topic is when you start to have these very complex
cybernated systems, the control on the cybernated systems affects a lot. So do we try to govern
the controls on them or do we try to make it user-adjustable? Yes, it's being thought of in all
of these places. I would say one of the places the conversation has advanced a lot is the control
of social media algorithm settings, knowing how problematic they are currently when you optimize
for time on site and engagement in ways that increase depression and tribalism and sanctimony
and certainty and wrongness at the same time. It's obvious that they need control differently.
Does the government step in to control the algorithm and ensure that it's in some positive
fiduciary relationship with the society and the individuals who kind of figures out and adjudicates
what that algorithm set should be? Or does each user get to decide for themselves?
And can the user even actually do that well if they can't factor the complexity of what
the non-obvious effects of those settings are? And then also, what are the default settings?
Because even if I can change the settings, almost nobody will. And so the ease of change
in the default settings end up being significant. So yeah, I think this is a very important
is a very important topic. But the place I was going is utopianism causing violence.
The version right now that I see a lot is especially the movement in places that think
about long-termism through utilitarian ethical lenses can say in the most amount of suffering
and the most amount of benefit could occur by humanity existing over billions of years.
And the huge number of people that will happen when we're an interplanetary species. So to
the amount of harm that happens from not getting there is so much more that it justifies anything
we need to do to get there, therefore, dot, dot, dot. And in the name of excessive epistemic
certainty about future cases, we can justify a lot of violence now because it's less harmful than
the other thing in our thought model. And of course, we can do this with climate change because
climate change is going to be so bad, we need to do X, Y, and Z in geoengineering project.
So I would say in general, I want people to forecast and make good choices now based on that,
but I'm also dubious of them being over certain of their forecasts. And as a result,
from a totally ethical standpoint, justifying harmful actions now because it seems less harmful
than in action. And so how to make sure that when you're doing that thing, you're calculating
that you're weighing your own uncertainty heavily enough is really important.
Disclaimer noted, now we want to hear the vision of the future that you want to see and live in.
So let's say that we look at a civilization using the Marvin Harris model of a civilization is,
can be thought of in terms of its infrastructure, its social structures, and its superstructure,
infrastructure being the entire technology stack that it utilizes, social structures being all of
the agreement fields, the economics, the governance, the law, the institutions that it uses for
collective choice making, and the superstructure being its culture and its basically shared values,
definition of the good life, things like that. So if we instead of saying infrastructure,
social structures, superstructure, we could say technology, political economy and culture,
something like that. Then we could say these three things inevitably inter-effect each other.
No one is the most fundamental. Different schools of thought will argue, like Marvin Harris argued,
actually infrastructure drives the other ones almost exclusively. Almost every spiritual and
religious ideology says why superstructure can guide the other ones. Obviously, when in so far as
we're thinking about economic theory and theory of governance, we're focused on the way the social
structures will guide the other ones. So just to go very fast through this, they all inter-effect
each other and there are necessary things that must happen criteria-wise in each of those that
lead to a set of reinforcing dynamics that can lead to a metastable civilization.
If we were to talk about what you're saying, each of those three is nested in ecology. Yes,
of course. These are the three aspects of human civilization and exactly we're totally
nested in ecology and you could say the ecology is nested in physics if you want.
So a couple of things. We talk about the infrastructure side a lot, though in this
conversation we've actually been talking about social structure, currencies and governance and
like that, but we've been talking about abstractly. We talk about infrastructure like a closed-loop
materials economy. We have to ensure that we are not meeting human needs via turning nature
unrenewably into meeting those human needs and particularly unrenewably at larger scales
and can continue to happen and then creating waste faster than can be processed. So one,
you have to move to completely closed-loop atomic accounting like at an atomic and molecular level
closed-loops and you have to go post-growth. It doesn't mean there can't be growth, but it means
that the embedded growth obligation has to go away so that any growth that happens is actually
calibratable with the environment. So now what actually does that materials economy look like?
I think, well obviously our energy production has to transition in that world to a renewable energy
source and a post-pollution energy source or where we can be processing that pollution within a
set of processes that can be described as closed-loop adequately and I think that
this is where if you just say what is the very long-term versus what is the very near-term
based on intermentancy issues and based on energy return on energy investment.
Do we get off of hydrocarbons exclusively with kind of decentralized renewables? Probably not.
Probably some large-scale centralized renewable type things like deep geothermal and
maybe space-based solar or whatever are probably important and probably nuclear is important but
here's where I would say thorium nuclear and phase three or phase four and micronuclear in
non-seismic areas with hardened energy grids and different waste management is really really
different than phase one and phase two nuclear without a hardened energy grid in seismic areas.
So you shouldn't think of nuclear as a category just like you shouldn't think of vaccines as a
category you should think of each one with its own unique pros and cons but there is obviously
it takes a certain amount of hydrocarbon energy to build any of the new energy since that's where
energy comes from so there is a transition of the energy economy and then a transition of our
materials acquisition and our waste management and so we can think about that there's a whole
lot more in terms of what should be local what should be at various scales what should be at the
city scale the bioregions scale the continent scale or the multi-continent or planetary scale
obviously there are some minerals that are really important for things that we care about that we
don't know how to operate without that are not evenly distributed across the world that can't be
only at a local scale on and on but like do we need to be processing organic matter across
bioregions or should organic matter all process closed loop within a bioregion probably should
and water cycles and things like that but so there's a whole set of criteria for what is the
future of infrastructure the scale the criteria and the transition of the specific subsets of the
technologies when it comes to superstructure and social structure the first thing I think
is really worth saying is that if your social structure i.e. your system of governance and
collective choice making that is enactable via something like rule of law and maybe a monopoly
of violence if that social structure is not grounded in superstructure i.e. if it's not grounded in
the collective values and will of the people it will be oppressive and yet at the same time if the
collective values and will of the people want things for themselves that is bad for other people
or bad for the world the thing that is grounded in their values will also suck so ultimately you
have like the cultural work to have people understand all of the trade-offs well like actually
one to have them care about every part of the web of life they're interconnected with and have an
identity that is an emergent part of an interconnected web of life not a separate our
nation our people are religion separate from the other guys or our species a culture that
doesn't do that is a culture that will be bound to war and environmental conflict because the desire
the identity of the people is separate from the world enough that they will use their technology
in ways that cause harm to it so the identity and then the associated values not all value sets are
actually commensurable with being able to steward the power of exponential technology only a few
of them are and given if you can affect everything all that much you have to care about it which means
you have to have an identity that's bound with it I do think that becomes obligate of any worldview
that advances itself and the ones that don't can't be part of a worldview that advances themselves
because warfare which most all worldviews in the past have done with exponential technology
equaling exponential warfare self-terminates and externality is which pretty much every
which you know many worldviews throughout history have driven lots of environmental externalities
also cap out so only worldviews that don't drive externalities and warfare i.e. care about everything
and of the complexity to process it have the possibility of continuing in the ecology and
physics of the universal way that it is hard question here for you what do we do with people
who have a warfare worldview how do you how does that transformation this is kind of like moving
into the critical path or like the path dependency you know what what how do we dance along that line
to decompose or compost or you know allow people to to regenerate or evolve their worldview
you know because obviously if you impose if you act with a worldview that can conceive of sort of
like cleansing anyone who has that worldview you've just collapsed into it right so what's the
yeah i'd love to hear you riff on that for just a moment in service to the like toggling back
