of political system, which is what I see now and what I feel is this kind of political nihilism,
because we have two parties who are pretending to be different, but they're so similar in everything
that they end up doing that the actual choices seem inconsequential. So it's like, fuck it.
It's an old libertarian trope, same shit, different piles. It doesn't really matter.
And of course, I respect that there are certain differences in certain things and whatever. And
some of those things are important, arguably. But for the most part, everybody's kind of doing
the same shit and it's not really helping either way as we go. So then it ends up being like,
all right, politics aren't the way, which is just, again, retreating to this political nihilism,
which is not going to work because then there's literally no solution.
So what you're talking about is actually cleaning up the epistemic commons,
the ability for us to access information and then reinvigorating faith
in a political system by making demands that politics actually work so that we actually care
about it. I mean, that to me, it seems like I don't see another solution here.
Yes. Yes. Say something divergent first and then come back. I was just smiling because I was
remembering a conversation I had with a friend the other day. And he said, you know, there's a big
chunk of Americans who kind of hold it, not consciously, but intuitively, that the founding
fathers and Moses went up the hill together and came down with the tablets and the Constitution was
on them. And it's this kind of like, they're treated like scripture, right, treated like
anything that you can do to change it is definitely making it worse, which shows both a good respect
for the wisdom of it, which was actually really important, shows how much the attempts to change
it have been in bad faith or at least ill-informed, but also doesn't acknowledge how fundamentally
different the world is today than then and how well formed they were at the time, given the
capacities and yet also simultaneously inadequate. And this is where one of the deepest dialectics,
when we think about left and right or the various ways political polarization occurs,
one of the dialectics, there's a lot of ways to talk about this, is traditionalism and progressivism.
And the traditional impulse says, let's conserve the conservative impulse. Let's conserve the old
institutional wisdom, insights, et cetera, that have worked for a long time because of something,
whether it was a religious system or a political system, if something got us here,
and most of the civilizations failed, most of the religions failed, most of the, there's some
evidence that that thing was affected, and maybe we don't even understand it as well as we think.
So we break it and it was doing something that we didn't know. So there's almost like an intuitive
sense of conserving the wisdom that is in that thing, right? And some people almost intuitively,
they wouldn't even frame it that way. They have that sense. And then there's a progressive intuition.
And the progressive intuition says, we're dealing with novel problems that we have not dealt with
before. And evolution moves forward, it doesn't move backwards, new adaptive capacities based on new
environments, opportunities, stimuli. So we need to actually rethink situations and come up with
new novel innovations and insights. Obviously, all of tech has that focus, right? We're coming up with
new and science has that focus. So can we apply the kind of the fact that the technosphere is
changing means we might need different ways of thinking about and governing it? And can we apply
that same kind of progressive insight that we're doing in science to our social systems? Those
need to not be in a fight. Those need to not be in a debate, but in a dialectic that says,
where do the previous systems and previous thinking have real wisdom that we have not,
if we haven't been good enough students of history, we don't understand as well as we think. So we
criticize it as irrelevant old dumb, whatever, because we didn't go through a war, we don't
actually know, we don't understand real politic from an embodied experience, because we live in
a rich abundant time, not the difficult time or whatever it is, right? So we break the thing down
and then we realize that we fucked ourselves because we didn't understand it well enough.
How do we make sure we understand it well enough that we conserve what should be conserved?
And then how do we understand both new possibilities that can make this better than could
have been made by the wisest people at the time, and new problems that need new things and create
new progressive insights that are fully logically consistent with the previous ones that are worth
conserving, right? How do we do that together? That dialectical process of, and there's a similar
one for individualism and collectivism, right? We don't want the individual advantaging themselves
in a way where they're ruining the commons and in rivalrous dynamics with other people. We also
don't want a way that prioritizing the commons ends up being oppressive for individuals. So how
do we make a situation where you have collective structures like systems of education and healthcare
and economic incentives that condition better lives for the people that are born into it,
but better not meaning just dependent on the system, better meaning more sovereign and self-directing?
And then how do you develop people that also have a civic virtue? So they in turn work to
improve the system. So you have a virtuous cycle of more individualism and more collectivism
simultaneously, you know, more benefit to the individual and more harmony between the people
and the commons. And so one of the key things, defining things in the kind of to get past
culture war, narrative war, is to take the values that different sides are speaking to and recognize
that there is some true value on both sides that doesn't have to be held as fundamentally
dichotomous, but actually is necessarily symbiotic, synergistic and say, how do we hold the true part
of both sides of that dialectic and seek something that is at the level of the synthesis?
Yeah. And this is not something that we're seeing happen in our political system to any degree. And
I think it's something that we ultimately have to demand, but it's not just the political system,
like you can look at this at another polarized topic, like vegetarianism versus, you know,
eating meat and even a carnivore diet, right? There's so much identity wrapped up in each camp
that there's virtues to both of these different, both of these different dietary styles, but people
are so entrenched in their own ideology that it's not a dialectic. It ultimately becomes
a heated and, you know, debate with all kinds of ad hominem attacks and all kinds of different
logical fallacies being used and obstruction and misinterpretation or interpretation to
advantage of all different kinds of data. And we end up getting in this place where
we have that kind of epistemic, you know, hubris on one hand, which is something you talked about,
where people are very overconfident about what they believe to be true because of the information
that they've seen. And then other people are like, well, fuck it, we have these experts who know more
than I ever will about these different topics, and they're all disagreeing. So I don't even know
what to care about. So then you retreat into nihilism. And then you end up in either one of
these two sides where you're confident and you're sure that you know everything because science said,
you know, or you're like, who knows, because science is saying this way or this way or science
is saying this way or this way. And this is the place that we find ourselves in in these key
important issues. Let's go back to your case of Thanos. Thanos was motivated, at least explicitly,
right? Maybe there's other implicit motivation, but explicitly by the desire to serve the
universe and life. With the utilitarian ethic that says I'm willing to cause the ends justify
the means I'm willing to cause some harm to prevent worse harms, it's a trolley problem
calculus, right? And he had a certainty that the universe was going to end and self terminate if he
didn't do that thing. So doing that thing was not only ethically okay, it was ethically obligate,
because the ethics of inaction for him guaranteed an outcome that was so much more horrible,
right? This is now a key thing of why utilitarian ethics can be dangerous. We of course need to
do utilitarian calculus of saying, well, not just does my action seem intrinsically right,
but what will the consequences be? Because, okay, don't ever lie. When the Nazis come to my house
and ask if I have any Jews there and I do, I lie, I say, no, I have no Jews here. In that moment,
the virtue ethic that says don't ever lie is less important than the utilitarian calculus of I would
rather lie to the Nazi in this situation than send these people to the slaughter. So utilitarian
calculus is necessary, but it's also not sufficient. It creates real problems in the main place it
creates problems is when we believe, pretend that we have more certainty than is actually
epistemically warranted about what's going to happen in the future. Then, and that's what Thanos
had, it was an excessive certainty that he had that the universe would be worse if he didn't do
that, that it made it ethically obligate for him to do that thing. So very often, we have some
argument of, okay, well, the future is definitely going to go this way if I don't act. And that
definite is actually unjustified. It's based on some simple rationale that seems unavoidable to
us. But how much is in the unknown, unknown set that is relevant that we don't know that we aren't
factoring is the place where we get, that's where the real critical thing is. So we can't predict
whether 10 days out with the very best supercomputing types of capabilities that we have, because
complex systems are complex, right? That's the thing to understand. And yet, when I'm so certain
that such and such is going to happen for civilization, that if I don't, whatever, then the
utilitarian calculus can make me do super unethical things, where contextually, they're actually the
only ethical thing. So this is where excessive certainty is extraordinarily dangerous. And you
notice that the holy wars were fought based on certainty, not uncertainty. People don't say,
I don't know the reality of God. It's an interesting question. I feel humbled by the awe of it. Let me
go slaughter some people. It's like, I know for certain. And so I'm willing to go kill and die
for that thing. So if we don't overcome the certainty bias, one, we don't learn very well,
because now you have confirmation bias that just seeks confirmation of the thing you already believe,
and it's actually only a depth of curiosity that makes you keep learning well. And two,
we actually become dangerous. And particularly so with increased power, exponential tech power.
The thing that should give us a good, healthy dose of understanding of the mystery, the factors
that we cannot factor in, which is really the argument that Charles Eisenstein makes in the
more beautiful world, our hearts know as possible, is it's an argument from his own spiritual gnosis,
gnosis with his knowing with the G. This felt sense of the state of inner being, the connection
that we have to capital S source, what you can call God, but God has a lot of connotations to it.
But it's a felt thing for many of us who've gone through different plant medicine journeys and
experiences, this understanding that the universe, the earth itself is alive and that there is a
binding and common force and a wisdom and this felt sense of love being the currency that is
underpinning everything else. That there's a reason why that's been called the mystery,
capital M mystery, because we can't fathom it, but we can tap into it and we can feel it.
And so to assume that we can understand what's going to happen with that in play, and this is
something we talked briefly about before, you know, the Dow, whatever you want to call it,
that is ultimately the mystery to pretend that we know that thing. It's incomprehensible. It is
the ineffable. So there's a certain humility that we all have to have saying, we actually don't
understand more than we do understand. And you can look at it, even if you want to take it out of
the spiritual contest, look at the bleeding edge of quantum physics and our understanding of what's
happening at the subatomic level and the laws of the universe, which are continually evolving.
We should have just a tempered epistemological humility, epistemic humility, which is really
one of the solutions you're pointing to. But when you really understand that, that there are the
ineffable factors, it only makes really a lot of sense.
Okay, so this is, this is a topic that's on my mind most of the time. We're wanting to encourage
and we're wanting to develop a culture where people do much better sense making,
using rigorous processes while understanding the upper bounds of their own sense making.
And that the upper bounds don't give the kind of absolute certainty that
before certain kinds of development where people can be emotionally oriented to want.
So, you know, you and I were talking about this previously, the first verse of the Tao Te Ching,
this always was so meaningful to me that, so loud, so whoever that was going to write a book
of wisdom. And the very first thing in a book of words was the Tao that is nameable is not the
eternal Tao, the knowledge that is knowable is not the eternal knowledge. And the naming
is the creation of the 10,000 things and the 10,000 things will obscure you from understanding
the Tao. So it's like, okay, here's a book about the Tao and you can't do it in words. If you do
it in words, it's not the thing. But then finishes the book, right? That's the key insight was it
wasn't just the first verse and then nothing else is then the rest of the book, which is saying
the words are pointing to something beyond what can be captured in them and see if you can notice
what that is. And so it's like, okay, is that just kind of superstitious mumbo jumbo or what is
what does that really mean? So you're mentioning quantum mechanics. So Heisenberg's uncertainty
principle says, we can't know the position and momentum of a quantum particle simultaneously
in full, the more you know about the position, the less you know about the momentum and vice
versa. That's an upper bound on the knowable itself. Right? It's saying that there is a rigorously
unknowable that is at the foundation of all of reality, that there is an upper bound to know
ability that is fucking fundamental. That's important. Then you go to girdle's theorem in
mathematics. And girdle's theorem showed that for any arithmetic set, because David Hilbert
was trying to make this process of a complete set of mathematics, a fundamental set of math,
a standard model from which all math can be derived. And in the process, Girdle did a proof
that showed that that could never happen. And one of the most significant things that ever
happened in math, because it was again, an upper bound on know ability said for any arithmetic
set, I have a finite set of propositions in it. There is some other proposition that is true with
that, meaning logically consistent, that can't be derived from a combination of those things,
meaning that there is no finite set that will propagate the entire set, meaning if the system
is to be consistent, it can never be complete. And so I can just like position and momentum,
I can get consistency, but not completeness. So I'm bound to incompleteness, that's why it's
called girdle's incompleteness theorem. And that's a huge deal. Then Tarski's theorem was a
generalization of that from arithmetic to all formal logical systems. The axioms in the
formal logical system can't be proven within the formal logical system, they have to be taken. So
the system can show its own validity, but it can't do soundness. There's something outside of the
system necessary to say, does that validity map to what is outside of it? So these are all upper
bounds on noability itself. And so another way of saying it is, you take it, you know,
all of the study of human medicine before the genome, you're like, wow, there was some really
critical show we didn't even know existed, then we get the genome, but we don't have the epigenome,
we don't have the transcriptome, we don't understand the proteome, we don't understand
the exosome, the whatever. So then there's a new thing, we're like, wow, this thing is kind of
everything. The history of the epistemic hubris is not paying attention enough to how much
whole new fields emerge that we didn't know we didn't know before, they were in the unknown,
unknown set that answer huge amounts of stuff, right? So can I prove that there's nothing that's
in the unknown, unknown set that is relevant to the topic that I'm looking at? So what that means
is I'm studying a thing, I'm studying a body or a cell or a plant or a market or whatever it is.
And I try to model it through some small number of variables I can make sense of, there's this
thing called supply and demand and rational choices and whatever. So or there's a thing called mitochondria
and ATP and NAD, I'm going to try and model the thing, right? Using what I know of it. And I don't
know how tiny a subset that is of what's actually happening. I can't even know how tiny the subset
is of the things that are happening. So my model of reality, and this is what science is doing,
and it's helpful, right? It allows us to build tech and the fact that tech works is very interesting.
But the model of the thing is not actually the thing, right? This is the map is not the territory
thing. And so if I have a thing, I make a model of it. Now the map of the thing is the thing for
tech, for physical tech that we made, right? And that's the difference between complicated systems
that we build that don't self organize and don't evolve and don't replicate versus complex systems
that do self organize, self correct. There's a fundamental difference in the nature of those
systems. But when we're trying to understand complex systems, which is psychology, biology,
sociology, ecology, the foundational things, the model of it isn't the thing. So when I then
optimize for the model, wherever the model's wrong, wherever it's missing something is where
that thing that I'm optimizing can externalize harm. So I find a specific way that a disease is
acting, I find a specific molecular target, I create a drug for that molecular target, that model,
and it works, and it stops that symptomology or that aspect of pathogenesis. It causes side
