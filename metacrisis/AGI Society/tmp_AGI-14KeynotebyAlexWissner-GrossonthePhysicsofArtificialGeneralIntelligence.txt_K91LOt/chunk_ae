a chess game restricts their future
freedom of action is they're trying to win
their way through a bottleneck with the promise
that over a longer time scales
they're going to experience greater future freedom of action
maybe they're going to get monetary rewards
that allow them to do more things over long
terms than they can over short terms
I think this is a very satisfying answer
and maybe even better
analogy is in finance
where we have this ubiquitous concept
of investment we're willing to
reduce our liquidity over
shorter time scales which is also
reducing our future freedom of action
in return for the expectation that over
longer time scales we will increase
our net liquidity
but at the fundamental level
all these this principle of investment
appropriately generalized just produces
down to this idea that there are bottlenecks
of some sort in future path space
and if we're
if we really are just future freedom of action
maximizers over
some appropriately longer time horizon
then we are willing to sacrifice
future freedom of action over
shorter time horizons to win through bottlenecks
yes
this is a particularly good example
where you limit your short term
options in order to maximize
your long term options
sorry so what's the question
would possibly be ethics
be a very good general example of this
where you constrain your short term
options in order to greatly
maximize your long term options
obviously in other words ethical behavior
about ethics is abstract
so to proceed further to the part two
before one
paperclip maximizers everyone's
favorite hobby horse for
unintentionally unfriendly AGI
so an analogy I like to use
is this notion of paperclip maximizer
I'm sure everyone here is familiar
with the concept
can be treated as
just one data point and a very large
set of unintentional consequences
of optimizers in general
but in the same sense
that
for appropriate definitions you could say
embed an entire class of Julia
sets in a Mandelbrot set
my
this hasn't been formalized yet
but my
intuition and my expectation is that
we'll discover that you can embed
a wide class of
sort of naive optimizers
including paperclip maximizers
in this much broader theory
and then to speak
more
to the question of friendliness
so after the paper was published
got a lot of correspondence
and one of the most interesting pieces
of correspondence was the question
so could you actually go use this
theory and would you
formalize as about concepts
of friendliness
and first I thought
what's a really silly, overly
ambitious goal
but I thought about it a bit more
and certainly in light of
Polonius
and Fubo's work on empowerment
I think there may actually be
a possibility
here that this type of approach
appropriately generalize
would let us take
something like Asperger's Laws of Robotics
and say
start from zero flaw
not to spoil a series for anyone
but
we've all read it anyway
so to take the zero with law
that our robot may not
harm humanity
and say okay so if we really
sort of took this goal seriously
we want to sit down and lock ourselves in a room
and write down an equation
for Asimov's Laws
so that we could do this rigorously
and make sure that our robot friends treat us well
how do we do that
and I think there is a plausible
reason why I'm thinking about what we might try to do
is maximize the causal
transfer entropy from robots
to humanity so zero with law
trivially follows out of that
that instead of trying to maximize its own
future freedom of action we're trying to maximize
humanity's future freedom of action
so that gives us the zero with law
for the first law that
robot may not harm human being
except where that confers to the zero
with law later on
in his series
I understand why if you kill a human being
or injure a human being
let's forget about the zero with law for a minute
so when a human is injured
that's reducing your future freedom of action
if you're no longer able to walk that reduces
your ability to explore phase space
all other things being equal
so I think intuitively
you can see how maybe the first law
would fall out of this principle as well
that all other things being equal
regarding the zero with law
it wouldn't be an interest maximizing
the freedom of action of humanity
if you injure a human being
or through inaction allow a human being
to come to harm
secondly for the second law
so the second law is
a robot must follow
essentially must follow orders from human beings
so if you imagine humanity
stuck in a container with
a set of AGI robots
that as well imagined
so this
combined system and humanity in particular
would be able to realize a far greater diversity
of paths if it's able to
leverage robot abilities
to do things
and so if we're trying to maximize future freedom of action
of humans humans can do a lot more
if they have access to robots
and can control robots by orders
than if they don't
so you can see how the second law
falls out of this principle
and then finally the third law which says
a robot can't destroy itself
or can't hurt itself
a robot can't allow itself to come to harm
unless this conflicts with the first
three laws 0 for 2
again so if we imagine
humanity and robots in a container together
then if the robots
for some reason that doesn't conflict with the first three laws
allow themselves to come to harm
then they won't be usable by humanity in the future
to accomplish things that humanity
wouldn't have been able to accomplish other laws
so this is a very
you know sort of shorthand heuristic
and entertaining
I would say sketch
for how one might try to take as most
3 plus 1 laws
and try to reduce these down to a single
information of your right principle
so that's our question here
right so this is kind of your end point
that you're now going to and working forward
or backwards I guess
I love giving talks backwards
so are we going to be happy
with conscious robots that are slaves
would they be happy in that condition
because this is kind of implying
a slave based kind of
separation between the robot
class and the human class
and wouldn't it be better
