and you could ask
so does this theory
speak to that at all
and the answer is yes
so if you imagine going back to your example
you live in a universe that has a finite
set of n resources which
can be consumed
and you have let's say
only one time step to consume them in
so that means there are impossible actions
you could take to consume
so the cause lines would be associated
with that time horizon of one step
would be log n
and interestingly
you get the log n for the fact that you're maximizing
the cause lines over here so you could
posit that
this is sort of a side project that might
have corresponded with a number of
economists who specialize
in the information area of economics
a major outstanding question
in economics both micro and macro is
what is wealth
how do you define it
if you ask the leading practitioners
today they'll say wealth is some
linear combination of labor
capital plus natural resources capital
real estate
currency a few different things added together
but maybe that's not as satisfying
as say measuring wealth
in physical units of kilograms or meters
so some of the
economists that have corresponded with
think that this notion of causal entropy
is sort of first plausible
mathematical formalization of the notion
of wealth so that you can actually
calculate the wealth of a system
or a person or economy
and then going back finally to your example
that there are resources in a constrained universe
we get this
law of
finite resources out for free
because we're measuring an entropy
so it's purely speculative
obviously but
if I had a gun to my head
and I was being asked so
where does this law of wealth happiness
affect our products and maybe
this is a reason
so lots of hats now
from left to right
I'm wondering if this would work
remind me a bit of the work of Simon Kirby
and other linguists at the end
who tried to explain the structure
of human language
and in terms of
the communication
but we have a finite amount of time
to communicate a lot of information
and unless there's some kind of structure
in that it's just the
it's very chaotic
they use the term
right so or maybe
another example since everyone loves
slide 5 metaphors
has everyone seen
the movie from the 70s
Colossus the Forbidden Project
scary movie
so what do the two AIs
the beginning of the movie first do
the USAI and the Soviet
for those that remember
first thing they do
they start communicating with each other
so one can say immediately
now we know why they're communicating
from first principles because they can
access more path through face to face
if they're coordinating versus not
and this actually I mean sort of
it's a silly example but
for more sort of real world systems
energy systems
smart grids where you have to decide in real time
do we keep these patches of smart grids disconnected
do we allow them to be completely independent
and uncoordinated or do we want to connect them together
this allows you to actually shut up
and calculate answers to that question
whereas previously there was no really good
formal way of doing it and the answer would be
so you connect them up if your theory shows
that you're going to access more
future path through face to face by connecting them up
in coordinate, yes
so my question is a problem to make
which is actually I don't see
how you don't try to localize
amount of that
since you're following the branches
there may well be a point where you have
lots of open possibilities
but then for having more
possibilities you need to follow one
particular path where you won't have any possibilities
for a long time
and then have more possibilities there
good question, so the question was
since we're just ostensibly
doing naive gradient following
why don't we fall into local minima
local maxima, whatever
why don't we, you know, how do we avoid getting trapped
the answer is
this is the difference between a conventional
and a causal entropic force
so we can see around local minima
local maxima
with this time horizon
so think of this in the sense
of sort of smoothing out
a landscape that can be enormously
gillian discontinuous
but because we're looking into the future
we can effectively smooth over
these local maxima
and the other way of putting it
is if we take this
time horizon extended to infinity
if we get ourselves caught at a local maxima
then we're not going to
have access to all the possibilities
we would have if we were out of global maxima
and by stretching
this time horizon out above the
time scale needed to jump from a local maxima
to a global maxima we've effectively
smoothed over that
that hilly landscape
so the short answer
to your question is
the causal part of causal entropic maximization
smooths that part over
so that with a long enough time scale
again with a few reasonable assumptions
regarding say, herbidicity
you don't have to worry about getting caught at local traps
what is it?
yes
so I guess we're going to pull out one question
so what I see here in the eye terms
is a heuristic result
of problems in the world of heuristics
if you go up to a fixed step
from a horizon apex to a heuristic
you go up to a fixed step
you have in a full AGI
you'll have to be multiple heuristics
so in my mispandemic
I built an old championship bubble
a bubble program
mobility was one of the heuristics
potential mobility in the future was one of the heuristics
those alone couldn't solve the problem
you needed additional knowledge
so there's knowledge in an intelligent system
about how you define the spaces
that you search
how you search the spaces
if you've only got things like
mobility or entropy
it's very easy to construct problems
in which
there's lots of space over here
there's little over here, the solution's over here
but it's further down than your
sure
so without additional knowledge
you can never solve such a problem effectively
with this kind of approach
it's one heuristic
for one aspect of intelligence
it's not a proposal for all of intelligence
so I can turn the question around
and say something anti-symmetric to you
which is that
you implicitly had some external utility function
that you were passing into the system
anyway which is win a fellow
