simple incorporation
of online learning
into the process accumulates
assets exponentially
so let me close with just this slide
I think
this sort of speaks to the ambition
of the larger program here
which is trying to formulate
problems in AGI
in terms of simple physical
principles with the idea that
if we know if we can
sort of summarize in a single sentence
that can be readily encoded in software
a protocol
that AGI follows without specifying the mechanism
this is only chasing after
a protocol that if this system
accomplishes these types of effects
then let's call that intelligent
if we know if we can sort of establish
what a protocol for AGI is
first then
I think and hopefully some of these examples
take baby steps towards demonstrating
that if we know the principle
phenomenology of the protocol
we can then work backwards to
identify the most efficient mechanisms
for achieving that protocol
so with that I'll close and with whatever remaining time
I have, happy to take more questions
I want to pose one question
so
you probably saw that this
psychologist and columnist Gary Marcus
he wrote
a response to your paper
is Gary here?
no, I don't think so, but it was not entirely
favorable
to briefly summarize
his views seem to be
that psychology
and complex systems
are sufficiently rich
as to elude any simple
mathematical principle like this
and the examples that you've given
appear to us
to be analogous to various complex
real world cases, but really
they're extremely simplified examples
with few variables
and without the contextual richness
of ordinary life
so I guess his view would be
even if some entropic principles
explain
physics at a lower level
and can explain some
simplified examples that look to us
like human psychology
or sociology
to explain the rich behavior of human beings
you would need
in practice
rather than in some theoretical principle
of reducing humans to particles
in practice you would need a host
of other principles
alongside anyone
principle like this and it's going to be
a complex
mass of explanations more like you see
an actual cognitive science
that sort of relates to what
Paul was saying
not exactly the same
sort of
rationality
which provides simple theories
if you don't pay attention to boundedness notion
for example it's boundedness that
causes all these other things to have
to be part of the theory
to make up for the fact that you can't be
purely rational of infinite knowledge
in a complex universe
I'm curious for your summary reaction
to Gary Marcus's complaints
which you probably have encountered before
so I'll first give
my summary to your summary of your
comments
so my response to your summary of his comments
is to say
I was an undergrad at MIT
my first research advisor was Marvin Minsky
Marvin
ultimately
I would say sort of had this
somewhat similar belief
to what you're summarizing
that intelligence at the end of the day
isn't one sort of simple effect
it's an accumulation of lots of effects
that live in a society
and are sort of all competing
and cooperating at any given time
and that there's no at the end of the day
no sort of simple equation that we can use
to summarize intelligence
that is one school of thought
that said
I think we can be more ambitious
than just saying
sort of throwing up our hands and saying
there's no simple explanation for intelligence
like there is for Lyft
print physical principles or phenomenologies
that once understood enable us to do
really incredible things
I think it's an incredibly valuable enterprise
to continue the search for simple
unifying principles that will let us do
these ambitious things rather than just saying
without having spent more than
half a century on this problem
oh it's just a complex combination
of lots of things that we don't really understand
and ironically Marvin used to scold me
for using the term
inversion he would always slap me on the wrist
and say whenever you use the term
emergent that's just indicating to everyone
that you don't actually understand
Marvin doesn't like
self-organizing complexity
ironically and yet there's
full school of thought that says intelligence
is just a combination of a million things that we don't fully understand
I don't come from that
school at the end I'm very hopeful
that we will be able to find simple
unifying explanations for all of these things
and if we don't
we don't lose anything from the search
but if the search succeeds
now we found an e equals mc squared
for intelligence at best
intelligent planning at worst
that will let us do really powerful things
and understand through
really simple thought experiments
how an AGI would behave under a variety of
circumstances
yes
when you are using this kind of argument
that using the
entropy principle
allows you to do very useful
things then it seems that you are
substituting
your entropy principle under a utility principle
also when you said that
you can write the Go program
which wins
it seems that the entropy model
is just a sub-model
towards utility
I would say it the other way that
writing a Go program that wins
writing any program that
maximizes some narrower utility function
like a winning Go program
and just a side effect of a deeper principle
that is much broader than that narrow
context
you can put a gun to the head of the
computer Go player and say
I am going to punish you
through reduction of your future free of action
if you don't do a good job
and then you can see immediately how that side effect
follows up but humans if you put a human
in a room with a Go board and you say
do whatever you like
then a human won't naturally play Go
we need incentives
this is the most
we talk about reinforced learning
about this community this is in general
I would say a theory that underlies reinforcement
without requiring the specification
of a narrow utility function
this is a fascinating
discussion but I think
in the interest of not having
