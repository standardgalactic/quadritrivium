to actually have
the robots internalize the values
and see themselves in humanity
as one expression
so I
pause with this I would say
only as a thought experiment to
try to sketch out how one
possible play out of the universe
as about the universe could be described
in terms of very simple information
here at principles is not
in any way to endorse as much vision
or anything else but for those
sci-fi fans of the audience that is in fact
the very end of the foundation series
what you described about that
I didn't read it but I'm saying
sorry this is a clear
implication of what he's saying
is that in this slave class
we're going to create an AI
that I think is actually inhuman
because any AI we create
like I would say is human
it's a can of worms
put it that way and I'm curious
at the show of hands how many people have read
some substantial fraction of Asimov's
universe
okay almost every one
I'm glad and so there was another question
over here yes
yes from previous last year I've seen
I've got the impression that you wanted this goal
to maximize the future freedom to be able
to perform the intelligence
itself but here you are basically saying
I can maximize your own future freedom
to maximize the humanities
that's right so
I mean the concern about safety
is presumably about the first
scenario where it's maximizing its own
things and I was wondering
if you did get the goal to maximize
somebody else's future freedom of action
would it still be
attention according to your theory
because I thought it was necessary to maximize
it's a great question
so the question is
in previous talks that I've given
the focus is on systems that maximize
their own future freedom of action
they don't displace this also
on other systems
I don't have a terrific answer
to that except to say
as a provocation maybe we need
to generalize
the notion of the information
theory notion of freedom of action
maximization as
something that can happen to any system
versus itself
but that's a great provocative question
yes question here
I mean more people who've actually understood
and internalized the point of Asimov's novels
it's not a great question
so I
would say this is a universal
utility function so this is not
specific to any particular contingency
and when we tell stories
it's often the case that stories contain
implicit bottlenecks in the future right
we have to go on this adventure to save
someone because if we don't
save the person then the world falls
apart so I would say
if anything this is sort of a storytelling
bias that we have that we like
stories that contain bottlenecks
and so it's very natural
to interpret or even dare I
say misinterpret a fundamental
process which is maximization
of future freedom of action as
something else that I'd say
by trying to maximize a narrow utility function
that's story specific
only because we like to tell ourselves stories
that contain bottlenecks
you say it's not a racial thing
and I don't
my feeling is that
right
basic entropy
from ourselves on to the
AI it seems like to me
you know we're moving the entropy
burden on ourselves
well this is sort of what
my thought about
friendliness
and causal entropy maximization
is
the question becomes friendliness
to what entity
because I mean if
I view myself as a system
or we look at the whole system of humanity
or my extended self or whatever
and the system is acting
so as to maximize
its future avenues for growth
expansion learning however you want to
formalize it probably
my way of doing that over the next
thousand years is not going to
involve remaining a human being
nor remaining hardly anything
in common with the current Ben Goertzel
but there would be a
maximization of growth possibilities
by replacing my brain with something
much much much smarter
so then that's not
what's normally meant by friendliness
because people in the friendly
AI community tend to mean
friendliness to human beings
and something like recognizable form
but it is friendliness
to causal entropy growth
if that's what we want to be trying to do
that's a well on an astro for too much
longer since it seems to
but so
something that
as most of you self recognized was that
his
for laws were in a certain sense
fundamentally unfriendly to a human singularity
that the robots
what's programmed with these laws didn't
really want humans to ever transcend
their biological limitations
because they had already done that
they were programmed to keep humanity
in a recognizable form
and I don't want to spoil too much more of the foundation
yes
good thank you
but it's
so it definitely seems to me
like next month causal entropy
is a big part of the intelligence equation
but I'm not sure it captures
all of it and I have this image
in my head of like a bunch of humans
with like a lot of energy sources
and they go like spend them on something
that seems cool and the AI is like no no
no wait you can't spend
the energy
you need to like keep all your options open
like keeping your options open is
like
sometimes a good tool for taking what you want
but it doesn't seem like a cool story
so that's an excellent question
I'm glad we moved off
so
the way to answer that is what you're positing is
imagine we have a closed universe that has
finite resources
how do you act in a universe that has
finite resources
if the resources can be consumed irreversibly
then all other things being equal
you want to try to conserve
and avoid consuming them
I would say
maybe a better example
a more explanatory example is there's all this
microeconomics literature that suggests that
humans evaluate costs
logarithmically rather than linearly
so if you want to say
if I'm worth a million dollars how happy
am I about my
my self
versus a hundred thousand dollars
and there's a wealth of microeconomics literature
that says your self-happiness
grows approximately logarithmically
with your wealth rather than linearly with your wealth
