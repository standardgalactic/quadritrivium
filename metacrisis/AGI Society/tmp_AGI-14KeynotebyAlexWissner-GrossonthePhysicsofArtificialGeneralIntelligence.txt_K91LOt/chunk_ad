five contingencies for how your life could evolve
over the next five years.
What do we do with those paths?
So what this prescription says is
it's basically a way of smoothing out
these points in
very high dimensional path space
to try to arrive at a posterior probability
distribution for
what the entire landscape
for future path space looks like.
The way to do that, again very simple
is to take
each of these five paths
and divide up your future path space
into Voronoi cells
and say ok well
maybe here's a really exotic way
my life could evolve
I'm going to take some huge segment of this
high dimensional path space
and I'm going to allocate the segment
just to that one point. So it turns out this is also
relatively easy calculation to do
and at a high level all we're really trying to do
is take a sparse sample of the future
path space and smooth it to the point
where we can do approximate calculations
of the Shannon path entropy
of this space. Was there a question?
Yeah, is it the case that
in a state of maximum causal
entropy
what the present state tells you about
the future is minimized?
In other words, as you maximize
the causal entropy you lose memory
You lose
prediction of the future?
Sure, absolutely.
That's an excellent point I guess.
So if you lose initial state dependence
for reasonable assumptions
Yes?
So it's not true in general, right?
That's why I'm saying reasonable assumptions. For example, if we live in a deterministic universe
that you're not going to forget
your initial state ever
If you have access to the entire system
that's a closed system in a deterministic world
you'd never lose memory.
So that's why I'm saying you have to make a few additional assumptions
for that to be true.
So
I'm not sure that
so could you ask the question a different one?
Yeah, so it seems to me
if I understand what's going on
you're taking
knowledge of the physical system
which the agent may not have
and you may not have
and you're cranking this equation
and saying the more intelligent
the more possible it is to this behavior
but that intelligence
incorporates like how much it does
Good, excellent.
So if I can try to
take a stab at the core of the question
which is where does the model come from?
Is that sort of the core line?
So this is a system that presumes you have a model of a system
a model of the universe
and now you can crank using that model
and it specifies a control policy
but where does that model come from?
We're assuming transition probabilities
say where does this first order Markov model
of the universe come from?
So I would say this is one of the
the most, I'll answer that
as provocatively as possible
which is
it doesn't care
and that's one of, going back to
one of the first slides about
physical phenomenologies
this is a theory for
AGI behavior
that's what it inspires to be
that is completely agnostic
as to where models come from
or even if there are any models
in principle this is a
where you could say
have a quantum system
where despite all
of your best model building
the system will remain stochastic
from your perspective, non deterministic
and
you can specify a real physical theory
that's completely model independent
so that's what this inspires to be
the short answer and maybe the most disturbing
answer is this is a protocol
not an implementation and it doesn't care
where the model
the universe comes from
it just computes based on a model
so in the equations
I don't know what little f is
and I don't see the derivative of the entropy
anywhere so maybe with two answers or less
sure
so let's see
little f here
so I haven't gone through all the definitions
but little f here is instantaneous action
so we're looking at all possible
enumerated by J, we're looking at possible
legal next actions
with continuous systems, unlike Go
which is a discrete system
where it's not possible to exhaustively enumerate
this continuous set
of all possible next actions
so instead we're sparsely sampling
the space of next available actions
so that's what little f is
little f is an enumerated sparse
subset of possible next actions
but again in the style of logo
and then as to
your question Yoshua of where the derivative goes
there's
no elegant answer for that
it's simply the product of lots of
chugging
the derivative conveniently
vanishes due to the chugging
and there's just no beautiful
answer to that I'm afraid
so it seems as if we have
two radically different
impressions
of AGI intelligence
there's this one
goal based idea
where the AGI is
driving towards a goal
which sort of
by effectiveness
means it starts cutting down its options
as it gets near towards the goal
and its head
or we've got this other scheme whereby
the AGI is basically trying
to empower itself and keep all
its options open and make everything
possible
so I'm going to give this talk now in reverse
because everyone wants to hear the second half of the talk before the first
okay
so we'll do part two before part one
so let's jump to the conclusions
before we actually
try doing any experiments
so the answer to your question
is that you can understand
goal seeking as a side effect
of cause-electrical maximization
so if you imagine say a chess player
chess as a convenient example
imagine a human playing chess
why would an expert human play chess
well maybe human would play chess
or receive social rewards
or to refine his or her knowledge
for a variety of external reasons
so my claim is
in answer to your question of
this very traditional
approach to AGI where you specify
externally a utility function you say
okay go off and maximize this by whatever means
but be really clever in doing it
so that I know you're intelligent
I would say that
that is an effect, a side effect
of a deeper principle
maximizing to the extent possible
your future freedom of action
in the presence of a bottleneck and path
space so
why would a human chess player do this well
so the answer is the reason they would
deliberately over a short time scales
say over an hour time scales would win
