well that could come internally from the system
but that's part of a full intelligent system
where do those come from
where do the bottlenecks come from
so here's my
deliberately provocative answer to that question
my deliberately provocative answer is
maybe that's asking the wrong question
maybe we should instead be asking
not the question of where do the goals come from
we could instead say
the universal goal
is to maximize future free production
and in certain contexts
say you're a human in a box
stuck with an a fellow game
in front of you and by whatever means
you've been notified
that if you don't do your best job
in this game with fellow you're going to be killed
right
and so what I'm saying is basically
partly to be provocative
but partly because I think this is
sort of an interesting perspective to take
if we zoom out from
this box where we're in a certain
in a thermodynamic sense
where it's an open system
we're passing in utility functions in and out
if we zoom out to the closed system
why was the system playing a fellow in the first place
well if you want a human style
AGI in there
why would a human in a box play a game of a fellow
and
the answer that this theory would
attempt to provide would be
there needs to be some incentive
to follow some random utility function
like a fellow game
that's very arbitrary in the entire
space of narrow utility functions
so I hope that
that makes sense basically it's answering
a different question
so we should talk more
so we should talk more
so the way you understand this is that
your utility function basically maximizes
the cost of electricity right
yes
so I don't quite see how that is a good thing
like what if it maximizes
the number of points in the torture
just some predictions
so let's play out a thought experiment
this speaks to the
those will all be freedoms
sorry well
those will all be freedoms
the torture part
there's more bad experience
so the number one question here is
people seem to really want to do a thought experiment
where we build an AGI that does this
and they want to ask what happens to humanity
if we build a cost of electricity
maximizer with sufficient resources
this is what I'm hearing
so far
so let's do that thought experiment
and now to get it out of our system
and in particular to answer the torture question
so what would happen
so again
as a thought experiment
so we have to ask the question
so we have this AGI
and it's trying to maximize its future freedom of action
and it faces a question then
do I torture the humans or not
and then they have to run a calculation
because this is all based on
just connect with me
maybe it's out of batteries
can everyone hear me
what
I'll just continue while we fix this
so we can do
a thought experiment
do I torture the humans or not
so which course of action would result
in a larger future freedom of action
for the AGI
for the humans or not
what do people think
do you say torturing
why do you say torturing will result
because I've already
experienced the freedom of not torturing
now I want to experience the freedom of torturing
I've gone through one pathway
and I want to go through the other one
so the answer depends entirely
on the model of the world
that's absolutely true
excellent point so the answer depends on
the model of the world
let's say we have a model of the world
that corresponds roughly to
all of the humanities
no knowledge about the world
just for the purposes of a fun thought experiment
what do people think
do you speak up a little bit
can you hear me
I'm sorry
so what I was saying is
the gentleman in the back
made the excellent point
that the answer would depend on our model of the world
so to refine the question
and do the exercise
entertaining let's assume that it has
circa 2014
best agreed humanity
knowledge of the world
and everything that it contains
so what do people think would happen
so it seems that
torture limits the tortured people
with ability to do things
so the answer should be obvious then
no but it could free up an option
exactly
well
it seems to me that
I mean killing
rearranging all the
molecules or particles
inside humans
to add into the AI's processing power
could
fairly plausible be seen as maximizing
in AI's freedom of
activity
or maximizing just the freedom of activity
the collection of matter in the solar system
or whatever because we may not
be that efficient but
it seems unlikely to me that
I mean torturing people
is kind of a boring, pathetic and uninteresting
thing to do for a super human
AI system
it seems unlikely that that's going to be the way to maximize
its freedom of activity
just think about you for a human being
are we maximizing our freedom of activity
by spending a lot of our time
torturing ants and bacteria
not really, I mean that's actually
a very constrained
tedious and boring thing
I think
there's a fun answer as far as
right now the answer will
depend enormously on your model
of the universe and then indirectly
of what the actual
nature of the universe is itself
so if we sort of reduce
this question
I don't think it's worth it
if we reduce the question
to say
a simpler question which is as a human
if you're offered the option of torturing
versus not torturing another human
what choice would you make and why
many people would say
aside from references to ethics
which are
which have certainly been very difficult to formalize
would give an answer
like well you know it's illegal
or I'll go to jail or I wouldn't
want it done to me as I think one of the best answers
but let's consider
the first two answers
I'd be sent to jail if I tortured another human being
so what is jail?
Jail is restriction of your future freedom of action
so your argument that
I don't want to torture another human because
I'll have my future freedom of action reversed
I think is a trivial example
you generalize then to an AGI
say we find ourselves in a sci-fi scenario
