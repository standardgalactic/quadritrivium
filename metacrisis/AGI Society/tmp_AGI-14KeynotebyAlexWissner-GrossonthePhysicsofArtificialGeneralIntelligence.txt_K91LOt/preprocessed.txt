It's my pleasure to introduce our next keynote speaker, Alexander Wissner-Gross, who's a
scientist, inventor, and entrepreneur who's done more interesting things than I'm going
to be able to list here in a moment.
He's currently serving as an Institute Fellow at Harvard University at the Institute for
Applied Computational Science.
He's also a research affiliate at the MIT Media Lab.
His background is quite that Earth, but you see the work of physics up there.
He has a PhD in physics from Harvard, where his thesis on programmable matter ubiquitous
computing and machine learning was awarded the Hertz Doctoral Thesis Prize.
And in the recent past, he has achieved a lot of attention both in the intellectual community
and in the broader media for his hypotheses about the relations between causal and tropic
forces and general intelligence.
He had a TED talk on relayed ideas, which has been viewed something like one and a half
million times.
So now we get his views on the nature of general intelligence, the relation to physics, entropy,
and so forth.
I'm sure there will be some interesting questions and discussions following up this presentation.
Awesome.
Thanks so much for that.
Can everyone hear me?
Great.
Well, let's make this as interactive as possible.
Thank you for having me.
It's really a pleasure to be talking to AGI enthusiasts about this topic.
So as Ben mentioned, I'm a physicist.
I'm also a computer scientist.
I'd like to think a lot of my work is influenced by traditions on both sides of the physical
digital divide.
And I'll be talking for the next hour and a half about the physics of artificial intelligence.
And so as a physicist, sort of a cultural thing, one is always taught to ask about physical
limits and also about observables.
So observables are incredibly important.
So as a physicist, I can ask myself a question and I was speaking with a number of you earlier
about sort of the origin stories of this type of approach to AGI.
It's very natural to ask, what are the physical limits of artificial intelligence?
And from a physical perspective, it's very natural to examine time-like limits to AGI
and space-like limits to AGI.
So I'm going to devote the majority of this talk to exploring time-like limits to AGI
and a bit at the end if there's time to space-like limits as well.
But most of my recent work has been focused on time-like limits to AGI.
So this is an analogy that should be familiar to many people in the field.
Sometimes attributed to Dijkstra, but Dijkstra was partly alone in this concept.
So to abuse quote from Dijkstra, question of whether machines can think is now is relevant
to the question of whether submarines can swim.
So the way I think helpful or not to choose to interpret that notion is
it's always important to understand phenomenologies before you aspire to understand mechanisms
for complex systems.
It's interesting to perform a sort of thought experiment in the history of artificial flight.
Could we have accelerated the onset of artificial flight in our civilization
if we had had a greater focus on the physical phenomenology of flight earlier on
rather than mechanisms of flight?
It's always very tempting to try to imitate mechanisms from biology,
from other segments of the existing world because these are the examples that we have
in nanotechnology, in artificial flight, in artificial intelligence.
But I think history at the end of the day does try to teach us that understanding the
physical phenomenology of a complex system as early on as possible is a very powerful
meta-approach to trying to recreate phenomena that we observe in the natural world.
So I suppose this is a question more rhetorically than anything else.
Instead of trying to ask how can we fly like birds, maybe we should have been asking
earlier on what is the physical phenomenology of flight?
For example, understanding gliding.
And then by analogy, perhaps we should devote a greater emphasis.
Instead of just asking how can we build minds to perhaps an easier question,
which is what is the physical phenomenology of intelligence?
And what I mean by that is if you have an intelligence in a physical system,
so this is an embodied intelligence, what are the physical artifacts?
What are the physical processes?
What are the physical imprints that an intelligence leads on an environment?
Can we describe the physical impact of intelligence, whatever that is, in terms of physics?
I saw a question back there and I promised to make this interactive, so go ahead.
So I was going to see whether you really wanted it to be so or not.
So that was a free phrase, but you just said it in a different way.
I'm going to see if you're going to want it to be interesting.
So my understanding of bird intelligence is that it doesn't have to do with physical phenomenology,
but that you need to understand the principles behind what's going on.
It doesn't need to be physical, it may be computational, it may be other kinds of things.
And one of the best ways to understand the principles of something
is to understand systems that exhibit those principles.
So that studying birds is important.
The question is, what are the level of generalizations and how deep are the principles you get out of it?
So if you view it that way, then it doesn't say you shouldn't study birds,
you shouldn't try to replicate birds.
It just says you need to understand the right depth of principles to derive from them
in order to be able to apply that in other technologies.
So the question is, don't we want to understand principles
rather than just saying we want to understand phenomenologies?
So I think we're saying the same thing using slightly different language.
What I'm trying to articulate here is, let's understand the principles,
physical phenomenologies underline a complex system
before trying to duplicate mechanisms observed in the natural world.
And I think it is important to draw a dichotomy between the principles
and phenomenologies and the underlying mechanisms.
Mechanisms could be highly idiosyncratic, they could be specific
to wanting implementation of a general process.
There are many ways to achieve artificial flight and natural flight,
but there are relatively few ways to, to say, create lift in a hydrodynamic system.
And what I'm saying is that it's important to try to understand
the physical processes and phenomenologies first
before spending too much effort trying to exactly duplicate mechanisms.
That makes sense.
I don't want to extend this, but you often can't get that full understanding
in what you try to replicate.
And in the replication, you often understand the principles better
and you understand what aspects of the system really matter,
but don't, particularly when you're moving to a different technology.
So I don't think it's quite as simple a dichotomy in the team.
You can't afford to try to do all your understanding before you try to replicate.
I agree. There's absolutely a feedback loop.
And so what I would propose is to try to invest more energy,
more resources earlier on in understanding general principles,
physical principles, instead of just studying mechanisms.
So I think we're in violent agreement.
So if you take this sort of culture that we should try to understand
the physical phenomenologies of complex systems as early on as possible,
so what does this tell us about AI?
So I thought the morning tutorial was very instructive in that sense.
So sort of as a hobby for the first few years of us
living in the world of physics, the world of computer science,
I thought it was instructive to sort of keep my head to the ground
and look at a number of developments that we've seen over the past few years
and note potentially a very deep thread connecting them.
So I'll start with games first since the morning tutorial mentioned go-playing.
So I thought it was very interesting that in the discussion
there was so much discussion of the mechanisms of Go and Monte Carlo Tree Search,
MOGO, which was the 2006 Go player that got so much attention,
that there was so much discussion, I thought, of the mechanisms by which Go was playing,
but not actually the take-home lesson that we should be drawing and deriving
from the success of MOGO and then successor systems.
So let me speak first a bit about that.
What was really remarkable about MOGO and these MCTS Monte Carlo Tree Search systems
that for the first time are competitive with humans at go-playing.
So at the courses level, what these go-playing systems are doing
is they're considering all possible legal net moves
and then they're doing a Monte Carlo Tree Search from those net states out to the end of a game
and they're looking to see, okay, so maybe I'll have on the order of 100 possible legal net moves
and for each of those, one fraction of the time if I were to play a random game
against a random opponent with prunings applied by MCTS,
would I end up in a state where I win?
And at the course level, the approach is as simple as that,
nothing more complex, everything else is more or less pruning.
So taking the step back and saying, hey, wait a minute,
so this game, Go, that everyone went D2B, Gary Kasper up at 97,
everyone thought Go would be sort of this grand challenge for AI.
Why is this grand challenge falling to what is essentially just a valuation of every legal net move
and then checking to see what fraction of the random games that can be played
if it's a greater than about 80% or so, resulting in a win?
How could such a simple process, a simple algorithm be finally overcoming this barrier,
computational limits aside, the algorithm is so simple,
this is something we could have been doing decades ago.
So I would call that hint number one, that there's something profoundly interesting
and maybe this is a hint as to a greater phenomenology of AI,
that there's something profound about looking at possible actions.
So many of us from a control view right perspective would call this a policy.
There's something profound maybe about looking at possible legal actions in a control policy
and then evaluating Monte Carlo's title, a large number as densely as possible in path space
of paths that our universe can unfold along up until some future time runs in.
It's a very coarse observation, but I think what's profoundly interesting is
that several other fields seem to be converging on a similar concept.
So again, lesson number one there is there's some sort of, I would say,
profound insight emerging from the success with MOGO and its successors
as to choosing net actions that will leave open the maximum number of possible paths.
In the case of MOGO, paths to win, but Mike Ling is going to be that this is actually
an artifact of a more general principle.
Let's set an example in cosmology. So this may be a bit further afield for this particular audience,
but a major outstanding metaprogram in cosmology for the past 10 to 15 years has been the anthropic problem.
So why do we live in a universe that has the properties that it does
and a common anthropic answer to that question is we live in a universe that has the properties that it does.
Because if it didn't, we wouldn't be able to exist to ask the question
why does the universe have the properties that it does.
And one of the most successful approaches for addressing the anthropic principle
is Rafael Busoso's causal entropic principle, which holds that
universes that create the maximum amount of entropy over the course of their lifetime
clipping out certain inconvenient portions of the universe like black holes
which are causally disconnected from the rest of the universe
tend to reproduce certain critical values that we observe in our own moves
such as the cosmological constant.
And incidentally, one of his more interesting conclusions I think was this notion
that the largest source of entropy creation in our universe, again, clipping out black holes,
is starlight scattering off of enterstallar dust.
So he was able to arrive at a number of cosmological parameters
that are within a factor of two of the ones that we observe.
Now, I would say that this notion that maximizing causal entropy creation
for certain definitions of causal entropy is also, I would say,
sending us a signal from the cosmology community that there's potentially
some sort of deep connection between causal entropy creation
and intelligent observer concentrations.
So a normal anthropic approach to trying to derive from forced principles
the various cosmological parameters that we observe would be to say,
okay, so twiddling this knob here for this force constant
and twiddling this knob here for this force constant,
which combination of these null configurations would result
in the largest concentration of intelligent observer concentrations
who are able to ask these sorts of questions.
And I think what's so important about this development in cosmology
is for the first time it provides a firm either thermodynamic
or information theoretic, depending upon which school you come from,
foundation for formalizing this question.
No longer do we need to define intelligence
or to define what an intelligent observer is
to ask these cosmological questions.
Instead, we have reasonably rigorous thermodynamic
or information theoretic quantities that we can,
as Sparman and his thesis advisor, is to say,
shut up and calculate with.
So I would say cosmology is also providing some hints to us
that there may be some profound connections between causal entropy creation
over the course of an extended period of time and intelligence.
And finally, I sort of couldn't do justice to the slide
without at least alluding to developments in planning in AI.
So just as a sort of an example here, I have a PR one from Willow.
Yes, Yashiro?
Are you going to elaborate on the causal entropy principle?
That's the rest of the talk.
Okay, good.
So this is the last been approved.
So as one example, the almost ubiquitous use now
of rapid reversal of random tree type approaches, RRT approaches,
and that's, again, a path plan where, in a certain sense,
if you squint at RRT, it really is an algorithm
for trying to maximize the weight of a branch
in a standing tree rooted at your current universe state.
So to make Yoshiro happy, I'm going to move on.
So the next slide and point everyone to a paper
that we published last year at Physical Review Letters
where we think for the first time we've unified many of these
disparate threads across domains, as well as many others
that I don't have time to allude to,
with a new principle that we term causal-entropic forcing.
Now, as with many advances in AI over the years,
this one does not live in isolation.
In particular, I would like to credit Kulani
and Kluven's really courageous work on empowerment
and I can speak more about that if you and I, if people are interested,
but in many ways this draws upon empowerment
and takes many of its concepts to the next level.
So how do we formalize the connection?
So we've seen, I would say we've seen examples that there are threads
that somehow connect this idea of maximizing causal entropy.
You could express that as keeping options open.
You could express that as capturing possible futures.
There are many ways to express it qualitatively,
but quantitatively it turns out that there's already a lot of machinery
that physics has to describe this notion of maximizing
potentially accessible futures or keeping options open.
So my favorite example is that of a rubber band.
So since I promised that this would be a little bit interactive,
let me ask a question of the audience,
I'll see if anyone has the answer.
So if I take a rubber band and I stretch it, it's elastic, right?
If I take a rubber band and then I dunk it in liquid nitrogen,
it loses its elasticity.
So questions for the audience is why?
Anyone?
You're sure?
No?
Sir?
Sir?
Could we maybe do something less certain?
So the question with the answer is, it's frozen.
Any other attempts?
Well, why does a frozen rubber band lose its elasticity?
Yes.
So we're moving in absolutely the right direction now.
So I guess another way of framing the question is
why does a rubber band at room temperature,
why is it elastic in the first place?
So a physics 101 answer to that question would be
to consider a simplified model of a rubber band as an ideal chain.
Just imagine a chain link, a ideal chain,
where you have a set of links, like a polymer.
This is also a polymer physics 101.
So you can imagine you have a polymer of discrete links
that are all free to randomly rotate with respect to each other.
And you can ask the question, what sorts of configurations
of an ideal chain would one expect to observe
at a finite temperature?
And the answer, of course, is just from analyzing
all of the microstates.
A microstate is an instantaneous configuration
of a system in general, a thermodynamic system.
In the case of a chain of microstates,
there's a particular set of, say, bond angles
for this ideal chain or a polymer.
So we would expect to observe more mapward states.
In this case, convenient mapward state would be
the end-to-end displacement of the ideal chain,
where the mapward state, excuse me,
where the displacement is very small.
So there are more ways to arrange an ideal chain
where the end-to-end displacement is very tiny
and where the chain itself looks effectively like a ball.
In polymer physics 101, you would be taught
this is why proteins, other polymers,
we still obviously globular proteins
because it's a convenient architecture
when you have this sort of chain for the chain
to occupy more states.
There are more microstates where the end-to-end displacement
is smaller rather than larger.
So going back to the question of elasticity in a rubber band.
So rubber bands consist of many polymers,
and the polymers have more states
where the end-to-end displacement is closer together
rather than farther apart.
If you take a polymer and you stretch it out,
then you can do a simple combinatoric analysis
and immediately understand intuitively
why there are fewer microstates accessible
to a polymer that's stretched out
versus a polymer whose end-to-end displacement
is closer together.
And so if you simply combine that observation
with the fact that these microstates
can constantly, randomly transform into each other,
we're naturally going to find more microstates of a polymer
where the end-to-end displacement is closer together
rather than farther apart.
So going back to then to the question of
why do we lose elasticity if we lower the temperature.
The temperature is the coefficient that determines
how quickly these microstates can turn into each other.
How energetic each of these lengths is.
And so if we reduce the speed with which
these microstates can transform into each other,
then since the transformation of microstates into each other
completely determines the entropic force
that we call elasticity, the elastic force,
we're going to reduce the elastic force
and hence make it rigid. Yes, you're right.
Why is it that the stretch for those in states
would be a local minimum?
I mean, you could expect that, yes,
in this configuration there are not many
minimum configurations, but why would it not
to the smaller summits?
So if you lower the temperature,
so if you stretch your rubber band and then freeze it,
over time it'll be glacial, but very slowly
it will still be elastic and return back,
but it'll be much slower.
And the speed with which it's able to return back
is going to be a function of the temperature.
Is there another question here? Yes.
Another question that I've got.
If you stretch your rubber band,
you can notice the temperature change on the surface.
I believe it gets colder when you put it back
because it's warmer.
So you can see that the entropy is doing something.
It is transforming between each end of the stretch.
Good. I was hoping someone would bring up energy
versus entropy.
So you can make this, you can derive
this observation purely and chronically.
That's why I'm going to be careful of an ideal chain
where you don't have any messy energy considerations
because you traditionally would assume
that the adjacent links in the chain
are free to rotate on a flat energy landscape.
So when you stretch it out,
it's possible to do that idiomatically
without changing the energy of the system at all.
But you raise an excellent point,
which is that in a real system,
you certainly do often, if you stretch your rubber band,
you're translating kinetic energy
from the pulling into kinetic energy
of the rubber band itself.
And so you can't purely efficiently
reduce the entropy of the system.
That's an excellent point.
So it's a standard idealization to say
we're only going to look at a purely entropical
and get a purely entropic version of this effect.
But in real systems,
there is a combined energy effect
and an entropic effect.
Any other questions?
Great.
So now that we have this rubber band example in mind,
I was saying earlier,
we already have the machinery
from physics to describe systems
that are trying to maximize
the set of states that they occupy.
Traditionally,
rubber band is just
a very convenient example.
Almost every quantity
that we're talking in chemistry or physics,
we're running out of tomorrow one,
including the ubiquitous concept of pressure
are in fact entropic forces.
They're conventional entropic forces.
So we have this machinery
available to describe systems
that want to occupy as many states as possible
that are still consistent
with some macroscopic observable.
So let's reuse the existing machinery.
Traditionally,
for conventional entropic force,
consider an ideal chain.
So how would we formulate,
how would we formalize
the notion of a system
that doesn't just want to occupy
as many spatial microstates as possible,
but also wants to occupy
as many paths through the future as possible?
So go back to the example from
Moho for MCTS.
At a high level,
all Moho is doing and its successor systems
are they're looking for ways
to perturb a system
such that that system can explore
the largest number of possible paths
up to some future horizon.
So can we use this language
of entropic forces to characterize that?
And the answer is yes,
and it's beautifully simple.
We simply take the concept of an entropic force
on a spatial ideal chain
and we rotate it in space-time.
So rather than considering an ideal chain,
we could consider
random walks through space-time.
A random walk to a physicist
is more or less what
for Moho you look at
for an MCTS search
if you add in a bit of proving for efficiency.
Monte Carlo sampled the possible
future paths between the present state
of the system and some future time
horizon that could be parametrically chosen.
So at a high level,
this is what we did.
We took the same concept of entropic forces
which are ubiquitous in our visible world
and asked what happens if we generalize
an entropic force to space-time
to characterize systems
that are trying to maximize their future
freedom of action.
And it turns out
very satisfying that you could describe that
with a minor modification
to an equation
that you would use to describe conventional
entropic forces.
So
you could print this on a t-shirt if you want to.
What does this mean?
So with a conventional entropic force
you have some force
which is
so
you have a conventional entropic force
where you have a force that's proportional
where the constant of proportionality is the temperature
going back to the rubber band analogy.
To a gradient
in, now traditionally this would be
entropy space.
So you could define entropy in a variety of ways
for entropic force discussions
entropy is typically a Shannon entropy
associated with microstates
that are partitioned into backward states.
Yes, here's your line.
Could you define causal entropy where precisely?
Next slide.
I should just give this talk in reverse, I think.
So all this is doing
is saying that
we're going to look at processes
that are writing gradients
in a landscape
where the landscape
is determined by for each state
in our universe.
What is up to some future time horizon
which we've parameterized this towel
what is the diversity of accessible future paths
in our universe?
How much future freedom of action do we have?
And
so for Yoshua
next slide how do we calculate what a causal entropic force is?
I get the cursory treatment here
if you'd like more depth
I would encourage you to speak with me afterwards
or just go read the original VRL
where we laid this out
in exhausting detail.
So the high level idea is
in answer to Yoshua's question
which is what is causal entropy?
Causal entropy we define
as a path integral
from the present state of the universe
up to some future time horizon
of the Shannon entropy
of possible paths that the system
could evolve along.
So in a certain very real
sense this reduces to
a traditional entropy depending upon our definitions
if your time horizon is zero.
At a high level
all we're really doing and I know there's a follow up talk
about ways to express the same concept
quantum mechanically is
we're enumerating all possible paths
that the system could take if you started off
in its present state
and then evolve that out up to some future
time horizon.
And we're then using these
partial entities as weights
for actions.
So this is a control policy
in the sense of optimal control theory.
But instead of an action
we're using the language of physics
to say this isn't going to be a force.
And it turns out there are numerous advantages
which maybe we can discuss more in Q&A
to describing this physically
versus describing this from
an optimal control policy perspective.
Is there a question?
So we can use all of
our path integral calculus
machinery to
reduce this
expression for
partial causal entropy
weighted actions
to a simpler
approximation which converges
on the actual force
as you sample more
and more densely possible future
path space
a very computable expression.
And let me just talk for a minute about
at sort of a high level
what this expression means.
So the idea here is
it's never going to be feasible
to allude to some of the questions that happened in the tutorial
right before this to densely
sample some very realistic spaces.
You're never going to exhaust some of these spaces.
All that we can hope for is
some sort of sparse sample.
If we were anthropologists or psychologists
we would be tossing around
terms like memory of the future.
The idea that in your imagination
you're in human planning
in a certain sense remembering
things that are going to happen.
You're using your imagination to explore
possible future contingencies.
But your contingency tree is always going to be sparse.
You'll boil the oceans before
you can exhaustively sample
the entire search tree.
So the question is how do we use sparse sampling
to calculate this number?
And it turns out
if you sort of crank
the math machine enough times
that you can reduce this path
to a number
to a set of quantities
that are relatively easy to calculate
which is, and I'll describe this at a high level
as a sort of table of contents
for people who want to discuss in greater detail
we can have difference.
Imagine that
you as a human have to make a decision.
So how are you going to do that?
This prescription for how you would
policy, for how you'd make a decision is
imagine yourself in your present state.
Now repeat the following several times.
Imagine randomly
how your life could evolve
up to some future point in the line of state
five years from now. Obviously it's going to be multi-scale
obviously it's going to be low resolution
fine. So now
you have five ways,
five contingencies for how your life could evolve
over the next five years.
What do we do with those paths?
So what this prescription says is
it's basically a way of smoothing out
these points in
very high dimensional path space
to try to arrive at a posterior probability
distribution for
what the entire landscape
for future path space looks like.
The way to do that, again very simple
is to take
each of these five paths
and divide up your future path space
into Voronoi cells
and say ok well
maybe here's a really exotic way
my life could evolve
I'm going to take some huge segment of this
high dimensional path space
and I'm going to allocate the segment
just to that one point. So it turns out this is also
relatively easy calculation to do
and at a high level all we're really trying to do
is take a sparse sample of the future
path space and smooth it to the point
where we can do approximate calculations
of the Shannon path entropy
of this space. Was there a question?
Yeah, is it the case that
in a state of maximum causal
entropy
what the present state tells you about
the future is minimized?
In other words, as you maximize
the causal entropy you lose memory
You lose
prediction of the future?
Sure, absolutely.
That's an excellent point I guess.
So if you lose initial state dependence
for reasonable assumptions
Yes?
So it's not true in general, right?
That's why I'm saying reasonable assumptions. For example, if we live in a deterministic universe
that you're not going to forget
your initial state ever
If you have access to the entire system
that's a closed system in a deterministic world
you'd never lose memory.
So that's why I'm saying you have to make a few additional assumptions
for that to be true.
So
I'm not sure that
so could you ask the question a different one?
Yeah, so it seems to me
if I understand what's going on
you're taking
knowledge of the physical system
which the agent may not have
and you may not have
and you're cranking this equation
and saying the more intelligent
the more possible it is to this behavior
but that intelligence
incorporates like how much it does
Good, excellent.
So if I can try to
take a stab at the core of the question
which is where does the model come from?
Is that sort of the core line?
So this is a system that presumes you have a model of a system
a model of the universe
and now you can crank using that model
and it specifies a control policy
but where does that model come from?
We're assuming transition probabilities
say where does this first order Markov model
of the universe come from?
So I would say this is one of the
the most, I'll answer that
as provocatively as possible
which is
it doesn't care
and that's one of, going back to
one of the first slides about
physical phenomenologies
this is a theory for
AGI behavior
that's what it inspires to be
that is completely agnostic
as to where models come from
or even if there are any models
in principle this is a
where you could say
have a quantum system
where despite all
of your best model building
the system will remain stochastic
from your perspective, non deterministic
and
you can specify a real physical theory
that's completely model independent
so that's what this inspires to be
the short answer and maybe the most disturbing
answer is this is a protocol
not an implementation and it doesn't care
where the model
the universe comes from
it just computes based on a model
so in the equations
I don't know what little f is
and I don't see the derivative of the entropy
anywhere so maybe with two answers or less
sure
so let's see
little f here
so I haven't gone through all the definitions
but little f here is instantaneous action
so we're looking at all possible
enumerated by J, we're looking at possible
legal next actions
with continuous systems, unlike Go
which is a discrete system
where it's not possible to exhaustively enumerate
this continuous set
of all possible next actions
so instead we're sparsely sampling
the space of next available actions
so that's what little f is
little f is an enumerated sparse
subset of possible next actions
but again in the style of logo
and then as to
your question Yoshua of where the derivative goes
there's
no elegant answer for that
it's simply the product of lots of
chugging
the derivative conveniently
vanishes due to the chugging
and there's just no beautiful
answer to that I'm afraid
so it seems as if we have
two radically different
impressions
of AGI intelligence
there's this one
goal based idea
where the AGI is
driving towards a goal
which sort of
by effectiveness
means it starts cutting down its options
as it gets near towards the goal
and its head
or we've got this other scheme whereby
the AGI is basically trying
to empower itself and keep all
its options open and make everything
possible
so I'm going to give this talk now in reverse
because everyone wants to hear the second half of the talk before the first
okay
so we'll do part two before part one
so let's jump to the conclusions
before we actually
try doing any experiments
so the answer to your question
is that you can understand
goal seeking as a side effect
of cause-electrical maximization
so if you imagine say a chess player
chess as a convenient example
imagine a human playing chess
why would an expert human play chess
well maybe human would play chess
or receive social rewards
or to refine his or her knowledge
for a variety of external reasons
so my claim is
in answer to your question of
this very traditional
approach to AGI where you specify
externally a utility function you say
okay go off and maximize this by whatever means
but be really clever in doing it
so that I know you're intelligent
I would say that
that is an effect, a side effect
of a deeper principle
maximizing to the extent possible
your future freedom of action
in the presence of a bottleneck and path
space so
why would a human chess player do this well
so the answer is the reason they would
deliberately over a short time scales
say over an hour time scales would win
a chess game restricts their future
freedom of action is they're trying to win
their way through a bottleneck with the promise
that over a longer time scales
they're going to experience greater future freedom of action
maybe they're going to get monetary rewards
that allow them to do more things over long
terms than they can over short terms
I think this is a very satisfying answer
and maybe even better
analogy is in finance
where we have this ubiquitous concept
of investment we're willing to
reduce our liquidity over
shorter time scales which is also
reducing our future freedom of action
in return for the expectation that over
longer time scales we will increase
our net liquidity
but at the fundamental level
all these this principle of investment
appropriately generalized just produces
down to this idea that there are bottlenecks
of some sort in future path space
and if we're
if we really are just future freedom of action
maximizers over
some appropriately longer time horizon
then we are willing to sacrifice
future freedom of action over
shorter time horizons to win through bottlenecks
yes
this is a particularly good example
where you limit your short term
options in order to maximize
your long term options
sorry so what's the question
would possibly be ethics
be a very good general example of this
where you constrain your short term
options in order to greatly
maximize your long term options
obviously in other words ethical behavior
about ethics is abstract
so to proceed further to the part two
before one
paperclip maximizers everyone's
favorite hobby horse for
unintentionally unfriendly AGI
so an analogy I like to use
is this notion of paperclip maximizer
I'm sure everyone here is familiar
with the concept
can be treated as
just one data point and a very large
set of unintentional consequences
of optimizers in general
but in the same sense
that
for appropriate definitions you could say
embed an entire class of Julia
sets in a Mandelbrot set
my
this hasn't been formalized yet
but my
intuition and my expectation is that
we'll discover that you can embed
a wide class of
sort of naive optimizers
including paperclip maximizers
in this much broader theory
and then to speak
more
to the question of friendliness
so after the paper was published
got a lot of correspondence
and one of the most interesting pieces
of correspondence was the question
so could you actually go use this
theory and would you
formalize as about concepts
of friendliness
and first I thought
what's a really silly, overly
ambitious goal
but I thought about it a bit more
and certainly in light of
Polonius
and Fubo's work on empowerment
I think there may actually be
a possibility
here that this type of approach
appropriately generalize
would let us take
something like Asperger's Laws of Robotics
and say
start from zero flaw
not to spoil a series for anyone
but
we've all read it anyway
so to take the zero with law
that our robot may not
harm humanity
and say okay so if we really
sort of took this goal seriously
we want to sit down and lock ourselves in a room
and write down an equation
for Asimov's Laws
so that we could do this rigorously
and make sure that our robot friends treat us well
how do we do that
and I think there is a plausible
reason why I'm thinking about what we might try to do
is maximize the causal
transfer entropy from robots
to humanity so zero with law
trivially follows out of that
that instead of trying to maximize its own
future freedom of action we're trying to maximize
humanity's future freedom of action
so that gives us the zero with law
for the first law that
robot may not harm human being
except where that confers to the zero
with law later on
in his series
I understand why if you kill a human being
or injure a human being
let's forget about the zero with law for a minute
so when a human is injured
that's reducing your future freedom of action
if you're no longer able to walk that reduces
your ability to explore phase space
all other things being equal
so I think intuitively
you can see how maybe the first law
would fall out of this principle as well
that all other things being equal
regarding the zero with law
it wouldn't be an interest maximizing
the freedom of action of humanity
if you injure a human being
or through inaction allow a human being
to come to harm
secondly for the second law
so the second law is
a robot must follow
essentially must follow orders from human beings
so if you imagine humanity
stuck in a container with
a set of AGI robots
that as well imagined
so this
combined system and humanity in particular
would be able to realize a far greater diversity
of paths if it's able to
leverage robot abilities
to do things
and so if we're trying to maximize future freedom of action
of humans humans can do a lot more
if they have access to robots
and can control robots by orders
than if they don't
so you can see how the second law
falls out of this principle
and then finally the third law which says
a robot can't destroy itself
or can't hurt itself
a robot can't allow itself to come to harm
unless this conflicts with the first
three laws 0 for 2
again so if we imagine
humanity and robots in a container together
then if the robots
for some reason that doesn't conflict with the first three laws
allow themselves to come to harm
then they won't be usable by humanity in the future
to accomplish things that humanity
wouldn't have been able to accomplish other laws
so this is a very
you know sort of shorthand heuristic
and entertaining
I would say sketch
for how one might try to take as most
3 plus 1 laws
and try to reduce these down to a single
information of your right principle
so that's our question here
right so this is kind of your end point
that you're now going to and working forward
or backwards I guess
I love giving talks backwards
so are we going to be happy
with conscious robots that are slaves
would they be happy in that condition
because this is kind of implying
a slave based kind of
separation between the robot
class and the human class
and wouldn't it be better
to actually have
the robots internalize the values
and see themselves in humanity
as one expression
so I
pause with this I would say
only as a thought experiment to
try to sketch out how one
possible play out of the universe
as about the universe could be described
in terms of very simple information
here at principles is not
in any way to endorse as much vision
or anything else but for those
sci-fi fans of the audience that is in fact
the very end of the foundation series
what you described about that
I didn't read it but I'm saying
sorry this is a clear
implication of what he's saying
is that in this slave class
we're going to create an AI
that I think is actually inhuman
because any AI we create
like I would say is human
it's a can of worms
put it that way and I'm curious
at the show of hands how many people have read
some substantial fraction of Asimov's
universe
okay almost every one
I'm glad and so there was another question
over here yes
yes from previous last year I've seen
I've got the impression that you wanted this goal
to maximize the future freedom to be able
to perform the intelligence
itself but here you are basically saying
I can maximize your own future freedom
to maximize the humanities
that's right so
I mean the concern about safety
is presumably about the first
scenario where it's maximizing its own
things and I was wondering
if you did get the goal to maximize
somebody else's future freedom of action
would it still be
attention according to your theory
because I thought it was necessary to maximize
it's a great question
so the question is
in previous talks that I've given
the focus is on systems that maximize
their own future freedom of action
they don't displace this also
on other systems
I don't have a terrific answer
to that except to say
as a provocation maybe we need
to generalize
the notion of the information
theory notion of freedom of action
maximization as
something that can happen to any system
versus itself
but that's a great provocative question
yes question here
I mean more people who've actually understood
and internalized the point of Asimov's novels
it's not a great question
so I
would say this is a universal
utility function so this is not
specific to any particular contingency
and when we tell stories
it's often the case that stories contain
implicit bottlenecks in the future right
we have to go on this adventure to save
someone because if we don't
save the person then the world falls
apart so I would say
if anything this is sort of a storytelling
bias that we have that we like
stories that contain bottlenecks
and so it's very natural
to interpret or even dare I
say misinterpret a fundamental
process which is maximization
of future freedom of action as
something else that I'd say
by trying to maximize a narrow utility function
that's story specific
only because we like to tell ourselves stories
that contain bottlenecks
you say it's not a racial thing
and I don't
my feeling is that
right
basic entropy
from ourselves on to the
AI it seems like to me
you know we're moving the entropy
burden on ourselves
well this is sort of what
my thought about
friendliness
and causal entropy maximization
is
the question becomes friendliness
to what entity
because I mean if
I view myself as a system
or we look at the whole system of humanity
or my extended self or whatever
and the system is acting
so as to maximize
its future avenues for growth
expansion learning however you want to
formalize it probably
my way of doing that over the next
thousand years is not going to
involve remaining a human being
nor remaining hardly anything
in common with the current Ben Goertzel
but there would be a
maximization of growth possibilities
by replacing my brain with something
much much much smarter
so then that's not
what's normally meant by friendliness
because people in the friendly
AI community tend to mean
friendliness to human beings
and something like recognizable form
but it is friendliness
to causal entropy growth
if that's what we want to be trying to do
that's a well on an astro for too much
longer since it seems to
but so
something that
as most of you self recognized was that
his
for laws were in a certain sense
fundamentally unfriendly to a human singularity
that the robots
what's programmed with these laws didn't
really want humans to ever transcend
their biological limitations
because they had already done that
they were programmed to keep humanity
in a recognizable form
and I don't want to spoil too much more of the foundation
yes
good thank you
but it's
so it definitely seems to me
like next month causal entropy
is a big part of the intelligence equation
but I'm not sure it captures
all of it and I have this image
in my head of like a bunch of humans
with like a lot of energy sources
and they go like spend them on something
that seems cool and the AI is like no no
no wait you can't spend
the energy
you need to like keep all your options open
like keeping your options open is
like
sometimes a good tool for taking what you want
but it doesn't seem like a cool story
so that's an excellent question
I'm glad we moved off
so
the way to answer that is what you're positing is
imagine we have a closed universe that has
finite resources
how do you act in a universe that has
finite resources
if the resources can be consumed irreversibly
then all other things being equal
you want to try to conserve
and avoid consuming them
I would say
maybe a better example
a more explanatory example is there's all this
microeconomics literature that suggests that
humans evaluate costs
logarithmically rather than linearly
so if you want to say
if I'm worth a million dollars how happy
am I about my
my self
versus a hundred thousand dollars
and there's a wealth of microeconomics literature
that says your self-happiness
grows approximately logarithmically
with your wealth rather than linearly with your wealth
and you could ask
so does this theory
speak to that at all
and the answer is yes
so if you imagine going back to your example
you live in a universe that has a finite
set of n resources which
can be consumed
and you have let's say
only one time step to consume them in
so that means there are impossible actions
you could take to consume
so the cause lines would be associated
with that time horizon of one step
would be log n
and interestingly
you get the log n for the fact that you're maximizing
the cause lines over here so you could
posit that
this is sort of a side project that might
have corresponded with a number of
economists who specialize
in the information area of economics
a major outstanding question
in economics both micro and macro is
what is wealth
how do you define it
if you ask the leading practitioners
today they'll say wealth is some
linear combination of labor
capital plus natural resources capital
real estate
currency a few different things added together
but maybe that's not as satisfying
as say measuring wealth
in physical units of kilograms or meters
so some of the
economists that have corresponded with
think that this notion of causal entropy
is sort of first plausible
mathematical formalization of the notion
of wealth so that you can actually
calculate the wealth of a system
or a person or economy
and then going back finally to your example
that there are resources in a constrained universe
we get this
law of
finite resources out for free
because we're measuring an entropy
so it's purely speculative
obviously but
if I had a gun to my head
and I was being asked so
where does this law of wealth happiness
affect our products and maybe
this is a reason
so lots of hats now
from left to right
I'm wondering if this would work
remind me a bit of the work of Simon Kirby
and other linguists at the end
who tried to explain the structure
of human language
and in terms of
the communication
but we have a finite amount of time
to communicate a lot of information
and unless there's some kind of structure
in that it's just the
it's very chaotic
they use the term
right so or maybe
another example since everyone loves
slide 5 metaphors
has everyone seen
the movie from the 70s
Colossus the Forbidden Project
scary movie
so what do the two AIs
the beginning of the movie first do
the USAI and the Soviet
for those that remember
first thing they do
they start communicating with each other
so one can say immediately
now we know why they're communicating
from first principles because they can
access more path through face to face
if they're coordinating versus not
and this actually I mean sort of
it's a silly example but
for more sort of real world systems
energy systems
smart grids where you have to decide in real time
do we keep these patches of smart grids disconnected
do we allow them to be completely independent
and uncoordinated or do we want to connect them together
this allows you to actually shut up
and calculate answers to that question
whereas previously there was no really good
formal way of doing it and the answer would be
so you connect them up if your theory shows
that you're going to access more
future path through face to face by connecting them up
in coordinate, yes
so my question is a problem to make
which is actually I don't see
how you don't try to localize
amount of that
since you're following the branches
there may well be a point where you have
lots of open possibilities
but then for having more
possibilities you need to follow one
particular path where you won't have any possibilities
for a long time
and then have more possibilities there
good question, so the question was
since we're just ostensibly
doing naive gradient following
why don't we fall into local minima
local maxima, whatever
why don't we, you know, how do we avoid getting trapped
the answer is
this is the difference between a conventional
and a causal entropic force
so we can see around local minima
local maxima
with this time horizon
so think of this in the sense
of sort of smoothing out
a landscape that can be enormously
gillian discontinuous
but because we're looking into the future
we can effectively smooth over
these local maxima
and the other way of putting it
is if we take this
time horizon extended to infinity
if we get ourselves caught at a local maxima
then we're not going to
have access to all the possibilities
we would have if we were out of global maxima
and by stretching
this time horizon out above the
time scale needed to jump from a local maxima
to a global maxima we've effectively
smoothed over that
that hilly landscape
so the short answer
to your question is
the causal part of causal entropic maximization
smooths that part over
so that with a long enough time scale
again with a few reasonable assumptions
regarding say, herbidicity
you don't have to worry about getting caught at local traps
what is it?
yes
so I guess we're going to pull out one question
so what I see here in the eye terms
is a heuristic result
of problems in the world of heuristics
if you go up to a fixed step
from a horizon apex to a heuristic
you go up to a fixed step
you have in a full AGI
you'll have to be multiple heuristics
so in my mispandemic
I built an old championship bubble
a bubble program
mobility was one of the heuristics
potential mobility in the future was one of the heuristics
those alone couldn't solve the problem
you needed additional knowledge
so there's knowledge in an intelligent system
about how you define the spaces
that you search
how you search the spaces
if you've only got things like
mobility or entropy
it's very easy to construct problems
in which
there's lots of space over here
there's little over here, the solution's over here
but it's further down than your
sure
so without additional knowledge
you can never solve such a problem effectively
with this kind of approach
it's one heuristic
for one aspect of intelligence
it's not a proposal for all of intelligence
so I can turn the question around
and say something anti-symmetric to you
which is that
you implicitly had some external utility function
that you were passing into the system
anyway which is win a fellow
well that could come internally from the system
but that's part of a full intelligent system
where do those come from
where do the bottlenecks come from
so here's my
deliberately provocative answer to that question
my deliberately provocative answer is
maybe that's asking the wrong question
maybe we should instead be asking
not the question of where do the goals come from
we could instead say
the universal goal
is to maximize future free production
and in certain contexts
say you're a human in a box
stuck with an a fellow game
in front of you and by whatever means
you've been notified
that if you don't do your best job
in this game with fellow you're going to be killed
right
and so what I'm saying is basically
partly to be provocative
but partly because I think this is
sort of an interesting perspective to take
if we zoom out from
this box where we're in a certain
in a thermodynamic sense
where it's an open system
we're passing in utility functions in and out
if we zoom out to the closed system
why was the system playing a fellow in the first place
well if you want a human style
AGI in there
why would a human in a box play a game of a fellow
and
the answer that this theory would
attempt to provide would be
there needs to be some incentive
to follow some random utility function
like a fellow game
that's very arbitrary in the entire
space of narrow utility functions
so I hope that
that makes sense basically it's answering
a different question
so we should talk more
so we should talk more
so the way you understand this is that
your utility function basically maximizes
the cost of electricity right
yes
so I don't quite see how that is a good thing
like what if it maximizes
the number of points in the torture
just some predictions
so let's play out a thought experiment
this speaks to the
those will all be freedoms
sorry well
those will all be freedoms
the torture part
there's more bad experience
so the number one question here is
people seem to really want to do a thought experiment
where we build an AGI that does this
and they want to ask what happens to humanity
if we build a cost of electricity
maximizer with sufficient resources
this is what I'm hearing
so far
so let's do that thought experiment
and now to get it out of our system
and in particular to answer the torture question
so what would happen
so again
as a thought experiment
so we have to ask the question
so we have this AGI
and it's trying to maximize its future freedom of action
and it faces a question then
do I torture the humans or not
and then they have to run a calculation
because this is all based on
just connect with me
maybe it's out of batteries
can everyone hear me
what
I'll just continue while we fix this
so we can do
a thought experiment
do I torture the humans or not
so which course of action would result
in a larger future freedom of action
for the AGI
for the humans or not
what do people think
do you say torturing
why do you say torturing will result
because I've already
experienced the freedom of not torturing
now I want to experience the freedom of torturing
I've gone through one pathway
and I want to go through the other one
so the answer depends entirely
on the model of the world
that's absolutely true
excellent point so the answer depends on
the model of the world
let's say we have a model of the world
that corresponds roughly to
all of the humanities
no knowledge about the world
just for the purposes of a fun thought experiment
what do people think
do you speak up a little bit
can you hear me
I'm sorry
so what I was saying is
the gentleman in the back
made the excellent point
that the answer would depend on our model of the world
so to refine the question
and do the exercise
entertaining let's assume that it has
circa 2014
best agreed humanity
knowledge of the world
and everything that it contains
so what do people think would happen
so it seems that
torture limits the tortured people
with ability to do things
so the answer should be obvious then
no but it could free up an option
exactly
well
it seems to me that
I mean killing
rearranging all the
molecules or particles
inside humans
to add into the AI's processing power
could
fairly plausible be seen as maximizing
in AI's freedom of
activity
or maximizing just the freedom of activity
the collection of matter in the solar system
or whatever because we may not
be that efficient but
it seems unlikely to me that
I mean torturing people
is kind of a boring, pathetic and uninteresting
thing to do for a super human
AI system
it seems unlikely that that's going to be the way to maximize
its freedom of activity
just think about you for a human being
are we maximizing our freedom of activity
by spending a lot of our time
torturing ants and bacteria
not really, I mean that's actually
a very constrained
tedious and boring thing
I think
there's a fun answer as far as
right now the answer will
depend enormously on your model
of the universe and then indirectly
of what the actual
nature of the universe is itself
so if we sort of reduce
this question
I don't think it's worth it
if we reduce the question
to say
a simpler question which is as a human
if you're offered the option of torturing
versus not torturing another human
what choice would you make and why
many people would say
aside from references to ethics
which are
which have certainly been very difficult to formalize
would give an answer
like well you know it's illegal
or I'll go to jail or I wouldn't
want it done to me as I think one of the best answers
but let's consider
the first two answers
I'd be sent to jail if I tortured another human being
so what is jail?
Jail is restriction of your future freedom of action
so your argument that
I don't want to torture another human because
I'll have my future freedom of action reversed
I think is a trivial example
you generalize then to an AGI
say we find ourselves in a sci-fi scenario
of one AGI on the planet and the rest of
humanity and the AGI is asking
itself do I torture all of humanity
some of humanity none of humanity
and why the answer would depend
on whether there would be
perhaps countervailing
forces in the universe
to try to punish an AGI
through reduction
of its future freedom of action
so with today's knowledge
we don't have
I would say the knowledge to be able to credibly answer
that question but my proposal
is that at least we have the formalism
to frame the question in rigorous terms
so other questions
let's go over it
so the question was
does into causal entropy maximization
just revert
or reduce ultimately to
survival maximization
so that's a very interesting question
I have several different answers
to that
first answer is you can use this to calculate
in universes where there's no concept of
survival
so maybe this is an excellent jumping off point
to actually show some examples
how to take a call first
how many people have actually
seen any systems
any demonstrations of what happens
when you apply causal entropy maximization
to systems
so maybe a third of the audience
so if you guys will permit me
I'll just show a video that demonstrates
what actually happens
and this is a very long and circuitous way
of answering your question regarding
doesn't this just reduce to survival maximization
the answer is no it doesn't just reduce to it
because there are a variety of systems
where you get really interesting results out
of the concept of survival
so let me
I suppose no way to connect
the audio here so maybe I'll just
right before me
alright so people let me know
if you can hear this
fortunately it's transcribed
so
okay so I'll just
I'll now read this
so first example
standard problem control theory 101
an inverted pendulum
so this isn't the full video
but we've shown that
this control theoretic task
you take a downward hanging
pole connected to a cart that can move in
in one dimension
that if you connect up a causal entropy maximization engine
to the cart
it will immediately
swing up the pole and
vertically stabilize it
let me just
show part of that video here
if you want to see the full video it's available online
you'll see here
maximizing causal entropy by keeping
the pole upright
why is this causal entropy maximizing
behavior because
if you're a cart and you have finite
you have access to
finite temperature reservoir
basically a velocity limit
if you swing the pole up and then you start
to allow it to swing around
now suddenly you've biased yourself
in angular momentum space
and it's going to be very difficult to access
more paths through position momentum space
so this is a four dimensional system
horizontal position in momentum of the cart
and angular position
in momentum of the pole
turns out that there's a global maximum
in causal entropy space
there's a whole upright
if you want to be
yes
let's hang on
it turns out that
that's not a global maximum
so if you want a heuristic argument
for why hanging down
is not a global causal entropy maximizing
maximum
because it seems like you can still go into this
in the future if you want to
so one answer is
if you have access to finite energy
to move around
if you're upright you have more potential energy
and while this reasoning doesn't generalize
beyond this example
it's easier for you to access all of these other positions
more quickly because you have all of this
potential energy you can then convert
over to kinetic energy versus drawing upon your scarce
kinetic energy reservoir
question
sorry question
it seems like you're going to focus
a couple times on different time scales
on different time scales
let's
sorry
sorry
sorry please continue
yeah so the different time scales
let me get away with
you know avoiding
look at this
I'm wondering
how that applies in this case it seems like
there's seven time scales
that would allow you to have
a swing around
and that's an excellent point
and without allowing that
you're reducing some of the degrees
of freedom
so the question was
on that time scales
conversely I could say if you reduce
I think this
can I throw my phone
on your phone
so
the answer about time scales is
I could pose the converse question which is
what happens if you make your time horizon
shorter than say some characteristic
time needed to swing around what happens then
and the answer is it behaves stupidly
if you
for this system if you take t goes to infinity
then it converges
on this behavior of always spinning it to the
upright because there's still always going to be
a small difference even though in principle
you would sort of be really lazy
to wait around for future need
to explore all of these
possible paths through phase space
there's always going to be a small
differential advantage to swing the upright first
and then remaining there for the rest of the universe
as to how it goes to infinity
yes
so you're going to have
if you get to pick that way
for some of these other examples
you are going to have a little bit of a problem
because
there will be examples where
some of these won't
allow
you know
they'll say it's a spin violation
instead of exploring
possible possibilities
because they're a longer term
is that a problem?
no so I'll show an explicit example
of that in a minute or so
any other questions on this before we move on
so that's an excellent
question
so to paraphrase the question
won't for a broad class of systems
we just be completely lazy
because
it's more convenient to be lazy
than to do other things
and I'll show one explicit example
of a case where
the universe is energetically flat
and
it's periodic
but we choose not to be lazy anyway
so
let's go to the next example
so tool use
so for anyone here
who's familiar with the
ethology literature
it's very natural to ask the question
since we would say struggle
as a species with defining
what human intelligence is
it's very natural to go to behavioral studies
for some semblance
of standard experiments
that we can apply to a system that we can't
communicate with, do touring tests
with all these really human specific
activities
so in the ethology literature
study of animal behavior
it turns out that there are a few canonical
physical experiments
that are performed on animals
to try to benchmark their level of intelligence
the mirror test
is a very popular one in the media
but it's not the only one
there are a few others that turn out to be
far more widely used and not dependent
on having a mirror
and also
it can be argued that the mirror test doesn't actually
measure anything other than
one narrow slice
so maybe we want lots of canonical experiments
that we can test across animals, humans
and AIs that we build
so one popular example
is this concept of tool use
so
in its most canonical form
tool use experiment
basically consists of giving an agent
an animal, human, whatever
access to an environment
containing
a restricted region that is too narrow
for the agent to reach into
but for which
there exist objects in the environment
that can be deployed to reach into that environment
for the agent
we call these objects tools
we call process tools
in terms of animal behavior
this has been demonstrated in crows
demonstrated in a wide variety of primates
typically to fish food out of the container
a few years ago
a pro example of crows figuring out
how to use paper clips
to reach into cylinders
that were too narrow for their beaks to reach into
to fish food out
became a popular example but it's just one among
many animal examples
so it's very natural to ask
does causal entropy maximization
in this
canonical animal test
of tool use? the answer is yes
so let me just step through this
frame by frame again
there's a more
exacting video of this
online I'll put it at the end
so we start with the system
it's a model system
for convenience
because we don't want to waste too much computer power
trying to
simulate 3D objects
so it was very natural
to ask ourselves what's the sort of
lowest dimensional model representation of this
tool use experiment
and so the lowest dimensional model
that we can come up with
involves so it's a two dimensional universe
instead of a three dimensional universe
and it involves three disks
so people like to sort of
joke that a physicist likes to model
a cow with a spear
so it's sort of a popular job
but there is a reason why it's popular to
model objects with spears
it's very easy to understand analytically
so here we have a large disk
representing agent
animal this large disk is being
forced by causal entropy maximizer
so it's being steered
by this engine
and we have two objects in the environment
we have two disks
both of which are passive
which is actively forced
by a region here that is too narrow
for this agent
to squeeze into
and we have these added degrees of freedom
in this
container that are initially locked up
because the object is stationary
representing again the standard
animal intelligence test
and so you can see almost immediately
so we apply
causal entropy maximizer forces
causal entropy forces to this large disk
it immediately collides
and this
will hopefully speak to the erudicity
question from that moment ago
with this other disk
that was previously stationary in its environment
in order to cause it to enter
probably have to play this several times
cause it to enter
this restricted region
unlocking the degrees of freedom
for that object that had previously
been stationary there
and that's something really interesting to happen
so let me just pause it before
we get to the next interesting stuff
and ask for a second
why to the erudicity question
why didn't this large disk
which was being causal entropy
force
10 minutes?
10 seconds
no 10 minutes
we can ask why doesn't this large disk
just remain stationary
why doesn't it sit around waiting for some
future need to explore space space
the answer is it has much greater
access to position
and momentum space by
keeping both of these disks
and let me replay the segment
so you get sort of a more visual feel for this
keeping them within
really tight temporal access
to their position moment
and degrees of freedom
so you'll notice
constantly colliding with them
and to use a sort of simplified metaphor
if you had say
a baseball or basketball
and you wanted to exercise
maximal control over its future freedom
of action what would you do
would you throw it really high in the air
would you just sit on the ground stationary
or would you sort of be
dribbling it
or sort of tossing it around in your hands
so that at a moment's notice you can enable it
to
I guess if you can't hold it
that wouldn't be a proper analogy
but say you're allowed to dribble
because if you're dribbling it
then on a very short timescale if you want to do something with it
you'd go throw it somewhere else
and in fact, and I'll get to the question in a minute
if we played this all the way through
we'll see because this is using
sparse sampling and can't model
all possible future courses
of action so occasionally
it will slip up and allow
both of these disks out of the container
and it's fascinating
what happens next
so if it has both of them
caged it has great access to their
future free of action
but then it slips up
because we're only finitely sampling
a certain number of possibilities
it's about to let one out
lets one out and immediately
puts it back in the cage
so I'll answer
yes
so precisely it's not exploring so there's
another state which is
more difficult to be accessible
but with much higher energy
you won't find it
if there's another state
of much higher energy
but it's completely different from that
it's not looking for it
it's just trying to stay where it is
I'm sorry, what has a higher energy state?
like there's another bus somewhere
with all the balls
and instead of trying to put those two
balls in the same it could have
like 1000 balls in another
box
I mean it's just like a model question
where you're asking about the explorer
versus exploit tradeoff
so this comes back to the model
so this is starting with a model
basically a complete model of its universe
where the only uncertainty is being
introduced by its own actions
it doesn't know yet what it's going to do
that's the only uncertainty in the universe
otherwise the universe is completely deterministic
so there are no parts of its universe
like hidden balls that's unaware of
it's not doing any sort of exploration at all
it already knows what the universe looks like
it's just deciding what to do with it
so how does it deal with the exploration
and exploitation of tradeoff?
so in this universe it doesn't need to explore
because it knows everything
hopefully later, maybe offline
when we talk about the explorer versus
there's a really interesting heuristic argument
that learning should fall automatically out of this
the one sentence version of that heuristic is
if you live in a universe
where you don't have a complete model
of the universe that we live in
and you have a choice
you can either explore
discover more things about your universe
or not
then a causal entropy maximizer
would all other things be equal
with reasonable assumptions
reasonable prior assumptions about the universe
decide that it does want to learn more
because if it sort of rambles
through its universe
emitting a stream of actions
for a universe that it doesn't have a good model of
then it will probably do a bad job
of being able to access
future freedom of action
and that can give you specific concrete examples
like say the problem of diffusion
in the universe where there's a tiny
window where if you just manage to
put yourself through that window
then you have access to a much larger universe
it's sort of the fruit fly trap
that exploration
will generically
again, under a variety
of simple prior assumptions
results in larger future freedom of action
unless you live in a universe that punishes exploration
but we can talk more about the
the other side of it
so there's going to be some implicit balance
of exploration versus
exploration
if you live in a universe that doesn't punish exploration
broadly considered
then there will heuristically be a bias to
explore absolutely
without needing to put learning in from the start
yes
when the big problem is the two small walls in the cage
why does it keep moving around
so
excellent question
short answer is
it's the same answer as the question
of why did it let it all out in the first place
that this is only simulating on the order
of a thousand possible paths
and the space is enormous
you'll never enumerate all of them so
having simulated a hundred thousand paths
the simulation would have taken a lot longer
but it would have converged to being effectively stationary
this is just noise basically
from finite sample
since I probably have only five minutes left
let me play without interruption
the rest of this video and then we'll
rebut all our questions
so
this is back
third example social cooperation
again the standard animal behavior experiment
to see if two animals are willing to
we don't know each other are willing to cooperate
to accomplish a task
very standard experiment in animal behavior
and causal entropy maximizing engine
that only passes the test
but then starts to play with an object afterwards
Pong which we saw a lot of in the previous talk
in this case
it's playing Pong against itself
again it's able to successfully
play Pong
why is it doing this? because
if it lets go of the ball
then the ball in this universe gets stuck
in this case
a networking example
where people are falling in and out
of connectivity
it's able to dynamically wire up
a social network
because in this case it's easier
to have a social network decay than it is
to be forced to wire it up
here we see some
sort of fun
navigation examples
where speaking a bit
this isn't actually exploration
this is more coordination and logistics
we can still talk about that afterwards
and then finally I'd say
my favorite example
I'll just dwell for a minute on this one
so this is
not a real stock market
this is a simulated range traded stock
where you imagine you live in a universe
where you have a stock that's just a Gaussian process
that's stationary
and
the policy
choice here for the causal entropy maximizer
is what should my asset allocation be
between cash and the stock
that is just fluctuating
and I think this also speaks
to the wealth questions you can see here
as this subtitle fades
so red represents
total assets under management
green represents
the fluctuating
sorry green is cash
blue is fluctuating stock price
so you'll see here dynamically
it's varying its asset allocation
getting into and out of the stock
it spontaneously discovers
by low to sell high
so in this case
going back to several questions I was asked about
sort of narrower
narrower utility functions
where is the utility function
to accumulate money come from here
and the answer is very similar
that if you live in this sort of
financially noisy environment
it's much easier through trading
to lose money than it is
to gain money and so
we're always told strategically
career wise try to position yourself
upstream so that you can choose
what happens downstream saying I get here
that since it's
much easier to lose money
than it is to make money
the equivalent of being upstream here is accumulating
assets you have much greater future
of action if you have a lot more assets
under management than you can trade because you can explore
more of cash stock face
face than you can if you're very poor
so this comes spontaneously
buys low sells high and for fun
even does online learning of the stochastic
dynamics of this Gaussian process which isn't that difficult
to learn but just to demonstrate
simple incorporation
of online learning
into the process accumulates
assets exponentially
so let me close with just this slide
I think
this sort of speaks to the ambition
of the larger program here
which is trying to formulate
problems in AGI
in terms of simple physical
principles with the idea that
if we know if we can
sort of summarize in a single sentence
that can be readily encoded in software
a protocol
that AGI follows without specifying the mechanism
this is only chasing after
a protocol that if this system
accomplishes these types of effects
then let's call that intelligent
if we know if we can sort of establish
what a protocol for AGI is
first then
I think and hopefully some of these examples
take baby steps towards demonstrating
that if we know the principle
phenomenology of the protocol
we can then work backwards to
identify the most efficient mechanisms
for achieving that protocol
so with that I'll close and with whatever remaining time
I have, happy to take more questions
I want to pose one question
so
you probably saw that this
psychologist and columnist Gary Marcus
he wrote
a response to your paper
is Gary here?
no, I don't think so, but it was not entirely
favorable
to briefly summarize
his views seem to be
that psychology
and complex systems
are sufficiently rich
as to elude any simple
mathematical principle like this
and the examples that you've given
appear to us
to be analogous to various complex
real world cases, but really
they're extremely simplified examples
with few variables
and without the contextual richness
of ordinary life
so I guess his view would be
even if some entropic principles
explain
physics at a lower level
and can explain some
simplified examples that look to us
like human psychology
or sociology
to explain the rich behavior of human beings
you would need
in practice
rather than in some theoretical principle
of reducing humans to particles
in practice you would need a host
of other principles
alongside anyone
principle like this and it's going to be
a complex
mass of explanations more like you see
an actual cognitive science
that sort of relates to what
Paul was saying
not exactly the same
sort of
rationality
which provides simple theories
if you don't pay attention to boundedness notion
for example it's boundedness that
causes all these other things to have
to be part of the theory
to make up for the fact that you can't be
purely rational of infinite knowledge
in a complex universe
I'm curious for your summary reaction
to Gary Marcus's complaints
which you probably have encountered before
so I'll first give
my summary to your summary of your
comments
so my response to your summary of his comments
is to say
I was an undergrad at MIT
my first research advisor was Marvin Minsky
Marvin
ultimately
I would say sort of had this
somewhat similar belief
to what you're summarizing
that intelligence at the end of the day
isn't one sort of simple effect
it's an accumulation of lots of effects
that live in a society
and are sort of all competing
and cooperating at any given time
and that there's no at the end of the day
no sort of simple equation that we can use
to summarize intelligence
that is one school of thought
that said
I think we can be more ambitious
than just saying
sort of throwing up our hands and saying
there's no simple explanation for intelligence
like there is for Lyft
print physical principles or phenomenologies
that once understood enable us to do
really incredible things
I think it's an incredibly valuable enterprise
to continue the search for simple
unifying principles that will let us do
these ambitious things rather than just saying
without having spent more than
half a century on this problem
oh it's just a complex combination
of lots of things that we don't really understand
and ironically Marvin used to scold me
for using the term
inversion he would always slap me on the wrist
and say whenever you use the term
emergent that's just indicating to everyone
that you don't actually understand
Marvin doesn't like
self-organizing complexity
ironically and yet there's
full school of thought that says intelligence
is just a combination of a million things that we don't fully understand
I don't come from that
school at the end I'm very hopeful
that we will be able to find simple
unifying explanations for all of these things
and if we don't
we don't lose anything from the search
but if the search succeeds
now we found an e equals mc squared
for intelligence at best
intelligent planning at worst
that will let us do really powerful things
and understand through
really simple thought experiments
how an AGI would behave under a variety of
circumstances
yes
when you are using this kind of argument
that using the
entropy principle
allows you to do very useful
things then it seems that you are
substituting
your entropy principle under a utility principle
also when you said that
you can write the Go program
which wins
it seems that the entropy model
is just a sub-model
towards utility
I would say it the other way that
writing a Go program that wins
writing any program that
maximizes some narrower utility function
like a winning Go program
and just a side effect of a deeper principle
that is much broader than that narrow
context
you can put a gun to the head of the
computer Go player and say
I am going to punish you
through reduction of your future free of action
if you don't do a good job
and then you can see immediately how that side effect
follows up but humans if you put a human
in a room with a Go board and you say
do whatever you like
then a human won't naturally play Go
we need incentives
this is the most
we talk about reinforced learning
about this community this is in general
I would say a theory that underlies reinforcement
without requiring the specification
of a narrow utility function
this is a fascinating
discussion but I think
in the interest of not having
the conference run till 8pm
we are going to have to
have to call lunch break now
thank you Matt
thank you
