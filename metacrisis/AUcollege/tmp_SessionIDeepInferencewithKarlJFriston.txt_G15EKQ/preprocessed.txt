Thank you, Amos. Hello, everybody. Amos, that was a lovely tribute to Mike and so I won't say
too much more other than to say that I can't believe the firefighter problem has come back
into our workshop. I think so many of us remember the spirited debate that ensued when that firefighter
problem first was introduced and so much so that I think that we actually had to declare a moratorium
on discussions in subsequent meetings because it kind of took over our debate so it was lovely to
be reminded of that happier time and that happier moment. I'm just I just want to say that you
know as Amos mentioned Mike was a constant in the activities of our institute and just
for me personally having him as a co-chair on the advisory board made decision making making
easier and it was just a comfort to know we were pursuing a shared vision and so I too will
miss him greatly and there's a large hole there that I'm not sure how we're going to fill.
Moving on to the first session and I don't I haven't seen whether Carl is here but I
assume Carl is here somewhere and I can't tell from the zoom but let's assume that Carl is here.
Our first speaker I'm delighted to introduce is for our invited session is Carl Friston.
He's a theoretical neuroscientist and an authority on brain imaging. He invented statistical
parametric mapping voxel based morphometry and dynamic causal modeling. These contributions
were motivated by schizophrenia research and theoretical studies of value learning formulated
as the disconnection hypothesis of schizophrenia. His mathematical contributions include variational
Laplacian procedures and generalized filtering for hierarchical Bayesian model inversion.
He currently works on models and a functional integration in the human brain
and the principles that underline neuronal interactions. His main contributions to theoretical
neurobiology is a free energy principle for action and perception known as active inference.
His awards are way too numerous to list here but one thing I will note I asked him whether
there was anything he wanted me to add and he said I was free to do that and some of you know
how dangerous that can be but I will add that I noticed that he's got and this is as an economist
this is just massive he's got over 144 thousand Google scholar citations so without further ado
and it gives me great pleasure to introduce Carl Friston. Carl take it away. Thank you very much.
That was indeed one of my favorite facts about me that you picked up. Let me just get myself
organized and then. Again thank you for that lovely introduction and it's a great pleasure
and honor to be invited to speak to you particularly on this sort of sad occasion.
I'm intrigued by the firefighter problem that sounds something I'd like to discuss even
if you're not allowed to discuss it. I'm not sure I'm going to get to that depth or problem or
challenge. My hope was in the next 30-35 minutes was to introduce a perspective on
evidence accumulation, the handling of information from the point of view of normative models of
sentient creatures, creatures like us or indeed viruses and plants and I'm going to take my lead
from a simple assumption that every self-organizing system, every information processing system
is at the end of the day thinking and behaving in a way to maximize the evidence for its models
of the world and I'm going to try and motivate that from a sort of machine learning perspective
practically and as the talk goes on show how these ideas translate in terms of the sentient
behavior of biotic systems like animals including ourselves. As has been mentioned this comes under
the rubric in the life sciences of as active inference, active sensing, sensing in the spirit
of trying to disclose those data or that sensory information which resolves the greatest amount
of uncertainty about your model or hypothesis about how those data were caused. Now if we have
time I'll give you a brief simulation of the sort of epistemic foraging that ensues from this.
So self-evidence is very simple, all it's saying is that you can account for most
perspectives on behavior in terms of this single underlying imperative which is that your internal
states of the processing system say internal brain states and the way that you act upon the world
and the things that you can control on the outside are both there in order to maximize the
probability of some observables or outcomes given or conditioned upon me or a model of how those
outcomes were generated and I'm just motivating the simplicity of this assumption from the
perspective of a number of global approaches to behavior and self-organization in the life and
sometimes the physical sciences. So if these outcomes are the most likely kind of outcomes
that I would expect to encounter then they define operationally my characteristic outcomes
to which I will aspire and apparently work towards. So in economics you could think of
the log of the probability of an outcome given it is experienced or witnessed by a particular model
or me as standing in for a value or a utility and from that one can spin off reinforcement learning,
optimal control theory and indeed in economics expected utility theory. That's interesting
because the negative quantity of this way of reading value is self-information, surprise a lot
more, simply surprise. That means that this imperative here, the self-evidencing, simply
translates as the imperative to maximize the mutual information, for example the information
principle in neuroscience or the principles of minimum redundancy maximum efficiency from
Horace Barlow and indeed the free energy principle which we have been involved with where
this free energy serves as a bound on this quantity of self-information here.
That itself in turn is interesting because the time average of self-information is entropy
which means that this imperative the self-evidencing can also be read as the holy
grail of self-organization may need to minimize the dispersion or the entropy or the dissipation
of my observable states that you find in theories of self-organization ranging from
cybernetics to synergetics and of course if you're a physiologist this is just a statement of home
astasis keeping physiological states within viable bounds. But I'm going to take another
perspective on this quantity and treat this quantity as the probability of some data given
a generative model where in Bayesian statistics that would be called the Bayesian model evidence
or more generally the marginal likelihood marginalizing over all the parameters that
engendered or generated those data there and in my world that leads to things like the
Bayesian brain hypothesis, formulations of evidence accumulation, predictive coding in
engineering and now indeed in the in the neurosciences. So I'm just going to now unpack
that kind of self-evidencing in a slightly more technical way with a special emphasis on
this variational free energy that serves in machine learning as a bound on the log evidence
in the sense that we can create a variational bound simply by equipping the log evidence
with a quantity that can never be less than zero namely a KL divergence and in this instance it's
a KL divergence between the some approximate or Bayesian beliefs about states of the world
generating the data and the true posterior over those states if I were able given those data.
So by optimizing or extremizing this free energy we have on the one hand a convergence of our
beliefs encoded by our internal states to the true posterior whilst on the other hand
this bound now becomes a bound approximation to the log evidence that this self-evidencing
principle aspires to to maximize and just to motivate why this might be a useful perspective
in what you get from this is essentially an explainable artificial intelligence in the sense
that having a generative model there commits you to writing down a way in which your data
were generated that you can explain to somebody else. It also requires you to make explicit
and transparent your prior assumptions that are entailed by the form and the structure of that
generative model and any hyper parameters associated with it. In a sense it also gives you a design
optimality in terms of making optimal decisions of an abductive sort in terms of using this model
in order to make inferences about the causes or the way that your data were generated
and that itself we will see leads to a principled way for data mining or data foraging and indeed
epistemics and generalized or artificial general intelligence particularly with sparse data and
we'll come to that but there is another perspective though or another way of carving out this
variation free energy functional and so just by rearranging the terms I can also read it
as a mixture of accuracy and complexity so essentially the free energy is accuracy minus
the complexity where the complexity here scores the difference between my Bayesian beliefs
after seeing some data and my prior beliefs so literally the degrees of freedom or the amount
to which I have changed my mind in the face of some data and that complexity term plays an
important role in many guys is it's just an expression of Occam's principle in the brain at
least the things like factorizations and functional specializations a simpler way
of explaining or making sense of data with minimum redundancy and maximum efficiency
if you're an engineer it also implies that you're aspiring to provide an accurate account of
observable data with the minimum computational cost leading to notions of bounded rationality
literally based upon an evidence bound known in my world as approximate Bayesian inference
that rests upon priors that sometimes people refer to as heuristics and of course one can appeal now
to the physics of information via the Zhazinsky equality and Landau's principle which means that
minimizing computational cost also implies a thermodynamic efficiency that in principle
the best way of accounting for data or making sense of data should be that way that uses up
the least energy and is performed in the minimum amount of time so the basic story I want to tell
in this short presentation rests upon thinking about well that's fine for understanding
senting behavior and a principle for sort of optimal action in the moment what would it mean
if I considered devices artifacts or creatures that can plan into the future and have a generative
model of the consequences of their action and what would this self-evidence imperative mean
for plans and the kinds of behavior we'd expect to see under that imperative
and the answer is or one answer at least is very simple because now I am
trying to maximize or optimize a free energy bound on evidence but in the absence of actual
outcomes the outcomes become a random variable and I can simply take the expectation of this
evidence bound under the predictive distribution of the outcomes that would ensue if I did that
and if we follow that through what we're going to see is that basically we can explain
the best kinds of plans the self-evidenceing plans as fulfilling two imperatives one that of
Bayes optimal design experimental design in terms of maximizing expected accuracy
whilst making Bayes optimal decisions by minimizing risk by minimizing the expected complexity
and I'm going to basically rehearse the observational suggestion that minimizing expected
sorry maximizing expected free energy is basically a mixture of these two things
just to make this a bit more heuristic what I would normally do to your psychology audiences
ask them to imagine that they are an owl and that you're hungry and then I'd normally ask a
member of the audience what are you going to do I won't do that now but I'm hoping that you're
all thinking well I'm going to go and look for food indeed that is the absolutely the correct
answer but in answering like that what you have said is something quite fundamental and I'm going
to try and unpack how important that answer is just by comparing and contrasting two ways of writing
down normative or objective functions for good behavior and I'm actually going to repair these
two perspectives in a few slides but I'm going to deliberately sort of introduce and celebrate this
this distinction or this dialectic first of all I could assume that if I make a move on the world
I implement a particular action you then there are going to be certain states of the world that
ensue and I may have a value function over those states and if I had a value function of states
then there exists a policy that is defined in terms of the best action to take from any given
state namely that which maximizes the value of the ensuing state however there is another way
of formulating good behavior which speaks to the answer I'm going to look for food because
looking and searching can be construed as a way of minimizing uncertainty in this instance where
the prey is an uncertainty is an attribute of a probability distribution I'm going to refer
that as a Bayesian belief or conditional probability distribution so that tells us something really
important it says that the best behaviors the best actions the best plans cannot be functions
of states of the world they have to be functionals of beliefs about states of the world
and that answer looking for food also tells you something else because it matters whether I look
for my prey and then I eat it or whether I try to eat it and then look for my prey so that
introduces a notion of sequential policies or sequential actions that have to be optimized
in relation to this functional of beliefs about states of the world and I'm going to frame that
aspect of optimizing a trajectory or a sequence of plans in terms of a principle of least action
where action here is just the path of the time integral of a functional beliefs and indeed it's
going to be the expected free energy and hence the time the path of the time integral of the
free energy is an action so what we have at hand here are two very different ways of thinking about
a good way to exchange with our world on the one hand you've got Bellman's optimality principle
that rests upon a value function of states and many of you will be familiar with that in terms
of optimal control theory, dynamic programming, deep reinforcement learning, Bayesian decision
theory and so on but there is another way of thinking about it and that's possibly
slightly simpler appeals to Hamilton's principle of stationary action or leased action and that's
really the principle that the free energy principle inherits from. We'll also see that this
is very closely aligned with notions of artificial curiosity and intrinsic motivation
in robotics, the principles behind acquiring the right kind of data, optimal Bayesian design
and sequential policy optimization. So I'm just going to take you through the functional
form of this objective function one more time because I think its simplicity is compelling
and it speaks exactly to a variational or an appeal to Hamilton's principle of leased action as
possibly another way of thinking about behavior or at least sentient behavior. So this is the
basic setup that we use to understand and simulate and indeed model behavior and choices that are
informed by observations. We have some observations that come in, they are used to optimize our
Bayesian beliefs about hidden states of the world behind the observations generating those
observations and we do that by finding the most accurate account that's as simple as possible
of those data and then we take those beliefs about hidden states or latent states of the world
and then we use them to evaluate the expected free energy given a particular policy
and in so doing we are effectively taking the expected complexity and the expected accuracy
conditioned upon a way of moving forward and that equips every way of moving forward, every policy
with a score and expected free energy that can be associated with a long probability of me doing
that and then I can just apply a softmax operator, find the most likely plan, select my next action,
change the outside world, that would generate new data and so the cycle continues. So that's the
basic idea, it is exactly the same equations that I've unpacked as previously just to reiterate
that we can either regard this free energy function on this evidence bound, excuse me,
as a mixture of complexity and accuracy or we can just rearrange it to express it as a bound
on log evidence as you do say in machine learning or deep learning with things like
variation autoencoders and the reason I've done it like that is just to see what happens to these
terms when we take the expectation to form the expected free energy and what happens is that
the complexity becomes a risk, the inaccuracy becomes ambiguity, the evidence bound becomes
something known in economics and robotics as intrinsic value or motivation and the log evidence
becomes an extrinsic value that we'll see in a moment can be associated with things like
expected utility. So what licenses my interpretation of these expected terms? Well
let me just go through and unpack this expected free energy by looking at a number of different
special cases. Let's pretend that my prior beliefs about outcomes given I am me encode my
preferences, my prior preferences about the sort of outcomes I expect to encounter following all
actions and let's assume I'm agnostic about what happens. I have no particular preferences for this
of that outcome. That just leaves this quantity here which is the expected evidence bound and
this quantity is just the mutual information or expecting mutual information between the causes
and the consequences under a particular plan of soliciting those outcomes there.
In neurobiology, particularly in the visual search literature where people try to work out
where am I going to look next? What motivates that visual palpation of the world? The motivation
is just the intrinsic value of resolving uncertainty called Bayesian surprise. More I
think revealingly is just the degree of belief updating or information gain afforded some outcome
in other words the difference or the KL divergence between my Bayesian beliefs about hidden states
if I were able to see the outcomes that ensued following a policy compared to my beliefs before
looking at those outcomes again speaking to the difference between a post-era and a prior but now
in the future. So it's an expected divergence or belief updating or information gain
that is intrinsically valuable irrespective of what you think all will prefer to happen.
Let me now take another source of uncertainty off the table namely the ambiguity. I'm going to make
a simplifying assumption that I can see all the hidden states of the world that my sensations,
my sensory observables are a direct mapping of the states of the world that generated them
and in this instance the S's become the O's and we're only left this ambiguity term disappears
and we're just left with this quantity here which is another KL divergence or relative entropy here
namely the difference between my anticipated outcomes or states given a particular policy
and my preferred outcomes, my prior preferences. So all we're saying here is that the actions that
have the optimal expected free energy are just those in the absence of any ambiguity that minimize
risk as scored by the difference between what I anticipate will happen and what a prior I prefer
to happen. And if I make a final move which is to take the last kind of uncertainty out of the
game namely the uncertainty that depends about states of the world that depends upon my action
that I'm just left with this term here the extrinsic value that if you remember we're
associating with utility in economics or negative loss functions in optimal control theory which
just means we're trying to optimize the expected log probability of our preferred outcomes or
expected value here. So what we have at hand is a decomposition that applies to
policies when scored in terms of an expected free energy functional
that can be unpacked or carved into an expected value that underwrites base optimal decisions
but importantly this epistemic term this information gain that is formally identical
to the principles that underlie base optimal design and things like active learning with unknown
parameters. So in short the expected value plus the information gain is literally equal to the
expected free energy. I slipped this slide in because I thought some of you might like to
think that in terms of information diagrams but given the time constraints and the opportunity
for discussion I'm not going to go into that it's just an interesting game that you can play in
terms of thinking about an information theoretic partition in terms of basic design information
bottlenecks empowerment and other takes on the right kinds of information gathering behavior.
I want to whip through now a simulation because you know we want to leave as much time for
any discussion that sort of exemplifies how these mechanics would unfold in empirical behavior
in my world. Here looking at sort of small animals or rats making decisions what we normally do is
build a generator model based upon a Markov decision process of discrete states that generate
outcomes parameterized by a likelihood mapping A and then we make the transitions over time
among the hidden states depend upon some policy depending upon probability transition matrices
where the policy itself rests upon my prior preferences on some cost function and then I can
equip this model with a few initial priors and hyper parameters and I've now got a completely
fairly complete description of any kind of behavior that can be written down in terms of
discrete updates of hidden or latent states that generate observables. I can then apply
standard variational techniques to optimize my variational and expected free energies
essentially using off the shelf technology that rests upon defining this approximate
posterior just for my interest. When one takes these off the shelf belief updating basic belief
updating variational message passing or belief propagation schemes they look remarkably similar
to the way that neuroscientists understand message passing and belief updating in the
in the brain in terms of perceptions updating beliefs about expected states of the world
policy selection usually associated with the striatum or integration between parts of the
surface of the brain and the striatum that rests upon this expected free energy and indeed
some hyper priors here or hyper precision parameters reflecting the confidence or
the uncertainty that I afford my beliefs about what I'm going to do next we can do learning
and then we just select the action on the basis of these updated expectations about policies.
So when we do that and I'll just close with an example of a synthetic rat doing this
and so this little paradigm here involves a rat in a teammate's it can go and secure a reward
that's either on the right or the left of the two upper arms it could only make two moves
but it's got the opportunity to go to the lower arm and get an instructional cue that tells it
that the cue the reward is on the left or on the right hand side so it's got a choice it can
either take a gamble and go to the one of the two upper arms and be right or wrong and it has to
stay there so these are absorbing states when it gets to the baited areas or it can waste one of
its moves by finding out exactly where the reward is and then go and secure its reward with a high
degree of uncertainty so from the point of view of expected utility these are fairly balanced
options but clearly from the point of view of this self-evidencing in the future
the information gain the intrinsic value of going to resolve your uncertainty or this rat's
uncertainty about the states of affairs is going to be more attractive and that's going to
compel the rat to be exploratory and go and look at the cue get that and then go and get its reward
and indeed that is what it does so I'm just summarizing the behavior that
follows from integrating those belief propagation or variational message passing
schemes under this geometry model but with a twist after the first couple of trials we leave
the reward here so that in principle the rat is going to start to learn that the reward is always
here and indeed it does that and of course as it becomes more and more confident that the reward
is here the intrinsic value the epistemic value the information gain afforded by going down here
gets smaller and smaller and smaller and wanes and once it passes a threshold where the expected
utility of going straight to the reward and staying there for an extra move and supervenes
then its behavior switches deterministically and it switches from an expiration to an exploitation
mode and that's exactly what happens in this example and after about the 20th trial here
and all this depends upon accumulating knowledge about the contingencies named the context where
on which side is the reward most likely to be over successive exposures that then couples back into
this online active inference planning as inference underwritten by this expected free energy this
is the last slide I'm putting in this in just to remind myself that there's an outstanding
challenge here I've sort of sold this self-evidencing as something that has constructed it in relation
to lots of other ways of describing good behavior there may be a deeper back story to this that
relates to statistical physics and things like the integral fluctuation theorem but to connect or
complete that story it requires one small move which I'm happy to talk about but not happy to waste
any more time I'm packing for you at the moment so I'll give the last word to Einstein that everything
should be made as simple as possible but not simpler so it just remains me for me to thank those people
whose ideas I've been talking about and of course thank you for your attention thank you very much
indeed terrific Carl thank you thank you so much for that really engaging talk and a great
way to start off our workshop I realized I actually forgot to mention to everybody else who's here
that what the format was going to be so we did actually a plan for some ample time for questions
and for discussion I think as we know as people who have been to our workshops before know that's
one of the hallmarks of our workshops is quite animated discussion and so thank you Carl also
for leaving ample time for that so I want to open the floor up to discussions and questions maybe
if you want to just drop a note into the chat to me just to tell me of a question so that we can
organize or line up the questions but I think the first question I know almost has
a few questions so we'll go to almost first but just drop me a note as I said if you have questions
to ask thank you Carl first it's nice to see you again I really enjoyed your talk and the idea
that you were able in such a simple way to put decision theory within information
theoretic framework and quantities mostly the KL divergence seem to be very interesting very nice
I myself even though I work on this area for many years I never really saw it in this way so
thank you for this I have a simple question though it could be outside the scope here
in in our previous time I've seen you you were talking about Marco Blankett and as as you were
passing through in one of the slides I saw that you put your framework within a Marco Blankett
maybe I missed it and it may be outside what you wanted to talk today but if it's not I would be
happy to hear a little bit more how this fits within your theory of of our Marco Blankett
all right that's a very astute question yes I tried to slip past the Marco Blankett because
he would take another half hour to motivate that but that's an excellent question because it
it speaks to that last slide where one's trying to find a deeper backstory in statistical physics
to why this kind of self-evidence behavior has to be there if any system manages to maintain itself
in some kind of steady state you know if it's in computational chemistry this will be self-assembly
if it's in biology be auto-polysis if it's in social neuroscience it's just basically maintaining
cohesions and in groups and and that's where the Marco Blankett comes in so in order to talk about
a system you'd have to actually write down well what do you mean by a system that has some states
how are those states differentiated from the states that do not belong to the system so that
implies a set of conditional independences that as you noted are usually assigned to the Blankett
states of the Markov Blankett so what you say is well in order to differentiate a system from
the rest of the universe I will assume the existence of some states that when I condition
the internal states on they become conditionally independent of the external states and those
are the Blankett states and then you look at the the density dynamics that you have under that
particular partition of states into inside and outside separated by Blankett states that still
permits so the system's still open because you can the outside can vicariously influence the inside
and vice versa through the Blankett states but you write you then look at the the statistical
physics of that partition and that's where the integral fluctuation theorem kicks in which is
why I was trying to and I'm still trying to connect the optimum Bayesian design plus optimum
Bayesian decision theory formulation in terms of information theory and KL divergences
with the the KL divergences underwrite the integral fluctuation theorems so that that's I
think I mean it's a little bit indulgent but I you know it really does if you like join the dots
between the statistical physics and the more practical aspects of information gathering and
and exploitation. Thank you. Great thank you almost I think our next question is from Bill
Lawless I know Bill you put it in the chat but go ahead and unmute yourself and just ask it.
Thanks for reminding me to unmute. It was a really nice talk I liked it a lot. It seems to me to be
based on on the information that's available or that could become available and that leads me
to my question it seems that the only way that you can uncover something like deception
is when it uncovers itself. Maybe you could address that but I'd also like to find the link
between which I like Bayes and maximum entropy production maybe you could also address that
as well if you have time. Yes we don't have time to address it properly but we can certainly
celebrate the existence of those important issues and questions. The maximum entropy principle
both as it pertains in the sort of classical genes in sense but also as it pertains to
entropy production in this sort of dynamic self-assembly formulation in terms of the integral
fluctuation theorems and Markov blankets plays an incredibly important role and just
almost emerges for free from the free energy formalism so if you just decompose the complexity
you've basically got the entropy of your beliefs about the causes of the data and that has been
maximized as mandated by things like Occam's principle so that's part of this sort of optimal
Bayesian design formulation and has its roots in sort of Geoffrey's priors and James's interpretation
of statistical physics and so that is really at the heart of this belief-based or at least
conditional probability distribution or Bayesian belief-based formulation of optimal behavior.
The deception issue is fascinating because it speaks to
the rather delicate and challenging and very real issue of applying this mechanics to
dyadic or multi-agent games where now you have the potential of the generative model
that's sort of in this instance that Markov decision process being a model of another model
so it's somebody that I am in exchange with in a game theoretical in a sort of language sense
and just you know the very notion of trust and deception or indeed just the very notion of a
generative model being able to explain outcomes generated by another generative model is deeply
challenging and we're talking about you know some metacognition through to theory of mind here
so I think just things like deception and regret are if you like the you know the most challenging
kinds of information exchange or the most challenging aspects of exchange dyadic exchange
that you would aspire to try and explain in terms of you know Bayesian optimality principles
in this framework I'm not sure it's been done yet there have been baby steps in terms of putting
these sort of free energy minimizing agents together and what tends to happen is that they
tend to cooperate because they want to make themselves mutually predictable so if you're
trying to minimize the surprise or the self-information one way of doing that is just to make sure that
I am as much like you as possible under the prior assumption that you are doing the same
and then our entire exchange becomes as mutually as predictable as possible
and that optimizes then the the joint free energy that that we share did you have anything
deeper in terms of deception because my answer was a little bit superficial in the sense that
it hasn't quite been done yet
yes I do but you mentioned the word assembly which I find to be even more intriguing
one of the issues that I have with Von Naumann's assembly idea is that there's no way to tell
when the correct pieces from Von Naumann's model I don't know if that's what you're addressing
there's no way to know that the assembly is working in the right direction that you're
actually improving the situation with assembly maybe you could address that
not in a scholarly way because I don't know the the you have a notion of assembly in a von
Naumann sense so I'll have to put that on the list with the firefighter problem to go and google
afterwards assembly in any sense how do you know that you're assembling a team of it based on what
you've said you want to assemble it so that the surprise is minimized so one one way of addressing
that is well certainly through numerical analyses and that's been done in the context of
dynamic interactions but also an interesting dance between the environment and a phenotype or an
assembly ensemble of phenotypes in the context of e-conniche construction and the bottom line
is that the because the this variation free energy is an extensive quantity you can read the goodness
of the assembly or the ensemble of systems are simply the sum of each individual's free energy
so if every individual of an assemble ensemble is optimizing its free energy then collectively that
is also true and then that sort of brings us back to what I was saying before them you know that very
much means that everybody's trying to to learn about each other so that they can render everything
else or indeed the environment as predictable as possible so we build traffic lights and roads
and we have languages and signs and deontic cues or I learn your language while you learn mine
so that that's a sort of the first order emergent property of this kind of multi agent assembly
that's what you meant but I do know of course that that doesn't leave any room for deception
so now you have to think about in groups and out groups assemblies of this kind versus that kind
and how you might actually use deception to exploit this sort of first order cooperative
mutual predictability imperative that comes out self-evidencing with other things like me
I have a somewhat um maybe it's related question but so so as as you were presenting I was I was
jotting down a note I made to myself and this is related to to things that I'm interested in about
the relationship so so as an economist I'm quite interested in people's perceptions and how
sometimes you know when we observe behavior that we would often characterize as not rational
it actually is probably behavior that is rational conditional on a set of of beliefs that are possibly
not correct or perception that's possibly not correct and so I was wondering about the distinction
between perception and and what I would call something like learned experience and your
slide on functional anatomy and message passing seemed to discuss that and you seem to emphasize
that beliefs are about uncertainty and so I was wondering if there was anything that incorporates
uncertainty associated with the learning side so so there's some uncertainty related to the outcome
that results from repeatedly doing the same thing so you know I think of in a rat experiment
you can design it so that there's a constancy of the experience so that after 20 times the rat knows
where the food's going to be but and presumably you can design a rat experiment where there's
some uncertainty as to whether the food will be there but but I was wondering about that about
how how how you think about learned experience and and how an individual knows whether whether
it's their perception or whether it's actually a series of of of experiences or you know basically
there's there's a dependence in there that I that I wasn't clear how to incorporate into into your
models right no no excellent question and that and that dependence is a really important part of
approximate Bayesian inference that rests upon a mean field approximation or a factorization
of beliefs about different kinds of things that when you actually do the belief updating and
necessarily you know repairs that dependency so that dependence is absolutely crucial and
central to any realization this this this Bayesian belief updating and in particular beliefs about
sort of time invariant parameters of a geratin model that encode laws and contingencies versus
quickly changing states in the moment so normally we talk about inference as basically
belief updating about latent states of the world while learning is a slower updating of the parameters
of a geratin model but you you bring up so many intriguing issues that I just want to mention a
couple of them bounded rationality seems to me that semantically and possibly rather cheekily
easily absorbed into the use of an evidence bound to move from exact Bayesian inference to bounded
Bayesian inference which sounds to me exactly like sort of bounded rational and you could even chase
that further with the complete class theorems that as you say for any given pair of loss functions
and behaviors there is a set of prior beliefs that render that behavior renders that behavior
pays optimal so there's a lovely duality there that all rests upon the prize that define the
generative model so that's that I think that's a really important point that you know we're all
different because we have different priors but we're all optimal at least given given those
priors practically you also brought the so the the inference and learning are both in the service
of minimizing this path integral of free energy that in an identical way but when when you unpack
the equations analytically you get this sort of fast belief updating that our neurons might do
for the inference part then you get this slow accumulation of evidence and belief updating
on connectivity that underwrites the parameters and when you start to do numerical analyses
something really simple but I think very telling happens that if you set off your I'm sorry I should
say that in practice what we generally do with discrete state space models is parameterize
the likelihood mappings and the prior usually transition matrices in terms of Dirichlet
parameters so you have to write down your initial Dirichlet counts now if you write them down very
very small the artifact then becomes very sensitive to evidence so that you know if you start off with
your two outcomes and you've got one Dirichlet parameter after just one experience one's going
to be sort of twice the other on the other hand if you start off with 200 Dirichlet counts and you
add one more you've got a ratio of 201 to 200 nothing has really changed so your prior beliefs that
encode an uncertainty about your beliefs about the contingencies that you're learning so if you come
in as a sort of young artifact that's just been born with very low Dirichlet counts you're going to be
very sensitive to any evidence that is accumulated through repeated exposure but if you're an old
wise recalcitrant rat you're not going to learn anything and then it's interesting when you come
back to the previous discussion you know when you put two of these agents together that are trying
to predict each other if you put the old one with lots of Dirichlet counts and interacting with the
baby one with very small Dirichlet counts then the small one learns a lot more from the old one
so it's a nice metaphor for you know for teachers and parents and they're like
sorry that's that's quite interesting I mean I can imagine a situation though where there's
I mean in some ways that you know young one and how it acquires experience is in some ways I think
of as an initial condition but over time you can imagine something where there's like you know
some limited memory or memory decay so that in some steady state there's you know it's not
the marginal additional piece of information has the same effect on all the different agents
now I don't want to take up all the time though because I know Duncan Foley also has a question so
I'm going to hand it over to Duncan Foley as well for his question well thank you for your presentation
that I'm particularly intrigued by the ability of your ability to bring out the parallels to
thermodynamic concepts in this way of partitioning the free energy relationships that are implicit
in the entropy reasoning I wanted to just get your thinking on a slightly different topic which
is what I would call novelty because I'm aware that in some models of learning an important
role is played by the ability of the system to create new categories or new states or
uh not just to update in the in the typical Bayesian way but to innovate in some way or
to accommodate novelty um and often there's a trade-off between the organism's willingness
to view something as a novelty and start treating it as something different or trying to accommodate
within the framework that's already been established by experience I just wondered
what your thinking is on how that relates to this general perspective of Bayesian updating learning
yeah I think it relates intimately and actually provides a sort of superordinate context and
interestingly actually speaks to Robin's last question which I would sort of frame in terms
of the problem of structure learning and when do you build an extra bit of your model to account for
this new piece of data or indeed remember that there's also this imperative to keep the model as
simple as possible so yes you have to have that optimal forgetting you have to have the loss of
redundant connections you have to you have to forget stuff so it introduces this really delicate
issue when is it optimal to believe update or indeed change the structure say the new number
of hierarchical levels in a deep network or add another level to some hidden hidden factor
so from the point of view of optimizing the free energy bound or model evidence this just is
Bayesian model selection so structural learning just is Bayesian model selection under some
prior beliefs about the volatility of the environment and there will be an answer to your
question should I add an extra level of say hierarchical depth to my model or not in light of
these data so you know if you had all the time in the world then what you would do is you'd
basically evaluate the marginal likelihood or the evidence bound with and without that extra level
and if the evidence the marginal likelihood increased you've retained that extra level
and if it decreased then then you then you would you lose the extra level and I frame it like that
because there are some people who think that that's why we go to sleep that we rehearse all the data
that we've accumulated during the day and effectively whilst doing things like dreaming
evaluate its capacity to explain those kinds of data and then identify those redundant parts
in the modern structure and remove those connections and simplify and do do housekeeping of a structural
sort so again it's speaking to this sort of optimal forgetting that Robin was alluding to
and I suspect your the thrust of your question which is which is you know when do I update
when I don't update and that would be one part another really important part of your question
there was a notion of novelty and novelty emerges in this scheme in a very simple way
it's simply the information gain or that intrinsic value not about hidden states in the world but
about the parameters that we've just been talking about so if in my evaluation of the expected
expected free energy of a particular move say looking over there for example or doing this or
opening that door there is certainly a an information gain an epistemic value or affordance
relating to the resolution of uncertainty about what's behind the door but there's also
an epistemic value or intrinsic value in terms of what would happen if I do that generally and
that's basically scored by the KL divergence between the posterior beliefs encoded in by
those Dirichlet parameters or the likelihood mappings or the priors before and after making
making that move it's actually very very simple to evaluate so that you've now got sort of two
kinds of information intrinsic value you've got the kind that scores the reduction of uncertainty
the information gain attributable to Bayesian beliefs over hidden states and the other flavor
which we can refer to say salience and the other kind that has mathematically exactly the same
form but it's a reduction a relative entropy or a reduction of uncertainty of beliefs about
parameters that you can think of as novelty so that hasn't been explored very much but it
becomes absolutely crucial when you start to try and explain sort of higher end executive
functions in cognitive neuroscience like things like sort of our hard moments or insights that
require you to go and learn something about the causal structure the regularities that endure
over multiple instances of a particular trial or a particular sort of sensory experience
so in my world in the active inference world that is starting to attract a lot of young people in
terms of how they want to simulate and write that down. Thank you. Great now I noticed Kenrick
Nelson posted something in the chat Kenrick do you want to do you want to mention it or is it
fine that it's just in the chat it's not really a question. I can just explain it a little bit
and then if there's interest people can follow up but you know as people are asking questions about
how you model deception in degrees of belief in updating data one of the things that came to
mind is there's some really interesting work by Fleming Topsoid who looks at these information
theoretic models and game theory and complexity and he uses generalized entropies to model your
willingness to update your beliefs and in particular you can parameterize it where
the control parameter becomes a your kind of perception of risk or your perception of complexity
and then you can have different players react differently to you know information games where
one player might you know just accept reality and thereby you know when they get new data they just
accept that data and update whereas another player might be skeptical about reality
and and be less willing to change their beliefs given new data so it's a different way of
modifying these models to account for biases and how people accept or reject data.
And I think very important if you take these ideas into the clinical realm in terms of
psychoanthropology and psychiatry you know that are thinking about things like delusions and
hallucinations or agnosias where people have very different interpretations about what's causing
their sensations and therefore very different weights to the sensory evidence at hand versus
their their priorities so I think that's a really important perspective just technically I don't
know this formulation but it sounds as if you would reproduce that mechanics by hyperparameterizing
the precision of various beliefs so you know if you want to make a rat more exploitative as opposed
to explorative you can certainly increase the precision of its prior preferences so that it's
the kind of rat that very definitively likes these kinds of outcomes as opposed to those kinds of
outcomes and in a similar way you can you can parameterize the precision of likelihood mapping
so if a likelihood mapping is very imprecise you're going to ignore a lot of data because you know
that the predictability of that data or its precision is low but on the other hand if you
assign your likelihood mappings undue precision you will be enslaved and your belief updating will
be very susceptible to any data and you may well start to overfit that data in relation to your
prior beliefs so I don't know but that it sounds as if that's the the the set of questions about
you know the relative weighting of different sources of information and the relative weighting
of sensory likelihood and you know relative to priors that are being addressed and just reiterate
that sounds that sounds important because it's in computational psychiatry that is usually
the seat of pathology that explains a lot of false inference and deep updating in psychiatric
conditions yeah yeah great so so everybody I think it's time especially I'm cognizant of the
fact that in this virtual world we have to keep on schedule so I'm gonna close this here but I just
want to first of all I want to thank Carl for an excellent presentation I want to thank everybody
else for the great discussion and just like in a regular you know sort of real world or in-person
conference I mean Bill's already posted a follow-up comment that that I think you know hopefully
that I find very interesting so hopefully we can take that into the break we can continue this
discussion in the break but I just want to close the session now so that people can take
10 minutes and do whatever they can do that but but again thank you so much Carl and hopefully we
can continue to have discussions into the break thanks very much Carl if you're still there I
think I think Bill had an interesting question and that it might be worth the having some discussion
about right sorry yes yes I was just about to get a cup of coffee so I'm sorry no no coffee for
presenters I'm sorry about that ah yeah this is the quantum biology quantum models of biology
wait Bill just Bill just ask your follow-up question yeah it's the owl problem the owl
in your view waits for prey to uncover itself but owls are also built in with the intelligence
that prey hide and so it's got to go into the business of uncovering a deception
Volkswagen had an extraordinary deception how do we uncover these deceptions ahead of time
that's the question I see that reminds me the stag hunt paradigm of you this is cooperation
versus competition in terms of yeah and yeah I don't have an informed answer other than to
say that you know that this is this is sort of high-end generative modelling in either a game
theoretic or game theory sense or in in a social neuroscience sense and the trick is to
build the sophistication of the generative models so that they can entertain or represent the
possibility that the the other agent knows about the you know their intentions and that requires
quite sophisticated quite sophisticated generative models and then one gets I presume into the same
realm of sophistication that is found in economics you know to what extent do I
represent your representations of me and then to what extent do I represent your representations
of you representing me representing you you know a lot level of recursion does you know
do you find that sort of that optimum in terms of this sort of boundary in the accuracy and the
complexity I don't know of any of any formal work within the sort of the active inference scheme
but I do know that there's a chap called Jean Daniseau who has looked at evolutionary stable
strategies and using sort of levels of recursions of beliefs in terms of diatic interactions
and actually found something quite quite it wasn't quite so much the the the prey camouflaging or
deceiving the predator but it was more about sort of populations of two different kinds
finding that evolutionary stable strategy as quantified by a minimum or an optimum
expected free energy and what he found was that the evolutionary stable strategy
was half the people had to be very unsophisticated and the other half had to be very sophisticated
and that was the only stable strategy otherwise you got this sort of leapfrogging
where you know the deceivers try to deceive the deceivers are deceiving the deceivers
but there was a stable strategy where there were basically sophisticated and non-sophisticated
agents out there I'm not sure it was framed in terms of deception though that I think this was more
just in terms of communication well thanks for your answer I'm sorry to have interrupted your
coffee so I got so I've got to do deception firefighter problems and what was the other thing
oh yes the the von Neumann assembly and these are things I have to go and do my homework on
