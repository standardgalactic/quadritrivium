Thank you, Amos. Hello, everybody. Amos, that was a lovely tribute to Mike and so I won't say
too much more other than to say that I can't believe the firefighter problem has come back
into our workshop. I think so many of us remember the spirited debate that ensued when that firefighter
problem first was introduced and so much so that I think that we actually had to declare a moratorium
on discussions in subsequent meetings because it kind of took over our debate so it was lovely to
be reminded of that happier time and that happier moment. I'm just I just want to say that you
know as Amos mentioned Mike was a constant in the activities of our institute and just
for me personally having him as a co-chair on the advisory board made decision making making
easier and it was just a comfort to know we were pursuing a shared vision and so I too will
miss him greatly and there's a large hole there that I'm not sure how we're going to fill.
Moving on to the first session and I don't I haven't seen whether Carl is here but I
assume Carl is here somewhere and I can't tell from the zoom but let's assume that Carl is here.
Our first speaker I'm delighted to introduce is for our invited session is Carl Friston.
He's a theoretical neuroscientist and an authority on brain imaging. He invented statistical
parametric mapping voxel based morphometry and dynamic causal modeling. These contributions
were motivated by schizophrenia research and theoretical studies of value learning formulated
as the disconnection hypothesis of schizophrenia. His mathematical contributions include variational
Laplacian procedures and generalized filtering for hierarchical Bayesian model inversion.
He currently works on models and a functional integration in the human brain
and the principles that underline neuronal interactions. His main contributions to theoretical
neurobiology is a free energy principle for action and perception known as active inference.
His awards are way too numerous to list here but one thing I will note I asked him whether
there was anything he wanted me to add and he said I was free to do that and some of you know
how dangerous that can be but I will add that I noticed that he's got and this is as an economist
this is just massive he's got over 144 thousand Google scholar citations so without further ado
and it gives me great pleasure to introduce Carl Friston. Carl take it away. Thank you very much.
That was indeed one of my favorite facts about me that you picked up. Let me just get myself
organized and then. Again thank you for that lovely introduction and it's a great pleasure
and honor to be invited to speak to you particularly on this sort of sad occasion.
I'm intrigued by the firefighter problem that sounds something I'd like to discuss even
if you're not allowed to discuss it. I'm not sure I'm going to get to that depth or problem or
challenge. My hope was in the next 30-35 minutes was to introduce a perspective on
evidence accumulation, the handling of information from the point of view of normative models of
sentient creatures, creatures like us or indeed viruses and plants and I'm going to take my lead
from a simple assumption that every self-organizing system, every information processing system
is at the end of the day thinking and behaving in a way to maximize the evidence for its models
of the world and I'm going to try and motivate that from a sort of machine learning perspective
practically and as the talk goes on show how these ideas translate in terms of the sentient
behavior of biotic systems like animals including ourselves. As has been mentioned this comes under
the rubric in the life sciences of as active inference, active sensing, sensing in the spirit
of trying to disclose those data or that sensory information which resolves the greatest amount
of uncertainty about your model or hypothesis about how those data were caused. Now if we have
time I'll give you a brief simulation of the sort of epistemic foraging that ensues from this.
So self-evidence is very simple, all it's saying is that you can account for most
perspectives on behavior in terms of this single underlying imperative which is that your internal
states of the processing system say internal brain states and the way that you act upon the world
and the things that you can control on the outside are both there in order to maximize the
probability of some observables or outcomes given or conditioned upon me or a model of how those
outcomes were generated and I'm just motivating the simplicity of this assumption from the
perspective of a number of global approaches to behavior and self-organization in the life and
sometimes the physical sciences. So if these outcomes are the most likely kind of outcomes
that I would expect to encounter then they define operationally my characteristic outcomes
to which I will aspire and apparently work towards. So in economics you could think of
the log of the probability of an outcome given it is experienced or witnessed by a particular model
or me as standing in for a value or a utility and from that one can spin off reinforcement learning,
optimal control theory and indeed in economics expected utility theory. That's interesting
because the negative quantity of this way of reading value is self-information, surprise a lot
more, simply surprise. That means that this imperative here, the self-evidencing, simply
translates as the imperative to maximize the mutual information, for example the information
principle in neuroscience or the principles of minimum redundancy maximum efficiency from
Horace Barlow and indeed the free energy principle which we have been involved with where
this free energy serves as a bound on this quantity of self-information here.
That itself in turn is interesting because the time average of self-information is entropy
which means that this imperative the self-evidencing can also be read as the holy
grail of self-organization may need to minimize the dispersion or the entropy or the dissipation
of my observable states that you find in theories of self-organization ranging from
cybernetics to synergetics and of course if you're a physiologist this is just a statement of home
astasis keeping physiological states within viable bounds. But I'm going to take another
perspective on this quantity and treat this quantity as the probability of some data given
a generative model where in Bayesian statistics that would be called the Bayesian model evidence
or more generally the marginal likelihood marginalizing over all the parameters that
engendered or generated those data there and in my world that leads to things like the
Bayesian brain hypothesis, formulations of evidence accumulation, predictive coding in
engineering and now indeed in the in the neurosciences. So I'm just going to now unpack
that kind of self-evidencing in a slightly more technical way with a special emphasis on
this variational free energy that serves in machine learning as a bound on the log evidence
in the sense that we can create a variational bound simply by equipping the log evidence
with a quantity that can never be less than zero namely a KL divergence and in this instance it's
a KL divergence between the some approximate or Bayesian beliefs about states of the world
generating the data and the true posterior over those states if I were able given those data.
So by optimizing or extremizing this free energy we have on the one hand a convergence of our
beliefs encoded by our internal states to the true posterior whilst on the other hand
this bound now becomes a bound approximation to the log evidence that this self-evidencing
principle aspires to to maximize and just to motivate why this might be a useful perspective
in what you get from this is essentially an explainable artificial intelligence in the sense
that having a generative model there commits you to writing down a way in which your data
were generated that you can explain to somebody else. It also requires you to make explicit
and transparent your prior assumptions that are entailed by the form and the structure of that
generative model and any hyper parameters associated with it. In a sense it also gives you a design
optimality in terms of making optimal decisions of an abductive sort in terms of using this model
in order to make inferences about the causes or the way that your data were generated
and that itself we will see leads to a principled way for data mining or data foraging and indeed
epistemics and generalized or artificial general intelligence particularly with sparse data and
we'll come to that but there is another perspective though or another way of carving out this
variation free energy functional and so just by rearranging the terms I can also read it
as a mixture of accuracy and complexity so essentially the free energy is accuracy minus
the complexity where the complexity here scores the difference between my Bayesian beliefs
after seeing some data and my prior beliefs so literally the degrees of freedom or the amount
to which I have changed my mind in the face of some data and that complexity term plays an
important role in many guys is it's just an expression of Occam's principle in the brain at
least the things like factorizations and functional specializations a simpler way
of explaining or making sense of data with minimum redundancy and maximum efficiency
if you're an engineer it also implies that you're aspiring to provide an accurate account of
observable data with the minimum computational cost leading to notions of bounded rationality
literally based upon an evidence bound known in my world as approximate Bayesian inference
that rests upon priors that sometimes people refer to as heuristics and of course one can appeal now
to the physics of information via the Zhazinsky equality and Landau's principle which means that
minimizing computational cost also implies a thermodynamic efficiency that in principle
the best way of accounting for data or making sense of data should be that way that uses up
the least energy and is performed in the minimum amount of time so the basic story I want to tell
in this short presentation rests upon thinking about well that's fine for understanding
senting behavior and a principle for sort of optimal action in the moment what would it mean
if I considered devices artifacts or creatures that can plan into the future and have a generative
model of the consequences of their action and what would this self-evidence imperative mean
for plans and the kinds of behavior we'd expect to see under that imperative
and the answer is or one answer at least is very simple because now I am
trying to maximize or optimize a free energy bound on evidence but in the absence of actual
outcomes the outcomes become a random variable and I can simply take the expectation of this
evidence bound under the predictive distribution of the outcomes that would ensue if I did that
and if we follow that through what we're going to see is that basically we can explain
the best kinds of plans the self-evidenceing plans as fulfilling two imperatives one that of
Bayes optimal design experimental design in terms of maximizing expected accuracy
whilst making Bayes optimal decisions by minimizing risk by minimizing the expected complexity
and I'm going to basically rehearse the observational suggestion that minimizing expected
sorry maximizing expected free energy is basically a mixture of these two things
just to make this a bit more heuristic what I would normally do to your psychology audiences
ask them to imagine that they are an owl and that you're hungry and then I'd normally ask a
member of the audience what are you going to do I won't do that now but I'm hoping that you're
all thinking well I'm going to go and look for food indeed that is the absolutely the correct
answer but in answering like that what you have said is something quite fundamental and I'm going
to try and unpack how important that answer is just by comparing and contrasting two ways of writing
down normative or objective functions for good behavior and I'm actually going to repair these
two perspectives in a few slides but I'm going to deliberately sort of introduce and celebrate this
this distinction or this dialectic first of all I could assume that if I make a move on the world
I implement a particular action you then there are going to be certain states of the world that
ensue and I may have a value function over those states and if I had a value function of states
then there exists a policy that is defined in terms of the best action to take from any given
state namely that which maximizes the value of the ensuing state however there is another way
of formulating good behavior which speaks to the answer I'm going to look for food because
looking and searching can be construed as a way of minimizing uncertainty in this instance where
the prey is an uncertainty is an attribute of a probability distribution I'm going to refer
that as a Bayesian belief or conditional probability distribution so that tells us something really
important it says that the best behaviors the best actions the best plans cannot be functions
of states of the world they have to be functionals of beliefs about states of the world
and that answer looking for food also tells you something else because it matters whether I look
for my prey and then I eat it or whether I try to eat it and then look for my prey so that
introduces a notion of sequential policies or sequential actions that have to be optimized
in relation to this functional of beliefs about states of the world and I'm going to frame that
aspect of optimizing a trajectory or a sequence of plans in terms of a principle of least action
where action here is just the path of the time integral of a functional beliefs and indeed it's
going to be the expected free energy and hence the time the path of the time integral of the
free energy is an action so what we have at hand here are two very different ways of thinking about
a good way to exchange with our world on the one hand you've got Bellman's optimality principle
that rests upon a value function of states and many of you will be familiar with that in terms
of optimal control theory, dynamic programming, deep reinforcement learning, Bayesian decision
theory and so on but there is another way of thinking about it and that's possibly
slightly simpler appeals to Hamilton's principle of stationary action or leased action and that's
really the principle that the free energy principle inherits from. We'll also see that this
is very closely aligned with notions of artificial curiosity and intrinsic motivation
in robotics, the principles behind acquiring the right kind of data, optimal Bayesian design
and sequential policy optimization. So I'm just going to take you through the functional
form of this objective function one more time because I think its simplicity is compelling
and it speaks exactly to a variational or an appeal to Hamilton's principle of leased action as
possibly another way of thinking about behavior or at least sentient behavior. So this is the
basic setup that we use to understand and simulate and indeed model behavior and choices that are
informed by observations. We have some observations that come in, they are used to optimize our
Bayesian beliefs about hidden states of the world behind the observations generating those
observations and we do that by finding the most accurate account that's as simple as possible
of those data and then we take those beliefs about hidden states or latent states of the world
and then we use them to evaluate the expected free energy given a particular policy
and in so doing we are effectively taking the expected complexity and the expected accuracy
conditioned upon a way of moving forward and that equips every way of moving forward, every policy
with a score and expected free energy that can be associated with a long probability of me doing
that and then I can just apply a softmax operator, find the most likely plan, select my next action,
change the outside world, that would generate new data and so the cycle continues. So that's the
basic idea, it is exactly the same equations that I've unpacked as previously just to reiterate
that we can either regard this free energy function on this evidence bound, excuse me,
as a mixture of complexity and accuracy or we can just rearrange it to express it as a bound
on log evidence as you do say in machine learning or deep learning with things like
variation autoencoders and the reason I've done it like that is just to see what happens to these
terms when we take the expectation to form the expected free energy and what happens is that
the complexity becomes a risk, the inaccuracy becomes ambiguity, the evidence bound becomes
something known in economics and robotics as intrinsic value or motivation and the log evidence
becomes an extrinsic value that we'll see in a moment can be associated with things like
expected utility. So what licenses my interpretation of these expected terms? Well
let me just go through and unpack this expected free energy by looking at a number of different
special cases. Let's pretend that my prior beliefs about outcomes given I am me encode my
preferences, my prior preferences about the sort of outcomes I expect to encounter following all
actions and let's assume I'm agnostic about what happens. I have no particular preferences for this
of that outcome. That just leaves this quantity here which is the expected evidence bound and
this quantity is just the mutual information or expecting mutual information between the causes
and the consequences under a particular plan of soliciting those outcomes there.
In neurobiology, particularly in the visual search literature where people try to work out
where am I going to look next? What motivates that visual palpation of the world? The motivation
is just the intrinsic value of resolving uncertainty called Bayesian surprise. More I
think revealingly is just the degree of belief updating or information gain afforded some outcome
in other words the difference or the KL divergence between my Bayesian beliefs about hidden states
if I were able to see the outcomes that ensued following a policy compared to my beliefs before
looking at those outcomes again speaking to the difference between a post-era and a prior but now
in the future. So it's an expected divergence or belief updating or information gain
