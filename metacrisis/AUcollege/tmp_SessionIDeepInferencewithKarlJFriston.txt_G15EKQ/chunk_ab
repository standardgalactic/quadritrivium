that is intrinsically valuable irrespective of what you think all will prefer to happen.
Let me now take another source of uncertainty off the table namely the ambiguity. I'm going to make
a simplifying assumption that I can see all the hidden states of the world that my sensations,
my sensory observables are a direct mapping of the states of the world that generated them
and in this instance the S's become the O's and we're only left this ambiguity term disappears
and we're just left with this quantity here which is another KL divergence or relative entropy here
namely the difference between my anticipated outcomes or states given a particular policy
and my preferred outcomes, my prior preferences. So all we're saying here is that the actions that
have the optimal expected free energy are just those in the absence of any ambiguity that minimize
risk as scored by the difference between what I anticipate will happen and what a prior I prefer
to happen. And if I make a final move which is to take the last kind of uncertainty out of the
game namely the uncertainty that depends about states of the world that depends upon my action
that I'm just left with this term here the extrinsic value that if you remember we're
associating with utility in economics or negative loss functions in optimal control theory which
just means we're trying to optimize the expected log probability of our preferred outcomes or
expected value here. So what we have at hand is a decomposition that applies to
policies when scored in terms of an expected free energy functional
that can be unpacked or carved into an expected value that underwrites base optimal decisions
but importantly this epistemic term this information gain that is formally identical
to the principles that underlie base optimal design and things like active learning with unknown
parameters. So in short the expected value plus the information gain is literally equal to the
expected free energy. I slipped this slide in because I thought some of you might like to
think that in terms of information diagrams but given the time constraints and the opportunity
for discussion I'm not going to go into that it's just an interesting game that you can play in
terms of thinking about an information theoretic partition in terms of basic design information
bottlenecks empowerment and other takes on the right kinds of information gathering behavior.
I want to whip through now a simulation because you know we want to leave as much time for
any discussion that sort of exemplifies how these mechanics would unfold in empirical behavior
in my world. Here looking at sort of small animals or rats making decisions what we normally do is
build a generator model based upon a Markov decision process of discrete states that generate
outcomes parameterized by a likelihood mapping A and then we make the transitions over time
among the hidden states depend upon some policy depending upon probability transition matrices
where the policy itself rests upon my prior preferences on some cost function and then I can
equip this model with a few initial priors and hyper parameters and I've now got a completely
fairly complete description of any kind of behavior that can be written down in terms of
discrete updates of hidden or latent states that generate observables. I can then apply
standard variational techniques to optimize my variational and expected free energies
essentially using off the shelf technology that rests upon defining this approximate
posterior just for my interest. When one takes these off the shelf belief updating basic belief
updating variational message passing or belief propagation schemes they look remarkably similar
to the way that neuroscientists understand message passing and belief updating in the
in the brain in terms of perceptions updating beliefs about expected states of the world
policy selection usually associated with the striatum or integration between parts of the
surface of the brain and the striatum that rests upon this expected free energy and indeed
some hyper priors here or hyper precision parameters reflecting the confidence or
the uncertainty that I afford my beliefs about what I'm going to do next we can do learning
and then we just select the action on the basis of these updated expectations about policies.
So when we do that and I'll just close with an example of a synthetic rat doing this
and so this little paradigm here involves a rat in a teammate's it can go and secure a reward
that's either on the right or the left of the two upper arms it could only make two moves
but it's got the opportunity to go to the lower arm and get an instructional cue that tells it
that the cue the reward is on the left or on the right hand side so it's got a choice it can
either take a gamble and go to the one of the two upper arms and be right or wrong and it has to
stay there so these are absorbing states when it gets to the baited areas or it can waste one of
its moves by finding out exactly where the reward is and then go and secure its reward with a high
degree of uncertainty so from the point of view of expected utility these are fairly balanced
options but clearly from the point of view of this self-evidencing in the future
the information gain the intrinsic value of going to resolve your uncertainty or this rat's
uncertainty about the states of affairs is going to be more attractive and that's going to
compel the rat to be exploratory and go and look at the cue get that and then go and get its reward
and indeed that is what it does so I'm just summarizing the behavior that
follows from integrating those belief propagation or variational message passing
schemes under this geometry model but with a twist after the first couple of trials we leave
the reward here so that in principle the rat is going to start to learn that the reward is always
here and indeed it does that and of course as it becomes more and more confident that the reward
is here the intrinsic value the epistemic value the information gain afforded by going down here
gets smaller and smaller and smaller and wanes and once it passes a threshold where the expected
utility of going straight to the reward and staying there for an extra move and supervenes
then its behavior switches deterministically and it switches from an expiration to an exploitation
mode and that's exactly what happens in this example and after about the 20th trial here
and all this depends upon accumulating knowledge about the contingencies named the context where
on which side is the reward most likely to be over successive exposures that then couples back into
this online active inference planning as inference underwritten by this expected free energy this
is the last slide I'm putting in this in just to remind myself that there's an outstanding
challenge here I've sort of sold this self-evidencing as something that has constructed it in relation
to lots of other ways of describing good behavior there may be a deeper back story to this that
relates to statistical physics and things like the integral fluctuation theorem but to connect or
complete that story it requires one small move which I'm happy to talk about but not happy to waste
any more time I'm packing for you at the moment so I'll give the last word to Einstein that everything
should be made as simple as possible but not simpler so it just remains me for me to thank those people
whose ideas I've been talking about and of course thank you for your attention thank you very much
indeed terrific Carl thank you thank you so much for that really engaging talk and a great
way to start off our workshop I realized I actually forgot to mention to everybody else who's here
that what the format was going to be so we did actually a plan for some ample time for questions
and for discussion I think as we know as people who have been to our workshops before know that's
one of the hallmarks of our workshops is quite animated discussion and so thank you Carl also
for leaving ample time for that so I want to open the floor up to discussions and questions maybe
if you want to just drop a note into the chat to me just to tell me of a question so that we can
organize or line up the questions but I think the first question I know almost has
a few questions so we'll go to almost first but just drop me a note as I said if you have questions
to ask thank you Carl first it's nice to see you again I really enjoyed your talk and the idea
that you were able in such a simple way to put decision theory within information
theoretic framework and quantities mostly the KL divergence seem to be very interesting very nice
I myself even though I work on this area for many years I never really saw it in this way so
thank you for this I have a simple question though it could be outside the scope here
in in our previous time I've seen you you were talking about Marco Blankett and as as you were
passing through in one of the slides I saw that you put your framework within a Marco Blankett
maybe I missed it and it may be outside what you wanted to talk today but if it's not I would be
happy to hear a little bit more how this fits within your theory of of our Marco Blankett
all right that's a very astute question yes I tried to slip past the Marco Blankett because
he would take another half hour to motivate that but that's an excellent question because it
it speaks to that last slide where one's trying to find a deeper backstory in statistical physics
to why this kind of self-evidence behavior has to be there if any system manages to maintain itself
in some kind of steady state you know if it's in computational chemistry this will be self-assembly
if it's in biology be auto-polysis if it's in social neuroscience it's just basically maintaining
cohesions and in groups and and that's where the Marco Blankett comes in so in order to talk about
a system you'd have to actually write down well what do you mean by a system that has some states
how are those states differentiated from the states that do not belong to the system so that
implies a set of conditional independences that as you noted are usually assigned to the Blankett
states of the Markov Blankett so what you say is well in order to differentiate a system from
the rest of the universe I will assume the existence of some states that when I condition
the internal states on they become conditionally independent of the external states and those
are the Blankett states and then you look at the the density dynamics that you have under that
particular partition of states into inside and outside separated by Blankett states that still
permits so the system's still open because you can the outside can vicariously influence the inside
and vice versa through the Blankett states but you write you then look at the the statistical
physics of that partition and that's where the integral fluctuation theorem kicks in which is
why I was trying to and I'm still trying to connect the optimum Bayesian design plus optimum
Bayesian decision theory formulation in terms of information theory and KL divergences
with the the KL divergences underwrite the integral fluctuation theorems so that that's I
think I mean it's a little bit indulgent but I you know it really does if you like join the dots
between the statistical physics and the more practical aspects of information gathering and
and exploitation. Thank you. Great thank you almost I think our next question is from Bill
Lawless I know Bill you put it in the chat but go ahead and unmute yourself and just ask it.
Thanks for reminding me to unmute. It was a really nice talk I liked it a lot. It seems to me to be
based on on the information that's available or that could become available and that leads me
to my question it seems that the only way that you can uncover something like deception
is when it uncovers itself. Maybe you could address that but I'd also like to find the link
between which I like Bayes and maximum entropy production maybe you could also address that
as well if you have time. Yes we don't have time to address it properly but we can certainly
celebrate the existence of those important issues and questions. The maximum entropy principle
both as it pertains in the sort of classical genes in sense but also as it pertains to
entropy production in this sort of dynamic self-assembly formulation in terms of the integral
fluctuation theorems and Markov blankets plays an incredibly important role and just
almost emerges for free from the free energy formalism so if you just decompose the complexity
you've basically got the entropy of your beliefs about the causes of the data and that has been
maximized as mandated by things like Occam's principle so that's part of this sort of optimal
Bayesian design formulation and has its roots in sort of Geoffrey's priors and James's interpretation
of statistical physics and so that is really at the heart of this belief-based or at least
conditional probability distribution or Bayesian belief-based formulation of optimal behavior.
The deception issue is fascinating because it speaks to
the rather delicate and challenging and very real issue of applying this mechanics to
dyadic or multi-agent games where now you have the potential of the generative model
that's sort of in this instance that Markov decision process being a model of another model
so it's somebody that I am in exchange with in a game theoretical in a sort of language sense
and just you know the very notion of trust and deception or indeed just the very notion of a
generative model being able to explain outcomes generated by another generative model is deeply
challenging and we're talking about you know some metacognition through to theory of mind here
so I think just things like deception and regret are if you like the you know the most challenging
kinds of information exchange or the most challenging aspects of exchange dyadic exchange
that you would aspire to try and explain in terms of you know Bayesian optimality principles
in this framework I'm not sure it's been done yet there have been baby steps in terms of putting
these sort of free energy minimizing agents together and what tends to happen is that they
tend to cooperate because they want to make themselves mutually predictable so if you're
trying to minimize the surprise or the self-information one way of doing that is just to make sure that
I am as much like you as possible under the prior assumption that you are doing the same
and then our entire exchange becomes as mutually as predictable as possible
and that optimizes then the the joint free energy that that we share did you have anything
deeper in terms of deception because my answer was a little bit superficial in the sense that
it hasn't quite been done yet
yes I do but you mentioned the word assembly which I find to be even more intriguing
one of the issues that I have with Von Naumann's assembly idea is that there's no way to tell
when the correct pieces from Von Naumann's model I don't know if that's what you're addressing
there's no way to know that the assembly is working in the right direction that you're
actually improving the situation with assembly maybe you could address that
not in a scholarly way because I don't know the the you have a notion of assembly in a von
Naumann sense so I'll have to put that on the list with the firefighter problem to go and google
afterwards assembly in any sense how do you know that you're assembling a team of it based on what
you've said you want to assemble it so that the surprise is minimized so one one way of addressing
that is well certainly through numerical analyses and that's been done in the context of
dynamic interactions but also an interesting dance between the environment and a phenotype or an
assembly ensemble of phenotypes in the context of e-conniche construction and the bottom line
is that the because the this variation free energy is an extensive quantity you can read the goodness
of the assembly or the ensemble of systems are simply the sum of each individual's free energy
so if every individual of an assemble ensemble is optimizing its free energy then collectively that
is also true and then that sort of brings us back to what I was saying before them you know that very
much means that everybody's trying to to learn about each other so that they can render everything
else or indeed the environment as predictable as possible so we build traffic lights and roads
and we have languages and signs and deontic cues or I learn your language while you learn mine
so that that's a sort of the first order emergent property of this kind of multi agent assembly
that's what you meant but I do know of course that that doesn't leave any room for deception
so now you have to think about in groups and out groups assemblies of this kind versus that kind
and how you might actually use deception to exploit this sort of first order cooperative
mutual predictability imperative that comes out self-evidencing with other things like me
I have a somewhat um maybe it's related question but so so as as you were presenting I was I was
jotting down a note I made to myself and this is related to to things that I'm interested in about
the relationship so so as an economist I'm quite interested in people's perceptions and how
sometimes you know when we observe behavior that we would often characterize as not rational
it actually is probably behavior that is rational conditional on a set of of beliefs that are possibly
not correct or perception that's possibly not correct and so I was wondering about the distinction
between perception and and what I would call something like learned experience and your
slide on functional anatomy and message passing seemed to discuss that and you seem to emphasize
that beliefs are about uncertainty and so I was wondering if there was anything that incorporates
uncertainty associated with the learning side so so there's some uncertainty related to the outcome
that results from repeatedly doing the same thing so you know I think of in a rat experiment
you can design it so that there's a constancy of the experience so that after 20 times the rat knows
where the food's going to be but and presumably you can design a rat experiment where there's
some uncertainty as to whether the food will be there but but I was wondering about that about
how how how you think about learned experience and and how an individual knows whether whether
it's their perception or whether it's actually a series of of of experiences or you know basically
