there's there's a dependence in there that I that I wasn't clear how to incorporate into into your
models right no no excellent question and that and that dependence is a really important part of
approximate Bayesian inference that rests upon a mean field approximation or a factorization
of beliefs about different kinds of things that when you actually do the belief updating and
necessarily you know repairs that dependency so that dependence is absolutely crucial and
central to any realization this this this Bayesian belief updating and in particular beliefs about
sort of time invariant parameters of a geratin model that encode laws and contingencies versus
quickly changing states in the moment so normally we talk about inference as basically
belief updating about latent states of the world while learning is a slower updating of the parameters
of a geratin model but you you bring up so many intriguing issues that I just want to mention a
couple of them bounded rationality seems to me that semantically and possibly rather cheekily
easily absorbed into the use of an evidence bound to move from exact Bayesian inference to bounded
Bayesian inference which sounds to me exactly like sort of bounded rational and you could even chase
that further with the complete class theorems that as you say for any given pair of loss functions
and behaviors there is a set of prior beliefs that render that behavior renders that behavior
pays optimal so there's a lovely duality there that all rests upon the prize that define the
generative model so that's that I think that's a really important point that you know we're all
different because we have different priors but we're all optimal at least given given those
priors practically you also brought the so the the inference and learning are both in the service
of minimizing this path integral of free energy that in an identical way but when when you unpack
the equations analytically you get this sort of fast belief updating that our neurons might do
for the inference part then you get this slow accumulation of evidence and belief updating
on connectivity that underwrites the parameters and when you start to do numerical analyses
something really simple but I think very telling happens that if you set off your I'm sorry I should
say that in practice what we generally do with discrete state space models is parameterize
the likelihood mappings and the prior usually transition matrices in terms of Dirichlet
parameters so you have to write down your initial Dirichlet counts now if you write them down very
very small the artifact then becomes very sensitive to evidence so that you know if you start off with
your two outcomes and you've got one Dirichlet parameter after just one experience one's going
to be sort of twice the other on the other hand if you start off with 200 Dirichlet counts and you
add one more you've got a ratio of 201 to 200 nothing has really changed so your prior beliefs that
encode an uncertainty about your beliefs about the contingencies that you're learning so if you come
in as a sort of young artifact that's just been born with very low Dirichlet counts you're going to be
very sensitive to any evidence that is accumulated through repeated exposure but if you're an old
wise recalcitrant rat you're not going to learn anything and then it's interesting when you come
back to the previous discussion you know when you put two of these agents together that are trying
to predict each other if you put the old one with lots of Dirichlet counts and interacting with the
baby one with very small Dirichlet counts then the small one learns a lot more from the old one
so it's a nice metaphor for you know for teachers and parents and they're like
sorry that's that's quite interesting I mean I can imagine a situation though where there's
I mean in some ways that you know young one and how it acquires experience is in some ways I think
of as an initial condition but over time you can imagine something where there's like you know
some limited memory or memory decay so that in some steady state there's you know it's not
the marginal additional piece of information has the same effect on all the different agents
now I don't want to take up all the time though because I know Duncan Foley also has a question so
I'm going to hand it over to Duncan Foley as well for his question well thank you for your presentation
that I'm particularly intrigued by the ability of your ability to bring out the parallels to
thermodynamic concepts in this way of partitioning the free energy relationships that are implicit
in the entropy reasoning I wanted to just get your thinking on a slightly different topic which
is what I would call novelty because I'm aware that in some models of learning an important
role is played by the ability of the system to create new categories or new states or
uh not just to update in the in the typical Bayesian way but to innovate in some way or
to accommodate novelty um and often there's a trade-off between the organism's willingness
to view something as a novelty and start treating it as something different or trying to accommodate
within the framework that's already been established by experience I just wondered
what your thinking is on how that relates to this general perspective of Bayesian updating learning
yeah I think it relates intimately and actually provides a sort of superordinate context and
interestingly actually speaks to Robin's last question which I would sort of frame in terms
of the problem of structure learning and when do you build an extra bit of your model to account for
this new piece of data or indeed remember that there's also this imperative to keep the model as
simple as possible so yes you have to have that optimal forgetting you have to have the loss of
redundant connections you have to you have to forget stuff so it introduces this really delicate
issue when is it optimal to believe update or indeed change the structure say the new number
of hierarchical levels in a deep network or add another level to some hidden hidden factor
so from the point of view of optimizing the free energy bound or model evidence this just is
Bayesian model selection so structural learning just is Bayesian model selection under some
prior beliefs about the volatility of the environment and there will be an answer to your
question should I add an extra level of say hierarchical depth to my model or not in light of
these data so you know if you had all the time in the world then what you would do is you'd
basically evaluate the marginal likelihood or the evidence bound with and without that extra level
and if the evidence the marginal likelihood increased you've retained that extra level
and if it decreased then then you then you would you lose the extra level and I frame it like that
because there are some people who think that that's why we go to sleep that we rehearse all the data
that we've accumulated during the day and effectively whilst doing things like dreaming
evaluate its capacity to explain those kinds of data and then identify those redundant parts
in the modern structure and remove those connections and simplify and do do housekeeping of a structural
sort so again it's speaking to this sort of optimal forgetting that Robin was alluding to
and I suspect your the thrust of your question which is which is you know when do I update
when I don't update and that would be one part another really important part of your question
there was a notion of novelty and novelty emerges in this scheme in a very simple way
it's simply the information gain or that intrinsic value not about hidden states in the world but
about the parameters that we've just been talking about so if in my evaluation of the expected
expected free energy of a particular move say looking over there for example or doing this or
opening that door there is certainly a an information gain an epistemic value or affordance
relating to the resolution of uncertainty about what's behind the door but there's also
an epistemic value or intrinsic value in terms of what would happen if I do that generally and
that's basically scored by the KL divergence between the posterior beliefs encoded in by
those Dirichlet parameters or the likelihood mappings or the priors before and after making
making that move it's actually very very simple to evaluate so that you've now got sort of two
kinds of information intrinsic value you've got the kind that scores the reduction of uncertainty
the information gain attributable to Bayesian beliefs over hidden states and the other flavor
which we can refer to say salience and the other kind that has mathematically exactly the same
form but it's a reduction a relative entropy or a reduction of uncertainty of beliefs about
parameters that you can think of as novelty so that hasn't been explored very much but it
becomes absolutely crucial when you start to try and explain sort of higher end executive
functions in cognitive neuroscience like things like sort of our hard moments or insights that
require you to go and learn something about the causal structure the regularities that endure
over multiple instances of a particular trial or a particular sort of sensory experience
so in my world in the active inference world that is starting to attract a lot of young people in
terms of how they want to simulate and write that down. Thank you. Great now I noticed Kenrick
Nelson posted something in the chat Kenrick do you want to do you want to mention it or is it
fine that it's just in the chat it's not really a question. I can just explain it a little bit
and then if there's interest people can follow up but you know as people are asking questions about
how you model deception in degrees of belief in updating data one of the things that came to
mind is there's some really interesting work by Fleming Topsoid who looks at these information
theoretic models and game theory and complexity and he uses generalized entropies to model your
willingness to update your beliefs and in particular you can parameterize it where
the control parameter becomes a your kind of perception of risk or your perception of complexity
and then you can have different players react differently to you know information games where
one player might you know just accept reality and thereby you know when they get new data they just
accept that data and update whereas another player might be skeptical about reality
and and be less willing to change their beliefs given new data so it's a different way of
modifying these models to account for biases and how people accept or reject data.
And I think very important if you take these ideas into the clinical realm in terms of
psychoanthropology and psychiatry you know that are thinking about things like delusions and
hallucinations or agnosias where people have very different interpretations about what's causing
their sensations and therefore very different weights to the sensory evidence at hand versus
their their priorities so I think that's a really important perspective just technically I don't
know this formulation but it sounds as if you would reproduce that mechanics by hyperparameterizing
the precision of various beliefs so you know if you want to make a rat more exploitative as opposed
to explorative you can certainly increase the precision of its prior preferences so that it's
the kind of rat that very definitively likes these kinds of outcomes as opposed to those kinds of
outcomes and in a similar way you can you can parameterize the precision of likelihood mapping
so if a likelihood mapping is very imprecise you're going to ignore a lot of data because you know
that the predictability of that data or its precision is low but on the other hand if you
assign your likelihood mappings undue precision you will be enslaved and your belief updating will
be very susceptible to any data and you may well start to overfit that data in relation to your
prior beliefs so I don't know but that it sounds as if that's the the the set of questions about
you know the relative weighting of different sources of information and the relative weighting
of sensory likelihood and you know relative to priors that are being addressed and just reiterate
that sounds that sounds important because it's in computational psychiatry that is usually
the seat of pathology that explains a lot of false inference and deep updating in psychiatric
conditions yeah yeah great so so everybody I think it's time especially I'm cognizant of the
fact that in this virtual world we have to keep on schedule so I'm gonna close this here but I just
want to first of all I want to thank Carl for an excellent presentation I want to thank everybody
else for the great discussion and just like in a regular you know sort of real world or in-person
conference I mean Bill's already posted a follow-up comment that that I think you know hopefully
that I find very interesting so hopefully we can take that into the break we can continue this
discussion in the break but I just want to close the session now so that people can take
10 minutes and do whatever they can do that but but again thank you so much Carl and hopefully we
can continue to have discussions into the break thanks very much Carl if you're still there I
think I think Bill had an interesting question and that it might be worth the having some discussion
about right sorry yes yes I was just about to get a cup of coffee so I'm sorry no no coffee for
presenters I'm sorry about that ah yeah this is the quantum biology quantum models of biology
wait Bill just Bill just ask your follow-up question yeah it's the owl problem the owl
in your view waits for prey to uncover itself but owls are also built in with the intelligence
that prey hide and so it's got to go into the business of uncovering a deception
Volkswagen had an extraordinary deception how do we uncover these deceptions ahead of time
that's the question I see that reminds me the stag hunt paradigm of you this is cooperation
versus competition in terms of yeah and yeah I don't have an informed answer other than to
say that you know that this is this is sort of high-end generative modelling in either a game
theoretic or game theory sense or in in a social neuroscience sense and the trick is to
build the sophistication of the generative models so that they can entertain or represent the
possibility that the the other agent knows about the you know their intentions and that requires
quite sophisticated quite sophisticated generative models and then one gets I presume into the same
realm of sophistication that is found in economics you know to what extent do I
represent your representations of me and then to what extent do I represent your representations
of you representing me representing you you know a lot level of recursion does you know
do you find that sort of that optimum in terms of this sort of boundary in the accuracy and the
complexity I don't know of any of any formal work within the sort of the active inference scheme
but I do know that there's a chap called Jean Daniseau who has looked at evolutionary stable
strategies and using sort of levels of recursions of beliefs in terms of diatic interactions
and actually found something quite quite it wasn't quite so much the the the prey camouflaging or
deceiving the predator but it was more about sort of populations of two different kinds
finding that evolutionary stable strategy as quantified by a minimum or an optimum
expected free energy and what he found was that the evolutionary stable strategy
was half the people had to be very unsophisticated and the other half had to be very sophisticated
and that was the only stable strategy otherwise you got this sort of leapfrogging
where you know the deceivers try to deceive the deceivers are deceiving the deceivers
but there was a stable strategy where there were basically sophisticated and non-sophisticated
agents out there I'm not sure it was framed in terms of deception though that I think this was more
just in terms of communication well thanks for your answer I'm sorry to have interrupted your
coffee so I got so I've got to do deception firefighter problems and what was the other thing
oh yes the the von Neumann assembly and these are things I have to go and do my homework on
