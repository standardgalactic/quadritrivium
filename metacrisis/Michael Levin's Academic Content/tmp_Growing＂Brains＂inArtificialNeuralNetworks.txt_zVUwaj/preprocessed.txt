I'm Xeming Liu. I'm from MIT and IFI and advised by Professor Max Targmark,
working in the intersection of AI and physics. Previously, although recently Max wishes the
research focused to safety, mechanistic interpretability, but there's still a lot of
overlap, right? We care in general science, like interpretability for science,
but maybe not only science, but recently also about large language models. Anyway,
so today I'm going to talk about growing brains in artificial neural networks,
which covers two of our recent work. Brain group is Brain for AI, where we develop a brain-inspired
method for AI interpretability. The second work, basically using the tool we built in the first
paper to encourage modularity, encourage AI for brain. Without further ado, let's jump right in.
Our first motivation for this line of work is interpretability. We know that neural networks
are extremely powerful machines, but they are notoriously difficult to be interpreted.
We want to, in order to keep them safe, we want to gain some interpretation of them,
but there are many levels of interpretation. There are neural level interpretation where
we want to understand what each neuron is doing. The other extreme is this model level
interpretation, where you just think of neural network as a oracle with this input-output pair.
There is this intermediate level, which we call the module level interpretability we think is the
most promising level. The reason why is that if you think of the trade-off between the complexity
of some of the interpretation, inaccuracy of the interpretation, the model level interpretation
is too inaccurate or informative, not informative, because it's not telling you what's going on inside
the black box. On the other hand, the neural level is maybe too subtle, too complicated,
because you need to explain what each neuron is doing. The module level hopes that you can
disentangle a network into different parts, and different parts responsible for some
certain human interpretable function. These modules are specially connected, again, in an
interpretable way. That's the hope, that's the plan, that's the hope with this module level
interpretability. It's not totally wild dreams, because in vision systems and language systems,
at least for some particular simple toy tasks, people have discovered such modularity can emerge
or networks. For computer vision tasks, people discovered that there are these curved detectors
and for language tasks, for this what they call indirect object identification tasks,
they find that in GPT-2, they can identify this nice circuit, which is quite aligned with our
concepts, how humans would do such tasks. If we believe in this module level interpretation,
how do we go about to make this interpretation even easier? I would argue that locality can make
things much easier, in the sense that if you have a neural network with no locality, well,
actually for common neural networks, there's no sense of locality. If you take a fully connected
neural networks, the neurons, they are all permutable, they have this permutation symmetry,
and there's no sense of spatial coordinates in it. For a neural network with no locality,
then to describe a network, to describe the network in terms of module, in the language
of module, you need to describe, you need to tell which each neuron is belonging to which
module, like the first neuron belongs to the red module, the second neuron belongs to the blue
module, so the description length, so to speak, of the description is proportional to the number
of neurons, because you need to assign a module label to each neuron, and it's also not clear how
to do that, but on the other hand, if you have locality in your model, basically, if one neuron
is a red neuron, then very likely its neighbor will also be a red neuron. It's very much like in
our human body, or the cerebral cortex, where, well, each local, like the localized regions
responds for certain functions, so if we have locality in your network, then we only need to,
the only thing we need to do is draw boundaries between these modules, like in this case,
if the red and blue modules, they are nicely separated, then all you need to do is to
have a knife cut through these two, and realize that the left part is red, and the right part is
blue, and then the description length is basically just on the order of depth, because you need only
to indicate where you want to cut through these two modules. So, can I ask a quick question?
Yeah, sure. Yeah, hi. So, when you say locality, you talk about in terms of function, right? Like
in your example, if one neuron is performing a contributing certain function, its neighboring
neuron is also likely contributing to the same function. Does that also imply locality in terms
of structure, meaning that in these neural networks, does it also require sort of local
connections, like topographic connections, or even with all to all connections, could you expect
locality? Yeah, that's a good question. So, maybe I need to make explicit distinction between
functional modularity and anatomical modularity. So, functional modularity means that two neurons
perform similar, like both responsible for some certain function, that's function modularity,
and anatomical modularity meaning that the neuronal connections are local. So, by here,
I hadn't gotten into details. So, here, maybe I meant both. You can think of it as both,
or some even abstract sense of modularity, but I will get into some detail after this. Yeah.
Sure. So, in this slide, I argue that locality is a very, is a simplifying property
to achieve module-level interpretation. So, we already know that our brains, at least in
the cerebral cortex, localize regions, like functional modular regions are localized
in space, but do our artificial neural networks have the same
property? And I argue that no, because there's a key difference between brains and
artificial neural networks, in that, for our brains, there's a selective advantage for modular
brains. So, for modular brains, I mean that relevant neurons, right, so functional modular
neurons, they're also anatomically modular in space, they're put close to space. So, the connection
between, so the average, so the connections between these neurons are relatively shorter,
hence the connection, the communication speed, the time delay, also shorter if you have a modular
brain. As a result, human beings with modular brains tend to react faster than people who don't
have modular brains. Hence, modular brains are more likely to survive, hence have a
selective or evolutionary advantage over non-modular brains.
However, artificial neural networks do not have such survival advantages, because
if you think about it, the training objective is just to make prediction as accurately as possible.
Well, you may think, you may argue, one may argue that there might be implicit
regularizations that may encourage modularity, like a dropout, like all kinds of regularization,
but they're just encourage modularity in an implicit, unclear way. So, in general, we do not
expect artificial neural networks to have modularity, because there's no survival
advantage for artificial neural networks to become modular. So, inspired by this,
I argue that we need to explicitly introduce some training techniques to explicitly induce
modularity in otherwise non-modular networks. Say, you take a fully connected neural network,
which by default are not modular. All the neurons in some consecutive layers, they're densely
connected, but we want to train the network in some strange or smart ways so that the modularity
can emerge after training. So, the key here is actually quite simple. I will spell the word
again. It's just the locality. So, in this paper, we propose the method called brain-spired
modular training. Although it has a fancy word called brain-spired, it's actually just introducing
locality, a sense of locality into neural networks. So, previously,
neurons in artificial neural networks do not have spatial coordinates associated to each neuron.
But here, we assign a two-dimensional coordinate, x and y, to each neuron and embed the whole neural
network into a two-dimensional Euclidean space. In general, it cannot only be
to the Euclidean space. It can be any arbitrary geometric space where you can define coordinates
and distances. But for simplicity, we can embed them into this two-dimensional space.
In bed with this metric, we can compute distances between neurons. In this sense, we can define
length of the weight connection, not only the magnitude of the weight, but also the length
of the connection. With this picture in mind, we penalized previously to encourage a network
to be sparse. People usually use the L1 regularization, which is basically lambda,
is the penalizing strength times the magnitude of the weight. But here, we basically take the L1
regularization, but it also be multiplied by the length of the connection, meaning that if the
connection is longer, then it's penalized more strongly than a connection that is shorter.
So this objective bias towards a solution that has shorter connections.
But actually, this doesn't work if you just penalize, if you just naively add this connection
cost term into your training because your network can still very easily get stuck at some
long connection configuration. A fourth experiment is that if you start off from a densely connected
network, which have many long-range connections, but it already achieved the task perfectly well,
then there is no incentive for it to simplify. So to avoid that, we also do this kind of discreet
optimization we call the neuron swapping. So suppose we have a neural network like this,
the red neurons, they come from a module, they're strongly connected, and the blue neurons,
they're another module and they're strongly connected. If we find that swapping, the two neurons,
the blue neuron and the red neuron in the middle layer can reduce the connection cost,
then we do the swapping. So basically, our method consists of two, has two gradients.
One ingredient is we have this differentiable penalizing term which encourage local connection.
Another ingredient, another ingredient is we allow neurons to swap. If swapping the two,
if swapping the two neurons can reduce, can reduce the connection cost. So we do these two steps
iteratively, like we train for a while, we do swapping and we then train for a while and then
swapping until it converges to something or like we meet the compute budget.
So an example of this, so let's consider a very simple example which is basically
regress polynomial function with a neural network. So the input here has four variables,
x1 to x4, and there are two output variables which basically just the second order polynomials
of these four variables, but it has some interesting structure like the first variable
and the first output is x1, x4 plus x2, x3 and the second output is x1, x4 minus x2, x3.
So we might imagine that the way we compute this is basically compute x1, x4 first, compute x2,
x3 first, and then linearly combine these two things to obtain the final output.
And so it's not surprising to find that with a densely connected network,
although your performance is quite good, it's still densely connected with revealing no obvious
structure from this dataset. If you use the Lina L1 regularization, you'll get a sparse network,
but it's still not clear what the structure is. But if we jump right to the rightmost
figure with our BIMP method, which involves both L1 and local, it's a local L1 and involves swapping,
then you see that something remarkably sparse and modular emerge out of it, and although I
don't have a plot here, we can check that the two neurons in the second to last layer are exactly
x1, x4, and x2, x3 up to some rescaling and translation. So by just looking at the plot,
you can immediately see some nice structure out of it. And without too much sacrifice in loss,
although they're like 10 times, around 10 times, or even 30 times, it becomes worse in terms of
MSC, but that's acceptable. The point is, here we obtained something interpretable with our method.
You can directly see the modules. These two modules in the first two layers, they don't
talk to each other. And it's clearly, at least the dependence, how these variables are dependent
on each other and how the variables are combined to get the final output, the computational graph
is clear with our method. Otherwise, you cannot get with other methods.
Can I ask a question? Yeah, sure. Like in the fourth figure, which is just L1 plus
SWAP. In theory, the SWAP operation shouldn't really do anything in this case for the actual
training procedure, am I right? You mean the subfigure D? Yeah. So here it looks like the
local part does not help that much, but it still refines a little bit. Yeah, they are very similar
in this example. Right. But if you look closely, maybe there's some
long range connections here. Yeah, I won't argue that the local part is necessary. I would say
it's a big example, dependent. Okay. Maybe like in this image, I don't know if there are very faint
lines in between. You mentioned back from the stream, I can't see those lines at all.
Yeah, it's also mentioned like for subfigure D, it's also like no one has proposed it before. It's
just a special case of R1. Okay, thanks. I guess the local part and SWAP part is for the same
objective, but maybe the local part sometimes it's non-differentiable, so you need SWAP,
and SWAP is not very accurate. So it's kind of balanced. Right, it's a kind of trade-off.
Like the thin lines, well, yeah. Yeah, it's a bit hard to analyze, but I agree with the picture.
Yeah. Can I ask another question in the previous figure? Yeah, sure. In subfigure E,
there are just two neurons in the second layer, right? Yeah. Do they by any chance
represent X1, X4, and X2, X3? Did you check that by any chance? Yeah, I checked that. They're exactly
X1, X4, X2, X3, up to some constant scaling. Yeah. And I was asking a question on how
reliable is the training procedure there? So how often do you get exactly these results,
or how often do you get suboptimal in terms of number of connections, for instance?
That's a good question. So the structure are basically similar, but in terms of detail, like,
well, it's again depends on how difficult the example is. Like for this example,
it's quite robust, even in the slightest details. But for more complicated examples,
the details of the connection can be different. And yeah, to think of it, here you are
X1, X4 is multiplying two numbers, right? But if you do some construction,
actually, you only need two neurons to do the task. If you're using this
silo activation, so you only need two neurons to do the task. But I ended up,
but the method finds three neurons. So in some sense, it's still not the most efficient
solution that's possible. But it's something easier to be found for networks. So the robustness,
yeah, again, in general, I didn't test really hard about it. But the short answer is the structures
are similar. But the slightest details can be different depending on random seeds.
Oh, thanks.
Yeah, the network in figure E is almost exactly bilaterally symmetrical. Is that by design or
is it like a byproduct of some aspect of it? It's discovered by itself. It's something
I personally found amazing. It's just that this is the simplest possible thing,
and he just finds it. Very cool. Yeah.
Another question, did you benchmark this, for instance, with similar methods or
not similar methods, but other topology searching methods like Neat? Because these
methods also allow us to find very, very small networks which achieve the task perfectly.
Yeah, that's a good question. So after we finished this project, we were thinking
maybe combining Neat with this can be a nice choice because Neat is good at producing regular
patterns. And yeah, maybe combining the best of both models would be nice. But then I tried,
not exactly Neat, but just I have a hyper network producing the weights. And then it doesn't work
very well because, well, in the network I show here, you can see that the weights actually
will have high variations. Like red line means it's a positive weight and blue line means it's
a negative weight. So it's oscillating too fast. And I'm not sure if Neat can capture this.
But maybe adding some positional encoding, adding some high dimensional,
sorry, high frequency biases can help Neat to learn it, but I still didn't get around that
technical issue. So yeah, if you guys have any idea how to combine these two in a more
consistent way, I'm more than happy to chat. Yeah.
Thank you. I can think about it. It's really interesting. Yeah, wonderful. Yeah.
Yeah, so here are just some more examples. The first example is that the first
output variable only depend on X2 and X4. And the second variable only depend on X1 and X3.
So the network learns to automatically split the
network into two parts, which don't talk to each other. So by looking at the graph, you know,
you can immediately tell the independent structure. And the second example, again,
it's very interesting. And similar to the example I showed in the last slide,
there are three, you need to do the function, you need to do the squared function three times.
And these three motifs here, they look extremely similar. It's like, I did not impose anything
to say that I encourage repetitive patterns, but it's just something that it rediscovered.
It's just, the squared function is just rediscovered for three times.
And the last example highlights it can discover, excuse me,
a highlight is can discover the compositionality of a formula. So if you compute this,
you would first imagine you would compute the squares, sum of squares, and then take the square
root. And this is exactly what the network finds. So you see that in the second to last,
in this layer, there's only one active hidden neuron. And if we plot it,
and plot it against the outputs, then you see you nicely got a square root function.
So basically, the network learns to compute squares, sum of squares first, and then take
two layers to compute the square root. So this is again, aligned with our human intuition.
Like if we don't know the formula a priori, by looking at the graph, we can know that there's
this compositionality property. And we can also do this for modular addition.
The setup is that, well, three plus five is eight, modular 11, like for each,
that sounds an extremely simple task, but our setup is more like in the large language model
setup, where each which number is recognized as a token, which has a token embeddings embedded in
like 32 dimensions, which are trainable and randomly initialized at first. So the networks are
tasked to learn both the weights of the model, but also the embeddings of the tokens. So that task
actually is not that trivial, you need to learn the representation of the numbers.
And usually we find that your neural network would put the numbers around the circle,
just like on a clock, like when you read a clock, three plus five is eight, because
the addition of the angle of three plus addition of the angle five gives you the angle of eight.
So this algorithmic datasets are used a lot in the machine learning community to
study the mechanistic interpretability of neural networks. Here, I just take it as an example
to show that by applying BIMT to this dataset, we can have sparse and modular structures to emerge
that are much more interprisable if you don't use BIMT. Well, what's interesting here is that you
see that there are three independent trunks emergent. And for each, there are three heads. And
for each head, if you look at the representation of the numbers, they're put on different circles.
And the last one is even a 3D object, it looks like a bow tie. But if you project it onto
2D properly, it's again a circle. And what's nice about this is that there's no need to search
for directions. Because if you think about, like commonly wording bad things have this
rotational symmetry. But if you add sparsity and add this kind of to minimize this connection cost,
then there's symmetry breaking, there's no longer this rotational symmetry. And each
component, each dimension will be itself be meaningful. You don't need to explicitly search
for direction that is meaningful. And we did some ablation studies and find that if we knock out
any one of the head, the performance severely degrades, meaning that it's basically like
majority voting, maybe not majority voting, because the lodges are additively contributed,
additively aggregated in the last layer. But there's some kind of voting mechanism going on,
which is also quite cute, because we didn't impose this such mechanism, but neural networks just
discovers it by itself. Well, here are just some more algorithmic data sets. Here we do,
we compute the permutation group. It's just show the structure of the network,
trained with BIMT. What's surprising here is that we find there's a separate input neuron here,
and we plot it, we find that it corresponds to the parity of the group element, whether it's an
even permutation or it's an odd permutation. So it's precisely the parity of the group element,
which is also quite cute, because it's aligned with human intuition and the mathematical concept.
Well, so before, I only embed a network in a 2D Euclidean space, but we can go beyond that.
If we're dealing with image data, the image itself is 2D, and we have an extra dimension,
depth dimension, then it's better to, it's more natural to embed it,
to embed the network into 3D Euclidean space. So here's what we do with the MNIST data.
So the neurons, both the input neurons and the hidden neurons, they are arranged on a two-dimensional
grid, and their distances are computed in 3D Euclidean space. So we do a similar thing, train,
apply BIM to MNIST classification, and we end up with this beautifully looking image. Although
I have to confess that it's not that interpretable, maybe that's because it's difficult to explain
with words, with language, how we do visual processing. But the point here is that it's not
limited to 2D Euclidean space. It can be a very general method.
Can I ask you a question? Most computer vision algorithms use convolutional neural networks,
so they basically enforce locality by using a finite dimensional window. Do you see this
thing emerging from the way you train the model? It becomes more similar to a convolutional neural
network? Yeah, so we indeed look at the convolutional filters. Sorry, I don't have it in the slides here,
but it's in the appendix of the paper. So the convolutional filters learned in our network
actually have a difference than convolutional filters in CNN. In CNN, at first you learn some
low-level structures like edge detector, and then you gradually compose these
low-level features to get high-level features. But in our case, the first layer already captures
high-level features. Well, maybe this is simply because it starts from a densely connected
network. It has larger receptive field. But in CNN, the size of a filter is usually quite small,
so you're only constrained to look at local ridges. But yeah, the learned filters in our
network is quite different from CNN. And another difference is that CNN explicitly
have this sharing mechanism, but we don't have any sharing mechanism. So I don't know if it's
a feature or a bug of our method, but I would say maybe it's a feature. But I'm also thinking about
is it possible to marry CNN to our method to both enjoy the more flexibility as in our method,
because it's not necessarily look at local features, but also utilizing the sharing mechanism
so that you can compress a network more or more like human visual system.
Yeah, that makes sense. Thanks.
Maybe a follow-up question on that. How robust are your networks in these 2D examples with
respect to spatial transformations, like moving the image or scaling the digits?
Oh, I didn't do that. I just take the original MNIST dataset. Sorry.
Yeah, sure, because that's small as the power behind CNNs, right? So that would be interesting.
For this case, how important your single neurons are or how robust they are with respect to
fluctuations in the input. I mean, you have a lot of different examples in the dataset, of course.
To continue this question, according to this image that you show,
the network reduce a lot of robustness, meaning that every decision is based on
fewer and fewer neuron, which is less what you will come up with with the biology,
which has much more robustness. So I would think that the network is also sensitive to hijack,
you know, the time that you can modify the image a little bit and then you decide if
that image is more like one and not zero, only because you had a sticker on somewhere.
Yeah, I totally agree. I totally agree. So robustness. So I would say that robustness and
there's a trade-off between robustness and sparsity at least. If it's too sparse,
then you can just add noise to the sparse parts and screw the whole thing. But if
it's not sparse, then it's much more robust. So like our goal is to interpret something.
So our strategy is that, well, we don't think about robustness for a while, we just get
the smallest network that can do the task in a non-robust way, but still interpretable.
And maybe then if we want the algorithm to be robust and then we ensemble them in a clever way,
but each part of them we can have some understanding of.
Yeah, sure. So above, I basically talked about the brain for AI. And next, I'm going to talk a
bit about AI for brain, but probably more like brains by AI for brain. It's a little bit mouthful.
By brains by AI, I simply mean the BIMP method that we proposed. But before diving right into
neuroscience or brain, I think the method we proposed, BIMP, can be a quite general tool.
For science in general, like many scientific problems can be formulated as a regression,
just given independent variables x1, x2, to xd. And independent variable y, we want to find a
function, we only find a relation, right, basically write y as a function of x1, x2, to xd.
So previously, what how scientists did it is maybe come up with a symbolic form for it and have some,
you know, feasible, trainable coefficients. And fit that you have some measurements in experiments,
and you have you have a conjectured symbolic form for that. And then you feed your theory,
you feed your theoretical model to your experimental data by adjust that coefficients.
But it's quite not flexible, right, because you need to come up with those symbolic,
those symbolic formulas with educated gases, and sometimes you can meet something. But I argue
that I haven't had many examples, but I argue that such like, like our BIMP method can help you
gain some, at least gain some insight on how to formulate these symbolic terms. Or actually,
this, actually, a less, even, even more humble goal is not to come up with a symbolic formula, but
just tell you what's the structure behind the data set. So, yeah, and, and recently we are
basically, we are eager to have, we are like actively collecting data sets from all kinds of
fields, including fluid mechanics, biophysics, quantum materials, etc. And applying BIMP to
all kinds of data set. Basically, as long as your, as long as your problem can be formulated
as a regression problem, BIMP can give you the try. And may or may not it can discover
something, but there's no loss in trying it. So again, here's an advertisement. If you guys have
any data set that you figure that maybe there's some interesting structures, interesting modularity,
modular structures hidden in data set, then I'm more than happy to collaborate. You can just send
me the data set and I can run the code and return back to you. And because you are the domain experts,
you can tell me whether there's something interesting or it's a total mess. But anyway,
so I see a lot of potential in applying BIMP to scientific problems in general.
But recently, we have tried one thing, which is applying BIMP to, like, to, to neuroscience.
Specifically, we, we, like, traditionally, we have a recurrent neural network to do cognitive
tasks. So the setup is that your inputs, you have one dimension telling you that it's fixation
or not. And you also receive stimulus, you have to receive stimulus of two different modalities.
And each stimulus is just a ring telling you which angle it is. And you also have this so-called
rule input, which tells you which task you are performing, like there are 20 tasks or 84 tasks.
Basically, the rule input is a one-hot vector. If you are performing the first task, then the first
component is activated. If you're performing the second task, the second component is activated.
Okay. And in the hidden, in the hidden layer, it's a recurrent structure. It's connecting, like,
they're just hidden to hidden connections, just recursively going around in the, in the hidden
layer. And until you read out to get, to produce some output. So this is like a standard recurrent
neural network for the cognitive tasks. And what's interesting is that after you train an RN to do
this cognitive task, is there any structure coming out of, like, is there any structure hidden in
this hidden neurons, in this hidden-to-hidden connections? So, so this is already known, like,
four years ago, robot Guan Yu Yang here at MIT. He wrote this paper saying that finding that there
functional modularity in this trained recurrent neural networks by functional
neural modularity means that there are neurons which have similar functions,
and they form, and they form clusters in function space. So to make it more concrete,
if a neuron or equivalently, if a unit is a constant function for a task, then it has no
importance over the task, because it's just a constant function, it's very boring. But if that's
unit has high variance for a task, by that I mean, so a task contains many samples, contains many
time series, many examples. So the variance is measured on those samples belonging to the same
task. So if a unit has high variance for a task, then it's important for the task. So for each neuron,
you can measure its task variances for all the tasks to get a task variance vector.
And then, if two neurons have the same task variance vector, we have a sense that they perform
functionally similar. So basically, this boils down to a clustering problem in this task variance
space. Each neuron is basically a point in the task variance space, and we want to run
clustering algorithm for other neurons to see if there are any cluster structures. And indeed,
you can see there are some cluster structures, some clusters responsible for the goal task,
some for the decision making, etc. Right, but here's the question. So this results imply that
there's this functional modularity, but what about anatomical modularity? That is, if two functions,
if two neurons perform similar functions, are they necessarily close in space? Actually, for
standard neural networks, as I said, there's no notion of locality, there's no notion of spatial
coordinate. So it's impossible to talk about, even talk about anatomical modularity in standard
neural networks, but it's possible with BIMT, where each neuron is assigned a spatial coordinate.
So we ask this question, is it possible with BIMT to create co-emergence of functional and
anatomical modularity, just like in our cerebral cortex? So again, the key is to introduce locality
into the whole thing. What we did is we placed hidden neurons on a 2D grid. So each neuron has
two coordinates x and y. And in training, we regularize hidden-to-hidden connections in the
BIMT style. And also we allow hidden neurons to swap if this can bring down the connection costs.
And now, in the hidden layer, you see that the peripheral neurons, they don't have any connection
to other neurons, which meaning that they are not important, they are pruned away, or they're dead.
So we can basically focus on the active neurons in the center. So we already have the methods
to classify what each neuron belongs to functionally. So now for each neuron, we can compute
which function modules it belongs to and colored with different colors.
So yeah, you can see that's the red neurons that basically cluster in space, although not
that perfectly. Like they're still scattered around in the plot, but you can still see some
giant clusters of blue, red, and green. And by contrast, if you use the regular L1 regularization,
there's no notion of space in it. So it's not surprising to see that the neurons with
similar functions are scattered around in the plot. I have a question. So since you have the
space notion, then some operation of space like a string or resize can be applied. So I'm wondering,
is it possible to change the resolution of the 2D panel when you're training or after training?
Would it affect the performance?
By resolution, you mean maybe increase the speed from 10 by 10 to 20 to 20 by doing some
interpolation in the... Yeah, or decrease it. So you reduce the performance or make it better?
Oh, yeah, that's very interesting. Because also in physics, there's a concept called
renormalization group where you coarse grain to units which are close in space. Yeah, that's a
very interesting point. We haven't tried it, but it's worth trying. Yeah.
I'd also say I used the same idea, but instead of renormalization when they're close in space,
I renormalized when they are close by correlation or by dynamic, and it made some
interesting coarse grating as well. So it might be worth looking at here as well.
Yeah, yeah, yeah. That's a good suggestion. Are you saying that you did that or
there are people already done that before? Somebody has done it. They did it really simply
in a data set from mice, and then I used the method when a live real mouse was doing a task
and was able to do some really interesting analysis using the resulting coarse grain signals.
Oh, interesting. Is there any reference you can point me to because that sounds extremely
relevant? Yeah, let me pull it up. And then if you're interested, we can talk about it in a
different... I'll post a link in the chat, and then if it's something you're interested in,
we can meet in a different space and I can show you what I did with it because I changed some
of what they did. Yeah, cool. Yeah, definitely. Thank you. I also have one question. Have you
tried this experiment, but with just L1 regularization and the swap mechanism?
Oh, you mean L1 regularization? I see. So that's basically setting...
That's a special case of BIMT. So here, BIMT, right. Because in BIMT, you also enforce the
locality, right, right, penalizing the weights that are far away, but if you don't penalize this,
but you make the swap mechanism. If I recall correctly, the results are similar. So
actually, the brain plot I show here, the local parameter is actually very small,
meaning that it's close to no penalty at all. And the ablation study, we didn't do that too
rigorously, but the impression is that swapping is much more important than locality,
a differentiable locality. Yeah. Okay, interesting.
Yeah, sure. So how do we measure quantitatively the progress here? How modular this brain is? So
we propose two quantitative measures. The first measure is the fraction of isolated neurons. So
we define a neuron is isolated if none of its neighbors have the same color. That is, none of
its neighbors belongs to the same modules. So to get a sense of what the baseline looks like,
so we randomly permute these active neurons. We basically randomly permute these active neurons
to compute the ratio of isolated neurons for these randomly permutable configurations,
and we do this like 10,000 times to get a histogram. And then we indicate where
dimmed is, and it's significantly out of distribution, meaning that it's at least
not a randomly permuted configuration. It's very different from a randomly random configuration,
thus nullifying the null hypothesis. By contrast for L1 and fully connected network,
you see that the fraction of isolated neurons, they are still in distribution. This also makes
sense because they don't even have the notion of locality. So in this sense, they are by nature,
they are by default randomly permuted. But this is just a sanity check. And the second
quantitative measure is the average cluster size. If something is localized in space,
then it must have smaller cluster size than if it's spread over all of the place. So again,
the blue histogram is the random baseline where we randomly permute these neurons.
And again, we see that dimmed is significantly out of distribution of a random brain,
meaning that at least they are nonzero anatomical modularity in the brain that we obtained here.
So yeah, so I think that's everything I prepared. I'm happy to chat more if you guys have questions.
Yeah, I have a biological question, which is you started this out.
This is Chris, by the way. You started this out saying something along the lines of
brains work better when they have anatomical modularity. And if you look at humans or mammals,
then there's a lot of anatomical modularity. And there are also a lot of long distance
connections. In particular, three classes of long distance connections are the
thalamocortical connections, the frontal parietal connections. And then the connections
across the corpus callosum. And that suggests that one could actually test this somewhat
philosophical claim that you made by looking at variation in connectivity across the corpus
callosum and seeing whether people with less connectivity across the corpus callosum actually
work better cognitively. And we have the extreme case of split brain patients where the corpus has
been completely disconnected. And they're apparently normal on many tasks and severely
unusual on other tasks. And maybe Wes knows the answer to this question. I don't actually know
if connectivity across the corpus callosum has ever been correlated with cognitive performance
across some interesting battery of tests. But one would think that would be something that
would be possible. And it might be interesting to try to chase down biological data that would be
relevant to when is anatomical modularity actually useful for problem solving, as opposed to just
being useful for explainability, which I think your experiments demonstrate utility for explainability
very nicely. And the question is, is it really useful for problem solving? Yeah, thanks for the
great comment. I think one route we can pursue is that being produces local connections, but
there are still long connections in the brain making it more like a critical system. Maybe it's
a parallel correlation or something. So an intriguing question for me, for myself, is that
how can we modify the method? Or do we require a completely different method to reproduce this
parallel statistics in brain? Not just, yeah, this, yeah.
Thanks, nice talk.
