you would first imagine you would compute the squares, sum of squares, and then take the square
root. And this is exactly what the network finds. So you see that in the second to last,
in this layer, there's only one active hidden neuron. And if we plot it,
and plot it against the outputs, then you see you nicely got a square root function.
So basically, the network learns to compute squares, sum of squares first, and then take
two layers to compute the square root. So this is again, aligned with our human intuition.
Like if we don't know the formula a priori, by looking at the graph, we can know that there's
this compositionality property. And we can also do this for modular addition.
The setup is that, well, three plus five is eight, modular 11, like for each,
that sounds an extremely simple task, but our setup is more like in the large language model
setup, where each which number is recognized as a token, which has a token embeddings embedded in
like 32 dimensions, which are trainable and randomly initialized at first. So the networks are
tasked to learn both the weights of the model, but also the embeddings of the tokens. So that task
actually is not that trivial, you need to learn the representation of the numbers.
And usually we find that your neural network would put the numbers around the circle,
just like on a clock, like when you read a clock, three plus five is eight, because
the addition of the angle of three plus addition of the angle five gives you the angle of eight.
So this algorithmic datasets are used a lot in the machine learning community to
study the mechanistic interpretability of neural networks. Here, I just take it as an example
to show that by applying BIMT to this dataset, we can have sparse and modular structures to emerge
that are much more interprisable if you don't use BIMT. Well, what's interesting here is that you
see that there are three independent trunks emergent. And for each, there are three heads. And
for each head, if you look at the representation of the numbers, they're put on different circles.
And the last one is even a 3D object, it looks like a bow tie. But if you project it onto
2D properly, it's again a circle. And what's nice about this is that there's no need to search
for directions. Because if you think about, like commonly wording bad things have this
rotational symmetry. But if you add sparsity and add this kind of to minimize this connection cost,
then there's symmetry breaking, there's no longer this rotational symmetry. And each
component, each dimension will be itself be meaningful. You don't need to explicitly search
for direction that is meaningful. And we did some ablation studies and find that if we knock out
any one of the head, the performance severely degrades, meaning that it's basically like
majority voting, maybe not majority voting, because the lodges are additively contributed,
additively aggregated in the last layer. But there's some kind of voting mechanism going on,
which is also quite cute, because we didn't impose this such mechanism, but neural networks just
discovers it by itself. Well, here are just some more algorithmic data sets. Here we do,
we compute the permutation group. It's just show the structure of the network,
trained with BIMT. What's surprising here is that we find there's a separate input neuron here,
and we plot it, we find that it corresponds to the parity of the group element, whether it's an
even permutation or it's an odd permutation. So it's precisely the parity of the group element,
which is also quite cute, because it's aligned with human intuition and the mathematical concept.
Well, so before, I only embed a network in a 2D Euclidean space, but we can go beyond that.
If we're dealing with image data, the image itself is 2D, and we have an extra dimension,
depth dimension, then it's better to, it's more natural to embed it,
to embed the network into 3D Euclidean space. So here's what we do with the MNIST data.
So the neurons, both the input neurons and the hidden neurons, they are arranged on a two-dimensional
grid, and their distances are computed in 3D Euclidean space. So we do a similar thing, train,
apply BIM to MNIST classification, and we end up with this beautifully looking image. Although
I have to confess that it's not that interpretable, maybe that's because it's difficult to explain
with words, with language, how we do visual processing. But the point here is that it's not
limited to 2D Euclidean space. It can be a very general method.
Can I ask you a question? Most computer vision algorithms use convolutional neural networks,
so they basically enforce locality by using a finite dimensional window. Do you see this
thing emerging from the way you train the model? It becomes more similar to a convolutional neural
network? Yeah, so we indeed look at the convolutional filters. Sorry, I don't have it in the slides here,
but it's in the appendix of the paper. So the convolutional filters learned in our network
actually have a difference than convolutional filters in CNN. In CNN, at first you learn some
low-level structures like edge detector, and then you gradually compose these
low-level features to get high-level features. But in our case, the first layer already captures
high-level features. Well, maybe this is simply because it starts from a densely connected
network. It has larger receptive field. But in CNN, the size of a filter is usually quite small,
so you're only constrained to look at local ridges. But yeah, the learned filters in our
network is quite different from CNN. And another difference is that CNN explicitly
have this sharing mechanism, but we don't have any sharing mechanism. So I don't know if it's
a feature or a bug of our method, but I would say maybe it's a feature. But I'm also thinking about
is it possible to marry CNN to our method to both enjoy the more flexibility as in our method,
because it's not necessarily look at local features, but also utilizing the sharing mechanism
so that you can compress a network more or more like human visual system.
Yeah, that makes sense. Thanks.
Maybe a follow-up question on that. How robust are your networks in these 2D examples with
respect to spatial transformations, like moving the image or scaling the digits?
Oh, I didn't do that. I just take the original MNIST dataset. Sorry.
Yeah, sure, because that's small as the power behind CNNs, right? So that would be interesting.
For this case, how important your single neurons are or how robust they are with respect to
fluctuations in the input. I mean, you have a lot of different examples in the dataset, of course.
To continue this question, according to this image that you show,
the network reduce a lot of robustness, meaning that every decision is based on
fewer and fewer neuron, which is less what you will come up with with the biology,
which has much more robustness. So I would think that the network is also sensitive to hijack,
you know, the time that you can modify the image a little bit and then you decide if
that image is more like one and not zero, only because you had a sticker on somewhere.
Yeah, I totally agree. I totally agree. So robustness. So I would say that robustness and
there's a trade-off between robustness and sparsity at least. If it's too sparse,
then you can just add noise to the sparse parts and screw the whole thing. But if
it's not sparse, then it's much more robust. So like our goal is to interpret something.
So our strategy is that, well, we don't think about robustness for a while, we just get
the smallest network that can do the task in a non-robust way, but still interpretable.
And maybe then if we want the algorithm to be robust and then we ensemble them in a clever way,
but each part of them we can have some understanding of.
Yeah, sure. So above, I basically talked about the brain for AI. And next, I'm going to talk a
bit about AI for brain, but probably more like brains by AI for brain. It's a little bit mouthful.
By brains by AI, I simply mean the BIMP method that we proposed. But before diving right into
neuroscience or brain, I think the method we proposed, BIMP, can be a quite general tool.
For science in general, like many scientific problems can be formulated as a regression,
just given independent variables x1, x2, to xd. And independent variable y, we want to find a
function, we only find a relation, right, basically write y as a function of x1, x2, to xd.
So previously, what how scientists did it is maybe come up with a symbolic form for it and have some,
you know, feasible, trainable coefficients. And fit that you have some measurements in experiments,
and you have you have a conjectured symbolic form for that. And then you feed your theory,
you feed your theoretical model to your experimental data by adjust that coefficients.
But it's quite not flexible, right, because you need to come up with those symbolic,
those symbolic formulas with educated gases, and sometimes you can meet something. But I argue
that I haven't had many examples, but I argue that such like, like our BIMP method can help you
gain some, at least gain some insight on how to formulate these symbolic terms. Or actually,
this, actually, a less, even, even more humble goal is not to come up with a symbolic formula, but
just tell you what's the structure behind the data set. So, yeah, and, and recently we are
basically, we are eager to have, we are like actively collecting data sets from all kinds of
fields, including fluid mechanics, biophysics, quantum materials, etc. And applying BIMP to
all kinds of data set. Basically, as long as your, as long as your problem can be formulated
as a regression problem, BIMP can give you the try. And may or may not it can discover
something, but there's no loss in trying it. So again, here's an advertisement. If you guys have
any data set that you figure that maybe there's some interesting structures, interesting modularity,
modular structures hidden in data set, then I'm more than happy to collaborate. You can just send
me the data set and I can run the code and return back to you. And because you are the domain experts,
you can tell me whether there's something interesting or it's a total mess. But anyway,
so I see a lot of potential in applying BIMP to scientific problems in general.
But recently, we have tried one thing, which is applying BIMP to, like, to, to neuroscience.
Specifically, we, we, like, traditionally, we have a recurrent neural network to do cognitive
tasks. So the setup is that your inputs, you have one dimension telling you that it's fixation
or not. And you also receive stimulus, you have to receive stimulus of two different modalities.
And each stimulus is just a ring telling you which angle it is. And you also have this so-called
rule input, which tells you which task you are performing, like there are 20 tasks or 84 tasks.
Basically, the rule input is a one-hot vector. If you are performing the first task, then the first
component is activated. If you're performing the second task, the second component is activated.
Okay. And in the hidden, in the hidden layer, it's a recurrent structure. It's connecting, like,
they're just hidden to hidden connections, just recursively going around in the, in the hidden
layer. And until you read out to get, to produce some output. So this is like a standard recurrent
neural network for the cognitive tasks. And what's interesting is that after you train an RN to do
this cognitive task, is there any structure coming out of, like, is there any structure hidden in
this hidden neurons, in this hidden-to-hidden connections? So, so this is already known, like,
four years ago, robot Guan Yu Yang here at MIT. He wrote this paper saying that finding that there
functional modularity in this trained recurrent neural networks by functional
neural modularity means that there are neurons which have similar functions,
and they form, and they form clusters in function space. So to make it more concrete,
if a neuron or equivalently, if a unit is a constant function for a task, then it has no
importance over the task, because it's just a constant function, it's very boring. But if that's
unit has high variance for a task, by that I mean, so a task contains many samples, contains many
time series, many examples. So the variance is measured on those samples belonging to the same
task. So if a unit has high variance for a task, then it's important for the task. So for each neuron,
you can measure its task variances for all the tasks to get a task variance vector.
And then, if two neurons have the same task variance vector, we have a sense that they perform
functionally similar. So basically, this boils down to a clustering problem in this task variance
space. Each neuron is basically a point in the task variance space, and we want to run
clustering algorithm for other neurons to see if there are any cluster structures. And indeed,
you can see there are some cluster structures, some clusters responsible for the goal task,
some for the decision making, etc. Right, but here's the question. So this results imply that
there's this functional modularity, but what about anatomical modularity? That is, if two functions,
if two neurons perform similar functions, are they necessarily close in space? Actually, for
standard neural networks, as I said, there's no notion of locality, there's no notion of spatial
coordinate. So it's impossible to talk about, even talk about anatomical modularity in standard
neural networks, but it's possible with BIMT, where each neuron is assigned a spatial coordinate.
So we ask this question, is it possible with BIMT to create co-emergence of functional and
anatomical modularity, just like in our cerebral cortex? So again, the key is to introduce locality
into the whole thing. What we did is we placed hidden neurons on a 2D grid. So each neuron has
two coordinates x and y. And in training, we regularize hidden-to-hidden connections in the
BIMT style. And also we allow hidden neurons to swap if this can bring down the connection costs.
And now, in the hidden layer, you see that the peripheral neurons, they don't have any connection
to other neurons, which meaning that they are not important, they are pruned away, or they're dead.
So we can basically focus on the active neurons in the center. So we already have the methods
to classify what each neuron belongs to functionally. So now for each neuron, we can compute
which function modules it belongs to and colored with different colors.
So yeah, you can see that's the red neurons that basically cluster in space, although not
that perfectly. Like they're still scattered around in the plot, but you can still see some
giant clusters of blue, red, and green. And by contrast, if you use the regular L1 regularization,
there's no notion of space in it. So it's not surprising to see that the neurons with
similar functions are scattered around in the plot. I have a question. So since you have the
space notion, then some operation of space like a string or resize can be applied. So I'm wondering,
is it possible to change the resolution of the 2D panel when you're training or after training?
Would it affect the performance?
By resolution, you mean maybe increase the speed from 10 by 10 to 20 to 20 by doing some
interpolation in the... Yeah, or decrease it. So you reduce the performance or make it better?
Oh, yeah, that's very interesting. Because also in physics, there's a concept called
renormalization group where you coarse grain to units which are close in space. Yeah, that's a
very interesting point. We haven't tried it, but it's worth trying. Yeah.
I'd also say I used the same idea, but instead of renormalization when they're close in space,
I renormalized when they are close by correlation or by dynamic, and it made some
interesting coarse grating as well. So it might be worth looking at here as well.
Yeah, yeah, yeah. That's a good suggestion. Are you saying that you did that or
there are people already done that before? Somebody has done it. They did it really simply
in a data set from mice, and then I used the method when a live real mouse was doing a task
and was able to do some really interesting analysis using the resulting coarse grain signals.
Oh, interesting. Is there any reference you can point me to because that sounds extremely
relevant? Yeah, let me pull it up. And then if you're interested, we can talk about it in a
different... I'll post a link in the chat, and then if it's something you're interested in,
we can meet in a different space and I can show you what I did with it because I changed some
of what they did. Yeah, cool. Yeah, definitely. Thank you. I also have one question. Have you
tried this experiment, but with just L1 regularization and the swap mechanism?
Oh, you mean L1 regularization? I see. So that's basically setting...
That's a special case of BIMT. So here, BIMT, right. Because in BIMT, you also enforce the
locality, right, right, penalizing the weights that are far away, but if you don't penalize this,
but you make the swap mechanism. If I recall correctly, the results are similar. So
actually, the brain plot I show here, the local parameter is actually very small,
meaning that it's close to no penalty at all. And the ablation study, we didn't do that too
rigorously, but the impression is that swapping is much more important than locality,
a differentiable locality. Yeah. Okay, interesting.
Yeah, sure. So how do we measure quantitatively the progress here? How modular this brain is? So
we propose two quantitative measures. The first measure is the fraction of isolated neurons. So
we define a neuron is isolated if none of its neighbors have the same color. That is, none of
its neighbors belongs to the same modules. So to get a sense of what the baseline looks like,
so we randomly permute these active neurons. We basically randomly permute these active neurons
to compute the ratio of isolated neurons for these randomly permutable configurations,
