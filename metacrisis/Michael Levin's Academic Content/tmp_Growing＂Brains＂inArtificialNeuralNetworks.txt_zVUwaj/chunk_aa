I'm Xeming Liu. I'm from MIT and IFI and advised by Professor Max Targmark,
working in the intersection of AI and physics. Previously, although recently Max wishes the
research focused to safety, mechanistic interpretability, but there's still a lot of
overlap, right? We care in general science, like interpretability for science,
but maybe not only science, but recently also about large language models. Anyway,
so today I'm going to talk about growing brains in artificial neural networks,
which covers two of our recent work. Brain group is Brain for AI, where we develop a brain-inspired
method for AI interpretability. The second work, basically using the tool we built in the first
paper to encourage modularity, encourage AI for brain. Without further ado, let's jump right in.
Our first motivation for this line of work is interpretability. We know that neural networks
are extremely powerful machines, but they are notoriously difficult to be interpreted.
We want to, in order to keep them safe, we want to gain some interpretation of them,
but there are many levels of interpretation. There are neural level interpretation where
we want to understand what each neuron is doing. The other extreme is this model level
interpretation, where you just think of neural network as a oracle with this input-output pair.
There is this intermediate level, which we call the module level interpretability we think is the
most promising level. The reason why is that if you think of the trade-off between the complexity
of some of the interpretation, inaccuracy of the interpretation, the model level interpretation
is too inaccurate or informative, not informative, because it's not telling you what's going on inside
the black box. On the other hand, the neural level is maybe too subtle, too complicated,
because you need to explain what each neuron is doing. The module level hopes that you can
disentangle a network into different parts, and different parts responsible for some
certain human interpretable function. These modules are specially connected, again, in an
interpretable way. That's the hope, that's the plan, that's the hope with this module level
interpretability. It's not totally wild dreams, because in vision systems and language systems,
at least for some particular simple toy tasks, people have discovered such modularity can emerge
or networks. For computer vision tasks, people discovered that there are these curved detectors
and for language tasks, for this what they call indirect object identification tasks,
they find that in GPT-2, they can identify this nice circuit, which is quite aligned with our
concepts, how humans would do such tasks. If we believe in this module level interpretation,
how do we go about to make this interpretation even easier? I would argue that locality can make
things much easier, in the sense that if you have a neural network with no locality, well,
actually for common neural networks, there's no sense of locality. If you take a fully connected
neural networks, the neurons, they are all permutable, they have this permutation symmetry,
and there's no sense of spatial coordinates in it. For a neural network with no locality,
then to describe a network, to describe the network in terms of module, in the language
of module, you need to describe, you need to tell which each neuron is belonging to which
module, like the first neuron belongs to the red module, the second neuron belongs to the blue
module, so the description length, so to speak, of the description is proportional to the number
of neurons, because you need to assign a module label to each neuron, and it's also not clear how
to do that, but on the other hand, if you have locality in your model, basically, if one neuron
is a red neuron, then very likely its neighbor will also be a red neuron. It's very much like in
our human body, or the cerebral cortex, where, well, each local, like the localized regions
responds for certain functions, so if we have locality in your network, then we only need to,
the only thing we need to do is draw boundaries between these modules, like in this case,
if the red and blue modules, they are nicely separated, then all you need to do is to
have a knife cut through these two, and realize that the left part is red, and the right part is
blue, and then the description length is basically just on the order of depth, because you need only
to indicate where you want to cut through these two modules. So, can I ask a quick question?
Yeah, sure. Yeah, hi. So, when you say locality, you talk about in terms of function, right? Like
in your example, if one neuron is performing a contributing certain function, its neighboring
neuron is also likely contributing to the same function. Does that also imply locality in terms
of structure, meaning that in these neural networks, does it also require sort of local
connections, like topographic connections, or even with all to all connections, could you expect
locality? Yeah, that's a good question. So, maybe I need to make explicit distinction between
functional modularity and anatomical modularity. So, functional modularity means that two neurons
perform similar, like both responsible for some certain function, that's function modularity,
and anatomical modularity meaning that the neuronal connections are local. So, by here,
I hadn't gotten into details. So, here, maybe I meant both. You can think of it as both,
or some even abstract sense of modularity, but I will get into some detail after this. Yeah.
Sure. So, in this slide, I argue that locality is a very, is a simplifying property
to achieve module-level interpretation. So, we already know that our brains, at least in
the cerebral cortex, localize regions, like functional modular regions are localized
in space, but do our artificial neural networks have the same
property? And I argue that no, because there's a key difference between brains and
artificial neural networks, in that, for our brains, there's a selective advantage for modular
brains. So, for modular brains, I mean that relevant neurons, right, so functional modular
neurons, they're also anatomically modular in space, they're put close to space. So, the connection
between, so the average, so the connections between these neurons are relatively shorter,
hence the connection, the communication speed, the time delay, also shorter if you have a modular
brain. As a result, human beings with modular brains tend to react faster than people who don't
have modular brains. Hence, modular brains are more likely to survive, hence have a
selective or evolutionary advantage over non-modular brains.
However, artificial neural networks do not have such survival advantages, because
if you think about it, the training objective is just to make prediction as accurately as possible.
Well, you may think, you may argue, one may argue that there might be implicit
regularizations that may encourage modularity, like a dropout, like all kinds of regularization,
but they're just encourage modularity in an implicit, unclear way. So, in general, we do not
expect artificial neural networks to have modularity, because there's no survival
advantage for artificial neural networks to become modular. So, inspired by this,
I argue that we need to explicitly introduce some training techniques to explicitly induce
modularity in otherwise non-modular networks. Say, you take a fully connected neural network,
which by default are not modular. All the neurons in some consecutive layers, they're densely
connected, but we want to train the network in some strange or smart ways so that the modularity
can emerge after training. So, the key here is actually quite simple. I will spell the word
again. It's just the locality. So, in this paper, we propose the method called brain-spired
modular training. Although it has a fancy word called brain-spired, it's actually just introducing
locality, a sense of locality into neural networks. So, previously,
neurons in artificial neural networks do not have spatial coordinates associated to each neuron.
But here, we assign a two-dimensional coordinate, x and y, to each neuron and embed the whole neural
network into a two-dimensional Euclidean space. In general, it cannot only be
to the Euclidean space. It can be any arbitrary geometric space where you can define coordinates
and distances. But for simplicity, we can embed them into this two-dimensional space.
In bed with this metric, we can compute distances between neurons. In this sense, we can define
length of the weight connection, not only the magnitude of the weight, but also the length
of the connection. With this picture in mind, we penalized previously to encourage a network
to be sparse. People usually use the L1 regularization, which is basically lambda,
is the penalizing strength times the magnitude of the weight. But here, we basically take the L1
regularization, but it also be multiplied by the length of the connection, meaning that if the
connection is longer, then it's penalized more strongly than a connection that is shorter.
So this objective bias towards a solution that has shorter connections.
But actually, this doesn't work if you just penalize, if you just naively add this connection
cost term into your training because your network can still very easily get stuck at some
long connection configuration. A fourth experiment is that if you start off from a densely connected
network, which have many long-range connections, but it already achieved the task perfectly well,
then there is no incentive for it to simplify. So to avoid that, we also do this kind of discreet
optimization we call the neuron swapping. So suppose we have a neural network like this,
the red neurons, they come from a module, they're strongly connected, and the blue neurons,
they're another module and they're strongly connected. If we find that swapping, the two neurons,
the blue neuron and the red neuron in the middle layer can reduce the connection cost,
then we do the swapping. So basically, our method consists of two, has two gradients.
One ingredient is we have this differentiable penalizing term which encourage local connection.
Another ingredient, another ingredient is we allow neurons to swap. If swapping the two,
if swapping the two neurons can reduce, can reduce the connection cost. So we do these two steps
iteratively, like we train for a while, we do swapping and we then train for a while and then
swapping until it converges to something or like we meet the compute budget.
So an example of this, so let's consider a very simple example which is basically
regress polynomial function with a neural network. So the input here has four variables,
x1 to x4, and there are two output variables which basically just the second order polynomials
of these four variables, but it has some interesting structure like the first variable
and the first output is x1, x4 plus x2, x3 and the second output is x1, x4 minus x2, x3.
So we might imagine that the way we compute this is basically compute x1, x4 first, compute x2,
x3 first, and then linearly combine these two things to obtain the final output.
And so it's not surprising to find that with a densely connected network,
although your performance is quite good, it's still densely connected with revealing no obvious
structure from this dataset. If you use the Lina L1 regularization, you'll get a sparse network,
but it's still not clear what the structure is. But if we jump right to the rightmost
figure with our BIMP method, which involves both L1 and local, it's a local L1 and involves swapping,
then you see that something remarkably sparse and modular emerge out of it, and although I
don't have a plot here, we can check that the two neurons in the second to last layer are exactly
x1, x4, and x2, x3 up to some rescaling and translation. So by just looking at the plot,
you can immediately see some nice structure out of it. And without too much sacrifice in loss,
although they're like 10 times, around 10 times, or even 30 times, it becomes worse in terms of
MSC, but that's acceptable. The point is, here we obtained something interpretable with our method.
You can directly see the modules. These two modules in the first two layers, they don't
talk to each other. And it's clearly, at least the dependence, how these variables are dependent
on each other and how the variables are combined to get the final output, the computational graph
is clear with our method. Otherwise, you cannot get with other methods.
Can I ask a question? Yeah, sure. Like in the fourth figure, which is just L1 plus
SWAP. In theory, the SWAP operation shouldn't really do anything in this case for the actual
training procedure, am I right? You mean the subfigure D? Yeah. So here it looks like the
local part does not help that much, but it still refines a little bit. Yeah, they are very similar
in this example. Right. But if you look closely, maybe there's some
long range connections here. Yeah, I won't argue that the local part is necessary. I would say
it's a big example, dependent. Okay. Maybe like in this image, I don't know if there are very faint
lines in between. You mentioned back from the stream, I can't see those lines at all.
Yeah, it's also mentioned like for subfigure D, it's also like no one has proposed it before. It's
just a special case of R1. Okay, thanks. I guess the local part and SWAP part is for the same
objective, but maybe the local part sometimes it's non-differentiable, so you need SWAP,
and SWAP is not very accurate. So it's kind of balanced. Right, it's a kind of trade-off.
Like the thin lines, well, yeah. Yeah, it's a bit hard to analyze, but I agree with the picture.
Yeah. Can I ask another question in the previous figure? Yeah, sure. In subfigure E,
there are just two neurons in the second layer, right? Yeah. Do they by any chance
represent X1, X4, and X2, X3? Did you check that by any chance? Yeah, I checked that. They're exactly
X1, X4, X2, X3, up to some constant scaling. Yeah. And I was asking a question on how
reliable is the training procedure there? So how often do you get exactly these results,
or how often do you get suboptimal in terms of number of connections, for instance?
That's a good question. So the structure are basically similar, but in terms of detail, like,
well, it's again depends on how difficult the example is. Like for this example,
it's quite robust, even in the slightest details. But for more complicated examples,
the details of the connection can be different. And yeah, to think of it, here you are
X1, X4 is multiplying two numbers, right? But if you do some construction,
actually, you only need two neurons to do the task. If you're using this
silo activation, so you only need two neurons to do the task. But I ended up,
but the method finds three neurons. So in some sense, it's still not the most efficient
solution that's possible. But it's something easier to be found for networks. So the robustness,
yeah, again, in general, I didn't test really hard about it. But the short answer is the structures
are similar. But the slightest details can be different depending on random seeds.
Oh, thanks.
Yeah, the network in figure E is almost exactly bilaterally symmetrical. Is that by design or
is it like a byproduct of some aspect of it? It's discovered by itself. It's something
I personally found amazing. It's just that this is the simplest possible thing,
and he just finds it. Very cool. Yeah.
Another question, did you benchmark this, for instance, with similar methods or
not similar methods, but other topology searching methods like Neat? Because these
methods also allow us to find very, very small networks which achieve the task perfectly.
Yeah, that's a good question. So after we finished this project, we were thinking
maybe combining Neat with this can be a nice choice because Neat is good at producing regular
patterns. And yeah, maybe combining the best of both models would be nice. But then I tried,
not exactly Neat, but just I have a hyper network producing the weights. And then it doesn't work
very well because, well, in the network I show here, you can see that the weights actually
will have high variations. Like red line means it's a positive weight and blue line means it's
a negative weight. So it's oscillating too fast. And I'm not sure if Neat can capture this.
But maybe adding some positional encoding, adding some high dimensional,
sorry, high frequency biases can help Neat to learn it, but I still didn't get around that
technical issue. So yeah, if you guys have any idea how to combine these two in a more
consistent way, I'm more than happy to chat. Yeah.
Thank you. I can think about it. It's really interesting. Yeah, wonderful. Yeah.
Yeah, so here are just some more examples. The first example is that the first
output variable only depend on X2 and X4. And the second variable only depend on X1 and X3.
So the network learns to automatically split the
network into two parts, which don't talk to each other. So by looking at the graph, you know,
you can immediately tell the independent structure. And the second example, again,
it's very interesting. And similar to the example I showed in the last slide,
there are three, you need to do the function, you need to do the squared function three times.
And these three motifs here, they look extremely similar. It's like, I did not impose anything
to say that I encourage repetitive patterns, but it's just something that it rediscovered.
It's just, the squared function is just rediscovered for three times.
And the last example highlights it can discover, excuse me,
a highlight is can discover the compositionality of a formula. So if you compute this,
