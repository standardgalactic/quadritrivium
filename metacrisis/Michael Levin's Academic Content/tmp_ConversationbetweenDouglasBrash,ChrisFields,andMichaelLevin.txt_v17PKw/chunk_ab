places they did, but now they're separated, and now they have separate controls. And then once
you start looking at the nervous system, you see all kinds of things like that. And then
end time to the same ends on the same line. And then what you were saying about the stress
response, that looks like it goes back to a Waddington small house and Baldwin again. And then
I noticed today, when I was rereading your paper, that well, what you're saying is, you're going to
do all these swaps before you select. And so you say it, well, okay, that gives you a chance to do
stuff before you can get killed by the selection pressure. And that's essentially what Waddington's
environmental inducibility is doing, saying, okay, if I'm going to develop calluses on my
ostriches, but it's only inducible, and there's a bad side effect to it, no problem, it's inducible,
it's not on all the time, get away with it until I make a perfect callus by mutating three other
genes. And then again, I think the same argument holds even better for cancer, where you don't
have a million years, you have to do it in like six months. Yeah, I mean, I think I think you're
right that that kind of like capacitor function is definite. So if you take, because so let's say
that I in the tadpole, you or the or the mouth, you know, you make a mutation, and it has some
kind of positive effect somewhere, but also makes the mouth off a little bit. Well, that mouth is
going to get back to where it needs to go before the embryo has to eat. And so that means that
whereas before you would have had a deleterious mutation that would erect the whole thing, and
you would have never seen all the positive effects. Now it buffers it. So a lot of these mutations
become neutral, whereas they would have been negative before. And so you get to you get to
explore whatever else this thing is doing, because because nowhere is the mouth will find where it
needs to go. So I think I think that that competency at the tissue level, very much smooths and sort
of makes the search space much nicer, because all the things that otherwise would have would have
killed you and now would have been that now you have the ability to tolerate it and maybe use it
later on. So it lets you carry all this all this stuff at the genetic level. Again, it isolates
you from from from mistakes at the genetic level. Because because, you know, if your cells are
slightly the wrong size, your kidney tubules will still figure out how to be the right diameter.
And if your mouth is off, it'll come back. So that's the answer then to Bayhese intelligent
design argument. I think so. I think so. So this is something that's interesting. I've been talking
about this with what with Richard Watson, which is that we now know from from computer science and
other things that you can have an intelligent system that is made of less intelligent parts,
you don't need everything to be intelligent. And so to me, the thing about evolution now,
what two things one is, I think whatever whatever intelligence it has is provided by the fact that
the part you're dealing with an agential material, you're not dealing with passive Lego blocks,
the evolution is search, you know, so all these cells used to be independent organisms. So they
have their own agendas. And so what evolution is really searching is this like space of behavior
shaping signals so that the cells can get each other to do various things. And that's where the
intelligence comes from the mutations sure they're random. That's fine. Nobody has to keep track of
the whole the whole thing. But but there is a degree of intelligence to the process because
the parts are able to make up for errors. And it doesn't tell you where you're going to end up
necessarily. But it does tell you that the process isn't as blind and stupid as you know,
as it's made out to be. But nor does it need to be, you know, super human level intelligence.
It's just, you know, it's a little bit it's got a little bit of smarts a little bit.
Well, I think also an aspect of this is that the parts don't need to be intelligent
in the same environment that the whole system is right. And in fact, they can't be
they need to be intelligent in their own environment, which for cells is an environment
of other cells, some of which are similar and some of which aren't. So you can expect the kind of
social relationships, modulo their communicative strategies and abilities that that that we have.
But so they might use the same kinds of tools in terms of social communication that we do.
But the environment itself that they're communicating about is very different.
Yeah, the problem. Go ahead. Well, I was just going to say, yeah, the problem space is different.
And we have another paper, a preprint out this, my postdoc Leo has the system where
you've got cells and the cells are operating in metabolic space. So all they know how to
their little homeostats and all they know how to do is try to get more food.
But the collective as a whole makes a French flag developmental pattern. So you see how you shift
problem spaces, right? So the cells of these little tiny goals local, and they're competent in
that they don't know anything about French flags or whatnot, but the collective is able to do that.
Right. And so and so I think, right. So Chris and I had that paper on
navigating different spaces, right? So evolution kind of pivoting these tricks
from one space into another by basically the same kind of scale free dynamics.
So do either of you have an opinion as to whether if you're trying to, well, I shouldn't say trying
to and I shouldn't say get to how to say it. If there is some larger concerted behavior,
do you have an opinion as to whether that's because of some goal and set point and cybernetics,
or whether it's an attractor that really doesn't know where it's heading? Chris, you want to try
first? I'm not sure. I don't see the distinction between those two alternatives. Yes. So yeah.
So I hadn't thought about it till recently. I think that or it's probably because I don't understand
attractors well enough, but for a cybernetic system, you have some sort of set point, which is
stored someplace, you have some sort of input signal, and you have somebody comparing the two,
and then telling somebody else which direction to change to reduce the difference. So you have some
goal, which is arguably some representation of some future state sitting there and some
computational apparatus. And then as I understand it, and I don't, attractors are pretty much like
a gravitational hole, and you just sort of get sucked there. Well, in a sense, the attractor,
you can view the attractor as a representation of a goal state.
And of a goal state. Okay. Yeah. And each particle or agent or whatever you want to call it that's
moving within the field of the attractor just has to make a local measurement about whether it's
going up or down. So it tells itself what to do based upon where it finds itself to be.
So there is a comparison, but it's to the local state, and the attractor is set up some kind of
gradient. Right. Okay. So I'd like to look at it slightly, slightly differently, which is somebody,
and I wish I remember who it was, but somebody had this amazing phrase that said that it's the
continuum between two magnets trying to get together, and Romeo and Julia trying to get
together. Right. So the difference is, right, there's a continuum. And the difference is how much
basically how much cleverness or competency do you expect along the way. So I think the system
that Chris just laid out, if that, in systems where that's true, that's kind of the magnet case,
and all they sort of can do. And if you put a barrier there, that's it. They're just going to
stay there pressed up against the barrier, they're never going to go around, they're never going to
do anything clever. That's all they know how to do. So that's sort of on the left of that continuum.
And then you, and then all the way on the right of that continuum are some super sort of smart
systems that not only do they see the gradient, but they have delayed gratification, so they can
sort of avoid being trapped in local minima. And they maybe have some planning and some
metacognition and some, you know, who knows. And then in the middle, you have all kinds of simpler
systems. So that have some of those capacities, right. So, so, so maybe all you're doing is following
a local gradient. But for example, we've been, we've been finding all kinds of examples, some of
them extremely simple things, like very simple algorithms that when you actually play them out,
it turns out, no, actually, they have some capacity to resist perturbations. And so,
to me, one of the things, like a lot of people will look at a system, and they will immediately
have a built in feeling about where that is, oh, that can't, you know, that's just, you know,
that's just dumb chemistry. That can't whatever. But I don't think you can ever tell unless you
start probing it and the way you probe it. And so I don't think you can tell from purely
observational data. I think you have to start probing it by getting in its way in various ways
and seeing what it does. How much can you get this thing to resolve the problems that you're
giving it? And does it always do the local gradient thing? Or, you know, can it, can it,
can it do something a little more clever and get around when there's a, and there's,
and there's a wide variety. So in biology, like one thing you can do is start everything off in
the wrong position. With the metamorphosis in the tadpole, it used to be thought that
in the tadpole, all the organs, all they know how to do is go the right way in the right direction
and for the right amount of time. And that's it. And so we scrambled them all in different
positions. And we found out, no, actually, they will keep going in weird paths to get to the right,
to get to the right place. So already, you know, it can't be as simple as all they know how to do
is go in the right direction. They're context sensitive. And there's tons of systems like
that where they look like that's all they know how to do. But when you interfere, you find out
that they actually have all kinds of capacities and they're sort of further along, you know,
they're further along than you think on that spectrum. So the difference between those two,
that's a nice analogy. The difference between the two is that then the space between where you are
now and the actual endpoint, the attractor, if it's an attractor, that intervening space is also
already programmed or deformed or whatever. Whereas in the Romeo and Juliet case, it's not.
You know, you have alternatives. Yeah, you have different different capacities to navigate that
space. You have different strategies. And I mean, people who build autonomous vehicles have,
you know, they think about that stuff all the time, right? You want to get from here to there,
but you're not just going to do as the crow flies unless you hit a barrier, you've got all sorts of
different tricks up your sleeve about both local and maybe global, sometimes global information
too, sometimes not. But, you know, one in particular that I really like that we use a lot is,
well, that is, is, is I, for lack of a better word, I call it delayed gratification. It just
means, are you able to temporarily get further from your goal in order to then do better later?
Some systems can't, right? They're always, you know, and I've got, I've got a great picture I took of
them, two dogs across a fence, and they're trying to get at each other. There's a hole in the fence,
five feet down. But in order to do that, you have to, you have to get away from where you want to
go. And they're just like, they're like this, right? And maybe they'll figure it out eventually,
or maybe they won't. But, but, but that's that ability to get further away is, is I think pretty,
pretty critical. It's a basic capacity not to get trapped in that local, you know, that local,
local optimum there. And then there's all kinds of complex tricks after that.
So an end run, basically. You may know this already, but primates and birds can do that,
and other animals cannot. No, the classic one is you, let's see, there's this experiment.
You, you have food, the monkey's in a cage. There's food outside, just outside his reach.
There's a stick behind him. And so is he going to figure out that if he turns around, grabs the
stick and then goes back, he can get the food. And so evidently primates and birds can do that,
and other animals can't. Yeah. Yeah, I think in, in terms of this landscape picture,
when we think about an attractor, it's typical to imagine, you know, a potential minimum,
and then just a smooth surface coming out of it. And that's the, the simple picture.
Whereas in, in real life, even if you have a fairly deep attractor like Romeo and Juliet,
the, the surface coming out of it is very complicated.
Oh, bumpy attractors. Yeah. With, with all sorts of, you know,
channels, some of which might be solenoidal even, to keep you from ever getting over the edge.
And so these competent systems have to be able to increase their temperature as, as
Mike was referring to earlier, to, to get out of, of local minima
in this, in this very complicated landscape that surrounds wherever it is they want to, to go.
But it's also the case that if you think in, in terms of a static landscape,
then that corresponds to an environment that's not malleable by the organism. Whereas again,
in real life, what the organism does in navigating the space not only changes its position in the
space, but it also changes the shape of the space. The organism actually modifies the environment
by taking each action, not just its position in the environment.
And you can change where the attractors are, I suppose.
Right. So, so the organism can even make the attractor move around based on what it's doing.
Yeah. If it's a sufficiently competent interactor with its environment,
and if the environment is sufficiently plastic. Yeah. So we see that in social interactions,
for example, the environment, the social environment is incredibly plastic.
We can change it in various ways just by talking. Yeah. Whereas, you know,
the environment of the desert is not all that plastic, you can wander around,
you don't change where the mountains are.
And so, and so you can imagine, right? So this is important during evolution,
I think, because you can imagine, so, okay, so here I have my thermostat,
and all it knows how to do is check the set point and get the temperature, right? Yeah.
And then there's a more advanced version of a thermostat, which also has a
metacognitive module that makes sure that the set point isn't being twiddled too much,
right, that monitors the set point. Now, why do you need that in biology? Because,
because you're constantly at threat of parasites and exploiters of various kinds.
So the minute you become programmable, like, like, you know, like a thermostat,
somebody out there is going to want to change your set point. And so you need to be able to
resist that. And in a more advanced version, you need to be able to tell whether it's you
resisting it, whether, whether any changes that are made, are you doing it, or is it
somebody else doing it to you, right? I think there would be a lot of evolutionary advantage
to knowing why did my set point just change? Did I change it or did somebody else change it for
me? Am I being exploited in some way, right? And as soon as you say that, then it starts to make
sense that somebody, I was just talking to Eric Welker, and he asked me if, if I thought that how
important threat perception was to, was to this whole process. And I was saying that I think
it's absolutely critical because if you weren't in, in danger of being hacked by, by, by competitors,
you wouldn't have to have a strong sense of self. In other words, you could remain an ocean, right?
And so, so, so, so I'm, I've just been obsessed with this thing that I'm, you're an embryo,
a blasted disk, you know, so 50,000 cells. And at some point you look at that and it becomes
one embryo. And so you look at then you say, how many embryos are people say one embryo?
What are you counting when you say one embryo? Well, what you're counting is,
is the following. A bunch of those cells are going to merge together to, among many other things,
determine a barrier between themselves and the outside world. So every cell is some other cells
neighbor. And so where do I end? And where does the world begin? Well, the embryo is going to
make that determination. And you'll have one embryo. If you take a little needle and you make
some scratches, and I used to do this in grad school, when working with in avian embryos,
you make some scratches. And until the scratches heal, every individual region which can't hear
the other regions becomes an embryo. And when they do heal, you have conjoined twins or triplets.
And so the question of how many individuals are in that embryo is not known in advance. It's not
genetic. You don't know how many it is. It's something that emerges over, over time, right?
So, and to me, that has, that has really interesting implications for the brain. The same
deal. If I show, if you didn't know what a human was, and I showed you a three and a half pound
brain, I said, how many individuals in there? Who would know who knows how many are going to be
in there, right? It's this process of taking individual pieces and pulling together some sort
of unified something that distinct from that forms some sort of a thing that makes a model of itself
as distinct from the environment. So if you weren't a threat of being hacked, you wouldn't need to do
that. It wouldn't matter where you wouldn't, you wouldn't have to maintain a strong self boundary
because you wouldn't need to know why things are changing because nobody would be, nobody would
be trying to exploit you. But of course, in biology, that doesn't, that doesn't work. You're
always a threat of being hacked. And, and then, and then that, I think that drives this requirement
of determining a strong self identity and then having ways to understand whether, you know,
am I learning or am I being trained? Right? What is the, I mean, I think this is a very
