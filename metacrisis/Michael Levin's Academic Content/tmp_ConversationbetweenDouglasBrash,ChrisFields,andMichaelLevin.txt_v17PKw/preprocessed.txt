Yeah, all right. Well, super, super happy to be able to have you guys both on because I think
we have some really interesting stuff to talk about. And I figured if you haven't met before,
maybe each person take a couple minutes and just kind of go over your background and interests and
so on. Oh, do you want me to go first, I guess? Sure, go first. So my background for the present
purpose is going to be different from what I tell everybody else unless I've had a beer,
but it's relevant here. So I got interested in science when I was a kid because I was
interested in time and what is it anyway, besides just a letter T that you let get bigger. And so
I majored in physics and halfway through college, I stumbled into Heismann Forster and eventually
became convinced that this was more of a biological or cognitive problem than a pure
physics problem. So I minored in physiological psychology and kind of switched into biophysics.
And then I was going to, I went to Ohio State to do theoretical biophysics with a guy
who had, well, you probably haven't heard of Carl Kornacker, but he was a theoretical
biophysicist. If you go back and look at Waddington's theoretical biology books, you'll see
he's in there as a young man. But then after a while, I became convinced that you really had to
do experiments at some point. So I noticed that the molecular biologists, when they were done,
knew what they had done, whereas the neurophysiologists were still kind of making up stories about
what might have happened when you stuck electrodes into things. And then so I wound up doing molecular
biology. And one of the things that happens, of course, is that once you become an expert in
something, it's very hard to convince you anybody to let you do something else. So these other things
I would continue to be interested in, but it's more of a spare time project. And with regard to
things at the end of my list and going back to the cognitive stuff, I continue to work on that.
And we have a way of parsing language that is essentially using semantics as the syntax can
get no one interested in that. Linguistics in particular, I find a particularly vicious field,
but anyway, it works. And then I think there are implications then as to how you do cognition.
Then that this particular conversation goes back more to the started off with the molecular biology
stuff, where we were, what I wound up doing is working out how sunlight causes skin cancer.
And so one of the things we're finding out now is the number of mutations you have in your skin,
you and me, is totally outrageous. And then the question is why the skin's working at all,
like 100,000 mutations per cell. And so that's what started this conversation off with Mike.
And then that got Mike sent me in some of his papers on organizing things from the top down,
which I kind of appreciate because I'd already read Pate and so forth. Is that how you pronounce
it? Is it Pate? Pate? Anyway, okay. And by the way, Carl Koenacker is the guy who first pointed
me out to, oh great, now I'm going to blank on his name. But the ion channels and development,
which is why one Mike called me up like 12 years ago, I immediately understood what he was doing.
The guy at Purdue. Oh, Jeffy, Lionel Jeffy. Yeah, Lionel Jeffy. Right. So anyway, it sounds
like you guys have overlapping interests, you know, on the one hand from the biology side that
could carry over into experiments with cancer, but demonstrating genetic assimilation kinds of
phenomena in cancer. And two, so one of your proposals for that looks a lot to me like a
physically embodied neural net, although I don't know, I haven't had time to see how literally
one could take it. And then Mike sent me some of your papers, Chris, which I wish I read years ago,
but some of them I'm going to have to read twice and I have some questions for you. But
it again looks like the three of us believe the same things are kind of possible.
It looks to me having seen all this stuff at the same time now that it may be possible to build
from the bottom up in a consistent way that achieves all of these things and do some experiments
to back them up. I could give you more details later if you're interested like the language
stuff, but I don't want to, I don't want to hog up all the time.
Yeah, so, Chris. Interesting. I also started in physics and ended up in molecular biology,
but with a turn in philosophy and then in AI in the middle.
And then basically departed from science altogether for several years
before coming back and taking on some of these much broader kinds of
issues at the borders between physics and cognitive science and biology,
which seems like a very comfortable and interesting place to be right now. It seems like a lot is
happening and that there's a lot of room for putting these different disciplines together
in a productive way. Do you find that people are listening to each other or is it all siloed off?
I think the bulk of academia is still very siloed, but I think there are increasing numbers of cracks
in that siloization. I mean, for example, I think Carl Friston has done a huge service in
breaking down some of the barriers between neuroscience and physics and biology in general
with his work and accumulated, you know, a highly multidisciplinary group around himself.
Start off on that and we'll talk about it. Yeah, sure. So where would you like to start
at the beginning or at the end and work backwards? So you guys, I guess, have already been talking
about the embryo kinds of stuff, right? You know what I could do is ask a question just to make
sure I'm understanding things. So there's this issue about competency, which I now understand
basically means swapping cells around to see if positionally in space to see whether
you get a better phenotypic fitness. One thing I didn't understand from the papers,
whether that swapping is random. Right. So you're talking specifically about this
Shrisha paper that my student Lakshman just put out. So what we were trying to do there is
just determine what does it do to evolution when the components are not purely passive.
So we have a very simple one-dimensional model that basically a one-dimensional axis and there's
a bunch of cells along that axis that have a preferred positional information and you could
either be completely passive and let standard evolutionary algorithms sort them and eventually,
of course, it sorts them. Or you can put in a developmental phase between the genetics and
the phenotype where the cells have tiny little preferences about who their neighbors are.
And so the moving around is not random. It's that every cell has a little bit of a look,
ability to sense who its neighbors are and to be happy or unhappy with who its neighbors are
relative to the positional information. So if I'm a five and my neighbor is a nine,
I know I've got a problem and so on. And the nine knows he has a problem too. And so they have
variable amounts of competency to recognize the problem and try to rectify it during that
developmental phase. And so now the question is when they have this, when you're dealing with
this material, it doesn't just sit where the genome puts it, but actually has the ability to
sort out what happens to evolution. And long story short, what we see happens is that
because evolution, once you have an individual that's a little bit sorted and evolution has a
hard time, selection has a hard time knowing whether you're sorted because your genome was
amazing or whether the genome was so-so, but your competency was good and you sorted yourself out,
it ends up not being able to see the best genomes, but rather doing more work on the
competency gene, which then makes the problem even worse, that hides even more information
from selection. And that sort of ratchets up. And eventually you get to something like a planarian
where the genomes are basically junk and the algorithm is so good that it almost doesn't
matter what the genome is, it will sort itself out in the right way, which is what we see in
planaria. And then other organisms are, they do some of that, but not as well as planarious,
there are clearly some factors that prevent this thing from ratcheting all the way
in every lineage. But that's kind of, yeah, that's, I mean, that's just it. It was this,
you know, this question of what does, what implications for evolution of the competency
of these things? And can we explain, and can we explain this amazing fact that planaria,
so, so planaria, right? The most, the most disorderly genome, and yet highly, you know,
perfectly regenerative, no cancer, no aging, you know, no, no mutants, right? There's no such thing
as a planarian mutant line, the way that you have with Drosophila and mouse and C. elegans, no, no,
the only abnormal line of planaria you will ever see is our two headed line. And those are not
genetic, those are not made by any genetic change. So, so why is it that what, why is it that you
can't get a strain of weird looking planaria? And I think this is why I think it's because
they've learned to basically ignore what much of the genome is doing, because, because they have
to, they accumulate so much junk being, you know, not going through an egg phase, they just accumulate
so much junk. So in your model, then, if you're swapping cells around, and the cell knows that,
like you say, it's a, it's a six and it's next to a nine. So there's a level of selection there,
right, already, in the sense that, well, maybe selection is not going to be the right word.
But there's a level there of recognition of a problem just at the cell level.
Exactly. That's the competency. The competency is I'm not a passive Lego block. I actually have
the ability to look to see who my neighbor is. And I have a preference, I have a local preference
about, about how much, what, what kind of a different neighbor am I willing to tolerate.
So if my neighbor is super different from me, then I'm not happy. I want a neighbor that's
pretty close to me. So it's perception plus kind of a goal, or what you want, and an ability to
move. And is the place you move dictated by the discrepancy between six and nine,
or is it a random movement? It's not random, but, but you don't have so, so, so, so it's not
random. You try to go in the direction that you think you should be going, but, but you don't
have a God's eye view where you know where to go and when to stop. You only have a limited ability
to sort of take a couple of steps in the, in the right direction. And that's it. So, and you can,
and you can, you can crank that up either, either either sort of by us, the experiment or you can
let that be evolveable. And then we did that too. And so there's a level of, there's a level of
competency, you know, how much, how much look ahead do you want? How much crawling ability
do you want? And so on. Right. So it's like,
no, it's not selection, but yeah. So, but it's local anyway. It's pure. Yeah. In this case,
it's purely local. Now, one of the things we're going to do next is we're going to implement
this kind of a stress mechanism. So, so I have this other wacky idea that one way to coalesce
individual cells into common purpose is to let them share stress. Because if I'm really unhappy,
if I'm a cell and I'm really unhappy about where I am, and you, my neighbor are perfectly happy
where you are, you're not going to let me buy. And I'm not going to get to move where I need to go
because you're, you're fixed in place. Why should you move? But if I am stressed and I can
let that stress leak out and start to stress you out, then you become a little more plastic too.
Because you are getting the same stress molecule that I have. You don't know that it's not you
being stressed. You know, as far as you're concerned, now you're stressed. So you're a little more
plastic and now I can, now I can get by. So it, by my problem becoming your problem, it sort of
binds everybody to a common purpose. Every, you know, the temperature sort of goes up, right?
And I, you know, and everybody gets a little more plastic. And then when I finally get to
where I'm going, everybody's stress can, can drop because I'll stop stressing everybody out.
So that's, so, so now what, what he's going to do, the, the, the lecturing is they're going to,
put the, put in this mechanism where basically evolution gets to say,
is the stress leaky? And if so, how leaky? And then we'll find out whether you can actually
use this sort of stress mechanism to, you know, and, and, and we're doing that experimentally too,
right? So, so experimentally, if you put a, if you put, if you make the eye of the tadpole on the,
you know, off to the side where it's not supposed to be, it'll eventually sort of move to where it
needs to go and everything rearranges. And, and so we need to now look at, we need, we need to
track the stress markers during this process, you know, is, is their tissue level stress when
nobody, the cells are not dying, they're not poisoned, DNA is not broken, no, no heat and
none of that, but the eyes in the wrong place, right? Is that stressful for, for at the tissue
level? So we're going to, we're going to find out, we have some preliminary data already, but we'll
find out. Yeah, I think this can perhaps be formulated as saying, not only does competency
involve this ability to perceive and act in a certain limited way, but it also involves the kind
of very particular perception action loop that we call communication. So spreading the stress
around is just a special case of communicating something to the neighbors that they can then
confirm, perhaps by using their communicative competencies as well.
I mean, I'm very reminded in looking at this experiment, like, of human evolution.
Evolution, in a sense, working on our cognitive competence, as opposed to, you know, turning
us into creatures that can run faster, bigger weights or fight better, have bigger teeth or
whatever. I mean, evolution didn't do those things. It worked on learning ability and general
cognitive ability instead. So, so that's a nice example. So, you know, I, in my notes, this thing
I say about the R selection and K selection just occurred to me this afternoon. It's not exactly
right, but I think you have the same kind of dichotomy here. And, you know, selecting for a
genome or selecting for a phenome. And like you're saying, Chris, you know, the brain did it one
way. And so he got this rapid evolution. There's this guy at Berkeley, he was kind of pushing that
idea 30 years ago, and he died prematurely. What was his name? I've forgotten. But, you know, the
idea, would you know this story? I'll dig it up. Alan? Oh, well. I'll dig it up. And then
that could then go faster than the genetic evolution. And I guess the humans are doing it one way,
you know, be careful. And then the plenaria are doing it another way. The brain has decided to
do it the plenaria way. By the way, so your competence in your computer experiment, Mike,
you're swapping positions of cells around, but it could just as well be differentiation states of
a cell, right? Saying that, you know, you just do some other differentiation. And then that's
could well be what a tumor is doing now. And so you got the society of cells that's
doing that kind of evolution. And then that now gets you into this whole other issue of
is there a direction to all this?
Yeah. I mean, it seems to me this thing has that it seems to me to be a ratchet. I think it does
have direction, right? Because once you start, it's very hard to go back once you once you start,
once you have a little bit of competency, and you have trouble seeing the judging the genomes. And
so you keep going. It's very hard to. So Steve Frank gave me this amazing example where,
you know, rate arrays in computers, right? For disks, you have these arrays of drives. And
basically, there's a parity system where if you make an if there's an error on one of the drives,
you'd correct because you have copies of the data elsewhere, right? And so it's like this.
So what he was pointing out is that once these rate arrays became popular, the quality of hard
drive media went down because it was because it was no longer that important to have, right?
Because you fix it all in software. So now, but now you're trapped, right? Because if the
quality of your disk is crap, you can't do away with the rate anymore. It's not going to work.
So, so it's kind of a one, you know, it's kind of a one way ticket, but, but clearly some species
sort of flatten out, you know, Plenaria went all the way with this thing. But, but, you know, we
can do some of that. And certainly, and the salamanders can do more. But, but Plenaria just
sort of took it took it all the way. So it does seem like this has a direction to me. To me,
this is a direction and an arrow for intelligence, just just, you know, baked into this whole thing.
So that's nice. So long ago, again, conversations with Kornak or back when I was a grad student,
it occurred to me that most of evolution, particularly if you look at the nervous system,
you could describe it, you know, so people are always asking, is there a direction, you know,
what's higher mean? And it struck me that what it is is separation of functions. You have one part
of the brain in particular that used to do two things badly. And now, if you go from chimps to
humans, you have two parts, they each do one thing better, enzyme to the same way. And so you
could pick a direction. And so then all you have to do is say that, well, one or the other of these
schemes is going to have the net phenotypic result of giving you separation of functions.
And then separation of functions gives you, you can now control them independently. And that
looks like intelligence, or it looks like up. Yeah, that's interesting. Yeah, the classic example
for me, this one of several things I should have written and never got around to the
the red nucleus, there's a pars magnicellular and a pars parvocellular, you know, big cells and
little cells. And in monkeys, there's just one nucleus in humans. And then they go to two different
places, you know, the connection goes to different places. In the humans, you have a center, which
is I think the parvocellular and surrounded by the larger cells, the magnicellular. So there's
now an anatomical distinction. And they project to two different places, or they go to the same
places they did, but now they're separated, and now they have separate controls. And then once
you start looking at the nervous system, you see all kinds of things like that. And then
end time to the same ends on the same line. And then what you were saying about the stress
response, that looks like it goes back to a Waddington small house and Baldwin again. And then
I noticed today, when I was rereading your paper, that well, what you're saying is, you're going to
do all these swaps before you select. And so you say it, well, okay, that gives you a chance to do
stuff before you can get killed by the selection pressure. And that's essentially what Waddington's
environmental inducibility is doing, saying, okay, if I'm going to develop calluses on my
ostriches, but it's only inducible, and there's a bad side effect to it, no problem, it's inducible,
it's not on all the time, get away with it until I make a perfect callus by mutating three other
genes. And then again, I think the same argument holds even better for cancer, where you don't
have a million years, you have to do it in like six months. Yeah, I mean, I think I think you're
right that that kind of like capacitor function is definite. So if you take, because so let's say
that I in the tadpole, you or the or the mouth, you know, you make a mutation, and it has some
kind of positive effect somewhere, but also makes the mouth off a little bit. Well, that mouth is
going to get back to where it needs to go before the embryo has to eat. And so that means that
whereas before you would have had a deleterious mutation that would erect the whole thing, and
you would have never seen all the positive effects. Now it buffers it. So a lot of these mutations
become neutral, whereas they would have been negative before. And so you get to you get to
explore whatever else this thing is doing, because because nowhere is the mouth will find where it
needs to go. So I think I think that that competency at the tissue level, very much smooths and sort
of makes the search space much nicer, because all the things that otherwise would have would have
killed you and now would have been that now you have the ability to tolerate it and maybe use it
later on. So it lets you carry all this all this stuff at the genetic level. Again, it isolates
you from from from mistakes at the genetic level. Because because, you know, if your cells are
slightly the wrong size, your kidney tubules will still figure out how to be the right diameter.
And if your mouth is off, it'll come back. So that's the answer then to Bayhese intelligent
design argument. I think so. I think so. So this is something that's interesting. I've been talking
about this with what with Richard Watson, which is that we now know from from computer science and
other things that you can have an intelligent system that is made of less intelligent parts,
you don't need everything to be intelligent. And so to me, the thing about evolution now,
what two things one is, I think whatever whatever intelligence it has is provided by the fact that
the part you're dealing with an agential material, you're not dealing with passive Lego blocks,
the evolution is search, you know, so all these cells used to be independent organisms. So they
have their own agendas. And so what evolution is really searching is this like space of behavior
shaping signals so that the cells can get each other to do various things. And that's where the
intelligence comes from the mutations sure they're random. That's fine. Nobody has to keep track of
the whole the whole thing. But but there is a degree of intelligence to the process because
the parts are able to make up for errors. And it doesn't tell you where you're going to end up
necessarily. But it does tell you that the process isn't as blind and stupid as you know,
as it's made out to be. But nor does it need to be, you know, super human level intelligence.
It's just, you know, it's a little bit it's got a little bit of smarts a little bit.
Well, I think also an aspect of this is that the parts don't need to be intelligent
in the same environment that the whole system is right. And in fact, they can't be
they need to be intelligent in their own environment, which for cells is an environment
of other cells, some of which are similar and some of which aren't. So you can expect the kind of
social relationships, modulo their communicative strategies and abilities that that that we have.
But so they might use the same kinds of tools in terms of social communication that we do.
But the environment itself that they're communicating about is very different.
Yeah, the problem. Go ahead. Well, I was just going to say, yeah, the problem space is different.
And we have another paper, a preprint out this, my postdoc Leo has the system where
you've got cells and the cells are operating in metabolic space. So all they know how to
their little homeostats and all they know how to do is try to get more food.
But the collective as a whole makes a French flag developmental pattern. So you see how you shift
problem spaces, right? So the cells of these little tiny goals local, and they're competent in
that they don't know anything about French flags or whatnot, but the collective is able to do that.
Right. And so and so I think, right. So Chris and I had that paper on
navigating different spaces, right? So evolution kind of pivoting these tricks
from one space into another by basically the same kind of scale free dynamics.
So do either of you have an opinion as to whether if you're trying to, well, I shouldn't say trying
to and I shouldn't say get to how to say it. If there is some larger concerted behavior,
do you have an opinion as to whether that's because of some goal and set point and cybernetics,
or whether it's an attractor that really doesn't know where it's heading? Chris, you want to try
first? I'm not sure. I don't see the distinction between those two alternatives. Yes. So yeah.
So I hadn't thought about it till recently. I think that or it's probably because I don't understand
attractors well enough, but for a cybernetic system, you have some sort of set point, which is
stored someplace, you have some sort of input signal, and you have somebody comparing the two,
and then telling somebody else which direction to change to reduce the difference. So you have some
goal, which is arguably some representation of some future state sitting there and some
computational apparatus. And then as I understand it, and I don't, attractors are pretty much like
a gravitational hole, and you just sort of get sucked there. Well, in a sense, the attractor,
you can view the attractor as a representation of a goal state.
And of a goal state. Okay. Yeah. And each particle or agent or whatever you want to call it that's
moving within the field of the attractor just has to make a local measurement about whether it's
going up or down. So it tells itself what to do based upon where it finds itself to be.
So there is a comparison, but it's to the local state, and the attractor is set up some kind of
gradient. Right. Okay. So I'd like to look at it slightly, slightly differently, which is somebody,
and I wish I remember who it was, but somebody had this amazing phrase that said that it's the
continuum between two magnets trying to get together, and Romeo and Julia trying to get
together. Right. So the difference is, right, there's a continuum. And the difference is how much
basically how much cleverness or competency do you expect along the way. So I think the system
that Chris just laid out, if that, in systems where that's true, that's kind of the magnet case,
and all they sort of can do. And if you put a barrier there, that's it. They're just going to
stay there pressed up against the barrier, they're never going to go around, they're never going to
do anything clever. That's all they know how to do. So that's sort of on the left of that continuum.
And then you, and then all the way on the right of that continuum are some super sort of smart
systems that not only do they see the gradient, but they have delayed gratification, so they can
sort of avoid being trapped in local minima. And they maybe have some planning and some
metacognition and some, you know, who knows. And then in the middle, you have all kinds of simpler
systems. So that have some of those capacities, right. So, so, so maybe all you're doing is following
a local gradient. But for example, we've been, we've been finding all kinds of examples, some of
them extremely simple things, like very simple algorithms that when you actually play them out,
it turns out, no, actually, they have some capacity to resist perturbations. And so,
to me, one of the things, like a lot of people will look at a system, and they will immediately
have a built in feeling about where that is, oh, that can't, you know, that's just, you know,
that's just dumb chemistry. That can't whatever. But I don't think you can ever tell unless you
start probing it and the way you probe it. And so I don't think you can tell from purely
observational data. I think you have to start probing it by getting in its way in various ways
and seeing what it does. How much can you get this thing to resolve the problems that you're
giving it? And does it always do the local gradient thing? Or, you know, can it, can it,
can it do something a little more clever and get around when there's a, and there's,
and there's a wide variety. So in biology, like one thing you can do is start everything off in
the wrong position. With the metamorphosis in the tadpole, it used to be thought that
in the tadpole, all the organs, all they know how to do is go the right way in the right direction
and for the right amount of time. And that's it. And so we scrambled them all in different
positions. And we found out, no, actually, they will keep going in weird paths to get to the right,
to get to the right place. So already, you know, it can't be as simple as all they know how to do
is go in the right direction. They're context sensitive. And there's tons of systems like
that where they look like that's all they know how to do. But when you interfere, you find out
that they actually have all kinds of capacities and they're sort of further along, you know,
they're further along than you think on that spectrum. So the difference between those two,
that's a nice analogy. The difference between the two is that then the space between where you are
now and the actual endpoint, the attractor, if it's an attractor, that intervening space is also
already programmed or deformed or whatever. Whereas in the Romeo and Juliet case, it's not.
You know, you have alternatives. Yeah, you have different different capacities to navigate that
space. You have different strategies. And I mean, people who build autonomous vehicles have,
you know, they think about that stuff all the time, right? You want to get from here to there,
but you're not just going to do as the crow flies unless you hit a barrier, you've got all sorts of
different tricks up your sleeve about both local and maybe global, sometimes global information
too, sometimes not. But, you know, one in particular that I really like that we use a lot is,
well, that is, is, is I, for lack of a better word, I call it delayed gratification. It just
means, are you able to temporarily get further from your goal in order to then do better later?
Some systems can't, right? They're always, you know, and I've got, I've got a great picture I took of
them, two dogs across a fence, and they're trying to get at each other. There's a hole in the fence,
five feet down. But in order to do that, you have to, you have to get away from where you want to
go. And they're just like, they're like this, right? And maybe they'll figure it out eventually,
or maybe they won't. But, but, but that's that ability to get further away is, is I think pretty,
pretty critical. It's a basic capacity not to get trapped in that local, you know, that local,
local optimum there. And then there's all kinds of complex tricks after that.
So an end run, basically. You may know this already, but primates and birds can do that,
and other animals cannot. No, the classic one is you, let's see, there's this experiment.
You, you have food, the monkey's in a cage. There's food outside, just outside his reach.
There's a stick behind him. And so is he going to figure out that if he turns around, grabs the
stick and then goes back, he can get the food. And so evidently primates and birds can do that,
and other animals can't. Yeah. Yeah, I think in, in terms of this landscape picture,
when we think about an attractor, it's typical to imagine, you know, a potential minimum,
and then just a smooth surface coming out of it. And that's the, the simple picture.
Whereas in, in real life, even if you have a fairly deep attractor like Romeo and Juliet,
the, the surface coming out of it is very complicated.
Oh, bumpy attractors. Yeah. With, with all sorts of, you know,
channels, some of which might be solenoidal even, to keep you from ever getting over the edge.
And so these competent systems have to be able to increase their temperature as, as
Mike was referring to earlier, to, to get out of, of local minima
in this, in this very complicated landscape that surrounds wherever it is they want to, to go.
But it's also the case that if you think in, in terms of a static landscape,
then that corresponds to an environment that's not malleable by the organism. Whereas again,
in real life, what the organism does in navigating the space not only changes its position in the
space, but it also changes the shape of the space. The organism actually modifies the environment
by taking each action, not just its position in the environment.
And you can change where the attractors are, I suppose.
Right. So, so the organism can even make the attractor move around based on what it's doing.
Yeah. If it's a sufficiently competent interactor with its environment,
and if the environment is sufficiently plastic. Yeah. So we see that in social interactions,
for example, the environment, the social environment is incredibly plastic.
We can change it in various ways just by talking. Yeah. Whereas, you know,
the environment of the desert is not all that plastic, you can wander around,
you don't change where the mountains are.
And so, and so you can imagine, right? So this is important during evolution,
I think, because you can imagine, so, okay, so here I have my thermostat,
and all it knows how to do is check the set point and get the temperature, right? Yeah.
And then there's a more advanced version of a thermostat, which also has a
metacognitive module that makes sure that the set point isn't being twiddled too much,
right, that monitors the set point. Now, why do you need that in biology? Because,
because you're constantly at threat of parasites and exploiters of various kinds.
So the minute you become programmable, like, like, you know, like a thermostat,
somebody out there is going to want to change your set point. And so you need to be able to
resist that. And in a more advanced version, you need to be able to tell whether it's you
resisting it, whether, whether any changes that are made, are you doing it, or is it
somebody else doing it to you, right? I think there would be a lot of evolutionary advantage
to knowing why did my set point just change? Did I change it or did somebody else change it for
me? Am I being exploited in some way, right? And as soon as you say that, then it starts to make
sense that somebody, I was just talking to Eric Welker, and he asked me if, if I thought that how
important threat perception was to, was to this whole process. And I was saying that I think
it's absolutely critical because if you weren't in, in danger of being hacked by, by, by competitors,
you wouldn't have to have a strong sense of self. In other words, you could remain an ocean, right?
And so, so, so, so I'm, I've just been obsessed with this thing that I'm, you're an embryo,
a blasted disk, you know, so 50,000 cells. And at some point you look at that and it becomes
one embryo. And so you look at then you say, how many embryos are people say one embryo?
What are you counting when you say one embryo? Well, what you're counting is,
is the following. A bunch of those cells are going to merge together to, among many other things,
determine a barrier between themselves and the outside world. So every cell is some other cells
neighbor. And so where do I end? And where does the world begin? Well, the embryo is going to
make that determination. And you'll have one embryo. If you take a little needle and you make
some scratches, and I used to do this in grad school, when working with in avian embryos,
you make some scratches. And until the scratches heal, every individual region which can't hear
the other regions becomes an embryo. And when they do heal, you have conjoined twins or triplets.
And so the question of how many individuals are in that embryo is not known in advance. It's not
genetic. You don't know how many it is. It's something that emerges over, over time, right?
So, and to me, that has, that has really interesting implications for the brain. The same
deal. If I show, if you didn't know what a human was, and I showed you a three and a half pound
brain, I said, how many individuals in there? Who would know who knows how many are going to be
in there, right? It's this process of taking individual pieces and pulling together some sort
of unified something that distinct from that forms some sort of a thing that makes a model of itself
as distinct from the environment. So if you weren't a threat of being hacked, you wouldn't need to do
that. It wouldn't matter where you wouldn't, you wouldn't have to maintain a strong self boundary
because you wouldn't need to know why things are changing because nobody would be, nobody would
be trying to exploit you. But of course, in biology, that doesn't, that doesn't work. You're
always a threat of being hacked. And, and then, and then that, I think that drives this requirement
of determining a strong self identity and then having ways to understand whether, you know,
am I learning or am I being trained? Right? What is the, I mean, I think this is a very
important point that Chris was was saying about the environment. If it's just you and the desert,
the environment is a very low agency thing, and then you can assume that you're the boss,
you're learning, I'm deciding what I pay attention to, I'm learning. If you're in a social milieu,
and you're learning, it could well be that you're actually being trained, right? The partner on the
other end is a high agency thing, maybe higher than you, and maybe you're, maybe you're being
exploited. And so, right, somebody said, I forget who it was, but somebody gave a talk on how to
give talks. And he said, he said, every act of writing is a violent act, because what you're
hoping to do is to change the listener's cognitive structure, right? Your success means I've reached
in there through my signaling, and I've, you walk away from there altered, you believe things you
didn't believe before, that's a successful talk, right? So, yeah, anyway, so that's what I think.
So, I think that threat from hacking is maybe what drives, I wanted to show you guys a cool
picture. I mean, this is something else I've been obsessed with. Where is the, oh, here it is.
Check this out. So, this is, you see this thing, this is a gall formed on the leaf by,
what happens is this wasp embryo induces the flat green cells of the leaf to form this crazy
thing. This is what we're up against. This is, right, is if you're a cell that's smart enough to
form a leaf, you're also subject to hacking and being made to make something completely different,
right? And that's the arms race you're in. And now every single that comes in, I think,
you now have to decide, is that me changing what I do, or is that somebody changing me?
So, I don't know that that probably has various psychological implications too.
You know, where threat is threat level is somehow proportional to a sense of ego separation,
as I mean, I'm out of my, you know, pay grade here, but there's, you know, something like that.
So, that gets into the thing I was going to raise. Oh, do you want to say something?
Go ahead. Oh, okay. So, I raised this issue of defining a thing, which gets sort of to your
paper about objects. And that was one reason I was so interested in the goal versus attractor thing,
because the goal seemed to me to require an internal representation of some sort. Whereas
the attractor does not. And then Rodney Brooks, for example, with his robots is maintaining
that there isn't any internal representation. And just speaking for myself, I'm pretty sure I do
have internal representation, mental representations. And so, and I think that's a key part of
cognition. I'm happy for Rodney Brooks's computer robots not to be cognitive, but
you know, we're interested in that question. So, then what is it? And so,
Frank Forster got me thinking about things years ago. He had lots of little pieces to the problem,
but I think never put his finger on how to put them together. And the thing that occurs to me
for things, you know, you tend to define a thing you think about the edges and so forth.
And I think what it is is actually a little different. You have a list of properties.
And then you divide that list of properties into two columns, those that are essential,
which I call specified, and those which are allowed to change without you saying it's no
longer the same thing. I call those substitutable. So, that lets me take off my hat or put it on.
It lets an apple to have a drop of water on it, and it's still the same apple. My dog,
my daughter's dog, has trouble with the hat thing. If I put on a hat or if I put on a mask,
I get barked at. But so, I think that's the primordial cognitive distinction
that you make is the separation to specify and substituteable. And then one question I had
for, well, and one place that went is, okay, if you do that, now you can start out with Percepts
and build up this hierarchy of what I call constellations and things which can now change.
Quantum mechanics is full of things that only have one property and so they can only be created
and destroyed. But as long as you have both columns, things can now change, and you can
just build up this whole cognitive thing. And then one day I wondered, gee, I wonder if language
works the same way, you know, in the same hierarchy. And to make a long story short, but I can show you
because it doesn't take too long. If you say, okay, that's how language works. And if you have a
template that's a lot like the genetic reading frame, you know, which, you know, like three codons,
you know, codons to three bases, there's a similar thing for natural language. You can write,
well, I didn't write, a friend of mine wrote,
a three megabyte program that parses human language. I could talk about that in a minute
or show you if you want. But with regard to the thing issue, you're pointing out and you pay,
I have to read your paper a couple of times, Chris, the one on objects. But this issue of
superposition, it seems to me that before you can talk about an external thing, you have to solve
this superposition problem. And I didn't quite understand how it got resolved. But one thing,
again, that Heinz used to emphasize was, you've got two eyes, two ears, they're giving you
conflicting signals. So if you have two measuring instruments, does that solve the superposition
problem? And let you talk about an external object. Well, let's see, let me say several things.
One, your distinction between constant and variable properties of objects
is what we refer to as reference and pointer degrees of freedom in our work.
That's physics language that comes from the pointer on an old voltmeter or something. That's
the thing that moves that you're interested in. And everything else you're not interested in.
It's just what lets you identify the instrument. And physicists tend to ignore the non-pointer
variables, which I think in large part is responsible for the quantum measurement problem.
As a philosophical and theoretical issue,
because if you have to identify the system, it clearly can't be in a superposition of any
of your identifying variables. If I identify my laptop by its position on my desk,
then it can't be in a superposition or I'll never identify it.
That's my criterion. So the superposition has to be in the pointer variables?
You can only see superpositions in the pointer variables
by definition. Yeah, that's nice. So in a sense, as you were pointing out for elementary particles,
you do have to have two measurement instruments or reference frames or concepts or whatever
you want to call them. They do have distinct semantic roles by definition. So they aren't just
syntax. And one of them has to be a reference that you keep fixed
in order to identify the system. And the other one you can allow to be variable to get some
interesting information. I mean, the system having the properties that define it
is not interesting information. The system being identifiable or not is interesting information.
Right. Okay. If you look at things like electrons,
identical particles of any kind, we distinguish them by spatial position,
which if you take a field theoretic perspective is a completely artificial variable.
Now, they're in fact identical things and the fact that we think that one of them's over here
and one of them's over there from a field theoretic perspective, these are just field excitations.
Okay. So there's no such thing as a distinction. You just have two of them.
So, yeah, I think this is all very much on the right track from a physics perspective,
that when one has to include these kinds of distinctions in a description of the physics
itself, or you get things that don't make sense, like the measurement problem.
Is that still considered to be not understood?
Oh, yes. It's very much, I think everything I'm saying is very much a minority position.
Yeah, most people don't talk about system identification in physics at all.
Engineers talk about it every day. Oh, really? Physicists tend not to talk about it at all.
Oh, I see. You mean the engineers saying, okay, here's the one box, here's the other box,
and they talk to each other. And here's how I tell them apart, and here's how they tell each other
apart. Right. And the physicist is saying, well, okay, here's the needle, but never mind the housing
and all the rest of that. Right. Yeah. You'll see papers that say that explicitly.
There's a famous, there are two very well-known papers by Max Tegmark at MIT
that talk about decoherence. And in both of them, he uses exactly the same diagram,
which splits the universe up into three pieces, the part of the observer's mind
that observes the pointer state, the pointer state, and then everything else, which is the
environment. So, that's the whole object identification process just into some feature
of the external environment that we won't talk about.
You know, I love the distinction, things you can change and things you can't change,
and also telling things apart, because in biology, I think what we see is it's like
the ship of Theseus business, right? So, in a body, what varies? Well, all the molecular details
vary all the time. Molecules come and go. In fact, cells come and go. So, you have to have a system
that stays the same. So, what stays the same? The higher level system stays the same. On the
lower level, nothing stays the same. Everything is different within a couple of years. You're
all swapped out, as I understand. So, this leads to a couple of, I think, interesting things. One is
that in a cognitive system, you have a similar scenario where, if you're going to be a coherent
mind, what comes and goes are different thoughts. You don't want to be different every time a new
thought comes in. Well, that's it. Now I'm done. So, you have to have some kind of stable structure
that can persist, despite the fact that new thoughts, new experiences, all of this is going to come.
But something has to stay. There's some kind of higher level structure has to stay. So,
this question of what are we invariant to and how do we tell the difference? So, when I see you,
I'm not a Laplacian demon who says, well, that's not, Doug, because all the atoms are wrong. So,
that's it. I'm fooled. No, I ignore the microstates and they say, no, that's definitely you. And then,
cognitively, this is the same thing. Even though you've had a million thoughts and maybe today's
thoughts are different than yesterday's thoughts, we can still sort of recognize each other.
So, I think that's interesting. And then, going back to the pointer thing, another thought at the
physics instrument, I wonder if the reason why for physicists, everything is sort of bottom up and
low agency or zero agency and mechanical is because all their tools are. And right, the voltmeter
and things like that are very low agency things. So, of course, they only measure the microstates
of things. You need a completely different apparatus like a brain or an artificial neural
network or something to detect these high level invariance like a being whether a body or a
cognitive being that does not change when the parts swap out, right? So, physicists have no,
there is there's no apparatus that you have that will detect these virtual these large scale virtual
governors, right? You don't you don't see that all you ever see is microstates because you can tell
me if I'm if I'm wrong, because because what you're using is apparatus that always looks down at that
level. But but for example, you if you're a biologist, you can use a different apparatus that's
very good at detecting these things, right? So, so the brain is really good at detecting agency in
the environment, it skips over all the details to that extent and right. And so you use a different
anyway, that's that's that's that's where I, you know, my mind went for the stuff.
That's nice for two things. One is, for the physics instruments, they're simple like that so
that you get an unambiguous result. If it measured 10 things, you'd never really know
what you had measured because you got some function, your output is some function of 10 things.
Conversely, I realized as you were speaking, that we never ever, we always say, gee, I had a thought,
we never say my mind changed. So these thoughts are something we project into something external,
even if it's inside our head, it's external to me. Well, that's an interesting question,
right? Can you so so we have a feeling of we all sort of have an innate feeling of free will
normally. But we know you cannot control the next thought that you're going to have.
It, whatever pops up is what pops up, you have some sort of long term control in the sense that
you can undertake practices that will change the statistical distribution of future thoughts you
are likely to have, whether by education or meditation or whatever. So, you know, you sort of
have some hope of long term changing the ensemble of your thoughts, but you have no hope of controlling
what your next thought is going to be. And actually, this goes back to the other thing
about you saying that you had representations. So I also feel like I have representations. But
for example, Nick Chater would tell you, you don't have representations, right? So he wrote this
this interesting book called The Mind is Flat. And so, and Mike Kazanaga has a model of this too,
that where basically, there's a part of your brain that does things. And then there's another part
that tells stories about why they happened. And right. And so he would argue that, and I'm not
saying I go all the way with him, but the argument is that the deep underlying stuff is a
post hoc sort of story. And that basically, you know, and you've got experiments. So here's a
simple experiment. And there's a video of this on YouTube. It's an old experiment. There's an
electrode in the brain of somebody who I think was being treated for epilepsy or something.
And it happens to be in a center that makes him laugh. So when the experimenter pushes the button,
the guy starts laughing. So he's sitting there thinking about something serious,
the experimenter pushes the button, he starts laughing, you ask him, hey, why are you laughing?
The answer is never, I don't know, I was thinking about serious things, then my mouth
starts suddenly laughing. That's never the answer. The answer is, oh, because I thought of something
funny. That's always the answer you get. So this confabulation, right? So we're definitely good
at telling stories, post hoc about stuff that happened, whether that's all there is, I don't
know. But but some of it is, I think some of it is some of that. Daniel Dennett had a little bit
of a story sort of like that, that's making up stories about what your brain's saying. I mean,
I could buy that one thing that the brain top level does have to do with regard to the things
and say that all your agents are down in your joints and the robots or wherever else.
Or even if the various properties that I'm assigning to a thing, one of them comes from
the visual part of the brain, another one comes from the tactile, somebody has to say, oh, well,
I'm going to say those properties all belong to the same thing. Somebody has to do that. Otherwise,
it's just your different joints doing whatever they do. And so that may be what the top level guy up
here is doing. Well, I think Dan would say, I think Dan would say there is no binding problem
because there is no binding. I think he would, I think he would say that it seems like there is.
Now, now that brings up another question of seems to whom, right? So so I don't know, that's that,
that's still I still find that a little tough. But but but but but he's been battling for years,
this idea that there is any executive control that binds it all together. And basically,
basically saying that, yeah, it's a set of parallel processes, which occasionally sync up
with each other long enough to tell a coherent story. But mostly, it's, you know, just independent
modules that do stuff. Hmm. Interesting. I didn't know it was that extreme.
I mean, there is that extreme view that, you know, who knows where the truth is, but
interesting. So there is, there is also a lurking ambiguity in the use of the phrase of the term
representation that that that comes down to the question of representation for whom.
You know, take take a story like shaders, which which I do think he has a lot of it.
Correct. You know, this this meta processor is constantly for some of the time, whatever it
needs to constructing some story about what the brain is actually doing, what the organism is doing.
And it, it does that in some kind of representational language.
For example, self dialogue, in the case of humans.
And nothing in the lower level computations is using that particular representation for anything.
However, the low, the lower level processes are full of representations that they are using.
And so, you know, for example, retinotopic maps in the visual system are representations
that are very straightforward sense. Some matter, sensory systems have representations
of the body in a very straightforward sense. And those representations are never
accessible to the to this meta processor that's telling a representational story about what the
brain is doing. They're only accessible to neuroscientists, for example, or other levels
of the low lying cognitive system itself, right there. The representations that the brain is
using, but they're not representations that we can talk about in our own case.
So, and, and that's what's super interesting. This, this idea of representation to whom
is really interesting. So, Josh, bond guard, and I just wrote this paper on
poly computing. And the idea is, is there an objective story, an objectively true story
about what a given algorithm is computing. And so he's got this really cool data paper where you
can show that he's got some sort of poor, you know, a porous medium that if you look at it one
way, it's computing one logic function. If you look at it a different way, it's compute the
exact same process is computing a different function. And so now you've got this idea that to
even say what the what such a simple thing is computing is also observer dependent. And so
you could imagine, right, that the brain and everything else in our bodies, there are multiple
observers that are looking at these things in different ways and reaching different conclusions
about what the heck is is going on, right. And it's we and we we looked at, for example,
gene regulatory network models, and you can do the same thing, you can say without changing this
thing at all, can I find a perspective from which the action of this thing looks like
associated of learning. And there are six different kinds of memories that you can find,
if you just look at it in the right perspective. And so, and so then then you reach where I think
this is kind of an interesting philosophical question. Somebody writes an algorithm, and you
reinterpret it a different way. Is your opinion of what it's computing as valid as the guy who
wrote it, because, because that person will disagree, he'll say, what are you talking about,
I know what it does, I wrote the darn thing, this is what it does. But, you know, you're free to
interpret it in a different way, if you can, if you can make it work out for you.
So, right, so that I think that maps on to the to the neuroscience nicely.
And why and the other thing that bugs me, this has always bugged me is, if that's true,
why is it so easy for our verbal module, whatever it is to interpret our own brain states,
and it's so damn hard for neuroscientists to interpret somebody else's brain states,
right, like neural decoding is really hard. But we do pretty well, like not, you know,
not perfectly, of course, but I think we do pretty well with our own brain.
So that's, I hadn't thought about it that way. So I was, I would argue that at the top level,
anyway, there's a format by which we represent things. And then I use the same format and apply
it to language and it worked. And the format is simply like an engineers is an entity, another
entity and a relation between the two of them, like earth, sun, gravity, chair, the seat of the
chair or the back of the chair and a bolt holding the two in a particular relationship.
And then the question is how far could you go with that. And so, and then each of the entities
has to specify the substitutable division in it. And so that's what I built things up with.
Then it turns out that for language, you can do that in English.
Because the words are all ambiguous, or almost all words in English have multiple meanings.
So what that tells you is you need an exogenous structure, an external structure,
as the words are coming in, in order to sort out whether something's an entity or a relate
an entity word or relation word or whatever. So you have this track like the reading frame
just divided into entity relation, entity relation, like that. And as the words come in,
you drop them in. And then, so like, we have no trouble at all with the sentence,
he saw that I saw, he saw the saw that I saw, right? We had no trouble with that.
And so then so I'd argue that there's some neural thing that is creating that
track. And that beyond that, that then imposes the semantics once you've decided that something's
an entity rather than a relation, then, you know, that's a thing. And then what both cognition
and language are doing is building hierarchies out of these three piece pieces. So you've got
entity relation, entity, another one over here, and there's a relation between those,
and now you build a bigger one. And then the only reason that language looks complicated
is that there's some data compression and we leave stuff out. But it was not too hard to come
up with a table of what got left out. And so that for particular words, you can build a dictionary
and then put the things back in. And essentially, what happens is we leave out
words for system component relations. I know he can drive a car. I know that he can drive a car.
Well, we leave out that all the time. And the same thing between adjectives like a red apple
is essentially an apple that is composed of the property of redness. And so anyway, so if you
adopt this, then it all becomes very simple, you just drop things into this reading frame,
and then you just pop stuff in whenever something has been left out. If you get
two nouns in a row, so you can use the table and pop them back in. And then, you know, the argument
then becomes, oh, well, if that works, then is that how we think about the world? And oh,
I should say, English works that way. Japanese is entity-entity relation. And there are languages
that do it that way also have word endings, so that you know where the first entity,
and so you know where the first entity ends and the second entity begins, even though you
don't have a relation between them. Whereas languages use entity-relation entity, they're
using the relation itself to separate the two. And like I said, we've got a three megabyte
program and a three megabyte dictionary. Oh, so Mike Castellini. So anyway, so I wondered whether
your brain has a format in it that lets you do both cognition and language. But as you point out,
that's not probably at all what the somatotopic representation is in your brain. So this would
be a top level thing. So Mike, thanks. Yeah, thanks guys. This is amazing. So yeah, you know,
let's keep talking more. We can do more. Super interesting. I think the language thing has a
lot going on there. We should maybe talk a little bit about the large language models later on.
I'd love to hear what you guys have to say, you know, GPT-3 and all that. There's a bunch to talk
about. Yeah, it'd be interesting to try to apply these kinds of structures to
cell-cell communication languages and much simpler languages than human languages,
which I always advocate starting with something simpler than the human case.
The complicated things don't work. So anyway, yeah, so this has been terrific. So anytime you
guys want to do it again, and maybe you could do that, maybe the topic and start with the language
thing. I don't know a whole lot about the big models, except that they have amazing failures,
amazingly simple and obvious failures. And so it seems to me they haven't got it totally nailed down
yet, but anyhow. Okay, super. Thanks guys. Thanks so much. Have a good one. Nice to meet you. Yeah.
Thank you. All right. Bye-bye.
