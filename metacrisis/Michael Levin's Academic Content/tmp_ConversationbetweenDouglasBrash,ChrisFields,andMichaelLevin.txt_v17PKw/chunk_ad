exact same process is computing a different function. And so now you've got this idea that to
even say what the what such a simple thing is computing is also observer dependent. And so
you could imagine, right, that the brain and everything else in our bodies, there are multiple
observers that are looking at these things in different ways and reaching different conclusions
about what the heck is is going on, right. And it's we and we we looked at, for example,
gene regulatory network models, and you can do the same thing, you can say without changing this
thing at all, can I find a perspective from which the action of this thing looks like
associated of learning. And there are six different kinds of memories that you can find,
if you just look at it in the right perspective. And so, and so then then you reach where I think
this is kind of an interesting philosophical question. Somebody writes an algorithm, and you
reinterpret it a different way. Is your opinion of what it's computing as valid as the guy who
wrote it, because, because that person will disagree, he'll say, what are you talking about,
I know what it does, I wrote the darn thing, this is what it does. But, you know, you're free to
interpret it in a different way, if you can, if you can make it work out for you.
So, right, so that I think that maps on to the to the neuroscience nicely.
And why and the other thing that bugs me, this has always bugged me is, if that's true,
why is it so easy for our verbal module, whatever it is to interpret our own brain states,
and it's so damn hard for neuroscientists to interpret somebody else's brain states,
right, like neural decoding is really hard. But we do pretty well, like not, you know,
not perfectly, of course, but I think we do pretty well with our own brain.
So that's, I hadn't thought about it that way. So I was, I would argue that at the top level,
anyway, there's a format by which we represent things. And then I use the same format and apply
it to language and it worked. And the format is simply like an engineers is an entity, another
entity and a relation between the two of them, like earth, sun, gravity, chair, the seat of the
chair or the back of the chair and a bolt holding the two in a particular relationship.
And then the question is how far could you go with that. And so, and then each of the entities
has to specify the substitutable division in it. And so that's what I built things up with.
Then it turns out that for language, you can do that in English.
Because the words are all ambiguous, or almost all words in English have multiple meanings.
So what that tells you is you need an exogenous structure, an external structure,
as the words are coming in, in order to sort out whether something's an entity or a relate
an entity word or relation word or whatever. So you have this track like the reading frame
just divided into entity relation, entity relation, like that. And as the words come in,
you drop them in. And then, so like, we have no trouble at all with the sentence,
he saw that I saw, he saw the saw that I saw, right? We had no trouble with that.
And so then so I'd argue that there's some neural thing that is creating that
track. And that beyond that, that then imposes the semantics once you've decided that something's
an entity rather than a relation, then, you know, that's a thing. And then what both cognition
and language are doing is building hierarchies out of these three piece pieces. So you've got
entity relation, entity, another one over here, and there's a relation between those,
and now you build a bigger one. And then the only reason that language looks complicated
is that there's some data compression and we leave stuff out. But it was not too hard to come
up with a table of what got left out. And so that for particular words, you can build a dictionary
and then put the things back in. And essentially, what happens is we leave out
words for system component relations. I know he can drive a car. I know that he can drive a car.
Well, we leave out that all the time. And the same thing between adjectives like a red apple
is essentially an apple that is composed of the property of redness. And so anyway, so if you
adopt this, then it all becomes very simple, you just drop things into this reading frame,
and then you just pop stuff in whenever something has been left out. If you get
two nouns in a row, so you can use the table and pop them back in. And then, you know, the argument
then becomes, oh, well, if that works, then is that how we think about the world? And oh,
I should say, English works that way. Japanese is entity-entity relation. And there are languages
that do it that way also have word endings, so that you know where the first entity,
and so you know where the first entity ends and the second entity begins, even though you
don't have a relation between them. Whereas languages use entity-relation entity, they're
using the relation itself to separate the two. And like I said, we've got a three megabyte
program and a three megabyte dictionary. Oh, so Mike Castellini. So anyway, so I wondered whether
your brain has a format in it that lets you do both cognition and language. But as you point out,
that's not probably at all what the somatotopic representation is in your brain. So this would
be a top level thing. So Mike, thanks. Yeah, thanks guys. This is amazing. So yeah, you know,
let's keep talking more. We can do more. Super interesting. I think the language thing has a
lot going on there. We should maybe talk a little bit about the large language models later on.
I'd love to hear what you guys have to say, you know, GPT-3 and all that. There's a bunch to talk
about. Yeah, it'd be interesting to try to apply these kinds of structures to
cell-cell communication languages and much simpler languages than human languages,
which I always advocate starting with something simpler than the human case.
The complicated things don't work. So anyway, yeah, so this has been terrific. So anytime you
guys want to do it again, and maybe you could do that, maybe the topic and start with the language
thing. I don't know a whole lot about the big models, except that they have amazing failures,
amazingly simple and obvious failures. And so it seems to me they haven't got it totally nailed down
yet, but anyhow. Okay, super. Thanks guys. Thanks so much. Have a good one. Nice to meet you. Yeah.
Thank you. All right. Bye-bye.
