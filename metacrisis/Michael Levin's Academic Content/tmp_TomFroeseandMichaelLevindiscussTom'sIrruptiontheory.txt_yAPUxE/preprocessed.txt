go through it and I'd love to talk about eruption theory
and the thing you were saying the other day
about the information that's not lost and all that stuff.
Yeah.
Right, right, yeah.
And we can also, if we have time,
we can also talk about this issue
of how far down it goes and all that.
Yeah, ideally we want to have a measure, right?
Like a operationalized criterion for that.
So this is exactly what this is about.
Let me just see what's the best way of sharing this.
So I think,
is it okay if I put out my second screen?
I will be a little absent looking to the side.
Maybe if I record it better,
like if I have it in the background.
Well, it's mostly gonna be the, I mean,
just to share this, I don't even know
that we're gonna see you,
we're just gonna see the slide, so it's fine.
Just share that.
Okay, okay, all right.
Yeah, because I can see you basically,
it's easier that way.
That's fine.
Okay.
All right, Safari into game share.
How's that looking?
Perfect, yeah, I can see it, great.
Okay, let's put this over here.
Yeah, great.
Okay, so this is the new paper that just recently came out
and it's a more mature version of the eruption theory
that now both consider is not just the mental causation
or like the agency part,
but also the other direction,
like how does something non-mental become part of the mental,
part of subjective experience,
part of mental content.
And so now it has these both classic problems,
which, you know, come in many varieties.
But I think now in this paper,
for the first time I see a way in which we can do science
with them, which is getting me very excited.
So I'm gonna take us through like the main figures
because they kind of encapsulate
the different stages of the argument.
So we start here.
Okay, so the human version of the mind-body problem.
So, you know, if you're asking here the big questions,
how is the mind related to matter and vice versa?
And, you know, analytic philosophers have been banging
their head against this problem for a long time.
And if you read Kim's kind of like final book account of this,
I quite enjoyed it for its clarity of showing that
even if we solve the problem of mental causation
by going kind of a reductionist route
and just basically saying that, you know,
the mental is the physical
and therefore the physical can cause the physical, you know.
You lose a lot of what is nice about thinking about the mind
in terms of its qualities and teleology
and, you know, your intentions and free will.
Everything goes out the window,
but you might save causality, right?
So you still might get mental causation saved that way.
But the problem is that if some of our things that we do
depend on conscious experience,
then the problem is not fully solved.
Because that would mean that you also have to have an account
to solve the hard problem of consciousness,
to have a full account of mental causation.
And so right from the start,
I want to say, I want to be realist about this
in the sense that, you know,
I do think that our experiences make a difference
for how we behave, right?
And if someone wants to say that our experience
don't make a difference to how we behave,
we've kind of like have two different premises.
And like the conversation really kind of stops there,
but to my mind, you know, it's a very hard sell
to try to argue that our experiences
don't make a difference.
We basically lose most of the population of this planet,
basically just, you know, exits the conversation, right?
So we should do our best not to go down that route.
But what that means is that we also have to solve
the problem of consciousness.
So these are interlocked, right?
And this is the big problem.
It gets worse, because you might think,
well, that's just for people, you know,
because that's the problem of consciousness, you know,
but if you talk about rats or, you know,
even more basal cognition,
then this might not be an issue.
But what it proposes that actually there's a more general
mind-body problem working in the background.
So on the left hand side,
I call this the hard problem of efficacy,
which is that for any mental property,
let's say a representation, a goal state,
or whatever you have,
there's a problem explaining how that state as such
as being a mental state makes a difference
to the physical state.
So it's a generalization of the problem of mental causation.
So it's not good enough to say
there's a supervenience relationship or something like that,
but the causality at the bottom level maintains.
You know, what I want to say is,
how do we say that the goal as being a goal,
as being an intention, as having normativity conditions,
as being able to succeed or to fail,
or to be better or worse and things like that,
the whole normativity of it,
that also somehow needs to be able to account it
for in this kind of account, right?
And that's the hard problem of efficacy.
And then we have the hard problem of content,
which is how does anything even become part
of the mind in the first place?
And so representationism, for example,
just kind of rushes this away a little bit,
but let's say, even if we gave representationist
half of this and said, okay, let's don't worry
about how to naturalize content in terms of its origins.
Let's say some future point, we have a story
of how you go from something purely physical
to something that has semantic content,
that has mental content,
that has about these conditions or normativity.
Let's assume that problem is solved,
then we still have the problem
of how does that even make a difference
to something physical?
Take a thing like, I don't know,
like a neuron in the brain that's supposed to represent,
I don't know, your place in a maze or something like that,
your classic place cell stuff.
Well, the fact that it's representational,
that it has representational content,
which is kind of like Nobel Prize winning material, right?
So this is exciting stuff.
We find the correlation,
but then where in our analysis of the neural activity itself
does the content enter the picture?
All right, so when we look into the brain and say,
here's this neuron and we know it's correlated
with this, you know, content outside of the rat,
like being in its position in the maze,
but hey, you know, like the rest of what we're seeing here
is just physics, it's just biochemistry,
it's electrical potentials, right?
It's like, you know, membranes opening and closing
and molecules floating around, there's no content there,
there's no normativity, it's just physics, right?
So there's a kind of sense of disconnect here
where how do we even work across this gap?
And my feeling is that this is not,
we can't just, you know, talk it away.
My feeling is that we should face it head on and say,
yes, there is a big gap here, right?
On the one hand, we can talk about content
and consciousness and the mental and intentions
and norms, health, you know, being, you know,
worse or better and so on,
terms that have no frame of reference
in the purely physical sciences,
but they do have an existence
and we know they make a difference,
so we're realist about it.
So why don't they show up when we do our best science,
you know, and one of the things that, you know,
already should come out of this is that
just because they don't show up as such
doesn't mean that they don't make any difference, okay?
So that there are two different things here.
One would be a demand for observability
and even maybe intelligibility.
So I observe something and it makes sense as a content.
And the other one is to say,
well, maybe that's not what I can do here,
but I noticed there's something else that's happening
that wouldn't happen otherwise, you know,
if the system wouldn't be in such a state and so on.
So, you know, so far, most people have demanded
that the mechanism should be observable and intelligible,
but why should we assume that, right?
There's a big gap here.
And we know already from other fields,
quantum physics, a prime example,
that demanding intelligibility and observability
can be a big stumbling block, right?
Sometimes you just have to get in and say, yeah, you know,
we don't understand why it's happening,
but something's happening and we can measure that
and we can work with that, you know?
But so far, cognitive science hasn't done this,
this, you know, allowed itself to conceive this possibility.
So let's go down.
So what I want to propose is that
we do need to make this room for this conception.
That's why I call it a black box framework.
That's just for a moment,
accept that we don't understand this relationship.
So we have two versions of it.
Let's stick with the human one for a moment
because it's easier for us to relate to.
So we have mental causation.
The scenario is, you know,
putting our neuroscience hats on or whatever, you know,
looking inside an organism, okay?
If something mental makes a difference
to its material basis of this behavior,
you've got an unobservable mental cause, right?
Like I said, just said,
we just have physical stuff happening.
We don't see the mind of such when we go inside the organism,
causing an unintelligible material event.
And it's unintelligible because we cannot directly observe
what caused it, what made the difference.
Okay, that's outside of the scope of observation
when we make quantitative assessments.
And the other way around is the same.
So, you know, we know that some things happening in the brain
make a difference to mental content
or make a difference to subjective experience,
but we can't measure subjective experience directly
inside the brain.
It's not there.
It's not something that we can actually quantify directly.
So that would mean that to the extent
that something is making a difference to experience,
you've got our cause without an observable effect.
Okay, so you've got an unintelligible material effect,
event here, that something is happening,
some change is happening,
but we don't understand why it's happening.
Okay, so we've got these two categories.
And what's interesting is that they point in different ways.
So that already suggests that, you know,
the signatures for these kinds of relationships
will be different ones.
And so this is then where I introduced my proposal
for how to work with this kind of situation.
And this is the eruptions theory,
where if we have the mind and the matter
relating to each other and we don't know how, okay,
this is the heart problem of efficacy or mental causation
or the heart problem of consciousness.
We know that they're related somehow,
but when we trace one domain into the other, it escapes us.
Okay, so let's just go through this one more time, right?
I have an intention of saying words,
of moving my hand to make a point.
And I can notice that my intention
is making a difference to my body, right?
If not, it would be crazy, right?
So, you know, I have the experience
that there's a coherence
between what I want to do and what my body does,
except that I don't know how my body does it.
Okay, something gets lost along the way.
I can see the effect of my intention,
but I don't have access to how it has that effect.
So that's one aspect of the black box, right?
And it's also true on the other side.
So now imagine I'm a neuroscientist
and I look at what's happening in the pathway
from my brain to my arm to make my arm move.
If it's really the intention as such
that's making my arm move,
well, the neuroscientist doesn't have access to that.
He can't measure intentions using his, you know,
I don't know, EEG apparatus or whatever he's might be using.
So again, something that gets lost.
I can go work backwards from the behavior
to all the activity in my body,
but at the end, something gets lost.
I cannot actually make the jump to the other side.
So that's what I mean with the black box.
We know these things are related,
but by some reason, maybe by necessity,
we can't trace properly, like transparently across.
And so that's why I propose the way to think about this
is in terms of absorption and eruption.
Absorption means that on the side of the mental,
when I inject my intention to do something,
it basically, there's a compression effect.
You know, Dr. Iffes talked about this
in terms of absorbed coping, right?
So when I'm really engrossed in my activity,
actually, you know, there's a narrowing of my vision,
you know, the world disappears a little bit
into the background, I lose myself and so on.
There's a kind of like compression
of the variability of my experience
as it's invested into the activity of my behavior.
But what happens on the other side?
Okay, now you're totally involved
in generating your behavior,
but that involvement is subjective involvement.
It can't be, you know, quantified.
It can't be translated into something happening
purely quantitative terms.
So then that means that there's a hidden variable.
Okay, now there are factors making a difference
to how your behavior is generated
that cannot be, you know, traced to causes
at that level of description.
That's what I call eruption.
Now there's a kind of diversification,
an increase in variability
that in principle would remain unexplained
at that level of description, you know,
and we can trace it back to the higher level.
And that was the original proposal of eruption theory
in the first paper.
And I like elaborated on that part a lot.
And there's many examples that we can talk about of,
you know, how this fits with some of the empirical data.
When we talk about neural entropy and complexity
and all these kinds of things, you know,
and why organisms are such noisy systems in the first place,
you know, like a lot of this starts to make sense
from this point of view.
But then let's talk about the other side first for a moment.
That's the absorption side.
That's a new part of this paper,
which is to say that if there's a cause
that has an effect which can't be observed,
it's a little bit like a reduction in variability.
The difference that would have been made
is now not being made
because that difference appearing in a domain
that I cannot directly measure
in the one that I'm currently observing.
So this is a reduction in variability.
So two things kind of collapsing
or canceling each other out.
There's information loss of a different kind
as things are getting kind of translated
into the other domain.
And so right now what I'm working on
is more on the right hand side,
trying to flesh out that part.
And, you know, it's coming together quite nicely
with the things we've been talking about,
like both time motifs and things like that,
which are very nice ways of canceling out things.
I think that's pretty much it.
So I just let me just end on one final note,
which is that in my mind,
this is just a first step of something much more general.
And so what it looks like to me now is that
this is just one special case of an interaction
that crosses two ontological domains,
or regional domains, regional domains of being.
Or if you want, it could also be considered
as crossing different scales of agency, right?
Going from the scale of agency of inside your body
and all the agents that are working there
to your agency as a person.
But basically, as long as we have a transition
between ontologies of some kind,
then this kind of frame will probably will apply too.
And that could even be true for cellular biology,
for example.
So when I was reading your work on,
how is it that the higher levels, you know,
transform the lower levels,
and how do they get incorporated
into the higher levels again?
You have very similar notions actually
as eruption and absorption already.
So with eruption, the way you phrase it is
that there's a deformation of the energy landscape
of the lower level, right?
So suddenly, you know,
these guys are happily doing whatever they're doing,
but now there's an unexpected change
in the kind of, in the game that they're playing
and they would have to study adjust.
And from their point of view,
well, they will never be able to figure out why that happened.
You know, what just happened?
That's all that's outside of their scope, right?
So it's a little bit like an eruption in that sense.
There's an increase in a change unexpected
and it can't be explained at that level of description.
Okay, it just goes beyond it.
And at the same time, when you talk about
how the components get integrated into the larger whole,
you talk about something like a loss of identity
or something like that, right?
So for example, the molecules produced by one cell,
well, you know, they're not tagged with the name, right?
So if another cell produces it, you know,
they can't distinguish where it's coming from.
So they kind of like, you know, the identity starts to merge
between the different little components.
So that's, you know, it's very close to what I'm talking here
in terms of absorption.
And so one thing that would be very nice to contemplate
is whether this kind of framework gets a quantitative grip
on some of the things that are happening at those scales.
So not just talking about the mind-body problem
or mental causation,
but just talking about multi-scaler integration
to some extent.
So principles that would apply to the left-hand side,
the eruption side would be, you know,
increases in noise, increases in hidden variables,
dimensionality, you know, entropy, these kinds of things.
So there's an expansion of variability
and that expansion is unexpected
and to some extent irreducible
to the things that are happening normally in that domain.
And on the right-hand side,
you have things like compression,
maybe synchrony, symmetries, order parameters,
this kind of thing
where there's a kind of informational loss
as multi-components that could be behaving differently normally
don't behave differently.
And that's maybe the both time motive
and things like that fit into that category.
So anyway, that's kind of like the mini overview
of what the paper proposes.
Cool, yeah, very, very interesting.
What's your thought on,
is there, do you think there is a similar problem
in the case of, so let's say we have
some sort of classic computational device,
it's running some sort of algorithm, you know,
and then there's the physics perspective
where you can see the electrons shuffling around,
you don't actually see the algorithm,
whatever it's doing.
Do you think there's a similar dynamic going on there
or is there something unique to the biological case
that isn't captured in the software hardware dichotomy
if there is?
I think it's a really good question.
It's one that I've started exploring now
with some colleagues at O'Nam.
And I think I'm changing my mind about it.
So I used to be very resistant to that idea.
I'm slowly coming around to it,
especially on the side of absorption.
It's almost like computers
are the perfect absorption devices.
All that variability that you have
at the level of electrons and so on,
computers are designed such that variability
does not matter for the kind of algorithms
that are being implemented, right?
So if anything, that is the most excellent example
of the right hand side, but here's the problem.
It's like one sided, right?
It's like, I don't want to put it this way,
but it's like we created devices
that are super rich in experience, possibly, right?
So they've got lots of variability
that they're absorbing
and possibly creating experiences for them,
but they can't do anything about it.
It's the Russian side that's blocked,
because if you do have things that are acting
on those things, well, that's like,
the old school Windows blue screen of death kind of stuff.
Like suddenly you've got an unexplained change
that can't be reduced to the rules at that level.
Well, you've got error correction, right?
So either it gets thrown out
or if it can't be thrown out,
well, then you just got a system failure.
So it seems to me that the issue is on that side.
It's like, if you wanted to make computers
that could fit into this framework,
the question would be, how do we loosen them
such that the higher levels can make a difference
to the lower levels in their own terms?
Yeah.
Yeah, yeah.
So we have a weird example that might be relevant to this
and that's, maybe, you know, this,
so I'm very interested in higher order behavior
that seems to be, in some important sense,
decoupled from either the algorithm
or the mechanisms underneath
in that it is not just emergent complexity
because emergent complexity is easy, you know,
you get that with cellular automata, whatever,
that's easy.
I'm talking about more interesting
emergent goal-directed behaviors.
And, you know, you've seen this preprint of ours
on the sorting algorithms, you know,
I purposely wanted to start with the dumbest,
simplest system that is transparent, deterministic.
It's, you know, six lines of code.
Everybody, you know, thinks they know what these things do
and these sorting algorithms.
And it turns out that they have some really interesting
behaviors, including this clustering thing,
which is, by itself, nowhere in the algorithm.
And so I wonder, now, clearly super primitive,
but that was kind of the point.
We wanted a very basal example of this,
but I wonder if it's related to what you just said
where they, on the one hand, yes,
in our current architectures, they can't do anything with it.
On the other hand, there seems to me,
and I think this is all over the place in biology too,
there seems to me that even though constrained
by deterministic algorithm that, you know,
there's no magic, it doesn't screw up, you know,
it follows the algorithm, there's no error in that sense.
But it also manages to do some things
that are not, so to speak, in the algorithm.
And I wonder if, and I think biological things
are super good at it.
In fact, I think that's probably what we call biology
or systems that are really good at this.
But fundamentally, I feel like it starts very, very early.
I don't think you need much to start seeing
these things appear.
And, you know, where do they come from
is a whole other thing.
You know, we face that question
with our various synthetic, you know,
Xenobots and Anthropots and things
where they've never existed before.
There's no evolutionary long history of selection
that would explain why they have certain properties.
So where it comes from.
And I mean, I'm pretty sympathetic
to this kind of black box approach
because I think looking for mechanism
and looking for explicit representations
in terms of how it works and where things come from,
I think may not always be tractable.
But yeah, I think already we have some of these issues
in a very minimal way with even just very simple,
stupid computer architectures.
And then there's the whole polycomputing thing, right?
So this is what Josh Bongard and I
have been working on on this notion
that when you have a physical process,
what it is that it's computing
is in the eye of an observer.
And multiple observers, in fact, Josh has some amazing,
his student Atusa has some amazing actual, you know,
data on this showing that the same physical processes
can look like very different computations
depending on how you look at them.
And what is it that it's really computing
is up to an observer's interpretation,
which I think in biology,
that's what's going on is that every subsystem
is interpreting every other subsystem, however it can.
And then that also gets to this issue
of like mental content and the subjectivity of it,
which is, you know, what is it really?
What are these mental causes and so on?
What is their actual content?
And is there a privileged one answer to say,
okay, that's the content of that mental state?
Or could we do this kind of polycomputing thing
and say, well, there are modules inside
and based on what you were telling me before,
there may be even physically external other beings
that you may be coupled with in some way
that will also have an interpretation
of what any given mental state is.
So, you know, yeah, I think this is partially why,
you know, we were talking about like how far down it goes.
I think these issues are very deep
and they crop up very early.
I don't think they,
I don't think we have to wait until we get brains
before these deep questions come up, you know,
of interpretation and that's-
I've heard you talk before about this, you know,
polyfunctionism or what do you call it?
But isn't it the case that, was it Putnam or someone
who used the argument that you can read any kind of function
into physical process as I deduct you
at observant for computationalism,
in the sense that if you want to be realist
about the implementation of the algorithm,
then, you know, it shouldn't just be the observer
who, you know, determines what kind of algorithm
is being run, right?
Because I don't know what this argument was,
but it was basically like, you know,
if you take any bunch of molecules
and you just sub select, you know,
the right kind of properties,
they'll be running windows or something like that,
you know, like because there's such a mind blowing
degree of freedom that, you know,
there's enough there that, you know,
you just have to be selective
and then, you know, you get whatever you want.
But, you know, but the question is like,
does that have downstream consequences
for the process, right?
And so there you don't want necessarily the observer
to be the one who determines what it is that was there,
right?
You know, so I don't know what the redeco there is,
maybe I need to look it up, but it seems so far,
it seems fine, everything you just said
seems completely fine to me
because what I think happens in biology is,
so in computers, we're used to the fact
that there's somebody who wrote the algorithm
and we sort of, we tend to take that interpretation
as the privileged, like I know what this is,
I wrote it, that's what I'm telling you,
it's a bubble sort, whatever it is,
but in biology, you don't get that.
And so every subsystem with cells,
subcellular components, tissues
are looking at all the stuff going on around it
and they don't, you know, they don't get a manual to say,
well, what the hell does this mean?
There's signals coming, what computation is this?
And I think what they have to do for adaptive advantages
interpret it however they can.
So they form their own internal model
of what the computation is and they say,
oh, I see what this is, this is telling me,
you know, that the metabolism is gonna shoot up
five minutes from now.
And somebody else, some other module is looking at that,
no, no, no, what I see here is that
this set of genes is gonna be expressed
and therefore I'm gonna do this and that.
And in that case, and in Josh's work,
you can see this like, it's this particulate material
and depending on how you look at it,
you see an AND gate or an OR gate
and there is no privileged answer to,
well, which one is it really?
I mean, there is no really.
And I think biology has that, takes that to an extreme.
But many, many, many, many, many, many, many, many, many,
maybe I should read this, but, you know,
if I had a computer built and like, you know,
there was an ambiguity whether an AND gate
was an AND gate or an OR gate, would the computer work?
No, right?
I mean, like you have to depend on
that it does a particular translation
of its inputs to its outputs.
And if it doesn't, then, you know,
well, then you have a kind of proper holistic machine
or something like that, but you know,
it would be some kind of other kind of system.
Well, so let me push back on that.
I think that that's true if you stick to one observer
and the idea that there is a definitive one thing
that that observer wants it to do.
And then, yes, if one observer can't tell what it's doing,
that's a real problem.
But I don't think it's a problem
if you have a physical device
and there are multiple observers
and one observer says, oh, this is great.
This thing's generating prime numbers
and somebody else looks at it and says, oh, whatever.
What I see is that it's, I don't know,
it's doing some other function.
Now, you know, for us, I mean,
it's statistically very unlikely that you can say,
oh, look, it's running Windows and somebody else says,
oh, no, it's completely different
because it's hard to have a process
that matches both those descriptions at the same time.
But I think what happens in biology
because there's noise,
there's a high tolerance for fuzziness and all that,
there are lots of systems
that are looking at the same set of events
and interpreting, building internal models of those events
as completely different things.
And it's like, you know what?
Okay, but I agree with that.
But that's from the point of view of looking at it, right?
So if the point of view is that
there are multiple interpretations possible
and that diversity of points of view is helpful in some way
for the systems around it,
I can totally agree with that.
And I think that's interesting to explore.
But that's very different than saying
that those multiple possible perspectives
make a difference to the process itself,
which is lending itself to be interpreted
in these multiple ways.
Well, what I think then happens is,
so the other side of the equation,
and I think you're absolutely right
in how you divide this, right?
And so the two sides.
So the other side is tenon bombs.
Have you seen his paper, The Child as a Hacker?
The Child as a Hacker, it's also me.
It's really good.
And it's this notion overall.
So I sort of expand that beyond,
I mean, he's studying brains and human development and so on.
The notion of hacking is from where what's fundamental there
is that you don't know or care
what the correct way to interact with a system is.
You are going to exploit it however you can, right?
That's kind of this notion of it.
And so I think what happens in the biology
is that part of interpreting these things however you want
is that you are also going to use that
to control them however you can.
So you try to find ways, right?
So I think that's basically what's happening
in biological material is that every,
both within levels and across levels,
you have systems that are constantly hacking each other
and trying.
No, I completely agree.
So here we're on the same page.
Actually, like, you know, this is something
that also follows from the framework of proposing.
Because if there's a black box,
basically components at different levels
have to have a high level of tolerance of uncertainty.
And they have to have trust to some extent.
And sometimes from their point of view,
it doesn't make sense what's going on around them.
But that doesn't mean that they should disengage
or kind of like, you know, fight it
or try to resist it or counteract it.
Because what it could mean is that actually
it's a different level of agency,
a higher level of agency rearranging things, aligning things.
And, you know, you need to just roll with it
even if you don't have everything that is required
in order to understand where that's coming from.
And but if, because of that, there is a gap
for, you know, free riders or pathogens or something like that
to take advantage of exactly the same trust,
trusting nature that, you know, okay,
today I'm producing this other molecule.
I don't know why I'm producing it.
Yeah, it turned out you're a hijacked by a virus.
And it's not, you know, the higher levels
that are kind of like, you know, messing with you.
So, but yeah, that's kind of built into this system.
So you can't avoid it, you know,
that's because it's such an indirect architecture,
that which allows the multi-scaler integration
is also that which allows like, you know,
bad things happening to some extent.
Well, absolutely.
And my guess is, and we're now starting
to explore this experimentally,
is that systems have probably way,
biological systems probably have ways
to try to determine whether something that's happening
was caused by me versus.
Nice.
I want to do the same for the brain.
Yeah.
There should be probably some frame of reference
in the background that tells you how much unexpectedness
should you be expecting at each moment?
Exactly.
What's the rate of unexpected events, for example?
Yeah.
And if it exceeds your expected rate
of unexpected events, something else is messing with you.
Yeah, that's right.
So it's the question of, there's a couple of ways to pose it.
Like, am I learning or am I being trained?
That's an interesting distinction, right?
Because how much agency is there in my environment?
Is there another agenda that's responsible
for my learning process?
And if you're a cell, so the way we're going to do it
is basically to look at, our readout is going
to be stress response and some other things.
But you can imagine a cell or a group of cells,
and you can imagine messing with it in progressively different
degrees of internal targeting.
So here's a signal that comes from the outside of the cell.
Fine.
Here's a signal that we generated in the second messenger
pathway right under the membrane.
Here's something that we did in the nucleus itself.
Awesome.
At what point does the cell say, yeah, that's cool.
That's what I'm doing versus, oh, no, this is clearly
coming from outside.
And I think that's nice.
So this has performance like a brain stimulation.
So I want to get into that for our lab.
And one of the reasons is that if eruption theory is
on the right track, doing TMS to the brain or whatever
stimulation is basically eruption simulation.
So suddenly you have an uncaused fluctuation in activity.
But from the point of view of the local components
in the brain, they're used to that.
That's already how they always get impacted
by higher levels of organization.
So then it's kind of like we're speaking the language
of the higher levels of organization
by injecting this external variability.
And that's nice, because then we can actually play with this.
Like we can say, what's the rate of agency
that the system is currently expecting?
And there's all kinds of interesting things,
like why is deep brain stimulation
helpful for overcoming obsessive-compulsive disorder
or maybe helpful in depression and so on?
So certain conditions where our agency becomes constricted,
somehow compressed or limited, are
helped by what otherwise looked like crude interventions.
Why should just putting this random stimulation somewhere
in your brain have the same effect
as you opening up your space of agency possibilities?
That's very strange.
But if the way in which agency of a personal level
connects with the sub-person level
is only indirectly through this deformation of the state
space in unexpected ways, basically,
well, then all you have to do is deform it
in unexpected ways to mimic to some extent
the signals that they're expecting from the higher levels.
That's a really interesting point.
I wonder if, and this is totally beyond my expertise,
but I wonder if some aspects of plastogens and psychedelics
and things like this can be understood
by what you're really doing is lowering the vigilance
of the system to hack from the higher levels
so that you're more willing to not resist.
I mean, this issue of resistance, knowing
that you're being hacked, whether laterally,
whether by a parasite, whether by a malfunctioning
component, or from above by some higher level of organization,
that resistance, biometically, this is huge.
Because one reason, I think, why we
have to trouble designing new drugs that actually
fix anything, and we have almost,
other than antibiotics and surgery,
we have basically nothing that fixes anything, right?
These drugs hold down some symptoms, best case scenario,
but they don't really fix anything.
And I think part of it is that we
are operating in a way that's very easy for the cells
to tell and try to fight back.
I like it.
Yes.
I think that's the limit, right?
The limit of molecular medicine.
I know that someone else is messing with me.
That's not coming from the higher level of organization.
Yeah, because it's such micromanagement,
and the cell, what the hell is this?
There's this receptor that's suddenly
being targeted by some frequency.
That's why you see all these drugs,
and then there's a list of potential side effects
that are a mile long that your head will fall off
and you'll go blind all of a sudden.
Yeah, you're actually on the right track here.
So I'm thinking that there could be a spectrum of possibilities.
Because we're now separating mind and matter a little bit,
just a tiny gap between them.
It opens up two possibilities.
Imagine we have a reduction in agency, like dementia.
So people stop being able to express themselves.
Language goes away.
They stop being able to take care of themselves and so on.
But in that case, it could be that the brain itself
is no longer flexible enough to be
able to receive these kinds of perturbations
and let them percolate through the system
and scale them up in the right kinds of ways.
It could be that agency is almost intact, to some extent.
The mind is still there, but you just
cannot find the channel to express itself.
On the other hand, maybe in some other cases,
like I say depression, it could be that the brain is fine.
It's actually ready to receive whatever you're going to send it.
But there's something happening at the level of motivations
and will and experience, which the person is overwhelmed
for whatever reason.
It's not sending the kind of impactful interventions
that would normally make the body move in the right kind of ways.
So both mind from the outside looks similar in the sense
that there's a reduction of engagement with the world,
but the physiology of it could be very different.
So I like that because it feels more natural
to have these two possibility axes.
It's not always everything in the same place.
And what you're saying about the fact
that we need to be a little bit more clever about how
we interact with that, I really like that.
So that makes a lot of sense.
And what you're saying about intervening
in the different levels, we're starting
to think about that too in terms of humans.
So we can trigger muscles directly
by applying contraction to your nerve fibers here.
You can grasp like that.
And that also can feel like you're actually doing it
if it's doing it in a good way.
But what if we do it from here?
Or what if we do it by doing something here?
At which point, is there a sense for, hey,
this is not me doing it anymore or something like that?
Yeah, that's very interesting.
I was actually trying to look up some papers
on this related to confabulation.
And people with, like I've seen examples
where somebody has an electrode in their brain
and it triggers laughing.
And so push the button and your mouth starts laughing.
The patient doesn't report.
That's weird.
I was having a serious thought and then my mouth starts
laughing.
But they say, oh, I had a funny joke.
That's what they say.
So this issue of confabulation and to what extent
does the system, as you said, does the system
take this on as, OK, I'm going to tell a coherent story of why
it was me versus I'm going to accept it.
And actually, dementia in another,
I just wanted to say another quick thing about dementia
and those things.
I have a collaborator who is a hospice nurse.
And she cares for, she's not a scientist,
but she's a long term hospice nurse with a lot of experience.
And we're actually writing a paper on case studies
where what seems to, there's this phenomenon
called terminal lucidity, which is really interesting,
where basically you have somebody
that has been in a very debilitated state
for a really long time.
And it's just progressively been going down.
They don't speak, all that stuff.
And you think, OK, that's it.
Like it's all gone.
And then I think, I forget the exact amount of time,
but I think it's like a couple days before they actually die,
it all comes back.
And they have a lucid conversation with people
and they remember things and they give some instructions
and whatever.
And there's this sudden burst like, and it's right.
I don't think there's a good clinical understanding of it,
because if it's just the fact of the hardware degrading,
then that's it.
That's nice, yeah.
So having a slight gap in our account
would allow for that, make sense of that possibly, yeah.
Exactly, yeah.
Yeah, I like it, yeah.
So I think there's that clinical component.
I think it would be cool to study some of that stuff
from the perspective of the model that you're putting forward.
I think that could help.
Here's another interesting example, clinical example,
which is if we think about the absorption side,
what would be the maximum version of it?
It would be if almost the whole brain is in synchrony.
So that's like all the local variability.
Each neuron has tens of thousands
or connections coming in, all the variability,
suddenly they're all like firing in unison, right?
So it's a huge reduction of complexity,
lots of information loss.
When you do that, agency goes out the window,
okay, you collapse, okay?
And that also makes sense
because now you can't have any more eruption, right?
Everything is ordered, everything is regimented.
It's no possibility for injecting
experienced variability anymore.
It just gets, you know, there's no room for it, okay?
So no more agency, but the model would predict
that experience might still happen.
In fact, it might even be more happening
than in a normal state.
And so I started, you know, just a little bit investigated,
but for some people, indeed having an elliptic fit
can come with all kinds of experiences, you know,
even like divine intervention kind of experiences, right?
So, you know, like, you know, having a, you know,
something that changes your life kind of experience
can happen under those states.
So that's interesting in the sense that it fits again
with this model that, you know, on the one hand,
although it knocks our agency,
now your experience channels have been opening up,
you know, on the other hand.
So it's, and something, you know, like there's something there
that, you know, there's lots of this kind of thing
that, you know, it mentions psychedelics.
We didn't talk about this, but, you know,
it's like the other way, maybe it's actually loosening things
up so it becomes easier to integrate with your brain.
And that fits very nicely also with other work
that shows that if you're under anesthesia, you know,
if you measure neural entropy, complexity,
diversity of signals, it turns out that if you're doing
a cognitive task while you're slowly falling unconscious,
if you're doing the class well,
your neural entropy is higher than baseline.
So even though you're falling unconscious,
your neural entropy goes up more than expected.
But if you're actually falling unconscious
and you're stopped being able to do the task,
then it goes down.
So assuming that both groups of people
actually are losing consciousness,
then this kind of entropy is not tracking
the state of awareness, it's tracking your cognitive effort.
How involved are you mentally in what's happening?
And that's going back to dementia.
Like one of the best things you can do
for preventing onset of dementia
is learning extra languages, you know,
like, you know, engaging in extra normative frameworks,
you know, becoming able to respond
in a much more multifaceted way to your environment.
And if you think about the way in which we live
before, you know, modernization to some extent,
we were embedded in a symbolic world, you know,
everything had meaning, you know,
nothing happens just by chance, you know,
like the fact that this cat crossed the road
might mean that my mother will die
or I don't know something, you know?
So everything had super significance.
This kind of symbolic framework was overlaying
on all of our perception.
And if involvement, mental involvement in our behavior
is actually causing eruptions,
well, then that has very loosening effects on everything.
So it makes sense to me that the best way of preventing
your, you know, like things freezing up, so to speak,
is to actually become more involved normatively.
And it's not just about, you know, autopilot stuff.
This is important, right?
So it's not, if you're just an autopilot, you know,
your body can take over.
And maybe that's also a little bit about what you were saying
in terms of stimulation and then, you know,
this kind of confabulation happening, right?
It's kind of like the habit.
The habit takes over.
It's not like you're really freely willing
the words coming forward.
It's just that your body has already made response
to the situation that's kind of like, you know,
unfolding in that moment.
And I think a lot of our behavior is like that.
And what I've been speculating a little bit
that the success of large language models,
to some extent hints that, you know,
even when we talk like this conversation right now,
a lot of it is actually habits
and our body's predispositions just unfolding themselves
with the affordances and the environment.
And that's why they can't be modeled, you know,
by just looking at, you know,
linguistic patterns and so on.
And so this is another consequence of the eruption ideas.
And if you have this little gap,
it also means that we're no longer directly in control
of our behavior, right?
We can set the intentions.
We can open the space of possibilities.
We can make things more flexible, for example.
But then you have to be in the right context.
You have to have the right affordances.
You have to have the right history of interactions.
You have to have the right body for that behavior
then to also express itself in the right way.
And if some of these things start crumbling
or deteriorating or something like that,
then maybe intentions don't connect that well anymore
with your behavior or with your regulation.
But when things work well, it looks like we're in control
because, you know, the body just makes it happen
like magic almost.
But, you know, all we can do really
is to go open spaces of possibilities.
But to turn those into actualities
is coming out of our embodied history
of interaction with the environment.
So it's a different take on free will, it's interesting
because yeah, you're free in the sense
that you still have the choice
of opening possibility spaces,
but you don't have the choice of how to close them.
The only way to do that is to cultivate
the right kind of environment and the right kind of skills
such that you're always poised to respond
in a way that you want to be responding.
Yeah, yeah, yeah, I completely agree with that.
I like a notion of free will
that basically extends it to a longer time scale
to the idea that what's free is modification
of self and environment to enable you
to do it in different ways in the future.
That's where you get to really exert it, right?
Yeah.
And it's a bit strange to think about that in the first place.
I mean, just thinking about like,
hey, I can't directly control the next word
that's coming out of my mouth.
I just have to trust my body
that it will choose the right word.
Otherwise, I get a Freudian slip or something like that.
Yeah, but once you're okay with that,
then your focus shifts and says,
well, what can I do today such that tomorrow
it will be better, this kind of thing?
Yeah, I think that's a much more useful view of it.
Yeah, and I think it's true of all of biology.
It goes the same way.
So, there's a slight indirection
which is basically the higher level of organization
can't directly control what the lower level is gonna do, right?
Or you can set the conditions for it.
I guess it gives you more possibilities to do things
or I give you less, but within that space,
I just have to trust that the tendencies are the right ones.
Mm-hmm, mm-hmm.
Yeah.
Excellent, yeah.
Thanks so much.
This has been very helpful.
Yeah, I think it's a really interesting framework.
So, yeah, lots and lots to talk about.
Great, yes, thank you.
I look forward to seeing how you work with this experimentally
when you start intervening at these different stages.
I think that's gonna be very insightful.
Yeah, yeah, well, we'll talk more
because I wanna get your thoughts on some of the,
basically using some of the brain clinical data
to inform how we do some of the things in cells.
So, we'll talk.
Yeah, yeah, I think that there's a new horizon
opening up with this way of thinking.
Suddenly, we can quantify these things
and then everything changes.
Yeah, yeah.
