So if another cell produces it, you know,
they can't distinguish where it's coming from.
So they kind of like, you know, the identity starts to merge
between the different little components.
So that's, you know, it's very close to what I'm talking here
in terms of absorption.
And so one thing that would be very nice to contemplate
is whether this kind of framework gets a quantitative grip
on some of the things that are happening at those scales.
So not just talking about the mind-body problem
or mental causation,
but just talking about multi-scaler integration
to some extent.
So principles that would apply to the left-hand side,
the eruption side would be, you know,
increases in noise, increases in hidden variables,
dimensionality, you know, entropy, these kinds of things.
So there's an expansion of variability
and that expansion is unexpected
and to some extent irreducible
to the things that are happening normally in that domain.
And on the right-hand side,
you have things like compression,
maybe synchrony, symmetries, order parameters,
this kind of thing
where there's a kind of informational loss
as multi-components that could be behaving differently normally
don't behave differently.
And that's maybe the both time motive
and things like that fit into that category.
So anyway, that's kind of like the mini overview
of what the paper proposes.
Cool, yeah, very, very interesting.
What's your thought on,
is there, do you think there is a similar problem
in the case of, so let's say we have
some sort of classic computational device,
it's running some sort of algorithm, you know,
and then there's the physics perspective
where you can see the electrons shuffling around,
you don't actually see the algorithm,
whatever it's doing.
Do you think there's a similar dynamic going on there
or is there something unique to the biological case
that isn't captured in the software hardware dichotomy
if there is?
I think it's a really good question.
It's one that I've started exploring now
with some colleagues at O'Nam.
And I think I'm changing my mind about it.
So I used to be very resistant to that idea.
I'm slowly coming around to it,
especially on the side of absorption.
It's almost like computers
are the perfect absorption devices.
All that variability that you have
at the level of electrons and so on,
computers are designed such that variability
does not matter for the kind of algorithms
that are being implemented, right?
So if anything, that is the most excellent example
of the right hand side, but here's the problem.
It's like one sided, right?
It's like, I don't want to put it this way,
but it's like we created devices
that are super rich in experience, possibly, right?
So they've got lots of variability
that they're absorbing
and possibly creating experiences for them,
but they can't do anything about it.
It's the Russian side that's blocked,
because if you do have things that are acting
on those things, well, that's like,
the old school Windows blue screen of death kind of stuff.
Like suddenly you've got an unexplained change
that can't be reduced to the rules at that level.
Well, you've got error correction, right?
So either it gets thrown out
or if it can't be thrown out,
well, then you just got a system failure.
So it seems to me that the issue is on that side.
It's like, if you wanted to make computers
that could fit into this framework,
the question would be, how do we loosen them
such that the higher levels can make a difference
to the lower levels in their own terms?
Yeah.
Yeah, yeah.
So we have a weird example that might be relevant to this
and that's, maybe, you know, this,
so I'm very interested in higher order behavior
that seems to be, in some important sense,
decoupled from either the algorithm
or the mechanisms underneath
in that it is not just emergent complexity
because emergent complexity is easy, you know,
you get that with cellular automata, whatever,
that's easy.
I'm talking about more interesting
emergent goal-directed behaviors.
And, you know, you've seen this preprint of ours
on the sorting algorithms, you know,
I purposely wanted to start with the dumbest,
simplest system that is transparent, deterministic.
It's, you know, six lines of code.
Everybody, you know, thinks they know what these things do
and these sorting algorithms.
And it turns out that they have some really interesting
behaviors, including this clustering thing,
which is, by itself, nowhere in the algorithm.
And so I wonder, now, clearly super primitive,
but that was kind of the point.
We wanted a very basal example of this,
but I wonder if it's related to what you just said
where they, on the one hand, yes,
in our current architectures, they can't do anything with it.
On the other hand, there seems to me,
and I think this is all over the place in biology too,
there seems to me that even though constrained
by deterministic algorithm that, you know,
there's no magic, it doesn't screw up, you know,
it follows the algorithm, there's no error in that sense.
But it also manages to do some things
that are not, so to speak, in the algorithm.
And I wonder if, and I think biological things
are super good at it.
In fact, I think that's probably what we call biology
or systems that are really good at this.
But fundamentally, I feel like it starts very, very early.
I don't think you need much to start seeing
these things appear.
And, you know, where do they come from
is a whole other thing.
You know, we face that question
with our various synthetic, you know,
Xenobots and Anthropots and things
where they've never existed before.
There's no evolutionary long history of selection
that would explain why they have certain properties.
So where it comes from.
And I mean, I'm pretty sympathetic
to this kind of black box approach
because I think looking for mechanism
and looking for explicit representations
in terms of how it works and where things come from,
I think may not always be tractable.
But yeah, I think already we have some of these issues
in a very minimal way with even just very simple,
stupid computer architectures.
And then there's the whole polycomputing thing, right?
So this is what Josh Bongard and I
have been working on on this notion
that when you have a physical process,
what it is that it's computing
is in the eye of an observer.
And multiple observers, in fact, Josh has some amazing,
his student Atusa has some amazing actual, you know,
data on this showing that the same physical processes
can look like very different computations
depending on how you look at them.
And what is it that it's really computing
is up to an observer's interpretation,
which I think in biology,
that's what's going on is that every subsystem
is interpreting every other subsystem, however it can.
And then that also gets to this issue
of like mental content and the subjectivity of it,
which is, you know, what is it really?
What are these mental causes and so on?
What is their actual content?
And is there a privileged one answer to say,
okay, that's the content of that mental state?
Or could we do this kind of polycomputing thing
and say, well, there are modules inside
and based on what you were telling me before,
there may be even physically external other beings
that you may be coupled with in some way
that will also have an interpretation
of what any given mental state is.
So, you know, yeah, I think this is partially why,
you know, we were talking about like how far down it goes.
I think these issues are very deep
and they crop up very early.
I don't think they,
I don't think we have to wait until we get brains
before these deep questions come up, you know,
of interpretation and that's-
I've heard you talk before about this, you know,
polyfunctionism or what do you call it?
But isn't it the case that, was it Putnam or someone
who used the argument that you can read any kind of function
into physical process as I deduct you
at observant for computationalism,
in the sense that if you want to be realist
about the implementation of the algorithm,
then, you know, it shouldn't just be the observer
who, you know, determines what kind of algorithm
is being run, right?
Because I don't know what this argument was,
but it was basically like, you know,
