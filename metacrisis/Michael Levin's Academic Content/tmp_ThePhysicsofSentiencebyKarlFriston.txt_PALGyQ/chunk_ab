organ generating fantasies or hypotheses explanations on the inside that then are thought
to sensory impressions to see whether they are apt to explain what's going on and if they match
that's fine if they don't then you get some kind of Bayesian belief updating. I find this very nicely
demonstrated by this 16th century oil painter fame for doing still lives that when viewed
from a different direction and give you a very different impression oops I got them the wrong
way around so you're meant to see the the bowl of fruit first and then after that the face and
if you do see a face now the point being made here is that you made that face and it's something
that you had on the inside in your internal states that is a good explanation for the sensory
impressions on your retina or your your your epithelia and then this notion you can find
throughout history and philosophy I'm not a scholar but I find beautifully summarized by
Helmholtz for example objects are always imagined as being present in the field of vision as I have
to be there in order to produce the same impression on the nervous mechanism so again what he's saying
is it has to be on the inside before you can explain that sensory impression almost exactly the same
ideas from people like Richard Gregory the notion of perception as an hypothesis testing ideas used
to great effect by people like Jeffrey Hinton and Peter Diane in machine learning who indeed
built a Helmholtz machine based upon the work of or the Bayesian inference and the work of
people like Richard Feynman articulating that mathematically in terms of this minimization
of free energy so from our point of view though let's just come back to this notion of impressions
on the nervous mechanism or sensory impressions on our Markov blanket the sensory part of our
Markov blanket so I've cartooned that here in terms of some sensory shadows and our sensory veil
and if this dynamics if this Bayesian mechanics induced by Markov blanket is true then what that
means is I will look as if I am trying to find expressions for this sensory impression what caused
this particular shadow so what would that look like well we know exactly what it would look like it
has to be written down in this form from the physics so my internal states change in a way
that look as if they are trying to minimize this free energy form of this surprising or
self-information here and it transpires that the functional form of this can be universally interpreted
as a prediction error so what does that mean it means my internal states say my neural activity
is changing as a function of my current neural activity plus a prediction error
um and if you were an engineer you'd recognize these as the equations behind predictive coding
or Kalman filtering usually separated into a prediction what I predict the world will do
given its currents my expectations about its current state and then an update that is informed by
the sensory data that I sample so what's this free energy gradient here this prediction error
well imagine that I had this sensory impression here and I had an expectation a Bayesian belief
that it was caused by a dog and if I had a generative model that could generate what I would see
if I was correct I can then generate the prediction and compare it to the sensation
to form a prediction error which is simply the difference and all this fundamental gradient
flow is saying this predictive coding or Kalman filtering equation is saying is I'm going to change
my mind in the sense of changing my internal states in my neuronal states until the prediction
error has been eliminated and I've minimized my free energy and the gradients have been destroyed
so is that plausible in fact it's very plausible it speaks to a picture which we'll see in a second
where we want to look at I think both Kevin and Anil showed sort of cartoons of this one can look
at the brain as a hierarchical generative model generating predictions so evaluating the quality
of those predictions by evaluating a prediction error the free energy gradients and then revising
beliefs doing Bayesian belief updating on the basis of the mismatch now notice at no point
well I ever know what's going on out there all I'm doing is finding a sufficiently good explanation
in terms of that which minimizes my prediction errors in this instance it was actually a cat
and but that's nice because what it means is forget about all the physics and just summarized
the existential imperative that inherits from the physics in terms of minimizing prediction error
and there are two ways of doing that we can either change our minds to make our predictions more
like sensations or we can act on the world to solicit some more sensations that are closer to
our predictions so we can just realize our predictions actively by changing the configuration
of our body for example or in our internal state we talk about interceptive inference
and autonomic reflexes or we can engage motor reflexes or I can look somewhere else until
I see what I expect to see and thereby minimizing my prediction errors I won't go through this this
is what I would take people through if they did neuroscience in terms of the hierarchical organization
of this kind of scheme the basic message behind this architecture is that say for example visual
input comes in it's a receipt of top-down predictions from the visual part of the brain
it elaborates a prediction error that's then sent forward to revise my beliefs to provide better
elemental descriptions but crucially these expectations are themselves being predicted
in a hierarchical sense with these top-down descending predictions to produce a prediction
error that then drives the high-level expectations more abstract representations to provide an
account of what's going on below this is exactly what Kevin was talking about yesterday in terms of
this layer or level of the hierarchy looking at informing and being informed by the level below
and so on to any hierarchal depth required the particular twist in this graphic though is that
it incorporates action so here there's another kind of prediction error that comes from the
muscles in my eye that I could predict and I could use a prediction error to infer where my eye was
currently pointing but there's a much simpler way that I can minimize these prediction errors
I can just change the stretch of my muscle to match the predictions of the stretch receptors
and what I'm describing here is a classical reflex arc in motor control if I was doing
interceptive inference this would be an autonomic reflex basically actively and reflexively
minimizing prediction errors in relation to deeply hierarchically informed predictions
that are generated by my model that I'm trying to maximize the evidence for
so that's the basic story I just want to finish the story with saying well actually not quite
before I do that let me just illustrate the kind of active inference and the kind of sort of
engagement with the world and the one way of establishing a very simple synchrony with the
world that inherits from the architecture I've just shown you and what we did here is basically
equip a synthetic subject with a generative model that had dynamics autonomy and this kind of
itinerancy that was implicitly referred to yesterday in the form of a central pattern
generator and then map this abstract dynamic to some point in extra personal space and we told
the synthetic subject or part of its generative model was that there was an invisible point that
was moving around and there was an invisible spring that was pulling her finger towards the
moving point so that means that this synthetic subject is now expecting to predicting to feel
her hand being moved around and see her hand being moved around but because she also has reflexes
then she's going to automatically fulfill the addictions of the hand movements and cause the
hand to move thereby fulfilling the visual predictions and with this very simple kind
of reflexive setup we can simulate quite realistic biological kinds of motion and also
by simply removing the proprioceptive or the movement sensing information we can simulate not
just action but what it would be like to observe something very similar performing the same action
but just visually without the proprioceptive input so that's kind of simulation rests upon this sort
of overall architecture that we get some sensations that they are used to do our basing belief updating
by minimizing this variation free energy or what to do next and those predictions are
then used to generate action and this rests on the the markoff blanket that I showed right
at the beginning of the presentation what I'm going to do now is make one very very simple move
and take this sparse coupling that defines the thing from everything else make it slightly
sparser in a very simple but very important way I'm going to remove the influence of my active
states on my internal states and as soon as I do that then from the point of view of the internal
states my active states now become vicarious causes of my sensory states which means that
there now exists an interpretation of my internal states as modeling not just the external states
as causing or using a model of the external states to predict the sensory states but the
external states and the active states now become causes of my sensations so now I have a model
of the causes of my sensations that include my own actions and it's this particular move
that I'm suggesting or from the point of view of simulating these kinds of this kind of self
organization may be a necessary move to introduce agency in the sense that agents will have some
notion some or this can be described as having some sense of their own agency the consequences of
their own action because the their own action is now not directly accessible it can only be
observed by the sensory consequences but therefore have to be modeled and this takes us into a
slightly different and I think richer world where you now have the notion of a model that
incorporates explicitly the consequences of the agent's action I'm not saying at this stage
there's any awareness or sentence of that agency but just it is there that is then used to plan into
the future to evaluate the different consequences of different actions so that you are now implicitly
introducing the notion of planning and a temporal depth into the dynamics that then provide the
predictions of what I'm actually going to do next I won't go through this in any detail but just use
it to make the point that the underlying maths is quite simple and has a telonomy or at least an
interpretation in terms of things that's people's statistics machine learning and economics would
recognize immediately in terms of things like risk and ambiguity or accuracy and complexity
and intrinsic and extrinsic value where it turns out that all of these quantities that underwrite
the planning that ensues from having to have a model of the consequences of my action are well
established in different kinds of literature so for example the intrinsic value is just
the quantity used in visual search known as Bayesian surprise it's also exactly the same
quantity underwrite something I mentioned earlier which is the principle of maximum
neutral information or minimum redundancy principle and it is effectively the thing that
drives our curiosity it's the the thing that minimizes expected surprise by maximizing information
gain and I can sort of strip away various sources of uncertainty and get back to expected utility
but I really want to focus in the final slide on this epistemic affordance that falls out
and is a necessary consequence of just having a model of the consequences of my actions in the
future that I repeat entail a degree of planning and if I'm planning to where am I going to look
next I'm going to look to the I'm going to make the next eye movement that resolves the greatest
amount of uncertainty about the states of affairs out there because that has the greatest intrinsic
value it has the greatest intrinsic motivation it has the greatest epistemic importance it is
salient for me it has meaning for me in terms of what I don't know about the world beyond my
sensorium and you can write this down with particular examples and produce salience maps
that do actually describe quite accurately the empirical behavior of people choosing where to
look next and indeed you can simulate this in terms of a little agent this particular agent
at a very simple universe she was either in universe that where all her sensory input was
being caused by an upright face a sideways face or an inverted face and by carefully choosing
where to look next noting that she could only sample a very small part of the visual field
with her fovea as indicated by these images here and this movie here she can choose the best places
to look to maximize the space in surprise to maximize or respond to these epistemic affordances
and resolve her uncertainty what is she is looking at and she's indeed correctly inferring
she's looking at an upright face so all that can be much more gracefully summarized by Helmholtz
and as follows each element we make by which we alter the appearance of objects should be
thought of as an experiment designed to test whether we've understood correctly the environment
relations of the phenomena before us that is their existence in definite spatial relations
and with that it only remains for me to thank those people whose ideas I've been talking about
and of course to thank you for your attention thank you very much indeed
