Can everyone see my screen?
Cool.
All right.
Thank you, Mike, for giving me this exciting opportunity
to present my work to your lab.
And I'm super excited to hear what y'all think about it
and get your feedback on these ideas.
OK, so my background is in computer science
and experimental neuroscience.
And I've always thought that understanding the brain
requires somehow connecting these two fields
to a greater degree than what we already have.
And so I began my career with the belief
that this sort of connection needs
to be made at the level of perhaps tens of thousands
of neurons.
But very soon, as I dive deeper into the biology
and I thought more about this problem,
I found myself being compelled to think about solutions
at the molecular level.
And right now, I think that solving cognition
at a mechanistic level might lead
to some broader implications across many other subfields
of biology, not just neuroscience.
So what I'm going to do today is I'm
going to present a theoretical model that demonstrates
that with a handful of very simple editing
rules on RNA molecules, you can achieve something
called universal computation.
So to properly convey the motivation and the implications
of this model, I'm going to first give you
some background that sets the stage for the question
that I'm trying to answer.
And please feel free to interrupt,
because I kind of want to make sure
that everyone's following all of this.
I'd much rather not go through a rehearsed version
and have people understand the model as much as possible.
So first, let me start off with a broad topic
of biological computation.
So computational problems arise at many different domains
of biology.
For example, when a bee has to gather
nectar for the hive and compute its spatial location
and keep track of where it is, or in language acquisition
in humans, in human children, or a single cell behavior,
or in domains like embryonic development.
These are all domains that heavily involve computation.
Now, what mediates these kinds of computations?
We think that for the domains of cognition and learning,
it's done through neurophysiology, neural network
dynamics, plasticity rules, and things like that.
And for cell behavior and development,
we think the computations there are mediated
through molecular transduction pathways,
gene regulatory networks, non-neural electric signaling,
and things like that.
But the widely held assumption is
that these are the main building blocks of computation.
And that biological organisms accomplish
these highly complex transformations of information
by putting these kinds of building blocks together
in the right way.
And maybe there's some other building blocks out there
that are used to optimize computation in special cases.
But these, in principle, should be
sufficient for life's purposes.
But the question I want to ask is, are we sure about that?
And how do we know that these are sufficient?
And I think that there are some fundamental insights
from the theory of computation, whose implications
for our study of natural organisms
still haven't fully been explored yet.
So what does a theory of computation
say?
What does computation even mean?
How is it formalized?
And can we apply it to biology?
So in the theory of computation, we
can define every problem, every computation problem,
as a function that maps an input domain to an output domain.
OK, so for example, the problem of addition,
we can define the input domain to be pairs of numbers
and the output domain to be a single number.
Now, a computation system is something
that solves not just one, but many computation problems.
And the function that it solves is determined
by its descriptor, which could be the configurations,
the instructions, the parameters, the text that
represents the code.
So for example, Python, the programming language,
is a computation system.
The descriptor is the text, the code that you type in.
And that determines the problem that it solves.
It determines the input-output function.
Another example is neural networks.
Neural networks are a computation system.
The descriptor can be defined as the set of nodes, the connections,
the weights, the activation functions,
and every descriptor defines a different input-output function.
So what we've learned from the theory of computation
is that different systems, different computation systems,
have different scopes of problems that they can solve.
So as a simple example, one computation system
that we can imagine are Boolean logic gates
or combinatorial logic.
So you can put gates together to form these circuits.
And then you can ask, OK, what kinds of problems
can Boolean logic circuits solve?
And crucially, we're talking about circuits
that are finite in size.
The descriptor is always finite.
It should be, you have to be able to convey it
in a finite amount of time.
So you can use this problem to solve any problem
with a finite size domain.
So for example, 8-bit addition.
You can create a circuit that performs 8-bit addition.
But you can't create a circuit that
solves unbounded addition or addition with arbitrary size.
However, there are programs, like you
can write a program in Python that
can solve problems that deal with arbitrary sized inputs.
So I can write a code that adds two numbers
without an intrinsic limit to how long the inputs can get.
So combinatorial logic, the Boolean logic circuits
that you're seeing here, are a pretty weak system
of computation.
Usually when people draw the hierarchies,
it kind of falls into the innermost layer.
And it's thought of as a very weak system.
So one of the most, I think, most fundamental discoveries
mathematical discoveries of the 20th century
is that with a very simple set of rules,
you can get to something called universal computation.
And what that means is you can create a system that
can solve any solvable problem, any solvable computation
problem.
And the most popularized example of a universal computation
system are Turing machines.
Turing machines are these machines
that work on a one-dimensional tape.
In every square of the tape, there could be a symbol.
And then you have a set of instructions
that define very simple operations
for how to interact with a tape.
If you read this symbol, write another symbol,
and then go to another line in the code.
And again, the length of the program has to be finite.
And with this system, you can solve any computable problem,
meaning that for every problem, for every computable function,
there exists a descriptor that solves that function.
And the crazy thing is it doesn't get more powerful than this.
So you can give it a larger alphabet.
You can give it a 2D sheet instead of a 1D tape.
You can give it fancier instructions.
But no matter what you do, you can't take it
to a higher power of computation.
You can't give it the capability to solve new problems
that it couldn't have solved before.
Now, there are many other universal computation systems
that don't even look like Turing machine.
And in fact, it took mathematicians of the 1930s quite some time
to realize that the many competing models of computation
that don't really look like one another
are actually all equivalent.
So it's not obvious like a general recursion theory.
Can I ask a question?
Yeah, I'm good.
So when you're saying it's not more powerful,
you're not saying it's not have no less or more speed.
It's just the type of the problem.
Yes, yes.
That's a very good point.
You can do that power at least.
It will be faster, but not solving different problems.
Yes, absolutely.
That's a very good point.
So you could change the architecture
to get improvements in the algorithm complexity
or memory efficiency, all that kind of stuff.
But in terms of the mapping of problems to descriptors,
you can expand the types of problems that it can solve.
You can add a new kind of problem.
You might be able to make it faster.
Yeah, that's a very good point.
So this fundamental insight that you can really easily
reach a system of computation that's
capable of solving any computable problem,
this has led to the birth of computer science.
It's revolutionized and shaped modern technology.
But what about its implications for biology?
Now, the thing I want to claim is
that the kind of systems that we have so far envisioned
for computation and biology do not
reach this level of computation power.
So most of the models of biological computation
can be formalized in terms of finite dimensional
dynamical systems.
Now, there's some work showing that finite dimensional
