Can everyone see my screen?
Cool.
All right.
Thank you, Mike, for giving me this exciting opportunity
to present my work to your lab.
And I'm super excited to hear what y'all think about it
and get your feedback on these ideas.
OK, so my background is in computer science
and experimental neuroscience.
And I've always thought that understanding the brain
requires somehow connecting these two fields
to a greater degree than what we already have.
And so I began my career with the belief
that this sort of connection needs
to be made at the level of perhaps tens of thousands
of neurons.
But very soon, as I dive deeper into the biology
and I thought more about this problem,
I found myself being compelled to think about solutions
at the molecular level.
And right now, I think that solving cognition
at a mechanistic level might lead
to some broader implications across many other subfields
of biology, not just neuroscience.
So what I'm going to do today is I'm
going to present a theoretical model that demonstrates
that with a handful of very simple editing
rules on RNA molecules, you can achieve something
called universal computation.
So to properly convey the motivation and the implications
of this model, I'm going to first give you
some background that sets the stage for the question
that I'm trying to answer.
And please feel free to interrupt,
because I kind of want to make sure
that everyone's following all of this.
I'd much rather not go through a rehearsed version
and have people understand the model as much as possible.
So first, let me start off with a broad topic
of biological computation.
So computational problems arise at many different domains
of biology.
For example, when a bee has to gather
nectar for the hive and compute its spatial location
and keep track of where it is, or in language acquisition
in humans, in human children, or a single cell behavior,
or in domains like embryonic development.
These are all domains that heavily involve computation.
Now, what mediates these kinds of computations?
We think that for the domains of cognition and learning,
it's done through neurophysiology, neural network
dynamics, plasticity rules, and things like that.
And for cell behavior and development,
we think the computations there are mediated
through molecular transduction pathways,
gene regulatory networks, non-neural electric signaling,
and things like that.
But the widely held assumption is
that these are the main building blocks of computation.
And that biological organisms accomplish
these highly complex transformations of information
by putting these kinds of building blocks together
in the right way.
And maybe there's some other building blocks out there
that are used to optimize computation in special cases.
But these, in principle, should be
sufficient for life's purposes.
But the question I want to ask is, are we sure about that?
And how do we know that these are sufficient?
And I think that there are some fundamental insights
from the theory of computation, whose implications
for our study of natural organisms
still haven't fully been explored yet.
So what does a theory of computation
say?
What does computation even mean?
How is it formalized?
And can we apply it to biology?
So in the theory of computation, we
can define every problem, every computation problem,
as a function that maps an input domain to an output domain.
OK, so for example, the problem of addition,
we can define the input domain to be pairs of numbers
and the output domain to be a single number.
Now, a computation system is something
that solves not just one, but many computation problems.
And the function that it solves is determined
by its descriptor, which could be the configurations,
the instructions, the parameters, the text that
represents the code.
So for example, Python, the programming language,
is a computation system.
The descriptor is the text, the code that you type in.
And that determines the problem that it solves.
It determines the input-output function.
Another example is neural networks.
Neural networks are a computation system.
The descriptor can be defined as the set of nodes, the connections,
the weights, the activation functions,
and every descriptor defines a different input-output function.
So what we've learned from the theory of computation
is that different systems, different computation systems,
have different scopes of problems that they can solve.
So as a simple example, one computation system
that we can imagine are Boolean logic gates
or combinatorial logic.
So you can put gates together to form these circuits.
And then you can ask, OK, what kinds of problems
can Boolean logic circuits solve?
And crucially, we're talking about circuits
that are finite in size.
The descriptor is always finite.
It should be, you have to be able to convey it
in a finite amount of time.
So you can use this problem to solve any problem
with a finite size domain.
So for example, 8-bit addition.
You can create a circuit that performs 8-bit addition.
But you can't create a circuit that
solves unbounded addition or addition with arbitrary size.
However, there are programs, like you
can write a program in Python that
can solve problems that deal with arbitrary sized inputs.
So I can write a code that adds two numbers
without an intrinsic limit to how long the inputs can get.
So combinatorial logic, the Boolean logic circuits
that you're seeing here, are a pretty weak system
of computation.
Usually when people draw the hierarchies,
it kind of falls into the innermost layer.
And it's thought of as a very weak system.
So one of the most, I think, most fundamental discoveries
mathematical discoveries of the 20th century
is that with a very simple set of rules,
you can get to something called universal computation.
And what that means is you can create a system that
can solve any solvable problem, any solvable computation
problem.
And the most popularized example of a universal computation
system are Turing machines.
Turing machines are these machines
that work on a one-dimensional tape.
In every square of the tape, there could be a symbol.
And then you have a set of instructions
that define very simple operations
for how to interact with a tape.
If you read this symbol, write another symbol,
and then go to another line in the code.
And again, the length of the program has to be finite.
And with this system, you can solve any computable problem,
meaning that for every problem, for every computable function,
there exists a descriptor that solves that function.
And the crazy thing is it doesn't get more powerful than this.
So you can give it a larger alphabet.
You can give it a 2D sheet instead of a 1D tape.
You can give it fancier instructions.
But no matter what you do, you can't take it
to a higher power of computation.
You can't give it the capability to solve new problems
that it couldn't have solved before.
Now, there are many other universal computation systems
that don't even look like Turing machine.
And in fact, it took mathematicians of the 1930s quite some time
to realize that the many competing models of computation
that don't really look like one another
are actually all equivalent.
So it's not obvious like a general recursion theory.
Can I ask a question?
Yeah, I'm good.
So when you're saying it's not more powerful,
you're not saying it's not have no less or more speed.
It's just the type of the problem.
Yes, yes.
That's a very good point.
You can do that power at least.
It will be faster, but not solving different problems.
Yes, absolutely.
That's a very good point.
So you could change the architecture
to get improvements in the algorithm complexity
or memory efficiency, all that kind of stuff.
But in terms of the mapping of problems to descriptors,
you can expand the types of problems that it can solve.
You can add a new kind of problem.
You might be able to make it faster.
Yeah, that's a very good point.
So this fundamental insight that you can really easily
reach a system of computation that's
capable of solving any computable problem,
this has led to the birth of computer science.
It's revolutionized and shaped modern technology.
But what about its implications for biology?
Now, the thing I want to claim is
that the kind of systems that we have so far envisioned
for computation and biology do not
reach this level of computation power.
So most of the models of biological computation
can be formalized in terms of finite dimensional
dynamical systems.
Now, there's some work showing that finite dimensional
dynamical systems are universal.
But the fact about this is that the kind of networks
and dynamical systems that have so far been
shown to be capable of universal computation,
they lack a certain feature called structural stability
that makes them impossible to build practically
or impossible to find to occur in nature.
And there's a really nice paper by Chris Moore that
explains this and the rationale behind why
structural stability is a criteria
for practical realizability.
So this opens a problem for us to solve.
Why haven't we found a universal computation system
where you've been able to imagine a universal computation
system in nature?
And maybe you might say, OK, that's fine.
Biology is messy.
And it gets by with what it can.
And it doesn't really require universal computation
for the kinds of tasks, the kinds of computation tasks
that it wants to perform.
But I find this idea difficult to accept,
especially given that it's very easy to accidentally stumble
upon universal computation.
So these are three examples, weighing tiles, Conway's Game
of Life, Wolfram's Rule 110.
And none of these systems were initially
intended to serve as a powerful computation system,
but they were later proven to be universal.
And back in the 1930s, also, the different kinds
of computation systems that the mathematicians came up
with, general recursion theory, lambda calculus,
and Turing's Automatic Machine, Post's Machine,
all of these ended up being the same.
It seems like there is a bucket of computation power
that's really easy to accidentally fall into.
And we use universal computation systems.
We use the von Neumann architecture
for things that don't really require universal computation.
We use them in toasters, vacuum cleaners.
We use them in appliances that could have been fine
with just analog computers.
So the way I think about it is it's easy to reach
and there are many, in terms of survival and reproduction,
there are many benefits to having a computation system.
So, and also we know that evolution is capable
of building highly complex systems
that abide to the principles of optics,
like your eye or mechanics.
So why not principles of computation?
So I just want to, for the rest of this talk,
let's just entertain the idea
that there is a natural universal computer,
but we just haven't found it yet.
What could it look like?
And I think the most promising place
to search for a universal computation system
is at the level of DNA and RNA with a polynucleotides.
And the reason for this, there's-
Can I ask a question first?
Go ahead, yeah.
Why do you say that physical computers
with an in biology are like,
they cannot be built, they are stable?
Yeah, okay, so there's this property
called structural stability.
Sorry, do you hear me?
I don't see my, okay.
There's this property called structural stability,
which some dynamical systems have it, some don't.
What it means is that the system we're talking about
has a non-zero measure in the space
of all dynamical systems,
meaning that there's a neighborhood
of close enough dynamical systems
that are homomorphic to the system,
meaning that the number of orbits,
the number of like attractor points,
all that is the same.
You can map the space of nearby dynamical systems
to that dynamical system.
If a system is not structurally stable,
that means you can't approximate it
as you get closer to it.
It's not like the closer you get,
the more it resembles the intended dynamics
of the system you're trying to construct.
So even if you construct a system
that's close to like with an error of one part per billion,
you still haven't created a system
that can be mapped onto the intended system.
And here we're talking about noise,
not during runtime, but noise in the parameters.
So you have to fine tune the parameters of your system
so much that there's zero noise.
There can't be any amount of noise.
Does that make sense?
Yeah, it makes sense.
And do you have any reference for this stuff
because I think it can be interesting?
Yeah, of course.
So there's this paper by Moore
and in that he references a bunch of other,
his own papers,
papers that have shown that universal computation
is possible with, let's say neural networks,
but all of those are structurally instable.
So if you, I can also send this paper to you
if you want after this.
No, I took a note, I will look it up later.
Of course, yeah, right.
Okay.
Okay, so I think the most promising place to search for it
is at the level of RNA and DNA.
And people recognize this going all the way back
to the 70s, but most of the previous work
have been attempts to build synthetic models of computation
using these molecules.
And some of these models are universal.
But the question that we're interested in
is not whether we can build
a universal computation system with RNA and DNA,
but whether nature has already done so.
Okay, so but what could it look like?
Of course, you can imagine
some very complex molecular machine
that looks something like a universal Turing machine
that operates on DNA,
but that sort of machine would have to be huge.
Ribosomes, which do the very simple task of translation
are 20 nanometers in diameter
and they're visible in electron microscopy.
So a universal Turing machine that would have to follow
some much more complex logic would have to be enormous.
So the question is,
can we imagine a molecular computation system
that doesn't require these kinds of extraordinarily complex
molecular machines and could have evaded detection until now?
And that's what I'm going to try to answer
with the model that I'll present.
The details of the model that I'm gonna present
are almost certainly incorrect,
but they're only meant as a proof of principle
that you can imagine something like this can exist
without being undetected.
And then we can say, all right,
let's try to look for similar systems
or some kind of in the space of all possible
computation systems that could have evaded detection.
Let's find what's going on in biology, all right?
Okay, so the model that I'm going to present to you
is all about bridging two fields,
already molecular biology and a combinatorial logic
for equivalently lambda calculus
and recognizing the parallels between the structures
in these two fields and how they're operated on, okay?
So I'm gonna present each one of these.
I'm gonna first present a combinatorial logic,
what it is, and then briefly how to use RNA biology
or how to imagine RNA biology could be
implementing a combinatorial logic.
So in combinatorial logic,
we have these entities called combinators.
And combinators are essentially just functions
of functions to functions.
They take other combinators as inputs and output a combinator.
For example, the I combinator is a function
that returns whatever it receives as input.
The K combinator can take two inputs
and return the first input and deletes the second input.
The C combinator takes three inputs,
returns the first applied to the third applied to the second.
So it basically swaps two of its inputs.
So let me just show you how this looks like in action
with two simple examples.
If I wanted to compute CKKI, I look up here,
what's the rule for C?
I have to swap the second and third input.
So if I do that here, I get KIK.
Now to compute this, to apply the left most combinator,
I look up, what's the rule here?
It says KXY equals X, I have to delete the second input.
So I just return the first inputs.
So this ends up being I.
Here's another example with parentheses
to show you that you can treat combinations of combinators
as a single entity, a single term.
Okay, so the really cool thing is
with only a handful of combinators,
a handful of these kinds of rules,
you can construct a system
that's capable of universal computation.
So here's a list of a bunch
of different primitive combinators
and two basic sets, two sets of combinator primitives
that achieve universal computation are BCK and W or SNK.
And these are the most popularized examples
of universal sets.
So with just these four combinators, BCW and K,
putting them together in the right way,
I can construct functions that solve
any computable function.
For example, if I want to construct something
that solves this transformation of information,
something that mimics this Boolean logic circuit,
I can do that with this term
that I divided across two lines.
So this function, it takes ones and zeros
represented with K and KI.
And if I mindlessly apply each one of these rules
step by step, I end up with the correct answer
that corresponds to the output
of this Boolean logic circuit.
Now, I want to emphasize that, again,
Boolean logic circuits are the weakest form of computation.
This is just an example of something
that a combinator logic can do,
but you can also implement more complex functions
that Boolean logic circuits can't compute.
So for example, you can have a recursion,
you can implement unbounded addition,
you can basically any program,
any algorithm that you can code up
in your favorite programming language,
you can also code it up in the language
of combinator logic, okay?
So it's a very powerful versatile computation language.
Now, how do I implement this with RNA?
Well, representing strings is trivial with RNA.
We even have a precedent for that in biology.
We know that biology uses RNA strands,
which are composed of an alphabet of nucleotides,
four symbols to represent strings of amino acids.
So it's using these codons,
we're using triplets of nucleotides.
So it's easy to imagine some kind of representation scheme
that maps motifs of nucleotides
to the symbols of combinator logic,
but implementing the rules of combinator logic
is where the challenge lies, not just representing.
So representing terms is easy.
We want to figure out what kind of rules
can we imagine happening at the molecular level
that correspond to the application rules
of combinator logic.
And we know that RNA strands are frequently edited.
One of the most common editing rules is splicing,
almost all RNA strands undergo splicing
where some part of the strand is excised
and the two ends are ligated together.
And what I want to do is try to imagine
how can we get editing rules like this,
very simple operations like this,
to map onto a set of application rules
that compose a universal computation system.
Now, the most challenging part of doing this
is parenthesis matching.
In combinator logic, if I want to execute
the application rules properly,
I have to know what a full term is.
So if I'm confronted with a open parenthesis symbol,
I have to be able to tell where the corresponding
closed parenthesis is to treat that whole stretch
of nucleotides as a single term, to maybe swap it,
delete it, or do whatever I want to do with it.
So if I show you this string and I ask you,
what's the corresponding closed parenthesis
to this open parenthesis?
You can't do it immediately.
You need to scan the string with your eyes,
keep track of the parenthesis depth,
adding for every open parenthesis,
subtracting for every closed parenthesis,
until you get to zero.
And then you can call this entire stretch of symbols
a single term and then do whatever you want to do with it.
Now, to imagine something like this happening
at the molecular level and natural,
and endogenously in cells,
that would be too much to expect
from an undiscovered molecule
because you'd have to traverse an RNA strand,
which already kind of looks like what a ribosome is doing.
And you also have to keep track of an arbitrary integer,
ads, track, there's a lot of things
that would need to go on to actually carry out
this operation that we do in our heads.
But fortunately for us,
RNA strands already solve this problem
of parenthesis matching.
The same way that DNA strands can come together
and form double helices,
RNA strands can also base pair with each other.
And sometimes the base pairing happens
within the same strand.
So it's two different segments of the same strand
can come together and form these base pairs.
And what results are these highly intricate
tree-like structures of a base paired segments
of the same RNA strand?
And now we can interpret these base pairings as parentheses.
So if I'm scanning an RNA strand,
when I approach one of these stems,
I can call this part an open parenthesis,
the content inside the stem, which is called the loop,
but you can call that the content,
and you can interpret that as the content
within the parentheses.
And then this other part of the stem
would be the closed parenthesis, all right?
And this example that I'm showing on the right
is an actually existing non-coding RNA strand.
So it doesn't code for proteins.
It's over 2,000 nucleotides long.
It's expressed in skin cells.
There's a debate whether it's really important or not,
but it's highly conserved.
There's kind of like a lot of mysteries
going on with non-protein coding RNAs.
So if we interpret secondary structure
as encoding a parenthesis structure,
now I can give you examples of what the operation rules
could actually look like in the RNA world.
So the applications are combinatorial logic.
I can show you how you can implement them
with very simple editing rules in RNA.
So here's one example that shows how to apply
the B combinator.
The rule for the B combinator is shown on the right
over here, and one important detail here
is that I'm using a different parenthesis convention
from what is typical in the field of combinatorial logic.
And combinatorial logic, usually they use
left parenthesis convention, meaning that if you write XYZ,
that means a parenthesis around X and Y,
and then a whole parenthesis around XY and Z.
But that makes, the difficulties with that,
if you wanna implement that with RNA, for some reason,
that I can get into a right associated parenthesis
suddenly magically kind of work out nicer
so that you only need cleavage and ligation
to apply these rules.
So okay, so putting that detail aside,
this is the application rule for B.
Now, let's walk through how this transformation here
corresponds to this transformation here.
First, I'm gonna show you how this RNA structure
corresponds to this term here.
So if we start from the three prime end,
you all can see my mouse, right?
Yes, okay.
So if we start from the three prime end,
this part can be interpreted as open parenthesis,
then open parenthesis, then open parenthesis.
And as I'm reading this, you can notice
how this corresponds to the symbols over here.
And then the B motif, which could be some hypothetical motif
or some codon or something that represents the B combinator.
And then we have X, and X could be anything.
It could include other stems inside itself,
and that's how you can get variables, basically,
nested parenthesis inside parenthesis.
So that's X, and then close parenthesis,
and then Y, and then close parenthesis,
and then Z, and then close parenthesis.
So X, Y, and Z could be anything.
And the structure on the right here
also corresponds to this.
So start from the three prime end, open, open, X, close,
open, Y, close, Z, close.
And I just read out this string over here.
To get from here to here, all you need to do
is a few cuts and ligations.
So you only need an enzyme to come in,
recognize the B motif,
and then perform cleavages and ligations
at fixed relative positions
to where it found the B motif.
And then you can get this, this structure on the right.
And then that enzyme could float off
and do something else.
And these are examples,
these are the transformations for two other combinators,
for the C combinator and the K combinator.
And again, all of these can be implemented
with only cleavage and ligation, locally defined.
Because we have this parenthesis structure,
the enzyme that comes in to carry out
these application rules doesn't really need to care
about detecting what counts as a full term,
where the matching parentheses are,
it's already there in the RNA structure.
And that makes it easy to imagine to already occur.
Okay.
Can I ask a quick question?
Of course.
So for the strings,
there is an intrinsic left to right ordering, right?
And I noticed that for the RNA strands,
it chose to go from the right strand.
Is there something intrinsic about the RNA strands
that sort of conveys this right to left ordering
or it just chose to do it that particular way?
So maybe I understood your question
if I didn't correct me,
but RNA strands, they have an intrinsic directionality
and people usually denote that with the three prime end
and the five prime end.
So there is a directionality in how you can traverse
an RNA strand.
In fact, that's how a single RNA strand
can uniquely encode a protein structure,
which is a string of amino acids
without accidentally coding for the reverse representation.
Is that was that your question or?
Yeah.
Yeah, that answers the question.
Thank you.
Yeah.
Yeah.
So there's no ambiguity in terms of what the string represents
because of directionality issues.
Okay.
So these three combinators by themselves,
they don't form a universal set.
So going back to this chart over here,
I've shown you B, C and K, but not W.
And the reason I didn't show W
is that I can't imagine W happening
with only cleavage and ligation
because W requires something more complex.
It requires duplicating a strand of arbitrary length.
So if you look at rule for W,
it says WXY equals XYY.
So there's a term that appears twice on the right side
and once on the left side.
And to imagine something like that happening naturally
in cells is kind of difficult.
You need some kind of enzyme to go in
and duplicate a strand of arbitrary length
and then reintegrate that back into the strand,
the template strand.
And we have things like RNA dependent RNA polymerase,
which uses an RNA strand as a template
to produce another RNA strand.
But first of all, we can't imagine a way for that
to systematically integrate that back to the same strand.
And also, RNA dependent RNA polymerase
doesn't exist across all life forms.
So, and I'm trying to imagine a system
that could exist across all life forms
and not be detected yet.
So fortunately, again, there's another trick
that can help us circumvent this need for the W combinator.
And to get to that, I wanna first introduce
how we can implement variables
and referencing with the system of computation.
So if we interpret parentheses as being represented
by these RNA duplexes,
I could imagine a system of addressable memory
within the cell,
where every variable is represented with an address sequence
and it can be referred to by its reference sequence.
So a strand that's representing a variable
might have the address sequence on its five prime end
and the content of that variable in the rest of the strand.
And then another strand could reference that sequence
with the reverse complementary sequence.
When these two strands come together,
they can base pair.
And with only a single cleavage and two ligations,
you can get the content of that variable nested
in parentheses exactly where it was referenced, okay?
So this creates a system where you can have
a shared addressable memory space within a single cell
where RNA strands can interact with one another,
they can refer to other strands
and they can even refer to copies of themselves.
And through that, you can implement recursion.
So if we have recursion,
we really don't need the W combinator
because one of the uses of a W combinator
is to basically define a function in terms of itself.
But if we already have a system that's capable of doing that,
we don't need the W combinator.
So this system slightly deviates from combinator logic
because it has explicit variable naming
and it has explicit references.
And to prove that this is a universal computation system,
I had to construct a proof
that you can simulate any Turing machine
with linear algorithm complexity
using that addressing and referencing system
and only these three combinators, okay?
So without the W combinator,
with that addressable memory system,
you can still achieve universal computation, okay?
So as an example of what you can compute with the system,
I constructed this RNA program
that solves unbounded addition.
It consists of two functions,
this A function and the B function.
B is recursive, it refers to itself
and A is recursive, it refers to itself
and also refers to B.
And to calculate something like six plus two,
you can do that through building this strand
which has some part of the strand
is representing the number two,
some part of the strand is representing the number six
in some contrived binary representation scheme.
And this strand refers to the program A.
So through those rules,
A gets integrated into this strand,
the operations, the BCK operations get applied,
they can be applied in any order.
So the order doesn't matter,
that's something, another cool thing
that kind of magically works out with combinator logic
where the order of operations doesn't matter
and the end result, so it can happen in parallel.
And at the end, you get a strand
that represents the number eight,
which requires four digits,
not the inputs were a two digit number,
a three digit number, but the number eight
is a four digit number, but that's fine,
it can deal with adding digits.
And if I simulate this,
just to show that it's efficient
in terms of its algorithmic complexity,
you can see that the number of operations
grow logarithmically with respect to the sum,
the final result, okay.
So this is just a demonstrate,
I don't think this is how addition actually happens in cells,
this is just a demonstrate that you can treat this system
as a programming language,
just like any other computer programming language.
You can construct any algorithm you want,
there's no like non-linear slowdowns
in terms of the number of operations.
So to summarize, DNA contains,
to summarize the theoretical model,
DNA contains a static library of functions
which are used for computation.
And sequences which encode these functions
are transcribed from DNA to RNA.
And a handful of cleavage and ligation rules
execute the steps of these functions.
I showed you three rules,
but you could imagine other sets of combinators
that execute different kinds of application rules
and also reach that level of computation power
that's needed for universal computation.
And a function can refer to any other function
including the copy of the same function
and this implements percussion.
Okay, so this is a theoretical model
and the question is,
does something like this happen in cells?
Obviously we don't know,
but the idea behind this model
was to inspire empirical investigation.
So now that we know that something like this can exist
and can be mediated through very simple rules
that could have got unattected,
now the goal is, okay,
can we try to find how nature actually does this?
And can we use some of the details in these kinds of models
to guide us in finding a universal computation system?
So those are the steps that I'm currently pursuing.
So one of the things I'm looking at is if this is right,
if RNA strands are being edited,
it should be possible to find traces
of these editing rules in sequencing data
if you're using the right kind of method for sequencing.
So what I'm looking at first is evidence
for swapped segments in RNA strands.
We already have ways to imagine how segments
within a standard can be deleted with things like splicing.
So that wouldn't be surprising,
but if we can find systematic rules
that determine how two parts of a strand
can show up in reversed order, that would be surprising.
And that might lead us to one of the,
maybe one of the operation rules
that implement a universal computation system.
So what I'm looking at right now
are high fidelity, full length reads,
sequence with packed bio from zebra finch brain tissue.
The data is publicly available,
it's published by the Jarvis lab here
at Rockefeller University.
And this example that I'm showing you on my screen
is one of the biggest examples
that I found in this dataset.
You can see that there are two reads to RNA strands
that contain sequences that are both
over a thousand nucleotides long,
but appear in reversed order.
And we currently don't know of a canonical rule
that could, or a canonical, I don't know,
our name modification rule that could lead to this,
lead to having parts of the same RNA strand swapped.
And this is just one example of a hit,
but if I plot the distribution of all the sequences
that I found, all the identified pairs ordered by
the sum length of the swap sequences,
I get this histogram over here.
And I'm only showing you the far right end
of this distribution on the left side is hidden
because there's a really huge peak,
which is where all the noise is.
And then there's this distinguished clump of hits.
And I think this is one of the reasons why I think
this is not just noise and something systematic
is happening is you can see this set of all these hits
that are very distinguished from the noise cluster.
Okay, and another direction that I intend to pursue
is I wanna look at learning and memory assays
in behaving organisms and try to find correlates
of memory in RNA sequences.
So most of my postdoctoral work is working
on fly path integration.
And ultimately I want to find RNA correlates of memory
of the spatial location, not just in the expression levels
of known transcripts, but in the actual primary sequences
of the RNA reads.
I'm hoping that we can find unique reads
that might represent cognitive memories
within these organisms.
And I don't wanna limit this just to flies.
I wanna look at other tasks and other organisms
and collaborate with other people to try to find
RNA correlates of learning and memory in other species.
So an obvious question,
if RNA is underlying cognition and memory,
an obvious question is where is the interface?
And how can, where's the interface between this RNA system
and like, I don't know, neural spiking?
And you can imagine the different possible input
and output mediums to this RNA computation system
and there's a recent paper that proposes
that the same way that we have the nanopore sequencing
that convert RNA sequences to memory and potentials,
you can imagine maybe something similar
like this is happening inside cells
where you can convert the precise primary sequence
into a sequence of voltage signals.
So I want to end with a quote from an underappreciated
cell biologist that I believe was far ahead of her time,
Beatrice Gelber was studying adaptive learning
in the Paramesian, a single cell organism back in the 1960s.
And in one of her papers,
she characterized her work as a new approach
to behavioral problems,
which might be called molecular biophysics.
Simply stated is the hypothesis
that the memory and grant must be encoded
in macromolecules, possibly the biochemical
and cellular physiological processes,
which encode new responses are continuous
across the phylo as genetic codes are,
and therefore would be reasonably similar
for a protozoa and a mammal.
And I really like this quote because one of the recurring
themes in the history of science is realizing
that phenomena that look very different
and appear to be dissimilar are actually governed
by the same principles, like the falling of an apple
and the orbiting of the moon.
And I think that would be a very exciting prospect
for the future of biology.
And with that, I'd like to thank all the people
who have helped and inspired me in this work.
Gabby Maiman, my advisor here at Rockefeller,
Jeremy Ditman, Abbas Brizvy, Randy Galistel, and John Matt.
And thank you all for listening.
Super, thanks so much.
Any questions?
Anybody else?
I did have another question.
Could you explain a little bit more
how the address referencing works?
Because in computers, addresses have a sort of convention
and implicit convention.
And the other sort of concern is that, at least coming
from the internet, is that there's
that at least coming from something like C programming,
the address variables are explicitly
defined in a way that's different from other variables.
So they see the star of conventions.
The star of the variable means it's a referencing variable.
So when it comes to RNA strands, are there
like specific differences between strands
that enable or facilitate this sort of distinction
between memory variables and ordinary variables?
Yeah, that's an excellent question.
And that is a detail that I kind of glossed over.
So whatever the representation scheme is,
it cannot interfere with how the combinators are represented.
So ideally, there wouldn't be an ambiguity
when you're looking at a sequence whether this
is like referring to another strand
or whether it's representing like, I don't know,
B apply to C apply to K.
And you could do that by coming up
with unambiguous representation schemes for like in the way
the same way that RNA strands represent amino acids,
they also need to represent operations like start and stop
for the ribosome to know when to stop transcribing,
sorry, when to stop translating and when to start.
And it uses the same scheme of representation.
It uses codons.
So you need some, but it doesn't like, what is it called?
It's not ambiguous.
So it's not like sometimes it's not clear whether this is
a stop code on or a certain amino acid.
So you just have to come up with a representation scheme
where the differentiates between motifs that
are used for address sequences or the combinators.
Does that make sense?
Like for example, I don't know, it's
going to sound very arbitrary.
That's why I didn't really give an example.
But let's say if it starts with GC,
then the following 10 terms are going to be,
the following 10 nucleotides are going to be an address.
But if it starts with AC, sorry, yeah,
if it starts with AC, then it's going
to be one of the three combinators, something like that.
Yeah, that makes sense.
How about the address convention?
So for example, if you have a single dimensional grid,
then you can say that the leftmost location is one.
And then as you go on the right, it's two, three, four.
And it's like a two dimensional grid.
Then we can assign some sort of convention.
But it sort of suggests that there must
be an agreed upon convention at Strolls somewhere.
How does that convention work here?
Is there sort of an implicit convention or some sort?
Yeah, so the space of variables is not really it does it.
There's no way to spatially organize it necessarily.
There's no single way to spatially organize it.
It would just be every sequence would correspond to a variable.
So it's kind of like the space of all strings,
as if every string would represent a different variable.
And you can refer to that string.
And these RNA strands are kind of floating around in the cell.
So an RNA with a certain address could be anywhere in the cell.
I don't know, those are details which are kind of like even
more speculative than what I've speculated already.
I have a question about something that I mentioned before.
You mentioned that I truly understand that some devices,
for example, vacuum cleaner, don't really need a universal machine
to operate.
They can do whatever they need to do with some analog computation.
Maybe we don't need also the universal computers
in order to describe consciousness or whatever
we would like to describe.
Maybe we created a machine that have more complex theoretical,
complexity mathematical, more than us,
but that not necessarily needed to be in order to describe us.
That's make any sense.
So maybe I'm saying that the whole search
for universal Turing machine in nature
is maybe not the right question.
Yeah, that absolutely makes sense.
And that's how I used to think about it.
But my thinking has kind of changed for several reasons.
One is that I might be repeating myself,
but if I am, feel free to interrupt and say
what you're concerned about these arguments.
So first of all, biology could come up
with very complex systems through evolution
that abide to the principles of optics.
Like our eye has an aperture.
It has a lens, adjustable lens, adjustable aperture.
And it looks really much like a camera.
And it's not just a coincidence.
They're following principles of optics.
If you want to project a 2D image of your environment,
you kind of have to follow these rules.
And then for universal computation,
I feel like it's even less complex.
It's even easier to accidentally stumble
upon a universal computation system.
Like my intuition is that you need Darwinian evolution
to evolve an eye, but you don't necessarily
need Darwinian evolution to evolve a universal computer.
You might not just accidentally create it by chance
without a Darwinian evolution or a selection.
So these two things kind of make me think
that it's very likely for it to already exist
and then be used for domains that don't necessarily need it.
But it's also possible, and I have to acknowledge this,
it's possible that maybe it just doesn't exist in biology.
Maybe as you said, universal computation
is a man-made construction.
But I guess there's something about that that just
doesn't sit well with me.
I completely agree.
But I'm saying maybe universal Turing machine
is a machine that is very, very sensitive in terms
of input and output is not like analog computers, which
analog computers are much more robust in terms
of definition of all the input range and output range.
And you can decompose and compose more parts on that
and which universal Turing machine,
you require lots of definition and a lot of interface
in order to define exactly the input output of each one
of those machines.
So maybe that's why biology is not doing this.
I don't know if my intuition agrees with that.
I feel like my intuition is that digital computation is
much more robust to noise, especially
because it can deal with issues of noise
at the algorithmic level.
Like you can have checksums and stuff like that.
And the nature of a digital computer,
there's discrete attractor points for bits and things
like that.
I don't see the advantage of analog computers,
especially in terms of robustness to noise.
But I agree in terms of complete noise.
But in neurons, for example, there
are sometimes spontaneously spikes.
They create a lot of noise inside the mechanism itself.
That's an assumption, right?
We don't know what those spontaneous spikes are.
They could mean something.
And we just haven't discovered what the code is.
Especially, it doesn't contradict this kind of model
to see spikes that we can't explain.
Right, yeah, I understand your point.
It's not a question.
I just want to hear what you think about it,
unless you're not done, Hannah, now.
I don't know if I have seen anyone directly say this,
but I could be very wrong, because I'm bad at reading.
So often when we search for universal computing systems,
you take a system like this RNA idea and say,
ah, this could do universal computation, so we're fine.
Could it also be, though, or if we go back to the Moore paper,
you have a dynamical system that does not
have structural stability, so this is hard, blah, blah, blah.
But it could be that you have many different subsystems
that then, in and of themselves, as an individual system,
are not that.
But when coupled together, they could be.
So could you build a universal computer?
It's out of pieces that are not universal computers.
Like, you put neural activity on some of these at some point.
So maybe neurons in and of themselves
are not universal computers, nor are LFP fluctuations.
But when you couple the two things together,
when you combine them, they can then
do universal computation.
So instead of looking for one individual system that
can be this thing, maybe it's wiser,
or has anyone thought about this, or have you thought about this?
Could it be that you only find this
when you couple things together?
Like, a tape doesn't have to be a tape.
What if the tape is this, and the head is this,
and then the computation says, if you couple things together,
it might be that system A, when reading from systems B and C,
is all of a sudden universal in a given context.
But in another context, it's not, because it needs to decouple.
So I imagine, in my head, at least,
that you have multiple dynamical systems coupling
and uncoupling from something that, when they're
coupled a certain way, they're universal.
When they're not, they're not.
And what do you think about that?
Is that insane?
Yeah, yeah.
Yeah, I totally get it.
So I don't know of an example of this, where you have,
I don't know, you can put LFPs and something else together,
and then suddenly it becomes universal.
But I think tangential to your point,
you could use neurons to construct
a universal computation system.
It just wouldn't be a traditional neural network.
So for example, to give you an example of how it could work,
you could simulate cellular automaton,
a similar automaton system, with neurons.
And people have come up with models for that.
And if you have the right update rules for your cellular
automaton, you could achieve universal computation,
like Conway's Game of Life.
So it's not that neurons in themselves,
they can't reach this level.
It's just, you have to come up with,
you have to explain what the system is.
The difference between that system
and what we conventionally think of as neural networks
is there, with a cellular automaton simulating model,
when I say my network solves addition
or solves this problem, what I mean by my network
is like an additional activity pattern.
I'd be saying, OK, this on, this on, this on, this off,
this off, this off, this is the solution to addition.
And this can be embedded in a neural substrate
of arbitrary size, or unbounded size, or whatever.
And if it runs out of size, you can embed it in a larger
network, and then it'll compute the problem that you want.
But when people say neural networks,
what they typically mean when they say,
hey, this neural network solves this problem,
is nothing like that.
But what they mean is a set of precise weights,
the actual nodes, and the activation functions to put them
together, create a neural network,
that's the thing that solves an input-output function.
And that's what I'm criticizing here.
That doesn't, as far as we know, that
doesn't arrive at the level of universal computation.
But it's very possible to imagine it happening,
I don't know, with neurons, or even with at the cellular level,
like, you know, implement cellular automaton
using just single cells that aren't neurons.
It's just, it's just, I,
