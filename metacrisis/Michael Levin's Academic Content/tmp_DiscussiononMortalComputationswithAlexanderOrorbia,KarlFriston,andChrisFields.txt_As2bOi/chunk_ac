give you access to some of that
because maybe you can do some analyses.
You know, we have bioelectric simulators
and things like that.
You know, another kind of model system to think about,
and this is something that Patrick is doing at the bench.
And then I have somebody who is doing this,
you know, the computational analysis of it,
are these gene regulatory networks, right?
And so the abstraction, of course, is quite simple.
It's just, in the continuous case, it's a few ODE's
and they just, you know, their nodes
that turn each other on and off and that's it.
But if you study these things,
you find some really interesting features.
For us, one of the most interesting things is that
if you do temporary stimulation
of the different nodes, right?
So you just grab one of the node values
and you crank it up or down for a little bit
and you keep the structure of the network completely fixed.
So you're not changing the weights,
you're not changing the topology,
the hardware is completely fixed.
All you get to do is temporarily raise or lower
the activation of any node
and then you wait and you see what happens, right?
So if you do that and if you treat it
in a sort of behavioral science context,
you can show things like habituation,
basically six different kinds of memory,
including Pavlovian conditioning.
So these things learn.
And we've been very interested in this question.
I mean, so I have a couple of papers showing how they learn,
but one of the really interesting things
is because we don't let the hardware vary.
So this is not a scenario
where there's some kind of synapse
whose weight gets tweaked by experience.
The fact that they learn the most,
to me, one of the most interesting things about it is,
where is the learning stored?
And this is something that all of the reviewers
of the original two papers got hung up on
because we say again and again, the hardware does not change.
And then they all said, great,
but then you can't have it
because where could the memory possibly be, right?
And it's this dynamical systems thing
where they get chased into a regime
where future stimuli are going to cause
very different outcomes because of their history
than past outcomes.
But I wonder, and so this is what I was gonna ask you guys
to kind of think of them,
to talk about from your framework's perspective.
I wonder if the business of uniqueness
is related to this sort of issue.
I think maybe called privacy or something like that,
this idea that there is an inner perspective to a system
that's had a certain set of experiences, right?
It has a history in the world
that is not available to outside observers.
And this is, we spent a lot of time with my postdoc,
Federico and I spent a lot of time thinking about,
you look at a network,
can you tell whether it's been trained?
And if so, what hasn't been, like, can you read its mind,
you know, this kind of neural decoding kind of thing
because you're not gonna get it from the hardware.
You can, the nodes are no different, right?
So in fact, we have a visualizer
that tries to show various aspects of it.
And if you, you know, on the left and right of the screen,
first you start off with the hardware of view of it.
And that never changes throughout the whole time.
But as it learns, right,
over multiple experiences and stimuli,
something absolutely changes.
And then we have some ways of thinking about it.
But this question of, can you, as an outsider,
is there anything about mortal computation
that speaks to this issue of what you can tell
about a system as an outside observer
versus what you know as the system yourself?
You know, from the inner perspective,
is that something you guys think about?
Well, I'm gonna give a piece of it
and then I'm gonna hope Carl can tag in a little bit
because I think he can flesh this out a little bit better.
So, and this might be confusion
over what you might have explained, Michael,
about the reviewers.
So you said, I fixed the hardware
and on top of that, I fixed the plasticity
because you said we can't change the, you know,
the values of the synapses or the connection strengths.
And I do think mortal computer, mortal computation,
we did address this.
So Carl and I decomposed it in, again,
it goes back to mills.
But again, I might be misunderstanding.
So we're gonna decouple the privacy
and the observer perspective
because I wanna hear what Carl might have to say to that.
But for why learning would still happen,
even when you fix those things, it's just the inference.
And the way that we looked at it in mills
was there's these different time scales of learning.
So if you were to pin the structure of the S and mills
and then pin L and say, you can't modify those.
Well, we still had one more piece,
which was the very fast time scale.
And you talk in your poly computing paper,
I've done a lot of work in that.
Carl obviously has done a lot as well,
predictive coding, predictive processing.
We always have the inference dynamic.
So the idea is that, and I'm sure you thought of this.
This is why I was kind of surprised
the reviewers were maybe not understanding.
So there's like short-term plasticity, right?
So the idea is that when you're doing
expectation maximization in a predictive coding network,
I can still change the neuronal activities,
the firing rates or the spiking rates,
depending on what model you're constructing.
And the synapses never change
and forget about the morphology
because that's a whole nother ball game.
And I would get adaptation.
And there was a very interesting paper that came out,
I don't know now, just like two weeks ago,
Wolfgang Maas in spiking neural nets talked about,
well, look, I don't need to modify the synapses.
I'm gonna do everything in my spiking neural architecture
with just homeostatic variables,
which is, you didn't call it them,
but that's just the adaptive thresholds.
He's like, if these change,
so we have this short-term kind of non-synaptic adaptation,
you get all these effects.
And he actually showed it again,
it's a machine intelligence task,
but showing in all these tasks
without learning in the sense of modifying synapses.
And that was very interesting that you can go very far.
And I'll try to dig up that paper.
It was something I wanted to go in more detail later myself.
So in Mills, right, we're just saying,
well, okay, we're obviously under mortal,
but the inference dynamics
and the fact that these still follow the gradient flow
of the variational free energy
that defines your system
or your functionals that you're looking at
would explain why that adaptation that you found would happen.
And I'm sure you thought of that.
I don't know why the reviewers specifically wouldn't have said,
well, this doesn't make sense.
How could you learn?
If you would pin doll three,
well, it's a static system.
You are freezing it in time.
And then that would baffle me.
So that's my comment
that I do think the framework definitely speaks to that
because Carl and I were very adamant
about the separation of time scales,
at least these big time scales.
I mean, there's all these intermediate ones
that I'm sure you could bring up.
And you need them all
because there's a causal circularity
if you want to build the most powerful type
of mortal computer.
And there was a sentence I can't remember
because Carl and I have done many revisions of that paper.
It might have been in one of the earlier ones
where I mentioned something like,
well, even though I'm seeing morphology as important,
technically, if I was only allowed one,
I still have mills.
It's just a very simple search space, right?
It's a, well, we know that you're here.
You can't change the architecture.
So we didn't break our framework.
So that would allow us to subsume machine learning
and say, well, machine learning
is like this very, very narrow case.
It is doing something that you mills could explain.
It's not mortal, but at least it has like a fixed topology
and synaptic plasticity is there.
And we are just really, really speeding up
the inference dynamics by making it one step
because we don't use EM most times.
And deep neural nets for sure we don't.
So that was my comment about addressing the learning,
the fact that things, if you fix so much,
