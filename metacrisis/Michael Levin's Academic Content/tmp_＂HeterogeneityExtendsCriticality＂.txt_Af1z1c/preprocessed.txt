Thank you very much for the invitation.
Pleasure to be here.
So I wanted to share some of our recent work,
but it took several years to develop,
to finally crystallize it in its present form.
And it has its origins in some work
that began more than 10 years ago with several colleagues
from the physics institute back in Mexico City.
So we began studying rank dynamics.
We were first using rankings of the Google Vengrams data set.
So the words in different languages, they can be ranked.
And you get a distribution which is nominal seed flow
because you're actually described it in the 1930s
and when the distribution had been described already
in the late 19th century.
But let's say we had enough data so that we could
study how these distributions change in time.
We explored many things.
And a couple of years ago, we published a paper
in Nature Communications where we, let's say,
after analyzing dozens of different data sets,
where we had enough data to study how rankings change
in time, we found some universalities
and we explained why these type of dynamics are universal.
But even before that, we were wondering
why, let's say, in languages, or in sports,
or in 1,400, 500 companies, or in city population.
I mean, all these different data sets
where you can get rankings and then
these rankings change in time, we
were finding similar dynamics or, like in this image,
rankings of universities.
Roughly speaking, in all of these,
you have the low ranks.
In other words, the most relevant elements changing slower
than the least relevant elements.
And our hypothesis was that this was useful for evolution
because at the same time, you could
have in a system robustness and adaptability, which
you want the essential features of a system
to be preserved so that its function is maintained,
but you also want some adaptability.
And it makes sense that the least relevant elements
are the ones that change so that, let's say,
you can explore at the same time that you maintain
the most relevant features of your system.
And I mean, this has been studied sort of in complex systems
and it's related to criticality that I will mention in a moment.
But let's say it seems that when we had this temporal heterogeneity,
we were getting for free this at the same time
robustness and adaptability that you find in criticality,
but I mean, I will go more into it.
So we wanted to study different types of heterogeneity, which
is basically when you have a diversity of elements.
It's usually easier and simpler and convenient to model
systems as homogeneous because then you
can treat all the elements as the same
and then you can make several simplifications.
However, it's actually easier for nature
to have heterogeneity than to impose
some mechanisms that will ensure homogeneity.
And an example you can see in synchronization
that it's easier not to have synchronization
and synchronization would be an example of homogeneity,
temporal homogeneity.
And if you have different elements with their own frequency,
then you have heterogeneity.
But of course, mathematically, it's
easier to speak about homogeneity.
But let's say we have powerful computers
so we can study heterogeneity and we have enough data
to try to match this with real phenomena.
So I briefly mentioned three types of heterogeneity
that we studied.
So structural heterogeneity, which
has to do with topology and has been studied thoroughly
with network science.
And one example is the commercial air problem network.
So you have airports as nodes and then
you link the airports if there are flights between them
and then the size of the nodes.
It's how many flights they have to different destinations.
And of course, there are some bigger ports like LA,
San Francisco, this one might be Denver,
Atlanta, in Europe, probably London,
Paris, Amsterdam, Frankfurt, and so on.
And these topologies have been studied thoroughly.
And they are more or less scan-free.
I mean, in some cases, it's not precisely scan-free.
But in general, you have few elements
with lots of connections and most elements
with not so many connections.
And this has advantages because you
can have an increased variety.
If you have a diversity of elements,
you can have systems that have functions
with multiple scales, then you can have an increased variety.
And this is related to Ashby's law of requisite variety, which
was proposed in cybernetics, which tells
that if you want a controller, it requires at least the same
variety as that, which is trying to control.
And this applies to machines and to animals.
So imagine if you want a robot in a factory
to distinguish between five different situations,
then it needs to have a variety of those five different
situations.
Variety is simply the number of states that system can be in.
So of course, there are different mechanisms
that will produce structural heterogeneity.
And I mean, this has been studied thoroughly.
And temporal heterogeneity has also been studied.
I mean, there are plenty of examples like solar flares.
They're also known as burst dynamics.
So human communications, all of this pattern,
like texts or phone calls or internet visits or usage
of money, brain activity is also following this pattern.
Epidemics, wars, and also it seems
that maybe mass extinctions have these distributions.
Earthquakes, foraging in different animals.
And the advantages of having different temporal scales
are similar to the advantages of having different spatial
scales or structural scales in a sense
that you can have an increased variety.
And systems can adapt at all those multiple scales, which
is useful if you don't know at which scale you need to respond
and say there's a perturbation.
And finally, we also study functional heterogeneity,
which has also been studied thoroughly because it's
everywhere, but because it's everywhere,
it's not named as such.
So I mean, here's an image of the seven samurai.
And I mean, could be an Avengers film or Ocean's 11.
So imagine a film where all the characters
have the same abilities.
It would be pretty boring because everybody does the same.
So you need to have the smart guy and the strong guy
and the funny guy.
And they all have different abilities.
And then that makes the film entertaining.
And at the same time, they're able to solve whatever
challenge they're facing.
Armageddon would be another example to do.
So if you have different elements in a system that
are better for different functions,
then the variety of the system will be greater than if all
have the same function.
Of course, if you have a homogeneous system,
then your system will be more robust.
Because if, let's say, one of the elements
fails for whatever reason or is destroyed,
then the other elements can maintain
the function of the system.
So if, let's say, depending on how many elements
and how much they cost, if our system requires a high variety,
it's better to have heterogeneity.
And if the system requires a low variety,
then it's better to have homogeneity.
And in most cases, it's just a balance
between those two extremes.
But let's say, in many different disciplines,
this has been studied in terms of specialization
or division of labor.
So the study of cooperation, coordination,
and communication will have to do
with functional heterogeneity.
In a sense, if you have heterogeneity,
that may imply that you have less competition.
So it can favor cooperation.
And for example, in sport teams, you usually
have heterogeneity.
So in soccer, you can have the best goal scorer.
But if you don't have good middlefielders,
then the goal scorer will never get the ball.
So of course, you need heterogeneity
of good players in different positions
in order to have a good team, and of course also
that they are coordinated properly.
And if you have a sports team, let's stay with soccer,
where all the players have the same ability,
then very probably it would be a very lousy sports team.
Because let's say, of course, you could make changes
and you could maintain the same level of the team.
But then, let's say, you wouldn't have a very good scorer
or a very good goalkeeper or a very good defender, and so on.
So in a sense, if you have heterogeneity,
you can do more with the same number of elements.
But if you have a repetitive task, mainly lower variety,
then you don't really need that increased variety,
so then homogeneity would be maybe cheaper.
And also, it might be easier to coordinate
homogenous elements and heterogeneous elements.
So yeah, it's a balance between redundancy,
where you have all the elements being the same,
and variety where each element is different.
So if you imagine a fight between two armies,
one where all the soldiers are exactly the same,
and another army where all the soldiers
are completely different, and there's
no overlap in their functionality, who would win.
And I mean, I think either case, it's not desirable.
So say some balance between extreme heterogeneity
and homogeneity would be best.
And of course, the question is what would be the best balance.
So we wanted to study what was the effect of having
or not having all these three heterogeneities,
structural, functional, and temporal.
And we used random bullet networks.
And also, we used the ISING model,
but I will not present those results,
but basically the same conclusions hold.
So let me briefly tell you about random bullet networks
in case you're not familiar with them.
They were proposed by Storck-Kathman in 1969
as models of genetic regulator networks.
So you have nodes which represent genes, which
can be on or off, so they're Boolean variables.
And then they're linked to K other genes
that basically will determine what
will be the future states of all those genes.
So I mean, back in 69, we had no idea
how many genes any species had.
There was this idea from Jacob and Monod
that there were genetic regulatory networks
from the early 60s.
We had no idea what kind of topology they had.
So what Storck did was to generate random networks.
So it's like, OK, we don't know which kind of network,
so let's just explore the space of all possible networks
and see in that space of parameters
which ones have the properties required for life.
So he basically generated ensembles
of random networks with different parameters
and then studied their statistical properties.
So a network has n Boolean nodes with K inputs
in the classic version because there are many variations.
And then we have lookup tables that determine
what will be the future state of each node.
So each node has its own lookup table.
And we generate a random topology if we don't have data
because, of course, more recently now we
have data about field genetic regulatory networks.
And then we can build realistic models with the same principles.
But let's say, before and still now,
we can just generate the connections randomly
and also the lookup tables.
So this will be an example of a lookup table.
So if we just focus on node O, it has inputs, nodes N and P.
So depending on the states of N and P,
the state of O will change in the next time step.
So this lookup table considers all possible combinations
between N and P, time T, and then time T plus 1.
I mean, this will be what the next node function.
But basically, you generate this randomly with a bias P.
And that P would be the probability of having
once in this column of the lookup table.
If P is 0, then you have only 0s.
And then it's a very boring function.
If P is 1, it's only 1.
So it's also boring.
So if P equals 0.5, we say that there is no bias.
So you have the same probability of 0s and 1s in a lookup table.
So that gives you the most complex dynamics.
So here it's just the dynamics of a randomly generated network.
I don't know, maybe with K equals 2.
And you start with random initial conditions.
And then after some transitions, you
find there's a repeating pattern.
Each column would be a different time step
or a different state of the network.
And each row here would be a different node
as it evolves in time.
And one color would be on and another color would be off.
So the topology and the lookup tables, let's say the structure
and the function define the state space.
So we have n nodes, but then the state space
is 2 to the n nodes or each node of n bits.
So now these letters represent states of the whole network.
And once you repeat a state, you say you fall in an attractor
because these are deterministic systems.
But this flavor of randomly generated is deterministic.
And they're also dissipative.
So once you repeat a state, you know you are in an attractor.
And then all the states leading to that attractor
are called the attractor basins.
And so you have some states which
are called guardian of hidden states
because you can only reach them from initial conditions.
And then states can have multiple predecessors,
but they have only one successor.
So the question of the relationship
between the topological network and the state networks,
it's similar to what's the relationship between structure
and function.
So it's what's the relationship between making changes here
or here, which changes will make in the state space.
That's like an open question.
But with these type of models, you
can explore in a systematic way.
And there are three different dynamical regimes.
So for p equals 0.5, when there's no bias in the lookup tables,
you will have a phase transition between order
and chaos at k equals 2.
If you have few connections, you
have order dynamics that are very robust.
Most nodes don't change.
And you have convergence to similar states.
So if you make a mutation somewhere here,
very probably the network will return to its attractor.
When you have lots of connections between nodes,
you have chaotic dynamics.
So the randomness, most nodes are changing.
You have fragile dynamics.
You have divergence of similar states,
meaning that there is a small perturbation.
And then you will continue the trajectory of that state.
Very quickly, you would diverge me
that the states would be very different.
And then close to the phase transition,
we have a third regime, which is critical,
where there's a balance between order and chaos.
So you have some static nodes, some changing nodes,
some changes spread locally.
So let's say it's rare that you will
have branches that spread all over through the network.
And in this region of the parameters,
you have maximization of information storage,
of coherent information transfer,
of fissure information, and many other nice properties.
So in the late 80s, early 90s, actually, right now,
I'm in Santa Fe Institute.
So precisely here, people like Chris Langton
and Stuart Kaufman argued that life and computation
had to exist close to the so-called age of chaos, which
was how they call back then, criticality.
Now, people usually don't use that name
in the sense that if you have order dynamics,
then you have robustness, but then you can't adapt.
And if you have chaos, then, in principle,
you can have adaptation.
But then, probably, if you already had some good solutions,
you would lose them because there's too much change.
So somewhere in between, there's, let's say,
this Goldilocks region where you have enough robustness
and enough adaptability so that life can emerge.
And much later, I mean, less than 15 years ago,
when there was already enough data from real genetic regulatory
networks, then they analyzed them.
And it turns out that they are either critical or close
to critical, but, let's say, on the other side.
And I'll mention why they are on the other side.
So let's say, when you have networks
that have these properties, then you
can have a balance between robustness and information
storage and variability and computation and exploration,
which basically is necessary for evolution.
And then there have been other works
that explore how could evolution reach these type of networks.
And there are many different studies.
But the thing is that people were trying to, well,
makes sense that we want critical dynamics for these reasons
that you have this balance between robustness and adaptability.
But then the parameter region where you have critical dynamics
is very narrow.
So the question is, OK, from all possible networks,
very few are actually critical.
How could evolution find those precise critical networks,
the precise parameters that give you that criticality?
So in these models, we introduce heterogeneity.
So for structural heterogeneity, we
compare exponential distributions
versus Poisson distributions.
I mean, Poisson distributions are not homogeneous,
but let's say they are more homogeneous than exponential.
For temporal heterogeneity, back in my masters,
I proposed thermistic asynchronous updating.
So basically, different nodes update at different speeds.
So basically, we see how many outputs a node has.
And then that number would be its period.
So if I have one or zero outputs,
I will update every time step.
And if I have 10 outputs, I will update only every 10
time steps.
And so the more connected, well, the nodes
with the greatest inference will be the slowest,
which is what we observed in non-rand dynamics work,
that the most influential or relevant elements
change slower.
And then for functional heterogeneity,
we play with this parameter p, which
is the probability of having ones in the lookup table.
So instead of having all nodes, let's say,
to generate the lookup tables of all nodes
with p equals 0.5, we have, I believe,
it's a possible distribution with mean 0.5.
So some nodes actually will have a bit slower than 0.5,
and some will have a bit higher than 0.5,
or maybe it's a Gaussian distribution, I forget.
So in these results, we plot the complexity.
I will speak how we get this measure of complexity,
but it's basically a proxy for criticality
for different combinations of homogeneity and heterogeneity
of networks of size 100.
So let's say the classic all homogenous networks
is the green curve.
So the phase transition is precisely
where the complexity is maximal, which
would be critical dynamics.
And let's say to the left it's order region,
to the right it's chaotic region.
So we see that as we add heterogeneities,
so for example, this one is structural heterogeneity,
then the curve shifts to the right,
but the area where you have high complexity is much higher.
And then if we add two types of heterogeneity,
so let's say that this would be only functional heterogeneity,
this one only temporal heterogeneity, which
shifts to the right, but that's the only case
where the area is not greater.
But then if you add two types of heterogeneity,
then shifts even more to the right,
and then the area extends even more.
And if you have all three heterogeneities, the black curve,
and basically you have, for a majority of networks,
you have critical dynamics.
So what we say is that the heterogeneity
extends criticality because now instead of having
a very narrow area of the parameter space
where you have critical dynamics, now most of the parameter space
has critical dynamics.
This is just an example of k equals 10
to see how these type of dynamics look like with high complexity.
And it extends not only criticality,
but also antifragility.
So let me quickly explain what's antifragility.
So this was proposed by Nassim Taleb just 10 years ago
in his book, Antifragile.
And it's basically the property of systems
that benefit from noise or perturbations.
So imagine that you have some fragile thing,
like some crystal cups that you want to send by post.
Then you put a label that says, do not mishandle.
This is fragile.
Why?
Because if you shake it, it will break.
So fragile system is one in which perturbations
destroy the function of the system
or degrade the function of the system.
So you don't want that.
And then you make everything possible to, let's say,
you build buffers or you try to protect the system
from perturbations because it's fragile.
If you have a robot system, let's say,
if you have rocks or logs and just send them my posts,
then you don't put anything because you don't
care if they are perturbed or mishandled.
So a robot system is one in which the function does not
change in the face of perturbations.
But then if you have some antifragile system,
then you send it by post and you put that label says,
please shake because when it reaches its destination,
it will be in better condition than if you don't shake it.
So that's an example of a system that benefits
from perturbations.
And initially, it might seem counterintuitive,
but I'm from Mexico.
So we have in Mexico aguas frescas,
which is basically water with fruit pulp and sugar,
and especially the tamarind water.
It's very quickly settles in the bottom.
So in order for it to taste good, you need to shake it.
You need to stir it so that it kind of mixes a bit.
And then you drink it and then it's tasty.
If you don't shake it, it's firstly tastes only the water
and then at the end you get all the pulp and then it's not nice.
So aguas frescas are antifragile.
But I mean, the immune system is antifragile.
Sport, it's another example of antifragility
in sense that our bodies increase our oxygen consumption,
which in theory should accelerate aging of our cells
because there's more oxygen.
So then they should oxidize faster,
but then our body generates antioxidants,
which overcompensate the increased oxygen.
So the total effect is rejuvenating.
But of course, if you run a marathon a day,
probably you age faster than if you don't do anything.
And more generally, hormesis will be an example of antifragility.
But of course, there's always a point
where too much noise starts degrading the system.
So we propose a measure of antifragility,
which is actually this is a measure of fragility.
And we basically measure the perturbation,
which would be delta x, or let's say the effect
of the perturbation in the system,
how much the system changes.
And delta sigma would be changing satisfaction.
And the satisfaction, it's very arbitrary.
So whatever we want the system to do,
that's if it's functioning well, that's high satisfaction.
If it's not doing what we want the system to do,
then it's low satisfaction.
So if the perturbation improves the function of the system,
then it's antifragile.
That's basically what this measure represents.
Aleph proposed a different measure of antifragility,
but nobody I know understands it.
So we just propose the rule.
And we have to use this in many different contexts,
in ecosystems, in cryptocurrencies, in stock market,
and here with random bulletin.
So we can just focus on the left side of this figure.
So as we increase the perturbations,
we see for networks of different connectivity
how their fragility changes.
So this would be all homogeneous, topology,
temporality, and function.
This will be all heterogeneous.
And instead of just presenting all eight cases,
I'm just showing you the extreme.
So it would be the green curve and the black curve.
So for homogeneous networks, let's say
you have antifragility for k equals 1
and for k equals 2 just a bit.
But then for k equals 3, it's already fragile.
So basically, the x means how many nodes will
be perturbed every time step.
And then the satisfaction is basically the complexity
because we want the networks to have complex dynamics.
And then, of course, if you have a high k,
you already start, I mean, you don't need perturbations
to have complex dynamics.
You just, let's say, increase your change or your chaos
and your fragility.
So if you have heterogeneity, you
have antifragility for networks of k all the way to 4,
which, let's say, in this case, are already chaotic.
But here, they are slightly antifragile.
And then with k equals 1, they will be antifragile still
for x equals 100.
So you can be perturbing all nodes constantly.
And it still will be slightly antifragile
in the sense that it will be close to with this amount
of noise.
Basically, if these curves are negative,
let's say below this 0 dot of the line,
it means that the perturbations will take it closer
to criticality or, let's say, to the phase transition.
However, if we want maximum antifragility,
then the homogeneous case will be best
because, let's say, this is the k equals 1
with all homogeneous.
That gives you maximum antifragility.
Let's say this will be broader, but this
will be deeper.
So it's like, also, it depends what kind of antifragility
you want.
And then, I mean, when we were working on this,
we were thinking, OK, let's come up maybe
with some other examples we did there.
I think, well, we got very similar results.
But instead of just, say, doing, I don't know, heterogeneous
prisoner-less dilemma, heterogeneous game of life,
I don't know, whatever model you would like,
heterogeneous neural networks, we just thought, OK,
let's not measure the complexity of all these different systems.
Let's just measure the distribution
over this function of complexity.
So this measure was about 10 years ago
based on channel information.
So basically, channel information
correlates with change.
So the more change you have, the higher this information is,
and then you just normalize it to interval 0, 1.
So this is, well, let's say, if you have a random string,
then that will have maximum information.
If you have a regular string, let's say only 0s or only 1s,
that will have minimum information.
So the probability of having 1s in a string,
it's here on the x-axis.
So this would be channel information, maximum 1.5,
which is the same probability of having 0s or 1s.
If you generalize it, channel information
is basically a measure of the homogeneity
of a probability distribution.
If it's completely homogenous, if you
have the same probability of having any symbol in your distribution,
then you have maximum information.
If you have probability of one or having only one symbol,
then you have zero information.
And then let's say, we're interested
about measuring emergence and self-organization.
This is a longer story, and my need for a little justification.
But basically, we define complexity
as a balance between emergence and self-organization.
And self-organization, it's like 1 minus entropy,
which is information, because organization is precisely
the opposite of what I mentioned.
If you're interested, I can explain why later.
But if not, then you can just believe me.
And then complexity is just the balance between these two,
which if you know the logistic function,
you might ask, OK, why does this have
the shape of the logistic map with a parameter that
would give you maximum chaos?
I have no idea.
But it seems to have to do with some operational principle.
But yeah, it's the same equation.
So let's say this red function would
be a function of complexity.
So instead of measuring what complexity values we will get
with some model, then it's heterogeneous version.
We just thought, OK, what would be
the complexity of different distributions?
So it's like, OK, we'll just generate different distributions,
and then we'll just measure the complexity.
So let's say the blue dots would be homogeneous cases,
where basically all the elements have the same probability
of having one cylinder of strings.
So it follows the same complexity curve.
And then this will be heterogeneous distribution.
So basically, we generate distributions
with a certain mean.
And this is, let's say, where we draw the angles.
And then we see that it has intermediate.
So basically, when the homogeneous case is lower than half,
then heterogeneous is better and vice versa.
Why?
Because let's say if you have here, let me go back here.
Maybe it's clear.
So if we focus just on the red curve,
if we have a distribution of points somewhere here,
then let's say we will get some higher points of complexity
somewhere here.
But if all the elements have the same value,
then we will have very low complexity.
So let's say if in the distribution,
the average is greater than the mean,
then you'll be able to have a greater complexity.
But then we generalize because, let's say,
maybe not everybody likes this measure of complexity.
So we'll just look at any function
when heterogeneity gives you a higher value than homogeneity.
And we can get this with Jensen's inequality.
So if we just have a function, whatever function,
that we are trying to maximize or minimize,
then we just flip what I'm about to say.
And the homogeneous case will be when you have a value
and all the elements have that value.
And then the function of that value
will be the function, let's say, of the zeta
because all the elements have the same value.
So for the homogeneous case, the function of the mean,
let's say you get the function of the mean
because all the elements have the same value.
But then if you have a heterogeneous distribution,
then each of the elements will have a different function.
And then you have a distribution
of the function of the elements.
So if the mean of the functions
is greater than the function of the means,
then heterogeneity is better than homogeneity.
And we know from Jensen's inequality
that this is the case for any convex function.
And if it's convex, then homogeneity
will give you greater values than heterogeneity.
And if it's a linear function,
then we don't care because they give you the same value.
So that's like a very general way of deciding
probably when heterogeneity is better than homogeneity.
And we have applied this already to a few cases,
particularly in search, to genetic algorithms,
to the problem sales problem.
Here I just showed the n-quins problem,
which is this chess problem where you have n-quins,
so how many we have here, eight.
Quints in a eight by eight board.
And then you need to find the position
so that no queen is attacking any other queen.
But of course, you can increase the size of the boards,
which equals the number of queens.
So let's say this for 500 queens,
in a 500 by 500 boards,
and let's say with temporal heterogeneity,
you get faster and better solutions about 10%.
But then the question that we're interested is,
okay, how could we find optimal heterogeneity?
Because, okay, if we see that heterogeneity
in many cases is beneficial,
but then we could have more or less heterogeneity,
what would be the optimal heterogeneity?
And because of the non-flux theorem,
very probably this depends on the problem.
Well, actually we already have some results
that show that there is no single optimal heterogeneity.
It does depend on the problem,
but we want to analyze systematically
some general scenarios
because probably we will be able to say something about,
let's say what type of heterogeneity
will be useful to start with for different situations.
And of course, in the examples I have shown,
I don't consider cost,
because for example, for random muller networks,
we get greater complexity for more heterogeneity,
but this also implies more connectivity.
And then if connectivity is expensive,
then you might not want such greater complexity
or criticality because you want to save
on the number of connections.
I mean, that's just another factor
that right now we did not consider.
And another open question is
how heterogeneity has played a role in evolution.
How heterogeneity might benefit or simplify
or facilitate evolution with which mechanisms,
where has it occurred, what are the effects and so on?
I mean, this would be, it's a greater enterprise
and it's more, let's say like an invitation
or an open question,
but also in the projects that the lab is working with,
I can imagine that might be interesting
to study different types of heterogeneity,
like for example, in neural dynamics,
with some students we are starting to explore
different types of heterogeneity
in realistic models of neural networks
and also from the group of Wolfsinger in Germany,
they found out that if they introduced heterogeneity
in their artificial neural networks,
then they would classify much better.
And it seemed that this general understanding
that heterogeneity can increase the variety of a system
that's linked to Ashley's low-requisite variety,
it seems to me like a general explanation
for why this is the case,
but then I think we're still missing
like a systematic understanding
of what will be the precise heterogeneity
that should be used in different situations.
So just to conclude,
most models of complex systems
that have been studied are homogeneous
and we have seen that in many cases,
heterogeneity gives criticality for free.
So if we want criticality,
it makes sense to take advantage of heterogeneity
because then we don't need to search too much
in the parameter space to find that type of dynamics.
And also it would be interesting to explore
heterogeneous versions of different models.
The question of understanding better optimal heterogeneity
is open and once we understand that,
then we could develop algorithms
that would adapt the heterogeneity of systems
to their current situation so that they perform better.
Yeah, so if you have any questions,
I just invite you to subscribe to Complexly Digest,
which we edit already for some years
and then you can find the latest news
and papers about complex systems.
Thank you.
