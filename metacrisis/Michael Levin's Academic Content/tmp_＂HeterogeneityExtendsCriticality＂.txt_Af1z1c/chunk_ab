So say some balance between extreme heterogeneity
and homogeneity would be best.
And of course, the question is what would be the best balance.
So we wanted to study what was the effect of having
or not having all these three heterogeneities,
structural, functional, and temporal.
And we used random bullet networks.
And also, we used the ISING model,
but I will not present those results,
but basically the same conclusions hold.
So let me briefly tell you about random bullet networks
in case you're not familiar with them.
They were proposed by Storck-Kathman in 1969
as models of genetic regulator networks.
So you have nodes which represent genes, which
can be on or off, so they're Boolean variables.
And then they're linked to K other genes
that basically will determine what
will be the future states of all those genes.
So I mean, back in 69, we had no idea
how many genes any species had.
There was this idea from Jacob and Monod
that there were genetic regulatory networks
from the early 60s.
We had no idea what kind of topology they had.
So what Storck did was to generate random networks.
So it's like, OK, we don't know which kind of network,
so let's just explore the space of all possible networks
and see in that space of parameters
which ones have the properties required for life.
So he basically generated ensembles
of random networks with different parameters
and then studied their statistical properties.
So a network has n Boolean nodes with K inputs
in the classic version because there are many variations.
And then we have lookup tables that determine
what will be the future state of each node.
So each node has its own lookup table.
And we generate a random topology if we don't have data
because, of course, more recently now we
have data about field genetic regulatory networks.
And then we can build realistic models with the same principles.
But let's say, before and still now,
we can just generate the connections randomly
and also the lookup tables.
So this will be an example of a lookup table.
So if we just focus on node O, it has inputs, nodes N and P.
So depending on the states of N and P,
the state of O will change in the next time step.
So this lookup table considers all possible combinations
between N and P, time T, and then time T plus 1.
I mean, this will be what the next node function.
But basically, you generate this randomly with a bias P.
And that P would be the probability of having
once in this column of the lookup table.
If P is 0, then you have only 0s.
And then it's a very boring function.
If P is 1, it's only 1.
So it's also boring.
So if P equals 0.5, we say that there is no bias.
So you have the same probability of 0s and 1s in a lookup table.
So that gives you the most complex dynamics.
So here it's just the dynamics of a randomly generated network.
I don't know, maybe with K equals 2.
And you start with random initial conditions.
And then after some transitions, you
find there's a repeating pattern.
Each column would be a different time step
or a different state of the network.
And each row here would be a different node
as it evolves in time.
And one color would be on and another color would be off.
So the topology and the lookup tables, let's say the structure
and the function define the state space.
So we have n nodes, but then the state space
is 2 to the n nodes or each node of n bits.
So now these letters represent states of the whole network.
And once you repeat a state, you say you fall in an attractor
because these are deterministic systems.
But this flavor of randomly generated is deterministic.
And they're also dissipative.
So once you repeat a state, you know you are in an attractor.
And then all the states leading to that attractor
are called the attractor basins.
And so you have some states which
are called guardian of hidden states
because you can only reach them from initial conditions.
And then states can have multiple predecessors,
but they have only one successor.
So the question of the relationship
between the topological network and the state networks,
it's similar to what's the relationship between structure
and function.
So it's what's the relationship between making changes here
or here, which changes will make in the state space.
That's like an open question.
But with these type of models, you
can explore in a systematic way.
And there are three different dynamical regimes.
So for p equals 0.5, when there's no bias in the lookup tables,
you will have a phase transition between order
and chaos at k equals 2.
If you have few connections, you
have order dynamics that are very robust.
Most nodes don't change.
And you have convergence to similar states.
So if you make a mutation somewhere here,
very probably the network will return to its attractor.
When you have lots of connections between nodes,
you have chaotic dynamics.
So the randomness, most nodes are changing.
You have fragile dynamics.
You have divergence of similar states,
meaning that there is a small perturbation.
And then you will continue the trajectory of that state.
Very quickly, you would diverge me
that the states would be very different.
And then close to the phase transition,
we have a third regime, which is critical,
where there's a balance between order and chaos.
So you have some static nodes, some changing nodes,
some changes spread locally.
So let's say it's rare that you will
have branches that spread all over through the network.
And in this region of the parameters,
you have maximization of information storage,
of coherent information transfer,
of fissure information, and many other nice properties.
So in the late 80s, early 90s, actually, right now,
I'm in Santa Fe Institute.
So precisely here, people like Chris Langton
and Stuart Kaufman argued that life and computation
had to exist close to the so-called age of chaos, which
was how they call back then, criticality.
Now, people usually don't use that name
in the sense that if you have order dynamics,
then you have robustness, but then you can't adapt.
And if you have chaos, then, in principle,
you can have adaptation.
But then, probably, if you already had some good solutions,
you would lose them because there's too much change.
So somewhere in between, there's, let's say,
this Goldilocks region where you have enough robustness
and enough adaptability so that life can emerge.
And much later, I mean, less than 15 years ago,
when there was already enough data from real genetic regulatory
networks, then they analyzed them.
And it turns out that they are either critical or close
to critical, but, let's say, on the other side.
And I'll mention why they are on the other side.
So let's say, when you have networks
that have these properties, then you
can have a balance between robustness and information
storage and variability and computation and exploration,
which basically is necessary for evolution.
And then there have been other works
that explore how could evolution reach these type of networks.
And there are many different studies.
But the thing is that people were trying to, well,
makes sense that we want critical dynamics for these reasons
that you have this balance between robustness and adaptability.
But then the parameter region where you have critical dynamics
is very narrow.
So the question is, OK, from all possible networks,
very few are actually critical.
How could evolution find those precise critical networks,
the precise parameters that give you that criticality?
So in these models, we introduce heterogeneity.
So for structural heterogeneity, we
compare exponential distributions
versus Poisson distributions.
I mean, Poisson distributions are not homogeneous,
but let's say they are more homogeneous than exponential.
For temporal heterogeneity, back in my masters,
I proposed thermistic asynchronous updating.
So basically, different nodes update at different speeds.
So basically, we see how many outputs a node has.
And then that number would be its period.
So if I have one or zero outputs,
I will update every time step.
And if I have 10 outputs, I will update only every 10
time steps.
And so the more connected, well, the nodes
with the greatest inference will be the slowest,
which is what we observed in non-rand dynamics work,
that the most influential or relevant elements
change slower.
And then for functional heterogeneity,
we play with this parameter p, which
is the probability of having ones in the lookup table.
So instead of having all nodes, let's say,
to generate the lookup tables of all nodes
with p equals 0.5, we have, I believe,
it's a possible distribution with mean 0.5.
So some nodes actually will have a bit slower than 0.5,
and some will have a bit higher than 0.5,
or maybe it's a Gaussian distribution, I forget.
So in these results, we plot the complexity.
I will speak how we get this measure of complexity,
but it's basically a proxy for criticality
