for different combinations of homogeneity and heterogeneity
of networks of size 100.
So let's say the classic all homogenous networks
is the green curve.
So the phase transition is precisely
where the complexity is maximal, which
would be critical dynamics.
And let's say to the left it's order region,
to the right it's chaotic region.
So we see that as we add heterogeneities,
so for example, this one is structural heterogeneity,
then the curve shifts to the right,
but the area where you have high complexity is much higher.
And then if we add two types of heterogeneity,
so let's say that this would be only functional heterogeneity,
this one only temporal heterogeneity, which
shifts to the right, but that's the only case
where the area is not greater.
But then if you add two types of heterogeneity,
then shifts even more to the right,
and then the area extends even more.
And if you have all three heterogeneities, the black curve,
and basically you have, for a majority of networks,
you have critical dynamics.
So what we say is that the heterogeneity
extends criticality because now instead of having
a very narrow area of the parameter space
where you have critical dynamics, now most of the parameter space
has critical dynamics.
This is just an example of k equals 10
to see how these type of dynamics look like with high complexity.
And it extends not only criticality,
but also antifragility.
So let me quickly explain what's antifragility.
So this was proposed by Nassim Taleb just 10 years ago
in his book, Antifragile.
And it's basically the property of systems
that benefit from noise or perturbations.
So imagine that you have some fragile thing,
like some crystal cups that you want to send by post.
Then you put a label that says, do not mishandle.
This is fragile.
Why?
Because if you shake it, it will break.
So fragile system is one in which perturbations
destroy the function of the system
or degrade the function of the system.
So you don't want that.
And then you make everything possible to, let's say,
you build buffers or you try to protect the system
from perturbations because it's fragile.
If you have a robot system, let's say,
if you have rocks or logs and just send them my posts,
then you don't put anything because you don't
care if they are perturbed or mishandled.
So a robot system is one in which the function does not
change in the face of perturbations.
But then if you have some antifragile system,
then you send it by post and you put that label says,
please shake because when it reaches its destination,
it will be in better condition than if you don't shake it.
So that's an example of a system that benefits
from perturbations.
And initially, it might seem counterintuitive,
but I'm from Mexico.
So we have in Mexico aguas frescas,
which is basically water with fruit pulp and sugar,
and especially the tamarind water.
It's very quickly settles in the bottom.
So in order for it to taste good, you need to shake it.
You need to stir it so that it kind of mixes a bit.
And then you drink it and then it's tasty.
If you don't shake it, it's firstly tastes only the water
and then at the end you get all the pulp and then it's not nice.
So aguas frescas are antifragile.
But I mean, the immune system is antifragile.
Sport, it's another example of antifragility
in sense that our bodies increase our oxygen consumption,
which in theory should accelerate aging of our cells
because there's more oxygen.
So then they should oxidize faster,
but then our body generates antioxidants,
which overcompensate the increased oxygen.
So the total effect is rejuvenating.
But of course, if you run a marathon a day,
probably you age faster than if you don't do anything.
And more generally, hormesis will be an example of antifragility.
But of course, there's always a point
where too much noise starts degrading the system.
So we propose a measure of antifragility,
which is actually this is a measure of fragility.
And we basically measure the perturbation,
which would be delta x, or let's say the effect
of the perturbation in the system,
how much the system changes.
And delta sigma would be changing satisfaction.
And the satisfaction, it's very arbitrary.
So whatever we want the system to do,
that's if it's functioning well, that's high satisfaction.
If it's not doing what we want the system to do,
then it's low satisfaction.
So if the perturbation improves the function of the system,
then it's antifragile.
That's basically what this measure represents.
Aleph proposed a different measure of antifragility,
but nobody I know understands it.
So we just propose the rule.
And we have to use this in many different contexts,
in ecosystems, in cryptocurrencies, in stock market,
and here with random bulletin.
So we can just focus on the left side of this figure.
So as we increase the perturbations,
we see for networks of different connectivity
how their fragility changes.
So this would be all homogeneous, topology,
temporality, and function.
This will be all heterogeneous.
And instead of just presenting all eight cases,
I'm just showing you the extreme.
So it would be the green curve and the black curve.
So for homogeneous networks, let's say
you have antifragility for k equals 1
and for k equals 2 just a bit.
But then for k equals 3, it's already fragile.
So basically, the x means how many nodes will
be perturbed every time step.
And then the satisfaction is basically the complexity
because we want the networks to have complex dynamics.
And then, of course, if you have a high k,
you already start, I mean, you don't need perturbations
to have complex dynamics.
You just, let's say, increase your change or your chaos
and your fragility.
So if you have heterogeneity, you
have antifragility for networks of k all the way to 4,
which, let's say, in this case, are already chaotic.
But here, they are slightly antifragile.
And then with k equals 1, they will be antifragile still
for x equals 100.
So you can be perturbing all nodes constantly.
And it still will be slightly antifragile
in the sense that it will be close to with this amount
of noise.
Basically, if these curves are negative,
let's say below this 0 dot of the line,
it means that the perturbations will take it closer
to criticality or, let's say, to the phase transition.
However, if we want maximum antifragility,
then the homogeneous case will be best
because, let's say, this is the k equals 1
with all homogeneous.
That gives you maximum antifragility.
Let's say this will be broader, but this
will be deeper.
So it's like, also, it depends what kind of antifragility
you want.
And then, I mean, when we were working on this,
we were thinking, OK, let's come up maybe
with some other examples we did there.
I think, well, we got very similar results.
But instead of just, say, doing, I don't know, heterogeneous
prisoner-less dilemma, heterogeneous game of life,
I don't know, whatever model you would like,
heterogeneous neural networks, we just thought, OK,
let's not measure the complexity of all these different systems.
Let's just measure the distribution
over this function of complexity.
So this measure was about 10 years ago
based on channel information.
So basically, channel information
correlates with change.
So the more change you have, the higher this information is,
and then you just normalize it to interval 0, 1.
So this is, well, let's say, if you have a random string,
then that will have maximum information.
If you have a regular string, let's say only 0s or only 1s,
that will have minimum information.
So the probability of having 1s in a string,
it's here on the x-axis.
So this would be channel information, maximum 1.5,
which is the same probability of having 0s or 1s.
If you generalize it, channel information
is basically a measure of the homogeneity
of a probability distribution.
If it's completely homogenous, if you
have the same probability of having any symbol in your distribution,
then you have maximum information.
If you have probability of one or having only one symbol,
then you have zero information.
And then let's say, we're interested
about measuring emergence and self-organization.
This is a longer story, and my need for a little justification.
But basically, we define complexity
as a balance between emergence and self-organization.
And self-organization, it's like 1 minus entropy,
which is information, because organization is precisely
the opposite of what I mentioned.
If you're interested, I can explain why later.
But if not, then you can just believe me.
And then complexity is just the balance between these two,
