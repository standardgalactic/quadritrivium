That's just faith.
And, and it's really hard to get past that to even see that there's a question there.
Yeah.
I mean, you know, there's another aspect to this, which is looking sort of looking backwards to say,
does this framework explain what we've already seen?
That's one and people do that all the time.
But I'm actually even more interested because that one, that one is hard to do for exactly
the reason that Richard just said, you know, people can tell all kinds of stories and well,
maybe that's plausible.
I don't know.
But to me, the real asset test is looking forwards and say, okay,
how good is your lens on these things at generating new research, new predictions,
new capabilities?
I mean, there's all kinds of things that I think the standard models,
including the kind of the standard Neo Darwinian model is just not going to get us to.
It's just a barrier to research in certain areas.
It's it does not facilitate the discovery of certain types of things that other frameworks
might facilitate better.
And so I think we have to, we have to kind of compliment this idea of explaining past data
which two models may be hard to distinguish from that alone.
But then the next step is, OK, fine.
But how good is it at generating next step, you know, the next advance?
How good is it at opening new, you know, new, you don't just mean testable predictions?
No, I don't mean just testable predictions, although that's part of it.
I mean, we'll just just kind of a, you know, kind of a dumb example in the case of,
you know, the game of life, right?
The cellular automaton, you know, you got you got these very simple rules
and the little cells turn on and off.
OK, so so you could say, OK, I have a very reductionist view of this,
and I don't believe that anything exists.
For example, these gliders, these patterns that move around.
I don't believe any of that exists.
What I believe in are the little elements and they each have an on and off state.
That's it.
That's what I believe in.
So the so the thing there is it's not that that framework is not usable
to explain everything that's ever happened in the game of life,
because you can explain everything that's ever happened that way.
But the thing you're not going to do with that view is to do what people have done,
which is to make a Turing machine out of gliders in the game of life,
because you don't believe in gliders.
And if your frame doesn't help you think about, oh, wait, there are these these these higher
level permanent entities that propagate from here to there and maybe they carry information.
And oh, I can engineer this thing.
If they sort of cross, I can make a logic.
You're not going to think of any of that, right?
If if if all if your entire view.
And so so that's what that's what I mean, right?
It's not just about predictions.
It's about how good is it to get you to invent the next?
And I don't mean technologically invent, but you know, to see the possibilities of other things,
even if your framework is really good at just sort of saying, well, I could have, you know,
like it's a micro reductionist that I could have any story you tell me.
Well, it's it's consistent with with my worldview.
Yeah, it's consistent.
But how fruitful is your worldview for new stuff, right?
That's, you know, that's kind of that's what that's that's what I mean.
Yes, it's slippery, isn't it?
Because in my experience, even when you get even when you get an audience to put their hands up
at the beginning of the talk and say, what do you think will happen next?
Yeah, right.
And then you show them something that they didn't expect.
And then you say, so what do you make of that?
And they did say, well, you didn't ask the question right.
Or, you know, something like that is like, what in what sense did I not ask the question right?
You were completely wrong about what you thought would happen here.
But they just like, they just they'll just make up a reason for their answer,
having been reasonable, given the way that I posed the question or something.
It's like, no.
But now that you know, now that you've told us how it works, I mean, of course,
that would happen.
It's like, yeah, I know, right after I've explained it.
That's exactly right.
That's exactly right.
Yeah, yeah, that can always be done.
You know, this looking backwards, almost anything can be can be shoehorned into your story.
But the question is, yeah, but why didn't you do that experiment?
Right.
There's a reason why somebody else did it and you didn't do it.
It's because there are different frames and gender, different types of research programs.
I'm curious what you guys think about.
We were just talking about the link between evolution and these these language models and
all that.
So my dad asked me a really interesting question the other way the other day.
And I wonder what what you guys think, is there a plausible evolutionary path?
You know, could you imagine a possible earth somewhere a possible world
where the thing to evolve would have been these kinds of an entity that's like these
kind of language models.
So not the thing that's like us that's multi-scaled and it had to go through,
you know, particular self-construction and autopoiesis and all this.
But something literally, right, could we imagine a world where the thing that naturally evolved
is something that runs on exactly these kinds of these kinds of principles that that these
current models do, right?
Is there a path backwards that's natural or does it require us to have evolved first?
And then we engineers sort of engineer this crazy thing that's very different from us.
Is there a natural path to it?
Like I wonder, and another way of saying it is, do you think that the major features of
our cognitive system, right?
So the fact that we are the fact that we feel a unified that we have an innate sense of
action or free will and all this kind of stuff, is that is that inevitable?
Right, meaning any cognitive agent is going to be like that ever?
Or is it possible that we could have evolved in a completely different way?
What do you think about that?
So could we have bypassed all that biology and just gone straight to the
apparently cognitive artifacts?
Yeah, in other words, is there a possible world where we show up on a planet, we look
at it and say, whoa, there are no biological creatures, including engineers that look like us.
It went right to what evolved looks very much like our LLMs.
Like that's what it is.
It's not like there was never a step that's engineers like us.
No.
That's interesting, right?
Because if you think the answer is no, then the claim is that that kind of architecture
that we have is in some way essential, that every living thing is going, every sentient
thing we find is going to be like us, right?
Is that the claim?
Interesting.
So I'm inclined to think that AI systems, as we find them now, couldn't have occurred naturally
because they are, in a sense, a mirror or an imitation of the cognition that we do
that was created by our cognition.
And that's not to say that we couldn't create an artificial one that was like us,
but I don't think the current ones are.
And I think that they are, as we've discussed before, not connected through enough causal
levels to the stuff that they're made out of, right there.
There's, you know, you can have a sort of a thin superficial intelligence that looks
intelligence on the surface.
And when you scratch it a little bit, it's like it immediately shows its naivety.
Or you could have one that was a bit deeper, which, you know, you scratch it a bit,
you can dig a bit deeper before it starts to show its naivety.
But it's still the case that there's a yawning chasm beneath there that it isn't connected to
a substrate that is actually meaningful to it like we are.
Yeah, I have a stronger opinion about that than I realize.
Thanks for asking.
Interesting.
I mean, right, so we can agree.
So for example, we could say that, yes, that's true.
And they fundamentally are lacking, you know, that juice of meaning.
But might there not be a planet somewhere where these minimally meaning agents are running around,
right? There couldn't be some, I mean, we can we can accept that they are, for example,
that they don't have it.
But yet, you know, did they didn't really need to go through us?
I don't know.
I've been I've been thinking about I've been trying to invent some kind of an artificial
selection scenario with with bacteria or something that would be to get them to carry out the,
you know, I don't know, back propagation and whatever, whatever, whatever it is that I don't
know, you know, everything about these things nowadays done.
But it seems like there could be worlds like that, right?
Where you go straight to that.
But I guess, yeah, anyway.
Yeah.
Interesting.
I mean, another yet another way of saying it is, if you do find a world like that,
is your immediate conclusion that, oh, there must have been engineers here and they left or
something.
If you don't see them now, they must have been here at one point.
This this cannot show up on its own.
I think that's a good way to put it, right?
So a natural intelligence would have to be connected all the way down.
Right.
It would have to be it would have to be connected on the cognition of the subatomic
particles on which it was built.
Wow.
So so so that that makes me think of the old Paley's watch argument, right?
Remember this?
So now so now I don't know any rights.
So so so you find this thing and it and it talks to you.
And on the one hand, you would say, well, that's even worse than the watch.
That definitely means there was an engineer here somewhere.
But on the other hand, I'm actually not convinced of that at all that there isn't
the path to something like this that, you know, even if it is shallow, even even if it's a.
Yeah.
So OK, I'm I'm I'm taking your guys class here.
So I got a question.
Why why wouldn't the layered architecture metaphor just.
Absorb all of this.
I mean, there's no way that people who push the layer to John.
Do you know John Doyle's work?
I mean, the way he talks about it, the layers have their own logic and physics to them.
And they they have a protocol between the layer below it and so forth.
But they they have no knowledge of what's going on, nor do they want it.
So if you have that view, I don't see why this is.
I mean, that's just the way systems are built.
Yeah, I can see how, you know, like how many how many layers do I want before?
I think it's a it's a real cognitive thing, right?
Exactly.
You know, if I want 50 of them, can I can I can I have them at layers 100 to 150 instead
of from 0 to 50 is like what if I've got 50 layers.
Then there's a sense in which it becomes substrate independent about what the bottom layer is.
Yeah, maybe I am thinking a little bit too bottom up.
So is it one of the arguments these guys make that the reason we kick things up to the social
layer is that we can't solve it. And we're going to have to have the social idea manage us
because our internal our mental capacities are our feelings about stuff just aren't
are helping us. And so yeah, we think we think things should be fixed.
And therefore we do social structures, etc.
That can move it along faster.
So it's interesting that you mentioned so I'm I'm cool with the idea that a layer can
have its own logic. But it's the protocols of the communication between the layers are vital,
right? Because otherwise you only have a single layer logic and it just it doesn't have any depth
to it, right? And how those yeah. So
I despite the things that I just said in the last 10 minutes, I'm much more
