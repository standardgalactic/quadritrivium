nodes as your CSUSNR, you can show that that thing is doing associative learning. But only
if that's your perspective on the system, if you have a different perspective, you'll see it doing
something completely different. And in fact, multiple observers can have two different mappings,
and they will see it, they will have two different pictures of what it's doing,
but nobody's touching the actual network. You're not rewiring it. You're not changing
synaptic weights for memory. You're not doing any of that stuff. It's literally what perspective
you're taking as an observer. So, and there's many, many examples of symbiologists. That's
another thing, right? You've got this, you've got some kind of a code encoding system, and it's
working great for one thing. Pretty soon, something will evolve, which takes advantage of that just
by interpreting that as a different kind of computation that you can make some sort of use
out of. So that's, you know, that's kind of another aspect of this.
So basically, what's going on is underlying it all has been some set of correlations.
And then you're projecting it into some other world where it could have been completely gibberish,
but at least they're still correlated. Yeah. Because they inherited the correlation.
Yeah. That's cute. Yeah. And then I'll find it momentarily. But Jeremy Gay, who does all our
graphic design, he did a, I asked him to do a version of, you know, the classic Gerd Lescher
Bach cover from Hofstadter's book. So he did, he did an amazing version of that for embryos
for us, which for that, for that book, which I'll show momentarily. But that's exactly,
that's exactly what you just said. It's, it's, it may, depending on how you look at it, it may
look like gibberish or it may not. Yeah. In a sense to put this in more computer
sciencey language, this is what interpreters and compilers are doing, but particularly interpreters.
Oh, right. You have down at the, at the machine level, or slightly above the machine level,
you've got a whole bunch of processing going on and layering a high level language on top of it
is layering an interpretation. It's assigning a different semantics to all of that processing
that's already happening. And within that, using that semantics, one can interpret what's going on
as, you know, a zoom call or something. Yeah. Whereas without that semantics,
you have a completely different idea of what's happening.
Well, and look at how powerful that is, right? If, if, if you had a, you know, if you were,
if you were running a software company or something and you had a, you had a candidate
that you were interviewing and they say to you, well, I'm a reductionist, I don't, but, you know,
I think the, you know, there's no such thing as an algorithm. It's the, it's Maxwell's equations
that govern what the electrons do. They just do what they're going to do. You'd never hire that
person because they wouldn't be able to, they wouldn't code anything. It's, it's, it's hugely
empowering to think that the algorithm makes the electrons dance, right? Because then you, then,
then you would go on and you would write things when you, when you're thinking about it at that
level. If you lack that abstraction, you know, it's, it's, it's extremely hard to, to, to do
anything. And I think I'm here, check, check this out. This is the, this is the, this is the cover.
So that, that Jeremy made, right? And so the idea is you've got that, you've got the same
batch of DNA. So this in particular is, is, is, is our Xenobot example. You have the exact same
batch of DNA. We don't change the genome, but depending on how that DNA ends up being interpreted,
you end up with a frog embryo or you end up with a Xenobot. And it's the exact same, right? It's
the exact same DNA that is somehow, you know, what is it encoding, right? Well, it depends.
If it doesn't just encode a frog, who knows what the heck else it, you know,
down here, who knows what else. So it's, it's this idea of, I don't know, info information or, or
environment or something is sort of, is the prompt that gets this thing to generate. It's like a,
it's like a generative in the, you know, encoding of some sort. And there's a prompt that gets it to
go in a particular direction and you end up with something like this, or you end up with something
like that. Almost like a hologram, right? Because as I recall, if you shine the light in different
directions, you get us a different readout from your flat piece of hologram film.
Well, yeah, right. And that's interesting too, back to the memory thing. So, so, yeah, so, so I
think the point there is that, yeah, you can, you can store multiple images sort of on the same piece
of film and just recover them with different, different signals. There's a, there's an amazing
book called Shuffle Brain. And it's this guy, Paul Peach, back in the 80s, who did all these
experiments in memory in Salamander. And he basically was, you know, he started out by looking
for memory and taking out different pieces of the brain and showing them, well, it's actually
everywhere. And, and then he would move the pieces around and the salamander was, you know, would
basically be completely fine and do all the tasks and everything. And then, and then he would move
pieces from goldfish to a salamander and the vegetarian would become a meat eater and me,
and the meat eater vegetarian. It's just amazing. But the second half as well. So the first half
of this book is all these experiments. The second half of this book is, is a holographic
model of memory, where he's sort of inspired by these experiments of non-locality, like he couldn't
find it anywhere in a particular region and so on. He pulls out all these analogies of trying to,
trying to store multiple different memories and, you know, whatever else is in there,
in literally the same hardware, right, they all have to overlap somehow.
Then I guess that gets us back again to where what's the location where all this is stored
and your question of this macro storage thing, you know, deducing the micro from the macro
roughly. And that reminded me, as I was reading, I was reading this paper for two
wrote last year. And, you know, one of the citations was to David Pine's paper from
year 2000 about mac phenomenon physics that were macro not deducible from micro.
And I don't know enough physics to know where those constraints are stored.
Do you know, Chris, how, let's see, he gave a few specific examples.
I took a quick look at it this morning.
Oh, the theory of everything paper. So he says the quantum hall effect is one,
Josephson effect is another, and you can't predict them from the micro level. So do you know
what where the constraint is coming from? I don't.
Well, in many cases, the constraint is coming from the macro, the environment, which is macroscopic.
And is providing top down boundary conditions of one kind or other on behavior that
that one can describe microscopically only by putting it into a kind of box that's specified
at this larger scale. Huh. Okay. So this is beginning to remind me of like gravity effects,
your Markov blanket. And there's something else that just get me on all this. But anyway,
I see what you mean. Yeah, you know, this issue of where is it's where is it stored has been
driving me nuts for a long time because there are many such there are many such things.
When, you know, the distribution of primes and all the way where like that stuff doesn't seem to
be, you know, it doesn't seem to depend on the physical facts of the universe, like where is
all that, or even just the, you know, the thing where you've seen this Galton board. Yeah, it's
just it's a it's a piece of wood and it's got a bunch of nails stuck into it. You take a box of
marbles and you dump it over and they go boom, boom, boom, boom, boom. And in the end, you get
this bell curve, right? Yeah, you always get the bell curve. So now you could ask where is the shape
of this bell curve stored. So you start looking at the wood and you look at the nails and then you
look at the distribution of the nails. None of those have anything with, you know, none of them
make it in code in the strict set. So where the heck does it come from? Right? There's a million
of these things. Yeah. Why are there normal distributions? Right. Or truth tables, right?
So you evolve, I like this too, you evolve an iron, a voltage gated ion channel, which is
basically a voltage gated current conductance, it's basically a transistor, you have a couple
of these things, you can make a gate, if you have a logic gate, you have a truth table.
Where's this truth table, like you gain and the fact that you know, the NAND is special and all
this kind of stuff. Where is all that it wasn't it sure wasn't in your in your, you know, ion channel
design, it's it's you get that for free somehow, it's like this, this incredible free gift from
from, I don't know where it comes from. It's relational information
that we typically don't know how to predict, given the low level.
My quantum mechanics professor used to joke, we'd be complaining about how long it was
taking to do the homework. And he said, yeah, but the add-ons can do it like that.
Well, I could ask questions about the other topic, if that's okay. Go for it.
So this has more to do with the things and cognition. And I noticed a couple of things there.
The Markov Blanket way of looking at it took me the longest time to kind of wrap my head around
that. And I suddenly, suddenly realized that, oh, basically, you guys and Tristan are trying to exclude
stuff out in the environment that would normally just be bumping into the thing that you're trying
to look at. Whereas I've been looking at it the other way, which I got all these totally isolated
parts, how do I put them together anyway, to make something? But it's sort of the same problem.
So like I've been looking at it from the construction point of view, and that's sort of looking at it
from the installation point of view. And either way, you can make hierarchies out of them. So I like
the idea of hierarchies of Markov Blankets. That makes sense. And there's this old idea from,
I can't remember names anymore, G. Spencer Brown, Laws of Form of Outdrawing Distinction.
And there's, I can tell, the Markov Blanket idea is that, oh, that distinction actually has some
structure to it. You can divide it into sensors and effectors and so forth. So that is all fine.
And then am I right that you guys are trying to find a general principle for agents that can do
self-organization? And all I'm trying to do is find a particular organization used by humans.
But so anything that I might say about language and cognition is essentially my proposal for the
special case that probably satisfies the constraints that you guys have been working out mathematically.
So I'm pretty much all fine with that, although I have a long list of things that I need to
find definitions for before I can actually really understand, you know, all your derivation,
but I've got the gist of it. And then in particular, and then Chris and I emailed a little bit,
your reference states are, I think, exactly what I'm calling the specified
relations that define the things. And your pointer states are the various
substitutable ones that I've been saying that, well, okay, you could, it could be this or it could be
that, you know, I could have a hat on or not. And it's still me. And so I think that is all
in parallel. I do have one quick question. Is there an advantage to having pointers rather than
talking about the state itself? Or is that just a mathematical convenience?
Oh, it's just a historically traditional name. Oh, okay. Physicists, physicists talked about
pointer states, looking back to the idea of old analog meters that actually had pointers that
could point to one or two or two and a half or whatever. Oh, I was thinking more like language
comes from. I see I was thinking of like computer pointers, you know, pointing from here to there
or something. Okay, I see what you mean. Yeah. Okay. All right. No, it's a far older language.
I just got all tangled up. Okay, there's an advertisement for Bluetooth.
Okay, that's that's why that that's why the pointer states are the states that can vary.
Okay, all right. So the rest of the rest of the meter keeps its shape. And the pointer swings
around from one number to another. Okay, then so far, we're in agreement about what's going on.
Then the only thing I would say is that the reference state is not something we identify,
but the primitive cognitive act is to stipulate the reference state or whatever. And then that is
what makes you now a cognitive organism, which I define as something that can detect things.
Otherwise, it's just a bunch of flashes of light and noise coming and going. And I'm not
cognizing, but as soon as I can define a thing, then I'm capable of doing cognition.
Yeah. If I if I encode a particular reference frame that lets me identify a table,
then I'm in effect stipulating what counts for me as a table. Yeah. Okay. Oh, I see. So, okay,
that's what in anything that fits those criteria is a table by stipulation. Yeah. Okay. Okay. Maybe
totally different for you. Fine, fair. And then what I would say is that various things you do
in cognition, like an abstraction, are just moving things from one column to the other. So,
I've got a list of properties. I'll say what I mean by a property in a minute, but and those
properties are either these specified or substitutable, you know, your reference or pointer,
and the and the substitutable, I think, are the same as swappable, evidently. And then,
so if, for example, I say I have an organism, and I stipulate that it moves and it eat plants,
then, okay, that defines a vegetarian. If instead, I say it moves and eats meat, well, that wasn't
swappable. So, the meat, the meat versus plants was not swappable. I changed it down to definition
level. The open definitive book level I've got now cardboard. On the other hand, if I move
the the bit about exactly what it eats from being in a specified column over to the substitutable
column, I've now generalized from vegetarians and carnivores, I've generalized to animal.
And so that you could pretty much do this on a laptop, I suppose, just shuffling stuff from
one column to the other is either specified or substitutable, and you do various cognitive things
like, you know, abstraction definition. And then there's a hierarchy of things, I would say. So,
if you start out with per seps, those can only be created and destroyed. And the next level up,
I would say, you can have a set of them, I call them constellations, but you can't change them,
they can only be created or destroyed. So, there's still not things. It's a set of things in some
sort of arbitrary relationship. But as soon as I have something that has both specified and
substitutable, now I have a thing. And it is no longer just only created and destroyed,
it can change. And that's what lets us do thinking.
Yeah, that's super, super interesting, because it ties to some stuff that I've been thinking
about with respect to in development and metamorphosis and regeneration and all that,
which is this old idea, this old ship of thesis idea. And so, thinking about the fact that the
important thing about the ship of thesis was they replace the planks and all that and it stays the
same ship is that what allows all this to happen is the policies in the ones doing the replacing.
So, it's the policies of the people, cells, whoever it's going to be that are replacing the
components that makes this thing the same because they need to execute their changes
in a way that preserves some kind of invariant for them. So, they're going to choose where to put
the boards and everything in a way that preserves what they think of as the ship. That's the only
way this is going to work. So, again, we're back to this idea of observers and starting off with
the notion that if you want the thing to stay the same, despite molecules come and go and the body
cells come and go, if you're a cognitive system, ideas come and go and mental states come and go,
but you're going to stay the same. For you to stay the same, there has to be a replacement policy
of some sort. And then that gives you the ability to do what you just said, which is
better than staying the same is actually a policy for changing you slowly into something else.
So, if you're a caterpillar, you are maintained for a while, but eventually there's a new policy
that actually maintains you and turns you into a butterfly. And are you the same? You've got
some of the same memories and you've got some other stuff, but a lot of things have changed.
But it's all about the policies that are not just keeping you the same, but actually
slowly transitioning you to some future sort of representation of where you're what you're
going to be. And in order to do that, you have to place the molecules, the cells, the information
in the right places to be consistent. To take this back to the previous conversation,
this is precisely the kind of relational information or boundary condition information
that we were talking about with respect to Pine's paper earlier. It's exactly the kind of
macroscale structures that aren't predictable, but they end up being stipulated by something,
some aspect of the environment. In this case, the aspect of the environment is the actual
user of the representation that says, what gives this representation an identity condition for me
is a particular utility. And as long as I alter the representation in a way that
changes that utility only slowly, I can count it as the same thing.
Otherwise, there will be some dramatic failure. It loses its identity
for me because it no longer does the job I need it to do.
And analogy I have for that is sort of like jokes where there's this cartoon I saw once,
where somebody goes up to his bicycle, he grabs it, starts to walk with it,
but the rear wheel stays right where it was. Well, okay, so you can define everything like
being next to each other that does not yet get you a thing because it didn't have any utility
