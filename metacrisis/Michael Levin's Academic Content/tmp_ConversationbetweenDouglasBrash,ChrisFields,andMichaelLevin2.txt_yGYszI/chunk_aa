you. Yeah, good. Hey, Chris.
So I don't know if you guys have an agenda. I have been assembling stuff. I just, I wanted to
be more organized, but I just shot you guys an email that has some of my thoughts down and writing
in case that makes it easier. Go for it. Yeah, I just received it. Thanks. Oh, okay. Yeah, go for it.
Yeah. So I have infinitely many questions, but probably I could do some, you know, like
definitions of things as I was going through the quantum free energy paper. But the more
interesting than specific questions are something that just hit me last night. So there's like two
basically two main topics. One is like, what's the organizing principle for embryos and where is it?
And what's that telling us? And the other one is, okay, you know, what's the definition of a thing
and how does that get us anywhere in cognition? And the two are sort of beginning to merge in my
mind, although, you know, I wish I understood all of your papers better. But the thing that
occurred to me last night is about the competency experiment. So what, so that's the one where you
have this array of numbers, like one through 10, they're scrambled up and you're trying to get them
back one through 10 by doing swapping. Okay. And it dawned on me that while there's actually two
things in that box, there's numerals that are getting rearranged and there's numbers,
which are where the information is actually sitting and the constraint is actually sitting.
And you're basically using the number information and comparing current positions to the number
ordering. Numerals don't have an ordering property, you know, they're just scratches on paper.
And then as soon as you have that dichotomy, you're now basically at the same dichotomy as the genotype
phenotype blueprint object that you're making and so forth and so on. And so among those,
and that confers in a number of interesting properties. First of all, you can swap things
at the blueprint level and not necessarily get any change at the phenotype level.
Or maybe you do, maybe you don't. And that's the sort of option that lets the stress selection and
genetic simulation work and so forth. The other is that as soon as you have, think of it in terms
of like these two levels with blueprint and the output, the first thing you want is to have parts.
And so by analogy with the DNA, which is actually, I think probably a pretty good analogy in many
ways, you know, you've got different bases. And then on the other hand, you've got amino acids.
And then, so what actually happens with that? Well, you've got a very rigid
code that goes from DNA to the amino acids and then to the protein.
But which proteins you decide to make and how those proteins assemble or get assembled by
another protein. That's the part that is subject to optimization, I would guess. And so if I were
going to look for a place to apply a free energy principle, that's where I would guess it would
look. Now, maybe in evolutionary history, there's a similar principle for just deriving a code.
But in any event, so then now if you're looking for some kind of electrical code that's the analog
to that, then A, you would like some parts. Sorry, if you're liking one, an electrical blueprint
that is the macroscopic blueprint that's constraining your organism, even if you push its eyes around
or something, then A, you would like some parts of some kind, like sub electric fields. And B,
you'd like some code for how that gets translated over into actually building the organism.
And then the funny thing about the genetic code, so we know about the reading frame as
basically restricting how to go from ATGC to an amino acid. But I remember in grad school,
I asked, well, okay, where is that code sitting? And where it's sitting is in the tRNA synthet
case. And so then the question is, okay, can you look for analogs of all this stuff
with the electric fields? Yeah, I mean, it's, you're right, you're right. And it's really critical
for all these bioelectric states to ask who the interpreter is. So the mapping, right, much like
with the DNA, the mapping between a distribution of voltage states and then some anatomical,
you know, the state later on is really critical. And we spent a lot of time and we're still thinking
about, originally, we thought that it might be specific voltage levels mapped to specific organs,
and then we saw that that wasn't right. And then it really appears to be a pattern
of, you know, differences across space. So what was all the interesting thing happens when
certain cells of a particular voltage are sitting next to cells of a different voltage,
and it's the difference that's actually meaningful to the outcome, not the absolute values of either
side. It's the difference that matters. And so all of this brings to mind, okay, well, who's reading
this? You know, what's the interpretation machinery? So we have a few models now.
We put out one, and there's one that's in revision right now, of asking, how does a collection of
cells read a spatially distributed bioelectric pattern and turn on specific genes as a consequence
of this? So not an individual cell voltage turns on genes within that cell. That's kind of easy,
and we found that years ago. There are five or six different transduction pathways that do it. But
much more interesting is how do they recognize a pattern and how do they know if the pattern
is correct? How do they know what it means? And so on. And so, yeah, so that's what we're
wrestling now with now, and there's definitely an interpretation issue there. And so there are,
you know, we have a computational model of how that happens. It also, I think, ties into a very
deep issue about memory, because there are these memory transfer experiments, where you transfer,
you know, somebody like David Glansman might just transfer RNA from a trained animal into a naive
brain. We've done, we've transferred pieces. So we'll do things like transfer pieces of an animal
from one from one to another. And we look at propagation of morphological memory, propagation
of behavioral memories, and so on. And you have the same issue with the decoding on the other end.
So the thing that's always bothered me about, and it's not just for RNA, it's for any material
substrate for memories, is that if I train an animal to some sort of weird relationship that
never, you know, sort of not evolutionarily prepared for, so, you know, I don't know,
three yellow light flashes means take two steps to your left, or otherwise you get shocked, right?
So animals can learn this. So let's say that ends up being encoded in the NNN gram of some kind of
crazy molecular structure, maybe it's an RNA, maybe it's something else. I take that, I shove it into
into a naive brain. And there's got to be a, there's got to be a decoding mechanism that can
look at that structure, go, oh, look at this hairpin turn, oh, I see three yellow light flashes,
ah, right, got it. Well, where's that, where's that code book? Like, you know, and you can imagine
something like that, something for something evolutionarily expected, like fear of the dark
or something, we can all share the same code book, because you know, it's built in. But for
these really arbitrary things that is not plausible, that we share a code book that,
that already pre-specifies that it becomes really hard. And then the worst problem is,
it's bad enough when you're moving it from, from, from body to body,
but recall within the same body is the exact same issue, because, because for, right, for,
for me to read my NN grams, I don't have access to the past, all I have is, is whatever was left,
whatever traces were left in my brain and body, right, maybe not, not even just the brain,
but I have to interpret those. So, you know, the three hours from now, I'm going to have to
interpret whatever was left by past me as a message to my future self, I have to do the
exact same thing, I have to look at these, at these, at whatever this is, and, and, and figure
out what they mean, and just sort of on the fly, keep, keep rebuilding this. Yeah. So, yeah, so,
I think, I think, I don't know, Chris, what, what you think, but I think this, this interpretation
machinery is, is, is really the key to all of this.
Yeah, I, I, I agree that it's a very interesting and very tangled issue.
You know, and if you think about an electric field, for example, and you want to think about
components, then really the, the only options are charge center locations, which you could mix and
match into an electric field to change its shape, or frequencies, mix and match into a field to
change its, its temporal shape. But that doesn't give you a whole lot to play with. And in the
developmental setting, it's not clear that frequencies are playing a role, whereas in the
brain, for example, they clearly are playing a role. So, you know, which, which degrees of freedom
in the encoding itself is an interesting and difficult question, what counts as a component.
But in the developmental setting, it's very clear that you have cells as components. So,
you have components in the reader, even if you don't have components in the source.
And so the question becomes, how do the different components of the reader
communicate with each other about their joint interpretation of the field? So you have the field
becomes one communication medium, that these, these multiple components have to jointly interpret.
But to do the joint interpretation, they have to somehow talk to each other.
Otherwise, they would have no basis for knowing what a difference is, for example.
So, we're faced in this case, I think, with a minimum of two different languages. The language that's
embedded in the field itself, talk to each other about their local measurements.
So they may be talking bioelectrically through gap junctions or something.
But that's, that's a code, you know, bioelectric code that's in addition to the overall code that
they're reading. Can you get, and this is, I was going to say, you know, and this translates into
thinking about natural languages in terms of, you know, positional effects and grammars and that
kind of thing, that influence what the semantics of the different words are.
Can, can you get any traction, or is there anything that gets provided by either having some of this
bioelectricity being a carrier wave or something, helping cells, individual cells
contribute to some overall pattern that then gets communicated back to each of the guys who
contributed to the party, or resonance doing something like that?
Yeah. We don't, we don't have any data yet on resonance, but I think you're absolutely right.
And that, well, first of all, the carrier wave business. So, so every cell, by itself,
at, during the cell cycle and just, you know, sitting there, it has these little,
these little fluctuations. So, of VMM. And so in addition to that, you know, whatever, whatever
bi-electrics they're doing as part of patterning has to sit on top of this baseline. In fact,
back in, I don't know, 2000, when I was first talking about starting to try to manipulate
resting potential, this is what everybody said is that, is that, well, this is a housekeeping
parameter, you know, you can't, you can't mess with it, otherwise the cells will die and, you
know, nothing, nothing will happen. So, so, so, yeah, they do have this like baseline wave,
and then on everything else happens on top of that as, so I think, I think the carrier wave thing
makes sense. Also, I, yeah, I think it's, it is true that, that functionally, the electrical
activities of these cells are added up to what in effect is a global computation that then ends up,
it's the same, I mean, it's, it's funny, the, the interpreter is the same cells as the generator,
in this case, because the cells, right, they, they have to generate these patterns, it's the
same cells that are then going to read that pattern and do something as a result. So they're
sort of talking to themselves in a way, but, but, but, but there's also a jump in level of
organization, because by what the computations that they're doing by this bi-electrical signaling
take place in an entirely different space. So the individual cells are computing things like,
when do I divide, how's my metabolic state, you know, who's my neighbor, this kind of stuff,
but the collective has to, the collective has to make decisions about huge things like,
how many fingers do we have, and where do the eyes go, and how many eyes do we have, right,
it's an entirely different, different problem space. And so that, that, they're executing
these, these computations as, as part of a collective intelligence, which then immediately
filters back down and says, okay, you're going to be a bone cell, and you're going to be a nerve
cell, and you're going to be a muscle cell, but that isn't what the initial computations were,
it sort of comes up and then goes back down. So since you mentioned the computations, another
thing that this two level hierarchy, the genotype phenotype kind of hierarchy, or syntax semantics
kind of hierarchy, buys you is the ability to do particular computations instead of mixing.
Do you guys know about William Abler's particular principle? So I can send you guys the paper,
it's actually, I think very important, and almost nobody knows about it. But
so the question is, if you have a red and white, and do something, do you get
some variations of red or white, or do you get pink? And of one of the reasons Darwin was stuck
with going to Lamarck, was not that he liked Lamarck, but he couldn't see any way out of the
red plus white equals pink. Whereas as soon as you have a genotype and a phenotype, you can now
be mixing the genes, and so in the case of Darwin, so you've got tall and short. Well,
a couple generations down, everybody's going to be in the middle, if all you do is mixing.
There won't be any tall people or any short people. And so how do you get out of that? And
the solution with genes were essentially, they do the particular computation. So you don't lose
the origin, the identity of the original element to the computation, they're still sitting there.
You just recombine them in different ways, and then you now generate a phenotype from the new
combination. Yeah, you had some other stuff. When we think about writers and readers being
the same systems, it naturally leads one to think of the field as a memory structure.
You know, Mike, as we were discussing with Santosh the other day in computational meeting.
So you can think of the cells as writing to this memory structure,
and then reading from this memory structure.
But one memory can also read from it, and they may be reading from it in a different language
than you wrote to it. So the collective may be interpreting the same memory structure
using a different syntax and a different semantics from the
entities that wrote into the memory structure. And in a sense, you see this in the genetic code
because you have coding redundancy for amino acids,
and different codons code for amino acids with different efficiencies because of the
kinetics of the different tRNAs are different. But from the protein's point of view, and hence
for the evolutionary system that's selecting changes in the genotype, it may not be sensitive
to that language at all. It's sensitive to the reproductive capacity of that entire system,
be it a cell or an organism or whatever. So the collective is speaking, in a sense,
a completely different language from the codon language that's spoken by the DNA
that's used by the DNA to write the instructions for proteins.
And the proteins in this case are serving, in a sense, as the shared memory device.
That's nice. And it happens to work. That reminds me of an old paper which I can also send you
having to do, basically, the question is, okay, we speak languages, but we can also understand
the guy. How is it that speakers and listeners converged on using the same language, you know,
human speakers? And basically, if you introduce a couple of constraints, which in my case,
I think it turns out to actually be this analog to a reading frame that I think language is used,
but you introduce a couple of constraints. And then it's not all that hard to converge on
a common generator, you know, reader and writer, that even if they're different,
can at least understand each other.
There's another lens on this, too, which Josh Bongard and I just put out a paper on
polycomputing, which is this idea that, you know, the exact same set of physical events
can be interpreted in different ways by different observers and thus be doing different computations
literally at the same time. And so we talked about he and his student, Atusa, have this amazing
sort of mechanical example of multiple computations being done by the same, you know,
piece of particulate matter. And I picked out a bunch of biological examples from that paper,
where basically one way to, the title, which Josh came up with is, there's plenty of room right here,
which goes back to like this, this idea of room at the bottom, right? And so this idea that in
biology, there isn't any room anywhere else because it's all packed full of stuff, there's
stuff everywhere. And so the way to squeeze more out of it is to evolve additional observers who
do interesting things with what is already going on, right? So as opposed to trying to
tack on new mechanisms. And one thing that's interesting about that is that if what you're
evolving is a new perspective on an existing set of events, it means that you don't risk breaking
those events, that's completely different than trying to make tweaks when hope that you don't
lose your past gains, right? You basically don't touch the, you know, it's observation only, right?
So you don't touch the thing that's going on. And as a simple example of that, we had this,
we had this other paper recently where you take a gene regulatory network, and depending on how
you look at it, meaning you pick three nodes and you call one of them the condition stimulus,
one of them the unconditioned stimulus, and one of them is the response. If you pick the right
