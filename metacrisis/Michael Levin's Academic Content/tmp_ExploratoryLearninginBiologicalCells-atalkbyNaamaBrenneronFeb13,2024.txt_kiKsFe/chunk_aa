Title is exploratory learning in biological cells.
And what I'm going to do is I want
to talk about one particular type of learning.
And you guys have been thinking, I
think basal cognition is the title
of generalized learning in different embodiments.
One particular type of learning that I've been interested
in is exploratory learning.
And I want to, it has roots in psychology and in neuroscience.
So I don't have to yell, right?
No, no, no.
I feel like you don't have the right response, are you?
You can hear it.
OK, right?
I'm talking too loud.
Yeah, they can hear it.
All right.
So this particular kind of learning
has roots in psychology and in neuroscience.
And I'm going to present it in this context first.
And then I'm going to talk about how we kind of transfer
this, or apply it to cell biology,
in the context of gene regulation right now.
So this is quite old work by now.
And then I'm going to talk time permitting about our efforts
following that to model this type of learning in random networks.
And the bottom line of these modeling studies
can be stated in a way that also resonates with something
of this group's result is that learning is feasible,
but it's not universal.
It doesn't happen with any just random networks.
It depends on certain properties.
But on the other hand, I will argue
that these properties are not so rare,
and they are shared by many real systems.
So this just might be a relevant type of learning
for cell biology.
OK, so I start more than 100 years ago with Soren Dijk's
Animal Behavior, a classic book in animal behavior
with the same title.
And I want to refer to one of his experiments
where he took cats.
He had built these puzzle cages.
And he placed hungry cats inside the puzzle cages,
and they were supposed to find their way outside
to get to the food.
And the cage was built such that there was some lever
or something to be pulled or pressed.
And the animal had to find that in order
to get out and get the food.
And then he repeated this after the animal.
In that, it was repeated many cats.
And what he found was that first for a while,
these animals just tried out many different random motions,
any different ways to move their body and their limbs.
And at some point, they hit upon something successful
that brought them outside to the food.
And his observation was that over long time scales,
after many repetitions, they had associated
this last successful motion with the reward.
And so then they could repeat that much faster.
And we can see here, this blue line below or sort of a little
more here.
So what you can see here below the box
is a graph of the average time required, the escape time,
how much time it took the cat to get out of the box
as a function of trials.
And you can see that there is a decrease.
So this is a type of learning.
The cat has associated successful actions with the reward.
And so, yeah, so this is the behavior that Thorndyke
formulated as behavior leading to a satisfying situation
or avoiding a stressful one are likely to be.
This is a kind of trial and error.
And many years later, I mean, it's like already 20 years ago,
but much later than this original experiment,
Shimon Mahon, who is a member of our center,
decided to take this from the behavioral level
down several levels of organization to an ex vivo network.
And I've heard already this morning
that people are working with or have been working
with such networks, so maybe you know this already.
By the way, I didn't say this, but please start me if you want,
if I'm saying obvious things or if you want to ask.
So here's Shimon's experiment.
There are neurons taken from newborn, usually rats,
and placed on this chip with electrodes.
And you can talk with this system
through stimulation and reporting in any one of the electrodes.
Hanana can say about the system much more than I can.
But here, the thing I want to note I want to point to
is this experiment design that was formulated
in order to put to the test the question
whether such a system of neural substrate,
which has no anatomy and is not embodied in an animal,
can perform in a way similar to drive reduction
as formulated by the law.
So here's the translation.
There is...
The task that was identified for the system
is specific input-output response.
So I choose one electrode as the stimulating electrode,
and then I require the system to respond
in another predefined electrode.
And the assumption which has been studied beforehand
is that electrical stimulation drives the system to be modified,
to modify its connections, to modify its states as a network.
And so when you put in stimulation, you're driving it to change.
And so the idea was that I'm just driving, driving, driving.
Once I get the required response, I shut the driving down.
So this is a closed-loop system with a computer.
And when I read the response that I had anticipated
or I had required, I stopped the stimulation.
So this is the analogy of the cat's hunger that's driving it,
and then when it's already not hungry, it's not searching.
All right.
So here's an example of a measurement.
So this is one electrode.
If I choose this electrode to answer to respond after a predefined time window,
50 milliseconds in this case, a priori before the experiment,
there's not a lot of reaction in this time, not a lot of response.
But after the experiment is finished,
I have a lot of response in this time window.
And the fact that this experiment actually worked
is highly non-trivial results.
So this is an implementation, again, in an ex vivo system
of this animal learning or animal behavior concept.
So here's what the escape times look like.
I have on the x-axis the trial number, like in cats.
And I have in the y-axis the time to the response,
to the required response.
And these are jumping up and down.
Also, the cats do this.
Also, animal behavior does this.
And when you average over many trials, you get this decrease,
a smooth decrease in the response time.
Response time is the time that this takes to get the required response.
And then you will say, the system has learned what I wanted to teach it.
And this is the control.
If you open up the loop and there's no response dependent on the system,
then this time just increases.
So is this, does everybody know this paper?
Is it clear, though?
Yeah, OK, so this was, I think this is a highly influential,
was a highly influential paper, although many people
try to repeat this experiment with not a lot of success.
So experimentally, this has not turned into a paradigm in neuroscience
for reasons that I don't really understand,
but I think conceptually it was very influential.
So let me zoom out for a second and just say things
that I think in this group are going to be quite obvious to all of you
and just kind of make a contrast between two ways of forming a task
or inducing a desired behavior.
One of them is a deterministic algorithm, a series of directed actions.
For example, I could write for you if the cat could read,
I could write for the cat, do this and after that do that
and then give them a series of things.
This has analogs in other areas.
For example, when you think about learning,
what's the algorithm for strengthening the synapse?
There are time-dependent plasticity rules.
So there are microscopic rules, same with gene circuits
when this gene is up, that one turns on.
This is a very, could be an efficient way, but it's very specific.
It has microscopic world-defined goals and it's repeatable on all levels.
And I will call this an instructive way of forming a task.
The problem, of course, is that this is limited,
can only store so much information about a finite list of tasks
that you can work out in this way.
And to contrast that is the other extreme is trial and error
or exploratory learning where you do something, you try something out,
you estimate your error from some feedback
and then you modulate maybe through a random walk.
And this type is inefficient, but could be inefficient,
but it's very, very general.
If this works like we think it does for the neural net or for the cap,
then in principle they can learn anything.
And an important feature of this is that there are,
there is a macroscopic goal,
but I do not instruct the system on a microscopic level how to do this.
And so there could in principle be multiple solutions.
Okay, and this somehow is a selective way
that I would like to fortify or strengthen the correct,
or stabilize the correct trials.
Okay, so what I would like,
what we tried to do and at that point is,
this is a joint work with Eris Brown at the Tech of Neon.
We wanted to take these concepts and apply them to cells, okay?
So we're going even one step below the network
that was composed of live cells and all the other things,
and we're going to the single cell.
And then we were thinking how this relates to gene regulation.
So gene regulation is often thought of as a control system
that has the first type,
this is the instructive type of a structure, right?
It has, you can make a map of the interactions,
this gene drives that gene, et cetera.
And there's a blueprint for a program, in this case,
no, this is the developmental program for C-urchin.
But actually gene regulatory networks do much more than this.
