but the answer is that it's not very
sensitive to the way the stress is driven back to the system.
It could be a step function, it could
be a smooth function.
It's a little bit sensitive, but not extremely sensitive
to the way you feedback the stress to the system.
OK, now the short answer is that sometimes this converts.
Then I go through the dynamics.
I simulate these equations.
And then I would like to ask, what
are the chances that this converges to something
that I define?
And the short answer is that sometimes it converges.
And here's an example.
This is a network of size 1,500.
This is a network which is kind of stressed, right?
It has the red or orange color.
It's under stress.
This is the projection.
The projection defines the stress which feeds back.
And here is the dynamics.
So the dynamics goes on and on.
The projection is also changing as a function of time.
And as long as the stress is high,
also the j is the connection matrix.
So the j, these are just four out of, I don't know,
whatever number, larger than 1,500.
I had connections in this one.
And they're slowly changing.
So there's a separation of time scales here, if you notice.
The dynamics is fast.
The little dimensional projection is a little bit slower.
And actually the connections which
are being modified, that's plasticity, is much slower.
But it's enough.
And it doesn't change by that much.
But there are many connections.
So eventually at some point, the system
stays, finds a good configuration.
And at that point, it hits fixed point.
And that fixed point complies with my demand.
And then the system goes, statement, yeah, question.
Is it appropriate to think about what your modeling year
is kind of like a large network of homeostats,
kind of like the Ashby homeostat, where
there are basically random connections
that were allowed to change up or down some, whatever.
You put a fixed point, and they bounce around,
and eventually, we'll find their way.
Yeah, I think that it's a high dimensional.
High dimensional.
I mean, I've read Ashby's model, and I,
that's in the direction.
So it's not bad to use that as a,
yeah, because the system is kind of, you know,
kind of self-fueling something, self-monitoring,
self-modulating until it finds it and attracts.
OK, now, but what I told you is that, you know,
when does this happen?
That's the question.
And the answer is, I have the answer yet.
This is the short version of the answer.
And the result is that it can work,
like in the example that I showed you,
but it doesn't always work.
And this question, whether it works or not,
whether it converges or not,
depends on the network structure, right?
And the short answer is that outgoing hubs
in the network support exploratory, right?
How do we know this?
I'm now getting a little bit deeper
into the mathematical part, into the computational part.
So this is how, this is how we studied this.
So we defined a collection of statistical examples.
So you know that in a network,
there are outgoing connections or incoming connections.
Now, stop me, please, if there's a question.
The network is characterized by some structure.
And in statistical terms, we say there's a distribution
of outgoing connections and a distribution
of incoming connections.
And I created, we created this collection,
where we picked every one of these distributions
from a different, with a different character.
So for example, the simplest network
is one where you have a binomial distribution
of incoming and outgoing.
Is that a question?
No, no, it's someone good coming in or coming out.
Ah, okay.
This is the simplest network over here at the bottom.
You just take a thousand nodes
and then you just, you know,
with some probability you connect
any two of them at complete range.
This network has not converged ever
in our experiments, even for very long times.
It can never learn the thing that we tried to teach.
All right?
Another type of distribution is a scale-free distribution
which has a long tail,
which means that you have some nodes
that are really highly connected.
Right, there's a range,
but some of them are really highly connected.
And okay, so you can see that different topologies
and different structures of network behave very different
from zero to more than 70% learning capacity.
What that means is that if I ran the simulation
for a hundred times,
I get 72 simulations where this network was able to learn.
Question?
For the simple network,
do you think that the stress is too high
so they don't even try it?
They can learn or is it?
I think I can say a little bit more about that later
as this unfolds, okay?
All right, so what you can see is that,
first of all, the different topologies behave differently.
And secondly, the three best ensembles
or the three best types of networks
are those that have scale-free
in their outgoing distributions, right?
So that means they have nodes
that are simultaneously driving many other nodes, okay?
All right, why is this important?
Let me go back to yeast cells for a minute
and try to connect this to gene regulatory networks.
So we know that people have mapped out,
there are more modern papers,
but this goes back 20 years,
and people have mapped the structure
of gene regulatory networks
by different experimental techniques.
And what you're seeing here is a distribution,
experimental distribution from binding experiments
and showing what the distribution of connections are.
So here's the picture for yeast, okay?
If I'm a gene, I have regulated by relatively small number
of genes, not shown here, but if I'm a transcription factor,
right, this is the graph for transcription factors,
a lot of them are regulating one, three, 10 other genes,
but some of them, look at this tail, okay?
I have few genes, here's one, here's two genes,
I have a few of them that are regulating
hundreds of other genes, all right?
And these are exactly these out hoeing,
not scale-free exactly,
but I do have these nodes that are affecting
simultaneously hundreds of other ones, okay?
And this is only like a,
to flash the experimental,
I'm not going into the details of the experiment,
but the experiment shows that out of this collection
of ensembles, the closest one to experiment is this one,
is the one that has a scale-free out distribution
that you can see here and an exponential in distribution,
which I'm not showing, okay?
So this gives me a hint that in networks
that look like gene regulatory networks of the yeast,
which are not completely random, they have structure,
maybe this algorithm of exploratory learning
is indeed feasible, it converges 60% of the time,
okay, also in the experiments, as I showed before,
not, this doesn't work 100% of the time,
but it works a significant fraction of the time, okay?
All right, so the picture that is starting to emerge
that is that maybe what random systems cannot do,
still regulatory systems can't do
because they have, to apologize,
they have structure that supports certain kinds
of statistical learning, all right?
However, you know, if you're, again,
if you're a little bit computation on the incline,
then, you know, people see this and they jump up and say,
this is not a scale-free distribution, right?
If you've worked on these avalanches,
this is a debate that goes on for decades.
You know, there's just like several dozen,
two dozen points here, is this a scale,
is this a power law, is this not a power law?
But still, I can see at the edge,
I can see these large hubs that are affecting
hundreds of other genes, all right?
So my experiment in my model is the following,
I take a look at this, I'm gonna skip this one,
but take a look at this right-hand image,
I'm gonna start from the network, which is very generic
and does not know how to do exploratory learning.
And then I'm gonna add one by one a number of hubs.
And like I said, in the beginning,
when it's a completely random network,
it does not know how to learn at all.
By the time I've added 10 out of 1500, right,
10 outgoing hubs, I get 40% there.
What this comes to tell me is that I don't really need
this mathematical structure of a power law.
I had used this when I wanted to create different topologies
