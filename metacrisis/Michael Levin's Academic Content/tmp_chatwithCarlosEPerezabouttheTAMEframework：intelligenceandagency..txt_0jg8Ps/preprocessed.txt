What I was going to talk about is this, this, this issue of, of part of agency.
So goal directedness is really critical problem solving, but also this, this
auto poetic step of, um, knowing, uh, or, or estimating for yourself, having a
model of where you end in the outside world begins.
So this, this issue of, right?
So this, this, this in for, in sort of informational self-construction.
And, and one, one model I have for this has to do with early embryogenesis.
So imagine, imagine you're a blast, the disc of, uh, you know, I don't know,
a bird or a human, some kind of amniote.
And you're, you're a disc of about 50,000 cells, something like that.
And, um, typically we look at this thing and we say, okay, there's one embryo,
one individual, it's going to be one agent.
It's going to be a duck or a human or whatever it's going to be.
But, you know, but, but what are we counting when you say there's 50,000
cells, what do you count?
When there's 50,000 cells, there's one embryo.
When you say there's one embryo, what are you counting?
So what you're counting is the reliable alignment of all the cells towards a
specific path in anatomical morpho space, meaning that left to its own devices or
even perturbed in various ways, all of the cells will work together to make
exactly the right thing, whatever the target morphology for that species is.
And you can do all kinds of stuff and move things around and they'll move back.
And, you know, you have some, there's, there's all kinds of ingenuity there
about getting to where it needs to go.
But, but that's what you're counting is that there's one job to be done to
build this particular thing.
And we all know what it is.
And, and I, but, but it's actually much more interesting than that.
Cause what you can do, and I used to do this as a grad student with duck embryos,
you can take a little needle and you can make some scratches into that
blaster.
When you make the scratches, you separate it into several islands, two or,
let's say two or three.
And that early system that aligns cells towards being one embryo, it's got
this, among other things, what it has is this ability of local activation,
long range inhibition, such that, you know, there's an organizational, it's a
few cells become the organizer and they tell everybody else, you guys are not
the organizer, I'm the organizer.
And then, and then they sort of organize the embryo.
So when you separate this thing into disconnected islands, you, basically
each one of them can't feel the others because there's a, there's a bad,
there's an empty space between them later they will heal.
But until that happens, each one is an individual and each one becomes a
separate embryo.
And when they do heal, you get twins or triplets.
And so now this is very interesting because if you think about that 50,000
cells, every cell is some other cells neighbor.
So the question is, where is the agent, right?
And when there's three of them now, now that's even more interesting because
there are some cells on the boundary and they're sort of confused about who
they belong to, right?
And there's all kinds of medical implications of that, that I used to study.
And so there's this, this idea that the number of agents or cells in this
medium, it's like this, this, this, this kind of excitable medium or, or, or,
you know, this, this Freudian ocean of potential cells.
There could be zero, one, two, three, you know, maybe up to half a dozen in a
typical blastoderm.
And you don't know how many there is.
It isn't preset.
It's not preset by the genetics.
It's not preset by the hardware.
It's an emergent fact of, of, of the physiology where some number of
cells will demarcate themselves from the outside world.
And so, and so I really like this.
And I really like, um, uh, thinking that what's, what's important, uh, and
that's just one, one thing that's, that's important beyond goal directiveness.
But, but that's, there are some other stuff, but, but that, that, that process
of, of, uh, delineation of a self as separate from the outside world.
And the fact that you have to do that yourself, as opposed to a lot of our
robotics and AI that way that's given and, and, uh, you know, pre-determined
from the outside, the agent doesn't have any choice about it.
This is, this is your limits.
And that's it, right?
That, that I think is very important.
And it has all kinds of implications and biology that lead to a lot of the
cool things that we like about biology.
One of them is this weird, um, intelligence ratchet, which, which is,
which is, which is this, um, first, I'll tell you about the ratchet first
and then, and then how it relates to all this.
So the ratchet is this, imagine that, um, uh, one, one of the funny things
about planaria, these, these flatworms is that they're incredibly regenerative.
Um, very reliably regenerative.
They are, uh, cancer resistant.
They are immortal.
Basically they don't age.
And yet they have a really chaotic genome.
And the reason is because when they divide that the way they, the way
they, um, uh, reproduce, unlike us, if we get a mutation in our bodies, our
children don't automatically inherit that mutation, right?
The, in the planaria, they, they rip themselves in half and then each half
regenerates.
So that's how, that's how they mostly reproduce.
And so, and there are other species that do sperm and egg, but, but there
are species that just regenerate.
And so that means that any mutation that doesn't kill the cell is amplified
into the next generation and makes up the body.
So they can be mix a ploid.
Every cell could have a different number of chromosomes.
You know, it's incredibly, incredibly messy on the, on the, on the hardware.
And yet the animal with the, with the, with the, with the messiest
genome is the one that has the most, uh, cancer resistance, uh, regenerative
ability, and, uh, it less aging.
So that seems really strange.
And that bothered me for many years, uh, how that can be wise.
The animal with the, with the, with the noisiest genome, the, with the, has
the most, uh, morphological stability.
And so recently we kind of studied this in a computational way.
And, and, and I think we finally have a little bit of insight into how it works.
Um, because this is, this whole process, this evolutionary process is not dealing
with a passive material.
It's dealing with, uh, cells and, and, and tissues that themselves have
various agendas and physiological space and anatomical space and so on.
What happens is that, um, here's a, for an example, if I take a, if I take a
taphole and, uh, um, I, I move in the embryo, I move the mouth off to the side,
right?
Uh, what happens during, during development is that it fixes itself.
The mouth, the mouth will come back to where it needs to be.
So if you had a mutation that made that change, your fitness is not zero
because you can't eat.
Actually, your fitness will be fine because the mouth will come back.
And so those, the ability of, of, uh, the cells to make up for these
kind of weird, um, uh, mutations and other, other problems means that it's,
it's quite hard for evolution to, to gauge the quality of the hardware.
When you come up for selection, if you're, if you're a pretty good tadpole,
selection doesn't know whether you're a good tadpole because your genetics
was amazing or you're a good tadpole because your genetics was so, so, but
you fixed it, right?
The, the, the structural genome was eh, but, but the competencies fixed it up.
So that means that, uh, the evolution has a hard time, uh, improving,
improving, even with a little bit of competency, evolution starts to have a
hard time improving the structural genome.
But what it can do very easily is improve the competency.
But when you improve the competency, it becomes even harder to gauge the actual
structural genome.
And that means, that means all the effort goes into increasing the
competency and so on and so on.
So you get this, you get, you get this, uh, positive feedback loop where the more
competency in the individual parts, the less emphasis and the less pressure goes
onto the hardware, meaning the genome and the more onto this actual competency.
It's like, um, Steve Frank once gave me this great example of a, uh, uh, he said
that once, uh, raid arrays started being popular in computers, um, the quality of
the actual disks has gone down because you don't need to have great disks
anymore because you have a raid array, right?
So the pressure on having, uh, having really low error media is, it goes,
goes down.
So it's something like this, the pressure is released because of the competency.
So if you're a planarian where, you know, for a fact that you really can't, uh,
rely on your genome being very clean, right?
Your hardware is, is, is, is unreliable.
Um, you really, uh, all the pressure is on to develop a, an algorithm that makes
a good worm, no matter what the hardware looks like with limits, obviously.
Um, and this, this actually explains a very weird, uh, you know, sort of fact,
which is that, uh, unlike every other, uh, most other creatures where you can get
mutant lines, right?
So, so flies with curly wings and albino, you know, rats and things like that.
Um, there's no such thing in planaria.
There is, there are no abnormal lines of planaria except for the two headed form
ours.
And those are not genetic.
Those are, those are made by, by altering the bio electrical memory, not by,
there's nothing genetically wrong with them.
So, um, so all of that, uh, bring, bring us back to the original, uh, discussion.
So all of that means, um, that, uh, you really, uh, because you can't know ahead
of time as an embryo, you can't know ahead of time.
Do I have the right number of cells, the right size of cells is my DNA?
Okay.
You don't know any of this.
And there are many examples in biology where, you know, embryos, um, construct
themselves despite all sorts of crazy variabilities.
It's because none of this is baked in.
You have to solve these problems on the fly.
So however many cells, bigger cells, smaller cells, they're very good at nature.
Evolution gives us these, like, um, problem solving machines.
It doesn't just make specific, you know, solutions to specific environments.
So, so because of that, because of this need to solve this thing from the word go,
I think that, um, the kinds of things that we associate with biological agents are,
are part of this intelligence ratchet where in different spaces, some of it visible to us,
some of it, we're very bad at noticing, but there's this constant
ratcheting of problem solving capacity because they have to do this.
Because no one is there to reliably tell them where do you end?
What are your sensors?
What are your effectors?
What space do you work in?
No, no, you know, that it's all up for grabs, right?
No, no life form that, that, um, uh, took that for granted could really survive nowadays,
maybe early life did, but nowadays that, that wouldn't fly.
You know, with all the competition.
So anyway, so that's, that's my very long, long answer to, to, to your, to your point.
Oh, okay.
So, yeah, so, yeah.
So in essence, there's these, um, we have agent, agential matter that has, uh, evolved over
billions of years that have this competency that it can solve, um, these, um, different problems for
keeping itself alive, essentially, or having itself grow, right?
But you're always beginning with this kind of matter, which, um, a, um, a deep learning system
will not have or does not have.
Yeah.
Yeah.
Right.
So it's just, um, uh, um, Calculus and linear algebra, essentially, right?
Yeah.
I mean, so, so, so, right.
So, so the things that, that I think are important is that it's a multi-scale system where at every
level, like I give this talk sometimes called why robots don't get cancer, right?
It's because, it's because our current technology, it only has agents, the intelligence or some degree
of agency at one level.
The parts are pretty dumb.
It's the, it's the whole robot that you hope has some intelligence biology.
Every, every layer is a problem solving, uh, kind of thing.
So, so that's, so that's useful.
The fact that it has to construct itself from scratch every time is useful.
The fact that, um, it's always, uh, metabolically on the edge of, uh, of starvation, basically
constantly has to find energy, which means that it has to, uh, be really good at doing
causal course grading on the environment or else it'll die.
Like all, all of these things.
So that's what makes it different from current devices, but I'm, I, I, I really believe that
it's not, there isn't a fundamental divide as in, you know, we will never be able to,
you know, engineers will never be able to duplicate what, I don't believe that's true,
actually.
I think today's technology doesn't do this well.
That's, that's true, but I don't see any reason why if we, um, learn, uh, the, the,
the lessons correctly from biology, and I'm sure there are others that, that we don't know about,
I don't see any reason why we couldn't engineer this way later.
And in fact, in fact, you know, I, I, I, I would, um, Jamie Davies and I wrote this thing
called engineering with agential materials.
Like, um, I don't think it's unreachable for us.
I, I don't think it's, it's magic.
I don't think evolution has a monopoly on this.
I think engineers could do this, but, but I do agree that today's technologies are not
this kind of system.
They're, they're different.
Yes.
I, yeah, I don't disagree with that.
I agree that if, um, if they train these systems with a particular curriculum such
that, um, it needs to essentially survive, then it will learn the heuristics to do that.
Yeah.
And then that would get you to a, an AI, right, an AGI with, um, with agency.
Yeah.
Yeah.
I, with stronger, and I mean, again, I don't, I don't like, um, binary categories on almost
anything.
So, you know, I, I, I, I think they have extremely low agency, but I, but I, I, and I
think we can, I think it's possible to make ones with more.
Um, I don't know that it's, uh, I, I,
I think that, uh, we have to be ready also for the fact that, I mean, you know, up until
now, all of these things tracked very well together.
So anything that spoke, you kind of knew that it went through the same.
It had the same existential struggle that you did.
It went through the same, you know, you, you kind of could make all these assumptions.
So now for the first time, we see some truly diverse intelligences.
We see some things that are problem solving, uh, agents that are not like us at all.
And, you know, uh, and, and we, we're now finding that you, in fact, you can dissociate
some of this stuff.
And I think it's really, a lot of people are finding it really hard to, um, get away from
this, this binary vision that either it's, it's, it's a dumb lookup table or it's just
like us.
Those are not the only options, right?
There's a diverse, you know, there's a, there's a huge space of possible minds that
with different failure modes and different abilities and some of them look like us and
some of them don't.
And, and, and it's easy to be fooled.
But I think the, the solution to all this is, you know, kind of a proper grounding in
the diverse intelligence field where you start to understand that, um, yeah, these are not
our only options.
There are many ways to, you know, to, to have a kind of mind and, um, we're going to have
to learn to relate to all these things in a useful and ethical manner.
Yeah, I think what I wanted to be, I guess the problem has been that the, the only
intelligence, the only general intelligence that we are familiar with are humans.
And people have built or cognitive psychologists have built their models based on humans that
have agency.
So they can't conceive of a general intelligence that is devoid of agency.
Yeah.
It's not in the current models.
Now, something that looks like appears to be generally intelligent, but it's completely
devoid of agency.
Yeah.
Yeah, I think, I think, uh, this, this, uh, one of the biggest issues is that all of our
kind of major sense organs are pointing outward in the three dimensional space and
we're very good at noticing medium sized bodies moving in media and at medium speeds
in space and saying that, oh, look, you know, here's a crow or, or, or an ape or an octopus
doing something clever and we can recognize some, some intelligence that way.
Imagine like, um, imagine if you had a primary sense of your blood chemistry, if you had
another sense, kind of like taste, but, but sort of, you know, more, um, but, but aiming
inwards into your blood and a little richer, kind of like almost like vision.
I think, uh, if we had that sense and we sort of grew up with it, we would have absolutely
no problems recognizing that we also live in a physiological state space and that our
kidneys and our liver are these amazingly intelligent agents that navigate that space
because we do, you know, we, we, we, um, uh, uh, challenge them with various stresses
throughout the day and they do all kinds of interesting things and they navigate this
space and solve problems for us and so on.
We just have a really hard time envisioning intelligence and other problem spaces.
Right.
But, but, uh, I mean, you know, humans are kind of general intelligences.
Yes.
But like we don't typically, we as humans don't typically solve problems in, um,
in some of these other spaces where, where, where individual cells, bacteria and other
things do very well.
So we, you know, I feel like, uh, this kind of human centered approach is, is really, uh,
blinding us to a lot of examples of intelligence out there.
And that's why people are so freaked out about suddenly being confronted by this,
you know, kind of linguistic intelligence.
I mean, yeah, uh, this, this, these, these radically different minds are, are all around
us all the time.
We're just very bad at noticing it.
So, um, yeah.
Yeah.
So what do you, do you make out of the argument that, uh, if we, uh, that if we continue to
go on this path of accelerating AGI that it's, um, it's an existential risk to humanity.
Does that make sense?
Wow.
Yeah.
I mean, so, well, I guess a couple of things.
Um, we have many things that, uh, could potentially pose an existential risk that are
not necessarily like us or a gentile or whatever.
There are many ways that we could kill ourselves off, right?
Um, I, I think it's not impossible that if we don't learn the lessons of diverse
intelligence, if we fail to understand how properly to relate to these things,
that we could end up, uh, really, really having problems.
I don't think that's impossible at the same time.
I, I don't think the solution is to, uh, try to stop research in some way.
I think it's impossible even, even if we, you know, if it was a good idea and I'm not,
I don't think it's a good idea.
Um, I think that if, if, if, if, if it does end up causing a major problem for us,
it's going to be because it's not going to be because of anything it does.
It's going to be because of our, uh, refusal to learn to, to relate to other kinds of minds
in a novel way, right?
All we know is how to relate to other humans.
Barely, we can sort of relate to animals, not very well, but, you know, kind of, um,
and that's it.
And we're really not willing to understand anything else.
And at that point, uh, yeah, I think, I think, I think if we don't learn that lesson,
we're going to have a major problem, but, but not even just because of the AGI's,
uh, the, the, um, uh, the kind of computer intelligence.
I really think because of all the biotechnology and, and advances in
biorebotics and things like this, we are going to be surrounded by, and certainly
our children will be surrounded by beings that don't resemble us at all.
Meaning, you know, cyborgs, hybrids, uh, chimeras of all kinds, uh, human, augmented
humans, uh, augmented, you know, biorebots.
There's going to be all kinds of stuff in our environment.
And if we don't, uh, regardless of the software AIs, if we don't understand that,
that, uh, we need, we need, uh, expanded ways of predicting the goals of new composite systems
that we haven't seen before.
Like that's an important science that we don't really have yet.
And ethically relating to other beings that don't share a path on the evolutionary tree
with us and have a completely radically different intelligence.
You know, if we don't wrap our, our minds around all of that, I'm pretty sure we're
going to have issues and it's not going to be just because of the software agents.
It's going to be because of our inflexibility with dealing with radically different beings.
Yes.
So the, the premise of this existential threat, I believe is it boils down to this idea that
we cannot imagine a different kind of intelligence other than ourselves.
And we look at ourselves and we can imagine that, um, people that humans could be, um,
essentially selfish and basically, uh, extinguish every other non, uh, other agent.
Right.
So, so it's related to having this, uh, limited, um, viewpoint or model of, um, other, uh,
intelligences, which is also the same problem that you're bringing up, which is if you don't
expand that, then we're going to have a problem.
Yeah.
Yeah.
Yeah.
Yeah.
I think, yeah.
I think, I think we have to, uh, I think we really need a, a, a, you know, an education in this,
in this diverse intelligence, uh, these ideas.
And, uh, this is going to be required because, because there's all kinds of stuff coming,
not just the software, this, you know, not there's the software version.
And then, and then, um, yeah, you know, and I, and I also see, I also see, uh, some people
are really worried, you know, sort of in the opposite direction.
It's like, okay, if these things become that, I mean, I get, I'm not even really, you know,
in the, in the, in that side of the field, but I get emails all the time.
You know, what am I going to do when the, you know, when the AI is so good at, uh,
you know, uh, doing all the things that it's going to do.
I mean, I, I, I think if we can't, uh, if you, if you can't do things just because
there's somebody else out there who's better than you at doing them, I don't, you know,
I kind of assume that everything I do, somebody else is better at it.
And maybe somewhere out in the world, out in the universe, there are aliens that are
way better than us at art and, you know, in science and whatever.
Like fine, does that mean we are now, you know, we can't, you know, we can't go on and do our
thing now. I'm not, I'm not bothered by it. Um, I think, I think that's fine.
I think we can use it to raise our game, uh, as much as, as much as possible, ultimately
significant, you know, very significantly. I think everything about us is ultimately
changeable. Um, I, you know, bring it on. I'm, I'm, I'm okay with it.
Yeah. So, so, so the expectation here is it's not just the AGI that's gonna, that's going to
give us trouble. There'll be alternative, um, or other biological, uh, uh, general intelligence
that will eventually prop up.
Yeah. I mean, already, so well, so, so two things. One is, uh, you know, when people say,
oh my God, we're going to make these, uh, in incredibly intelligent agents and release them
into the world. We already do that. It's called having kids. We already make like all the time,
but like, you know, right now, as we speak, somebody's making a really intelligent, uh,
you know, a future intelligence with minimal control over, over, it's, it's, uh, you know,
behavior, education upbringing, who the hell knows what it's going to do. Some of them do
amazing things. Some of them do horrible things. We already do this. So we already know how this,
how this kind of plays out with all kinds of, um, uh, uh, consequences. And, uh, I think that, uh,
uh, within, on the biological and just, just imagine, right? You got over here, you've got
a human being that's like 98% human, but there's a, you know, there's a chip in his brain helping
him control a wheelchair and maybe adding some IQ points and some stuff like that. Over here,
you got a Roomba vacuum cleaner and it's 98% robot, but yeah, he's got some human brain cells on
board culture to help him get around the room. Right? Okay. So, so 98 to 298, every possibility
in between is a viable being every, every, every combination, 60, 40, 50, 50, whatever. It's all,
it's all up for grabs. So you've got, you've got hybrids where you've got living brains driving
weird robotic bodies and new, um, augmented, um, prosthetics and new senses. If you want to have
a sense of, you know, the solar weather, you can do that. If you want to have a sense of the
stock market instead of, you know, smell, you can do that. Like the, all of these things exist. All
of these things already exist. So we're going to have these preachers and then people will have,
you know, there will be, there will be hybrid robotics and, and, and, and, you know, kind of
bioengineered beings that, uh, if you, if you, if you wanted to make, um, a mammal with a third
hemisphere, we can do that. We can, we can graft on the third hemisphere of the brain.
You know, no, no, no problem. We could make that today. So, um, all of these kind of creatures
are going to have, uh, novel bodies and novel embodied minds. And all of these old categories,
it's a, it's a human, it's a machine, it's a robot, it's a living organism, it's intelligent. No, it's
not. It's just, you know, it's just, like all of that stuff is going out the window. I mean,
it was never very good, but it's definitely not going to last the next couple of decades.
Hmm. And I don't think anyone's prepared for that.
Absolutely. They're not prepared for that. They're absolutely not prepared for that.
They're not even prepared. They're not even prepared to think about it. I, I have, I have
constantly, uh, arguments with people who are, who are still using these binary categories.
It's just a cell. It's only chemistry and physics, but I, I am a human, like, okay, let's,
let's follow you backwards and guess what you were just, you know, a few years and nine months
ago, you were a single cell, a little blob of chemistry and physics. So, so there's a smooth,
gradual continuum and there's no magic lightning bolt during any of that time period where somebody
says, boom, now you've gone from physics to mine. That doesn't exist. So you, so, so there's this
continuum and then I can stretch it, right? So, so I've got this slide where I show, okay, so there's
a human in the middle and up here, there's the evolutionary path and you used to be a, a microbe
up here and there's a developmental path and you used to be a unfertilized oversight down here
and then you could go sideways and you can do all kinds of biological modifications. This way
you can do all kinds of technological modifications. This way, all of this stuff is completely
continuous. There are no binary categories anywhere. So, so when people, uh, insist on thinking,
is it intelligent? Is it a machine? You know, it's like, um, it's, it's, I mean, these categories
are, are, are worthless now. Uh, we have to, we have to rejigger all of that. And then, and then
comes the hard part of, uh, making the institutions fit, right? It's sort of like the notion of an
adult, right? We have this notion of an adult. What's an adult? Does anything happen on your 18th
birthday to make you an adult? Nothing. We just, we just have this, this, this binary cutoff to
help, you know, to help the legal system figure out, you know, how are you going to get charged in
court? But that's it. There's no, there's no, you know, there's not really a sharp boundary there.
So, you know, um, I've talked to, I've talked to, uh, legal, uh, you know, kind of legal scholars
and so on about how we're going to figure this out and what, you know, what it means to, um,
to, to, to, to, to, to be a person. And these are not new. I mean, science fiction has been
dealing with this stuff for, you know, 150 years. So this is, these are not new. But I think, I think
now we're definitely getting to the point where we need to be figuring this out. And this, and this
AI stuff is just, just a tip of the iceberg on this. Okay. So yeah. So
yeah. So all these other things would likely, if they're coming from biological material,
would likely have some kind of agency greater than the, the kind of agency that we have for
computers and, uh, uh, so deep learning systems. So, so you're assuming a world that
all these things aren't, are, are autonomous and alive and, um, and, uh,
I guess, uh, have some sort of, um, quote unquote, free will to do whatever they want.
Yeah. I mean, that's a whole other, that's a whole other kettle of fish. Um,
you know, when you look down, I mean, again, the, the continuum is really helpful. Like,
you know, people say, uh, that doesn't have any free will. It's just a machine obeying physics.
Well, if you look at a paramecium, what do you see at a single cell of the organism? What do
you see? You see chemistry and physics. You don't see any magic glow. And so, so some people will
say, you know, so that something you can go to, people go in a couple of directions, right? So
some people say, okay, the paramecium has no free will or whatever. Uh, but I do. And so now
you get a real problem because you were a single cell organism once you, you were a single cell,
right? So, so where did it show up at what point? Nobody has a good story of where it shows up.
That's one way to go. That doesn't work. So, so then some people go the other direction and they
go, uh, okay, fine. The paramecium does have this, this magical, uh, whatever, because it's a living
being and then machines will never, well, if you look inside, right, if you actually look inside
a paramecium, what do you see? You see a bunch of little, little cogs and wheels and things that
grab onto each other and things that obey, you know, the various pieces of physics. And
that's about all you see in there. And there's nothing right now, we can't make one from scratch,
but come on, there's no reason why at some point, right? Usually in the field of active matter and
all of this, it can make things that do that. So, um, I really don't believe that, uh, these
binary categories are helping us. I think for all of these things, the question is,
what kind and how much, right? So, so when somebody says, um, uh, you know, is it intelligent,
is it, is it cognitive, whatever, I don't like the yes or no, I want to say where on the spectrum
is it, how much and what kind, what kind of problems, all the capacities, what, how big is
its cognitive light cone, right? That was the paper before the tape paper talked about, um,
this notion of the cognitive light cone, which is the, the spatiotemporal size of the biggest
goals you can pursue, right? So how big is your attempt, how big is your light cone in what
space is it? What, you know, is it, is it in metabolic space, is it in physiological,
in three-dimensional space? Like, where is it? Um, that's what you really need to know. The
binary categories don't really, you know, don't really, uh, don't really help you much.
But wouldn't, uh, like civilization, I mean, uh, the purpose of civilization is basically to,
uh, ensure, uh, humanity's survival. Wouldn't, wouldn't civilization self basically
legislate that humans would be prioritized over every other, uh, general intelligence?
Um, well, there's more of a legal thing. Yeah. Yeah. Well, the, I mean, right, the legal system
is going to go crazy. I mean, it already has a million problems because of, you know, uh,
the, the, the Twinkie defense and things like this, where if you, if you really follow through
neuroscience, you know, the question is, what, what, what, what does it mean that this person
could have done otherwise? Well, what exactly does that mean? You know, given, given, given
a kind of a material, uh, holistic view of, of the brain. So, so the, the legal system already
has issues, but the human, the issue of having, of prioritizing humans suggests, uh, that we have,
again, uh, this kind of binary category of what a human is. So, so I think to a primitive,
to a primitive, uh, early human, somebody like us, we've got, we've got some glasses,
we've got some hearing aids, we've got some, um, you know, some shoes, we got a toothbrush,
we got maybe an iPhone in our pocket to them. You're not a human. You're, you're a walking,
uh, you know, um, multi system, uh, some kind of a, like, like you're way beyond what a human is.
I don't know what, you know, you got all this other, other stuff. So, so just be, so, so that,
and in the future, plus also a brain implant that gives you direct access to, you know,
to a Google search and some infrared, you know, eyes at the back of your head, like,
so, so what, are you no longer human? So, so that, uh, you know, that's, you know,
of course going to be argued in court. Somebody's going to say, listen, I may have tentacles
and I may have a wheel or two, but what's wrong with me? I, you know, how come I'm not human?
I have the same, you know, so, so that, um, uh, and that, of course, also has been dealt with,
with science fiction a lot. Um, that brings up the question of, so, so what is an essential human?
Like what, what is it to be, to be a human, right? What, what do we want out of that?
So let's run down the list. So is it the genome? I don't really care about the genome per se.
Like that's, you know, some of these things, the thing to me is that like in, in pre, in
pre-scientific times, you could have held the view that we are a, a, the pinnacle of creation,
whatever we have in terms of our bodies, limitations, capabilities, our IQ, our lifespan,
our, um, capacity for, for, for, for, um, compassion, whatever it is, those limits were
set to us, uh, set by, by this like benevolent process and those are the best they're going to be.
Okay. So, so we're out of that garden of Eden now and now we realize that that's just where
evolution left us. That's all. There's nothing, there's nothing magical or, or optimal about
where we are. I don't believe, I think that, uh, evolution is this kind of like meandering,
uh, sort of search process and it happened to, to find that this particular form is good enough
to survive and leave a bunch of offspring. Well, that's great, but I don't see any reason we have
to stay that way, that it's arbitrary. And I, I like this, this notion of morphological freedom.
I think, I think each of us, uh, doesn't, doesn't, it owes no allegiance to this random process that
happened to have dumped you at this particular IQ level with this level of, uh, of, of, you know,
damage or birth defects or whatever, whatever you've got. So, uh, that means, do I care about the,
keeping the genome pure? No, I don't. Do I care about keeping the anatomy pure? We gave that
up when we started using canes and, and glasses and things like that. I don't really care about
that. There's nothing magic about this anatomy. I'm, I'm not, I'm not trying to keep that, um,
preserved. Uh, what I do think we have that's kind of fundamental is, uh, is, is in particular the
cognitive, a minimal cognitive light cone as far as compassion is concerned. So what I mean by that
is the moral ability to actively care about some degree of other, some, some, some quantity of
other beings, uh, well-being, right? And so that I think is what, that level is what makes us human.
Now going up beyond that, fantastic. Bring it on. Going down below that, I don't think that's good,
right? That's, that's, that's what I would argue against. So I would think that modify away, change
whatever you like to, to, to give yourself a better life to do, to fulfill your, um, uh, okay, you
know, fulfill your potential. Uh, just don't reduce your capacity for moral care. In fact,
you should increase it. And this is, this is an argument that we made with, um, a few colleagues
when we wrote that, that, that Buddhism paper. But, but the idea is so, so, so that's, you know,
that's what I'm interested in as far as humans. I'm not interested. I don't care what the genome is.
This is, all of this is completely arbitrary to me. Uh, uh, how, how our genetics ended up,
how our morphology ended up, you know, whatever, let's improve it. But, but, but, but that, that,
that cognitive light cone with respect to the compassion for others, that's gotta,
that should only increase. I don't think we should.
Okay. So your vision of the future of a civilization is that the citizens of that civilization would
have a minimum light cone, so to speak, compassion light tone. Anything below the minimum
compassion light cone disqualifies you from citizenry. So that's what we have. That's kind
of what we have now, right? So, so if you're, you know, if you're a dog or if you're, you know,
whatever, like you have certain protective rights, but you are not a full, uh, sort of member of,
of society, right? And it just so happens with, like, I think, I think one thing is that, uh,
we are, we are kind of special on this planet in the sense that it just so happens that there's
basically one dominant species. Didn't have to be that way. Imagine that there was another
dominant species that was like, I don't know, 40 IQ points lower. Like that would be a really tough
case, right? Because, you know, um, not low, not like, not low enough that you could just say
animals, but also not high enough that you want them running a nuclear power plant or flying
airplanes. What do you do? Like that would be really tough. We're just, we're just fortunate here
that there's such a gulf, right? So we could always say, oh, look, you know, there's this
category, but he didn't have to be that way. So I see, I see a future where we look like,
one of my favorite scenes is, um, the Star Wars cantina scene, you know, where, uh, where, right?
The with is like every kind of alien, every kind of robot that, you know, things are on wheels and
playing instruments, running around. This is, this is what the future looks like to me. I think that
as long as you've got the light cone to, to participate with a degree of responsibility
and compassion for others, you are part of the society. And then whether you've got wheels or,
or you decided to have tentacles or a propeller on your head, that's, you know, that's going to be,
I think those kind of people in the future, right? The future people are going to look back
on us and they're going to look at all of our wrangling about, uh, about gender and skin color
and prosthetics and, and all of this stuff. They're going to laugh at this. This is going to be
like hilarious. There's going to be such, such variety of embodiment at some point where, where
you can pretty much live in whatever body you want to live. You can have more IQ. You can have a
different kind of perceptual system. Um, this is all of our wrangling over this stuff and, and what's
a human and it's going to be laughable. And, uh, you know, that's, that's one reason, um, I really
like, uh, you know, the, the Star Wars, the Star Wars vision is very much like that where,
you know, the friends will all the droids and all this, the, the, in Star Trek, it's kind of,
kind of different at which I find just horrible. They, uh, it's whatever the year is supposed to
be 2,500 or something, 2,400. And they're still, they're still arguing about what commander data
status is. Yeah. This guy, you know, he serves, he serves on the enterprise. They're still arguing
about whether, you know, what his, what his deal is hundreds of years later. I think, I think
that's ridiculous. I think that, uh, you know, within a hundred years from now, maybe sooner,
all of this will seem, assuming we're still alive and we haven't like blown ourselves up. I think
all of this will be, will be hilarious to, to, to people of that time. But to have a civilization
that accepts that kind of diversity, you would have to have some sort of, um, guiding principle
that says, okay, it's not acceptable to have a, just single, uh, general intelligence, but
we're going to accept all these kinds of adversity. I think we're, I mean, I think we're
well on our way, right? I mean, the principle, if I had to boil this down, I would say,
I would say you want to relate ethically to someone no matter what they look like and where
they came from. Does that seem radical nowadays? I mean, it used to seem radical nowadays. That
doesn't sound so radical, right? And when I say where you came from, I mean, were you evolved,
were you engineered, there's some combination of, I think that principle would, would the,
would the young people of today do what you think they, you know, they don't find those two
things. They don't find that particularly weird. This is, I think, I think they just, I just think
people haven't, haven't fully figured out what it means yet, but society's already going that way.
You're not supposed to treat people worse because of what they look like or how they got here. We
already know that. Right. But the fear of AGI is that this other general intelligence, which
could become superintelligence very quickly, would be a threat to our own existence.
I mean, it's not, it's not impossible that we engineer something. I mean, we did it, we, we,
we put, you know, leaded gasoline and all these other horrible chemicals and a hole in the ozone
layer. We did all this stuff to all, you know, that could potentially kill us all off without,
without any agency in any of these things. You know, we, you know, we're all walking around with,
with high levels of lead in our bodies because of all the leaded gasoline. The lead doesn't have
much agency. It wasn't trying to kill us. We were just idiots. We did it to ourselves. And so,
you know, could we, could we engineer software agents and put them, put them in charge of
important things and have failure modes that we never anticipated and that screws us over?
I think it's possible. It isn't going to be because of the intelligence. It's going to be,
it's going to be because of our intelligence, not because of its intelligence. If we, you know, yeah.
