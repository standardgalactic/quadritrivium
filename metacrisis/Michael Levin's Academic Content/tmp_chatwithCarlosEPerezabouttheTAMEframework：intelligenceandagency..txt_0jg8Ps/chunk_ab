I mean, so, so, so, right.
So, so the things that, that I think are important is that it's a multi-scale system where at every
level, like I give this talk sometimes called why robots don't get cancer, right?
It's because, it's because our current technology, it only has agents, the intelligence or some degree
of agency at one level.
The parts are pretty dumb.
It's the, it's the whole robot that you hope has some intelligence biology.
Every, every layer is a problem solving, uh, kind of thing.
So, so that's, so that's useful.
The fact that it has to construct itself from scratch every time is useful.
The fact that, um, it's always, uh, metabolically on the edge of, uh, of starvation, basically
constantly has to find energy, which means that it has to, uh, be really good at doing
causal course grading on the environment or else it'll die.
Like all, all of these things.
So that's what makes it different from current devices, but I'm, I, I, I really believe that
it's not, there isn't a fundamental divide as in, you know, we will never be able to,
you know, engineers will never be able to duplicate what, I don't believe that's true,
actually.
I think today's technology doesn't do this well.
That's, that's true, but I don't see any reason why if we, um, learn, uh, the, the,
the lessons correctly from biology, and I'm sure there are others that, that we don't know about,
I don't see any reason why we couldn't engineer this way later.
And in fact, in fact, you know, I, I, I, I would, um, Jamie Davies and I wrote this thing
called engineering with agential materials.
Like, um, I don't think it's unreachable for us.
I, I don't think it's, it's magic.
I don't think evolution has a monopoly on this.
I think engineers could do this, but, but I do agree that today's technologies are not
this kind of system.
They're, they're different.
Yes.
I, yeah, I don't disagree with that.
I agree that if, um, if they train these systems with a particular curriculum such
that, um, it needs to essentially survive, then it will learn the heuristics to do that.
Yeah.
And then that would get you to a, an AI, right, an AGI with, um, with agency.
Yeah.
Yeah.
I, with stronger, and I mean, again, I don't, I don't like, um, binary categories on almost
anything.
So, you know, I, I, I, I think they have extremely low agency, but I, but I, I, and I
think we can, I think it's possible to make ones with more.
Um, I don't know that it's, uh, I, I,
I think that, uh, we have to be ready also for the fact that, I mean, you know, up until
now, all of these things tracked very well together.
So anything that spoke, you kind of knew that it went through the same.
It had the same existential struggle that you did.
It went through the same, you know, you, you kind of could make all these assumptions.
So now for the first time, we see some truly diverse intelligences.
We see some things that are problem solving, uh, agents that are not like us at all.
And, you know, uh, and, and we, we're now finding that you, in fact, you can dissociate
some of this stuff.
And I think it's really, a lot of people are finding it really hard to, um, get away from
this, this binary vision that either it's, it's, it's a dumb lookup table or it's just
like us.
Those are not the only options, right?
There's a diverse, you know, there's a, there's a huge space of possible minds that
with different failure modes and different abilities and some of them look like us and
some of them don't.
And, and, and it's easy to be fooled.
But I think the, the solution to all this is, you know, kind of a proper grounding in
the diverse intelligence field where you start to understand that, um, yeah, these are not
our only options.
There are many ways to, you know, to, to have a kind of mind and, um, we're going to have
to learn to relate to all these things in a useful and ethical manner.
Yeah, I think what I wanted to be, I guess the problem has been that the, the only
intelligence, the only general intelligence that we are familiar with are humans.
And people have built or cognitive psychologists have built their models based on humans that
have agency.
So they can't conceive of a general intelligence that is devoid of agency.
Yeah.
It's not in the current models.
Now, something that looks like appears to be generally intelligent, but it's completely
devoid of agency.
Yeah.
Yeah, I think, I think, uh, this, this, uh, one of the biggest issues is that all of our
kind of major sense organs are pointing outward in the three dimensional space and
we're very good at noticing medium sized bodies moving in media and at medium speeds
in space and saying that, oh, look, you know, here's a crow or, or, or an ape or an octopus
doing something clever and we can recognize some, some intelligence that way.
Imagine like, um, imagine if you had a primary sense of your blood chemistry, if you had
another sense, kind of like taste, but, but sort of, you know, more, um, but, but aiming
inwards into your blood and a little richer, kind of like almost like vision.
I think, uh, if we had that sense and we sort of grew up with it, we would have absolutely
no problems recognizing that we also live in a physiological state space and that our
kidneys and our liver are these amazingly intelligent agents that navigate that space
because we do, you know, we, we, we, um, uh, uh, challenge them with various stresses
throughout the day and they do all kinds of interesting things and they navigate this
space and solve problems for us and so on.
We just have a really hard time envisioning intelligence and other problem spaces.
Right.
But, but, uh, I mean, you know, humans are kind of general intelligences.
Yes.
But like we don't typically, we as humans don't typically solve problems in, um,
in some of these other spaces where, where, where individual cells, bacteria and other
things do very well.
So we, you know, I feel like, uh, this kind of human centered approach is, is really, uh,
blinding us to a lot of examples of intelligence out there.
And that's why people are so freaked out about suddenly being confronted by this,
you know, kind of linguistic intelligence.
I mean, yeah, uh, this, this, these, these radically different minds are, are all around
us all the time.
We're just very bad at noticing it.
So, um, yeah.
Yeah.
So what do you, do you make out of the argument that, uh, if we, uh, that if we continue to
go on this path of accelerating AGI that it's, um, it's an existential risk to humanity.
Does that make sense?
Wow.
Yeah.
I mean, so, well, I guess a couple of things.
Um, we have many things that, uh, could potentially pose an existential risk that are
not necessarily like us or a gentile or whatever.
There are many ways that we could kill ourselves off, right?
Um, I, I think it's not impossible that if we don't learn the lessons of diverse
intelligence, if we fail to understand how properly to relate to these things,
that we could end up, uh, really, really having problems.
I don't think that's impossible at the same time.
I, I don't think the solution is to, uh, try to stop research in some way.
I think it's impossible even, even if we, you know, if it was a good idea and I'm not,
I don't think it's a good idea.
Um, I think that if, if, if, if, if it does end up causing a major problem for us,
it's going to be because it's not going to be because of anything it does.
It's going to be because of our, uh, refusal to learn to, to relate to other kinds of minds
in a novel way, right?
All we know is how to relate to other humans.
Barely, we can sort of relate to animals, not very well, but, you know, kind of, um,
and that's it.
And we're really not willing to understand anything else.
And at that point, uh, yeah, I think, I think, I think if we don't learn that lesson,
we're going to have a major problem, but, but not even just because of the AGI's,
uh, the, the, um, uh, the kind of computer intelligence.
I really think because of all the biotechnology and, and advances in
biorebotics and things like this, we are going to be surrounded by, and certainly
our children will be surrounded by beings that don't resemble us at all.
Meaning, you know, cyborgs, hybrids, uh, chimeras of all kinds, uh, human, augmented
humans, uh, augmented, you know, biorebots.
There's going to be all kinds of stuff in our environment.
And if we don't, uh, regardless of the software AIs, if we don't understand that,
that, uh, we need, we need, uh, expanded ways of predicting the goals of new composite systems
that we haven't seen before.
Like that's an important science that we don't really have yet.
And ethically relating to other beings that don't share a path on the evolutionary tree
with us and have a completely radically different intelligence.
You know, if we don't wrap our, our minds around all of that, I'm pretty sure we're
going to have issues and it's not going to be just because of the software agents.
It's going to be because of our inflexibility with dealing with radically different beings.
Yes.
So the, the premise of this existential threat, I believe is it boils down to this idea that
we cannot imagine a different kind of intelligence other than ourselves.
And we look at ourselves and we can imagine that, um, people that humans could be, um,
essentially selfish and basically, uh, extinguish every other non, uh, other agent.
Right.
So, so it's related to having this, uh, limited, um, viewpoint or model of, um, other, uh,
intelligences, which is also the same problem that you're bringing up, which is if you don't
expand that, then we're going to have a problem.
Yeah.
Yeah.
Yeah.
Yeah.
I think, yeah.
I think, I think we have to, uh, I think we really need a, a, a, you know, an education in this,
in this diverse intelligence, uh, these ideas.
And, uh, this is going to be required because, because there's all kinds of stuff coming,
not just the software, this, you know, not there's the software version.
And then, and then, um, yeah, you know, and I, and I also see, I also see, uh, some people
are really worried, you know, sort of in the opposite direction.
It's like, okay, if these things become that, I mean, I get, I'm not even really, you know,
in the, in the, in that side of the field, but I get emails all the time.
You know, what am I going to do when the, you know, when the AI is so good at, uh,
you know, uh, doing all the things that it's going to do.
I mean, I, I, I think if we can't, uh, if you, if you can't do things just because
there's somebody else out there who's better than you at doing them, I don't, you know,
I kind of assume that everything I do, somebody else is better at it.
And maybe somewhere out in the world, out in the universe, there are aliens that are
way better than us at art and, you know, in science and whatever.
Like fine, does that mean we are now, you know, we can't, you know, we can't go on and do our
thing now. I'm not, I'm not bothered by it. Um, I think, I think that's fine.
I think we can use it to raise our game, uh, as much as, as much as possible, ultimately
significant, you know, very significantly. I think everything about us is ultimately
changeable. Um, I, you know, bring it on. I'm, I'm, I'm okay with it.
Yeah. So, so, so the expectation here is it's not just the AGI that's gonna, that's going to
give us trouble. There'll be alternative, um, or other biological, uh, uh, general intelligence
that will eventually prop up.
Yeah. I mean, already, so well, so, so two things. One is, uh, you know, when people say,
oh my God, we're going to make these, uh, in incredibly intelligent agents and release them
into the world. We already do that. It's called having kids. We already make like all the time,
but like, you know, right now, as we speak, somebody's making a really intelligent, uh,
you know, a future intelligence with minimal control over, over, it's, it's, uh, you know,
behavior, education upbringing, who the hell knows what it's going to do. Some of them do
amazing things. Some of them do horrible things. We already do this. So we already know how this,
how this kind of plays out with all kinds of, um, uh, uh, consequences. And, uh, I think that, uh,
uh, within, on the biological and just, just imagine, right? You got over here, you've got
a human being that's like 98% human, but there's a, you know, there's a chip in his brain helping
him control a wheelchair and maybe adding some IQ points and some stuff like that. Over here,
you got a Roomba vacuum cleaner and it's 98% robot, but yeah, he's got some human brain cells on
board culture to help him get around the room. Right? Okay. So, so 98 to 298, every possibility
in between is a viable being every, every, every combination, 60, 40, 50, 50, whatever. It's all,
it's all up for grabs. So you've got, you've got hybrids where you've got living brains driving
weird robotic bodies and new, um, augmented, um, prosthetics and new senses. If you want to have
