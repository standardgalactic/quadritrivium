Great. So what I was hoping, Carla, is that we could get your thoughts on how the whole
Active Inference Framework could be applied to something that we've been developing. And
I don't know if you had a chance to look at any of this stuff, but I'll just give you
a brief, you know, kind of a brief summary so that it's clear what we've got. And then
I have some basic questions and then a couple of wacky ideas to kind of bounce off of you
and see what you think. So the basic thing is this, that what I was trying to do is to
have a very basal model of distributed intelligence. And the idea was that we were interested in
unexpected competencies in places where unlike in biology, you know, in biology, no matter
how simple your model, you never have all the information about mechanisms and somebody
can always say, well, there is a mechanism for that, you just haven't found it yet, right?
So we wanted something that was incredibly simple, incredibly transparent, deterministic,
something that everybody thinks they know what it does. And then we can, we can apply
some of the approaches that we take in my lab about taking something that doesn't seem
cognitive and saying, okay, but what, what actual competencies might it have, right?
And so we chose this thing called sorting algorithms. And so these are the same simple
algorithms that all computer science students study for, you know, and they've been studied
for decades. And then we made a couple of, a couple of twists to it. One is that we visualize
their progress from being a jumbled set of digits to an ordered set of digits as a kind
of a traversal of space, right? So the idea is they start in different locations and they
all sooner or later, they all end up in one location where everything is. And so once,
once you view it as navigating that space, then you can ask some questions about what
are their competencies and navigating that space under, under odd perturbations. One
of the perturbations that we made was the introduction of what we call broken cells or
barriers in the space. So if the algorithm wants to swap two numbers in order to proceed
in its, in its sorting trajectory, well, one of the numbers could be broken, it doesn't
move, you can't, you can't move. And so, and we have two kinds of broken numbers, ones
that never initiate swaps and ones that actually never, never swapped no matter, you know,
who initiates them. And so that allows us to ask questions about things like delayed
gratification. In other words, can it go further from it? If it encounters a barrier,
can it go further away from its goal in order to then acquire gains afterwards? And this
is, you know, William James talked about this, of course, as, as an important type of basal
intelligence. And, and that, and that breaks a common assumption with these algorithms,
which is typically you assume that the material is, is robust. In other words, when you, when
an algorithm says to do something, it gets done. But, but in our case, not necessarily. And
we never introduced, this is important, we never introduced any extra code to check if
things got done. It's, it's the standard algorithm. So it just keeps on rolling. There is no
code to see how am I doing? Did things work out? No, no code for any of that. The second
thing we did was to break the, the, the general version of this is centralized. So there's
like this omniscient controller, and it's following one of several algorithms to kind
of move numbers around. And we got rid of that and instead made it all bottom up. So every
digit, aka every cell now has its own version of the algorithm running, and it has a limited
local view of who its neighbors are. And it's just following the steps of the algorithm to
try to improve its local environment, that there is no more global, global control. So
it's distributed. And, and, and, you know, we learned a few things. We learned that a, the
distributed version of this works quite well. It actually, you know, they do sort nicely. So,
so that's great. We did see some delayed gratification in the sense that if you, if you
sprinkle in some broken cells, it actually will go backwards and unsort the string a little bit,
and it's effort to then go around the defect as it were. So that's kind of cool. But the most
kind of surprising thing, which is what I'd love to get your take on is this. We, because now
it's distributed and every cell is following its own algorithm, that enables us to do an
experiment that otherwise you couldn't do, which is to make a chimeric string. And it's what we do
in developmental biology, when we put together, you know, axolotl cells and frog cells, and now
you get this frog a lot, and you can ask questions like, well, what shape is it going to have,
right? So, so what you can make is a chimeric string where some of the numbers are following one
algorithm, some of them are following a different algorithm. And again, there is this is important,
there is no code to determine either your own or your neighbor's algotype. And algotype is the
word that Adam coined for this, like, you know, what algorithm, what set of properties are you
actually following a policy? Is it actually following? So there is no code for any of that,
but we know which, which algotype all the cells. So that, and that also works. So chimeric strings
also sort, sort the, sort the arrays, and that's fine. What we then did was we asked basically a
developmental biology question was to say, okay, at any particular point during its journey,
what is the distribution of algotypes within the string? And what we know, and we defined
a quantity called clustering, which basically just means you look, you look next to you and what,
what's the probability that your neighbor next to you is the same algotype as you are. So what
happens is that in the very beginning, that probability is 50% because the algotypes are
randomly assigned to the digits. So 50%. That's our baseline. At the very end, it's also 50%
because at the end, everybody has to be sorted. And there is no relationship between the actual
sort order of the numbers and the algotype. So again, it's 50%. But the wild thing is that in
between those two points, if you actually plot that curve over time, it actually goes like this.
And in between, it's quite a bit higher in statistically very significantly higher than 50%.
And what we see is clustering significant tendency of cells with the same algotype to locate close
together. Eventually, the inevitable, the inevitable physics of the algorithm will yank
them apart and make sure that everybody's in numerical order. But until that happens, they
enjoy some amount of clustering with their, you know, with their conspecific, so to speak, until
then. And so, you know, any thoughts you might have, but more specifically, like one hypothesis
the one could make, even though there's no explicit mechanism for this, but it might be a,
you know, an emergent thing, could they be preferring to be next to their neighbors because
their neighbors be one of the same algotype or more predictable, right? It's less surprise,
you're less surprised when you're sitting next to somebody who's following exactly the same
policies as you are. So I'm curious what you think about that. And I'm curious if there, you know,
what might be a set of experiments that we could do to test that what's going on here is some sort
of implicit surprise minimization, even though there's no actual code for it. So I'll stop there
and listen to what you've got to say. Sorry, I think you're still muted.
I was saying that was a very succinct and clear, a nice summary. I reread the paper a couple of
days ago just to refresh myself, my memory for this conversation. So I didn't realize that that was
the, that final chimeric demonstration was sort of, you know, the most intriguing from your point of
view. But indeed, the way you express it, you know, that does call for further analysis, understanding
and numerical experiments. So overall, just to endorse the choice of the sorting algorithm as,
if you like, a minimal kind of self-organization. I think that, you know, that the self-organization
word needs to be centre stage in terms of, you know, what you're trying to understand here.
And framing like that, it does remind me a lot of self-organizing maps. I don't know if you remember
van der Molzberg's treatment and people, Peter Dion's supervisor, I've forgotten now.
So this notion of self-organizing maps as a very sort of biomimetic aspect of self-organization,
I think certainly puts sorting like algorithms centre stage in terms of biological self-organization,
particularly in things like the visual cortex and why you get that kind of pinwheel architecture,
for example, where receptive field properties tend to cluster together in a smooth way and you get
all sorts of interesting symmetry breaking when you're trying to represent, say, a 5D perceptual
space on a 2D manifold. So that does strike me having this sort of linear sorting algorithm.
And without really having not thought about it before, but certainly the dual pressure to find
a free energy minimizing solution, viewing free energy as an extensive quantity,
more simply, you know, the collective free energy being the joint free energy minimum solution.
You're asking us where the free energy just bounds the likelihood of this particular arrangement.
Then you're looking for the precise functional form of the free energy. And if you've got this kind
of a ponency between the similarity of the algorithm and the similarity of the content of the value,
then I can certainly see interesting behaviors arise in exactly the same spirit that you get
these interesting structures in not epithelia, but in the functional specialization of
cortical representations or sensory epithelia that try to sort of pack three dimensions into one
dimension or are accountable to two kinds of constraints. So again, without really thinking
about it, because I wasn't anticipating that particular question, but I think what you would be
the first thing that you would be looking for is basically what is the Lagrangian or the energy
function that is being minimized. So you could regard this as the sorting algorithm as an application
of the sorting algorithm as a process that is trying to minimize some energy function very
much in the spirit of Markov-Random fields, but in your instance, you just got a one dimensional
field. But the technology of Markov-Random fields I think would be apt to try to understand the
functional forms of the energy functions were under the special constraint, which of course is
the one that you're predicating this whole thesis on, that interactions are only local and therefore
any collective behavior has to be an emergent property, which is truly distributed. So the
definitive aspect of a Markov-Random field is you just have local, you just have local interactions.
And I think that's a really another important architectural feature that comes along with
the choice of the sorting algorithm, which you should foreground, because any distributed,
collective or emergent behavior at a scale beyond local interactions, that emergence
is truly emergent in the sense that all your interactions are local. And of course that is
what the Markov-Random field gives you. It says that you can only express the energy function,
which is the probability of getting this particular arrangement or these particular numbers
in this local clique. You can only express that in terms of a local energy function. And then
of course you can tell all sorts of stories about the importance of that for machine learning and
the like. But you probably want to stick to self-organization. So, and I would imagine the
energy function is now going to be some simple measure of the local differences of the local
gradients. And of course what one would anticipate would be a smoothing, a resolution of, as you say,
the differences. So I think that would be one way of approaching naturalizing this phenomena in
terms of maths by just invoking an arbitrary, not a variational free energy in the spirit of
the free energy principle, but just a Lagrangian or try to identify what is the generic free energy
function that's being minimized here. So that now your view through this sorting space or
your morphological space is now a progression on some wonderful landscape that is defined
by this free energy functional. And whether you can reverse engineer that or not, I don't think
it really matters other than because I think the nice aspect of that is then you can talk about
the dynamics on this free energy landscape. And once led then to very similar sort of notions
in computational chemistry and protein folding and the like, there's a very complex, sorry,
there is a complex Wellington landscape or free energy landscape that self-organization
and computational chemistry adheres to and can be understood in terms of free energy minimum.
Indeed, most of computational chemistry sort of follows this. And indeed, that is identifying
that landscape is the whole point of applying things like large language models or deep RL to
sort of protein folding and other applications. So that would be certainly one view to get a free
energy like formalism or naturalization of this behavior, which I repeat has lots of really
interesting links with self-organizing maps, marker, random fields, image reconstruction,
and self-organization in certainly in some things like the visual cortex, I don't imagine
any mapped representation would conform to these rules. To get this into a
to get it into a free energy principle story, I think you would have to commit to the notion that
each of the cell has its own boundary. And now you're starting to interpret each number
as a thing. And in so doing, acknowledge its openness to everything else, or in this instance,
just its neighbors, which will require sort of a by your formalism of the bi-directional exchange,
so that the value of my next door neighbor is something that I can sense and is, and likewise,
the broadcasting of my number to the next door neighbor is something is an action. So you've got
this openness that is mediated in the simplest way, which is just the broadcasting and sensing
of one unit dimensional, one number, if that is a discrete number. And the view like that,
that means you can then I think deploy the free energy principle in the sense that any
non equilibrium or far from equilibrium, steady state, which I think you would probably have here,
just in virtue of the fact that there is going to be some
breaking of detail balance in the itinerant way in which you move through this space in a
developmental to get to your steady state. One can imagine, well, perhaps not, but
you know, if one puts a little bit of dynamics into this, I would imagine you would advance very,
very clearly the breaking of detail balance, and you know, and have those kinds of solenoidal flows.
I mean, sorry, I distracted myself just by the addition of the frozen cells. That's one way of
breaking. In a sense, the detail balance, and you know, just open brackets. It's exactly the same
device that I resorted to in the very first paper on the life as we know it paper when
simulating the little macro molecules using Lorenz attractors that had inherent dynamics.
But to make it interesting, you had to have a small a certain popular number of the of the
synthetic macro molecules that were insensitive to influences from other macro molecules and
another proportion that could not influence the other one. So it's almost exactly the same
choice. And that's what gave it the interesting behavior. Otherwise, it just basically converged
either to a gas or it at a certain temperature, it would just convert to a crystal.
So both of them being steady state solutions, free energy minimizing solutions. But things got
interesting when you broke the detail balance symmetry breaking by having this
this, you know, this requisite variety in terms of the frozenness in terms of action or sensation.
So I think that's another that's another important thing to foreground. That this may be this
kind of requisite variety may be absolutely necessary for symmetry breaking. And in this
particular instance, breaking detail bounds to get this kind of cell biologically plausible
by mimetic kind of stuff organization, you're unlikely to get that if you're in the absence
of it in the sense that it would converge to a crystal in your instance, just a linear sorting
perfect sorting, which which is which is, you know, doesn't doesn't have that chimeric or itinerant
itinerant aspect to it. So sorry, close back it. So where were we? Oh, yeah. So
if you've got now an interesting system that has a non equilibrium steady state, and in your case
actually because you haven't got dynamics, it will also be an equilibrium steady state,
but it'll still be a free energy minimizing solution. Then you are perfectly entitled to
interpret the numbers as things and inferring things. And all they're trying to infer is the
cause of their sensations, which is just the value of the numbers on one side and the other side,
and they are broadcasting their inferences through broadcasting their own number, which of course
will be the average of well, when sorted, it will be the average of the neighboring numbers.
So on that view, I think you could very easily license an active inference interpretation
a teleology. You mean you would actually need this to simulate protein folding or self organizing
maps or anything, but you would be able to say there is a teleological interpretation of the
self organization using the rhetoric of inference and belief updating simply because we can treat
each number now as a Markov blanket and then something which will never be accessible, but we
can imply or induce internal to each number could be interpreted as an inference process,
and then the story, which you've already said what the answer is, under the assumption that
I live in a world that is maximally predictable, then everything around me is the same as me and
therefore I am going my free energy, my variational free energy minimizer is now going to be
bound when there's the least surprising input. And if I believe that everything is like me,
then that will be when the numbers that I am sensing in my peak are as similar to the estimate of
the number, the place that I should be coming back to our sort of no-deal place paper.
