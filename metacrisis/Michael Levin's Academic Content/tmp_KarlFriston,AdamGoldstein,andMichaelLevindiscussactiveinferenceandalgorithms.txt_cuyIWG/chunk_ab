And I think that kind of story will have to be nuanced for the same algorithm
so I'd have to think about that a little bit more, but certainly at least at a narrative level or
a conceptual level I think you can tell the same story there that if the sequence of moves that I
see my neighbour doing in relation to what I know about my neighbour belies the same underlying
dynamic or algorithmic computations, then in some sense they are predictable if I have exactly the
same algorithm under the hood. And therefore, mathematically speaking, that would be the
free energy minimising solution if I can now read my broadcasting of the number as a broadcasting
my posterior beliefs about the number, the estimate of this locale, my niche
in this instance is just labelled with one number. So the number that I have
is basically my prior belief about my niche and I'm just now going to
move my niche around in a sort of egocentric frame until it is consistent with my prior belief
that this is my place, my niche is number 62 for example. And that should be, you should be
able to reproduce the same kind of sorting either analytically through showing that with an appropriately
configured Lagrangian or free energy functional that the system operationally appears to be
minimising, you can now write down the generative model and then show that this can also be
interpreted as an inference process. I repeat under the assumption that the best way to make the
world predictable is to surround yourself with things like you, which and also of course the
locality assumption that I can only talk to the person to whom I'm immediately connected.
So those are some of my thoughts but a lot of those were invented on the fly in response to
your question I'm afraid. Superb. I've got many questions but Adam why don't you ask yours?
Yeah, so it strikes me that up until now we've talked about the relevant sort of agent as being
the individual number with an algoritite. You can think of it as a cell but it strikes me that
there's an interesting macro phenomenon that occurs in the process of sorting which is that it
appears that the list actually minimizes the Komogorov complexity or the description length
necessary to render it. So let's just say you've got an unsorted list with random distribution
of algotypes and there's 10 items in the list. You would need to enumerate 10 numbers and 10
algotypes and there's no reason a priori to think that that would be compressible in any way.
I mean maybe maybe you'd get lucky and there'd be a string of a certain number,
a string of a certain algoritite but in the general case I think you'd actually need to
write out every single entry. But as the list starts to sort itself it actually starts to create
these longer strings of algotypes which means that the minimum description length actually gets
shorter. Yet you still need to write each number out but you can coarse-grain the descriptions
of the algotypes. You can say the first five numbers have the same you know algotype and then
the next three have the same and so on. Now that that's a macro phenomenon but I'm wondering if
there's any evidence or any research that suggests that sort of these self-organizing systems
have a tendency to minimize their description length to minimize the number of factors needed
or something like that. Because if that's the case then it gives us another view where there's
this sort of emergent complexity minimization happening at the collective level. Yes, no that's
an excellent point. I think the simple answer is yes absolutely and I can sort of give you my take
on the literature or the citations that you'd want to appeal to. But I should say it's going to be
a nuanced yes because of the particular focus on the clustering of the algotype. Now the algotype
induces a certain kind of dynamics into the game so it's not as simple as a self-organizing map.
It's how the map actually self-organizes so there's a process under the hood and that I think makes
it slightly more complicated than just understanding self-organized maps. But in terms of another thing
you might want to look into here of course and you probably know more about this than I do but
this has a lot of resonance with artificial life
games in the 1990s and 1980s. It also could be if you wanted to so do
interestingly linked to Stephen Wolfram's Ruliad which is also another local scheme
that generates everything apparently. There's the same sort of notion so he has algorithms
which he calls rules and the rules are recursively applied in a local fashion to generate everything
including black holes apparently and quantum physics and everything. There might be an
interesting point of contact here with these sorry but to come back to the simple answer yes
absolutely so certainly from the point of view of self-organization as described by the free
energy principle. So notice here the free energy principle is just a description of systems that
self-organize to a far from equilibrium non-equilibrium as I said he stayed. It's not a recipe for sorry
in its statement it is not a description or a theological description of inferential
processing. You are licensed to equip your explanation of the self-organization with
reference to inference but that's an application of the free energy principle in itself it's just
a description of anything that self-organizes or any things that self-organize. So in that sense
if there is self-organization under the hood and the free energy principle has to apply and you can
motivate the free energy principle along two lines one would be the sort of playing the Feynman
card which is basically looking at the minimization of free energy as an optimization process
which can be viewed as a gradient descent on sub-fitness landscape or free energy landscape
or into landscape or you can take the Russian perspective which would be the Kalmolov complexity
and from the Kalmolov complexity you get to Solov induction and from that you get to universal
computation which is the home of the minimum description length and minimum message length
so it's the algorithmic complexity version of free energy and so David McKay wrote a quirky
little paper I think 1992 for where he interpreted variational free energy in relation to minimum
message length using crypto analysis as a vehicle to tell that story but to my mind I think wonderfully
connected to two different two different perspectives on exactly the same phenomenon
the ways of describing self-organizing systems that basically both entail a minimization of complexity
a simplification an emergence of order of a particular sort that entails
either compression hence the minimum description or the minimum message length
view from the algorithmic complexity in terms of sort of you know rate coding theorems rate
distortion theorems and the like or you can write it down in terms of continuous probability
distributions and sort of follow through from the Feynman's path integral I think they're both
saying the same thing you know the way I think of this is the you know the end point of any
self-organizing thing or set of things is just going to be the most likely configuration that
they occupy given the kind of things that they are and that basically means that you can always
describe this in a statistical and theological sense as everything providing an accurate prediction
of what its sense is that is minimally complex in exactly the same spirit as the the way that you
would frame complexity in terms of lossy or not losses but lossy compression or minimum
description length or minimum algorithmic complexity so I think that if you if you
if you tell the story that the free energy is an extensive quantity which means that all the
set of numbers or any subset of numbers any partition will all will all look as if they
are minimizing a free energy functional then you can I think say that you know one view of this
functional is to minimize the complexity of the arrangement which should be manifest in terms of
a minimization of algorithmic complexity and you can use it I can never I can never remember
Zemmell Lippf or Lippf what do you know what I'm talking about there's one of these
hierarchical sequential entropy measures
you know there's one way of quickly enumerating the the algorithmic complexity so I think the
if you could join the dots that would be a really powerful view of this and indeed you know
it would be interesting if you could
just for using numerical experiments join the dots
quantitatively in terms of this handcrafted intuitive free energy Lagrangian just based
upon you given three numbers you have to now write down an energy function that is always
going to be minimized by the sorting algorithm so the endpoint conforms shares the same minima of
your energy function it could be really simple it could be this the two differences squared
and added together something as simple as that and if you can prove that the the minima of this
is the same is the same as the the endpoint of your self-organization then you can say this is one
free energy functional that very much in the spirit of hopfield nets and harmony functions
you know in the early days of neural networks spin glass models pots models all of these
mark of random fields you have to write down this kind of energy function and then you just
simulate you know you can you can just do a gradient descent or rearrangement in order to
minimize that so that's one very simple kind of free energy description of it then you'd have
an inferential one under an assumed generative model so if you assume each number actually
has a little mind and a generative model it's trying to estimate or trying to act upon its world
to realize its beliefs about what it's sensing you'd have a variational free energy but then you'd
also have the the algorithmic free energy that you could that apply to any partition
and the point that if you can show that all three all three share the same minimum at the point of
attaining non-equilibrium steady state I think that will be really you know a really nice illustration
that all of these are different facets of exactly the same thing I mean you know it is just a
description of self-organization but articulated in slightly different ways but I repeat you know
once you've got different algorithms I think the process of sorting now is is somewhat constrained
so that it's you know because you've got three different ways of doing this they may have different
they may have different functionals that are being minimized and it may be but I'm not absolutely
sure that the the order matters and as soon as the order matters then you've got dynamics in play
once you've got dynamics in play that I think slightly complicates the simple
algorithmic complexity argument because the algorithmic complexity the universal computation
view it's not really fit for purpose to understand dynamics of organization and indeed most people
would argue it's not fit for purpose to do anything because it's intractable but it's a
beautiful mathematical object does that make sense yeah so one thing then for with Adam's point so
I think that's a really interesting point and it raises another question which is if on the
compression so it's on the compression issue so if we say that what you're trying to compress is
the actual list of numbers plus the ordering of the algotypes then you know everything as you guys
just said but I wonder couldn't somebody argue that in fact there is no list of algotypes to
compress there's only the numbers because it's sort of like you know by the time you get to the end
it's kind of like it's immaterial information it doesn't do you know it gets lost by the time
you've sorted the numbers what do you need the list of algotypes for right they're not really
I don't know I there's something here no I don't think so like if you take the position that the
algotypes aren't relevant once they stop being used then you're imposing as an observer an assumption
that the list is finished moving right but like how do you know that yeah yeah no that's that's
that's super interesting and and and it's like the bigger question of there is this notion of
algotypes that maybe you have to take into account what else do you have to take into account that
we don't know about right like that's that's one of the things that I see is so interesting about
this and then the next thing I was going to ask you Carl is what's the status of the fact that
like all the things that we were just talking about about the cells you know being objects and
exchanging information with their neighbors about algotypes and having predictions I mean
none of that is actually in the algorithm you can see that the algorithm is like six lines of
code like you can see what the algorithm is none of that is there so what's you know it's more
of a philosophical question you know what what's the status of of something and I have the same
question when I first heard about you know photons and least action and all that I was like but there's
no mechanism to know you know to calculate which path you know is going to be best for you so
what do we do with this what what do we do I'm super interested in the sort of I don't know
why they're almost almost you know implicit things that it's doing whereas the explicit algorithm
doesn't have any of that what do you think about that um well probably think the same thing that
you do um I think um yep in a sorry in a sense what I was saying about a nuanced answer once
you're dealing with you know isomorphisms between the local algorithms was exactly this issue that
you bring to the table it's not you know um yeah it could be as simple as each algorithm has a
different objective function different free energy or lipoonov function
or it could be that they have the same but the the actual sequence of updates or moves
is somehow constrained so the movement on the same free energy surface is is it is somehow
constrained to be different so I'd have to know it precisely what the algorithms are
um it probably is the case that I'm just guessing um that they probably don't have
quite the same um objective function or or and often point of view the free energy principle
implicit generative model um so the chimerical um um self-organization is a reflection of the fact
that um not everything is trying to has the same generative model and therefore by definition
will not have the same free energy um functional so that that does complicate the situation and
makes it more interesting in fact um you know from the point of view of this this kind of
requisite variety um but now I've forgotten the your your actual question um which I did have
an answer to what can you remind me what the actual question was sure sure it's so so what
these algorithms have in common with some of the things that you and and chris fields have said
about particles and things and other people apart which is different from what happens in
biology right if if if in biology I said look this cell is exchanging information with that
cell and it's making decisions the next question is excellent what's the mechanism right like what
it like show me show me the the the explicit uh the set of steps by which this cell does that
but but here we don't have that and and presumably you know when we get down to particles and things
we don't have that either so what's what what's the status of these all these amazing things that
they're doing without a a mechanism to explicitly do it right and I'm going to give you an answer which
comes from conversations with philosophers of maths people like max or ramsted that that question
technically would be answered by appeal to what is a mechanics so a mechanics is um for example
the basic mechanics of the theology principle or Lagrangian or classical mechanics under certain
dynamics you know the non-dissipative or conservative so or quantum mechanics where
you can't uh you you you you have to focus exclusively on the on the dissipative dynamics
so the mechanics is a description of the realization of something
where the thing usually conforms to a principle of least action so this is where this is a sort
of deflationary answer to your question that the mechanics in and of itself is an emergent
property of a variational principle of least action that can be cast in gage theoretic terms or
or in terms of things like maximum entropy principles so there are principles
that just describe the shape the spacetime shape of our world these give rise to and usually you
can usually reduce all physics principles to principles of least action the the straight line
the path of least effort um um and once you've written you've written down your principle as a
principle of least action then the particular functional form of the system to which that
principle applies then gives you a mechanics and then that now um acquires a teleology in
conversation but only in conversation you don't need the mechanics mechanics does not engineer
anything it is just an expression of the principle of least action um so very much in the spirit of
basic mechanics are saying before the free energy principle is just a description of things that
self-organize you may or may not want to then go and say oh well this self-organization could be
described teologically as self-evidencing or active inference or decision-making or basal
cognition or distributed intelligence you don't have to do that but it sometimes it can be very
useful when talking to somebody else about it to to to teologically frame it like that and that I
think is your mission I think that's what you're bringing to the table in in the widest sense
you're saying that the mechanics of biotics self-organization have a certain tealogy which is
almost isomorphic to the same tealogy of finding psychiatry or immunotherapy or climate change
and we just got to find the cross-cutting themes so the mechanics the mechanisms
are really just tealogical unpacking of of of the mechanics so in this instance I gave you an
example before that simply the algorithm to implement the algorithm which is probably a
series of Boolean operators would need certain inputs they need arguments and they need certain
outputs those are the the sensory and active states those those define now the Markov blanket
then so you're taking the input is basically whatever my neighbor neighbor's value
