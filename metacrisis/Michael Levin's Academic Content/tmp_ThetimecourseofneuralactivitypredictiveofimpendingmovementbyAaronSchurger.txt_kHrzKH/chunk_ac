with respect to the leading edge of the window
could leak back in to the window.
So really, the classifier is telling you,
this is up until now, this is what I can tell you.
This is how I can predict between manual and automatic.
For those of you who aren't familiar with Ataboost,
the way Ataboost works in a nutshell
is to take a usually a quite large number of very weak,
simple classifiers, and learn how to combine their output,
or let's call it their opinions, in just the right way
so as to build a very powerful classifier
from the individual opinions of a large number of very weak
classifiers.
That's the essence of boosting.
What we used as features are these HAR wavelets.
Look like this.
And there's one that's missing here, which is just a flat line,
which is just basically an offset.
And that one does actually become important.
So we have basically three kinds.
We have that offset.
We have a monophasic and a biphasic one.
And of course, this represents the window
that we're looking at, the width of the sliding window.
And what Ataboost does is it will just
try every single possible version of each of these kinds
of wavelets, where you can vary the width and position
of this phase, or this square wave.
You can vary every possible combination of the width
or separation of these two phases.
So Ataboost will try all of those.
You take the dot product of this with your signal,
and then you apply a threshold.
And depending on if it's above or below,
you declare that exemplar to be either automatic or manual.
And that's one week classifier.
Because there are so many possible configurations of this,
you end up with many millions, actually, of week classifier.
Because you can apply each one of these
to the data from each sensor and every pairwise combination
or difference, every pairwise difference
between two sensors.
This is what some of these might look like.
So if you apply this monophasic wavelet,
then you get the area under here.
And if you apply this biphasic one,
you're ending up basically what you're getting
is a temporal difference.
And the one I'm not showing is the offset, just the flat line.
And that just gives you the bias, the mean in that time window.
Here's an example feature.
So let's say we're looking at the pairwise difference
between those two sensors.
We apply this, take the dot product of this wavelet
against that within the window.
And then you have a threshold on the output of that,
which is a scalar value.
So in general here, one week classifier is feature plus
threshold.
So these are called decision stumps.
So typically in boosting, you use decision trees.
That's very common.
We're not building a tree.
We're just using one rule.
And then of course, this entire process
is repeated separately at each position of the sliding window
and for every possible combination
or every possible version of the three different kinds
of our wavelet.
And of course, trying this at all different positions,
for example, this particular biphasic wavelet,
trying it at all possible positions within the window,
that just amounts to roughly a convolution operation.
And if you're clever, you can code it right
so that even though you have many millions of features
to try, it can be quite fast to run.
So if we look, these are in the data
from the first three subjects from whom
I got a large quantity of trials.
And this is what the readiness potential looks like
if you just take that time locked average.
And here it's a little more sophisticated
than just recording from a particular sensor.
What I've done is just built a spatial filter
on the temporal difference between what's happening early
and what's happening close to the movement.
Here's what the spatial filter looks like.
And if you project the data onto that filter,
you get this readiness potential.
And this is just to show we get this highly replicable result.
We get this nice readiness potential that stretches back,
arguably even almost up to two seconds prior to the onset
of movement.
So the question is, does this, let's say this point here,
two seconds before the movement, is that
reflected decision to move?
If that does reflect the decision,
and this is certainly we can resolve this quite well
if you go, say, just a second in advance,
you're really far from the noise floor.
So if this reflects a decision, then it should be predictive.
And that's the essence of the question we're asking.
If we do out of boost now in this sliding window,
can we predict?
We should be able to predict, like, a second in advance,
a half a second in advance, and so on.
At least that's what the readiness potential would suggest.
But when we actually do this, and so here's an example of,
so in white here, you have the sliding window.
Let me turn off this laser pointer.
So this white bit here is the sliding window.
So what Atta Boost is seeing in this little,
this is a video animation.
What Atta Boost is actually seeing
is just what's inside this white window here.
And these are just two example features,
an average feature that is negative for movement,
and then in blue here, an average feature
that's positive for movement.
So as I animate this, just pay attention to the ROC curve here,
and you can see how that evolves as the sliding window approaches
time zero here, which is when the movement actually happens.
So here we go.
So I'll pay that again, and you can see it again.
And what you might notice is that as the sliding window is
sliding over the early part of the readiness potential,
you don't seem to be getting any purchase until the sliding
window actually gets to about 150 or so milliseconds before zero.
Let's look at it again.
There's really nothing happening until about now.
And so if we look at the averages,
we look at the results of doing this over.
These were these first three subjects.
You see that Atta Boost is not doing such a great job of telling
them apart far before zero.
But at the roughly the time of movement initiation,
or maybe 100 or 150 milliseconds before,
then it starts to be able to do so.
And just at and after the onset of movement,
it can do it extremely well.
And these two, by the way, I believe the red line is the MEG
data, the blue is the EEG data.
So we've since run a number of more subjects.
Here's 10 more subjects that we've run recently.
This is just EEG data.
But you can see once again, it's not really different
from chance until you get about 150 milliseconds
before movement onset.
And right there and thereafter, it goes up quite high.
So it's not as if the machine learning algorithm isn't
able to perform well on these data right after movement onset.
It performs exceedingly well, upwards of 90% correct.
And you can see even the individual traces here.
And by the way, here in this case,
we're comparing in red a growing window
versus a sliding window.
And performance is about the same.
We expected that, but it was just a sanity check.
So I mentioned before that there is this trick you could use.
If you only had data that was aligned to a movement,
you didn't have any control data.
You can, and this is what others have done in the past.
You can say, well, I'll go way back in time
and call this little window a baseline.
And then I'll look at all these other epochs
of the same width, but getting closer and closer
to the onset of movement and just keep comparing those back
to this baseline.
And asking, for example, with machine learning,
can I tell them apart?
And now, conceptually now, hopefully,
you agree that that's biased.
And it's giving you an unnaturally
optimistic view of how well you can classify or it should.
So what we predicted, and when we did the analysis in this way
and here it's shown in red.
So when we only looked at the movement epochs
and did the machine learning comparing each position
of the window to a remote baseline,
then we end up on average this red curve.
So it looks as if we can classify very, very well,
even very early on.
But we're arguing that that's a false impression,
that that's cheating, so to speak.
And the real difference or the real ability to classify
is given by the blue line, which is the control method, which
is where we compare actual movement plus slide transition
epochs with just the slide transition epochs,
so the manual versus the automatic trials.
Again, this is with a more recent round of 10 subjects
that we recorded using EEG.
And here are the individual traces from those 10 subjects.
So you can see it holds up pretty well for each one.
And we're doing another round of 10, I think we're almost done.
But I have little doubt that we'll
have the same pattern of results for these other 10 subjects.
So the evidence do seem to support this late decision
account more so than an early decision account.
We've been able to show that probably among the reasons
that people thought this was an early decision was just
