It's stochastic, at least, from our relatively uninformed
point of view.
Maybe if we knew more, we could call it something else.
But it seems stochastic.
And noise in neural populations, even
at the single neuron level and at higher levels,
like in the residuals around reaction times
or other behavioral phenomena, even if they are noisy,
the kind of noise, it's not like white noise.
So it's auto-correlated.
It's one of ref noise.
And that kind of noise is primarily
driven by lower frequency fluctuations.
And there's ever less energy in the higher and higher
frequencies.
And so the idea was simple.
It was just, well, what if in a context
where you don't have a very strong imperative
to move it at a particular time, then in that context,
maybe this ongoing ebb and flow of activity in the motor
system actually starts to play a bit of a role
in, at the very least, determining
the precise moment at which a movement is initiated
or triggered.
So if there is a threshold in the system that
leads to triggering the movement, which I have illustrated
here is this red line, you might have an inclination
to move sometime soon, but the precise moment isn't really
fixed, then the noise might help to determine
the precise moment at which the threshold is crossed.
And then if I time lock to that threshold,
it's kind of a no-brainer.
I'm going to see the profile, you might say,
of that one of ref noise.
In fact, it turns out, if you look at this,
carefully, it actually gives you an estimate
of the autocorrelation function if you time lock
to these crossings and average over many of them.
So this led to what I call the stochastic decision model.
The premise is just that when the imperative to produce
a movement is weak, then the precise moment
at which the decision threshold is crossed
is largely determined by spontaneous sub-threshold
fluctuations.
Neural activity and those fluctuations
get caught in the event locked average,
looking like a gradual buildup.
And I modeled this using a simple leaky stochastic
accumulator, which is just basically a drift diffusion
model, but with this leaky term.
And the leak actually makes it more biologically plausible.
And in this case, in the case of my own work,
it ended up being necessary to adequately model
this or to fit the data.
So this is work that we published in back in 2012
that sort of upended the going interpretation
of the readiness potential, and hence
the interpretation of Libit studies and so on.
We could argue then, here we give
offered with this new perspective,
that this slow buildup that we see
maybe is accounted for by spontaneous fluctuations.
And the actual neural decision or the commitment
to initiate a movement doesn't really
happen until quite late in the game.
And we've called this a late decision account contrasted
with Libit's account, which is an early decision account,
saying that the decision happens at the readiness
potential, that's an early decision account.
And here, we're saying that the actual decision
or the commitment to move happens late.
So we distinguish between late and early decision accounts.
So one of the reasons that this even came up
and is even a problem is because of the way
that people do research on spontaneous voluntary movement.
You need epochs, you need to average,
because the data are quite noisy,
even single unit data are pretty noisy.
So you need to average, and so what do you do?
You have to find some events, and then you time lock
to those events, you extract epochs,
and then you average them together.
And of course, the events that you time lock to,
here are these blue arrows, are the movements.
That's when you initiate a movement.
So you have this sample of data epochs
that basically all of them terminate in a movement.
Well, that's a very biased sample.
It's inherently biased because of the nature of the question
and the nature of the way you do the data analysis.
But if you think about it, it's a bit like if you're asking,
well, what's predictive of movement, which is essentially
the question, it's like trying to learn to understand
and predict the onset of rainfall
by looking at just a sample of rainy days.
So that'd be a very biased sample.
You don't know what happens and the degree to which whatever
you see in the data is happening even when it doesn't rain.
So the caveat is that sampling the conditions that are active
when a movement is not about to happen
is integral to being able to reliably predict
and understand what's going on when movement is going to happen.
But this is the way that this kind of research has gone on
for since its inception.
We always just look at epochs that are time-locked
to the movement because that's the event, the only event we
have in the data to time-lock to.
Ideally, ideally, you'd want this.
You'd want some epochs without movement,
epochs with movement, compare, see what's going on.
But we never have that.
One thing that people have done to get around this
is you have just these epochs with movement.
But what you do is you take some time window that's quite early
and you call that the baseline.
And then you can maybe look at other time windows closer
and closer to movement and compare those.
But that's also very biased because the baseline
has a fixed and reliable temporal relationship
with the onset of movement.
So that's problematic as well.
We'll see.
I'll show you that, some of what we've done.
But so this work really started with the desire
to do this, to have these somehow have epochs without movement,
have a proper control condition with the prediction
that if this late decision account is correct,
then distinguishing between these
should be challenging, if not impossible,
well before the decision, the putative decision.
So again, this is work I initially
started with Rob Shapiri and Mehmet.
And then Luca came on board more recently.
We tried to map the time course of neural activity leading up
to a spontaneous voluntary movement using boosting,
using out-of-boost in this case.
I collected some MEG data, simultaneous MEGE data
from subjects.
This is one I was working at Narrow Spin, just near Paris,
France, with the Stan DeHans group.
And I had, I mean, one of the features of out-of-boost
is that it works quite well and provably
so, assuming sufficient amount of data.
So I had a few subjects, a small number of subjects,
but I had them come back for multiple sessions.
So I had a large number of data epochs
for each of a few subjects.
We later added more subjects with fewer sessions,
but that's how we started.
And the paradigm that we developed is quite simple.
What you do is, as a subject, you're
looking at these photographs of nature scenes,
south door scenes, or plants and stuff.
Nothing that has any strong valence to it or anything,
just pretty pictures to keep you engaged.
Except there are two kinds of viewing,
let's call them trials, because what you're really doing
in this task, it's just a slideshow for you to enjoy.
That's it.
It's just that you may or may not
be in charge of when the slide advances to the next one.
So before each slide, you get a Q word,
either manual or automatic.
If you see the Q manual, then that means,
and there's a fixation point on the image.
Because with EEG and MEG, eye movements
introduce big artifacts, so we have a fixation point.
On a manual trial, you're instructed
to just look at the image and enjoy it for a few seconds,
and then when you're ready, you make a movement.
You normally just press a button,
and then that advances to the next slide.
On these automatic trials, the instruction
is just to relax, and the slide will advance by itself
after a few seconds or after a little while.
And crucially, the viewing time on these automatic trials
is drawn from the subject's own viewing time distribution
on their manual trials, so that by the end of the experiment,
these two kinds of epochs are relatively well matched
for how long you were waiting for the slide transition.
So this gives us our matched control condition.
We have one set of data where we align the data now
to the slide transition, and in one case,
that slide transition is accompanied by a movement,
because the movement was what triggered the slide transition,
and then in this other set of data, there is no movement.
So you have movement, no movement.
And then what we do is just build a classifier using
Ataboost that, in a sliding window,
is trying to tell these two apart at each position
of the sliding window over the time period of an epoch that
goes back about three seconds.
And we align everything to the leading edge of the window,
because we want to know what the classifier had access
to before this point in time.
And we are very careful not only to align the leading edge,
but any kind of filtering that we did or anything involving
wavelets or anything like that made sure that it was like,
if we used a higher low pass filter,
we made sure it was causal so that no information in the future
