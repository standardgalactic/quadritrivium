state modifies the energy function and the change in the energy function modifies the state
trajectories. And it causes the state trajectories to find exceptionally low energy configurations,
even in the original constraints, even in the original springs, those are configurations
which are exceptionally low energy. So I understand that to be
a mechanism of adaptation, there's a whole other story about that isn't natural selection,
that's a different story. But in order to make it work, it needs to be, I noticed a couple of
things. One is that the way in which the springs push the masses and the way in which the masses
deform the springs needs to be reversible. If I was building, if this dynamical system was like
a regular neural network, which has a, you know, fires if the weighted sum of inputs over a
threshold, then that's not reversible. I can't put a current back down the axon and get it to
change the weights. The backprop does that, but I can't literally push on it. It's a one-way system.
So the natural induction process is kind of nice because it's a purely physical system,
it doesn't need to be designed or selected for the purpose of doing adaptation,
but it only works in physical systems that have that reversibility.
The other thing that I notice about it is that there's a shift between the configuration space in
which the states move and the configuration space in which the weights move, which is the one of the,
one is the correlations of the other. The one is, the state space is the straightforward state
configuration space, but the changes in the weights change the correlations of the other one.
And if you were to have, you know, if it's worth doing at all, it's worth doing recursively,
then the next level would be the sort of higher order correlations between those correlations
would be controlled with another kind of spring that connects two springs together.
And if that was all reversible, then you ought to be able to get it to do a deep learning process
instead of just an associative learning process. And that all feels to me like the implicate order
of bone. Shall I get your reflection on that first before I add more layers of thought onto it?
I mean, I think what Bohm was trying to talk about is entanglement
and the difference between quantum and classical information.
And what you're talking about here is, one could think of as kind of the
the first two terms in a series expansion, where if you take the whole expansion, you have entanglement.
Yeah. Yeah. Okay. That sounds like that is resonating with you a little bit.
Yeah. I mean, there's a nice long ago paper by Frank Tipler. He's kind of an odd character.
But what he shows is that if you take standard Laplacian classical mechanics
and you remove all the singularities that exist in classical mechanics because everything is a
hard boundary. And so you have collisions and they're instantaneous and so you have singularities
in the momentum. If you remove all those singularities by smoothing them out, then you get Bohmian
quantum mechanics. And the reason is that smoothing out the singularities is the same thing as
connecting everything to everything else. Yeah. That makes total sense to me, actually.
And I thought that was a really nice demonstration. It's a paper in PNAS from 2014
or something like that. I could dig it up and send it to you if you're interested.
Yeah, I'll ping you if I can't find it. Yeah. And the other part of the paper is a vigorous,
defensive Laplacian, which turns out to be the same as Bayesian theory of probability.
Right. So I've been thinking about it this way, right? In a deep neural network with a
feedforward architecture, you are getting one layer to provide inputs that controls the next
layer. Each layer is multiple inputs feeding in. It's losing information. It's coarse-graining
stuff as you go up. And if it does it in a raw or nothing, plus one minus one way, a discrete way,
then it's not reversible. And you can't push, if the output wasn't what you wanted, you can't push
back on it to change, to know how to change the weights or to know who was responsible for the
error. And the reason that we use a sigmoidal function is so that you can differentiate it,
so that you can get rid of those singularities you were just mentioning, so that you can push,
you can, given the error, you can use the error to push back on the weights and do the credit
assignment of who was responsible for this error and thus change the weights to form the weights
to give you a slightly better answer. And that, that reversibility is necessary for you to do
learning, right? Learning is I had an error in the coarse-grained low-dimensional space
that I didn't like. And I want to know how should I, how can I reverse engineer all of the changes
that I need to make to the internal organization of my machine in order to change that error?
And if everything is reversible, well, then you can just push on it and it gives you the
changes that you want, it just bends it to give you the changes that you want. But if everything
is reversible, then you can't do the nonlinearly separable deep functions that you actually want
to compute, right? You actually, the purpose of having a deep network with nonlinearities in it
is so that you can fold the feature space and fold it again and fold it again and fold it again,
so that you can have complex decision boundaries in, in the space. And although each layer was
a little bit reversible because you didn't use a step function you used a sigmoid,
it nonetheless becomes more and more difficult to reverse that function as the function becomes
more and more folded, that it becomes so massively undetermined that, you know, you have to push it
by infinitesimally tiny little bits. If you want to change a fold that you've made that's quite deep,
that's hard to do, right? So in deep learning, the way that that manifests is after I've trained
the network on a particular function, I'm basically fucked if I wanted to do something else, right?
I can't change the deep structure in that network without undoing all of the higher level folds that
I made, so that I can see that deep structure, push it to do something else and then put all
those folds back again. I can't change the deep correlations, the really, really low frequency
stuff without undoing all of the high frequency stuff that I put on top. Now, if you, my hypothesis is
that if we did a neural network that was built out of oscillators with periodic activation
functions instead of sigmoids, and we did the computation in phase space instead of amplitude
space, then it could do all of the same kind of computation. But the thing that would be different
is like, why would you want to do that? If it's equivalent, why bother doing the conversion?
The reason is because you could change the deep correlations in the network without having to undo
all of the high frequency correlations in the network, because you could just hum to it at the
low frequency, and that would turn the low frequency or the base of the orbit around in that low
frequency space without having to undo. You could drill through the higher frequency folds
and leave them untouched to communicate directly with the low dimensional folds.
So, you would have to know what frequency your error is occurring at,
and then you would have to
know how to correct for the side effects of changing the low frequency slightly
in terms of the resonance or off resonance behavior of all of the higher frequency components.
Yes. So, I'm not sure you get away with leaving all the higher frequency components fixed,
even in this kind of Fourier A representation. Yeah, my feeling is that the high frequency
ones are just as happy locked in a different phase as they were in the original phase.
Might change the semantics though.
Well, you know, up might be down, but they would be a reflection
that still had the same internal relationships. It's a left hand instead of a right hand,
but it's still a hand. And the way that you wouldn't, how would you know which frequency
the error was at, right? It's like, well, you don't because the way in which you're interacting
with the network when it's a low frequency thing that needs changing is because that was a causal
process that was happening at the low frequency. So, the causal process can happen between,
you know, when one organism interacts with another, for example, the kinds of interactions that they
have could be a really low frequency thing or a really high frequency thing, like they could
be having a conversation or they could be physically bumping into each other in the street.
And the response that you get is a change in the internal organization of the system either way.
But one could be changing something deep without, you know, I'm changing the ideas inside your head
without breaking your skull. And the other one breaks your skull without changing the ideas
inside your head, right? So, those causal processes are happening can happen between
an organism in its environment or another agent. If the environment is communicating with it,
enacting causal processes at that scale.
I mean, some living things facilitate that by providing an interface for which is learning,
you know, rewards and punishments and, you know, perception, all that for letting others
modify them in a particular way without having to get in there and literally change their,
their neuronal contents, right? I mean, who was it that I forget who it was, but somebody,
somebody gave a talk on how to give a talk. And the first thing he said is he said,
effective communication is a violent act, because fundamentally your goal, if you're good at it,
your goal is to reach in there and make sure the people that leave the, you know, leave in the
audience are not the same way they came in. But you need to, you know, they have to facilitate that
by being the kind of, by having that interface and being the kind of thing.
I repeated that to Eva and she said, why does it have to be violent?
Wasn't my, well, I mean, yeah, I don't know, maybe violence, not the best word, but there's
an element of this in there, because by coming to a lecture, you are taking a risk. You don't know
what you're going to leave there with, right? You might hear something, you can't unhear things,
you know, you might hear or you might see a piece of art or read something. I, you know,
I've certainly had that experience like, wow, I wish I hadn't read that. But, you know, but it's
too late. You once you've seen it, you can't, you can't unsee things. Yeah, I think that's right.
Your any, you know, any communicative act is, is
exerting forces on you, which transform you. Yeah.
I've often said that one frequency or another.
Park therapy and drug therapy are not all that different.
Yeah. Yeah. Well, we both do basically the same thing.
Yeah, Chris, I don't know if you were at the, at the, at that lab meeting we had, but
Fabrizio Benedetti, who gave that, that talk on the, on the placebo effects and everything he had
on the, on the, on the slide that said drugs and words have the same mechanism of action.
You know, he gave an hour, an hour lecture and that was amazing. I was, I was absolutely amazing.
Yeah, I, I didn't see that. That's very cool. It was, it was fantastic. He didn't want us to
record it. So I don't have, I don't have a recording of it, but it was, it was really good.
