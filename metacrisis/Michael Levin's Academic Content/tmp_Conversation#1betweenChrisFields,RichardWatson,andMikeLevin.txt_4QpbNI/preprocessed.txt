Well, excellent. Excellent. All right. Have you guys met each other before?
No, we have not. Oh, you gave a talk once to Mike's group. That's the only contact we've had.
Fantastic. Yeah. So maybe take a couple minutes and introduce yourselves to each other. It's great.
But we will see each other, I think, at UCLA at that meeting.
Oh, great. Oh, good. You're going to be there in person.
Right. Right. Excellent.
You know, I started writing some notes about what do I want to talk to Chris Fields about,
and it went on for a few pages, and I thought, okay, well, let's just write the,
you know, like the top level vision, and there was still a page and a half.
I am trying to, I'd like to develop a theory that links together cognition, evolution, adaptation,
development, and computation, just those. And in particular, I would like to do it in a way that
explains agential behavior, and how mechanistic processes at a given level of organization can be
more or less autonomous at that scale, but how that interacts with the scales above and below.
And I've been quite excited recently about how that links with resonance as a form of error
correction. And as a mechanism, the links together, all of those things that I'm interested in.
And it's connection to error correcting codes is one of the topics that I think would be
a good place to start. That's, they are off the top of my head. That was that eight pages I tried to
summarize. Well, we should make t-shirts for everyone who's interested in that set of topics.
Not sure what we should call the group.
But maybe Chris, say a little bit about, you know, your interests and what you've been,
what we've been working on, that kind of stuff. Well, we've been interested in
this intersection area between physics and cognitive science and biology.
And in how to
develop theoretical frameworks that are based very firmly in physics and that connect in a very
complex way to computer science. And that provided us formulation of what's going on in biology.
But complementary to that, we're interested in how to import biological and cognitive thinking
into physics in a kind of whole scale way. So one way to put all of that is to
see physics as fundamentally a theory of communication between agents who have some
computational structure and who have to exist within an environment
that exerts selective pressures of one kind or other. And of course, the environment in this
case is an agent also that has its own agenda. And the free energy principle provides a nice
way of formulating all of this, especially when it's translated into more quantum information
language, which makes a lot of its structure a little bit clearer than it is in the classical language.
And I suppose other aspects that the physics helps to clarify is
what context dependence is and how one goes about identifying and describing the
contexts of actions. Whether the system of interest can identify those contexts or not,
typically the system of interest can't. So one has all of these interesting kinds of
context effects that are commonplace in biology and psychology.
But that's like so figure and ground aspects of physics.
Right. But the physics language provides a nice way to talk about formally.
Mike, do you want to stare at it?
Yeah, yeah. So lots of things I think would need to be discussed. One is this whole notion of
the environment being an agent itself. I think it's critical for all this, because typically
speaking, as far as I know in all of these biology models, we assume that the agency of the
environment is zero and that, okay, there might be some parasites and there might be some competitors
or whatever, but the actual environment, it's mechanical, it just is what it is. And so I'd
love to have us unpack that some more. And the whole error correction thing I think is critical
because all that I see in biology is constant error correction. And so by the way, the one thing
that Chris and I have been kind of developing lately is perspectives on all this kind of
stuff that we talk about, but really centralizing the role of the observer. So this idea that
everything is from the perspective of some observer. So anything, all the stuff that Chris
says on that topic, you know, what the reference frames and the observers and everything,
I think would be super, super important to hear again for me.
And we've spoken before Mike, how that might intersect with how I've been thinking about
orbits representing functions. So the dimensionality, the orbit
tells you how many times the space is folded and how many the depth of the function class.
And the chord that you cut across that orbit defines a function in that function space.
But instead of cutting a simple cut, you could intersect that orbit with another orbit that
was itself complex. So that it's like, what does one oscillator see of another oscillator?
And when you do it that way, it's much more interesting because in order to, in order for
one orbit to sample another orbit, it necessarily needs to be at a similar frequency. And that
makes it vulnerable to being modified itself so that you can't just be an observer that doesn't
influence the system, you're necessarily coupled with it.
Yeah, which, which to me that immediately links to the notion of, you know, as we talk about,
we talk about the cognition and all of that, but we also want to talk about first person
perspective, right, consciousness, so to speak. And, and to me, the interesting thing about that
is that unlike typical experiments that we may do where you can do them in third person,
and you're not really changed much by them, you may learn something as a result, but typically,
you know, it's you prepare some system and then you just sort of stand there and watch it. But
to me, I think any kind of experiments or any kind of science with respect to consciousness,
you are the participant, like you have to participate. Otherwise, what you're learning
about is physiology and behavior. If you're going to learn about consciousness, you're going to be
involved in finding out what it's like or how to whatever it is. And so that gets back to this,
this thing that you just said, Richard, that that you are for those kind of experiments,
you are fundamentally participating, altered by it, part of it, you know, that's the those things
will all come into play if you're going to find out about the, you know, kind of the,
the really core cognitive aspects of things.
Hmm. It's like saying you, you, that you can't be an observer of consciousness.
You can only, you can only just do consciousness. And the, and so the best that you can get to
being an observer is things like sort of almost, I was going to say looking in a mirror, but it's
interacting with another consciousness that's like you and how much of it can you see and how
much does it affect you? You can't, you can't interact with it without being vulnerable to
being changed by it. Yeah, that makes sense. Right. It's, it's like, if you think about what a,
what a success, what, what does a successful theory of consciousness output,
it's not going to be behavioral data, you know, it needs to be what, so, so what is it, right?
It's what, what, what, what, what format does it give you as an answer to, to, to our questions?
And I think it's got to be protocols for putting you into that same state so that you can really
find out what it's like. Right. So, so first person, so, you know, CS Lewis said something like,
I'm going to mangle it, but it was something like, you know, don't tell me what it's like to be
scared, tell me something scary and then I'll know, you know, right. So, so these things are
sort of like, anything else is a third person description, it's not consciousness, it's,
it's physiology, it's behavior, it's whatever. But if you man, if you, if, so what is that poetry,
you know, art, I don't know, but, but some kind of stimulus, right, that puts you into that same
state. Well, now we know, now we know what it's like. So does that mean that a theory of
consciousness is indistinguishable from, from an experience in the conscious state that it's like,
you know, LSD is a theory of consciousness? That's interesting.
I mean, kind of, you know, so, so a good theory of consciousness would be, so, so I, I produce some
kind of crazy hybrid with three brain hemispheres and some wheels and whatever. And I say, all right,
you know, with your theory of consciousness, tell me what it's like to, to be this agent. And the
answer is, well, you need this much LSD and then you need some, you know, you need this experience
and we need to give it. And then, then you have some prayer. I mean, I don't know that you could
ever get. And I could tell you that recipe, but that wouldn't, but that you're saying that that,
but that isn't, that isn't the theory of consciousness. That's just the
interesting. There is a recipe for consciousness, not a theory of consciousness. You have to do it
before you do it, before you read it, before the theory has to be,
the ingredients have to be put into action before it's a theory.
Yeah, I think it's interesting that if you look at what actually passes for theories of consciousness
and literature, they're not about what it's like at all. They're, they're about describe this entity
and my theory will tell you what this entity can be conscious of.
Yeah. Without saying a thing about what it's consciousness of that is like.
Beyond some, you know, vague generalities, like it's, it's consciousness of this,
it sort of feels stressful. Well, what does that mean? It means whatever it means to you.
Yeah. And you just project that onto the other system. So, you know,
we talk about E. Coli being stressed by heat or, or lack of food or something like that.
We say, well, stress feels like this to me. So maybe it feels something like that to E. Coli.
So I wonder if that's an entry point where we could talk about the relationship between stress
and harmonics in vibrating systems and its relationship to error correcting codes and see
if that resonates with you, Chris. So I've been thinking about the two way relationship between
oscillations and form and function. So the form and function of a tuning fork,
sorry, not form and function shape and form, the shape and form of a tuning fork, a particular
macro scale geometry determines the note that it rings at.
But in the other direction is also possible that a particular oscillation can create a
shape and form that will hold that oscillation. So in that direction, I've been playing with
Faraday waves and Faraday worms of oil on liquids floating on oil. And you
vibrate the oil at a particular frequency and the liquid droplets elongate and take a particular
shape that is an integer number of wavelengths. So there's a two way relationship there between
what frequency does this shape vibrate at and what shape does this vibration make.
And I notice that there's, the discontinuities are important or the quantization of the shape
and form is important. That there's, it wouldn't be interesting if you just, you know, you turn
up the frequency and the worm gets longer, right? What's interesting is that you turn up the frequency
and the worms length increases in quanta, in integer multiples of the wavelength.
So the correction, the connection to error correcting codes,
I'm thinking about it as, you know, there are certain modes which fit in a particular geometry.
And the frequencies in between don't fit. They're excluded. They're not allowed, disallowed.
And an error correcting code crudely is what you need for that is a combinatorial space,
a large combinatorial space of possible messages, which has large gaps in between
so that when a message gets a little bit distorted and it falls in a gap, you can see that it
doesn't belong, that it's not correct. And in the domain of oscillating systems and waves and
resonance and stuff, it feels like that provides exactly that that the quantized modes that fit
harmonics that fit in a string, for example, provide you a combinatorial space of possibilities,
but the frequencies in between don't fit. And if you, as you turn up the energy on a vibrating
substrate, it flips discreetly from one mode to another, from one harmonic to another.
And that feels like the right sort of territory for
getting combinatorials, pseudo-discreet space, combinatorial space of possibilities from
continuous substrates.
Yeah, that makes sense.
I mean, from a quantum information point of view, there are no continuous substrates.
And so it's all discreet in the end. And what your vibrating systems are doing is
is coarse-graining at particular frequencies, so they're making macroscopic discreet states,
which is fine. But in the end, the question is always,
is the frequency within some very small error bar of this interesting frequency,
or this preferred frequency or not? And if it is, then you have a 1, and if it's not, you have a 0.
So you've created this discreet space. And an error correction code just needs a discreet space
in redundancy. So space or time, either one, provides redundancy. So if you have a discreet
space in either space or time, you can construct an error correction code.
So the cool thing about the oscillations, vibrations, and harmonics, though, is how
you can pack one octave inside another, right? So it's not just that you have
a effectively discreet space of possibilities at one level of organization. But at the octave above,
there are two wavelengths that fit within inside that. And at the octave above that,
there are four. And those are discreet too, and they have their own error correction stuff going on,
which is semi-independent of the octaves above and below, but not independent,
because you also get the subharmonic phase locking between the levels, right?
So that feels like the right kind of machinery for having
multi-scale autonomy, where there's a dynamical process going on at a particular level of
organization that is semi-independent of the levels above or below. But you don't want it to be
completely independent of the levels above or below, because then, well, for a number of reasons,
one, because that doesn't explain what we want to explain in the biology, right? We want to explain
how this process at this level interacts with that process at that level, right? Yeah.
But in a sense, in a physical system that's a single physical system,
you can't have independence between scales, right? You're going to have this coupling for free.
Yeah, the coupling is for free, but what's unusual is the slight independence, right?
So the coupling of the... If I have two oscillators of a given frequency,
and they phase lock with each other, then I...
That's a phenomenon that has... It's like a causal process that operates at that scale of
organization. But I can also get a phase locking between an oscillator and another oscillator
at twice the frequency. And I don't... One of those objects doesn't really know
whether the other object is twice the frequency or half the frequency,
twice the frequency or the same frequency. So there's a sort of crosstalk between levels.
But ordinarily, the temptation is to say, well, all of the levels above are determined by the
levels below. So there isn't any autonomy going on at the high levels at all. And we don't need
to talk about crosstalk between levels because there aren't really any levels. But when you have
a resonance happening at a particular scale, that's a... That's a dynamical causal process at
that scale, which is semi-independent of the scale below. Do you not think?
Well, I think it depends on what you mean by semi-independent. I mean, one could...
One can think of this in terms of building detectors, right? If you have a base frequency,
you know, it's a kilohertz or something, then one can think about building
a detector, an antenna at some lower frequency. And that process may be completely independent of
whatever's going on at a kilohertz. But you will either do it in a way that
you can capture a resonance or you don't. And if you do, then good for you. You've designed
something that is a resonant detector, even for this higher frequency signal,
and that produces for you with your detector something that looks like a low frequency signal.
But you could have designed the detector a little bit differently
and gotten no resonance at all, and you wouldn't see a signal at your lower frequency.
Mm-hmm. So in that sense, there's complete independence.
So what you see is not independent of what's going on at the lower level,
but what you end up designing and building is independent of what's going on at the lower level.
Can I say that back to you, see if I got it? So suppose I was working in a low frequency space
and I had one low frequency oscillator synchronizing with another low frequency oscillator,
and everything appears to be low frequency and everything appears to be quite sensible.
But then I noticed that at particular low frequencies, something weird happens,
because I'm starting to detect something from the higher frequency that it's resonating with.
Mm-hmm. So I had a causal process at this low frequency area, but at particular low frequency
areas, there is an interaction with the high frequency thing, right? Yeah. And when that happens,
assuming that the high frequency thing isn't just a transmitter, but that it's a
physical process that's reversible, I'm also influencing the high frequency thing as well,
right? Mm-hmm. It will synchronize with me and I synchronize with it as the resonance is mutual,
not one way. Yeah. Given that there's a little bit of slack in the high frequency process, yeah.
But you're going to be, if you're driving the low frequency process,
you're going to be feeding energy into the high frequency process.
Mm-hmm. So in a sense, the question about independence is, are there drivers at every scale,
which is one of the things that Mike talks about a lot in terms of
processes going on at different scales that inject energy or information into a system
at some particular scale. Interesting. So you could have a high frequency oscillation that
drives a low frequency one, and then the low frequency one could drive some other low frequency
one, and then the second low frequency one could drive the high frequency one or another high
frequency one, right? So you can have interactions at both scales, right? And you can't really tell
who's the driver or whether the drive was introduced at that scale or came from the
level below, can you? I would say in general, no. If you're only making measurements at one scale.
When you're releasing energy at that scale in order to drive something at that scale,
the process that was releasing energy was getting it from the scales below anyway.
I think it's from somewhere, certainly. Yeah. Even if you were just burning something,
it was coming from there. Yeah. So that's an important point, and we
often tend to think about the power supply as a completely external part of the picture, but
yeah, the power has to come from someplace, and it comes from at the end of the day,
it comes from the environment, which is the same place that the information that's being
processed comes from. Yeah. So you can ask, how are those two coupled within the environment
in hidden degrees of freedom that you can't see? Yeah. I mean, it's the simple thing that you
start off learning about sources and earths, sources and sinks, and then they later on you
learn about back EMF, and you think, oh, shit, how did that happen? I was supposed to think about
that as it was just a, it wasn't part of the system, it was just a source. And now suddenly,
there is a way in which it becomes part of the system, and it starts interacting backwards.
Yeah. So the sources and sinks are, they're the ways, they are the resonances which are
losing or gaining energy from other scales of organization.
Or they're just a way of cheating. I mean, they're just a heuristic for not talking about the things
you can't see. Which are exactly the things that we want to talk about.
Yeah. Okay, cool. Could we talk for a minute about
making observations and hopefully interventions at multiple scales or at larger scales, right? Because
one of the things, you know, one of the things that happens, so the kind of air correction that
we think about all the time is, let's say you're a salamander, you've lost your arm or some part
of it, and then the cells have to do the hard work of doing some stuff, but also the collective has
to and so on. But the thing they're correcting doesn't exist at the cellular level. So they went,
you know, they're going to have to correct the fact that there's half the number of fingers than
there needs to be. But there's no notion of finger at the level of the single cell. So
there are those stresses at the higher level have to be propagated down and sort of make the
lower levels dance to some extent. But then when we take measurements and we make interventions,
what happens is people will look at it and they say, oh, but you didn't actually intervene at this
higher level, because I see what you did. You actually, you know, you put in some ion channels
or you touched some kind of a chemical, like, you know, if you want to, you can always zoom
into that lower level if you want to, right? So one of the things we've been grappling with is how
to define when you've actually taken a measurement at a higher level or worse yet, when you've done
an intervention at a higher level, because for measurements, you can talk about the integrated
information and stuff like that. So that's at least a little bit that we got going. But,
but, you know, what does it mean to make an intervention at a higher level when you're
talking to the collective in some sense, right? You're communicating with behavior shaping the
collective and whatnot. How do we, how do we formalize that so that it's clear that, you know,
yes, I know I used a piece of DNA to do it, but I wasn't really manipulating the lower level. I
was actually, it was just the, you know, that was just the conduit. I was actually talking to the
tissue level agent. So I imagine the, the sub harmonic phase locking, if that's the right
way to think about it, right? So if I had a process that would have four peaks in that
interval, and I have another process that has two peaks, then there's two ways of me locking onto
that for the, of the higher level process of locking onto it, right? And they're equally good,
they're equally compatible, equally stress free in their relationship to the level of,
to the higher frequency oscillation. And I can, if I can push that higher frequency oscillation
from locking, locking like this to locking like that, that push is something that
I don't know how to say it, except that the lower level is, is like, it's completely invisible to
the lower level. It's perfectly happy with those, with those two possibilities. But if this whole
thing is really, you know, one system with two waves carried in the same substrate,
then, you know, you say, I pushed it at this level, and somebody else says, well, you pushed it at
that level, because it's the same system that you're pushing. But you have to, you have to make a
case, let's see, you'd have to make a case that I'm not pushing a thing, I'm pushing a frequency,
and pushing it, you know, pushing it at this frequency is, is relevant, is the same as saying,
I'm doing something causal at this scale, not at that, not at some other scale.
Yeah, I, I think this, this comes down to the answer that I think you gave in lab meeting
yesterday, Mike, which is it depends on what the theory is that tells you what the intervention
should be. And if the theory that tells you to intervene with this probe is a macro scale theory,
then, in a sense, you're interviewing at the macro scale, because that's the informational
context. Yeah. The reductionists don't believe in macro scale theories.
Well, okay, I mean, you can be that way, but try it with a computer scientist.
Yeah. Yeah, that's, that's what I was arguing yesterday in lab meeting that Chris is referring
to, that after something interesting has been, some interesting system has been prepared,
you can then be a reductionist and, and say, well, I know what's going to happen next, you
don't need any of these higher levels. But I think in addition to prediction, we need another,
and I don't know how to formalize it yet, but we need another notion of like pre invention or
something that measures something else, it measures how likely were you to come up with
this in the first place, given your given given the level at which you operate. So
it's the sort of thing that, you know, like, like in the, in the, in the, in the cellular
automaton game of life, right? Once somebody prepares something, you don't need to believe
in gliders, you can just micro, you know, fall track the states and you can predict it all the way
out. But if you don't believe in gliders, you're never going to make the thing that is, you know,
when somebody made this Turing machine and out of gliders, right, where like, you're not going to,
you're not going to be able to do that if you don't believe in those levels. And the same thing
happens, you know, sometimes I'll give a talk and people say, that's, that's really interesting
data. But why you keep talking about all this philosophical stuff, don't do that, just, just
do the experiments. And the answer is, well, we wouldn't have done this experiment without that,
without that way of thinking about it. So it's not, you know, after the fact, after I show you the
two headed worms that, you know, repeating or whatever, then anybody can say, well, of course,
it's chemistry underneath and it's genetics. I mean, I can track it down. Yes, you can after,
after the fact. To which they'll say, well, I don't care what's going on inside your head.
I don't care what your theory generating process was. That's not part of the system I'm studying.
I'm only interested in the worms and their molecules. And how you came up with that theory
is not part of the system I'm interested in. And how you came up with it doesn't affect what the
system is that I'm looking at from their point of view, right? To which a good response is,
so tell me again what a worm is. I mean, you're, you're talking about the difference between
trigonometry and arithmetic, right? It's very convenient to have this thing we call sign,
but we don't need it. I mean, all we actually need is addition and multiplication, right?
So, do, do you project your geometry with just addition and multiplication?
Yeah. But, but the, so I'll continue to be the reductionist statfica for a bit. It's like, yeah,
I'm not sure I really believe in worms, right? There's only biomolecules and there's only genes
and you know, they're just vehicles. The only agents here, if there's any kind of agent is a
gene and that's not really an agent either, it's just a molecule. And, you know, all, all of your
worries about whether that's one individual or two individuals or one worm or two worms or,
you know, that's just, that's just the downstream consequences of the real causal process happening
at the lower level. But is it not the case that, okay, so, so we have competing lenses on things
and then, and then one has to choose one, right? Or not forever, but, you know, at some, for some
particular stretch. And so, how do we choose between them? So, so the reductionist lays out
their lens, I will lay out my lens and then now, now we must choose. And so, how do we choose?
Because this is very foundational, right? And, and neither, you know, it's easy within one frame to
sort of make fun of the other and say, you know, you're talking about things that don't exist. But
but the decision between them has to be made sort of above that and we need some sort of criteria.
And so, no, I think you have to read Ian's book first, right? To recognize that the logic of the
left brain, it locks out the possible existence of any other kind of way of thinking. But the
reasoning of the right brain doesn't, right? The right brain can see the whole and it can see the
parts. And the left brain just says, fuck off, I can only see the parts. And I can, you know,
there isn't anything else except my own logic. How do you know I refer to my own logic, right?
And it's caught in this loop that it can't get out of. Who is Ian in this case?
Ian McGill, Chris McGill, Chris.
Yeah, we had a couple of a couple of meetings recently, he's written some really interesting
stuff. Yeah. Yeah, I don't know. I, you know, I try to sort of boil it down to to pull it out of
philosophy where people have been debating this for, I don't know how you write how long. And
to say that, well, we need, we need some way of deciding between these frames at any given moment.
There are different ways of looking at things. And one way is the degree of facilitation of
future progress, right? I mean, I don't know what else, what else, what else you would choose?
Efficient, I mean, efficiency is one, but, but okay, you know, maybe that's,
it's not the only thing. But does it, does it help you?
Well, yeah, computational complexity is a measure that can be substituted for efficiency.
I mean, sure, do it all in string theory.
But it's much easier to make coffee at a macro scale.
And the reason is you, you aren't trying to do some exponential computation.
So what about the, not just aesthetics?
What about the,
well, essentially the, the, the multiple realizability of the lower levels.
So suppose I claim to have a process operating between units at a particular scale
operated on at a particular frequency. And for this particular instantiation of that
process, they were built on top of some other particular units at some, at the scale below.
But if the causal process had some independence from the things below, and you wanted to prove
that it did, you could say, look, I can do the same thing in a system that is either,
you know, locked differently. So that, you know, when it's over here, I do the same thing,
and I get the same result. When it's over here, I do the same thing, and I get the same result.
But the genes that change when I did it here are those genes, and the genes that change
when I did it there are those genes. So they are, they can't be the cause, because they were
different in those two cases. So the difference maker has to be what was happening at the high
level of organization, not what was at the low level of organization. And then as you, as
then, you know, there's the possibility that that's, that's not just a different,
a different, a different phase lock, but that it's a, it's an entirely different kind of system,
right? That it's a, that I replaced my, I replaced my gene signaling pathway with a
morphogen pathway that went outside the cell or did something else or was electrical or
something like that. It doesn't matter so long as it provided the appropriate oscillation for the
other thing to live on. And then you can't say that it was the lower level thing that did it.
I mean, you can still say this, this is a good reason to try to teach all biology students the
theory of virtual machines. Because that's the best example we've got.
Just, just transporting. Now, so zoom, zoom between my Linux machine and, and
Mike's Microsoft machine and whatever you're running.
Really, really badly limping along Windows machine. Yeah.
Yeah. So I mean, the fact that that works is, is undeniable evidence for the utility of high
level of descriptions. Because without that, you couldn't do it just from an engineering
point of view. Yeah. But is it high? Is it undeniable evidence for the existence of
a higher level causal process rather than a higher level description?
Well, the program is just a description of what a piece of hardware is doing.
So not in any fundamental sense, but in a pragmatic sense. Yes. You know, I actually
write programs in a programming language, not with wires and a soldering wire.
And I could never construct zoom. Well, I could never construct zoom anyway, but
I certainly couldn't construct it with wires and a soldering wire.
And no one else could either. That's that. Yeah.
To, to, you know, sort of in another way of asking this maybe simplified, could, could you,
could you play chess against a proper reductionist, right? Because what would their, you know,
what would their next move be? So they see, they don't believe in pieces. They don't believe in
boards. They don't believe in the relationship between the pieces. There are micro states,
all the micro states, like the library of Babel, all them, they all look the same to them, right?
In a certain sense. How do you, how would you make a decision? How would you make the next move
if you didn't believe in this thing? But that's because they don't believe in themselves either.
They would say, well, all of my micro states create this process that plays chess against
all of your micro states. But they don't believe in themselves as an agent that took a decision
about what move to do. You know, this, this suggests an old adage, which one wins arguments
only with technologies. I wonder if I might
change tack a little bit to talk about the kind of computation that we can do with physical
systems and with oscillations and stuff, if that's okay. So the talk that you saw, Chris,
would have been about the mechanism that I call natural induction.
Yes, I think that was correct. So you have a dynamical system described by a network of
viscoelastic connections. And when you, its natural behavior is to find local minima in its energy
function. But as it spends time at each of those configurations, the viscoelastic connections give
way slightly, which makes the attractor of that particular configuration a bit larger.
And then you perturb the state, and it goes to another attractor, and the relaxation of the
connections makes that one a bit larger, so that it forms memory of its own past behavior.
And as that memory is in the connections and not in the states, it's a memory which can generalize,
so that's not just making it more likely to go to configurations that it's been to before,
but also to other configurations, including novel ones that have the same associative
relationships between the state configure state components. So that process of back and
forth between what I would now recall physical optimization is just local energy
minimization, and physical learning of changes to the internal structure of the system,
coupled with one another, produces an optimization process that the change in the
state modifies the energy function and the change in the energy function modifies the state
trajectories. And it causes the state trajectories to find exceptionally low energy configurations,
even in the original constraints, even in the original springs, those are configurations
which are exceptionally low energy. So I understand that to be
a mechanism of adaptation, there's a whole other story about that isn't natural selection,
that's a different story. But in order to make it work, it needs to be, I noticed a couple of
things. One is that the way in which the springs push the masses and the way in which the masses
deform the springs needs to be reversible. If I was building, if this dynamical system was like
a regular neural network, which has a, you know, fires if the weighted sum of inputs over a
threshold, then that's not reversible. I can't put a current back down the axon and get it to
change the weights. The backprop does that, but I can't literally push on it. It's a one-way system.
So the natural induction process is kind of nice because it's a purely physical system,
it doesn't need to be designed or selected for the purpose of doing adaptation,
but it only works in physical systems that have that reversibility.
The other thing that I notice about it is that there's a shift between the configuration space in
which the states move and the configuration space in which the weights move, which is the one of the,
one is the correlations of the other. The one is, the state space is the straightforward state
configuration space, but the changes in the weights change the correlations of the other one.
And if you were to have, you know, if it's worth doing at all, it's worth doing recursively,
then the next level would be the sort of higher order correlations between those correlations
would be controlled with another kind of spring that connects two springs together.
And if that was all reversible, then you ought to be able to get it to do a deep learning process
instead of just an associative learning process. And that all feels to me like the implicate order
of bone. Shall I get your reflection on that first before I add more layers of thought onto it?
I mean, I think what Bohm was trying to talk about is entanglement
and the difference between quantum and classical information.
And what you're talking about here is, one could think of as kind of the
the first two terms in a series expansion, where if you take the whole expansion, you have entanglement.
Yeah. Yeah. Okay. That sounds like that is resonating with you a little bit.
Yeah. I mean, there's a nice long ago paper by Frank Tipler. He's kind of an odd character.
But what he shows is that if you take standard Laplacian classical mechanics
and you remove all the singularities that exist in classical mechanics because everything is a
hard boundary. And so you have collisions and they're instantaneous and so you have singularities
in the momentum. If you remove all those singularities by smoothing them out, then you get Bohmian
quantum mechanics. And the reason is that smoothing out the singularities is the same thing as
connecting everything to everything else. Yeah. That makes total sense to me, actually.
And I thought that was a really nice demonstration. It's a paper in PNAS from 2014
or something like that. I could dig it up and send it to you if you're interested.
Yeah, I'll ping you if I can't find it. Yeah. And the other part of the paper is a vigorous,
defensive Laplacian, which turns out to be the same as Bayesian theory of probability.
Right. So I've been thinking about it this way, right? In a deep neural network with a
feedforward architecture, you are getting one layer to provide inputs that controls the next
layer. Each layer is multiple inputs feeding in. It's losing information. It's coarse-graining
stuff as you go up. And if it does it in a raw or nothing, plus one minus one way, a discrete way,
then it's not reversible. And you can't push, if the output wasn't what you wanted, you can't push
back on it to change, to know how to change the weights or to know who was responsible for the
error. And the reason that we use a sigmoidal function is so that you can differentiate it,
so that you can get rid of those singularities you were just mentioning, so that you can push,
you can, given the error, you can use the error to push back on the weights and do the credit
assignment of who was responsible for this error and thus change the weights to form the weights
to give you a slightly better answer. And that, that reversibility is necessary for you to do
learning, right? Learning is I had an error in the coarse-grained low-dimensional space
that I didn't like. And I want to know how should I, how can I reverse engineer all of the changes
that I need to make to the internal organization of my machine in order to change that error?
And if everything is reversible, well, then you can just push on it and it gives you the
changes that you want, it just bends it to give you the changes that you want. But if everything
is reversible, then you can't do the nonlinearly separable deep functions that you actually want
to compute, right? You actually, the purpose of having a deep network with nonlinearities in it
is so that you can fold the feature space and fold it again and fold it again and fold it again,
so that you can have complex decision boundaries in, in the space. And although each layer was
a little bit reversible because you didn't use a step function you used a sigmoid,
it nonetheless becomes more and more difficult to reverse that function as the function becomes
more and more folded, that it becomes so massively undetermined that, you know, you have to push it
by infinitesimally tiny little bits. If you want to change a fold that you've made that's quite deep,
that's hard to do, right? So in deep learning, the way that that manifests is after I've trained
the network on a particular function, I'm basically fucked if I wanted to do something else, right?
I can't change the deep structure in that network without undoing all of the higher level folds that
I made, so that I can see that deep structure, push it to do something else and then put all
those folds back again. I can't change the deep correlations, the really, really low frequency
stuff without undoing all of the high frequency stuff that I put on top. Now, if you, my hypothesis is
that if we did a neural network that was built out of oscillators with periodic activation
functions instead of sigmoids, and we did the computation in phase space instead of amplitude
space, then it could do all of the same kind of computation. But the thing that would be different
is like, why would you want to do that? If it's equivalent, why bother doing the conversion?
The reason is because you could change the deep correlations in the network without having to undo
all of the high frequency correlations in the network, because you could just hum to it at the
low frequency, and that would turn the low frequency or the base of the orbit around in that low
frequency space without having to undo. You could drill through the higher frequency folds
and leave them untouched to communicate directly with the low dimensional folds.
So, you would have to know what frequency your error is occurring at,
and then you would have to
know how to correct for the side effects of changing the low frequency slightly
in terms of the resonance or off resonance behavior of all of the higher frequency components.
Yes. So, I'm not sure you get away with leaving all the higher frequency components fixed,
even in this kind of Fourier A representation. Yeah, my feeling is that the high frequency
ones are just as happy locked in a different phase as they were in the original phase.
Might change the semantics though.
Well, you know, up might be down, but they would be a reflection
that still had the same internal relationships. It's a left hand instead of a right hand,
but it's still a hand. And the way that you wouldn't, how would you know which frequency
the error was at, right? It's like, well, you don't because the way in which you're interacting
with the network when it's a low frequency thing that needs changing is because that was a causal
process that was happening at the low frequency. So, the causal process can happen between,
you know, when one organism interacts with another, for example, the kinds of interactions that they
have could be a really low frequency thing or a really high frequency thing, like they could
be having a conversation or they could be physically bumping into each other in the street.
And the response that you get is a change in the internal organization of the system either way.
But one could be changing something deep without, you know, I'm changing the ideas inside your head
without breaking your skull. And the other one breaks your skull without changing the ideas
inside your head, right? So, those causal processes are happening can happen between
an organism in its environment or another agent. If the environment is communicating with it,
enacting causal processes at that scale.
I mean, some living things facilitate that by providing an interface for which is learning,
you know, rewards and punishments and, you know, perception, all that for letting others
modify them in a particular way without having to get in there and literally change their,
their neuronal contents, right? I mean, who was it that I forget who it was, but somebody,
somebody gave a talk on how to give a talk. And the first thing he said is he said,
effective communication is a violent act, because fundamentally your goal, if you're good at it,
your goal is to reach in there and make sure the people that leave the, you know, leave in the
audience are not the same way they came in. But you need to, you know, they have to facilitate that
by being the kind of, by having that interface and being the kind of thing.
I repeated that to Eva and she said, why does it have to be violent?
Wasn't my, well, I mean, yeah, I don't know, maybe violence, not the best word, but there's
an element of this in there, because by coming to a lecture, you are taking a risk. You don't know
what you're going to leave there with, right? You might hear something, you can't unhear things,
you know, you might hear or you might see a piece of art or read something. I, you know,
I've certainly had that experience like, wow, I wish I hadn't read that. But, you know, but it's
too late. You once you've seen it, you can't, you can't unsee things. Yeah, I think that's right.
Your any, you know, any communicative act is, is
exerting forces on you, which transform you. Yeah.
I've often said that one frequency or another.
Park therapy and drug therapy are not all that different.
Yeah. Yeah. Well, we both do basically the same thing.
Yeah, Chris, I don't know if you were at the, at the, at that lab meeting we had, but
Fabrizio Benedetti, who gave that, that talk on the, on the placebo effects and everything he had
on the, on the, on the slide that said drugs and words have the same mechanism of action.
You know, he gave an hour, an hour lecture and that was amazing. I was, I was absolutely amazing.
Yeah, I, I didn't see that. That's very cool. It was, it was fantastic. He didn't want us to
record it. So I don't have, I don't have a recording of it, but it was, it was really good.
