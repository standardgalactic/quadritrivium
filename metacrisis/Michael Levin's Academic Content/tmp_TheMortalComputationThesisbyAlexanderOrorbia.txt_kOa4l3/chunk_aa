Yeah. Thank you for having me, Michael. And thank you everyone for coming to the talk.
So I'm going to talk to you today about some work that my friend Carl and I have done.
We call it the mortal computation thesis. And I think it complements some of the things
that I know Michael's group does. Actually, in fact, we cite particular mortal computers
that I will get into a little bit later. And we can talk about that perhaps. So just
very briefly, who am I? So I'm Alex Rubia, and I'm the director of the Neural Adaptive
Computing Laboratory at Rochester Institute of Technology. These are my students, as you
can see. And we work on quite a few things, but our primary focus is building biomimetic
systems and the learning algorithms and computational architectures that would facilitate their
learning and inference with the goal of obviously building things in hardware substrate.
So again, neuromorphic computing might be familiar to some of you. We also work on a lot of things
based on the free energy principle. Some of you might have heard of predictive coding
or active inference and my group contributes to that. And we have a lot of collaborators,
but this is just our brief little intro. So you might be asking what exactly is mortal
computation? And if you're familiar with it, you might be you might be aware that Jeff
Hinton last year came up with a phrase called Mortal Computation. And one of his works,
the Ford Ford algorithm paper, the idea effectively is that software cannot be divorced from the
physical substrate or hardware in which it is instantiated. And the idea is that the
calculations and processing are embodied. And in fact, inseparable from the medium, medium,
breaks down stops functioning. The software ceases to existing cannot be carried out on
another medium. This sort of effectively says that the software should die as long along
with its quirks and its properties, if the hardware or substrate that executes it ceases
to function. And of course, this is going to have implications for edge devices, energy
efficient systems and robotics, and so on and so forth. However, this challenges the
notion of immortal computation in computer science, sort of a foundational idea that we
can essentially write our program independently of the medium that we're going to execute it.
And that this program or software can be copied or executed on another GPU or TPU server, for
example, if you're doing machine learning, and it's going to pretty much run the same. It's not
going to have any particularities that are tied to its actual medium. And so effectively, the
knowledge and characteristics that your program acquires is done irrespective of the medium in
which you're executing it. So this is the work that maybe I believe you were linked to. It's a
little bit long. So I don't blame me if maybe perhaps you didn't read it all. But this is work
that we sort of did like a review and then sort of a perspective on what mortal computation really
should be as Jeff sort of just introduced it in about a couple of sentences. So Carl and I wrote
40 pages roughly about it. So why exactly mortal computation? Well, part of it is a thermodynamic
consideration. And this is basically getting at the relationship between information processing
and thermodynamic efficiency. The idea effectively is that we want to adhere to Landau's principle.
And these are just some principles I'm going to briefly introduce. Those of you who might have
deep physics background might even understand them even better than myself. But the idea is that
there is a minimum amount of energy required for any irreversible operation. Think of like erasing
a bit on your computer. And it is proportional to the operational temperature of the computing system.
And I just give you the relationship here, pay B as Boltzmann's constant. And the idea is that an
irreversible change in a computer's stored information requires dissipation of a minimum level
of heat to the environment. This is paired with the Jarzynski inequality. And I have a floating
bullet point there, but that's because I wanted to remove some unnecessary detail. But the idea is
that we're talking about the difference of the free energies, the physics free energies between
two states, x and y. And we are saying that the difference between them can be constrained to be
equal to the average of the work, the physics work done from all paths taken from perhaps an
equilibrium state x to a typical non equilibrium state y. What does this really mean in plain
English? Well, roughly, we can know something about a system's equilibrium by observing the system
when it is not at equilibrium. And so the main takeaway when you pair these ideas together, as
Carl and I argue, is that there's a lower amount lower bound on the amount of thermodynamic work
that you need to do to change information content in an information system. And so this of course
has wonderful implications. If we can get our intelligence systems that we try to strive to
date in artificial intelligence to emulate to build something that adheres to in memory processing.
And effectively, this just means that we are changing the computer's information content. And we are
saying that it's equal to the inference belief updating in response to external perturbations. So as
the computing system is dealing with an environment, we essentially want to be working at as close as
possible to the substrate, these calculations should not be divorced. And then of course, we can tie
this as I'll show you a little bit later to the variational free energy formulation. And effectively
that when a computing system or an intelligence system is minimizing its variational free energy,
it's the same minima as the thermodynamic free energy. And there is equivalence in work that we
point you to. And so of course, the idea is that we need to essentially try to circumvent what's
called the von Neumann bottleneck or the memory wall. And that's the idea that when we are doing
things, you might be aware of deep learning and what is rocking the world of artificial
intelligence to the state, we're effectively executing our programs on software that is in
random access memory. And as you can see in the bottom left diagram, there's a lot of expended
energy to start moving information from long term memory, depending on your computing structure,
to random access memory. And so we essentially would like to overcome that. And that's sort of
what just one common example that we see today in memory computing is neuromorphic computing,
where we're effectively designing our biological models, our neuronal models, actually as close to
the hardware as possible, and using things like BEMRisters to adhere to synaptic connections, and
so on and so forth. But ultimately, the idea is that realizing thermodynamic efficiency of Bayesian
computations, which is effectively what biological systems we are doing, requires belief
updating and memory to be Bayes optimal is to be thermodynamically efficient and vice versa. This
also has some wonderful implications for green AI, which is this argument that we need to
essentially figure out how to do the intelligent operations we do today, without expending
mammoths of quantities of energy, chat GBT and transformers themselves are pretty expensive.
So then there's also cognitive philosophical motivations. I won't dwell too long here as
well. But the idea is that there is in cognitive science, the embodiment thesis or the embodiment
hypothesis and inactivism, which is just effectively saying at a high level that the mind your mind
or any mind is grounded in its sensory motor accompanying. So the ideas that we are shaped by
the actions that we take, the nature of mental activity depends on your body. Effectively, you
need to be in some type of actual physical instantiation. You cannot have what is known in
classical cognitive scientists known as brain and event. So the idea is that we don't have
something called isolated cognition, we are actually primarily driven by our body. And
inactivism takes us an extra step further. And the idea is that we are dependent on our
environment, our cognition is we are coupled to an environment we effectively that affects us as
well as affects our environment. So a mortal computer in effect is an active participant in
the generation of the information that it processes, thus shaped by the consequences of
how it acts, and has acted on its niche. Effectively, this is called niche construction. And then we
also talk a little bit about some other nice connections in naturalist philosophy and existential
ism. That actually refers to a little bit the initial quote that you might have seen in the
paper by Soren Kikegaard. But the idea is that there is a finite to the life, it endows us with
purpose entails reproduction, and motivates us to pass on knowledge to future generations.
Ultimately, death is a horizon that shapes our behavior and consciousness. Now, of course,
while mortal computation doesn't go as broad and as far reaching as existentialism and other
aspects of natural philosophy does, it does actually we do pull or extract a small part of it,
talk about that even animals in any organism is implicitly constrained and conscious or maybe
not conscious of the fact that they have a finite horizon and they act accordingly.
And of course, there's some other parts that we touch on in our framework about operational
closure, which is just the idea that the system must auto undergo auto poetic, met self assembly
and self maintenance to keep separate its internal states from its external states is going to
motivate the Markov blanket construction of the mortal computer I'll talk to you about later.
And then, of course, the idea of sense making, which is that there's a mutual dependence
between external processes and an entity, whether it's a biological organism or rather
any mortal computer as we generalize, because a mortal computer does not need to be artificial.
We are all effectively mortal computers, a biological system must distinguish itself
from its niche, yet be coupled to it in order to persist in that niche. And this will motivate
the idea that we don't want to dissolve in our heatbath, we don't want to cease to exist.
So effectively, a part of the work that again, you might have read is it's a review. This idea
is interestingly enough echoed for many, many decades across all kinds of thinkers and engineers
and wonderful ideas have cropped up throughout time. So we sort of unified them and scoped them
out. Obviously, this talk, you've been time constraints is not going to possibly go into all
the details. But I will give you a swath. So sort of you can think of the paper or the concept in
terms of three particular slices. That's why it's called the three slices of moral computation.
We have a biophysics physiological naturalist philosophies interpretation in the top left
towards the top bottom left, we have a cybernetics interpretation, and then a cognitive science
interpretation. And I'm going to give you some of the highlights. So we're going to start with
the biophysics aspect of our framework. And I'll just leave this diagram to sit here as I explain
just a few of the key concepts to sort of parse it. So the idea is that a mortal computer self
organization constitutes its thermodynamic and metabolic efficiency. And we talk about metabolism
and autocatalytic closure as some motivating concepts. But effectively, these ground the
mortal computer's agency or its ability to adapt. Ultimately, we need to understand
that a mortal computer is not a thermodynamic equilibrium. And it operates as a dissipative
system, given that energy and matter would be lost by essentially in the ideas that we are
adhering to the first order, first law of biological thermodynamics, which allows us to
reconcile and ensure that we are still adhering to the classical second law of thermodynamics.
Entropy has to increase in the environment. But in biological open systems, we don't see that we
see entropy decreasing. So the idea is you have to kind of view it all as one big closed system.
You need the environment and the entity itself. But ultimately, a mortal computer or any entity
that can be considered one, its metabolic organization stands far from thermodynamic
equilibrium. And so ultimately, it needs to forage like any living system or any system to
continue to acquire new resources. This sort of motivates like that bottom level, the diagram,
the metabolic processes are essential primitives. This is something that Carl and I chose his
language to pull a little bit from cybernetics and bring it back into the biophysics interpretation.
But ultimately, these are the foundational components of a computer or a mortal computer
or any system arranged within a morphology, of course. And then of course, we need homeostatic
or homeohedic processes that live on top of it. This is our active regulation of those essential
primitives. And the idea is that we have metabolic processes as we discuss reactions that use energy
to trans materials into structures that then harness further energy to transform transport
material, eliminate toxins or surplus material. And of course, this is sort of our way of keeping
track of set points if we want to stay within a certain range or near a particular value.
And so we need to be designing out homeostatic processes, which ultimately, by the way, you'll
notice that there's these arrows for influence in the diagram. So all of these processes in effect
should have some type of influence on their sensory motor action. This is motivated actually a
little bit by work by Egbert and others on chemotaxis and talking about metabolic dependencies
and the idea that action is shaped driven by your metabolic functions. Sometimes it can be
entirely dependent on only dependent on that. But we sort of generalize it a little bit to
allow some design flexibility. On top of this, any of you are most likely aware of what allostasis
is, but we also need allostatic processes because this is instead of having error reaction or reactive
processes, we have error correction processes as well. So this is what serves homeostasis or
homeohesis, precludes future deviations to the essential variables. And then of course, at the
very, very top is our auto paedic autonomy level. And this is where we need to account for the fact
that a mortal computer, if to be unification of artificial as well as biological systems,
needs to have the ability to make itself from within. It must continuously reproduce,
organize, maintain the relationships between its parts, and as well as do this without external
intervention. It must come from within. And of course, then we have the notions of morphogenesis
because the idea is that the mortal computer needs to persist longer than its actual physical
instantiation of its components. So it means its actual organization or relations live longer,
and that's its identity throughout time. This will also motivate the self-evidencing aspect
of mortal computation. And it's continuously reproducing itself. And then as I mentioned,
there are motor sensory motor dependencies that a couple this to its niche. So ultimately, what
does this really tell you at the highest level? Well, that we need to essentially design our
mortal computer system with the niche in mind. The niche cannot be this decoupled or disentangled.
We need to understand the properties of the niche as well. Again, some might be wondering from
computer computational backgrounds, won't this make the system rather brittle because it's rather
dependent on the body and the niche that you are designing. But that's sort of the point. The idea
is that your behavior and your cognitive functionality is directly dependent on your
physical instantiation and the environment or niche in which you live. So again, the organizational
closure of the mortal computer, it depends or sorry, it's the means that the mortal computer
operates on the basis of self-reproduced structures. And, you know, as I've said already,
the mortal computer is auto poetic. And so this is where we have sensing, actuation, and we need to
be modeling or essentially accounting for energy exchange and matter transformation. And so again,
at the bottom right is our reinterpretation of the homeostatic dependencies of some good work
over the years. Well, we just have that as a little, oh, and it was called, well, we generalize
and call it homeostasis dependent, but you know, it was originally called metabolistic or metabolism
dependencies. So now we can move on to what we call the cybernetics backend. And again,
this could take a while, but I'm not going to dwell too long on it. Effectively, what I want to say
to you is there's a couple of concepts that we can bring from classical cybernetics. It's sort of
like the forerunner to aspects of computer science and certainly to artificial intelligence. But
ultimately, cybernetics deals with this concept known as retroaction, a system must incorporate
ends or goals into its means or mechanisms in order to ensure goal attainment is absolutely
inevitable or almost inevitable. And so the system essentially the question that characterizes
cybernetics is how can a system learn what it needs to know in order to act effectively?
This gets into this notions of self organizing systems, elementary parts and local interactions
with upward and downward causation. And we detail these a bit more in the paper and point you to
plenty of references for all those details. But the idea is that the system is dealing with variety.
And this is the central concept behind cybernetics. And you can see as you see on the slide, it's the
number of distinguishable states in a system state space. Another way you can interpret it is the
degrees of freedom that a system has in choosing what state it will be in. This can be reduced
via selection. And this leads us to sort of organizing and another aspect of cybernetics is
there's a lot of principles and laws. And so we try to organize a couple of them into three central
pillars that we felt were useful and complementary to the notion of moral computation. So we organize
them into, as you can see in the bottom, stability, regulation and growth. And so effectively,
stability is going to relate really nicely to homeostasis and homeohesis as we discussed
in the biophysics part of the talk. And the idea is that the system ultimately,
its goal is to reach a state of stability or ultra stability. And all this means is that it
performs selections to cross state space to try to reach new states until it converges to a place
where essentially doesn't need to alter its part to part relationships. It sort of stabilizes
