and robotics, they went exactly a different way.
So we work super hard to make sure
that your hardware always works correctly.
And then we code on top of that,
knowing that our hardware is reliable
and you end up with a completely different set of systems
versus what biology does,
which is it knows all the stuff underneath
is going to change, it's going to die,
it's going to mutate, it's going to be poisoned,
and we're still going to remap.
So this is one thing
that I think we're learning about what we are.
That's really fascinating.
Have you ever, by any chance,
come across relational frame theory?
I don't know if it's a bridge off of scinarian theory,
but basically what it's essentially saying is
the operant is a relational set of patterns,
rather than a particular thing or stimulus.
But actually what you're doing is you're tracking patterns
and being pulled into operant patterns
through relational frames.
So listening to that, and that's very, I think, consistent,
both with John mentioned structural realism,
really the idea of what we can really track
in the world are patterns and track patterns.
And if we're building our recursive relevance realization,
salient structures in an unbelievably changing world,
what are the things that we can track?
Well, pattern relations might be the thing
that affords our cybernetic goal tracking.
Yeah, and one last piece to throw in there
is that if you think about, if the goal,
if what biology does is take some kind of complex state
of stimulus and effects and all of that,
squeeze it down into a very compressed representation
and then try to re-expand.
So the caterpillar learned all this stuff
and gets squeezed down into some sort of molecular substrate
and then re-expanded or remapped onto the butterfly.
That squeezing, so just two quick things about that.
One is that the squeezing and expanding thing is everywhere.
So metasome in organisms, you've got your organism,
you squeeze it down to an egg, you re-expand.
You and I having this conversation,
I have some sort of complex brain state.
If I gave you a spreadsheet of all my neuronal activation levels,
that would do you no good because your brain is different.
What we do is we use language, we squeeze it down
to a simple low bandwidth message.
You will have to re-expand and reinterpret that message.
Do I know that you re-expanded it the way that I did?
No, but we do our best.
But that squeeze, you can think of science this way
as in writing papers and giving talks
as like the squeezing down.
And so as we think about,
so I've been thinking about this a lot about,
what are the features of the architecture that would allow this,
you know, that would enable this kind of amazing, you know, process?
And one of the things that struck me
was William James had this really cool thing
when he said the thoughts are thinkers.
And if you can dissolve,
and I just like doing that, dissolving boundaries between things.
So if you dissolve the boundary between data
and the cognitive system that operates on that data,
then you might say that, well, maybe the data isn't just passive.
Maybe the thing you learned isn't just a passive thing
that sits there and is hoping for this other cognitive system
to come and read it and, you know, and remap it.
But maybe it's got a little bit of, I don't know how much,
but maybe it's got a little bit of activity on its own.
Maybe it's got an agenda.
Maybe the agenda that it has is to be properly
or optimally placed in some cognitive system.
Maybe it wants to be understood, you know, I'm using quotes
because I don't know to what degree,
but I actually don't think that there's a sharp boundary here.
So maybe memories are not actually...
I thought of this again because of the frame theory thing you mentioned.
Maybe these patterns, these frames,
and maybe even perspectives have a little bit of agency to them.
They help.
The reason that any of this works is because it's not just,
okay, here's a passive molecule.
Good luck figuring out what this meant to your past self.
But actually, maybe these things have a little bit of activity
in terms of working to get themselves remapped.
Maybe it's, again, it's like this two directional thing.
So I don't know, that's just some stuff that we've been working on lately.
Well, I want to reply to a lot of this.
This is really rich.
I want to start with that idea of kind of a bi-directional conformity
that is not only the mind is conforming to the world,
but the world is conforming to the mind.
Of course, you might get tired of me doing this.
This is a neoplatonic claim, right?
And this is the idea, this is sort of the central idea
behind what I call participatory knowing.
And so it's not just a passive reception.
It's a co-shaping.
It's a mutual affordance.
It's a coming together.
It's a logos.
I think that is deeply right.
I think that's at the core of what I try to get at when I talk about...
It is participatory knowing.
And I think relevance is a cognitive psychological phenomenon
that is exactly that.
We aspectualize the world and it's sort of...
But relevance isn't just objectively given.
We don't just read it off,
but we don't just project it onto an empty canvas.
The world and us shape and coordinate each other
so that we fit together.
And this is kind of like an analogous to how niche construction works.
And things like that, right?
There's activity on both ends.
There's shaping on both ends.
So I think that's deeply right.
And I think that what you just said, that compression...
And in the 2012 paper we did on relevance realization,
we talked about compression and particularization
as sort of the engine of how you get the mind,
at least what we were talking about in that paper,
how you get it to be doing something
that is structurally the same as what evolution is doing.
You get the variation and then the compression.
And this means that noise in the system
is actually inherently valuable,
as you indicated a few minutes ago.
And what's happening is machine learning
is actually finally figuring this out,
that you have to, at very many stages,
you have to throw noise into the system to break it up
so it doesn't get locked into local minima
and it can explore many more environments
than the one it's getting locked into.
And I think that is very important.
And I'm building towards an argument here,
because I think that maps into something
that goes with your butterfly that human beings do
and this is L.A. Paul and transformative experience.
Human beings go through these profound changes.
So she does the gedonkin experiment
of people offering to turn you into a vampire,
which is very much like your butterfly example.
And the problem is you don't know what it's going to be like,
what your perspectives are going to be like,
you don't know who you're going to be,
what your preference structure is going to be, your traits.
And so you don't know if you should do it or not do it
because you're ignorant, you're deeply ignorant.
So you can't do standard decision metric inference,
your way through.
And this is very interesting.
And she says, of course, you face this
when you decide to have a child
or you decide to take up long-term education
or you decide to get into a long-term romantic relationship,
et cetera.
So I think this is exactly right.
And I think transformative experience
is pervasive in our cognition.
And when you put that together with what we just said
a few minutes ago about the noise and all of this,
what this means is our model of rationality
has to be fundamentally changed because here's the,
and this is what Agnes Kellard is.
Well, I'm not very rational right now
and I'm aspiring to be more rational.
I'm actually aspiring to go through a transformative experience.
So this is actually central to being rational.
Like being rational is a normative demand
that I become more rational than I am.
And it's not just a quantitative more,
it's a qualitative, it's a transformative experience.
So somehow these non-linear, non-inferential processes
are central to being a rational agent
because rationality is fundamentally
a transformative experience.
And what I'm saying is this feeds back
and then that rationality also has to take account
of this perspectival and this participatory knowing
we're not representing things over there.
We're, as you're suggesting, we are participating,
the world and us, we're participating together
in the co-instanciation of important real relation.
And I think therefore that Bateson's paradox
actually slams into the paradox of self-transcendence,
which is, well, if I become something other than I am,
then it's not self-transcendence
because something other has come in
and if I just extend what I am,
then it's not transcendence, it's just growth.
And that paradox is only a paradox
if you have a static single model of the self.
But if you have a model that is flowing
