Normally, I say welcome to my guest when I start an idea cast interview, but I have three
persons here who are of great significance to me and in my learning path and journey
and my journey towards understanding wisdom and relating to wisdom, and I'm very much
looking forward to this conversation.
I welcome the YouTube audience.
John, Greg, Michael, welcome to the three of you.
I'm so glad to have you all here today.
Pleasure.
Thanks for having me.
I want to acknowledge that you all have your own YouTube platforms, your own huge social
relevance, and so to be here on this show, again, I'm humbled and in gratitude to the
three of you for doing this and coming here and allowing me to just open up the space
for you guys and be a close attendee of what's about to unfold.
So gratitude to you all for that.
And I'll ask your charity here.
I'm a lay person.
I'm one of those dreaded autodidacts, but I ground a lot of it in the humility of Epoche.
In Peronian inquiry.
So I consider myself a student of the work of the three of you, and in my trying to think
of how we start this out today is to look at what Michael talks about and the audacity
of the imaginal being grounded in empirical evidence and the data and that dance between
the two of them.
And maybe we're opening the aperture on what intelligence does and what it might be.
So and looking at Greg's layered ontology, the joint points between different stages
of self-organization, I think there's so much richness in territory.
And of course, John, with his compendium of ideas and epistemological rigor.
And so I just see a beautiful convergence here of the three of you.
So I will be a good host.
I'm going to step to the side.
And Greg, if you would like to just go ahead and maybe start the conversation and I know
it's going to flow really well.
So to you, Greg, I appreciate Justin.
And it's a wonderful opportunity to be here and to have some opportunity to share ideas
with people I admire greatly.
So here's what I'd like to just throw out there and see, Michael, you probably you may
have seen my depiction of a tree of knowledge diagram for sort of upside down cones as it
were, sort of emerging out of energy information, singularity source, sort of a one world naturalism
from a big history view.
And it's really an expansion of complexification.
But something happens at different points of it that result in a qualitative shift.
In particular, a life plane of life emerges, a plane of what I call mind animal and culture
person.
And it took me a long time to think about what these planes actually were coming off
of in particular matter.
The matter complexification seemed OK, but I wasn't sure exactly what was happening with
these life, mind and culture cones for a couple of months, years after I developed it.
And then I basically was like, oh, it's a complex adaptive system network together through
information processing and communication systems that are affording particular potentialities.
And they're then mediated by certain kinds of systems, gene cells, but it's simply nervous
system animal, propositional language, collective human intelligence.
I was enormously struck by your cognitive light cone analysis.
And one of the things I really wanted to talk with you about was how you conceive of a cognitive
light cone, how you conceive of intelligence, the emerging evolution.
And if there's a relationship between that cognitive light cone idea and the cones that
I am depicting in terms of emerging complexification through information networks, exploration
of design space.
And then what I'd like to do with that is connect, if that's the case, connect the edge
of that and the contact of that with recursive relevance utilization as a dynamic process
that we can at least apply to see how these things emerge through what, how do we understand.
So I'll throw that out there and see if you would then, you know, riff off of that and
then pull John in in relation and then see where this thing might explode after that.
Well, I guess, I guess I should start by just describing this cognitive light cone idea.
And I should preface this by saying that the version of it that's out now, which I think
what I published in 2019 or so is very much a 1.0 vision.
So it needs significant improvement that much is clear and I'm working on that now.
For what exists is it's the following.
We were, I was actually at a Templeton meeting, a conference of people studying diverse intelligence
and things like this and Pranab Das tasked us with a challenge of trying to come up with
a framework in which you can simultaneously think about truly diverse beings.
So we're talking about not just the familiar apes and birds that we're used to and not
just, you know, maybe an octopus and maybe a whale or something like that, but, but like
really diverse.
So we're talking colonial organisms.
We're talking synthetic biology beings that are going to be, that are and are going to
be made cyborgs, AIs, whether software or robotically embodied, hybrids or some sort
of combination of living material and engineered the material of possible aliens.
I mean, all of it, right?
So, so, so, and I've been thinking about this for a long time.
And I sort of took that as an opportunity to try, okay, let's, let's, let's try to formalize
some of this.
How do we do that?
And what I thought was really fundamental to any agent, any, any, any being that we're
interested in is the scale of its goals.
I thought that goal-directedness, right?
Some degree of it.
And I'm not, you know, I don't like binary categories.
I'm not, I don't believe that there is a thing as goal-directed or non-goal-directed.
I like Norbert Wiener's sort of cybernetic scale that goes from passive matter all the
way up to, you know, sort of human metacognition and, and whatever is beyond that.
And so, and so I thought, okay, so let's just, for each one of these potential beings, let's,
let's map out what is the largest goal that they could possibly pursue.
So this, and so, and so the obvious, and so you collapse space and time onto a two-dimensional
sheet.
And so you get this thing that looks like Mankowski's light cones and, and so the size
of the goals, right?
And so you can, you can start to think of different cases.
So, so if I ask you what you care about and you say, I care about sugar concentration
within this 10 micron radius and I have a memory that goes back about 20 minutes and
I have predicted capacity that goes forward about 10 minutes, you're probably a bacterium.
And you know, and if you tell me that you care about things that happen within a, I
don't know, 100 foot or 100 yard radius or so, and you've got some memory going back,
but you're really never going to care about what happens three months from now in the
next town.
I'm going to say you might be a dog, right?
That kind of thing.
And, and if you've got goals that are planetary scale goals about the way the financial markets
and world peace are going to look 100 years after you're gone, you're probably a human.
And, and if you tell me that, that you can actually care for in a linear range, you know,
thousands of millions, all, you know, sentient beings that I'm going to say you're something
beyond a standard, you know, to modern human.
I don't know what that is, but you know, we, we, we can't really do that yet.
So, so that's, so that's the idea of these cognitive icons.
And there's two kind of two things that I'll just say about that, then I'll stop.
One is that I think these, these cognitive light cones interpenetrate.
In other words, in the, in, let's say, let's just take a human body, for example, there
are many, many subsystems that have their own inner perspective and their own goals
that they're following in various problem spaces.
That doesn't just mean three dimensional space, right?
Your body is home to all kinds of structures that live and suffer and strive in other spaces,
physiological state space, metabolic space, transcription, gene expression space, anatomical
space, if you're an embryo or something like that.
So we're not very good at recognizing these and, and, and, and there are many of these
that, that cooperate compete and so on, all at the same time.
And that leads us to the second point, which I think is pretty critical.
I, I like this notion of, so, so we talk about goal directedness.
I like this notion of teleonomy and teleology, of course, being goal directedness.
And then there's this concept of teleonomy, which is defined as apparent goal directedness.
Now, some people use that word to soften the impact of teleology.
They say, well, look, it's not really teleology.
It's just, it's just apparent teleology, right?
I'm not using it that way.
I, I, I am full blown into teleology.
I think it absolutely is, is a necessary concept to, to have proper understanding.
What I think is important about teleonomy is this, it is, in fact, apparent goal directedness
because it reminds you to take the perspective of some observer.
There is some observer who has to set, who has to make hypotheses about what they're looking at.
Now that observer, in terms of what problem space is this agent operating in?
What are their goals?
What degree of competency do they have to reaching those goals when situations change?
All of this, all of these are, are hypotheses from the viewpoint of some other being,
or in fact, the system itself.
So, so once you're past the certain level of, of advancement on that spectrum,
you too tell story, internal models about yourself.
You have a model of yourself as an agent.
So, so, you know, parasites, you know, scientists, right?
As external observers, parasites, conspecifics, predators, and the system itself all have
these perspectives on things.
And so, so I think keeping that apparent in mind that all these things are not, I think,
not objective kind of universal truths, but actually some observer trying to make sense
of the world as they look at themselves and other things.
So that, that to me is, is, is the idea of these, these light cones.
Lovely.
And would you say, would it be fair to say then, let's say, you know,
life gets started or whatever four billion years ago and then explodes,
we would actually see in the universe, at least on planet Earth,
essentially an emergence of life light cones, right?
In, in relation, would you say that?
Yeah, I think that, and I know it's weird for a biologist to say this,
but I'm, I don't think life is a super interesting or discrete category.
I think that, that it's, that, that what's, what I think is more interesting
is cognition, the spectrum of cognition and a wide, a wide, you know,
a wide range of those things are overlapping.
So certainly the cognitive, you know, if you think of a Venn diagram,
right, the cognitive circle and the life circle overlap quite a bit,
but I don't think they're the same circle.
And I think you can have things that are on the spectrum that currently people
would not call alive, which is why I'm less interested in that, that characterization.
I do think that one thing that, if, if I had to give a definition of life,
which I don't do, but, but if I had to, what I would say is that life is what we call
things that are really good at scaling up their cognitive light cones.
So if you have a collection of pebbles, which are basically only good at
sort of energy minimization and things like that, and, and I, by the way,
I don't think that's zero on the cognitive light.
I think it's very low, but I don't think it's zero.
But when you have a, when you have a rock made of those pebbles,
you have not scaled the cognitive light cones, got exactly the same capabilities.
But once you have life, what you find out is that it's arranged in a way where the
components have little tiny light cones and the collective has a bigger light cone,
a bigger cognitive light cone that, that actually extends into new spaces.
So when we see that happening, when we see goal directed systems being multiplexed,
so that the goal, the, the size of their goals that they get, they get these grandiose, you know,
longer term spatially and temporally goals, we call that life.
I think that's what, that's what life is.
But, but I do think that, that things that we would be hard pressed today to recognize life
as, as life can, can have cognitive light cones and maybe large ones at some point.
All right, I'll pause and see if John wants to jump in here.
Well, yeah, there's a lot I want to talk about there.
I want to, I want to build off of Michael's idea of light cones,
which I do mention in some of my lectures at U of T, University of Toronto.
I want to note that there's two, at least two parameters within a light cone,
as I understand it, there's reach and clarity.
And I think that brings in some of the work I'm doing about cognition,
in which I talk about the two meta problems of that activity.
So if you're going to be a problem solver, there's two problems you're always solving
as you try to become a more adaptive problem solver.
One is anticipation, your ability.
And I don't just say prediction.
I think that's a little bit of a misnomer.
I think if you predict and can't prepare, that's not very adaptive.
And we have experimental evidence for at least living creatures.
That's the case, right?
So I use the term anticipation.
So do you want to anticipate as deep use Lee as you can?
Typically it enhances the, the number and the kinds of problems you can solve.
Because the earlier intervene in a causal pathway for a problem often,
not always, but very often the easier it is to solve that problem.
It's much easier to avoid the tiger than fight the tiger is my sort of slogan.
And that's this idea of the light cone.
But the problem with that, which is the second meta problem is as you increase the reach,
you increase the problem that has been the besetting obsession of my career,
which is the issue of relevance realization.
The amount of information that you have available,
the amount of information you have to store,
all the possible combinatorially explosive combinations goes up exponentially.
And you have the problem that you can't just arbitrarily choose from that,
what to pay attention to, you can't algorithmically search.
And so you're somewhere between the arbitrary and the algorithmic.
And this gives you the issue of relevance realization.
I have proposed a way in which the two theories,
because the two problems depend on each other,
you don't, you can avoid relevance realization,
but what you do is you shrink your cone of anticipation very considerably.
And then if you wanted to increase your anticipation,
you increase the relevance realization problem.
To make a very long, a complex argument as brief as possible,
it's something like the predictive processing is trying to always minimize error.
It hits inevitable tradeoff relationships of error.
If it tries to reduce bias and increases variance,
if it tries to reduce variance and increases bias,
if it tries to reduce the errors of exploring,
it will crash into other errors of exploitation.
And there's all these inevitable tradeoff relationships.
And the idea is the predictive processing is going to create these
opponent processes that give what's called an optimal grip on the world.
And that's what I mean by clarity.
It's not just that you reach out well,
but you know how to optimally grip what falls within your light cone.
And that's how I think those two go together.
Now, what comes out of both the recursive relevance realization
and especially the predictive processing is this idea of mutual modeling.
In predictive processing, you always have to model yourself.
I don't, I don't mean model yourself as a self.
Please hear that.
But you have to model yourself when you're modeling the environment,
because you have to deal with conflation errors,
that stuff that's happening because it's inside of you
is getting projected onto the environment.
This is, and so you're always trying to model the self to some degree
to discount the errors being caused by your own embodiment.
And so this is the great insight the predictive processing runs off.
Don't try to directly predict the world,
predict yourself interacting with the world.
And that will get, help to solve those problems in an interlocking fashion.
And so what you get is you get, when you're modeling the world,
you're always to some degree modeling yourself.
And as you're modeling yourself,
you're always to some degree modeling the world.
The two are interpenetrating.
And I think that goes a lot towards the teleonomy
that Michael was talking about,
that there is something like a self modeling going on.
Now, for me, and this might be where Mike and I are different,
I think that that self modeling and relevance realization
depends on a system in some sense taking care of itself.
My argument is to the effect that relevance realization
is always caring about this information
rather than caring about that information.
And care, and I'm not meaning the experiential affect
I'm trying to use as a very broad, almost Heideggerian sense.
Your caring for yourself is what gives you the capacity
to genuinely care about this information or that.
This information matters to you, that information doesn't.
Initially, because perhaps that matter actually matters to you.
You literally have to take it in or you're not going to continue.
And so I think that relevance realization grounds in autopoiesis.
And so that's something we can talk about.
I do think that life does represent a significant capacity change.
And we can talk about whether or not
there is cognition without caring.
Or maybe you have an analog for caring going all the way down, Mike.
And I'd like to hear that.
As you know, I'm very interested in this deep continuity.
I would put one thing to you
that's a little bit more of an abstract level, two points.
And then I'll stop talking.
One is if we all are sort of non-reductionists,
and if you have a continuum with non-reduction,
differences of degree eventually become differences of kind.
Because with non-reductive continuums,
you have to have properties at upper levels that aren't in lower levels.
And so I think you get real emergence.
And I think that's a difference in kind.
And I think that is a way in which your continuum
and Greg's series of cones could plausibly mesh together.
Here's my final point.
And this is the point that I've been also doing a lot of work on.
And Greg and I did a lot of it together
on our transcendent naturalism series.
Is as we start to get this understanding of cognition,
we see it as properly transjective,
always between the system and the world,
always between the organism and the world.
And that means these discoveries about minds are also discoveries.
They're ontological discoveries about something
about the structure of reality itself.
And those two have to be understood together,
how we are understanding.
I get it as a continuum, but we talk about it in levels.
And I accept that distinction.
The levels are properly epistemic.
The reality is a continuum.
But what I mean by that is as we find levels in the mind,
unless we're willing to bite the bullet
of a profound solipsism and skepticism,
we have to say that there's something corresponding
in levels of intelligibility in the world.
And that's an ontological claim.
And for me, that means we are deeply committed
to a different kind of ontology and the flat ontology
that we have been doing science in for quite some time.
And I won't belabor this, this is some of the deeper recovery
of an older neoplatonic ontology
rather than the sort of flat ontology we've been with.
And I think this is important because I think that
can ground a spirituality that is not just about
psychological hygiene, but about genuine epistemological
and ontological realization.
So that's what I have to say about that.
I hope that made sense.
That was really compressed.
I'm trying not to hog up all the time,
but Mike, you always say a tremendously provocative things
and I wanted to respond to them in kind.
Yeah, thanks.
That's great.
I don't disagree with almost any of that.
And I think that especially, and I mean,
the whole kind of some of these platonic ideas
are really starting to come to a fore in some of our work.
I haven't written too much about it, but I will,
as the arguments get better and so on.
So I'm on board with pretty much all of that.
I do, I'll just say one thing
about the kind of continuum of business.
And then I want to talk about another,
I want to add something to what you said,
which is very interesting.
One thing about, here's how I think about this difference
in terms of when differences in degree
become differences in kind, right?
And this is why I called my framework
tame as in technological approach to mind everywhere,
because I really want to ground it,
not because technology encompasses everything
that there is, obviously that's not the case,
but the technological approach,
I think is interesting for the following reason.
Let's just imagine the paradox of the heap, right?
So you got a pile of sand
and you start taking the grains off and you say, well, when?
So here's what I think all of these claims are,
including any kind of cognitive claim,
any kind of a claim about what systems can and can't do
in terms of intelligence and all that.
I think these are all interaction protocol claims.
They're engineering claims in the sense
that what you're really saying
is here is a way I can interact with that system.
So for example, let's talk about the heap first.
If you tell me you need to move a pile of sand,
I don't want to really know whether it's a heap or not.
Here's what I need to know, am I bringing a spoon?
Am I bringing a shovel, a bulldozer, dynamite?
What are we bringing?
And there will be lots of scenarios
in which actually either one, right?
A big shovel or a small bulldozer will do.
And so I think all of these things are fundamentally
around a claim about what is the right way to interact with it.
So when you tell me that a given system
is somewhere on this spectrum,
I'm less interested in finding sharp categories
and looking for kind of emergent new phase transitions
and things like that.
I'm much more interested in the question of
so what tools are you telling me are going to be appropriate?
So if you're telling me something is a simple machine,
I understand it's rewiring and hardware modification
and that's all you got.
If you tell me that it's a cybernetic thing,
I'm thinking, ah, so I've got tools of resetting set points
and other aspects of cybernetics.
If you tell me that it's a learning agent,
I say, okay, I understand we have training,
all the things that behavioral science can do.
And if you tell me that it's at the level of,
let's say human or above discourse,
I say, ah, that means that I have certain other tools
and also I may be changed by the encounter.
In other words, unlike with a simple machine,
after we're done exchanging,
I'm also going to hopefully benefit from your agency
and we're going to have a different relationship.
So to me, all of these things are not about looking for categories,
they're about looking for ways we're going to relate
to whatever the system is in question
and in a very specific way.
I say engineering, which is applicable
to kind of all of the left side of that spectrum
and then after that it becomes other things,
I don't know, psychoanalysis or something,
but love and friendship and whatnot.
But that's what I think these things are.
I think these things are interaction frames that we take up
and then we see, which is why this is really critical.
A lot of people really,
they have philosophical kind of pre-commitments
to where things are.
They'll talk about category errors.
They'll say, well, it's a category error
to say that the cells and tissues can think
and can, I mean, I use the word think,
but can we have intelligence and so on?
I'm like, well, in the Middle Ages,
it was a category error to think that it was the same forces
that moved rocks on earth and celestial objects in the sky.
That used to be a category error,
except that these categories need to evolve with the science
and I think these are all empirical questions.
I don't think we get to sit back and have feelings
about what is and isn't intelligence.
I think we have to do experiments.
And so you pick a frame, you try it,
you make a hypothesis about here's the space
I think it's working in.
Here's the goals I think it has.
Here's the degree of competency I claim it has.
Let's try it.
Let's do the experiments.
We'll intervene in some way.
We'll see this thing.
And then we'll know, am I overdoing it?
Am I under recognizing mind?
Am I over recognizing it, then we pick.
So I think it's a scientific problem
optimizing relationships in the end.
So I think that's great.
And I'm mostly in agreement with that too.
Two things come to mind.
I mean, I think there is still a proper philosophical job
in that scientific endeavors experiment presuppose things
that therefore can't be given by scientific experimentation.
That doesn't mean that philosophical level gets to dictate.
It means that the two discourses
have to continually talk to each other.
And that's of course why I'm a cognitive scientist.
For example, the model you propose, which I think is good,
there is a fundamental presupposition of relationality
being central to a grasp on ontology.
And that opens up the question, well,
notice that information and intelligibility
are inherently relational things.
Maybe we should be prioritizing relationality
over the rollata in our ontology.
That's a kind of philosophical question
that emerges by reflecting on what is presupposed in the science.
And I think, Mike, and I hope you take this not as an insult,
I think you're actually doing work
that is pushing towards that,
that's saying pay attention to the relationality
over the rollata and prioritize that.
And I think that is actually a deep and fundamental challenge
to our kind of standard ontological grammar,
which goes back to a Cartesian substance
where we talk about individual things,
having properties that can independently exist,
independent of their relations to other things.
And I think, and there's a lot of people,
and I'm one of those groups that are saying,
we need to challenge that fundamental Aristotelian ontology
in order to actually accommodate into our worldview
what the current science is disclosing.
What do you think about that, as what I just said?
Yeah, yeah, no, I think it's exactly right.
And I've sort of, you know, if you were to,
you ask some people, what is the central thing
that persists through time?
And they'll say, well, it's genes.
And somebody else will say, well, it's information.
And what I think it is, is perspectives.
I think what we have is perspectives, ways to actually reduce.
I mean, a perspective is a chosen reduction of all the stuff
you could take in from some vantage point.
You're going to agree to ignore some things.
You're going to emphasize other things.
So perspectives are what change, evolve, interact.
Like I think it's all about interaction and perspectives.
Observers, perspective interactions,
I think that's the basis of everything that we have to do in science.
And that's very similar to Ladyman's structural realism,
that what is persistent across all the sciences
are these kinds of broad, real patterns
by which we're doing sort of this, like you said,
this compression and selection of information.
And what survives are not the particular semantic content
we give to it, but sort of these structural patterns.
And stuff like that.
Yeah, I think that's something deep.
And so what you have is you have not only a neoplatonism up and down,
you have a neoplatonism across time,
which I think is really, really interesting.
So I'm going to stop for a bit,
because I feel that you and I are starting to get into a rhythm
and I don't want to exclude Greg at all.
And I really appreciate this, what you're doing.
I make reference to your work a lot.
My pleasure.
Because I think, yeah, thank you.
Yeah, I think our work is complementary
and we mutually strengthen each other's positions
in a way that's intellectually respectable and justifiable.
But I do think the same thing is the case for my work and Greg.
So I want Greg to talk now.
Well, actually, that's a nice segue,
because I do want to check in with you, Michael,
in terms of what?
I mean, you're doing such intense, brilliant theoretical work.
You and I touched on this a little bit in our private conversation.
You know, John and I talk about this meaning crisis.
I'm a clinician.
I'm deeply concerned with how we see ourselves as human beings
and what science says about what we are,
what we know, how we think about it,
and how does that connect to wisdom traditions
in a particular way?
And I see your work as brilliant empirical work
that challenges certain old, pre-existing notions
at times dominated the paradigmatic natural science view,
or at least it opens up a wide variety of different perspectives.
As a psychologist who looks at the way people think
about themselves in the world, I hope we evolve to new frames.
John and I are doing a series called Transcendent Naturalism,
basically anchoring us in a naturalistic way
to the potentialities of transcendence
at individual and collective levels.
What kinds of worldviews afford that?
And what kinds of scientific understandings
of the world afford that?
So I'd really like to hear your thoughts about that.
What has been your experience as you open up this realm,
as you share this teleomic perspective,
open up us, think about light cones across a wide variety
of different domains.
What does it say about us in the universe
from a scientific perspective?
And what does that mean?
Yeah, great topic.
So I'll say something general first,
and then I'll dive into a specific example
of what I think this means.
Overall, I think the whole crisis of meaning thing
is incredibly important.
The work that I try to do, I view very strongly
as trying to climb out of it,
not trying to reductively dig a hole deeper.
And this is really important because I'm not a clinician,
but I get tons of emails from people who say,
okay, I've read your paper.
I understand that I'm a collective intelligence of cells,
and now I don't know what to do with myself anymore.
Literally, what should I do?
And maybe I've read some Sapolsky,
and now I don't think I have free will anymore,
so I'm really confused,
and I don't have any idea what to do, right?
So I think that's important
because I think it's really critical
that the stuff that we do is seen as what I think it really is,
which is providing now a way to climb out of
all of the things that we were told
by evolutionary theory, by neuroscience, by physics.
Well, you don't have this,
and actually you don't have that,
and it's all about competition
and survival of the fittest, so we've got that.
And so, okay, there were a lot of bad ideas that needed to go.
Great, but now we've gotta climb our way up the other side
of this and rebuild on a better foundation,
rebuild some of the things that are necessary for us to flourish.
And yeah, I think that's partly what we're doing,
and I think a huge part of that
is the whole diverse intelligence field
and this idea of building tools that go beyond
our very narrow monkey brain affordances
that we have for recognizing other kinds of minds,
I think is critical.
I think if we do this, things are going to be...
Once we are able to recognize
other sentient beings around us,
and we commit to this notion
of enlarging our own cognitive light bone
so that we actually can recognize
and have compassion for beings that don't look like us,
they don't have the same origin as we do,
they're different in every way.
Yeah, I look forward to a future
in which the kinds of distinctions
that we make currently about within the normal human variation,
we say, oh, these are like us, that is other,
that they're not like us.
Like these things are gonna be so laughable
in the future when the wide,
when really a freedom of embodiment really takes off
and we're all in whatever, right, you come into this world
and you're not stuck with whatever body evolution just happened to have,
when genetics happened to have landed you in,
the diversity of bodies and minds
that are gonna be out there
is gonna make all these current distinctions
completely laughable.
And I think that's good.
I think we have to...
I'm too mature.
I think we have to drop a lot of old categories
which made sense in olden times,
but they don't make sense anymore
because they don't actually capture
what's unique about this essentially
and being worthy of compassion.
And so anyway, so that's kind of the general stuff.
I wanna say one thing about the more specific issue
of what we are.
So this goes back to John's point
about the problems that any being faces.
So there's one more interesting problem which is this.
It goes across scales and evolution
is called Bateson's Paradox.
And the idea is that if you're a species,
the world's gonna change and you've got two options.
If you don't change, you try to remain the same,
you're done for, you're gonna disappear.
If you do change, in a certain sense,
you've also disappeared
because now you're something else, you've changed.
So every agent faces this interesting problem
that if you're going to persist
or better yet learn and improve
and whatever your journey is gonna be,
you are not going to be the same.
So committing to a static representation
of what you are is doomed, right?
It's doomed at the evolutionary scale,
it's doomed at the personal scale for the following reason.
And this also goes back to the point
that John raised about the relevance or the salience
you called it or no, I called it salience,
you said relevance of information.
Imagine, let's just for a minute think
about the butterfly caterpillar kind of situation, right?
So you got a caterpillar,
caterpillar lives in a two-dimensional world, eats leaves,
and it has to turn into, and it's a soft-bodied creature,
so it's a very particular kind of controller
you have to have when you can't push on anything
there are no hard parts, has to turn into a butterfly.
So in order to do that,
what happens is the brain basically gets dissolved,
most of the cells are killed off,
all the connections are broken,
you build a new kind of brain.
So one amazing thing that has been found in various systems
is that the butterfly or moth actually remembers things
that you train the caterpillar on, so memories persist.
Now, you might focus on the question of,
wow, where is the memory?
If you refactor the brain, how do you still have it?
And so that's a fantastic question
for developmental biology, for actually computer science,
and we don't have any memory media that will work that way.
But there's a deeper issue here, which is that,
oh, and so I should say what it is that they learn.
So you have a disc of a particular color,
let's say, I don't know, purple,
and the caterpillars learn that they get fed on this purple disc,
and then when you get the butterfly,
it will go there and try to eat.
Well, here's the interesting thing.
So not only do butterflies and caterpillars
not eat the same stuff, caterpillar wants leaves,
butterfly doesn't care about leaves, butterfly wants nectar.
Not only that's the case,
but also the physical embodiment is completely different.
So what you have to do,
it's not enough to keep the memory as it is,
the memory as it is is completely useless.
You have to transform that memory,
keep the salience, dump the details,
and remap it into a new,
so another sort of a weirdly grandiose way of putting it is,
in your new life, in your new higher dimensional life,
like literally because the butterfly lives in a 3D world,
so literally in your new high dimensional life,
you will not store,
you will not keep the memories of your past life,
but you will keep the deep lessons you learn, right?
You're not gonna know that moving certain muscles
in a certain stimulus gets you to leaves,
you don't care about leaves,
you don't have those muscles anymore,
you have something completely different.
And so being able to remap across,
when everything changes,
being able to remap that information is really fundamental.
And so when we think about what we are,
here's what I'm getting at,
you might think that what we are,
so you might think, okay, so butterfly caterpillar,
that's a really sort of extreme example,
I mean, we don't do that,
plan area that learn and then you chop off their heads
and they regrow a new brain and they imprint their memories,
okay, we don't do that, so these are like weird.
I think this is all of us,
this is, we are absolutely that type of being,
that is not a static structure,
and our job is to keep that structure intact
against all the things that happen.
Fundamentally, I think that at any given moment,
you don't have access to the past as it were,
as it was, what you have access to are the engrams,
the messages that your past self left for you
in your brain and your body,
and you have to interpret those, right?
So puberty will alter your brain in various ways,
all your priorities will change,
your preferences will change in many ways,
when you're 90, you will still have memories
of your childhood, but not because you've kept,
there is no molecular structure in the brain
that stays the same for that period of time,
and everything's bubbling around,
molecules come in and out, cells, and so on.
What you are constantly doing
is reconstructing yourself and your memories
to make them applicable in the new scenario.
So what does this look like across scales?
For the human, it just means that as things in your brain
and body go in and out,
you are maintaining a coherent self-model of some sort.
In evolutionary terms, it means that evolution long before
we had brains or any of that,
double down on this idea that everything is going to change.
You know the environment is going to change,
your parts are going to change because you will be mutated,
we know you're going to change,
and so this is why we have these amazing examples of,
you know, when we make tadpoles with an eye
on its butt instead of in its head,
they don't need new generations of adaptation,
they can see and they can learn in visual assays immediately,
right? There are many, I write about this stuff a lot,
there are many amazing situations
where you can radically change the,
not just the environment,
but actually the parts themselves.
You can put in weird nanomaterials and then all this stuff.
You always get something coherent because, I think,
because what biology does is assumes that you can't just learn
the structure of the past,
you have to learn,
you have to make problem-solving agents
and the body and then eventually,
the brain and the mind are continuously reconstructing
because you know everything has changed.
So this, and there's some other things
that could be said about that,
but I'll stop in a minute.
This is just, I think this is one of those things
that we're learning from all of this,
is that if you want to know what we are,
it is less plausible to think of ourselves as
some sort of static structure
that tries to hold on to the n-grams of the past.
We are a continuous process of sense-making
and reinterpretation.
I mean, I'm obviously not the first person to say this,
but we now see that across scales,
from evolutionarily to molecularly to developmentally,
from the robustness of the body
to the robustness of cognitive systems,
confabulation, like all this,
the noise and the unreliability of the substrate
is not a bug, it's a feature.
It's the thing that makes us intelligent and robust
because you assume right off the bat
that everything's going to change
and that our number one fundamental capacity
is to remap onto new scenarios.
And if you think about,
if you think about what happened in computer science
and robotics, they went exactly a different way.
So we work super hard to make sure
that your hardware always works correctly.
And then we code on top of that,
knowing that our hardware is reliable
and you end up with a completely different set of systems
versus what biology does,
which is it knows all the stuff underneath
is going to change, it's going to die,
it's going to mutate, it's going to be poisoned,
and we're still going to remap.
So this is one thing
that I think we're learning about what we are.
That's really fascinating.
Have you ever, by any chance,
come across relational frame theory?
I don't know if it's a bridge off of scinarian theory,
but basically what it's essentially saying is
the operant is a relational set of patterns,
rather than a particular thing or stimulus.
But actually what you're doing is you're tracking patterns
and being pulled into operant patterns
through relational frames.
So listening to that, and that's very, I think, consistent,
both with John mentioned structural realism,
really the idea of what we can really track
in the world are patterns and track patterns.
And if we're building our recursive relevance realization,
salient structures in an unbelievably changing world,
what are the things that we can track?
Well, pattern relations might be the thing
that affords our cybernetic goal tracking.
Yeah, and one last piece to throw in there
is that if you think about, if the goal,
if what biology does is take some kind of complex state
of stimulus and effects and all of that,
squeeze it down into a very compressed representation
and then try to re-expand.
So the caterpillar learned all this stuff
and gets squeezed down into some sort of molecular substrate
and then re-expanded or remapped onto the butterfly.
That squeezing, so just two quick things about that.
One is that the squeezing and expanding thing is everywhere.
So metasome in organisms, you've got your organism,
you squeeze it down to an egg, you re-expand.
You and I having this conversation,
I have some sort of complex brain state.
If I gave you a spreadsheet of all my neuronal activation levels,
that would do you no good because your brain is different.
What we do is we use language, we squeeze it down
to a simple low bandwidth message.
You will have to re-expand and reinterpret that message.
Do I know that you re-expanded it the way that I did?
No, but we do our best.
But that squeeze, you can think of science this way
as in writing papers and giving talks
as like the squeezing down.
And so as we think about,
so I've been thinking about this a lot about,
what are the features of the architecture that would allow this,
you know, that would enable this kind of amazing, you know, process?
And one of the things that struck me
was William James had this really cool thing
when he said the thoughts are thinkers.
And if you can dissolve,
and I just like doing that, dissolving boundaries between things.
So if you dissolve the boundary between data
and the cognitive system that operates on that data,
then you might say that, well, maybe the data isn't just passive.
Maybe the thing you learned isn't just a passive thing
that sits there and is hoping for this other cognitive system
to come and read it and, you know, and remap it.
But maybe it's got a little bit of, I don't know how much,
but maybe it's got a little bit of activity on its own.
Maybe it's got an agenda.
Maybe the agenda that it has is to be properly
or optimally placed in some cognitive system.
Maybe it wants to be understood, you know, I'm using quotes
because I don't know to what degree,
but I actually don't think that there's a sharp boundary here.
So maybe memories are not actually...
I thought of this again because of the frame theory thing you mentioned.
Maybe these patterns, these frames,
and maybe even perspectives have a little bit of agency to them.
They help.
The reason that any of this works is because it's not just,
okay, here's a passive molecule.
Good luck figuring out what this meant to your past self.
But actually, maybe these things have a little bit of activity
in terms of working to get themselves remapped.
Maybe it's, again, it's like this two directional thing.
So I don't know, that's just some stuff that we've been working on lately.
Well, I want to reply to a lot of this.
This is really rich.
I want to start with that idea of kind of a bi-directional conformity
that is not only the mind is conforming to the world,
but the world is conforming to the mind.
Of course, you might get tired of me doing this.
This is a neoplatonic claim, right?
And this is the idea, this is sort of the central idea
behind what I call participatory knowing.
And so it's not just a passive reception.
It's a co-shaping.
It's a mutual affordance.
It's a coming together.
It's a logos.
I think that is deeply right.
I think that's at the core of what I try to get at when I talk about...
It is participatory knowing.
And I think relevance is a cognitive psychological phenomenon
that is exactly that.
We aspectualize the world and it's sort of...
But relevance isn't just objectively given.
We don't just read it off,
but we don't just project it onto an empty canvas.
The world and us shape and coordinate each other
so that we fit together.
And this is kind of like an analogous to how niche construction works.
And things like that, right?
There's activity on both ends.
There's shaping on both ends.
So I think that's deeply right.
And I think that what you just said, that compression...
And in the 2012 paper we did on relevance realization,
we talked about compression and particularization
as sort of the engine of how you get the mind,
at least what we were talking about in that paper,
how you get it to be doing something
that is structurally the same as what evolution is doing.
You get the variation and then the compression.
And this means that noise in the system
is actually inherently valuable,
as you indicated a few minutes ago.
And what's happening is machine learning
is actually finally figuring this out,
that you have to, at very many stages,
you have to throw noise into the system to break it up
so it doesn't get locked into local minima
and it can explore many more environments
than the one it's getting locked into.
And I think that is very important.
And I'm building towards an argument here,
because I think that maps into something
that goes with your butterfly that human beings do
and this is L.A. Paul and transformative experience.
Human beings go through these profound changes.
So she does the gedonkin experiment
of people offering to turn you into a vampire,
which is very much like your butterfly example.
And the problem is you don't know what it's going to be like,
what your perspectives are going to be like,
you don't know who you're going to be,
what your preference structure is going to be, your traits.
And so you don't know if you should do it or not do it
because you're ignorant, you're deeply ignorant.
So you can't do standard decision metric inference,
your way through.
And this is very interesting.
And she says, of course, you face this
when you decide to have a child
or you decide to take up long-term education
or you decide to get into a long-term romantic relationship,
et cetera.
So I think this is exactly right.
And I think transformative experience
is pervasive in our cognition.
And when you put that together with what we just said
a few minutes ago about the noise and all of this,
what this means is our model of rationality
has to be fundamentally changed because here's the,
and this is what Agnes Kellard is.
Well, I'm not very rational right now
and I'm aspiring to be more rational.
I'm actually aspiring to go through a transformative experience.
So this is actually central to being rational.
Like being rational is a normative demand
that I become more rational than I am.
And it's not just a quantitative more,
it's a qualitative, it's a transformative experience.
So somehow these non-linear, non-inferential processes
are central to being a rational agent
because rationality is fundamentally
a transformative experience.
And what I'm saying is this feeds back
and then that rationality also has to take account
of this perspectival and this participatory knowing
we're not representing things over there.
We're, as you're suggesting, we are participating,
the world and us, we're participating together
in the co-instanciation of important real relation.
And I think therefore that Bateson's paradox
actually slams into the paradox of self-transcendence,
which is, well, if I become something other than I am,
then it's not self-transcendence
because something other has come in
and if I just extend what I am,
then it's not transcendence, it's just growth.
And that paradox is only a paradox
if you have a static single model of the self.
But if you have a model that is flowing
and I'll connect to something else you say,
a model of the self that is inherently a collective
and flowing the way you're doing it,
I think you put those together
and you get, right,
multiply mutually evolving selves.
I don't think we are a self in any kind of magnetic sense.
I think, and this is what a lot of the therapy,
all the parts work and the IFS and a whole bunch of stuff,
we are properly dialogical.
We are dialogical within, we are dialogical without
and trying to find sort of the soul thing
that is the self is a mistaken category.
And this becomes important
because when you look at debates,
I'm teaching a course on the self right now
and Greg and I with Christopher Mastapietro
did a series called The Elusive Eye.
People will say the self isn't real
and then what they'll do is the arguments are
because they'll admit that all this stuff
we're talking about is going on and that's all there,
but that's not a self.
Well, why?
Because it doesn't give you something like a soul,
a single monadic substance
that's on the unchanging bearer of properties
and then they say, therefore it's not real.
And I turn around and I say,
well, then by that standard, nothing is real
because what science is showing us
is that nothing is a substance.
So all you're really saying is the self
is as real as everything else
or as unreal as everything else.
I think saying everything is unreal
is a useless thing to say.
I don't think that gets you anywhere
or advances anything.
And so I think what this self-no-self debate
is ultimately pointing to,
and I'm trying to show you,
it's deeply continuous with the biology
you've been arguing,
is we're facing a fundamental transformation
in what we understand the self to be,
dialogical, and what we understand rationality to be.
And I think those two things
are really profoundly important at a cultural level,
but if you've agreed with the argument I've made,
they ground out in deeper stuff in the biology
and let alone even in the physics.
And I think this gives them powerful plausibility
because we're proposing a fundamental paradigm shift.
Here's the final thing I'm going to say.
I think that mutual transformation
of the notion of self and rationality
is crucial to getting out of the meeting crisis.
I think as long as we remain
in that Cartesian framework, we are locked.
We are locked into nominalism.
We are locked into dualism.
We are locked into antagonistic processing.
And we are locked into all many of the central drivers
of the meeting crisis.
I think that's a...
Did you want to respond to that, Mike, or...?
I have things, but please go ahead.
Yeah.
Well, I mean, I think certainly so,
one of the things that I, you know,
one of the things that I would be looking for,
and this is what John and I are doing,
the transcendent naturalism,
is to consolidate certain kinds of messages
that afford people ways of gripping the world,
that enable them to make sense of their lives,
make meaning in their lives as ecological agents
in a particular exploration of design space
and finding that kind of participatory relation
as being sought in a...
You know, there is a way to embed oneself
on the cusp of this aging arena of relation,
I believe, that many wisdom traditions have identified
as being fundamentally core to one's sense
of being present in the world.
And to me, one of the things that your work is doing,
one of the things that I was so drawn to John's work,
and again, sort of as a way to share with people
ways of being in the world,
to me, what it is that this shows scientifically,
philosophically, and participatory,
is it points a particular direction
for in many ways at the core being in the world.
There's a relationship to the world
that emerges in this dynamic process,
I think from both of your work,
that is a very, very important transformation
for us to communicate society and embrace as we go through.
So that's, again, I kind of keep coming back from me,
is gripping these elements to embed our structure,
our grammatical structure of relating to nature,
to the world, to the future,
in a particular way is deeply important to me.
So I just wanted to make that point and resonate with it.
Yeah, no, I think, I love all of that,
I think you're absolutely right.
And I think it's critical for us,
for people to realize that when we re-imagine
what the self is and take us away from this notion of a substance,
you know, some kind of monadic substance and all that,
it's different than what you said before,
which is that, well, everything is equally illusory.
I mean, there's nothing at that point.
Well, that's a deeply destabilizing concept for a lot of people,
and I think that's where they think we're going.
And the example that I try to help people think about is this,
I mean, it is true that we are patterns more than anything else,
but like, okay, so you've got a rat,
and you train the rat to press a lever and get a reward.
Now, if you zoom into what's going on here,
you've got some cells that have interacted with the lever,
you've got some cells that got the sugar of the reward,
they're not the same cells.
There is no single cell that had both experiences.
Who owns the collective, who owns the associative learning
that just took place there?
So there's this rat, which is a group of competent subunits,
and there's some mechanisms,
which that's the research program is to study them,
I call them a cognitive glue, that's kind of what we work on,
is to figure out, okay, so something has appeared here,
it isn't nothing, something has appeared here,
which is a pattern that has memories,
it can have goals, it can have preferences,
it can have competencies that the individual parts don't have,
and it's perfectly reasonable.
So somebody literally said,
I read your thing about this collective intelligence,
what do I do now?
And all I could say was,
whatever amazing thing you were going to do before you read my paper,
go do that, you can still do all that,
because you can, even though you're a set of patterns
that they're interacting in a particular way,
you can become a better pattern,
a more interesting, a richer pattern,
and that is what we can do.
And so commit to a bigger cognitive light cone,
to helping others have a better embodiment,
whatever it's going to be.
It doesn't dissolve all that stuff,
it just gives you a new window on it,
and you still, after all that is said and done,
you still got the opportunity and the responsibility
of moving forward as that and doing things.
So, yeah.
I want to reply to that, I think that's right.
I think getting it clear to people that we're not dissolving,
we're revealing or disclosing,
we're disclosing as opposed to dissolving,
I think is, and that's what I was trying to argue for.
And I agree with what you said,
tell people to go back and maybe use this to reinterpret
so they can recover what has been lost
because of an inappropriate frame.
I, with the Verveki Foundation's help,
we set up ecologies of practices.
We have a practice called Dialectic in the Dialogos
that helps people get into mutually shared flow states
of cognitive exploration,
and people discover collective intelligence
as something that is phenomenologically present
and almost agentic in what's happening.
They get the we space that takes on a life of its own
and leads people into each other
and everybody beyond each other
into something deeper and more profound.
And people will say things like,
I discovered a kind of intimacy I didn't know existed
and I've always been looking for.
And if that doesn't sound platonic to you,
I don't know what does,
that's an amnesis through and through, right?
And so, I agree with what you said,
but what I'm also suggesting, Mike,
is if we, and we have to do this carefully and ethically,
virtuously and with virtuosity,
but we can reverse engineer by paying attention
to the wisdom traditions and the science,
the cognitive science, the kind of science you're doing,
we can reverse engineer practices for people
that help them to do the recovery
and also the development of the cognitive light cone,
right, of a recovery of a lot of what is lost
for people in the meeting crisis.
Just, there's so much more,
but just pick up on that sense of,
this is a, they say they had always been looking
for this kind of intimacy,
but they didn't realize that they were.
That's a really interesting state that they're,
and we're getting this,
and this, we get this across groups who come in,
I'm not pretending it's a random sample,
it's obviously self-selected.
People are coming in
because they have some orientation to my work,
so I'm not claiming this is like a scientific study,
but it's not nothing either.
The fact that many different groups of people,
religious, non-religious, many different backgrounds,
different places in the world come together,
and this is a reliable thing that happens.
I think that's indicating something that,
so yes, we can tell people,
yes, go back and try to recover,
do the wonderful things you're trying to do,
and don't make it,
don't try and dissect it away
because of a Cartesian framework,
but on the other hand,
here's a bunch of new practices,
or at least old practices that have been recovered,
or at least reverse engineered,
in which people can deeply recover
a lot of the experience and the learning
of what we're talking about here.
So it goes from being something
they may propositionally assert
into being something they procedurally
and perspectively and participatorily realize,
and I think that's an important thing to say as well.
When people ask,
and I'm not saying you have that responsibility,
I have chosen to take on that responsibility,
and a lot of people with me,
I'm not taking single credit,
but I think one of the things to say
is to say what you say by all means,
but also to say, well,
why don't you try ecologies of practices
that are based on this
and see the positivity that comes out
of being in these practices,
see what you realize and recover in these practices.
And I know Greg is doing something very similar
and our work, Greg is a powerful theorist,
but he's also creating an ecology of practices,
and his work and my work in the foundation's work
where we're doing a lot together.
And I just want to know what,
I mean, obviously there's great risk here.
There's people turning into gurus,
there's weird cult formations,
there's exploitation, there's money pumping,
you have to do a lot,
you have to try to build a lot into safeguard against this.
But I'm proposing that we could sort of reverse engineer
a complex ecology of practices
that could be properly understood as spiritual
in that it affords people transformative experiences
in which they are recovering this deep connectedness,
this intimacy,
their learning and reality and themselves
are being deeply disclosed together
within and without and between each other.
And you're getting the cultivation of a reorientation
towards meaning, virtue, wisdom.
I think this is also something we can say to people now.
Yeah, I think that's where you find the bridge
from a lot of the is of the science
to the odd of humanism and a new opening for fusion and connection.
One thing I wanted to ask you, Mike,
I haven't seen and I certainly haven't tracked all of your stuff,
but I am, I know you focus on continuity
and I know the approach that you take.
But I have, I'm curious,
I just wanted to make sure I had this opportunity to ask you,
when you think about the human condition
and the human intellect
and you think about kind of,
is there something, what is the thing,
what are the multiplicity of things
when you think about the human intelligence structure?
What do you identify, if anything,
that is kind of at the root of our explosion
over the last half million years
and to dominating the planet,
building technologies,
given rise to certain kinds of thought.
Where do you see that?
Do you think much about that particular kind of question
have you reflected on that?
I just love to get your thoughts
since I have, have you here.
Well, let's see.
I know it's a little bit of a switch topic,
but I wanted to check in with you on it.
Yeah, yeah.
So I don't think I have anything brilliant to add
over what a lot of smart people have said
about the unique capacities of humans
and why, and why we're,
this is such a successful embodiment and all that.
I can say a couple of things.
First, someone,
and I don't remember who it was,
but someone said that,
maybe Yuval Harari, I don't know.
Somebody said that the special thing that humans have
is that we're storytellers.
And I think that's a compelling vision,
except that I think all agents are storytellers,
fundamentally, from the first bacterium
that had to compress a very chaotic, noisy experience
into a simple model of what the hell is going on
and which of my effectors can I use
to improve certain scenarios.
You're now a storyteller.
You are now, you are no longer Laplace's demon
trying to track microstates.
You have committed to a certain story
of what effectors you have and what's going on.
I think we're all storytellers.
So I don't think it's that.
I mean, I think we crank it up to an amazing degree.
And I think that language,
you know, I'm sure language is an important part of it
in the sense that, that compressive,
that tool that can compress complex brain states
into a simple thing that can be passed on
to somebody else for uncompression,
I think is super powerful.
And as much as I like to use various tools
of cognitive and behavioral science in other places,
I've not seen anything that suggests
that language exists other than in brain.
So I wouldn't claim that, although we don't know.
I'm not saying it's impossible.
I'm saying we haven't seen anything like that.
So I think language is key.
I would say a couple of other things about humans.
One weird thing about humans
is that we have a cognitive light cone
that's longer than our lifespan,
which is a bit different.
You know, if you're a goldfish,
all of your goals are likely achievable, right?
You might have a 20 minute horizon of goals,
and you're probably going to live 20 minutes.
And most likely your goals are all achievable.
Humans are a uniquely, we have many goals
that are absolutely not achievable in our lifespan,
and we know it.
And so what kind of unusual pressures
or capabilities that unlocks, right?
Having goals that you can commit to
that you know are not achievable
within your own lifespan.
Maybe that's something.
And I guess the final thing I'll say is that,
and this becomes very important because people are now,
because of AI and all this,
people are trying to define proof of humanity certificates
and these kinds of things.
I want to say a couple of things about what a human is
and isn't according to my humble opinion.
If you, the first thing to realize is that,
and I have a diagram of this,
but I'll try to sort of pantomime it.
You got your standard modern human in the middle,
and it's got this like a gentle glow about him
and all the philosophies about the human, you know, the human.
And so going up back here above him
is a very smooth gradation of evolutionary stages
all the way back to a single cell microbe.
And when you say the human, well, which human?
So the human of today, the human of 100,000 years ago,
the human of 300,000 years, right?
It, you know, it's, and they say, well, it developed,
you know, this and that developed very fast.
They say, what's very fast?
One generation?
No, no, no, no, no, it takes you.
Well, then what was going on in between, right?
If you think that humans have responsibilities and right,
you know, they can be good.
And where exactly, right?
What can you blame one of these hominid ancestors
for what they did?
Or are they still, right?
So, so you got this spectrum, right?
I mean, I like continue on spectrum.
Okay, down below, you got the exact same thing
on a developmental time scale.
So again, what human?
You used to be an unfertilized OSI.
It was a very slow and gradual process of how we got here.
So which human are we talking about?
And then even more, more sort of widen this all out.
Horizontally, you can imagine now,
as we will, as we already are and will more with technology,
you can step away and you say, well, I can modify,
I can be modified biologically.
I might get some tentacles and I might live under water someday.
And I really would like to see an infrared.
I mean, what's this with these limited retinas, you know?
And so you can biologically, also technologically, right?
I can have implants and all kinds of, you know, at some point,
today, maybe 2% of my brain is an implant that's helping me out.
But eventually, it might be 58% of my brain is some kind of construct.
So you got all of this.
And then that really, and obviously,
science fiction has been on top of this for 100 years,
but a lot of people, especially who talk about AI,
are just now catching onto this idea that human is not a sharp category.
And then that raises the question of, so what do we really mean?
And I tend to think about this as the kind of thing that,
you know, you're going to Mars for the next 30 years.
You get to take something with you.
What do you take with you?
What's important that you take?
You know, you don't want a Roomba.
You don't want, you know, you want, what are you really looking for, right?
So I want a human companion.
What does that mean?
Is it the DNA?
Do you care about the DNA?
I don't.
A lot of people are super into the DNA.
And if you change your DNA, my God, then, you know,
you're no longer, I don't care about DNA.
It's then people like, well, it's the standard body.
You know, once you put wheels on and gotten, you know,
tentacles and a propeller, you're no longer human.
I don't care if you have all your standard parts
that evolution happens to have given you
and you're subjected to back, you know, lower back pain
and the stigmatism, all this dumb stuff that we ended up evolving.
I don't think that's, I don't think that's what we mean by human.
So what do we mean?
So, so I think it's really interesting to think about
what's essential about it.
And I think what we mean when we say human
is a certain impedance match between us with respect
to the size of your cognitive light bone of compassion,
literally like, what are the size of your goals?
And what, you know, what is the radius of compassion
that you can muster?
Because if you're, because, because it actually,
the mismatch can be in either direction.
If the cognitive light bone is tiny,
we're not going to have much of a relationship.
If you can't care about the same level of thing,
but conversely too, if you've got this like,
you know, galactic scale mind, we may not be able
to do the normal thing that, you know,
the normal human interaction.
So, so I think, I think that's what we're talking about.
We're talking about the size of your goals
and the things you can care about in the compassion sense
of the act, the practical, not the affect,
but the practical like pursuit of goals.
That's, that's what I think.
Lovely.
I really appreciate that.
Yeah.
I want to respond to that because I think that's important.
I think that I agree with Mike, the discussion,
I think the discussion around human
is actually an equivocation.
I think it's equivocation from some sort of biological notion
and Mike is just can just devastate that as he just did.
And another notion, which is a moral legal notion,
which is a person.
And there, we've got enough science fiction
that lets us know that you don't have to be humans
to be persons.
And then I think we try to find some anatomical locus
of personhood within a biological humanity.
And that is just a doomed project from the beginning.
That will not work.
And I think a lot of the tech people and the AI people
are like, they're bumping into this,
but they're bumping into it.
And we've said this multiple times with old categories
and old schemas, and they're saying often equivocal
and sloppy things about it.
I mean, and the move you made, and of course,
you brought this in, Mike, you brought in the notion
of compassion.
This is ultimately, you know, a Kantian,
but even properly a Hegelian move.
It's like, well, persons are beings that can recognize
each other as having moral responsibility
and moral obligations.
And I give that to you as you give that to me.
Do unto others as you would have them do unto you.
The golden rule, and I'm compressing a huge amount
of much more sophisticated argument.
But this notion of reciprocal recognition
of our responsibilities and our authority,
I can obligate you.
I can say, don't do that because that's immoral.
And I don't have to appeal to your desires.
I don't have to appeal to your projects.
I can just say, don't do that, that's immoral.
And you are, if you're a moral agent,
you're at least responsible to that.
You don't have to agree with me,
but you're responsible.
And I think, and you know, and Hegel said,
this is when we become geistly.
We become spiritual beings when we become capable
of this reciprocal recognition of moral authority
and moral responsibility such that we are no longer
driven just by our desires.
We can be driven by what we are obligated to do.
And less people think, I'm just talking about ethics.
Reason is that kind of obligation thing.
You should conclude this because of that.
And I can say that to you, regardless of your desires.
In fact, we criticize people, motivated reasoning,
who deviate from what they should conclude
because of their desires, et cetera.
Sorry for that.
They're building a battleship next door.
It's very annoying.
So I think that that compassion, if you understand it,
more broadly as this reciprocal recognition
of normative responsibility and normative authority,
that's what we're talking about
when we're talking about personhood.
And notice we do that even with human beings.
We, we do this weird thing.
We don't obligate two year olds to our moral obligations.
Because, and we, well, they're persons.
Well, they are and they aren't.
They're in this nebulous status.
They're persons in that we have moral obligations to them
because by undertaking those moral obligations,
we will actually turn them into persons.
And so, but we don't let two year olds get married.
We don't let them vote.
We don't let them bear arms.
We don't let them drive cars.
We can hold them in a location, kidnapping them.
We can force them to go where we want, like,
but many of the standards of personhood,
we don't allow them to have.
And so I think what needs to be done
is a clean separation from this discussion of human,
which can mean some kind of
psychobiological, psychosocial biological entity.
And, and I agree totally with you, Mike.
I think trying to pin that down is a fool's errand.
And I think they're reasoning the right,
what the reason why there are people
are trying to pin that down is they're trying
to find a place for personhood.
And here I know you don't like it,
but here I think that is a category mistake.
I think personhood is different from,
and it is not locatable in a psychosocial biological entity.
It's about this capacity for mutual recognition,
reciprocal recognition.
Yeah, I don't disagree with that.
I think that's exactly right.
I would just say that it's a degree, right?
That's all, that's all I'm saying is that I think it's a,
so, so for example, you know, we in the,
let's take the legal system,
we've arbitrarily decided that 18 means adult, right?
I mean, it's total nonsense.
Nothing happens on your 18th birthday.
However, at least in the U.S.,
if you want to rent a car, you got to be 25.
Why 25?
They didn't do what the legal system did,
which is just to kind of guess,
and well, just to kind of set it.
They have actuarial data,
and they just realized that 25 is when your brain's mature enough
that we ought to be, you can be trusted with a car.
That's empirical, you know, that comes from,
and so I, you know, I think putting a more understanding
that it's a, that it is a continuum,
and that certain things develop faster than other things.
And if we agree, right, if we can, and I don't know,
by the way, it's way beyond my pay grade
to try and figure out a legal system
that will work in the future of hybrids
and all this stuff, like, I don't know.
But just as a step to accept that it's not a yes or no thing,
that it's not, you know, the Twinkie defense is crazy,
but serotonin actually does make neurons go,
I mean, there's going to be a, like, it's a spectrum.
We need to figure this out.
That's, I think, part of it.
So I agree with you.
I just, you know, I can think of too many in-between cases,
which I think will all show up.
I think we're going to, you know, you got your,
like, right now you've got people
that we say are non-neurotypical.
I mean, wait till you see what's coming, right?
When everybody's got all kinds of, you know,
somebody's got a third hemisphere grafted on,
so now they can actually, you know,
they've got extra IQ points.
And so you say, you know, you read,
like the rest of us wouldn't have been responsible,
but you really should have known what you were doing,
because, you know, you got that third hemisphere.
You know, these kinds of things are eventually going to show up.
We're going to have to figure it out.
And I agree with you.
And that's why I brought up the example of children.
We don't have a definitive thing where they're persons.
And in fact, we have this weird capacity.
We can even do it to some degree with the raising of dogs.
If we can sort of treat things in the right way as persons,
and they start to approximate personhood.
And of course, we have individuals,
and people have seriously, and I don't mean just sloppily,
reflected on whether or not psychopaths,
people who seem to be amoral,
they seem to be blind to moral normativity,
are properly persons, precisely because they lack the ability
to undertake exactly that reciprocal recognition.
And so, again, I agree with you.
I wasn't proposing a hard deadline,
but I was proposing that there's confusion
around personhood and humanity.
And I think calling somebody a human being,
I think should be largely a psychobiological designation.
I think calling somebody a person
is we're bringing in a whole bunch of other criteria.
Those criteria are probably going to shift.
I don't think they're finally definitive
because I don't think anything is intrinsically
or inherently relevant.
And that goes back to your butterfly again, right?
But I do think there are mistakes
that are happening around this,
and that's what I'm trying to point to.
Yeah, yeah, absolutely.
Gentlemen, we've reached that time
that we had agreed prior to hitting record
that we would all have to come to a stop.
So if you want to wind it down in the next couple of minutes,
maybe some closing thoughts, and then we'll wrap this one.
And I'll say right now that if you want to come back to this show,
or if John and Greg, if you want to take this
on to transcendent naturalism
and continue the conversation, the option is yours.
But yeah, if you want to go ahead
and maybe make some final thoughts.
John?
Sure. Well, I'll offer some.
First off, it's been a joy.
Your continuum of intelligence, Michael,
is a beautiful thing to play with.
It's an enlightening thing.
And I deeply appreciate the way you're,
both the way you think about it,
the way you have researched it,
and the way you've articulated it here.
So for me, again, sort of, I'm coming back,
keep coming back to, I built,
you talk, unified theory of knowledge
for us a potentially new grip,
both in relationship to the world
and relationship to ourselves.
It affords this deep ontontological continuity
and the potential for enormous change going forward.
It embeds our understanding of categories
in a structural relational patterning,
a close to a process theology of Whitehead,
but also then not so much getting
into the incredible weeds,
but giving a basic optimal gripping of,
hey, energy, matter, life, mind, culture,
in these kind of, there's a continuity
and discontinuity that can frame us
and then place us as agents
in the arena in a particular way
that orients us more towards meaning in life.
And of course, this is John's work.
So to then get together and to jam and riff around that
and have that music come alive here
has been a real pleasure.
So I deeply enjoyed it.
Yeah, likewise.
So thank you, Justin, for putting this together.
And I think you guys, the work that you do
is super important.
And I'm extremely happy that some of the biology
and the computer science that we do can be connected
with these issues of personal, interpersonal,
these things that are very important for people.
So yeah, so thank you for doing that.
I think that's really important.
Yes, thank you, Justin, for putting this together.
Great pleasure.
Always a great pleasure to interact with you, Greg.
And Mike, I think this is the third time we've spoken
and I'm continually amazed by the deep conversions
between our work.
We started in very different places
and in some ways it looks like we're tackling
very different problems, but when you push on them,
they seem to converge in really important
and mutually supporting ways.
And I find that very, very powerfully encouraging
about the plausibility of the overall framework.
And so I'm deeply grateful for your work
and always a pleasure.
I hope that you and I talk again.
And we sort of share students here and there
and I, but it would be nice if you and I
talked a little bit more regularly.
So just opening the invitation to that.
Yeah, anytime, anytime.
Absolutely, yeah.
Thank you all so much.
Great fun.
Thank you.
Yeah, absolutely.
I'll say goodbye to you gentlemen
after we stop her cord real quick,
but I also want to acknowledge the YouTube audience.
Thank you, guys.
I hope this was a, I was jokingly thinking it's
tri-electric into trio logos here.
We had three incredible minds.
So that was a frame that I had set up
in speaking of frames.
I really appreciate that you guys shed some light on the
dimensionality of a frame that's very large
that we're working with.
Also, perhaps maybe some frames getting broken
on the smaller scale of beingness,
self-ness and the fluidity, the continuation
or the continuum of these self-ness beingness
of the psychosocial dynamic of that, all of that.
So thank you guys so much.
This was everything and more that I was aspiring to
when I imagined this get together.
So thank you very much.
I appreciate your hard work.
And again, I am a loyal student,
maybe a little shallow in depth,
but I aspire nonetheless.
So anyway, but like I said,
I'll say goodbye to you all off camera here
and bye-bye YouTube audience again.
Thank you.
Thank you.
