Normally, I say welcome to my guest when I start an idea cast interview, but I have three
persons here who are of great significance to me and in my learning path and journey
and my journey towards understanding wisdom and relating to wisdom, and I'm very much
looking forward to this conversation.
I welcome the YouTube audience.
John, Greg, Michael, welcome to the three of you.
I'm so glad to have you all here today.
Pleasure.
Thanks for having me.
I want to acknowledge that you all have your own YouTube platforms, your own huge social
relevance, and so to be here on this show, again, I'm humbled and in gratitude to the
three of you for doing this and coming here and allowing me to just open up the space
for you guys and be a close attendee of what's about to unfold.
So gratitude to you all for that.
And I'll ask your charity here.
I'm a lay person.
I'm one of those dreaded autodidacts, but I ground a lot of it in the humility of Epoche.
In Peronian inquiry.
So I consider myself a student of the work of the three of you, and in my trying to think
of how we start this out today is to look at what Michael talks about and the audacity
of the imaginal being grounded in empirical evidence and the data and that dance between
the two of them.
And maybe we're opening the aperture on what intelligence does and what it might be.
So and looking at Greg's layered ontology, the joint points between different stages
of self-organization, I think there's so much richness in territory.
And of course, John, with his compendium of ideas and epistemological rigor.
And so I just see a beautiful convergence here of the three of you.
So I will be a good host.
I'm going to step to the side.
And Greg, if you would like to just go ahead and maybe start the conversation and I know
it's going to flow really well.
So to you, Greg, I appreciate Justin.
And it's a wonderful opportunity to be here and to have some opportunity to share ideas
with people I admire greatly.
So here's what I'd like to just throw out there and see, Michael, you probably you may
have seen my depiction of a tree of knowledge diagram for sort of upside down cones as it
were, sort of emerging out of energy information, singularity source, sort of a one world naturalism
from a big history view.
And it's really an expansion of complexification.
But something happens at different points of it that result in a qualitative shift.
In particular, a life plane of life emerges, a plane of what I call mind animal and culture
person.
And it took me a long time to think about what these planes actually were coming off
of in particular matter.
The matter complexification seemed OK, but I wasn't sure exactly what was happening with
these life, mind and culture cones for a couple of months, years after I developed it.
And then I basically was like, oh, it's a complex adaptive system network together through
information processing and communication systems that are affording particular potentialities.
And they're then mediated by certain kinds of systems, gene cells, but it's simply nervous
system animal, propositional language, collective human intelligence.
I was enormously struck by your cognitive light cone analysis.
And one of the things I really wanted to talk with you about was how you conceive of a cognitive
light cone, how you conceive of intelligence, the emerging evolution.
And if there's a relationship between that cognitive light cone idea and the cones that
I am depicting in terms of emerging complexification through information networks, exploration
of design space.
And then what I'd like to do with that is connect, if that's the case, connect the edge
of that and the contact of that with recursive relevance utilization as a dynamic process
that we can at least apply to see how these things emerge through what, how do we understand.
So I'll throw that out there and see if you would then, you know, riff off of that and
then pull John in in relation and then see where this thing might explode after that.
Well, I guess, I guess I should start by just describing this cognitive light cone idea.
And I should preface this by saying that the version of it that's out now, which I think
what I published in 2019 or so is very much a 1.0 vision.
So it needs significant improvement that much is clear and I'm working on that now.
For what exists is it's the following.
We were, I was actually at a Templeton meeting, a conference of people studying diverse intelligence
and things like this and Pranab Das tasked us with a challenge of trying to come up with
a framework in which you can simultaneously think about truly diverse beings.
So we're talking about not just the familiar apes and birds that we're used to and not
just, you know, maybe an octopus and maybe a whale or something like that, but, but like
really diverse.
So we're talking colonial organisms.
We're talking synthetic biology beings that are going to be, that are and are going to
be made cyborgs, AIs, whether software or robotically embodied, hybrids or some sort
of combination of living material and engineered the material of possible aliens.
I mean, all of it, right?
So, so, so, and I've been thinking about this for a long time.
And I sort of took that as an opportunity to try, okay, let's, let's, let's try to formalize
some of this.
How do we do that?
And what I thought was really fundamental to any agent, any, any, any being that we're
interested in is the scale of its goals.
I thought that goal-directedness, right?
Some degree of it.
And I'm not, you know, I don't like binary categories.
I'm not, I don't believe that there is a thing as goal-directed or non-goal-directed.
I like Norbert Wiener's sort of cybernetic scale that goes from passive matter all the
way up to, you know, sort of human metacognition and, and whatever is beyond that.
And so, and so I thought, okay, so let's just, for each one of these potential beings, let's,
let's map out what is the largest goal that they could possibly pursue.
So this, and so, and so the obvious, and so you collapse space and time onto a two-dimensional
sheet.
And so you get this thing that looks like Mankowski's light cones and, and so the size
of the goals, right?
And so you can, you can start to think of different cases.
So, so if I ask you what you care about and you say, I care about sugar concentration
within this 10 micron radius and I have a memory that goes back about 20 minutes and
I have predicted capacity that goes forward about 10 minutes, you're probably a bacterium.
And you know, and if you tell me that you care about things that happen within a, I
don't know, 100 foot or 100 yard radius or so, and you've got some memory going back,
but you're really never going to care about what happens three months from now in the
next town.
I'm going to say you might be a dog, right?
That kind of thing.
And, and if you've got goals that are planetary scale goals about the way the financial markets
and world peace are going to look 100 years after you're gone, you're probably a human.
And, and if you tell me that, that you can actually care for in a linear range, you know,
thousands of millions, all, you know, sentient beings that I'm going to say you're something
beyond a standard, you know, to modern human.
I don't know what that is, but you know, we, we, we can't really do that yet.
So, so that's, so that's the idea of these cognitive icons.
And there's two kind of two things that I'll just say about that, then I'll stop.
One is that I think these, these cognitive light cones interpenetrate.
In other words, in the, in, let's say, let's just take a human body, for example, there
are many, many subsystems that have their own inner perspective and their own goals
that they're following in various problem spaces.
That doesn't just mean three dimensional space, right?
Your body is home to all kinds of structures that live and suffer and strive in other spaces,
physiological state space, metabolic space, transcription, gene expression space, anatomical
space, if you're an embryo or something like that.
So we're not very good at recognizing these and, and, and, and there are many of these
that, that cooperate compete and so on, all at the same time.
And that leads us to the second point, which I think is pretty critical.
I, I like this notion of, so, so we talk about goal directedness.
I like this notion of teleonomy and teleology, of course, being goal directedness.
And then there's this concept of teleonomy, which is defined as apparent goal directedness.
Now, some people use that word to soften the impact of teleology.
They say, well, look, it's not really teleology.
It's just, it's just apparent teleology, right?
I'm not using it that way.
I, I, I am full blown into teleology.
I think it absolutely is, is a necessary concept to, to have proper understanding.
What I think is important about teleonomy is this, it is, in fact, apparent goal directedness
because it reminds you to take the perspective of some observer.
There is some observer who has to set, who has to make hypotheses about what they're looking at.
Now that observer, in terms of what problem space is this agent operating in?
What are their goals?
What degree of competency do they have to reaching those goals when situations change?
All of this, all of these are, are hypotheses from the viewpoint of some other being,
or in fact, the system itself.
So, so once you're past the certain level of, of advancement on that spectrum,
you too tell story, internal models about yourself.
You have a model of yourself as an agent.
So, so, you know, parasites, you know, scientists, right?
As external observers, parasites, conspecifics, predators, and the system itself all have
these perspectives on things.
And so, so I think keeping that apparent in mind that all these things are not, I think,
not objective kind of universal truths, but actually some observer trying to make sense
of the world as they look at themselves and other things.
So that, that to me is, is, is the idea of these, these light cones.
Lovely.
And would you say, would it be fair to say then, let's say, you know,
life gets started or whatever four billion years ago and then explodes,
we would actually see in the universe, at least on planet Earth,
essentially an emergence of life light cones, right?
In, in relation, would you say that?
Yeah, I think that, and I know it's weird for a biologist to say this,
but I'm, I don't think life is a super interesting or discrete category.
I think that, that it's, that, that what's, what I think is more interesting
is cognition, the spectrum of cognition and a wide, a wide, you know,
a wide range of those things are overlapping.
So certainly the cognitive, you know, if you think of a Venn diagram,
right, the cognitive circle and the life circle overlap quite a bit,
but I don't think they're the same circle.
And I think you can have things that are on the spectrum that currently people
would not call alive, which is why I'm less interested in that, that characterization.
I do think that one thing that, if, if I had to give a definition of life,
which I don't do, but, but if I had to, what I would say is that life is what we call
things that are really good at scaling up their cognitive light cones.
So if you have a collection of pebbles, which are basically only good at
sort of energy minimization and things like that, and, and I, by the way,
I don't think that's zero on the cognitive light.
I think it's very low, but I don't think it's zero.
But when you have a, when you have a rock made of those pebbles,
you have not scaled the cognitive light cones, got exactly the same capabilities.
But once you have life, what you find out is that it's arranged in a way where the
components have little tiny light cones and the collective has a bigger light cone,
a bigger cognitive light cone that, that actually extends into new spaces.
So when we see that happening, when we see goal directed systems being multiplexed,
so that the goal, the, the size of their goals that they get, they get these grandiose, you know,
longer term spatially and temporally goals, we call that life.
I think that's what, that's what life is.
But, but I do think that, that things that we would be hard pressed today to recognize life
as, as life can, can have cognitive light cones and maybe large ones at some point.
All right, I'll pause and see if John wants to jump in here.
Well, yeah, there's a lot I want to talk about there.
I want to, I want to build off of Michael's idea of light cones,
which I do mention in some of my lectures at U of T, University of Toronto.
I want to note that there's two, at least two parameters within a light cone,
as I understand it, there's reach and clarity.
And I think that brings in some of the work I'm doing about cognition,
in which I talk about the two meta problems of that activity.
So if you're going to be a problem solver, there's two problems you're always solving
as you try to become a more adaptive problem solver.
One is anticipation, your ability.
And I don't just say prediction.
I think that's a little bit of a misnomer.
I think if you predict and can't prepare, that's not very adaptive.
And we have experimental evidence for at least living creatures.
