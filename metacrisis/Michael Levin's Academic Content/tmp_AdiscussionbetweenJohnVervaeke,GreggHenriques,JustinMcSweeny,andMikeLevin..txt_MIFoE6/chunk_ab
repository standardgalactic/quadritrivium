That's the case, right?
So I use the term anticipation.
So do you want to anticipate as deep use Lee as you can?
Typically it enhances the, the number and the kinds of problems you can solve.
Because the earlier intervene in a causal pathway for a problem often,
not always, but very often the easier it is to solve that problem.
It's much easier to avoid the tiger than fight the tiger is my sort of slogan.
And that's this idea of the light cone.
But the problem with that, which is the second meta problem is as you increase the reach,
you increase the problem that has been the besetting obsession of my career,
which is the issue of relevance realization.
The amount of information that you have available,
the amount of information you have to store,
all the possible combinatorially explosive combinations goes up exponentially.
And you have the problem that you can't just arbitrarily choose from that,
what to pay attention to, you can't algorithmically search.
And so you're somewhere between the arbitrary and the algorithmic.
And this gives you the issue of relevance realization.
I have proposed a way in which the two theories,
because the two problems depend on each other,
you don't, you can avoid relevance realization,
but what you do is you shrink your cone of anticipation very considerably.
And then if you wanted to increase your anticipation,
you increase the relevance realization problem.
To make a very long, a complex argument as brief as possible,
it's something like the predictive processing is trying to always minimize error.
It hits inevitable tradeoff relationships of error.
If it tries to reduce bias and increases variance,
if it tries to reduce variance and increases bias,
if it tries to reduce the errors of exploring,
it will crash into other errors of exploitation.
And there's all these inevitable tradeoff relationships.
And the idea is the predictive processing is going to create these
opponent processes that give what's called an optimal grip on the world.
And that's what I mean by clarity.
It's not just that you reach out well,
but you know how to optimally grip what falls within your light cone.
And that's how I think those two go together.
Now, what comes out of both the recursive relevance realization
and especially the predictive processing is this idea of mutual modeling.
In predictive processing, you always have to model yourself.
I don't, I don't mean model yourself as a self.
Please hear that.
But you have to model yourself when you're modeling the environment,
because you have to deal with conflation errors,
that stuff that's happening because it's inside of you
is getting projected onto the environment.
This is, and so you're always trying to model the self to some degree
to discount the errors being caused by your own embodiment.
And so this is the great insight the predictive processing runs off.
Don't try to directly predict the world,
predict yourself interacting with the world.
And that will get, help to solve those problems in an interlocking fashion.
And so what you get is you get, when you're modeling the world,
you're always to some degree modeling yourself.
And as you're modeling yourself,
you're always to some degree modeling the world.
The two are interpenetrating.
And I think that goes a lot towards the teleonomy
that Michael was talking about,
that there is something like a self modeling going on.
Now, for me, and this might be where Mike and I are different,
I think that that self modeling and relevance realization
depends on a system in some sense taking care of itself.
My argument is to the effect that relevance realization
is always caring about this information
rather than caring about that information.
And care, and I'm not meaning the experiential affect
I'm trying to use as a very broad, almost Heideggerian sense.
Your caring for yourself is what gives you the capacity
to genuinely care about this information or that.
This information matters to you, that information doesn't.
Initially, because perhaps that matter actually matters to you.
You literally have to take it in or you're not going to continue.
And so I think that relevance realization grounds in autopoiesis.
And so that's something we can talk about.
I do think that life does represent a significant capacity change.
And we can talk about whether or not
there is cognition without caring.
Or maybe you have an analog for caring going all the way down, Mike.
And I'd like to hear that.
As you know, I'm very interested in this deep continuity.
I would put one thing to you
that's a little bit more of an abstract level, two points.
And then I'll stop talking.
One is if we all are sort of non-reductionists,
and if you have a continuum with non-reduction,
differences of degree eventually become differences of kind.
Because with non-reductive continuums,
you have to have properties at upper levels that aren't in lower levels.
And so I think you get real emergence.
And I think that's a difference in kind.
And I think that is a way in which your continuum
and Greg's series of cones could plausibly mesh together.
Here's my final point.
And this is the point that I've been also doing a lot of work on.
And Greg and I did a lot of it together
on our transcendent naturalism series.
Is as we start to get this understanding of cognition,
we see it as properly transjective,
always between the system and the world,
always between the organism and the world.
And that means these discoveries about minds are also discoveries.
They're ontological discoveries about something
about the structure of reality itself.
And those two have to be understood together,
how we are understanding.
I get it as a continuum, but we talk about it in levels.
And I accept that distinction.
The levels are properly epistemic.
The reality is a continuum.
But what I mean by that is as we find levels in the mind,
unless we're willing to bite the bullet
of a profound solipsism and skepticism,
we have to say that there's something corresponding
in levels of intelligibility in the world.
And that's an ontological claim.
And for me, that means we are deeply committed
to a different kind of ontology and the flat ontology
that we have been doing science in for quite some time.
And I won't belabor this, this is some of the deeper recovery
of an older neoplatonic ontology
rather than the sort of flat ontology we've been with.
And I think this is important because I think that
can ground a spirituality that is not just about
psychological hygiene, but about genuine epistemological
and ontological realization.
So that's what I have to say about that.
I hope that made sense.
That was really compressed.
I'm trying not to hog up all the time,
but Mike, you always say a tremendously provocative things
and I wanted to respond to them in kind.
Yeah, thanks.
That's great.
I don't disagree with almost any of that.
And I think that especially, and I mean,
the whole kind of some of these platonic ideas
are really starting to come to a fore in some of our work.
I haven't written too much about it, but I will,
as the arguments get better and so on.
So I'm on board with pretty much all of that.
I do, I'll just say one thing
about the kind of continuum of business.
And then I want to talk about another,
I want to add something to what you said,
which is very interesting.
One thing about, here's how I think about this difference
in terms of when differences in degree
become differences in kind, right?
And this is why I called my framework
tame as in technological approach to mind everywhere,
because I really want to ground it,
not because technology encompasses everything
that there is, obviously that's not the case,
but the technological approach,
I think is interesting for the following reason.
Let's just imagine the paradox of the heap, right?
So you got a pile of sand
and you start taking the grains off and you say, well, when?
So here's what I think all of these claims are,
including any kind of cognitive claim,
any kind of a claim about what systems can and can't do
in terms of intelligence and all that.
I think these are all interaction protocol claims.
They're engineering claims in the sense
that what you're really saying
is here is a way I can interact with that system.
So for example, let's talk about the heap first.
If you tell me you need to move a pile of sand,
I don't want to really know whether it's a heap or not.
Here's what I need to know, am I bringing a spoon?
Am I bringing a shovel, a bulldozer, dynamite?
What are we bringing?
And there will be lots of scenarios
in which actually either one, right?
A big shovel or a small bulldozer will do.
And so I think all of these things are fundamentally
around a claim about what is the right way to interact with it.
So when you tell me that a given system
is somewhere on this spectrum,
I'm less interested in finding sharp categories
and looking for kind of emergent new phase transitions
and things like that.
I'm much more interested in the question of
so what tools are you telling me are going to be appropriate?
So if you're telling me something is a simple machine,
I understand it's rewiring and hardware modification
and that's all you got.
If you tell me that it's a cybernetic thing,
I'm thinking, ah, so I've got tools of resetting set points
and other aspects of cybernetics.
If you tell me that it's a learning agent,
I say, okay, I understand we have training,
all the things that behavioral science can do.
And if you tell me that it's at the level of,
let's say human or above discourse,
I say, ah, that means that I have certain other tools
and also I may be changed by the encounter.
In other words, unlike with a simple machine,
