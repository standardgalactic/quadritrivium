because it maps to one, four plus one is five,
that maps to two, et cetera.
So we keep mapping outward.
And what you see is that this actually behaves
like a ripple does in the real world.
If you had an infinite sized pool,
you dropped a stone in it.
And if there were no entropy,
you would just get it rippling outward infinitely
according to the initial pattern.
That's what we expect least action
to look like in the physical world.
And that's what doing things lazily
looks like in the mathematical world as well.
So this is what our mapping looks like visualized.
And this does in fact tell us
that the free monoid is optimal
versus the expensive monoid
because the free monoid contains the expensive monoid.
So what can we do with the free monoid?
Well, we can count linearly,
one, two, three, four, five, six, seven, et cetera.
But we can also count in a loop
because that's what this pattern is giving us.
We can see the looping pattern.
We can count with loops with this free monoid.
So it can do the job that the expensive monoid can.
But the expensive monoid cannot do the job
that the free monoid can
because it can only count in a loop.
It doesn't count linearly.
So the free monoid is like just able to do more tasks
than the expensive monoid does, it contains it.
And so even though we built it lazily,
it actually ends up being optimal.
So what have we learned about adjoint functors?
Adjoint functors are lazy ways
of translating between structures.
So we have different mathematical structures
like monoids versus sets.
Adjoint functors allow us to map between them
in lazy ways that create or destroy information.
When we do things in a leased action way,
adjoint functors end up giving
a sort of universal solution to the problem.
So like if we want to count by one, that's a problem.
What's sort of the, there's many ways we could do it,
but what's like the most sort of universal
or formulaic way of doing that?
Well, it's this way of doing that
because we can do looping if we want to,
but we also have linearity
which these other expensive versions don't have.
Something that's really interesting about adjoint functors,
just like in the physical world,
there's so many things that you can define
in terms of leased action.
Well, the same thing works in math.
Adjoint functors arise everywhere
as a common saying in math.
Many important mathematical things
turn out to be adjoint functors.
They turn out to be lazy ways of doing things.
And I don't know much biology,
but it just kind of seems to me that free monoids,
I kind of like the DNA of monoids
to give you the relational structure
to do all different kinds of counting,
linear counting, looping counting,
all kinds of different patterns.
So we have the relational hardware
to build any specific monoid if we want to.
So that actually brings me to the conclusion
of the math portion of this,
which is what we've seen so far,
which is that we have trade-offs in math
and we have scarcity, therefore, in math,
which happens because of logical consistency,
not physical limitations.
So as soon as we can commit ourselves to being consistent,
we end up committing ourselves to making trade-offs,
which is what we need to see agency.
And what's fascinating about this is that the agency of math,
it's not something that humans just use
to solve our problems in the physical world,
we can define problems in the mathematical world,
like how do you cast projections to A and B?
Or how do you count by one?
And we can find optimal solutions to those problems
that are optimal for math itself, not for humans,
but just in purely mathematical terms,
it ends up being optimal.
And optimality often ends up relating to laziness,
we can be formulaic and just turn our brains off
and do things in the most obvious way.
So when you're doing math, like as a mathematician,
it's often very useful to use a genetic concept.
So just like you can settle like debates about free will,
like, well, it's useful to think of myself
and making choices of like,
buying something in a restaurant.
Similarly, if you're doing math,
it ends up just being very useful to be like,
if I was this set here,
how would I want to solve this problem?
As someone who is currently trying to learn math,
I do find that perspective useful.
So math ends up doing agentic things,
behaving like an agent.
This isn't necessarily like hard proof that math is agentic,
but it does show that thinking of math in an agentic way
seems to work, there's no problem with it.
So why not try thinking of it that way?
Now that brings me to timeless agency
and I see that it's been about 40 minutes,
which means I probably talked way too fast.
So let me actually pause here
before we move on to timeless agency
and just see if there are any questions
about just what's going on, what am I saying?
If anyone just wants to pause here,
if there's just something
that really doesn't make sense or is niggling at them,
please feel free to ask
and otherwise I'll move on to timeless agency.
Seems like we're good so far,
so thanks everyone for bearing with me through this.
Let's move on to timeless agency.
So there's an obvious problem
with thinking about math as agentic,
which isn't that math doesn't do anything.
Anything that math does, a human does,
like we have to write down two plus two equals four,
the math doesn't write itself.
So humans can move around, math just holds still.
So how can we talk about agency if we don't have time?
The argument I'm going to make
is that we can think about agency in terms of predictions,
like free energy minimizations,
all about making predictions.
What a prediction does is something
that preserves structure through time.
Like if I'm predicting the weather,
I've got some structure, some models, some data,
and I can predict what it looks like in the future
as we preserve that structure.
And so what I'm going to ask is,
well, what if we just drop the time part?
What if we just talk about structure preservation
without time, because that's what math is all about.
It's all about structure preservation.
So much of math is just about structure preserving maps
from one object to another.
So what if we just drop the time portion of that
and just talk about structure preservation,
then we can talk about math as being making predictions,
which seems to be the heart of agency.
So what does timeless prediction look like?
Very simple example, let's say that you have two apples,
and I tell you that I'm going to bring you three more apples.
Well, you're going to predict
that you're going to have five apples.
But the core, now this is a prediction
that takes place in time, right?
You're predicting what you're going to have in the future.
But the core of this prediction, no pun intended,
is two plus three equals five,
which does not have a temporal aspect to it.
It's just one equation.
It's not like the two and three happen first,
and then you have five, it's just one thing.
So we're using timeless things to make predictions
through time, two plus three equals five
feels like a prediction kind of thing.
It's just there's no time element to it.
So let's look at an example we've seen before,
free monoid versus the expensive monoid.
Can we use this free monoid to make predictions
about the expensive monoid to know things,
what a prediction means is we're going to look
at the free monoid, and we're going to use it
to know things about the expensive monoid.
That's what a prediction is using one object
to gain knowledge about another object.
Well, yes, we can make this prediction,
which is with this mapping we've seen before.
We can use this mapping to know how this is going to behave
without actually having to look at the expensive monoid.
We can use our free monoid to make predictions about it.
So this tells us intuitively,
prediction is all about structure preservation.
We have a structure preserving map
from our free monoid to our expensive monoid.
And that's what allows us to make predictions without time.
There's no time happening in this prediction.
We're not making predictions about the future.
We're just making predictions
in the timeless mathematical world
with structure preserving maps.
So prediction is structure preservation.
