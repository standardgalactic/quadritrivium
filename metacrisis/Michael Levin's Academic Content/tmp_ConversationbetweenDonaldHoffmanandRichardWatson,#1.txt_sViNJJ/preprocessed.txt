So I'm pitching for cognition first theory of evolution.
Okay.
The usual orthodox understanding of the relationship between cognition and evolution is that cognition is a product of evolution.
And evolution by natural selection comes first and it sometimes will be it rarely produces things which are cognitive.
And I'm considering the reverse that cognition comes first and that in special cases sometimes perhaps rarely produces natural selection processes.
So the, you know, obviously that has some, you know, the words are slightly different, but it has some resonance with a consciousness first sort of theory of things.
But I don't want to end up with a model of things which has a sort of a strong physical stroke non physical distinction between things.
I would rather have a theory where there's a graded scale of things which are more physical and things which are less physical.
And that there are so there are different multiple different levels of causes, which are each of which is slightly real to the level see the side of it.
How are we doing so far.
Hang it in there.
Let's see how this goes.
All right.
So, one way.
One sort of motivation.
Let's do the motivation first right so I'm trying to keep things tied to biology and answer questions that need answering in biology.
And in biological systems, you know, the sort of the orthodox view that genes control everything at the bottom and everything that organisms do above that.
The regulatory activity, the cellular activity, the tissues and organs and organisms are all just products of the genes and none of the processes which are going on at those higher levels of organization matter except in so much as they are consequences of genes which affect
infection.
Is there, you know, everything at the bottom is the only level that matters.
Whereas, in reality, we know that there are self sustaining causal processes going on at all of those levels of organization.
You know, the cells are at the level of organization of cells, real things that interact with other cells with certain kinds of signaling that create self sustaining conditions for cycles that recreate the conditions for their own
information at that level of organization.
It's great to be able to say something like that and have somebody nod.
The thing about, you know, that's true in physical systems too, but you know, from, you know, quarks to cosmos, but the thing about biological systems, organisms is the connections between those levels and the relationships between them.
So the levels below smaller scales create the entities that get involved in the relationships at the higher level.
And the levels above create the context or boundary conditions in which those entities move or interact with each other.
So the level above and the level below is needed to define whatever is going on at the focal level that we're talking about.
But what's the relationship between those levels because it isn't, we're not satisfied with the story where the bottom level determines everything.
Right.
Because it doesn't determine the boundary conditions under which those entities move.
And we're a bit uncomfortable with starting at the other end that, you know, as though, because it seems to imply that there was a plan into which everything should fit.
A plan that created all of the parts that was necessary to make, you know, the high level thing that was intended in some sense.
So, let me see if I can stop my laptop pinging a second.
So, I want to suggest that the, that there's a, that the way in which those levels interact with each other has a specific general form that's to do with compression and expansion between levels.
And that when levels are linked in that way, the interaction between levels is what agency is interaction between levels is what cognition is.
And that that kind that kind of multi level structure arises spontaneously and naturally without presupposing any of this particular biological machinery.
And that it's a natural property of physical systems and that it's cognitive in nature so that it's capable of holding memories doing learning and using learned knowledge to act in the world in a way which constitutes intelligent problem solving.
So, I drew a little picture today riffing off a picture from your work, Don.
Okay.
Can I, let's find that document.
You'll note, I'll share it in a second, but you'll note that these are hot off the press because I drew them with the pen and paper and took a picture of them I haven't turned them into a, into a electronic form yet.
Let's see.
Share the screen.
Okay, can we see that.
Yes.
Okay, so in figure a, we have an agent separated from the world.
And the detail of the world is sensed by the agent into Percepts.
Sort of decision process cognitive process happens which turns that into a decision. And then the decision.
Organizes the actions back into the world. So that's inspired by your way of decomposing things don't make sense.
Okay, so now I'm going to turn that into a slightly more computational way of thinking about those relationships.
So out there in the world we have some system of entities with interactions between one another which is all sorts of complicated.
That detail is compressed into a lower dimensional model of what's going on in the world.
You run that model.
A little bit.
Things happen.
And the results of running that model and then expanded back into the world.
Information integration run the model and then collective action transferred back to the world.
So I think that, you know, from a sort of computational cognitive science cognitive intelligent agent way of thinking about things we would say that a system that did that was doing something.
And what you mean by being able to do something essential is that you have an abstracted view of the world that you can run forward in time.
Faster than the real world.
And then take actions as it were in advance of the things which were going to happen in the real words that you anticipate what happens in the world rather than reacting to it.
Okay.
So now I want all of that to just happen spontaneously for nothing for free.
That's not something that requires you to build, you know, a neural network or a deep order correlator or something like that, or to evolve machinery that does it that that's just something that happens for free in the natural world.
Okay.
And the way I'm going to get there is through harmonic resonance.
So in figure C, we have one causal process at the top of the figure, a cycle, you know, just a cyclic attractor something's happening in the world. That's a cyclic attractor.
And the thing that's happening in the world looks like there's different things, you know, there are black particles and white particles and they're not in the same space at the same time.
You know, there's stuff happening in the world.
But it turns out there are symmetries in the world that in the sense, you know, the black particle and the white particle are different from each other that different particles and different points in space and time.
But really, they're not so different from each other because they're also both particles both in the same space of entities.
They're in a sense a reflection of one another.
They might be not the same, but they're also not different in every respect.
And that symmetry means that the cycle, the causal loop that they're in can collapse, twist, collapse, twist and fold in such a way that it creates a loop of half the size where everything is going round twice as fast, a doubling of the frequency.
That's the model where the symmetry between the black particle and the white particle has been collapsed into that's just particles.
Like this is a general model of particles. It doesn't care about whether it's a black particle or a white particle.
I can run that general model, turn that general model through time to see what will happen in the next time step faster.
Because it's a higher frequency model like I go around that once I've actually gone around in the real world twice, right?
And as that loop then unfolds to unpack the symmetry that was in it to split that symmetry back out into an unfolding and connect it back to the world.
I've made something which is influencing the real world in a way which you can think of as anticipating what would happen.
But you can also think of it as, well, these are just two symmetries in the world which coexist at the same time and influence one another in both directions simultaneously.
Ah, still nodding. Okay.
Yes.
All right.
So now I'm just going to do that through multiple levels in figure D.
Right. So it's, you know, you start with, there are many different things in the world. You fold them up once.
That creates a sort of lower dimensional model. You fold them up again. That creates a lower dimensional model. You fold them up again all the way down, all the way down to what?
At the bottom of everything, it's like, everything is the same thing.
Like there's only one electron in the universe.
And time isn't even really a thing. Like nothing really changes. Everything's the same.
Nothing, there's nothing actually happening.
And as you come back up the other side unfolding, unfolding, unfolding, unfolding, all of the symmetries are broken.
And all of the complexity and diversity of physics, life, the universe and everything is recreated back again.
All of those levels are there simultaneously. And all of those levels are connected.
But they're not connected in an arbitrary way. They're connected in a specific way, which is to do with folding of removing one dimension from the space at a time.
Folding it in a line of symmetry that retains as much of the local integrity that you had at one level is maintained as you fold it into the next level.
And that in musical terms, each fold is a two to one octave, a two to one relationship.
So I don't think that this is just a model of what happens in physics, right?
What, you know, super symmetry stuff that happens in quarks and stuff.
I think that that's the stuff that happens at that kind of level of physics is the same as the stuff that we were talking about in figure A.
And that, you know, that arises spontaneously even when you do something simple like bow a violin string.
The violin string is imbued with agency when you do that.
And its ability to act agentially on the world is not very much, but it's not nothing.
And I can tell you the story about how it does that if you're ready.
Sure.
All right.
So let's say that the violin string is not vibrating to start with and we start dragging the bow along the string.
What's happening?
There are microscopic disorganized interactions between the bow and the string.
They're not in phase with one another.
They're not at the natural frequency of the string.
They're, you know, they're far away from the natural frequency of the string, but they're putting a little bit of energy into it, almost like, you know, just sort of heat at that stage, right?
So they are going to start resonating in the string.
The resonant frequencies that build up in the string, notice that they involve top down and bottom up causation at the same time, because the frequencies that build up in the string depend on the macro scale geometry of the string like its length.
And they also depend on the micro scale properties of the material that it's made of like the elastic bonds between the molecules at that particular tension.
And those bottom up and top down determine which frequencies build up in the string and which frequencies don't.
And the energy which is going into the string is converted into organized oscillations which stack up and form simultaneously at all of those levels.
What's interesting, though, is that in so doing, the string pushes back on the bow, and it changes the nature of the interface between the bow and the string.
How does it do that?
Well, what you needed in order to play the fundamental frequency, this massive long wavelength frequency that you get when the playing is when the stroke of the bow is in full flow, as it were, when the string is singing loudly.
What you need is to convert the linear motion of the bow into a reciprocal motion of the string.
Like, how do you do that, right?
The bow is just pushing it, but it needs to push the string when the string is going that way, and it needs to not push the string when the string is coming back.
So the string is organizing the stick and slip dynamics with the bow.
It's allowing the bow to push it when it's stuck.
And once the tension builds up, it slips, it joins to a different part of the bow, a little tick backwards, which then drags it forward again in the right direction.
So that you have, in a sense, a percussive motion of the bow.
The linear drawing of the bow is turned into a percussive motion of the bow, almost as though the string doesn't see the whole bow.
It only sees intervals on the bow that are the right distances apart for it to drive the fundamental.
So the string is organizing the way the interface that was providing the energy to give it energy at the right intervals that sustain the fundamental.
Once the fundamental is going, it's sort of obvious that it does that, but it had to create the fundamental in order to push back in that specific organized way.
The disorganized energy is converted into an organized energy that controls in a quantized way the interaction between the string and the bow in such a way that it sustains the fundamental, which wasn't even there to start with.
And I think, but I haven't heard anybody say it, that the agency of the string is of the, or rather of the stack of harmonics in the string is nonzero because it has the ability to compensate for small fluctuations in the stick and slip dynamics that are needed to drive the fundamental.
So if a slip, if one of the slips was a little bit too long or a little bit too short, the next slip would be just right to compensate for that, because the string will take energy from the higher frequency dynamics and push them convert them between those levels to do the right amount of compensation for the next stick and slip.
Yep, still not him.
Sounds good so far.
So the, what I'm, what I'm suggesting then is that living things are resonators like the string is, but they have better sustain.
Why do they have better sustain because they're more agential, why do they more agential because that's what it means to be more agential is to have better sustain it means that you have the ability to convert energy between different levels of organization.
Because that converting energy between different levels of organization is the agency diagram that we started with in a right, it's the ability to abstract the world to take something which is happening at one frequency and convert it into another frequency.
Run that model of the world forward in time.
And then act on the world based on the results of running that model as it were.
The reason that organisms have better sustain than strings is because you can set up multiple different harmonics in an organism and not just the octave relationships right when you skip levels with the with other intervals like the third or the fourth.
You're making bindings between different levels which converts energies from one level to a skipping a few levels to another level more easily and quickly which makes them more powerful more agential.
It's the same thing as saying that they have more internal geometry to hold on to and more internal energy to deploy in maintaining that geometry.
So one living organism, which we notionally think of as being created from a genotype is really an unfolding of compressed information into multiple different instantiations in the distribution of population right and selection we can think of as the folding process, which collapses the information from that distribution back to genetic modifications.
But that expansion and compression in biology in conventional biology we think that that has feed forward feed forward feed forward feed forward up to phenotype and then feedback feedback feedback feedback just in one step right the organism lives or dies and that's it but it's not like that.
That that feed forward and feedback is happening in the construction of the intracellular organelles from the genotype, but they are already pushing back on the genotype to say which genes should be expressed.
And in the construction of the cell from those organic molecules, but the cell is already pushing back on the organic molecules which are pushing back on the genes from the cells into the tissues, which are differentiating into into tissues, but it's the organ which is already controlling how the cells differentiate
which is controlling how the intracellular components metabolites and proteins are operating which is controlling how the genes are being expressed which is even modifying how the gene sequences, all of those different levels are cells are agents in the process of development.
And just as much as the organism as a whole is, and the feedbacks are happening at multiple timescales and multiple organizations, not just in that sort of conventional loop.
That's it.
That's wonderful.
That's, I agree that I can make a few few comments and please.
So, at the very start you talked about, we needed to have something that was going to be, you know, effectively computationally universal, right so that it can do all this stuff, right.
And yet we want, if we want to turn these ideas into a mathematically precise theory we need some kind of precise, but minimal mathematical structure to try to capture your ideas, right.
And the, the simplest, and yet completely general, except for one little exception we can talk about model mathematical model for these kinds of probabilistic interactions that we're talking about would be Markov Markov kernels right that would be they they are the minimal
mathematical way to talk about these probabilistic interactions and it's trivial to show that Markov kernels are computationally universal, so you networks of Markov kernels can do anything that neural networks can do.
Now, it's the reason I've been nodding is that we're writing a paper on this using Markov kernels and this is the papers.
I've been writing it just the last few days last actually a few weeks and probably be at it for another month or two.
We're going to call it traces of consciousness.
And there's a key idea that that really is a mathematical concomitant of what you're saying here. And so what we've discovered is a new property of Markov kernels.
They are partially ordered. There is a heterarchy of these kernels, just like you have a heterarchy of these interacting agents.
And the heterarchy has a very clean definition.
So, if you have a Markov kernel.
And just to be precise in case somebody out of this group looks at this video right we may post this and so forth so suppose also be very concrete suppose I have a kernel that's a 10 by 10 matrix, right.
And to be a Markov kernel each row consists of numbers between zero and one, and the sum of the numbers in that row is one.
That's that's what I mean. So it's a Markov kernel very very simple square matrix each row sums to one. They're all positive numbers between zero and one.
And if you if I, I can run one of these kernels right I can see if it's on 10 states I can see, you know, if it's in state one, what's the probability it goes to state two through 10 and so forth, or if it stays at state one I can just so I can look at this dynamics.
And ultimately, if it's got a kernel, I can find a stationary measure I can see long term probability of being in state 123 through 10 right.
Now so I can say, I only want to look at states one two and three. So I'm running the bigger kernel, but I'm ignoring I'm only attending to states one two and three.
So if I only do that, what is the Markov kernel that would apparently see only involving one two and three right which would be induced by the bigger one.
And in technical terms is called a trace chain. So in math, so there's a formula for it.
I can do it. If you want, I can put the formula up on the screen but it's, there's a formula for it. It involves the mathematics involves looking essentially at infinitely long sequences of of chains of the big one, and seeing how often they go out of this three that you're interested
to go and then go into it and so forth. But but it turns out if you look at that infinite sequence, you can give a closed form solution for so you don't have to do anything infinite you can do a closed form solution that captures this infinite sequence.
So, so here's the, the, the partial order. It's defined by one kernel M is less than or equal to another kernel N.
And if M is a trace chain of N, that's it. That simple.
It gives you a non Boolean logic on every possible dynamics.
In other words, all possible dynamics are now given a an order relationship and a logic. It's a non Boolean logic.
There's no global greatest element.
And that has many, many elements are incomparable.
So, you know, many dynamics are not less than or equal to each other. They're incomparable.
But if you take one kernel.
And you look at all of the kernels that are less than that one kernel.
And if you look at the subset of dynamics, they are a Boolean logic.
So you get a Boolean logic.
So what this allows is, and by the way, when you, when you take a kernel like this 10 by 10, and you look at the three by three that comes, you know, from trace, and you look at the probabilities on those three states, the transition.
They're utterly different from what they were on the 10 by 10. As far as you could tell, this is an utterly new kernel.
If you were thinking about the probabilities as free will, the free will decisions of the three by three are utterly different in their probabilities than the free will decisions on the same states of the 10 by 10.
In other words, they can be not they have to be right.
In fact, they have to be different in general.
In general, they will be different with probability one, I think they'll be be different. I haven't proven that but just intuitively it seems like with probability one, they will be different.
So what you get is this really from one trivial definition and less than M.
If N is a trace of M, that's it.
This whole beautiful logic falls out. Now, of course, these kernels have if you look at their eigenfunctions, you are going to find their vibration rates.
And in fact, in the paper that that we're publishing, I'm looking at what you can do is take these kernels, a regular Markov kernel, and just add one little feature to it, which is standard in Markov chain theory.
You add a little counter that increments every time you take a step of the chain. So you start off at zero and it goes one, two, three, four.
So you have the normal chain, but you then have this quote unquote time parameter, which you add to it, it's called a space time, and you can now take the the eigenfunctions of that space time kernel.
Okay. And when you do it, what you get is is a function which is identical in form to the quantum mechanical wave function for a free particle.
It's a momentum eigenstate for free particles.
Exactly. So we're going to show this like I'm, we'll give examples.
So if you want vibrations, you got vibrations here, you got all possible vibrations. You have an organization that good that a logic that ties all of them together in one beautiful symphony.
And it there's no single greatest at the top. This is a heterarchy. It's really, really quite interesting. So, I mean, if I was to go spiritual and say, instead of saying there is the one, say one consciousness, there is, I would have to say the whole, because there isn't just the one, it's it there is no
greatest element, there is something far richer than that.
So, I was nodding as it was going through because as you were going through all the stuff, I was ticking off on this back. Yeah, you can do that. Yeah. Everything that you're you were saying, here is a mathematical way to do it.
Now, I said, there was one proviso.
Here's the proviso.
Markov kernels are universal for any
probabilistic process that has a finite memory.
Now that finite memory can be as big as you want. In other words, like when we define a Turing machine, right, we say there's a tape, and the tape is as long as you need it to be.
And as everybody goes, okay, well, I don't care about that. There's no restriction here. It's the same thing with Markov chain. Yeah, it's fine, but it's as big as you want it to be just like the Turing tape can be as long as you want it to be.
So, in practice, it's universal.
In practice, it's universal. And so, so, so we're working on on this with this called the trace order and the trace logic.
And then there's a beautiful thing that comes out of it.
What we propose is, so there's been a big open problem in science. And that is, please, what is an observer.
In Newtonian physics, the observer was aloof.
Didn't affect what it was observing and you didn't have you didn't need a model of it because you could ignore it.
We now know that that's too simplistic in Einstein.
There is the observer and you have to use the notion of an observer but it's really just a reference frame. It's a system of clocks and coordinates. That's all.
There's no real deep theory of an observer in quantum theory.
Now the observer comes front and center, right, because the evolution of state in quantum theory is linear.
When the system is not observed, it's a shorting or a question, it's linear.
When you observe, the change of state is nonlinear.
You go from a superposition, you know, a complex superposition of eigenstates to a single eigenstate. That's nonlinear.
That means you cannot use quantum theory to give you a model of the observer, period.
Can't do it. The linear quantum apparatus cannot model it.
And decoherence doesn't solve it at all because all decoherence does is it maps from a complex superposition of the eigenstates to a classical mixture of eigenstates.
But it does not take you to a single eigenstate. So decoherence is a red herring. It doesn't do the job.
There is no job that can be done using quantum theory.
So that's why they've been pulling their hair out about the observer.
And Wolfgang Pauli said, you know, this is one of the big problems. In 1954, he said, this is a big problem. We need a theory of the observer.
And then just two years ago, Frank Wilczek said basically the same thing in an interview.
So quantum theorists are still, what is an observer? And I actually went to a physics conference a few years ago in Banff where it was all about the role of the observer in quantum physics.
And by the way, saying, well, we won't talk about observers, we'll talk about measuring apparatuses, measurement apparatuses, which is what Heisenberg did and Asher Parris did and a lot of people do.
Well, that solves nothing because the measurement apparatus must embody a non-linear process. It has to be non-linear.
So you have the same problem. Whether you call it an observer or not, you can't do it with quantum mechanics.
And measurement apparatus has no reductive explanation in quantum theory.
So that's one of the big open problems in science. So here's what we propose.
We've talked about these agents being represented by Markov kernels and so forth.
One agent observes another if it's a trace. End of story.
The trace operation. If one kernel is a trace of the other, it is observing the bigger kernel.
Now notice what this does. It's truly remarkable.
We wanted to have observers that were independent aloof and don't affect the, this is different.
If you observe, you are an organic aspect of the very thing that you're observing.
You are intimately and organically, not just looking at it, you are part of it. That's what this is saying.
And that seems right to me. That seems very, very right.
So this gives you a theory of observation. Now what are the outcomes of observations?
There are observations and then there are outcomes.
So I've said what an observer is and what an observation is.
But now what are possible outcomes for this and how do I model that?
And so one interesting thing about Markov kernels, if they're ergodic and unless they're periodic.
So you need to be ergodic. You need to be a periodic.
Right. And you need to have a single communicating class.
So, and we can talk about the others because the others are really interesting and I have some things to say about them, but.
But let's just talk about ergodic ones because they're the math that I'm at least on clean mathematical grounds and I can say precisely what the answer is.
For the ergodic ones, which are the bulk.
You can talk about the what's called the stationary measure of the of the Markov chain.
For example, for the 10 by 10, the stationary measure is a probability measure on the 10 states.
And what it describes is the long term probability and being state one or being in state two all the way up through state n.
So it actually it solves the equation if if the kernel is P capital P and the stationary measure is mu, then mu P equals mu.
That's the that's the equation you have to solve mu P equals mu pretty straightforward.
That's the stationary measure gives you the long term.
In some sense, the long term probability of what you're going to see the outcome of your observation.
So, so it turns out that we have this this non Boolean logic on observers, the trace logic.
So, what we're proposing is there's a map from the observers to these probability measures that are the stationary measures.
So we need to ask ourselves is there a logic on these probability measures, right, because we would like to have effectively a logic homomorphism between the observers and the kinds of probabilistic beliefs that observers might have as a result of observation.
Well, it turns out there is a logic.
And I and a couple collaborators published it 30 years ago. It's called the Lebesgue logic of probability measures.
And that logic has also a trivial definition.
And the definition is that one probability measure mu is less than or equal to a probability measure new.
If mu is a normalized restriction of new period, that's simple.
So, so mu is, you know, it's on a subset of states of new.
And when you restrict new to that subset of states, you get me that's all.
That gives you an incredibly beautiful logic. It's again non Boolean.
There is no greatest element.
It's locally Boolean. If you take a particular probability measure and you look at all the probability measures that are less than it, then they form a Boolean logic.
It's more general than the ortho modular complemented lattices of quantum logic theory.
So it's more, more general than that.
And it turns out, we're showing in this paper that the map between this trace logic of the dynamical systems into this Lebesgue logic probability measures, which is the, you know, the probabilistic beliefs that come out is a homomorphism.
So the whole thing ties up unbelievably beautiful. There's a homomorphism between observation and probabilistic belief.
So, and harmonic behavior is that is the core of it.
When you actually look at these Markov kernels, that's, that's what you look at all the eigenfunctions, for example, of the space time chains or just of the regular kernels themselves.
It's all about harmonics.
Now, you know, what we're so, so I'm, that's why I was nodding the whole time. I mean, everything.
I would, I could say, here's the piece of mathematics that models that that's that's what models that and it's so I mean, so I'm completely on board with what you're saying and so there's a wonderful dialogue, you know, to go for now, where to go where we're trying to go with this.
Can I can I reflect some some additional symmetries, which I noticed so the word that I'm using for a stack of frequencies in harmonic relations is a song.
Yes, the finite memory that you're talking about means that the song is always eventually cyclic right that it's going to it's going to come around again right that it's that it's a repeating thing right.
Not necessarily chaotic stuff stuff can happen out of finite systems.
Okay, we'll come back to that. Yeah, the trace in in my mind relates to what it means to resonate one song with another.
So when one song meets another song and they resonate.
That the two songs have to share some frequencies in common, otherwise they can't resonate right or have a harmonic relationship between those frequencies right. Absolutely.
And the, that means that in order to observe in order for one song to be sensitive to another song, they both have to be drawn from the same super song.
They have to be harmonically related to one another, otherwise they can't be observed.
And the ability to observe the detail or agential nature of one song requires an agent, an observer which is just as agential, which has the same harmonic relationships between it.
Yes, that's right.
And it's, it's not just a question of saying, you know, can you measure all the frequencies which are in this song we're like, like a Fourier decomposition does.
Because you say, you know, it's got it's got a lot of this frequency and it's got a lot of that frequency there I measured it, right. It's like, no, you didn't really see it yet, because that frequency and that frequency, you know, they, they're not really different from that frequency and that frequency that's moved a little bit,
but that's completely different. That's a completely different thing. Why is it a completely different thing? Because these were in the octave relationship and those weren't, right?
Right, right.
Because you're not just seeing the frequencies that are in it, you're seeing the relationships between the frequencies.
Absolutely.
See that you need to be the octave, not just have the two frequencies in it.
That's right. The way that we capture that kind of intuition in this trace logic is when we have, when you have a logic, there's what's called the meet and the join.
And the join of, of two entities, right? The and and the or.
So the meat is the and the, the join is the or, but, but they call it meet and join.
Now, if you have two Markov kernels and they both have say, they both have 10 states, but, but seven of the states are different than they only share three states between them, right?
So this guy has 10 states. This guy has 10 states. They share three states, but each has seven different states, right?
That, that, that for most of the time, if you just give me two random kernels like that, they are incomparable because they, they do not agree on the states where they overlap.
Which means they can't observe each other.
They can't observe each other. They can't form a join. They can't form a meet. They are incomparable.
Apples and oranges.
The apples and oranges and most of them are right. The probability of that is very, very high.
Yeah.
But if they do, if you have the two 10 by 10s, for example, they overlap in three, but they do have a meat. So they do agree.
And there is a trace that they both share on those, they both share the same trace chain on those three states.
Then you can take their union there. You can take their join.
And what that does is that then there are new transitions because there were no transitions before the seven extra states of the first guy and the seven extra states of the second guy never had any direct communication before, before, right?
Yeah.
But when you make the join, the new kernel has all the right connections between them to make the whole thing harmonious to make the right song.
A chemistry that makes a new thing that reacts.
It's unique in general. So there's a unique right song that melds the two original songs if they overlap. So that's why I was nodding all the time. Your your intuitions are really being captured.
So, so it crudely this meet and join it's like the inner and outer product and if you do it properly, it's like the Clifford algebra right with the wedge product.
I would love to prove that that that I mean I've been thinking about that.
And I've, I wrote down actually an order relationship on geometric algebra entities as well.
I said it may be homomorphic to these logics as well. So there may be. So, so, so, so yes I've been looking in that direction. And I actually, a few, few months ago, gave a paper to chase on my, my mathematic collaborator with a partial order on Clifford Algebras, which I should go back and look at it
because it may be homomorphic to this which which case would be really good.
So my intuition says that it is because the through from the songs and the frequencies you can get to the list of your figures and from the list of your figures you can get to the shapes and geometries.
And then the introduction of two songs is the geometric algebra that converts one shape into another shape.
Yes, yes.
I'm completely on board. And I should pull out that piece of paper again that I sent to Chaitan and then go over that logic.
That would be I might even want to include that in the paper we're doing right now but just point that out that's that's yeah.
So the, the computational part.
The second point is that when you, when you strobe a song at a particular frequency and it looks discreet, if you've got the right frequency it looks like a nice orbit, or it looks like a another list of your figure.
It's stationary if you if you strobe it at the right combination of frequencies that that structure that structure that it describes has correspondence with the Lambda calculus too.
So that you can you can describe what a song is the discrete the discrete stationary structure of a song when viewed at a particular frequency is a program and the interaction of two songs can be described as the application of one song to another as the application of one
Lambda calculus expression to another Lambda calculus expression to create an output.
Well that that I would be very interested to see that that would be lovely and that would be new to me that would be very new to me so I would I would really anything that you could send me on that I would be most most interested.
Yeah, I did just intuitive at the moment.
Well, sure, sure.
But but but also intuitively, when you look. So, if you look at a Markov kernel, and you look at one that is periodic.
So every row is all zeros except there's a single element that has a one.
And this.
And then, you know, you cycle through the end states in some order, one right to end. And then you so that has a specific clean frequency, right.
Now, suppose you take another Markov chain that's also 01 but it's also on 10 states say, but it's a different different order.
And now you add the two and waited. So maybe 0.3 of one and plus 0.7 of the other. Well, now you have a Markov kernel which is now ergodic.
It's now an ergodic kernel, but it actually has two basic frequencies. And if you keep doing this you realize that all these ergodic kernels are really just sums of these frequencies of basic kernels.
That's what you really and you're just waiting them. So, so each one of these complicated kernels is a complex harmonic score of all possible frequencies that are going on and you could have some frequencies and they don't all have to be frequency of 10.
So, so it's quite rich that way.
Awesome.
So the, how do you speak to the interest in whether, you know, so the Markov kernels and their ability to interact or relate to one another, you know, interacting is really just seeing what the relationship is between them, right.
How do you speak to the possibility of those that the origination of those kernels and how they come to have information that's shall we say intelligent in capable of producing intelligent action.
Well, the first thing to know again as we talked about before is that the Markov kernels are computationally universal.
Sure. So they could be anything. How do they get to be something specific.
Well, my guess is that anything that's possible is actual.
Why, why not any. So I think this entire trace order and all the possible kernels is a
I'll put this way. I think reality is no less complicated than that.
It's probably much my own attitude about scientific theories is everything we thought of so far is trivial compared to reality, including my the current theory that I put out there it's trivial compared to reality.
But so, but I would say that reality whatever it is is at least as complicated as the entire trace logic and all the possible kernels on it, and we're seeing just a little bit piece of it.
Yeah. So I mean, that I think I'm, I think I'm okay with that. But in our little corner of the universe where you're where we have where we have some shared history.
Yes, you know, and you know, we can talk about the same space time and the same entities and the particles in it and shit like that.
We were interested in agents that know stuff about the world that and and we're interested in the processes by which they came to know it.
And what it means to be able to, you know, act in the world intelligently, right?
Yes. Right. So that is a very high priority on stuff I'm doing right now that to try to answer that question and here's here's the direction that we're going on.
What I want to do is actually try to solve that kind of problem by showing that if I start with only this logic of the trace logic of Markov kernels.
I can build space time and quantum physics and general relativity out of it as a headset basically that certain of these conscious agents used to interact with others.
So that's so that's now to do that is a non trivial thing I want to use the architecture of these Markovian kernels as a computational architecture now like a neural network to build to actually build a space time as a user interface to answer to as a way to answer your question.
Now, to do that I really am going to ultimately have to get a mapping from the Markovian dynamics that I've been talking about into space, you know, a model of space time with quantum field theory and the whole bit.
Now, fortunately, we have some help in this from high energy theoretical physicists just in the last decade.
And here's what they've done.
They realized a few decades ago that space time cannot be fundamental get falls apart at the plank scale.
So it's not fundamental.
David Gross wrote a paper in 2005, you know, the centennial of Einstein's discovery of special relativity so as a, you know, an honor of Einstein said thank you Einstein for space time and then David gross then went on to say space time is doomed.
Thank you Einstein for giving us space time but space time is doomed. It cannot be fundamental and we need to. So in the, the intervening almost 20 years.
We've gone at it. And in the last 10 years they found new structures entirely outside of space time. So they're, these are not structures like curled up inside space time like you think about in string theory.
These are structures utterly beyond space time. And by the way, utterly beyond quantum theory. There are no Hilbert spaces here.
So they found that the new field is called the field of positive geometries and the European Research Council, just a few weeks ago, launched a 10 million euro multinational collaboration.
The conference just a few weeks ago, a couple weeks ago, 100 over 100 mathematicians and mathematical physicists. They're, they, and it's called universe plus you if you go online and look up ERC universe plus.
It's all caps universe and then a plus sign at the end of it, you can read their very ambitious statement, we're basically saying we're going outside of space time, beyond quantum theory we're using positive geometries for a new foundation for physics.
The scientists have realized space time is not fundamental. And in the last decade, they've stepped outside of space time that positive geometries are things like amp to Hydra, associate Hydra cosmological polytopes, and then words I've heard but yeah, right, right, right.
But yeah, they're, they're, they're, they're positive geometries they that's really interesting they're like polytopes. In some cases they are polytopes, but the the amp to Hydran is not a polytope but it's polytope like that they're positive geometries.
And they've also found these combinatorial objects that they can use to classify these, these positive geometry. And in particular, decorated permutations. So these are permutations with a little twist if you're interested I can tell you what the twist is but but for now just leave it as decorated permutations and if you want I'll tell you what the twist is.
But what we found. So, don't worry, I'm not going to say that's not a decorated permutation.
I'm not decorated enough for me. This question is that who ordered that and why, right. And so, here's what we're up to. We have these more coven dynamics we have this trace logic, and the Lebesgue logic, and so forth.
We've already made connection with the decorated permutations we actually said, the decorated permutations classify these positive geometries.
Can they classify our Markovian dynamics, and I can send you a paper we published that. Yes, they do. And what they and when we did the classification, what it said was the thing you want to look at are the recurrent classes.
And that's what corresponds to particles in physics. So that that that one step already told us where to look in making this map from the Markovian dynamics to to particle representation in space time, these recurrent communicating classes correspond to particles.
If you're going to build intelligent agents.
You're going to have to build space time particles and all the rest of it first, and then build intelligent agents out of that.
I want to do it in in what looks like a physicalist matter, sure.
I think of these, these conscious agents as already intelligent agents.
So, basically, it's building a space time is only to build a way of sort of giving a physicalist instantiation of this intelligence.
The reason we have to do that, by the way, is that's where we can make experimental tests right it's only inside space time that we can test our theories.
So I have to project this theory of conscious agents into space time. Otherwise, it's just airy fairy mathematics on testable.
So, our goal is to is to precisely predict the momentum distributions of quarks and gluons inside the proton using this this to actually show how space time arises.
So precisely what a quark and glue on is from our theory that we predict exactly to 10 decimal places the momentum distributions of quarks and gluons inside protons.
Then, then the theory is probably still not right but at least it should be taken seriously. So that's that's what we're up to.
I see that does sound hard.
I mean, nothing, nothing less is science right you if you have to go big or go home that there's no reason for anybody to take this theory seriously if we can't make a prediction that you can test at the large Hadron collider for example.
And the reason, by the way, we're going there is because those are the simplest predictions that we can make single quarks and gluons or small numbers.
If I look at the brain now I'm talking about quadrillions of quarks and gluons. Why should I start with quadrillions. Let me start with one or two, and then work my way up to quadrillions.
So that's, that's what we're going.
Maybe it's just as easy to start from the other end.
Well, and I once we publish this paper hope some people try to use the eye.
I have a short life. I've got to pick you have to pick what you think is your best bet and go for it and my best bet is three or four gluons, as opposed to a quadrillion is where I'm going to get the testable.
I think of rather than billing, it seems like you went from from, you know, there's only consciousness.
There are special kinds of geometries and mathematical relationships out of which I can build space time and particles.
And then in space time and particles, I could build complex assemblies of particles which eventually would look something like consciousness.
It would look something like an intelligent agent.
It wouldn't. It's not the consciousness that created it all, but it's the, it's something that looks like it that resembles agency and intelligence, right?
Yeah, well, it's the, it's the headset that we use, we build the headset that we use, and the headset gives us more or less insight into the consciousness behind.
Right. So, so on this point of view, I would, I would argue that the distinction we make between living and non living is not principled.
In the sense, so right now we're on a zoom, and I am seeing you only through a screen.
And some of the pixels are pixels of your face. And there are other pixels of a wall and a picture behind you.
Now the pixels on the wall give me no insight into consciousness whatsoever.
The pixels of your face give me quite a bit of insight into what you're thinking and your expressions and what you understand or don't understand or agree or disagree.
Now, if I would just were to say, aha, that means that there are some conscious pixels and some unconscious pixels.
That's, that's a really dumb mistake. And it's the same mistake that we make when we distinguish between conscious physical objects and unconscious physical objects.
It's exactly the same mistake. We're, we're, it's, it's not a principal distinction we're making.
So we're always interacting with consciousness, but a headset dumbs things down. That's what it does. That's what it's for.
And so sometimes it reveals less about the consciousness and sometimes it reveals more because it's dumbing things down.
And we then make a category error and say, Oh, those rock is not conscious and the human body is.
No, that's just the wrong way of thinking about it. No pixel in the headset is conscious or unconscious living or non living.
Right. I could communicate with you as a conscious agent through a communication channel that only allowed most code that only allowed a single bit at a time.
Or I could communicate with you as a conscious agent through a rich multidimensional interface and many frequencies simultaneously.
Exactly. Exactly.
I get that.
So I, I think about learning and intelligence in, in ordinary physical Newtonian systems in a very simplistic way, right?
So I have this model that I call natural induction where you have a system of particles connected by springs.
And the interactions of the, of the particles with one another or with an external environment creates tensions in the springs.
And if those springs are slightly plastic, then the springs deform in a way that changes the energy function of the particles.
And you can show that it changes in exactly the same way that you would expect Hebs rule to modify the connections of a neural network.
Oh, well, okay.
So the network becomes a model of its own history in such a way that it can then anticipate the original energy function and find better solutions to the problem of constraints that were originally in the, in the, in the weights.
So if it's, if it, all that's really happening is that you're allowing the forcing on the system to deform its internal arrangement.
You know, how did it get smart? It was pushed, right? It was, it was just pushed.
Well, that's not smart. Like, you know, I can make an imprint in clay and it's a record, but it's not smart.
What makes it smart is the folding of the space, right? That the, or the compression of the, the symmetries in the way in which was pushed, whether that's over time or over space are folded into an idealized compressed representation of what happened.
And then when that pushes back, it looks like it's doing something smart because it's doing coordinated action, which is informed by that past history.
Yes. It's not really anticipating anything. I like to think of it as, it's just reacting, not anticipating.
But when everything is circular, when, when all activity is circular and periodic, being just the right amount of late is the same as anticipating.
So smart entities, entities that are intelligent are, are modified by their history and pushing back on the world as a reaction with just the right amount of late that they look like they're anticipating.
You can't really tell whether time is going forwards or backwards.
Right.
I think that you can do that with ordinary particles and springs at the macro scale without any quantum funny nurse going on.
And that, that that's actually that the, the, the violin string is, is intelligent in the, in the same way, like on the same scale, but not the same amount in the same way as, as, as other kinds of, should we say, intelligence is we find more relatable.
I was going to, I was, I was tempted to say better intelligences.
Exactly. Right.
So, so that, so that I don't have to build it from the quarks up, right. I can start at any level of organization and the thing that makes it smart is the relationships between a few different levels connected together.
Does that make sense.
It absolutely does. And there's a couple levels of which I would think about that one is that I could model this with just the Markovian kernels outside of space time in an interesting way I could say I've got this big, say, 10 by 10 again, and I have this other
say a five by five, and they share three states.
And in some sense, so that the 10 by 10 is more complicated than the five by five, but if they actually share three states and they're compatible, then in some sense when I join them, I can get a resonance of the big dynamics now gets resonated into the smaller one, and it
has a more compact representation so within this trace logic I can begin to formally do with mathematical precision the kind of thing that you're doing and looking at it that way.
So that's what one one direction, but now looking at it, it things inside space time.
It seems to me.
So first I should say outside of space time. These Markovian kernels, the dynamics need not have increasing entropy, the entropy can be constant at each step.
Which means there need not be an arrow of time in the basic Markovian dynamics beyond space time.
But it's a theorem.
When you take a slice of them there is an entropy right.
When you when you lose any information in a projection, you get as an artifact of that loss of information, the entropy increases now in evolution.
The fundamental limited resource is time.
If you don't made in time, you don't reproduce if you don't eat in time you die if you don't breathe in time you die time is the fundamental limited resource.
My guess is that the arrow of time that we see inside space time is not an insight into a deeper reality at all.
It's entirely 100% an artifact of loss of information.
And that means that our entire picture, but by the way, let me preface what I'm about to say with this.
I love Darwin's theory.
And I've done a lot of work on Darwin's theory.
It's the best theory that we have of biological evolution.
There's nothing close to it.
So we just put that right out there.
Now, every scientific theory has its limits.
Everyone.
And my claim is that all of Darwin's theory is an artifact of the loss of information from in the project projection into space time.
And that means that the, the distinction that we make between organisms and resources and all of that, the whole thing, competing nature, red and tooth and claw, all of it is not an insight into a deeper reality beyond space time.
Every bit of it is an artifact of the limitations of our headset period, nothing, no insight.
Yeah.
So that's why looking inside space time for the evolution of intelligence may be the wrong thing.
That's the wrong in that framework.
Oh, yeah.
But I'm not looking for the evolution of intelligence.
Good. Good.
We're on the same page.
Evolution.
That's sorry.
Yeah.
No, evolution is a product of intelligence, not the, not the, not the process that creates it.
So it's a product of the loss of information about the way intelligence really works.
Yeah.
So, you know, when you, when, when harmonic relationships are set up in a resonator, they're already cognitive.
You didn't need any natural selection for that.
Right.
Right.
Exactly.
Right.
When you view it at a particular time slice, a particular stroke, when you look at it with
another song, it'll look like a discrete object that's reproducing.
Yes.
The, when you view one octave with another octave, what you see is the, instead of a big loop that
twists in folds on itself and then unfolds back into a big loop, it appears to do a big loop that
folds and twists and divides and creates two.
Yes.
Yes.
And so when you, when you view one octave with another octave that's a little bit off, you
get this sort of continuous expansion of, you know, creating stuff out of stuff out of stuff.
Right.
And it has, it has this weird property that you, it looks like you took something and you broke
it in half, but the two halves that you have, they're not halves, they're holes.
Right.
Right.
You know, because the hole is, is already folded inside, right?
It feels sort of pre-formationist, but that's what harmonics are, right?
The hole is already folded into all of the parts in a sort of hole.
Exactly right.
Right.
Exactly right.
You know, I agree.
Now that's, and I think those intuitions can be cashed out with this precise mathematics.
I have not done that, but, but I, but I would be a direction that I would agree is a very
fascinating direction.
So I wonder, I wonder if there's something, I wonder what, you know, basically I'm wondering
what's left for me to add, right?
If you've already done all the math, because I'm, because I'm working at an intuitive level,
but you've already done all the maths.
So I wonder if there's, if there's something in that notion of how a physical system can
come to have knowledge of an environment just through an ordinary Newtonian sort of deformation
of its internal structure, that, you know, it's just a ball rolling downhill.
It's just local energy minimization, which puts knowledge into it.
If it has this folded structure, there's two levels of architecture happening at once.
So that it's not just a language in which you can write intelligent things, but it's a,
it's a description of the process that puts the intelligence into it as well.
Well, first off, I don't want to give the impression that we've solved everything.
That's not the impression I want to give.
I think that we've taken a first step in what's going to be, I think, a really interesting
and long journey, but just the first baby step is the way I look at it.
For example, I can't tell you yet how to model even a quirk in our theory outside of space time, right?
So all the fun work is ahead still on that.
And we have hints, right?
So I can say what some of the hints are to try to make this kind of connection into space time.
One is,
we propose, and we do this in the paper that I'm writing right now,
that the mass of a particle corresponds to the entropy rate of the Markovian,
the recurrent communicating class of the Markovian kernel that is a projection of.
So the entropy rate is, so a kernel, each row is a probability measure, right?
Each row is a probability measure.
So you can talk about the entropy of each row.
So each row has its own entropy.
And then you, if it's an ergodic kernel, you have a stationary measure.
So you have a probability measure for each row.
So you can just add up all the entropies weighted by their stationary measure.
And it's called the entropy rate.
So it's a very, very simple. It's a nice clean notion of the entropy of the entire kernel.
And we propose that that is what corresponds to mass in physics.
The entropy rate, it's telling you how much each state,
the entropy rate of the system is telling you effectively how influential it is, right?
If you have all zeros and ones, then you don't hardly influence anybody else.
You only influence one thing.
So you're influenced, so that's, and that's going to be zero entropy.
So if you have a bunch of zeros and a one in a row, well, that row has zero entropy.
It's going to have no influence.
It has no mass.
And that entropy will be, so that's, you're only ever seeing the entropy of a particular projection, right?
Because you're not seeing the true entropy or the true mass, right?
And that's going to be really important empirical tests of our theory,
because we need to actually understand the statistics of this partial sampling process that's going to happen.
So the trace chains are, as I mentioned, they're assuming an infinite trace.
But we will have finite traces.
And what we plan to show is that that makes a difference in what we get in our physics.
So at the quarks and gluons inside the proton, I'll just mention.
When you look at the horses spatial and temporal scales inside the proton,
what they call Bjork and X, which is the temporal scale and Q-scored, which is the spatial scale.
You see three valence quarks, two up quarks and a down quark.
As you start to get finer and finer spatial and temporal resolution,
you'll see a bunch of quark, anti-quark pairs of what they call a quark-c,
and a bunch of gluon, an ocean of gluons.
And then as you continue to go even further down, you get just an ocean of gluons.
It's just like seething gluons, and that's all you see.
How are we going to explain that kind of thing?
Well, quarks are fermions, they're massive particles,
and gluons are massless.
They have no mass, so they're traveling at the speed of light inside a proton.
So it's frenetic.
The inside of a proton is frenetic because there are particles traveling back and forth
at the speed of light inside this tiny, tiny little thing.
This is truly a seething thing.
Now, what is a massless particle in our theory?
It corresponds to a matrix, a Markov matrix that has only zeros and ones.
So actually, given our definition of entropy rate being mass,
we now know what massless particles are.
The massless spin-1 particles are particles that are periodic.
So you can start to see, we can start to get this really beautiful dictionary.
Now, what happens when we start trying to build up a trace chain
at really, really high, so really small time samples and really fine resolution?
Well, the way you're going to do it is you're going to have to sample.
So I got this state, now I got this state, now I got this state.
So now I can start to figure out, okay, there's probably going from this state to this state.
So I can start to build up.
What am I going to see?
My initial matrices are going to be zeros and ones because I don't have enough data to do anything finer.
But that's not an insight into the nature of reality.
That's sampling error.
That's what we're going to claim about the gluon-C when they look at the proton in very, very high resolution.
When we're getting closer and closer to reality, know what you're seeing is entirely sampling error.
So doesn't that gluon-C, I've never heard the expression before,
but doesn't that gluon-C end up looking like something that's a continuum with very, that's almost like it has,
like in that sea there are waves that are of much lower frequency, that connects it right back up to the top level?
I think so.
I think that there's all sorts of weird structures that you can see and really unusual,
I forgot what they call these kinds of weird dynamics that they have down at that level, but there are some unusual dynamics.
So it's not just noise, it's noise with some kind of unusual coherence to it,
which is perhaps what you would expect when this is a sampling from something deeper that does have coherence.
Yeah.
A sampling error on something deeper.
That's like asking the tiny vibrations between the bow and the string are just heat,
they're just a sea of incoherent microscopic influences.
So how come then, when you add them all up, they become this fundamental, right?
So they weren't tiny incoherent things, they actually, they had structure to them,
but in some ways they wouldn't create the stick-slip dynamics of the particular structure, right?
That's right.
They can't be, they would all cancel out if they didn't have any structure to them.
That's right.
So if this is a sampling of a pre-existing kernel, there is a structure to that kernel,
which is going to effective, as you get more and more sampling,
you'll begin to see the harmonic structure of the underlying kernel.
Absolutely.
By the way, we have money to hire one or two postdocs who have recent PhDs in algebraic geometry
and know these positive geometries.
So we're about to put the word up, but if you know any bright young new PhDs in mathematics who know algebraic geometry,
that's not the kind of PhD I know I can barely do matrix multiplication myself.
But if I come across any, I'll let you know.
Yes.
Thank you.
If, what would be the reverse, if you have, if you had people that could do that kind of maths,
but understood the biology would want them to stay with you and do that kind of maths,
it's the, for me, it's the, I would like to explore those relationships between shape and form of the Clifford algebra,
the geometric algebra, the lambda calculus, you know, universal programming language and the adaptive processes.
So processes of adaptation and learning that happens spontaneously as a system deforms under stress.
So that's the sort of the calculus that I would like to be able to relate.
Absolutely.
Well, absolutely.
And me too.
I'm very interested in going there one step at a time, but, but absolutely we would like to like to do that.
The lambda calculus is quite fun.
When I was a graduate student at MIT, we had these list machines.
And of course, LISP is basically lambda calculus.
There's a programming language based on lambda.
So I wrote my entire dissertation programs in the lambda calculus on the list machine, which was keeping track of all the parentheses was back then, quite a sure.
Does that make sense to you that, you know, that when you think about, well, like, what's the difference between the program and the data in lambda calculus is just which side of the application that you write it on, right?
So when one song meets another song, if an observer takes the frame of reference of song A, then song A is a program that is operating on song B.
But if the observer takes the frame of reference of song B, then song B is the program that is operating on song A.
That's right.
So what I like taking the frame of reference of is just turning around to phase lock the components that are in common between the observer and song A, or turning around to phase lock with the components which are in common with song B.
That's right.
I agree.
One way to put it is how you decide to attend to the whole system.
Which way are you attending to it?
One way to think about this trace process is it's an attention.
So when I trace on these three states, I mean, what I'm doing is just I just want to attend to those three states.
So I'm just attending to it.
So it's really another way of thinking about this is it's there is this one universal consciousness in some sense the whole and just different ways of attending to aspects of it.
Yeah.
That's the way of thinking about this and you see different music when you look at it from different points of attention.
Fantastic.
A lot of fun.
Fantastic.
I'm so enthused.
Thank you.
Thank you.
It was very, very fun.
I was the synergy was surprisingly good between the ideas.
So I'm grateful.
And Michael will have us talk again.
When he left the recording stopped, but I pressed record again.
So we might be able to splice together if we need to.
That would be good.
Because I think this, this conversation might be one that would a lot of people be interested in.
Cool.
Cool.
Okay.
I'll meet you.
You too.
