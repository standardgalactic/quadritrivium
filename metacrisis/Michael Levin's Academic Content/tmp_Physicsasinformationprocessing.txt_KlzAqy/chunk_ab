But there are probably other boundaries
that we can't see just by looking at, say,
an electron micrograph, where they're classical APIs.
At least it raises that question,
where are the APIs inside the cell?
And how can we cut to something of modules
that we now have to communicate by some sort
of classical information instruction?
Given that it can't be happening, you haven't learned.
So anyway, so that's just a very brief summary of where
we've been in this now multi-month course.
And now we're finally to some of the parts
that's really biologically interesting to do with space.
But before I talk about space, and this
is actually motivated by questions
that were asked after one of the other sessions,
and Mike's seen some of what is immediately
to follow before, because I shared some of the previous
meeting that was the last few days.
I wanted to talk a little bit about explanation,
because it raises an interesting question
about the scale dependence of biological processes.
So let's start by contrasting reductionists
in scale-free theories.
So the sorts of undergraduate approaches
that we've learned in undergraduate school,
in undergraduate physics and chemistry, for example,
there's stuff going on at the molecular level,
and the stuff going out at the cellular level
is just some emergent process from all of that chemistry.
With a scale-free type of theory underlined,
that's underway by the free energy principle, which
is a scale-free description of how
systems of any type communicate with their environments
and build models of their environments
and behave in their environments.
So in any kind of reductionist theory,
the basic idea is the explanation.
All the real, the important stuff handles
happens at some preferred scale.
And whatever's happening at larger scales,
there's emergent, or epiphenol, or something like that.
It either doesn't matter at all, or it's
some sort of packaging of what's happening
at the lower scales.
And so the fundamental assumption
is that there's a fundamental scale at which that's most important.
Whereas in a scale-free theory, the approach
is very different.
A scale-free theory postulates that there's
some kind of dynamics, and that dynamics happens at every scale.
And as you move between scales, what you're seeing
are different implementations of the same theory.
So it's a very different way of thinking about theory.
And I would suggest that a lot of science is actually not
reductionist, even though scientists talk about it
as if it was reductionism, because I think they learn
that's what they should say.
And the scale-free approach is more
difficult to think about in certain sense.
So if we go back to that picture,
and systems targeting each other across the boundary,
and you ask about scale, the way to represent scale
is the density of which information is included.
And we can think about a whole spectrum of scales
that are already given to us by physical theory, for example.
We can do the energy scales, or space spatial scales,
or temporal scales.
And all of those are coupled together
by the basic physics, or basic physical constraints.
So the kind of smallest possible physically definable scale
was the Planck scale, where an energy of about 10
in the 19th GeV, so the 10 in the 19th billion
electron volts, is compressed into a spatial scale of 10
to the minus 35 meters.
So that defines the boundary of a black hole, basically.
Anything that includes information at that scale
is a black hole.
And 10 in the 19th GeV is about 15 orders of magnitude
larger than the energy density that we can get with the large
electron collider.
So it's a very high energy density,
compared to what we could produce in the laboratory.
So the scale about 1 GeV is the scale at which nucleons exist.
And that corresponds to a distance scale
of about 2 minus 50 meters, so 2 nucleons
about a femtometer in size.
And our scale, our natural scale, is about a meter.
And the associated energy is very low.
That's radio frequency energy.
So if you think about white, light's about 10 to the fifth
higher energy than radio waves.
So white feels energetic to us, and radio waves don't.
And that's just a fact about our size.
White seems very energetic compared to the size
of our gradient, which is our natural scale.
So we can think about the boundary
as encoding this information at quite different scales.
So now how do we think about science in this frame?
And we're all familiar with kind of reduction of science,
where we assume a fundamental middle scale.
And this is the best example.
You have some event that happens at some incredibly tiny scale
in space and time at very high energy.
And that sets all the parameters for all the physics
and produces all the organized structure that we see.
I imagine.
And that's called the Big Bang Theory.
So the Big Bang Theory is, in a sense,
the ultimate reductionist theory.
It says that all of the important parameters that
characterize everything are up to the waves and nothing
else fix the plank energy, the plank time, the plank spatial
scale, and everything else just happens.
So what we're looking for is a way
of thinking about science, and it's
something to be able to explain, and it's different from that.
And it's hard to find an example with this sort
of nice illustrated picture.
So we have to draw a picture.
And in the kind of notation we've used here,
the picture looks sort of like this.
We've got this system on the other side of the boundary.
And that system is changing through some kind of god's
eye type parameter.
And we're looking at it.
Our system, Alice, is looking at the boundary,
where Alice can see encodings of different scales.
And let's call those scales 1 and 2.
And what's happening on the boundary
at these different scales is changing.
So if Alice looks at a magnifying glass that
sees scale number 1, then she sees some changing pattern bits.
And if she looks at a different magnifying glass that
sees scale number 2, she sees some changing pattern bits.
And she can construct a theory about how these bits change
over time.
And that could be, for example, some Markov process
that captures that theory.
And then the interesting question is, how do those two
relate?
So can you construct a semantic theory, an embedding
theory that says, OK, bit patterns at this scale
have some correlation.
And there's some part of the correlation structure
of bits at this small scale that matches the behavior
of individual bits at this very large scale.
So I construct some sort of, if you will,
linguistic mapping from the language that
seems natural at this scale to the language that
seems natural at that scale.
And it's how those theories change through time.
It's a really interesting question.
And you like your semantic mapping to stay fixed,
because then you don't want to talk about it
as you progress through time.
They think it's not going to pass your measurements.
So this picture should look familiar,
because this is how computer science works.
And so we can translate all this into the language
of computer science.
Now the scales are different programs,
which may be things like an operating system
versus user interface, versus what
we call the hardware level, which really isn't
the hardware level.
It's a high-level distribution.
What a bunch of stuff is doing.
And we can keep going down the scales
with a laptop or something of our scale,
this kind of user interface scale.
But what we're doing in computer science
is constructing exactly this kind of semantic mapping
between different ways of describing one process.
And we do that by assigning meanings
and mapping those meanings in each other.
So that's what I call semantic mapping.
And with computers, we design them
so that they can't be so constant.
So you know, I start with the relationship between the C
compiler and some HTTP value or something
in a way that doesn't change.
So what we can then ask is, and here's
where we start to get into biology,
what's the actual relationship between these embedding
theories and biology?
You can think about biology in a scale-free way,
which if we think in a free energy principle framework
we're doing, then we can ask what the embedding theories
look like as we go from thinking about how we embed pathways
into cells, cells in the tissues, or tissues in the organs,
or organisms in the social parts of the human body,
all the way up to the biosphere level.
We have all these different embedding theories.
And this gives us a set of scale transitions.
And if we want to represent sets of scale transitions,
we can return to, again, a mathematical formalism
and represent it as a renormalization,
which is just a set of operators that move things
between different scales.
