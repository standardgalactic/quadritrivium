So I began okay. So I began reading the book but I haven't been able to finish that.
No, no, no, look, what can I say except that it's an enormous imposition to to read what I've written
because it's just so long. It's so not. But there we are. Yeah. But um, no, I'm just delighted
that you're interested really. And your interest comes from where exactly, Richard?
Well, I've been working on the relationship between evolution and learning. That's probably
the right place to start. Okay. So there's some old ideas that suggest that they are
sort of interchangeable processes, that that evolution by natural selection is a way of
doing one possible implementation of a sort of trial and error with reinforcement process.
And that what goes on in the head is likewise a kind of natural selection process that we're
trying out hypotheses and reinforcing those which work well. Yeah. But that that level of analogy
suggests that neither way of looking at it causes us to change our understanding of the system.
But actually, no, I think that there is an equivalence there, but there's also a disanalogy
there, which is really important. And not least, because, you know, we think of learning systems
as being clever and natural selection is being done. And at the same time as thinking that the
algorithm of natural selection is done, we observe what clever things it does.
So whereas, you know, with learning systems, we observe what clever things they do. And we
think that's not surprising. So if they're the same mechanism, either they should both be
surprising or neither of them should be surprising. So we need to sort of tease that part a bit.
Yes. I mean, one obvious answer to this would be to say that what we do when we're learning is
not as demanding and intelligent as it looks. But another way of looking at it would be to say
that actually the evolutionary process has intelligence. I mean, I'd be prepared to say that
without, you know, trying myself to the to the mast, to the mast of, you know, intelligent design
or anything like that. But there is some sort of something in this process is, I don't think you
can logically exclude it. Well, great. So we're starting on the same page then. So as per many
conversations with Mike, we often say, but you know, but intelligent systems are magic.
There are, you know, there are intelligent algorithms within machine learning, for example,
that although they might not capture everything that brains do by a long way,
they capture something more intelligent than the process of natural selection, as Darwin described
it. So can biological evolution implement things like that instead of an envariation and selection?
No, if the if if you can capture the principles of an intelligent optimization algorithm
in an algorithm in a step by step process, what are the necessary components of that to implement it
in a in some other substrate? Or I get generalized learning algorithm rather than a
the analog of generalized Darwinism, right, that you can do it in different substrates?
Yes, yes, I see. Yeah. And you're working on on that? Are you working on on finding
machine processes that could replicate this? Is that what? Well, having having as you just suggested
rather quickly come to come to the conclusion that evolution by natural selection wasn't going
to cut it. It wasn't going to explain the patient we observe. And then noticing that, you know,
learning systems do a more intelligent kind of optimization than random variation and selection
does. So we already know that there are adaptive processes smarter than natural selection. The
question is, our brain is the only thing that can do it, right? Or are there other kinds of
systems that can do it? So I've been trying to describe principles of, shall we say, intelligent
optimization, which could arise spontaneously in physical systems, in particular in networks
with connections that are viscoelastic or give way under stress. And that turns out to be enough
for a physical dynamical system to learn in a way which is smarter than natural selection.
That's extraordinary in itself. Yeah. So I was the reason that I was interested in
in some of the things which I heard in your discussion with Mike and the other things of
yours, which I watched later, this difference between the two hemispheres was that there appeared to be
a slight polarization between tendency towards inductive inference and deductive inference.
And from my initial reading, I might have got that the wrong way around between the two hemispheres,
but maybe that doesn't matter. But the thing that makes learning systems different from
our conventional understanding of natural selection is that learning systems do induction.
Learning systems form general rules from specific examples.
They also seem to do, before we come to hemispheres, but they also seem to do something else, don't
they, which is to respond intelligently to a never before seen situation for which they cannot
be as it were programmed. Exactly. I mean, this was first noticed by Barbara McClintock in the 80s
that a single cell could respond in an intelligent way to an insult that it could not naturally have
ever experienced. And yet it responded as a whole organism, not in a kind of an unintelligent series
of chain reactions. So that is itself very interesting. I think that when you come to the,
I mean, I don't want to rush on, but to the hemispheres thing, I think that in a nutshell,
although I've suggested that the right hemisphere does play an important part in deduction, I don't
doubt that the left hemisphere does too. And I think they both also rely on induction, but one
might have thought that induction was more something that the left hemisphere relied on,
because it's so alert to drawing lessons from experience. But it turns out that probably the
weight of the induction is in the left hemisphere, because it likes regularities, it needs regularities,
it is always looking for certainty. And that certainty to it comes from amassing enormous
numbers of instances where this seems to work. Whereas the right hemisphere is what Ramachandran
calls the devil's advocate, it's always going, yeah, but don't jump to conclusions, it may not
be like that. It's role is very strongly a kind of checking and in a productive way, a somewhat
adversarial stance towards just going, well, that swans have always been white, so they always will
be white. Yeah. So that point that you raised about responding appropriately to insults that
were novel, unfamiliar. So that's fundamentally requires induction. If you only need to respond
to things you've seen before, then you just need memory, you don't need to do induction.
But if you're going to extrapolate from things you've seen before to something you haven't seen
before, then you need to form a general rule from those specific examples, and then do something
deductive from that general rule for the new. It needs that element of deduction. And that new
element characteristically comes from the right hemisphere. It's the right hemisphere that tends
to be the one that both understands the new better and is more prepared to engage with a new
strategy than the left hemisphere, which tends to be very conservative. So the
a perspective, so yeah, it's interesting, right, because you can't tell that a system has done
induction until you ask it to use, it's that induced model to do another deduction, right?
You have to induce a model and then you have to use it. When you use it, you get deductively,
not inductively. So that it all gets a bit slippery when you think about things like, well,
if I've got some data and I need a hypothesis to explain that data, and I arrive at a hypothesis
to explain that data, have I done induction or have I done deduction? And the answer sort of depends
on what you think the starting point was. So if I gave you a set of hypotheses and said which ones
of these are consistent with the data, you would deduce which hypothesis was consistent with the
data. Now, you may then subsequently use that hypothesis for responding or classifying a
stimulus you haven't seen before, in which case, because it was a hypothesis that fitted the data,
it actually makes predictions that go beyond the data. But the way that you arrived at that
hypothesis and not some other was just by a process of eliminating the ones that weren't
consistent with the observations so far, right? So for example, if I have two hypotheses, all
swans are white or all swans are black, and from the data I've seen so far, all swans are white,
is the hypothesis which is consistent with the data, right? That's a deductive conclusion that
out of those two hypotheses, that's the one. But if I then use that hypothesis to predict the color
of a new swan, I'm using it in an inductive way. Inductively, yeah. But why wasn't the model,
so that feels like that leads Popper to say we should just do everything with deduction, right?
Even the way that we do everything is we'll eliminate the hypotheses which aren't consistent
with the data and then we'll never make a mistake, right? We'll never do something
wasn't supported by the data because deduction is the only thing I ever want to do and I never
want to do induction, that's bad. Stop it, right? What's missing there is why wasn't the hypothesis,
the first two swans are white, the second two swans are black, the third two swans,
you know, the next two swans are white again. Why wasn't that hypothesis in the set? Why wasn't
the hypothesis all swans are white except at five o'clock when they're pink, right? Why wasn't that
in the set of hypotheses, right? So the the initial set of hypotheses is always biased.
Yes, it doesn't include all possible hypotheses that were consistent with the data
and the set of it can't know it can't and if if even if in principle it could include all
possible hypotheses consistent with the data, that wouldn't have to include all the hypotheses that
predicted the next one to be white and all the hypotheses that predicted the next one to be
black, they're all in there. Though it actually doesn't make any prediction to do that. It has to
be biased in order to make a specific prediction. So that I think that you could you could understand
how it would be very slippery to determine whether a particular hemisphere was biased towards
induction or deduction because of this thing of well, if I was deductively eliminating hypotheses,
I'm doing deduction, but having arrived at a hypothesis from a biased set, ah, now I was
doing induction and those those two things are kind of they're at the very least they're slippery.
So I can see why in the text I was seeing lots of back and forth about well, you might think this
is going to be overdue. That's right. That's right. And that's why if you did have time to read
the chapters on the nature of reason, I think they would be more instructive, but I'm not
suggesting that you do. Sorry, I haven't already. Have to go to bed for a day or two.
But there's more there, but I just thought I'd send that along just really to show that
it doesn't neatly parcel up in some way that one might have thought it does. It's one of the
numerous areas that one finds that. And I think induction and what you've really
exemplified in what you've been talking about is that induction and deduction
are not entirely alien species that they need, they need one another. And it isn't possible to
do without one or the other, in fact, in living. And nor is it actually reason is completely,
this is a very much more important point to me, is somehow entirely devoid of intuition.
Reason cannot actually get above and behind intuition eventually. It has a certain
intuitions of its own. And in reasoning properly, we need to use intuitions and
intuiting correctly. We bring reason to bear. So these things are not these,
as they're often set up nowadays to be these two. And usually reason gets a tick and intuition
gets across. And there's this whole, no doubt, lucrative. Except in Star Trek.
Okay. I don't think it's out that Captain Kirk was right after all and Spock was wrong.
You'll have to educate me on Star Trek, I'm afraid. But no, I mean,
intuitions are bound up with reason, the good ones. And then they're certainly not
any more dangerous than reasoning if it has no intuition in it. I mean, the kind of reasoning
that is done by psychotic patients is absolutely impeccable. And as has been pointed out,
it's not that they've lost their reason, as we say, but they've lost everything but reason,
they can only reason, and therefore come to extraordinarily bizarre conclusions that anyone
who has experience of life will tell you it's not what's happening. So we need very strongly,
both of these things. Anyway, Mike, you've been very quiet. I'm sorry.
Yeah, yeah, no, but I'm greatly enjoying the discussion. The only thing I have to add so far
is that this issue that you guys were just bringing up of where did the models come from,
right? And what are the models before you can go ahead and sort of crank through them mechanically
goes back to the whole evolutionary aspect, which Andreas Wagner raises with this book,
Arrival of the Fittest, right? So once you have them, you can sort through them and pick the ones
that are most fit for a particular environment, and then you're good to go. But you need to make
sure that the good ones are in there somewhere. And so the whole thing has to be seen as some
sort of a generative scheme that is open-ended enough to produce these new solutions. And then
the question becomes, well, are they purely random? Where do they actually come from? Are they biased
towards ones that are actually going to have some utility, right, or the vast majority of them
completely useless? So I think that's a really interesting aspect of this. And back to this
issue of, you know, kind of the intelligence of the evolutionary process, I agree completely. And
I think the thing that is the move that's really important to make here is given to us by the field
of basal cognition, which is that back in the day, you really just had things that were mechanical and
dumb. And then you had humans and angels. And those were your two options, right? And so looking at
those two options, what do you want to say about evolution? Well, the scientists don't want to say
that it's sort of human level or above intelligence. So well, then I guess we'll have to say it's
completely stupid. But now we understand, right, especially through the field of basal cognition
that there's actually many options in between. And cybernetics gives us many different options in
between. And we can say that it doesn't have to be completely blind. It doesn't have to have an IQ
of zero, nor does it have to have some sort of grand superhuman level of intelligence. But it
can have some. And of course, this business of reacting appropriately to things you've never
seen before, yeah, we have many examples of this in developmental biology and cell biology.
And I think where it comes from is this idea that, and maybe it wasn't this way always,
maybe really primitive forms of life weren't like this, I don't know. But the life we have now
does not make too many assumptions. In other words, it's, I think, and this is a controversial view,
but I think that embryos and things like them, they're so plastic and flexible, because they
figure everything out from scratch every single time. In other words, it's not some weird unusual
thing when something novel happens and they somehow, you know, make up for it, every single
time they are, they come into the world, not really knowing how many cells do I have, what size
are my cells, do I have the right complement of genes, you don't know any of that, you have to
solve that. And so that, of course, makes you, if that's the architecture that makes you good
at handling novel things, I think what you actually have in these embryos is a problem
solving intelligent machine with a bunch of prompts. The prompts are maternal gene products
and cytoplasmic, all the stuff you inherit from the egg and your environment that isn't in the DNA.
Those are your prompts. And the prompts, both the prompts and the machine are evolutionarily
sort of shaped so that together they normally do the right thing. So all other things being equal,
acorns make oak trees and frog eggs make frogs and so on. But because of that architecture,
you can put in, and as we've done and other people have done experimentally, you can give
it different prompts and get completely different but coherent behaviors out of it.
And that's what I think, that's what I think gives it the intelligence is that
the assumption from the very beginning that you actually cannot, you don't over train on your
history, you actually don't know that anything you've seen before is going to be true now.
And you just have to figure it out from scratch. Where do my sort of borders end and the outside
world begins? What are the important things to pay attention to? Who is behavior shaping me? Is it
myself? Is it somebody else? Like all of these things have to be solved from scratch. That's
where I think the intelligence comes from. And how long do you think that process of not
knowing goes on? Because very clearly, they must build up stores of likely outcomes to certain
positions and actions pretty fast. Yeah, I mean, it's fast, but I think it remains incredibly
plastic. You know, the thing that strikes me about this, like you've seen this rubber hand
illusion. Yeah, the rubber hand illusion within what seven minutes, it convinces you that you
have an extra limb. How long have you had exactly four limbs, right? We've had exactly four limbs
from millions of years. You well know how many limbs you have. And all it takes is, you know,
a few minutes of experience to convince your brain that, oh, I guess all that was wrong. I've
got something else. And just that plasticity, the ability to override this, we see it all the
time. You know, if I make a tadpole with no eyes in the head, but the eyes on the tail,
they don't need evolutionary adaptation to use that eye. They can see out of the box
immediately. Those embryos can learn in visual assays. You get some weird itchy patch of tissue
on your tail. It's, oh, yeah, that's visual data. We know what to do with that. And by the way,
it doesn't come into the optic tectomy. It comes into the spinal cord at best. Yeah. No problem.
Now that's how we are. And so, yeah, I think they do make models from the start,
