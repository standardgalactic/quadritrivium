So in the broader picture that people who do neural decoding
often think about is, OK, can we scan a brain?
And by reading whatever properties
you think are important, could we
extract the cognitive content of the mind
that is implemented there?
And then so that's kind of the big context.
And then the question of why is it
harder for others to do it than for you to do it?
So from the outside, what's the difference
in reading your memory?
Anne Graham says a neuroscientist from the outside
versus for you reading them inside.
Again, not perfect access by any means,
but much better than we seem to have from outside.
And what does that mean?
And so I was thinking more in a much simpler system
of the kind of thing that Richard first did in, I think,
2010, and then we did after that in a slightly different way,
which is to train these gene regulatory networks.
So this idea that you've got a network, it's deterministic.
You know the rules by which the nodes turn each other up and down.
The structure does not change.
So the structure is coming.
So there is no synaptic plasticity, et cetera.
Go ahead.
But in my model, the structure does change over evolutionary time?
Yes, over evolutionary time.
But during the learning, does it or no?
So the only learning that occurs is structural change
over evolutionary time.
Got it.
Yes.
OK, got it.
So ours is different in that way, right?
So ours are completely locked down.
So the nodes don't change.
The weights don't change.
The relationship between them don't change.
All of that is completely locked down.
So only the activation state?
Correct.
Correct.
Yes, exactly.
Yeah, so what we then do is we treat the thing as an agent
and we provide stimulation.
So we pick in particular, let's just
talk about the case of associative learning.
They can do, I think, six different.
We found like six different kinds of memory.
But let's just look at associative learning.
So you pick a node and you call that the unconditioned stimulus.
And we know that if I stimulate that node,
there's another node somewhere.
We call it R for response.
That other node is going to go up.
So every time I tweak the UCS node, the R node goes up.
Then we pick another node, which we call the first,
it begins as a neutral stimulus.
And so that one, if we tweak that node, the R does not go up.
So that's how you choose them.
And then what we do is we apply that stimulation together.
So it's a classic Pavlovian kind of thing.
You've got the thing that's salient to the agent.
You've got the thing that initially is not salient.
You apply them together.
And then after a while, what happens
is that just the neutral stimulus becomes a conditioned
stimulus.
And when you hit that node alone, now the R goes up.
And so now you have a trained network
that has associated somehow those two stimuli.
And we looked for this in like 60 different biological networks.
And it was pretty prevalent.
And then we looked for it in random networks.
And it was quite rare.
So it's not the kind of stuff that Stukoffman studies,
where these are properties of all networks,
including random networks.
This is the way we found it.
That's so cool.
I didn't know about that work.
Oh, yeah, sorry.
Yeah, so we have two papers, one looking
at that in Boolean networks, and one looking that in ODE models.
And both of those, there's a website repository
where you can download these models.
And then we simply put them through a battery of tests
like you would for a new animal that you were studying
to ask, what is it capable of learning?
So you got hibituation and sensitization
and associative learning.
And they can count to small numbers and things like that.
And then there's a million other things
to check that we haven't gotten to yet.
So OK, so the question that I started thinking about
was this, and this, by the way, tripped up reviewers
for both papers.
This was a real nightmare for both of them
because the reviewers asked initially
a somewhat reasonable question, which is, where is the memory?
So if the structure doesn't change,
and there is no explicit scratch pad to save stuff in,
the structure doesn't change, the node weights don't change,
where is the memory?
And that's what a lot of people complain
about during that review.
So I started thinking about this question specifically.
And we explained, OK, it's in the dynamical state,
and you've got this notion of an effective network, which
is different once it's been through all that experience.
The phenotypic network, as we would put it,
is different from what the original structure was.
And it's all in the dynamic state.
But people found that very unsatisfying.
And can I say it back to you to see
if what I would guess about it resonates
with how you understand it?
Yeah, go for it.
So since it's a recurrent network,
it can hold a dynamical state in its attractor.
And in one attractor, the relationship between two nodes
can be different from the relationship between those two
nodes in another attractor.
And when you train it, you're pushing it
from one attractor to another.
Yep.
Yep.
That's all right?
OK.
Exactly.
That's basically how we understood it, Santosh and Surama.
And I understood it the exact same way.
But the first thing I realized is that people are actually
very uncomfortable with this, because they're
looking for a physical location.
They're looking for the n-gram.
And the n-gram is supposed to be some kind of a thing
that they can get their hands on and read out
and a little register or something where something changes,
something flips or shifts it.
It took a lot of work to sort of bang through this idea
of the different attractors.
So that's not precluded, right?
If you could read the dynamical state,
then you could read what the memory is.
Well, so let's talk about that, because I'm still not sure.
That's what it may be.
You can tell me.
Maybe that's true.
So OK, so I started thinking about exactly that question,
is OK, where is the memory from the following perspective,
just like the brain scanning idea?
So given a network, can I tell whether this thing has
been trained or not?
Can I tell whether it's gone through the process?
And more specifically, what do I need to do to be able to say,
oh, yes, this one has seen the stimulus.
10 times this one has not.
What information do I need?
And in particular, can I do it with a pure read?
In other words, can I do it without disturbing
slash stimulating the network or a model of the network,
for this purpose being the same thing,
can I do it without doing things to it
and seeing what happens?
Can I just purely read it?
And at that point, just out of curiosity,
I posted a question on Twitter for the neural decoding folks.
And I said, do you guys think that brains
can be read out in a pure read, a pure scan that
doesn't interact with the brain, doesn't
stimulate it, just assume you can read anything you want?
And I would say about 80% of the people
said yes, that they thought that eventually,
once we know how to read the right bits of the brain,
we don't need to interact with it.
You can do a pure scan, and you'll
know everything there is to know.
So there you go.
And then I have more thoughts about that,
but I'll shut up, and I want to hear what you guys think.
What can you derive from a network like that
without interacting with it?
OK, so let me see if I'm still catching up.
It's clear that if you wanted to know
whether this network has a particular relationship
between the conditioned stimulus and the response,
then you could poke the conditioned stimulus
and see if you get the response.
But you want to know, can you tell that that linkage is there
without actually what's it called,
fobbing the widget?
Exactly.
And you're allowed to read what the exact state,
the activation state of all the neurons
is at any particular time.
It's not like it's immaterial in that sense.
It is there.
The question is whether that could ever not
be enough to determine what the relationship would
be without simulating the relationship.
Exactly.
Exactly.
Yeah, can I back up a little bit here
and ask about the random networks in the paper
with Sarama and Wes.
Do the random networks have basically random dynamics?
And the biological networks have saturation dynamics?
Or do the random networks all have saturation dynamics
and all the reactions?
Well, I'm going to ask what you mean by saturation dynamics.
But just the first thing to say about the random networks
is that actually generating random networks
is really a non-trivial task.
Because if you want to sort of, if you don't want to bias,
how do I generate the space of all possible networks?
It's not easy.
So we did our best in trying to be fair.
But no doubt, we're still caught in a region
of the space of possible ones.
But what do you mean saturation dynamics?
Well, just dynamics that are basically
a shape that have a long saturated region at the bottom
and then some curves, roughly linear region, curve,
roughly saturated region.
Because if you drive a network or some components
of the network from the linear region to a saturation region,
it's going to behave differently.
It's going to behave now like an infinite source
or an infinite sink, depending on which of those
you drove it into.
And once you have something that acts like an infinite source,
it's going to behave as a memory for some time
until that source is depleted by constantly being read
and having to feed activation back into the system.
So I would think that one answer to your question
about reading is, can you tell what parts of the network
are saturated by looking at the activity levels
plus a plausible model?
And that might tell you what's being remembered, right?
Suppose you have some kinase that's saturated.
The whole population has been activated,
the whole available population.
So regardless of how the network is modulated,
that kinase is going to look active for some period of time
until those active kinases are deactivated somehow or other
by the activity of that network or some other network.
So it's going to look like it remembers
this part of the state space is always active.
Interesting.
But that kind of reading would require
having a model of the dynamics of that particular network.
And I think that's where your query to the neuroscientists
is very strange.
And neuroscience recognizes that brains are unique.
They have a unique developmental history.
And so they're similar at some gross scale,
and they get less similar as the scale goes down.
So without a predictive model of an individual brain,
you wouldn't be able to look at its dynamical state
and say much about the semantics.
You could say gross things about the semantics.
This person's representing our movement or something like that.
It's saying exactly which one would be difficult.
I mean, I think it sounds to me, and maybe I got it wrong,
but it sounds to me like the first part of what you said
about looking for the saturation and sinks and all that.
It sounds like you could tell that there was some kind of memory.
But just by finding these sinks, you
wouldn't be able to say what it was a memory of.
You wouldn't be able to recover the actual association.
That may well be the case.
You may be able to tell that it's in some memory,
yeah, without knowing which one.
Which is still cool.
That's sort of a step in the direction I was thinking about.
So maybe you can tell that this thing is...
Yeah, that it's learned something, but you couldn't tell what.
All right.
I mean, if that model is correct,
you should be able to look at the architecture of a network
and make predictions about what
unconditional stimuli to use to drive the system into a memory state.
Yeah, that's very interesting.
So we didn't do any of that, right?
So we did a purely empirical study.
We didn't try to predict how many memories there were going to be.
That's clearly a future thing we should do.
We just put every network through every possible combination
of holding every node as this role, that role,
and just did statistics on what works.
And we just found that the biological...
And they have multiple memories, of course,
that they can do simultaneously.
They can hold more than one and so on.
Yeah.
Well, some of those networks are quite complex.
Yeah.
And certainly of the ones you showed in the paper.
Yeah, some of them are,
although Santa pulled together, at least for the first paper,
some minimal ones that show each kind of memory
that are extremely simple,
like three nodes, just to illustrate
so that people could clearly see how it works.
So they don't really need to be very complex
just to show the basic phenomena.
But I was really interested in this notion
that if it's true that there are things that you cannot recover
about the contents of the network without interacting with it,
without being some kind of a pair with it,
that you are now stimulating it.
And now there's an interactive process
that I felt like that's interesting.
Like there might be some sort of quantity.
And I tried coming up with names for it and then and so on.
That might be like some sort of a metric
over the degree to which a system is resistant to understanding
from the outside without interacting with it.
Some kind of a metric of how much are you missing
by not being able to poke it by just pure inspection.
And I think it's a very steep curve
because already it looks to me like even these simple GRNs
already have some of that going on.
And then by brains, by family,
by the brains it must be very high.
Yeah, this reminds me of an ancient paper
by the logician Van Orman Kline,
who I think was at Harvard from the late fifties called,
I think the right paper is one called
Ontological Relativity.
But he does a thought experiment of an anthropologist
who's visiting some previously un-contacted tribe.
And he wanders around with members of this tribe
and they would point to various things
and say various words.
And Kline's point in this paper was,
without the assumption that their conceptual scheme
is exactly the same as yours,
which is not a justified assumption,
you won't be able to know that when the tribesman points
to something that looks to you like a rabbit
and says, gavagai, Kline's made up word,
that gavagai means rabbit.
It might mean furry animal at a certain distance from me
or any number of other things.
And his point is that even sort of asking
some finite number of questions, so even manipulating
the person or the situation in some finite number of ways,
you're not going to come up with a unique correct meaning
of this word.
I mean, right.
And the way to improve the situation
would be precisely with interventions
like show them something else and to test you,
to start testing the hypotheses.
So yeah, I mean, I think that's really interesting.
And that leads to this idea that the reason
that the interior inhabitant of the brain
has an easier time of it is because we are constantly
doing functional experiments via this active inverse kind
of loop.
You are constantly poking your own brain.
And so you do have access because of that loop
to these functional experiments that cannot be done by pure reads.
Yeah, and you've been doing it starting in infancy,
if not before.
So in particular, during your entire period
of learning the identities of external objects
or the identities for you of external objects
and the identities for you of words,
how you've been monitoring and building a model
of how your own brain is working.
And this gets back to the verbal babbling
and motor babbling and all that kind of babbling stuff.
Richard, you look unhappy.
Yeah, just because it's, you know, I should expect
this sort of mind bending from these conversations.
That's all. Two things.
One is, you know, just some thoughts
about what kind of attractors there are
in the natural networks that enable them
to do this kind of learning.
And the second one, who's doing the knowing
when you say, I know what this memory means?
So the first one is, so I'm imagining
that random networks might not have
very many different attractors in them,
in which case they just couldn't hold them.
Or they might have lots of attractors in them,
but the basin boundaries between one attractor
and another might be far away from the fixed points
so that you can be in this attractor,
which in a sense is holding a state,
or you can be in that attractor,
which isn't since it's holding a state.
But you can't teach it anything
because you have to get it from one attractor
to another in order for it to be in a different state.
So you need to be able to move it from one attractor
to another with training or with small perturbations,
which means that, and then it falls into the other attractor,
but it won't fall back, right?
Because otherwise it won't hold that state.
So the relationship between the positions
of the basin boundaries and the fixed points
of those attractors needs to be well close, basically.
And if you, if it's going to be possible for you to,
that's what's necessary for it to be,
as my intuition is,
that's what's necessary for it to be possible
for it to move from one state to another
through small perturbations and hold those states,
in other words, to be a flip flop.
But it's more than that, right?
Because you want it to be able to,
I can pick any attractor and by nudging it repeatedly
with this particular stimulus,
I can get it to go to a specific other attractor,
one where, where it's more comfortable
with that kind of nudging, basically.
Which I think might even suggest something like,
there's a basin boundary that's suitable for the thing
that you want to learn close to any fixed point attractor
that you might be starting from, something like that.
Like the basin boundaries need to be all folded up
and fractal and shit, because otherwise they can't all be
close enough to where you started from, to where you want to be.
Any thoughts on that first about what kind of attractor,
an attractor basin, an attractor structure a system
would need to have and whether you have any observations
about whether natural networks have that
and random networks don't?
Yeah, no, I follow you, we haven't done that,
we haven't done that analysis.
So that, that remains, you know, being able to,
because being able to predict that kind of stuff
is very important for us to be able to do that.
Being able to, because being able to predict that kind of stuff
is very valuable from the biomedical perspective,
you want to know if these networks, I mean,
so we're trying to exploit that now in addressing
drug habituation and so on, which is a big problem
in pharmaceuticals and being able to predict.
So that would be great, we haven't done any of that,
so I don't know yet.
Yeah, I wonder about the idea that, in a sense,
anything is learnable from any prior state.
You know, this is the great idea of the Jesuits
and B.F. Skinner and various other people.
But it may be that learning is more local
and that one can only learn certain things
if one's followed certain previous trajectories.
So that what you say about many, many attractors
from which a local, some local basin is accessible
may be true, but not globally.
I mean, there's always going to be an inductive bias,
isn't there?
Yeah, that would be one way of saying that or a prior knowledge bias.
Yeah, which is like what you're saying with the thought experiment
with the language too, right?
Yes, I gave you that stimulus and now I want to read it back,
but I didn't mean for you to learn that,
I meant for you to learn this.
This other thing, yeah.
Yeah, and if all possible states were reachable
through stimulation from all other possible states,
then there wouldn't be any inductive bias
and nothing would be learnable, I think.
Or anything would be learnable given enough vlogging,
which was sort of Skinner's position.
Yeah, but then, yeah, I think that doesn't make sense
because of the thought experiment,
because of the need that all induction has an inductive bias.
Yeah.
With respect to the being able to read a state
to see what the memory is,
this is just a sort of,
to test what exactly you mean by the question there.
So suppose I wanted to know whether the
attractor of the logistic map was a period four,
or period eight, or period two, or period three,
and I could read R.
Would you say,
like, what do I need to do to see,
like, because I can change R in a continuous way,
it's just a, you know, it's just a continuous variable,
and the memory can either be two, or four, or eight, or three,
depending on where I am in R space.
Does that mean that if I can read R,
then I have read the memory, or I could read R,
but I don't know what the memory is necessary unless I run it?
Yeah.
Yeah, I don't, to me, it would seem, I don't think you know what it is until,
unless you try it at the different things you want to try it at,
the different points you want to try that.
Yeah.
Yeah, and since knowing what the memory is, is knowing
from what previous attractor did the system fall into this new basin,
and since...
So the question is whether there's a syntactic manipulation you can do
to arrive at the result of what the period would be
without actually running the dynamical system,
which is a bit like saying, if I give you a program,
can you tell me what the output of the program is
without running the program?
So when I give you a lambda calculus expression,
and you do beta reductions on it, which are syntactic operations,
you're doing a sort of a shortcut that means
you don't actually have to run that bit,
because if you can do a beta reduction,
you found a sub expression that was the inverse of another sub expression,
and you cancel the margin, you don't have to run it.
Hmm, this question suggests RISES theorem,
which says that no Turing machine can tell you the function
computed by an arbitrary program.
Yeah, and if you can get different machines
by parameterizing continuous variables,
then you could read those continuous variables all day long,
and you don't know what the machine does
without running it, or having a simulation of it to run.
So I think that that might even be theoretically watertight,
that you can't necessarily do that.
Yeah, that's very interesting,
although I didn't realize that even something like this
is already got the halting problem,
you know, something as simple as this,
we are already in the halting problem.
Part of, so part of what motivates us is,
so it's funny, a number of people,
and I don't know who the reviewers were,
I don't know if they were biologists or computer scientists or what,
but there was at least one reviewer
who flatly claimed that it was impossible,
that if there isn't some register,
they can get their hands on about where the memory is,
if you don't, if nothing changes,
and that was his argument, right?
If literally, as we said in the introduction of the paper,
nothing changes about the structure of the network,
then where are you keeping these memories, right?
They just thought it wasn't...
But that's only because you use the qualifier structure, right?
I mean, something is changing, but it's not the structure.
Well, that's it, right?
So that to me is quite interesting,
you know, what people expect of these engrams,
and you know, you might think that then,
so the other thought I had about this is,
it's very much related to the scale you're looking at,
because if you did have,
let's say you did have a conventional flip-flop
or something that did have a register
that could store one or the other,
if you were at the wrong scale
and you picked up all the electrons and things,
you would say, well, none of these are scratched,
they're all in their original MN condition,
there is no memory here,
because you haven't changed the hardware at all, right?
And so then it seems like, yeah,
if you go down too far,
necessarily it will look like there isn't anywhere to store it,
it's always, right?
You can have it at one level and it doesn't look,
it doesn't make any sense at the level below,
because none of the parts have been altered.
I wonder, by the way, if you placed resolution limits
on the continuous numbers,
which might correspond to placing depth limits
on the computations,
then you don't have a halting problem anymore
and you can do syntactic reductions, I think.
So it is to do with the idea that in a continuous space
you can keep zooming in and there's more information
and you never know whether that information
is going to matter or not.
I don't think continuity is the issue.
I mean, Turing machines aren't continuous.
No, they're not, but if you...
It's just having the ability, yeah,
they have effectively infinite discrete memories.
Right, so the only way that a continuous fixed dimensional system
could do the same thing that a Turing machine does
is if it keeps folding extra information
into the spaces in between the numbers it's already used.
Otherwise, it's going to be fine.
And then you won't have a halting problem.
Right, but that can go to a discrete infinite limit
as opposed to...
It doesn't have to go to a continuous infinite limit.
I see.
So continuity isn't the issue, but it is a resource issue.
Right, so...
You're correct about that.
I see, so they could be if there isn't a maximum...
If there isn't a maximum and minimum on your values,
they could be discrete, but still be an infinite set
and you could have a halting problem.
If there was a maximum and minimum limit,
then they would have to be continuous in order to get...
No, there's still a countable number of rationals between one and zero.
Oh, the old infinities thing.
Okay, yeah.
Okay, so that all makes sense to me,
but then what's the deal with our...
In our first paper, it was all Boolean networks.
It was... They weren't continuous.
Right, so that means that there's a finite number of states
they can have and a finite number of programs they can be.
Yeah.
Which could be large and effectively hard to predict,
but not necessarily impossible.
So if everything's finite, you can, in principle, enumerate.
Right, just enumerate all the possible states
and all the possible trajectories through the state space.
But that enumeration...
Solving that enumeration problem is exponential,
so it's not feasible as the numbers get even...
And besides, Mike would count that as...
Well, you poked it and examined all the things that it did, I think.
Yeah, that sounds like...
Well, if it's a finite number of parts and a finite number of states,
then you know, in principle, all of the things that it could do
and you can enumerate them.
But wouldn't that... But wait a minute.
The problem is still intractable.
But wait a minute, so imagine the same Boolean network.
Here it is when we started, here it is once we've trained it.
The same finite number of states you can enumerate,
but how do you tell the difference between them?
You would still have to do...
You're allowed to read which state it's in, right?
Right. Yeah, you're starting from different states.
When you probe it...
I mean, it's got this gigantic Boolean state space, right?
And different parts of that state space respond very differently
to bit flip perturbations of part of the network.
Okay, so what you have there is basically a surrogate of the real thing,
because you've got all the possible things it could ever do.
So you've got this like exploded surrogate model of it,
but you don't have to perturb it.
You can just read it once you know which state.
Does that true?
If you knew what the...
You could probe it from some computer.
You could select some initial state out of this, you know,
two to the however many states.
And you could then look at all the paths of length,
two or three or four or whatever from that state,
which would explore a little local neighborhood of the search space.
Yeah, I'm trying to decide if that counts as cheating or not in my experience.
Yeah. Yeah, I see, I see.
Yeah.
But again, I think you couldn't extract semantics from that.
Unless you knew what perturbations were used to get into that little local part of the state space.
If you think of the semantics in this historical sense, what did the system learn?
Yeah.
Yeah.
So you could say, even if you could say what it does,
you couldn't say the meaning of what it does.
The meaning of what it does is given by the history, which caused it to do that.
So you could say, you know, you could say what it does,
is given by the history, which caused it to do that.
But isn't that interesting?
I don't know.
So maybe that's obvious, but I've been sort of shocked by this for weeks now,
and this idea that even something that simple has an internal perspective
that cannot be read out equally without becoming an interactive partner with it in some way.
So that was the other thing that was hurting in your conversation.
Hurting me in your conversation earlier was this idea of how come I know what my memories mean,
but it's more difficult to read somebody else's memories.
It's like, well, if I had had a lot of exposure to somebody else's memories,
then perhaps I could disambiguate what their internal model was through the impoverished
observations, which I may go for a long period of time.
But is that what I'm doing with my own memory?
Am I internalizing a model of my own internal memory by experiencing it over time?
If so, where am I putting that internal model?
It's like, that's not what I'm doing, right?
It's like, it is the model.
I mean, yes.
And certainly, I think Dan Danett would say that that whole way of phrasing
the question is too dualistic or whatever, but you're not reading your brain.
You are your brain.
Okay, but I understand.
But you can imagine clinical cases.
And actually, I think that what you see when people are coming out of anesthesia
is basically a small version of it, where if you I could easily imagine cases where
if it wasn't the case that it was so easy for us to interpret our engrams correctly,
just imagine that every 300 milliseconds or something, we would lose track of what the
hell was going on.
And then we would be as confused as modern neuroscientists are when they try to read
a brain, you know, they kind of sort of can tell a few things, but it's terrible.
Like you could easily imagine what it would look like for us to be bad at this.
So I do think there's a difference there.
And when people come out of anesthesia, you know, the general anesthesia has decoupled
all the gap junctions, right?
So the network is the network has to reform and sort of find its way.
And for the first couple of hours, they think they're pirates and gangsters and whatever
they're trying to sort of make sense of where and you can watch people.
It's very humorous.
You can watch people come on.
Some people, by the way, this is why they don't like to use general anesthesia when
they don't need to.
Some people don't.
So some people have psychotic breaks from it and it's rare, but it happens where they just
never come back to the right place.
And presumably the materials are there.
You haven't killed any cells.
You haven't destroyed any, you know, whatever, you know, glansmen thinks it's RNAs.
You haven't taken out any RNAs.
It's the dynamical thing.
And if you don't know how to interpret your own memories, it looks pretty bad for a while
until you figure it out.
So I do think there's a way that it wouldn't be this way, right?
Because I think Dennett's argument would be, how else could it be?
It has to be this way because you're just the big dynamical system.
And it is what it is.
I mean, it seems fine to me that you might not recover the same dynamical state,
but it doesn't seem fine to me when you say, if you don't know how to interpret those memories,
that language seems bothersome.
Yeah. And I think what proof, I get it.
And maybe we could eliminate all of that talk, except for, for example,
memory transfer experiments, right?
So something like what glansmen does in Eplizia or what people did in Plenaria,
where you take out some structure.
And so now we're back to having kind of a physical register for things
and shove it into a new brain.
The Eplizia work is amazing because you don't even have to,
he just injects the stuff into the brain, the space in between the neurons.
They didn't even put it in any specific place.
And they somehow take it up and it's interpreted in the correct way,
which is the same way that the training happened, right?
So much like with the Plenaria experiments, the recipient isn't confused,
you know, what this RNA molecule, what the hell could this mean?
They have the link between the stimulus and the fear conditioning, whatever it is.
So, right?
So there's, yeah, I don't know.
There is some aspect.
It's like you can't separate the meaning of the symbol from the symbol, right?
Like the, if you're going to transfer that bit of stuff,
it's going to mean the same thing wherever it goes.
It's binding.
It's grounding between the symbol and the outside world is intrinsic to it.
It's not interpreted by an external observer.
Well, and this is a case where, in a sense, humans are more vulnerable than a pleasure, right?
I mean, in the case of humans, I'll take the example of coming out of anesthesia.
I mean, one of the things that has to happen is that the language production system that allows
you to report what's going on to the doctor has to recouple correctly to the whole body
representation, you know, as well as all the bodily, you know,
event memories and things like that, to just think of the body representation.
I mean, if something goes wrong in that recoupling,
you may well get the wrong words assigned to emotions or things like that.
I mean, those systems are sparsely coupled.
They're not deeply coupled biochemically in this, the way that I think in the amplusia case,
it seems like a much deeper kind of coupling than we have
between our language systems and the rest of the brain.
Suppose that the parts of, suppose the different parts of my body had structures that core was
dynamical activation patterns to resonate in a particular way.
And inside my brain, a resonance that's created in the electrical activation patterns between
the neurons resonates to whatever it is is going on in that part of the body.
And if it does it there in the brain for a while, it will create structural connections
in the brain, which make those resonant dynamical patterns in the brain easier.
Then the structure that was the physiological part of the body and the structure that's inside
the brain are really the little models of each other in so much as they create the same resonance.
If both switched on again, would couple up in the same resonant connection,
which is like saying the thing that was in the brain wasn't an arbitrary symbol.
The thing that was in the brain was a model of the thing that it means.
It can be in a different substrate and the connections can be made of different materials and
stuff, but it still is a dynamical model. It's a deeply causal dynamical model of the same thing.
And it's binding to the thing that it means comes from that.
Analog relationship between them.
Yeah, that way of thinking would suggest that say reconnecting the somatosensory homunculus
is a lot more reliable than reconnecting the language system
for naming body parts or something like that.
Well, that's actually, I hadn't thought of that before, but Chris, that's a super interesting
idea for experiments because that suggests that if we were to give people sensory motor
augmentation, so new hands and extra, some extra thumbs as they have, all this kind of
prosthetic stuff that people get now, doing it during and then after anesthesia might be really
interesting because then you're coming back to not quite the same body. You've got extra
effectors and you've got to remap and how fast does that work. And that connects to this issue of
then if Richard is right, then you ought to be able to move these things across
widely divergent body implementations. We've thought about that a little bit in terms of
looking at memories in things like xenobots versus the frog they came from versus whatever
else we can make the xenobots into. To the extent that these things will carry their
sort of meaning internally, what else can you put them into? And can you, somebody in the 90s,
somebody did these crazy experiments putting Drosophila neurons into human, I think Parkinson's
patients or something. And so the question is how much can you get from a fly brain into a human
brain? But with all this synthetic stuff, we can do some of that now. We can do those experiments
and ask how much does it carry over. But it's amazing to me, people ask, like I've got this,
you know, the spectrum of persuadability or whatever and people say, well, how far down
does it go? It's just amazing that all of these issues already kick up by the time you have three
or four nodes in the in the GRN, you're already knee deep in all of this stuff. It doesn't take
much at all. You know, it's like so early that you already get into this. That's a very cool
observation. I am very intrigued by these thoughts and the possibility of
the ability to move between one attractor and another in a sort of nested or hierarchical
way so that the so that an association can be made that might be shallow or an association
can be made that might be deeper, that shares more history, shares more experience would create
deeper resonances. And the possibility that what that would look like when you were looking like
a way to tell the difference between a biological network and a random network would be to do with
how fractal the attractors are and how harmonic the dynamics are. And that's, you know, in random
networks, there wasn't anything very harmonic about the dynamics and that's why you couldn't
push them from one to another. Yeah, yeah, I think I think I think there's a bunch of work
that could be done on analyzing these things very, very similar to
yeah, to what people do with connectomics and in the in the brain for neural decoding and things
like that. And then again, trying to, I mean, there's a thing I hadn't thought of before,
can we can we move memories between GRNs? What would you have to do? What does a memory
transplant look like from right from a network? Well, yeah, so
isn't that the embedding theorem? I mean, if you have a continuous dynamical system in one
in one network, and you can read at least one of the variables on that network and connect it to
another network, which has suitably high dimensional internal dimensionality,
you can induce the dynamics from one network to the other.
The question would be, what does the what does the same dynamics mean when we move to a different
network? Yeah, yeah, yeah, yeah. Because because we can start right, we could we can start with a
net with a with a, you know, a cancer network. And then we can move that into, I don't know,
some sort of plant metabolism network that's in the database, and ask what does the what what
how does that get it. And again, I know, I know, you know, we may or may not want to use the word
interpreted. But but but in a different context, there is some version of asking that question,
right, is what what does this now mean? Well, it ought to mean the same as, you know, if if you've
been exposed to some training, and then I've been exposed to you, then what it means to me
ought to be ought to mean the same as if I'd been exposed to that training.
So that that's assuming a lot of shared semantics and communication, though. I mean,
I think this scenario is very similar to what you see with with side effects of drugs.
The drug is moving some network and some target cell into a new state.
But that same drug may move the same or a fairly similar network and some
non target cell into a new state. And that manifests as a side effect of some sort that's
undesirable. So in a sense, yeah, but that's because nothing with the memory in one case,
or creating constructing the memory in one case, leads you to one one semantics.
And doing the same thing in a closely related case leads to a completely different semantics
because it's embedded in a different context. Yeah, but the drug wasn't moving the memory
from one place to another. It was just moving a chemical from one place to another. And there
wasn't any good reason to believe that the effect of the chemical in one system would
be the same as the effect of the chemical in the other system. If you if the memory is encoded by
some part of the network state, you may be reproducing that network state in the different
network. If that network state is the engram in this case, then in a sense, you're moving the
memory or you're reproducing the memory. Well, but then if you were reproducing the memory,
then the drug would work. Not necessarily, because that memory may be pathological in a
different context, a different cellular context. I mean, that would be that would be like saying
that that would be like saying the drug did have the effect that I wanted it to have.
But in this patient, that effect wasn't the one that they wanted.
Which which really suggests something interesting for the therapeutics, which is that
if I mean, so Richard, what you were saying before is that being a, you know, if I
if I'm exposed to something and I learn it, then then you're exposed to me, then you will,
you know, then you will have the, I mean, that works because there's this nice communication
interface between us, which may be maybe linguistic or it may be something else.
So that suggests that in order to move these memories, what you need is some sort of a
the equivalent of a communication interface, right, to make it make sense. So I take it from
this context in an order for it to make, I can't just plop it down exactly as it is,
you know, which is right. So so if I've been exposed to something, me showing you my brain
states aren't nearly as good as going through this interface.
Yeah. Yeah. Like you have to, it's almost as though you have to be replaying it for me
that the interface has to induce the same dynamics in me that it did in you.
Which suggests that the communication channel
can't be, oh, I don't know, that seems a bit of a strong claim. I'll finish the sentence anyway,
that the communication channel channel can't be, that it can only be symbolic if you
presuppose that we have a shared inductive bias. Otherwise, it needs to be, you know,
non symbolic, like a resonance, like an imprint that's an, an, an analogical transfer.
But you have to, in a sense, sort of replay all the experiences you had in the same dimensionality
in which you observed them so that I can just learn from them like you did.
Which is actually deeply, this deeply reminds me of a problem that I'm currently working on,
which is general descriptions of communication protocols that involve two different kinds of
resources. And in this case, it's a quantum resource and a classical resource. And
the bottom line is you need two channels to have effective communication.
And if you have two systems and they're manipulating some shared resource,
they have to be able to talk about what they're doing to each other to coordinate their manipulations.
So that
my manipulation of the resources is interpretable to you. I mean, if you assume that they share
the same language, then you don't have this problem. But of course, the shared language
is another communication channel. It's just historic, not real time. Or the acquisition
of the shared language as a, as a historic shared communication channel.
So if Mike learned something by watching a video and then wanted me to know what he had learned,
one way that he could do that is just by being a video camera that recorded what he
looked at and then playing it to me. And then I could just learn it from that. And it didn't
require any interpretation. And it didn't require me to know anything about Mike's internal state
to do that. But if he's going to communicate it to me in a compressed way, then, well, now I need
to have some knowledge about his internal architectural structures in order for that to
make sense to me in the same way that it made sense to him, which I could get by knowing something
about his history, right? So if that would be like saying, well, if we, if we both watch this video,
and then Mike watches another one and tells me about it in the language of the first video,
then, then I know what he means in so much as it was interpreted through the history of the first
video. Yeah, you know, I mean, I mean, if the two of you watch the same video, and,
and in your conceptual scheme is completely scrambled with respect to Mike's,
then you won't get the same message out of the video. You know, if you have entirely different
notions of what counts as an object, for example, you're not going to get the same information
from the video. I mean, just thinking about that in real world cases, I've certainly watched
things with someone and realized that we got completely different things out of what we just
saw, right? And that, and that, in fact, the, and then in fact, doing what you just said, so being
the camera wouldn't have worked at all. And that if we want to get the shared thing, I got to do all
sorts of, you know, manipulations of the data to make sure that we actually got the same thing out
of a video, right? Yeah.
