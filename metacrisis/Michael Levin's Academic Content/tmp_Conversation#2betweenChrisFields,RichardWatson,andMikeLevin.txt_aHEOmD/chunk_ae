a way to tell the difference between a biological network and a random network would be to do with
how fractal the attractors are and how harmonic the dynamics are. And that's, you know, in random
networks, there wasn't anything very harmonic about the dynamics and that's why you couldn't
push them from one to another. Yeah, yeah, I think I think I think there's a bunch of work
that could be done on analyzing these things very, very similar to
yeah, to what people do with connectomics and in the in the brain for neural decoding and things
like that. And then again, trying to, I mean, there's a thing I hadn't thought of before,
can we can we move memories between GRNs? What would you have to do? What does a memory
transplant look like from right from a network? Well, yeah, so
isn't that the embedding theorem? I mean, if you have a continuous dynamical system in one
in one network, and you can read at least one of the variables on that network and connect it to
another network, which has suitably high dimensional internal dimensionality,
you can induce the dynamics from one network to the other.
The question would be, what does the what does the same dynamics mean when we move to a different
network? Yeah, yeah, yeah, yeah. Because because we can start right, we could we can start with a
net with a with a, you know, a cancer network. And then we can move that into, I don't know,
some sort of plant metabolism network that's in the database, and ask what does the what what
how does that get it. And again, I know, I know, you know, we may or may not want to use the word
interpreted. But but but in a different context, there is some version of asking that question,
right, is what what does this now mean? Well, it ought to mean the same as, you know, if if you've
been exposed to some training, and then I've been exposed to you, then what it means to me
ought to be ought to mean the same as if I'd been exposed to that training.
So that that's assuming a lot of shared semantics and communication, though. I mean,
I think this scenario is very similar to what you see with with side effects of drugs.
The drug is moving some network and some target cell into a new state.
But that same drug may move the same or a fairly similar network and some
non target cell into a new state. And that manifests as a side effect of some sort that's
undesirable. So in a sense, yeah, but that's because nothing with the memory in one case,
or creating constructing the memory in one case, leads you to one one semantics.
And doing the same thing in a closely related case leads to a completely different semantics
because it's embedded in a different context. Yeah, but the drug wasn't moving the memory
from one place to another. It was just moving a chemical from one place to another. And there
wasn't any good reason to believe that the effect of the chemical in one system would
be the same as the effect of the chemical in the other system. If you if the memory is encoded by
some part of the network state, you may be reproducing that network state in the different
network. If that network state is the engram in this case, then in a sense, you're moving the
memory or you're reproducing the memory. Well, but then if you were reproducing the memory,
then the drug would work. Not necessarily, because that memory may be pathological in a
different context, a different cellular context. I mean, that would be that would be like saying
that that would be like saying the drug did have the effect that I wanted it to have.
But in this patient, that effect wasn't the one that they wanted.
Which which really suggests something interesting for the therapeutics, which is that
if I mean, so Richard, what you were saying before is that being a, you know, if I
if I'm exposed to something and I learn it, then then you're exposed to me, then you will,
you know, then you will have the, I mean, that works because there's this nice communication
interface between us, which may be maybe linguistic or it may be something else.
So that suggests that in order to move these memories, what you need is some sort of a
the equivalent of a communication interface, right, to make it make sense. So I take it from
this context in an order for it to make, I can't just plop it down exactly as it is,
you know, which is right. So so if I've been exposed to something, me showing you my brain
states aren't nearly as good as going through this interface.
Yeah. Yeah. Like you have to, it's almost as though you have to be replaying it for me
that the interface has to induce the same dynamics in me that it did in you.
Which suggests that the communication channel
can't be, oh, I don't know, that seems a bit of a strong claim. I'll finish the sentence anyway,
that the communication channel channel can't be, that it can only be symbolic if you
presuppose that we have a shared inductive bias. Otherwise, it needs to be, you know,
non symbolic, like a resonance, like an imprint that's an, an, an analogical transfer.
But you have to, in a sense, sort of replay all the experiences you had in the same dimensionality
in which you observed them so that I can just learn from them like you did.
Which is actually deeply, this deeply reminds me of a problem that I'm currently working on,
which is general descriptions of communication protocols that involve two different kinds of
resources. And in this case, it's a quantum resource and a classical resource. And
the bottom line is you need two channels to have effective communication.
And if you have two systems and they're manipulating some shared resource,
they have to be able to talk about what they're doing to each other to coordinate their manipulations.
So that
my manipulation of the resources is interpretable to you. I mean, if you assume that they share
the same language, then you don't have this problem. But of course, the shared language
is another communication channel. It's just historic, not real time. Or the acquisition
of the shared language as a, as a historic shared communication channel.
So if Mike learned something by watching a video and then wanted me to know what he had learned,
one way that he could do that is just by being a video camera that recorded what he
looked at and then playing it to me. And then I could just learn it from that. And it didn't
require any interpretation. And it didn't require me to know anything about Mike's internal state
to do that. But if he's going to communicate it to me in a compressed way, then, well, now I need
to have some knowledge about his internal architectural structures in order for that to
make sense to me in the same way that it made sense to him, which I could get by knowing something
about his history, right? So if that would be like saying, well, if we, if we both watch this video,
and then Mike watches another one and tells me about it in the language of the first video,
then, then I know what he means in so much as it was interpreted through the history of the first
video. Yeah, you know, I mean, I mean, if the two of you watch the same video, and,
and in your conceptual scheme is completely scrambled with respect to Mike's,
then you won't get the same message out of the video. You know, if you have entirely different
notions of what counts as an object, for example, you're not going to get the same information
from the video. I mean, just thinking about that in real world cases, I've certainly watched
things with someone and realized that we got completely different things out of what we just
saw, right? And that, and that, in fact, the, and then in fact, doing what you just said, so being
the camera wouldn't have worked at all. And that if we want to get the shared thing, I got to do all
sorts of, you know, manipulations of the data to make sure that we actually got the same thing out
of a video, right? Yeah.
