be enough to determine what the relationship would
be without simulating the relationship.
Exactly.
Exactly.
Yeah, can I back up a little bit here
and ask about the random networks in the paper
with Sarama and Wes.
Do the random networks have basically random dynamics?
And the biological networks have saturation dynamics?
Or do the random networks all have saturation dynamics
and all the reactions?
Well, I'm going to ask what you mean by saturation dynamics.
But just the first thing to say about the random networks
is that actually generating random networks
is really a non-trivial task.
Because if you want to sort of, if you don't want to bias,
how do I generate the space of all possible networks?
It's not easy.
So we did our best in trying to be fair.
But no doubt, we're still caught in a region
of the space of possible ones.
But what do you mean saturation dynamics?
Well, just dynamics that are basically
a shape that have a long saturated region at the bottom
and then some curves, roughly linear region, curve,
roughly saturated region.
Because if you drive a network or some components
of the network from the linear region to a saturation region,
it's going to behave differently.
It's going to behave now like an infinite source
or an infinite sink, depending on which of those
you drove it into.
And once you have something that acts like an infinite source,
it's going to behave as a memory for some time
until that source is depleted by constantly being read
and having to feed activation back into the system.
So I would think that one answer to your question
about reading is, can you tell what parts of the network
are saturated by looking at the activity levels
plus a plausible model?
And that might tell you what's being remembered, right?
Suppose you have some kinase that's saturated.
The whole population has been activated,
the whole available population.
So regardless of how the network is modulated,
that kinase is going to look active for some period of time
until those active kinases are deactivated somehow or other
by the activity of that network or some other network.
So it's going to look like it remembers
this part of the state space is always active.
Interesting.
But that kind of reading would require
having a model of the dynamics of that particular network.
And I think that's where your query to the neuroscientists
is very strange.
And neuroscience recognizes that brains are unique.
They have a unique developmental history.
And so they're similar at some gross scale,
and they get less similar as the scale goes down.
So without a predictive model of an individual brain,
you wouldn't be able to look at its dynamical state
and say much about the semantics.
You could say gross things about the semantics.
This person's representing our movement or something like that.
It's saying exactly which one would be difficult.
I mean, I think it sounds to me, and maybe I got it wrong,
but it sounds to me like the first part of what you said
about looking for the saturation and sinks and all that.
It sounds like you could tell that there was some kind of memory.
But just by finding these sinks, you
wouldn't be able to say what it was a memory of.
You wouldn't be able to recover the actual association.
That may well be the case.
You may be able to tell that it's in some memory,
yeah, without knowing which one.
Which is still cool.
That's sort of a step in the direction I was thinking about.
So maybe you can tell that this thing is...
Yeah, that it's learned something, but you couldn't tell what.
All right.
I mean, if that model is correct,
you should be able to look at the architecture of a network
and make predictions about what
unconditional stimuli to use to drive the system into a memory state.
Yeah, that's very interesting.
So we didn't do any of that, right?
So we did a purely empirical study.
We didn't try to predict how many memories there were going to be.
That's clearly a future thing we should do.
We just put every network through every possible combination
of holding every node as this role, that role,
and just did statistics on what works.
And we just found that the biological...
And they have multiple memories, of course,
that they can do simultaneously.
They can hold more than one and so on.
Yeah.
Well, some of those networks are quite complex.
Yeah.
And certainly of the ones you showed in the paper.
Yeah, some of them are,
although Santa pulled together, at least for the first paper,
some minimal ones that show each kind of memory
that are extremely simple,
like three nodes, just to illustrate
so that people could clearly see how it works.
So they don't really need to be very complex
just to show the basic phenomena.
But I was really interested in this notion
that if it's true that there are things that you cannot recover
about the contents of the network without interacting with it,
without being some kind of a pair with it,
that you are now stimulating it.
And now there's an interactive process
that I felt like that's interesting.
Like there might be some sort of quantity.
And I tried coming up with names for it and then and so on.
That might be like some sort of a metric
over the degree to which a system is resistant to understanding
from the outside without interacting with it.
Some kind of a metric of how much are you missing
by not being able to poke it by just pure inspection.
And I think it's a very steep curve
because already it looks to me like even these simple GRNs
already have some of that going on.
And then by brains, by family,
by the brains it must be very high.
Yeah, this reminds me of an ancient paper
by the logician Van Orman Kline,
who I think was at Harvard from the late fifties called,
I think the right paper is one called
Ontological Relativity.
But he does a thought experiment of an anthropologist
who's visiting some previously un-contacted tribe.
And he wanders around with members of this tribe
and they would point to various things
and say various words.
And Kline's point in this paper was,
without the assumption that their conceptual scheme
is exactly the same as yours,
which is not a justified assumption,
you won't be able to know that when the tribesman points
to something that looks to you like a rabbit
and says, gavagai, Kline's made up word,
that gavagai means rabbit.
It might mean furry animal at a certain distance from me
or any number of other things.
And his point is that even sort of asking
some finite number of questions, so even manipulating
the person or the situation in some finite number of ways,
you're not going to come up with a unique correct meaning
of this word.
I mean, right.
And the way to improve the situation
would be precisely with interventions
like show them something else and to test you,
to start testing the hypotheses.
So yeah, I mean, I think that's really interesting.
And that leads to this idea that the reason
that the interior inhabitant of the brain
has an easier time of it is because we are constantly
doing functional experiments via this active inverse kind
of loop.
You are constantly poking your own brain.
And so you do have access because of that loop
to these functional experiments that cannot be done by pure reads.
Yeah, and you've been doing it starting in infancy,
if not before.
So in particular, during your entire period
of learning the identities of external objects
or the identities for you of external objects
and the identities for you of words,
how you've been monitoring and building a model
of how your own brain is working.
And this gets back to the verbal babbling
and motor babbling and all that kind of babbling stuff.
Richard, you look unhappy.
Yeah, just because it's, you know, I should expect
this sort of mind bending from these conversations.
That's all. Two things.
One is, you know, just some thoughts
about what kind of attractors there are
in the natural networks that enable them
to do this kind of learning.
And the second one, who's doing the knowing
when you say, I know what this memory means?
So the first one is, so I'm imagining
that random networks might not have
very many different attractors in them,
in which case they just couldn't hold them.
Or they might have lots of attractors in them,
but the basin boundaries between one attractor
and another might be far away from the fixed points
so that you can be in this attractor,
which in a sense is holding a state,
or you can be in that attractor,
which isn't since it's holding a state.
But you can't teach it anything
because you have to get it from one attractor
to another in order for it to be in a different state.
