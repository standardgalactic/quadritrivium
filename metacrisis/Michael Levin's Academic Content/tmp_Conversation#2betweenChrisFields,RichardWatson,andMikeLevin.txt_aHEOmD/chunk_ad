If there was a maximum and minimum limit,
then they would have to be continuous in order to get...
No, there's still a countable number of rationals between one and zero.
Oh, the old infinities thing.
Okay, yeah.
Okay, so that all makes sense to me,
but then what's the deal with our...
In our first paper, it was all Boolean networks.
It was... They weren't continuous.
Right, so that means that there's a finite number of states
they can have and a finite number of programs they can be.
Yeah.
Which could be large and effectively hard to predict,
but not necessarily impossible.
So if everything's finite, you can, in principle, enumerate.
Right, just enumerate all the possible states
and all the possible trajectories through the state space.
But that enumeration...
Solving that enumeration problem is exponential,
so it's not feasible as the numbers get even...
And besides, Mike would count that as...
Well, you poked it and examined all the things that it did, I think.
Yeah, that sounds like...
Well, if it's a finite number of parts and a finite number of states,
then you know, in principle, all of the things that it could do
and you can enumerate them.
But wouldn't that... But wait a minute.
The problem is still intractable.
But wait a minute, so imagine the same Boolean network.
Here it is when we started, here it is once we've trained it.
The same finite number of states you can enumerate,
but how do you tell the difference between them?
You would still have to do...
You're allowed to read which state it's in, right?
Right. Yeah, you're starting from different states.
When you probe it...
I mean, it's got this gigantic Boolean state space, right?
And different parts of that state space respond very differently
to bit flip perturbations of part of the network.
Okay, so what you have there is basically a surrogate of the real thing,
because you've got all the possible things it could ever do.
So you've got this like exploded surrogate model of it,
but you don't have to perturb it.
You can just read it once you know which state.
Does that true?
If you knew what the...
You could probe it from some computer.
You could select some initial state out of this, you know,
two to the however many states.
And you could then look at all the paths of length,
two or three or four or whatever from that state,
which would explore a little local neighborhood of the search space.
Yeah, I'm trying to decide if that counts as cheating or not in my experience.
Yeah. Yeah, I see, I see.
Yeah.
But again, I think you couldn't extract semantics from that.
Unless you knew what perturbations were used to get into that little local part of the state space.
If you think of the semantics in this historical sense, what did the system learn?
Yeah.
Yeah.
So you could say, even if you could say what it does,
you couldn't say the meaning of what it does.
The meaning of what it does is given by the history, which caused it to do that.
So you could say, you know, you could say what it does,
is given by the history, which caused it to do that.
But isn't that interesting?
I don't know.
So maybe that's obvious, but I've been sort of shocked by this for weeks now,
and this idea that even something that simple has an internal perspective
that cannot be read out equally without becoming an interactive partner with it in some way.
So that was the other thing that was hurting in your conversation.
Hurting me in your conversation earlier was this idea of how come I know what my memories mean,
but it's more difficult to read somebody else's memories.
It's like, well, if I had had a lot of exposure to somebody else's memories,
then perhaps I could disambiguate what their internal model was through the impoverished
observations, which I may go for a long period of time.
But is that what I'm doing with my own memory?
Am I internalizing a model of my own internal memory by experiencing it over time?
If so, where am I putting that internal model?
It's like, that's not what I'm doing, right?
It's like, it is the model.
I mean, yes.
And certainly, I think Dan Danett would say that that whole way of phrasing
the question is too dualistic or whatever, but you're not reading your brain.
You are your brain.
Okay, but I understand.
But you can imagine clinical cases.
And actually, I think that what you see when people are coming out of anesthesia
is basically a small version of it, where if you I could easily imagine cases where
if it wasn't the case that it was so easy for us to interpret our engrams correctly,
just imagine that every 300 milliseconds or something, we would lose track of what the
hell was going on.
And then we would be as confused as modern neuroscientists are when they try to read
a brain, you know, they kind of sort of can tell a few things, but it's terrible.
Like you could easily imagine what it would look like for us to be bad at this.
So I do think there's a difference there.
And when people come out of anesthesia, you know, the general anesthesia has decoupled
all the gap junctions, right?
So the network is the network has to reform and sort of find its way.
And for the first couple of hours, they think they're pirates and gangsters and whatever
they're trying to sort of make sense of where and you can watch people.
It's very humorous.
You can watch people come on.
Some people, by the way, this is why they don't like to use general anesthesia when
they don't need to.
Some people don't.
So some people have psychotic breaks from it and it's rare, but it happens where they just
never come back to the right place.
And presumably the materials are there.
You haven't killed any cells.
You haven't destroyed any, you know, whatever, you know, glansmen thinks it's RNAs.
You haven't taken out any RNAs.
It's the dynamical thing.
And if you don't know how to interpret your own memories, it looks pretty bad for a while
until you figure it out.
So I do think there's a way that it wouldn't be this way, right?
Because I think Dennett's argument would be, how else could it be?
It has to be this way because you're just the big dynamical system.
And it is what it is.
I mean, it seems fine to me that you might not recover the same dynamical state,
but it doesn't seem fine to me when you say, if you don't know how to interpret those memories,
that language seems bothersome.
Yeah. And I think what proof, I get it.
And maybe we could eliminate all of that talk, except for, for example,
memory transfer experiments, right?
So something like what glansmen does in Eplizia or what people did in Plenaria,
where you take out some structure.
And so now we're back to having kind of a physical register for things
and shove it into a new brain.
The Eplizia work is amazing because you don't even have to,
he just injects the stuff into the brain, the space in between the neurons.
They didn't even put it in any specific place.
And they somehow take it up and it's interpreted in the correct way,
which is the same way that the training happened, right?
So much like with the Plenaria experiments, the recipient isn't confused,
you know, what this RNA molecule, what the hell could this mean?
They have the link between the stimulus and the fear conditioning, whatever it is.
So, right?
So there's, yeah, I don't know.
There is some aspect.
It's like you can't separate the meaning of the symbol from the symbol, right?
Like the, if you're going to transfer that bit of stuff,
it's going to mean the same thing wherever it goes.
It's binding.
It's grounding between the symbol and the outside world is intrinsic to it.
It's not interpreted by an external observer.
Well, and this is a case where, in a sense, humans are more vulnerable than a pleasure, right?
I mean, in the case of humans, I'll take the example of coming out of anesthesia.
I mean, one of the things that has to happen is that the language production system that allows
you to report what's going on to the doctor has to recouple correctly to the whole body
representation, you know, as well as all the bodily, you know,
event memories and things like that, to just think of the body representation.
I mean, if something goes wrong in that recoupling,
you may well get the wrong words assigned to emotions or things like that.
I mean, those systems are sparsely coupled.
They're not deeply coupled biochemically in this, the way that I think in the amplusia case,
it seems like a much deeper kind of coupling than we have
between our language systems and the rest of the brain.
Suppose that the parts of, suppose the different parts of my body had structures that core was
dynamical activation patterns to resonate in a particular way.
And inside my brain, a resonance that's created in the electrical activation patterns between
the neurons resonates to whatever it is is going on in that part of the body.
And if it does it there in the brain for a while, it will create structural connections
in the brain, which make those resonant dynamical patterns in the brain easier.
Then the structure that was the physiological part of the body and the structure that's inside
the brain are really the little models of each other in so much as they create the same resonance.
If both switched on again, would couple up in the same resonant connection,
which is like saying the thing that was in the brain wasn't an arbitrary symbol.
The thing that was in the brain was a model of the thing that it means.
It can be in a different substrate and the connections can be made of different materials and
stuff, but it still is a dynamical model. It's a deeply causal dynamical model of the same thing.
And it's binding to the thing that it means comes from that.
Analog relationship between them.
Yeah, that way of thinking would suggest that say reconnecting the somatosensory homunculus
is a lot more reliable than reconnecting the language system
for naming body parts or something like that.
Well, that's actually, I hadn't thought of that before, but Chris, that's a super interesting
idea for experiments because that suggests that if we were to give people sensory motor
augmentation, so new hands and extra, some extra thumbs as they have, all this kind of
prosthetic stuff that people get now, doing it during and then after anesthesia might be really
interesting because then you're coming back to not quite the same body. You've got extra
effectors and you've got to remap and how fast does that work. And that connects to this issue of
then if Richard is right, then you ought to be able to move these things across
widely divergent body implementations. We've thought about that a little bit in terms of
looking at memories in things like xenobots versus the frog they came from versus whatever
else we can make the xenobots into. To the extent that these things will carry their
sort of meaning internally, what else can you put them into? And can you, somebody in the 90s,
somebody did these crazy experiments putting Drosophila neurons into human, I think Parkinson's
patients or something. And so the question is how much can you get from a fly brain into a human
brain? But with all this synthetic stuff, we can do some of that now. We can do those experiments
and ask how much does it carry over. But it's amazing to me, people ask, like I've got this,
you know, the spectrum of persuadability or whatever and people say, well, how far down
does it go? It's just amazing that all of these issues already kick up by the time you have three
or four nodes in the in the GRN, you're already knee deep in all of this stuff. It doesn't take
much at all. You know, it's like so early that you already get into this. That's a very cool
observation. I am very intrigued by these thoughts and the possibility of
the ability to move between one attractor and another in a sort of nested or hierarchical
way so that the so that an association can be made that might be shallow or an association
can be made that might be deeper, that shares more history, shares more experience would create
deeper resonances. And the possibility that what that would look like when you were looking like
