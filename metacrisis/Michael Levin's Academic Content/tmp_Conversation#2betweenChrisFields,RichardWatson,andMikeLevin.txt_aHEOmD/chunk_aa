So in the broader picture that people who do neural decoding
often think about is, OK, can we scan a brain?
And by reading whatever properties
you think are important, could we
extract the cognitive content of the mind
that is implemented there?
And then so that's kind of the big context.
And then the question of why is it
harder for others to do it than for you to do it?
So from the outside, what's the difference
in reading your memory?
Anne Graham says a neuroscientist from the outside
versus for you reading them inside.
Again, not perfect access by any means,
but much better than we seem to have from outside.
And what does that mean?
And so I was thinking more in a much simpler system
of the kind of thing that Richard first did in, I think,
2010, and then we did after that in a slightly different way,
which is to train these gene regulatory networks.
So this idea that you've got a network, it's deterministic.
You know the rules by which the nodes turn each other up and down.
The structure does not change.
So the structure is coming.
So there is no synaptic plasticity, et cetera.
Go ahead.
But in my model, the structure does change over evolutionary time?
Yes, over evolutionary time.
But during the learning, does it or no?
So the only learning that occurs is structural change
over evolutionary time.
Got it.
Yes.
OK, got it.
So ours is different in that way, right?
So ours are completely locked down.
So the nodes don't change.
The weights don't change.
The relationship between them don't change.
All of that is completely locked down.
So only the activation state?
Correct.
Correct.
Yes, exactly.
Yeah, so what we then do is we treat the thing as an agent
and we provide stimulation.
So we pick in particular, let's just
talk about the case of associative learning.
They can do, I think, six different.
We found like six different kinds of memory.
But let's just look at associative learning.
So you pick a node and you call that the unconditioned stimulus.
And we know that if I stimulate that node,
there's another node somewhere.
We call it R for response.
That other node is going to go up.
So every time I tweak the UCS node, the R node goes up.
Then we pick another node, which we call the first,
it begins as a neutral stimulus.
And so that one, if we tweak that node, the R does not go up.
So that's how you choose them.
And then what we do is we apply that stimulation together.
So it's a classic Pavlovian kind of thing.
You've got the thing that's salient to the agent.
You've got the thing that initially is not salient.
You apply them together.
And then after a while, what happens
is that just the neutral stimulus becomes a conditioned
stimulus.
And when you hit that node alone, now the R goes up.
And so now you have a trained network
that has associated somehow those two stimuli.
And we looked for this in like 60 different biological networks.
And it was pretty prevalent.
And then we looked for it in random networks.
And it was quite rare.
So it's not the kind of stuff that Stukoffman studies,
where these are properties of all networks,
including random networks.
This is the way we found it.
That's so cool.
I didn't know about that work.
Oh, yeah, sorry.
Yeah, so we have two papers, one looking
at that in Boolean networks, and one looking that in ODE models.
And both of those, there's a website repository
where you can download these models.
And then we simply put them through a battery of tests
like you would for a new animal that you were studying
to ask, what is it capable of learning?
So you got hibituation and sensitization
and associative learning.
And they can count to small numbers and things like that.
And then there's a million other things
to check that we haven't gotten to yet.
So OK, so the question that I started thinking about
was this, and this, by the way, tripped up reviewers
for both papers.
This was a real nightmare for both of them
because the reviewers asked initially
a somewhat reasonable question, which is, where is the memory?
So if the structure doesn't change,
and there is no explicit scratch pad to save stuff in,
the structure doesn't change, the node weights don't change,
where is the memory?
And that's what a lot of people complain
about during that review.
So I started thinking about this question specifically.
And we explained, OK, it's in the dynamical state,
and you've got this notion of an effective network, which
is different once it's been through all that experience.
The phenotypic network, as we would put it,
is different from what the original structure was.
And it's all in the dynamic state.
But people found that very unsatisfying.
And can I say it back to you to see
if what I would guess about it resonates
with how you understand it?
Yeah, go for it.
So since it's a recurrent network,
it can hold a dynamical state in its attractor.
And in one attractor, the relationship between two nodes
can be different from the relationship between those two
nodes in another attractor.
And when you train it, you're pushing it
from one attractor to another.
Yep.
Yep.
That's all right?
OK.
Exactly.
That's basically how we understood it, Santosh and Surama.
And I understood it the exact same way.
But the first thing I realized is that people are actually
very uncomfortable with this, because they're
looking for a physical location.
They're looking for the n-gram.
And the n-gram is supposed to be some kind of a thing
that they can get their hands on and read out
and a little register or something where something changes,
something flips or shifts it.
It took a lot of work to sort of bang through this idea
of the different attractors.
So that's not precluded, right?
If you could read the dynamical state,
then you could read what the memory is.
Well, so let's talk about that, because I'm still not sure.
That's what it may be.
You can tell me.
Maybe that's true.
So OK, so I started thinking about exactly that question,
is OK, where is the memory from the following perspective,
just like the brain scanning idea?
So given a network, can I tell whether this thing has
been trained or not?
Can I tell whether it's gone through the process?
And more specifically, what do I need to do to be able to say,
oh, yes, this one has seen the stimulus.
10 times this one has not.
What information do I need?
And in particular, can I do it with a pure read?
In other words, can I do it without disturbing
slash stimulating the network or a model of the network,
for this purpose being the same thing,
can I do it without doing things to it
and seeing what happens?
Can I just purely read it?
And at that point, just out of curiosity,
I posted a question on Twitter for the neural decoding folks.
And I said, do you guys think that brains
can be read out in a pure read, a pure scan that
doesn't interact with the brain, doesn't
stimulate it, just assume you can read anything you want?
And I would say about 80% of the people
said yes, that they thought that eventually,
once we know how to read the right bits of the brain,
we don't need to interact with it.
You can do a pure scan, and you'll
know everything there is to know.
So there you go.
And then I have more thoughts about that,
but I'll shut up, and I want to hear what you guys think.
What can you derive from a network like that
without interacting with it?
OK, so let me see if I'm still catching up.
It's clear that if you wanted to know
whether this network has a particular relationship
between the conditioned stimulus and the response,
then you could poke the conditioned stimulus
and see if you get the response.
But you want to know, can you tell that that linkage is there
without actually what's it called,
fobbing the widget?
Exactly.
And you're allowed to read what the exact state,
the activation state of all the neurons
is at any particular time.
It's not like it's immaterial in that sense.
It is there.
The question is whether that could ever not
