So you need to be able to move it from one attractor
to another with training or with small perturbations,
which means that, and then it falls into the other attractor,
but it won't fall back, right?
Because otherwise it won't hold that state.
So the relationship between the positions
of the basin boundaries and the fixed points
of those attractors needs to be well close, basically.
And if you, if it's going to be possible for you to,
that's what's necessary for it to be,
as my intuition is,
that's what's necessary for it to be possible
for it to move from one state to another
through small perturbations and hold those states,
in other words, to be a flip flop.
But it's more than that, right?
Because you want it to be able to,
I can pick any attractor and by nudging it repeatedly
with this particular stimulus,
I can get it to go to a specific other attractor,
one where, where it's more comfortable
with that kind of nudging, basically.
Which I think might even suggest something like,
there's a basin boundary that's suitable for the thing
that you want to learn close to any fixed point attractor
that you might be starting from, something like that.
Like the basin boundaries need to be all folded up
and fractal and shit, because otherwise they can't all be
close enough to where you started from, to where you want to be.
Any thoughts on that first about what kind of attractor,
an attractor basin, an attractor structure a system
would need to have and whether you have any observations
about whether natural networks have that
and random networks don't?
Yeah, no, I follow you, we haven't done that,
we haven't done that analysis.
So that, that remains, you know, being able to,
because being able to predict that kind of stuff
is very important for us to be able to do that.
Being able to, because being able to predict that kind of stuff
is very valuable from the biomedical perspective,
you want to know if these networks, I mean,
so we're trying to exploit that now in addressing
drug habituation and so on, which is a big problem
in pharmaceuticals and being able to predict.
So that would be great, we haven't done any of that,
so I don't know yet.
Yeah, I wonder about the idea that, in a sense,
anything is learnable from any prior state.
You know, this is the great idea of the Jesuits
and B.F. Skinner and various other people.
But it may be that learning is more local
and that one can only learn certain things
if one's followed certain previous trajectories.
So that what you say about many, many attractors
from which a local, some local basin is accessible
may be true, but not globally.
I mean, there's always going to be an inductive bias,
isn't there?
Yeah, that would be one way of saying that or a prior knowledge bias.
Yeah, which is like what you're saying with the thought experiment
with the language too, right?
Yes, I gave you that stimulus and now I want to read it back,
but I didn't mean for you to learn that,
I meant for you to learn this.
This other thing, yeah.
Yeah, and if all possible states were reachable
through stimulation from all other possible states,
then there wouldn't be any inductive bias
and nothing would be learnable, I think.
Or anything would be learnable given enough vlogging,
which was sort of Skinner's position.
Yeah, but then, yeah, I think that doesn't make sense
because of the thought experiment,
because of the need that all induction has an inductive bias.
Yeah.
With respect to the being able to read a state
to see what the memory is,
this is just a sort of,
to test what exactly you mean by the question there.
So suppose I wanted to know whether the
attractor of the logistic map was a period four,
or period eight, or period two, or period three,
and I could read R.
Would you say,
like, what do I need to do to see,
like, because I can change R in a continuous way,
it's just a, you know, it's just a continuous variable,
and the memory can either be two, or four, or eight, or three,
depending on where I am in R space.
Does that mean that if I can read R,
then I have read the memory, or I could read R,
but I don't know what the memory is necessary unless I run it?
Yeah.
Yeah, I don't, to me, it would seem, I don't think you know what it is until,
unless you try it at the different things you want to try it at,
the different points you want to try that.
Yeah.
Yeah, and since knowing what the memory is, is knowing
from what previous attractor did the system fall into this new basin,
and since...
So the question is whether there's a syntactic manipulation you can do
to arrive at the result of what the period would be
without actually running the dynamical system,
which is a bit like saying, if I give you a program,
can you tell me what the output of the program is
without running the program?
So when I give you a lambda calculus expression,
and you do beta reductions on it, which are syntactic operations,
you're doing a sort of a shortcut that means
you don't actually have to run that bit,
because if you can do a beta reduction,
you found a sub expression that was the inverse of another sub expression,
and you cancel the margin, you don't have to run it.
Hmm, this question suggests RISES theorem,
which says that no Turing machine can tell you the function
computed by an arbitrary program.
Yeah, and if you can get different machines
by parameterizing continuous variables,
then you could read those continuous variables all day long,
and you don't know what the machine does
without running it, or having a simulation of it to run.
So I think that that might even be theoretically watertight,
that you can't necessarily do that.
Yeah, that's very interesting,
although I didn't realize that even something like this
is already got the halting problem,
you know, something as simple as this,
we are already in the halting problem.
Part of, so part of what motivates us is,
so it's funny, a number of people,
and I don't know who the reviewers were,
I don't know if they were biologists or computer scientists or what,
but there was at least one reviewer
who flatly claimed that it was impossible,
that if there isn't some register,
they can get their hands on about where the memory is,
if you don't, if nothing changes,
and that was his argument, right?
If literally, as we said in the introduction of the paper,
nothing changes about the structure of the network,
then where are you keeping these memories, right?
They just thought it wasn't...
But that's only because you use the qualifier structure, right?
I mean, something is changing, but it's not the structure.
Well, that's it, right?
So that to me is quite interesting,
you know, what people expect of these engrams,
and you know, you might think that then,
so the other thought I had about this is,
it's very much related to the scale you're looking at,
because if you did have,
let's say you did have a conventional flip-flop
or something that did have a register
that could store one or the other,
if you were at the wrong scale
and you picked up all the electrons and things,
you would say, well, none of these are scratched,
they're all in their original MN condition,
there is no memory here,
because you haven't changed the hardware at all, right?
And so then it seems like, yeah,
if you go down too far,
necessarily it will look like there isn't anywhere to store it,
it's always, right?
You can have it at one level and it doesn't look,
it doesn't make any sense at the level below,
because none of the parts have been altered.
I wonder, by the way, if you placed resolution limits
on the continuous numbers,
which might correspond to placing depth limits
on the computations,
then you don't have a halting problem anymore
and you can do syntactic reductions, I think.
So it is to do with the idea that in a continuous space
you can keep zooming in and there's more information
and you never know whether that information
is going to matter or not.
I don't think continuity is the issue.
I mean, Turing machines aren't continuous.
No, they're not, but if you...
It's just having the ability, yeah,
they have effectively infinite discrete memories.
Right, so the only way that a continuous fixed dimensional system
could do the same thing that a Turing machine does
is if it keeps folding extra information
into the spaces in between the numbers it's already used.
Otherwise, it's going to be fine.
And then you won't have a halting problem.
Right, but that can go to a discrete infinite limit
as opposed to...
It doesn't have to go to a continuous infinite limit.
I see.
So continuity isn't the issue, but it is a resource issue.
Right, so...
You're correct about that.
I see, so they could be if there isn't a maximum...
If there isn't a maximum and minimum on your values,
they could be discrete, but still be an infinite set
and you could have a halting problem.
