I thought that that's likely to be on Mike's agenda.
That's actually that actually wasn't on the list so we can we can absolutely do that why don't we why don't we start there.
And also, do you want to get the ball rolling then Chris.
No, I think we left it with your statement that it was uniqueness of behavior that counted.
And I was just going to raise basically the same question of how how unique does behavior have to be to count as unique.
And with humans that's kind of easy to answer.
But with other systems, it seems more difficult to say what counts as a unique behavior or a novel behavior.
Today is as one goes down into the cellular scale or the scale of unicellular organisms, it becomes a lot more difficult, I think to say what counts as a novel behavior.
Although it may be a an old behavior executed under novel circumstances or something like that.
Like, you know, Mike's Barry and worms example, where they're, they're doing something familiar but they're doing it in previously unencountered circumstances perhaps that would render the novel behavior.
I don't know whether that fits within your definition, your idea of what novelty is.
Well, let me say, first of all, that, as always with you two.
It's not a matter of, you know, what my definition is because my definitions change all the time in response to conversations with the two of you.
So it's not like let me state how it is so much as let me, let me throw my hat into the ring.
You know, but I doubt it'll come back in the same shape because I think these these sorts of questions are there at the frontiers of you know what we need to think through I don't think that any of us has a really satisfactory
a thorough going formulation of what what view we should take on it.
The second thing I wanted to say is that when you said, well, it's, it depends what you mean by novel, you know, it's clear at the human level, there's lots of novelty.
But as you go to to to simpler and simpler organisms that becomes more difficult.
I think that's also what is to be expected, both the obvious that it's that that that there's more novelty with us and less novelty as you go to simpler organisms, but also that it's a that it's a graded
capacity you know it's not, it's not either it's present or it's absent. And, and of course that applies to consciousness to.
It's very unlikely that that that there's suddenly at some moment in biological time there's a dawn of consciousness.
There was some sort of something that's edging towards that something which possibly qualifies to be that. And then eventually you think no, so it's a matter of degrees of confidence, if you'll excuse the pun, you know, in in whether or not what you're
looking at is actually the meets the criteria.
With those preliminaries out the way.
What I was saying in the emails is that if an organism responds to a novel stimulus with a behavior that or an action. I don't I suppose we are using the word behavior very broadly to include any response.
If it if it responds with a behavior that is within its phenotypic fixed rep fix phenotypic repertoire.
Then, there's, there's two ways of interpreting that the one is that it has recognized an analogy, which implies intelligence. It's a thought process of some kind.
It's a difference, rather I should say, or alternatively it's a mistake. In other words, it is, it is, it is a miss identifying this stimulus as the other stimulus and therefore giving the response that it would give to the other stimulus, which
just happens to be the right one, or which which you know is it's within the same ballpark and that's why it triggers the same response.
And there are sorts of possibilities flow from that this last thing that I've just said, because there too there's, you know, for example, let me say for example, it may be that there's a range of responses.
There are 10 of which are completely erroneous, and the organism therefore expires. One of them randomly happens to be the right incorrect response if you know what I mean.
The right mistaken response and mistaken response which does work. And then that that that organism survives it would be a matter of, you know, is it, or what we're seeing there is just something to do with, you know, with random variation which is how natural
change works. Now, the, the, there's an increased chance that the, that the offspring, the descendants of that one out of 10 of that, of that species will always do the right thing in that situation.
That's, that's, that's one other issue that comes up and I think why I mentioned that is I think that the mechanism I'm going to specify now is standing contrast to that, because what what I think the adaptation is the adaptive
changes of the mechanism I'm going to describe is that you don't need to rely on natural selection you can actually learn from experience underlining the word experience.
You can learn from experience during your own onto genetic development. And that is a huge advance over having to having to lose a great proportion of the, of the species so that the, so that the ones that just happen to do the right thing can can survive
and now, now is the mechanism that I, that I have in mind the alternative to the natural selection, the phylogenetic mechanism. It is that the, the organism encounters a novel situation and a novel stimulus or a novel problem.
And the, at that point, it's determined, it's determined behaviors, fail it. In other words, it's reflexes, depending on how complex a creature we're talking about it might be you might say it's reflexes its instincts.
Everything that is that is that pre exists in its behavioral repertoire is is of no use to it.
So, at that point, the term a purely determined kind of response has to be replaced by something more in the nature of what either is replaced by what I was saying earlier, in other words, you know, they die there, they make a mistake, or they randomly do the right thing.
Rather than all of those, it behaves in a stochastic fashion, but not just stochastic fashion because it has affect. So it does, it does various things.
And it is able to to register which one of these is working best, because the need is the need, the problem proposed, you know, I now don't have a solution to this, to this situation I'm in a state of need.
Okay, this one, this behavior is meeting the need this behavior is not meeting the need. No is that no is that no is that so I'm going to carry on doing this.
I think that that's the fundamental
contribution made by affect. And you can see from what I'm saying is it underwrites choice that underwrites voluntary action.
Because it's not, it's not just random. It's initially, you know, I'm in a panic I don't know what to do, but I can feel my way through the responses that I'm generating, I can feel which one is working.
That in turn, leads to learning from experience. And now you have an additional
arrow to your bow the next time that you encounter the situation that it's now a situation for which you do have a prediction, but the so that that that is the moment of
suddenly expanded sort of degrees of freedom, and and the, the navigation of that situation is what feeling contributes, it's sort of, it provides value.
The value being, you know, survival is good, death is bad. And it allows you to register within that system of values that's what feelings do register within that system of values.
And I would only add one other thing, which is that it's not just goodness and badness, it's goodness and badness within multiple categories of need.
So it has a, it has a valence and it has a quality.
That's the mechanism that I tentatively am proposing is is instead of the earlier ones, this is something else, and that's, that's where where I think.
So it's, that would be my behavioral criterion for feeling reasonably confident that what I'm seeing is an organism which is which is using what we call feelings to, to solve a novel problem.
So, so it sounds like a, what you're proposing is a kind of general purpose analogy mechanism with affective response, some form of confidence that this will solve some kind of problem as the criterion for picking a good analogy.
I suppose you wouldn't have thought of putting it like that but as you say that, and I can see why you say that that does make sense.
But I'm glad you mentioned the issue of confidence because I don't know if you remember in my very raced, my very rapid presentation in Boston I spoke at the end about the importance of, of precision, you know, of, of palpating and modulating of confidence.
So the affect, the affect remember it is not just valence. It's valence within a category of need. So you're feeling increasing or decreasing, let us say, the suffocation alarm.
And, and the, let's say the increase in the suffocation alarm gives you less confidence in your current policy in the thing that you're doing.
And, and so you, so you, you have increasing your confidence in the, in the error signal that is the affect, and you have decreasing confidence in the, therefore in the, in the action.
You have to change your mind you do something else. And that that once that leads to increasing. Once that leads to decreasing.
You know, the affect is now moving in a positive direction in other words decreasing suffocation alarm decrease decreasing respiratory distress. Now you have more confidence in that policy.
And that in turn, the high precision situation like that drives the, the updating of the predictive model.
I wonder, you know, I was thinking earlier, this, this issue of what, what counts as a, as a, as a novel behavior. And I wonder if we should lean more heavily on the, on the notion of an observer here from the point of view of a
novel behavior is one that you as an observer of the system. And then we can say, oh, by the way also the system itself right is surprised by it. Could we could we close the loop that way and say that a truly novel behavior is one that surprises some
observer and that may be a scientist that may be some metacognitive module in the system itself but you know from that, I feel like there's a certain symmetry there.
I agree with you completely because it's actually a very neat formulation there because actually what you're saying about the observer.
It tightly mirrors what the organism the situation the organism found itself in which was a situation of surprise in other words I do not have a prediction for this.
Whatever you do then is going to be surprising.
I mean, initially, as I said, there's an element of randomness, you know, there's, there's, this is, this is not surprising. That's, that's one of the repertoire of responses that the organism has is stochastic behavior.
What's surprising is that it's able to read the gradient of, you know, this one is that this is a better direction than that. And so then you start seeing voluntary behavior in other words, a behavior that you didn't predict.
In other words, a behavior that's surprising to the observer. And that's a voluntary behavior and that's a novel behavior. It's no longer stochastic.
I also, I also.
I was going to say it also works.
In the sense of a behavior under uncertainty.
When you're just sort of trying things out that actually works is both surprising and pleasurable.
I mean, the case I always go to is tool use.
Because it's highly analogical.
And you see it in lots of different organisms.
And it's when it when it when a system is learning how to do tool use, you know, like a young human is learning how to use their toy tools or whatever.
So it's a very pleasurable activity when it works and it's very frustrating when it doesn't work.
So it seems like a good kind of model system for thinking about this.
You know, and I love that because because I bet we could we could engineer some kind of a cellular system and some sort of instrumental learning thing where the cell gets to do something and it works.
But it helps. So so a tool use scenario right and could we measure some sort of a positive affect, or at least a reduction of stress, where it gets that oh look I was able to make that happen great you know is there a primitive version of that that we could detect in the cell or a tissue.
Yeah, that give give it a chance to, to learn something like that and
that's the whole.
The whole point is to be able to come up with a with a agreed criterion and then certainly then then you can start doing exactly that that's the whole point.
The, the, but just to go back to Chris's remark about surprise, well both of your remark about surprise. I like that very much because because to me.
When an affect is generated, it's, it's always because of surprise.
And the, the, the negative affect is things are going worse than expected a positive affect is things are going better than expected because remember here it's this is random now.
The fact that this is reducing the need is a surprise but it's a pleasant surprise the fact that this works, you know, you don't expect random behavior to work, you don't have high confidence in stochastic behavior.
So it's a, it's a surprise in both directions, except it's worse than expected, better than expected. And then, you know, and then the surprise reduces and that's when you get to satiation.
Okay, now, now I know this works. It's no longer surprising and and I'm no longer in a state of uncertainty and I've now got a new behavior that's at my disposal.
I love what you said earlier about this.
Is it a generalization or is it a mistake, right that something was, and I think that's very cool for two reasons one I believe maybe it was Dan Dennett I don't know who it was but somebody said that,
making being able to make mistakes is a is a is a mark of agency right that you know chemistry doesn't make mistakes but if you're an agent but but if you've made a mistake that says a lot about what you've got going on because you've had expectations.
I agree I would I would I would specify what's meant by mistake there is wrong choice.
Yeah, yeah, in other words, you know, in other words it's only once you have the functionality of choice that you can speak meaningfully of mistakes in the in the sense that you're using the word now in if it was done who said that in Dan sense of the of the word.
You can speak of, because if by mistake you just mean it did something stupid.
It did it did something, you know it, it responded to stimulus a, as if it was from your stimulus be and therefore died is different from, I responded to it as if it was stimulus be that didn't work so I now moving to okay I don't know what kind of stimulus
this is, let me try various things, then you can make mistaken or not choices.
Well, and that strikes me as a, which I think is also good as a continuum, because you can make really dumb useless mistakes, and you can also make useful sort of inspired mistakes and everything in between.
Right.
You know, and I think and we deal with this in embryo genesis all the time like, what exactly is a birth defect right is it and is it an error by the, you know, because because one wrong shape is some other species perfectly good shape.
And so right and so we think about this all the time you know as these, these systems try to navigate that that morpho space.
When they make errors, you know, error with respect to what and then you start thinking about expectations and then, and some of these mistakes of course work out great and some are some are random and some are exactly as you pointed out more along a gradient of trying to improve things.
It's an effect and all that.
It's sort of so so okay so so that gets me into one of the things that that I was going to bring up which is in this issue of expectations and so on.
Okay, so so so so when you're talking of course in your in your book and everything else, you, you talked about kind of the intrinsically conscious properties of the paraphernalia complex and those areas and all that.
I'm curious what you think about. So let's say, let's say I extract it into a ex vivo culture system.
It's a paraphernalia complex it's now living in a petri dish on our on our bench top and it's being you know fed.
I wonder what you think about that aspect of it does it.
And whether I you know I was trying to decide whether, whether it would now think that it was in a sort of sensory deprivation tank kind of experience or whether, you know, I don't, I don't know what do you think about that once once it's once it's out of the body
does it maintain the special status that it had or not or what do you think.
Well, my answer to that is, is, I'll give a very concrete answer to it but I think there's a deeper principle that you're asking about so my concrete answer to this particular instance that you're raising might not address the deeper
principle in which case please please say so and then we can look at it again.
And it's really a concrete answer. When I say that the paraphernalia complex is when I say actually that all reticulate reticular activating system.
And reticulate nuclei are intrinsically consciousness producing nuclei I mean that is their function their function is to modulate precision.
And in the ways that I was describing earlier palpating of precision is is is an essential mechanism.
I do message passing in visual cortex and that when I say it's intrinsic I mean that the message passing in visual cortex doesn't need to be conscious. It's only becomes conscious when it's modulated by reticular activating
by reticular modulation.
But now here comes the specific answer.
The reticular activating modulation is that modulation of confidence that palpating of confidence and adjustment of confidence is in is in the in response to what's happening in
periaqueductal gray. The periaqueductal gray is not part of the reticular activating system. It's it's it's it's as it were the other end of the of the of the cycle.
So, I'm acting up here message passing you know I'm doing this, and I'm sensing that in response.
It's all modulated by by by reticular arousal. And then the residual error signal. In other words to the to what I have confidence in this in this policy.
But it results in this era. In other words my confidence is misplaced.
In periaqueductal gray. There's a, it's where all of these homeostatic.
I mean literally all of these homeostatic systems. They, they, they, they, all of them ultimately deliver their residual error to the periaqueductal gray, which is adjacent to the superior
and on the basis of periaqueductal gray registration of the of the the the the magnitude of the various error signals in relation to the current opportunities.
It makes a prioritizes. So the prioritization of an error signal is is the feeling of the effect. It's I am.
So, I've prioritized the fear over over thirst.
So, I so I feel fear that is the prioritization of fear. So now thirst is going to be dealt with automatically.
I'm going to palpate my precision in my actions in relation to thirst. That's rendered automatic. I have now prioritized fear. And so I set about an action program that is designed to meet that fear.
It's an action program, which is obviously it's the activation of long term memories, which, which in other words the expected.
It's an action program with any with with with expected consequences. And, and, but these are expected precisions, and here I'm palpate, so it's not fixed precisions I'm now palpating the precision in my in my policy in my predictions in response to the, the, to what extent do they
not meet the deliver the sensory consequences that that that were predicted.
So, that's a long way of saying that what you can't really speak about what the para break your complex is doing as as consciousness producing, unless you link it to.
It's actually modulating what the message passing that that's what that's what one type of consciousness it's producing is, you know, and the other one is, it's in relation to the periaqueductal gray consequences of that action cycle.
So, there's, there's the feeling generated in PAG, and then it's the feeling this about that is the other aspect of the consciousness. So, so the, the, the, the, the para break your complex or any other part of the reticular activating system.
If you just were to cut it out and put it in a petri dish it wouldn't have that functionality unless it was in that loop.
Yeah, interesting. I always, you know, we sometimes discuss in the lab with what what is the, when you do have something like that sitting in a petri dish. What is the input is the input.
Nothing is it, you know, because you still you know the cells still have the sensors on them they're still facing outwards they're in the media but is that is that the equivalent of turning everything off and getting no input or is that the equivalent of getting
those are inputs that are surprising or, or because because they're different than what you would get physiologically or is it even possible to turn it off entirely because you're always going to get some receptor and if you get rid of the receptor there's still some
reduction machinery that like there's always something and the question is, you know, what what happens like like in the, you know, in the, in the sensory deprivation tank and you try to shut that stuff off but it will immediately generate its own sort of.
I think, I think there's not going to be one answer to that you know what what what would happen would depend on all sorts of imponderables but of course something will happen as you say, but I want to just introduce one.
Again, I showed you in that brief talk in Boston at one instance of the sort of situation although it's a small instance of the sort of situation you're describing, which is which is hydranencephalic children.
So there they have a reticular activating system, but there's no cortex to modulate. And so, in what sense are they conscious.
What they're conscious of is the, the errors you know they're conscious of the of the feelings.
They don't know what the feelings are about. So they're still able to generate consciousness. And they're still creating a reticular activating response but I'm not sure what that would mean of course we also simplify when we speak like I am now because the reticular
activating system doesn't only activate cortex in fact it activates all over the place including spinal cord and you know and everything it's just that the main destination by far is up is ascending.
There is also they are also descending but there is also descending modulation right down, you know so it's modulating something but it's not modulating the generating of it's not generating conscious images.
This is very interesting because we're discussing the role of a particular set of cells in error detection on the one hand and precision modulation on the other hand.
But both of those sets of cells can also be considered to be active inference agents in their own right living within some environment that provides them with inputs and responds to their outputs.
And they have their own little models of how they expect their environment, which is whatever sends them inputs to respond to whatever outputs that they give it.
So we're really working at two different levels of description here and in the cutting it out and putting in a dish thought experiment one has to characterize that quote normal environment in C2 environment of this little system very carefully to know how one would give it inputs
and and measure and respond to its outputs in a way that was close enough to its model that it would behave in something like the way that it's behaving in the body, or behave in some way that's intelligible to you as an observer.
Yeah, that's, first of all, of course, you completely right.
And you write about, let us not forget that each one of these cells and each one of these nuclei have their own predictive model, and they're responding to their environment.
So the interesting thing that flows from what you were just saying is, you know, I don't do this kind of work like you guys, like you do Mike and you have done Chris.
I don't know to what extent is it possible to in that situation to to start doing a giving artificial inputs to the cell, or to the nucleus, and you know you can start actually doing experiments.
You know, to correctly address the sorts of questions we're, we're discussing here.
Well, I mean so so totally possible that was one of the things that was going to show you guys during the visit and we never got to it because the worms sort of stole the stole the show but what Wesley has these hybrids, right, which and many people have made different kinds of
brain structures and for us also non brain structures that are hooked up to artificial bodies. So people have published you know you can take a lamprey brain and make a drive a little robot cart around by just basic you know it's sort of it's sort of taking
sensory motor substitution and prosthetics and putting that on steroids and just saying okay, instead of you instead of your normal artificial limbs which is going to give you new stuff that you know new new effect new sensors and new effectors.
And that works pretty well I mean there have been robots powered by slime mold and by fish brains and by.
I think they did frog brains and so on and and you can certainly do it. And we have some of that going on and having artificial virtual reality worlds for a bunch of cells in a dish so that they can sort of inhabit that world and certain
things are distorted and other things are discouraged and so on. I mean, what you get from that, of course, what what we are able to collect from that is data on behavior and physiology.
We're not able to get to to get to touch first person consciousness of course right, unless, unless, and I think that will stay that way until until and unless we sort of merge with the experiment right so at some point you'll sort of like, you know, if you want to know
what you really like to be a composite system of whatever this crazy thing is in yourself you know you plug it into plug it into your brain and some in some high, you know, high data transfer way, and, and maybe maybe you'll have some experience of it but.
I don't know what it's like to be any of these agents doing their thing but you can absolutely explain all these brain regions and give them instrumentize them and give them artificial sensors and effectors and then and they will act sensibly in these virtual worlds so the these
eyebrows will drive around and do various adaptive things that that they're being rewarded for and so on.
I'd like to know what you guys think about this but I don't want to.
I'm going to keep it very brief because you know you got your list there Mike, but what you are talking about what we are now talking about is of course the infamous problem of other minds.
If you go back to the beginning of our conversation today, where we without being too exact and definitive, you know we seem to be in the region of some kind of consensus that that sounds like a reasonable formal description of what a feeling agent would and would not be able to do.
When we were speaking about novel problems and and novel solutions.
I told you what I think the mechanism is whereby that happens that that well I told you what I think the mechanism has to be that it has to be the the the organism is registering its own state.
It's registering its own state in relation to its to put it formally in relation to its free energy. In other words, is this going is this increasing or decreasing the chances of me continuing to exist as a system.
And, and it's only from its point of view.
It doesn't have the same value to any other organism because it's only so the registering of that state is intrinsically subjective.
The value only applies to the that organism.
Then it has to be within one of these categories of need. So it has to be qualitative has to be qualified.
You've got some a mechanism I've just described which is simply just is subjective and valenced and quality and qualitative and registered by the system for the system.
If you if that's the mechanistic description, then is that not just a description of a feeling what else is that but a feeling that's what a feeling is a feeling is what I've just described it's it's it's a.
So, you know, do we have to throw up our hands and say well problem of other minds we can never know, or, or can we approach it this way and say well, once we've given a mechanistic description of what a feeling is.
We see situations where such a mechanism is present and where such a mechanism is not present. We don't have to be the system. It's more, I think, a question of, at what point is it justified to take the viewpoint of the system to say that the system has a viewpoint at all.
You know, if it's justified to take the viewpoint of the system, then you say okay well, if it's got this mechanism and it shows that behavior then it must be feeling bad now and it must be feeling good now.
You know, it's.
What do you think of that.
Is there no possibility of an objective criterion that's I suppose what I'm saying.
Yeah, I think what that motivates at least for me is to try to understand in some.
Biogenetic lineage, for example, or looking at phylogeny is some sort of network of systems.
How that's how that assignment of affect is implemented.
In different kinds of organisms, you know, very, very simple organisms, for example.
You know, things like worms or paramecia or, or what have you, where we have some chance of understanding a pathway that does this kind of precision modulation.
That's why I was bringing up the system and even E. Coli where I mean there is precision modulation.
It seems like a bit of a stretch to interpret it in terms of feedback, but maybe it isn't, you know.
That's a very interesting conclusion, a provisional conclusion for us to have reached, and I will not defer to Mike's list.
Well now this I mean this is very good. There's this there's only one other thing on the list and that's we can do that's pretty quick I think just to you know to speak to your last point I mean I think that's very interesting and I agree with you that we don't have to throw up
hands and say, we have no clue what the conscious state is I think we can we can objectively make some some good statements about that.
But I wonder if that. So this I think this goes back to the to the Mary argument right about the blind, you know, color scientists and all that, in the sense that, I mean I think you can say quite a lot from from that objective sort of perspective.
I wonder if that's really everything in the sense that, you know, is it really is it really add nothing to then be the system, and, and say yeah that's you know so so so here's the system and we've made objectively all kinds of conclusions about
you're unhappy or whatever, and then magically you sort of become the system, and is your experience. Well yeah I already guessed all of that so nothing new here, or, or is it like, Oh, now it's sort of much more.
There's there's something at you know.
No, I don't agree with with that and I think that you don't either so let me just say it.
I think that to be.
We're talking. Let's say for example and we're engineering such a system. You know it's having artificial feelings about what is it like to run out of battery power.
It's been an inorganic system and I've never had a battery. So I can't, I can't imagine what it feels like to run out. I can, I can, I can't imagine but I can't know what it feels like to run out of battery power when you're a robot.
We've got this functional architecture, but that's not the same as Frank Jackson's knowledge problem, because in Frank Jackson's knowledge problem, Mary, the visual neuroscientist would, it's not only that she doesn't know what it's like to see.
It's that she heard knowledge of how vision works makes no predictions that it would be something that there would be something it is like to see there's no necessity for there to be something it is like to see you can understand the whole physical mechanism of vision and and leave out what it is like to see
and you've still got the whole mechanism so this is in some parallel universe, the what it is like to see it's it's neither necessary, nor predicted Mary would never have expected that there is something it is like to see.
In our case, the one we're talking about, we fully expect there will be something it is like from the viewpoint of the agent to be running out of battery power and we and we can say it will be unpleasant.
And we can say, you know that the the unpleasurable increase along this gradient that you can quantify because it has to do with the decreasing confidence in the policy as it leads towards critical values, and so on, but the exact quality you wouldn't know, but it's, it's, it's way different
from from this, the position that Mary was in.
And maybe Lewis Carroll or somebody was was saying that in order to to to transmit the feeling of the I think fear he was saying to do to make someone feel scared.
And the thing to do is not to describe what that's like but to actually provide some, some stimulus that actually scares them to, you know, say something that would actually scare them right.
And so, one kind of idea along this, this other minds thing is that the output of a proper, I mean, this is bothered me forever is like, what, what, what format should be in what format should be the output of a decent theory of consciousness.
Like if we had a golden let's say we had a gun, what the what does that output, right, because most of our theories output numbers and things about observable behavior but what does what does a theory of consciousness actually output.
And maybe what it outputs are is some sort of poetry or, or, or, you know, scary stories or whatever that that that literally try to put you in the same state that that they're that they're talking about.
Yeah. So, so some kind of stimuli some kind of scenario in which you get to experience at least to some extent, right as opposed as opposed to sort of a scientific objective story about you well you know this is what it's it's about this and it's linked to that and positive and negative and so on.
So, you know, that's that's why I was leaning on the on this sort of connection, because, because what what I think it would output is protocols protocols for you know you want to know what it's like to be this well here's a protocol for putting
yourself as close as you're going to get to the state that that this thing is in. And then you'll know.
So, I think, when you pose the question now to answers occurred to me in terms of the output of a theory of consciousness.
The first, the second one is closer to what you're talking about about Lewis Carroll's advice.
The first one is more remote. It's got to do with the numbers when you were saying well it won't be a system of numbers and so on. Some, it's something else, you know, because it's a theory of consciousness.
I was thinking, rather, actually, what it will produce is a system of numbers, its output will be, I mean, I'm speaking, poetically like you were you know it will be a formalism.
It will be as algorithms and formalisms which which which underwrite those algorithms and all of that. But those formalisms will describe both the behavior of the organism in other words the external observables, and it will describe the internal states.
But what is what is novel about what is what is the contribution of the output of a theory of consciousness is you'll have one formalism which will explain these tightly correlated variables, the physiological observables and the psychological
observables. That's, that's the yield, the scientific yield of a theory of consciousness. In other words, it does away with the mind body dualism. It gives us a dual aspect mechanism, which is not a metaphysical one but an actual
scientific one. But to come to the second answer which I think it gets closer to what you're getting at is that I think and I don't just think this. I've experienced it. As I was working out my own
uncertainty theory of consciousness. I started to have experiences that I hadn't had before. And those experiences were of the kind where I would suddenly realize oh that's why I'm feeling this now.
This is not just something I'm subject to. It's not just a thing that happened. I know why I'm feeling this in response to that and why my picture of the world is, I start noticing actually my picture of the world is not so continuous.
So I started to have different experiences which were yielded by the scientific account. But it wasn't that I started to have, it wasn't that I had feelings that I hadn't had before. It was more in the nature of I could understand my feelings in ways that I never had before.
And that has consequences. But it's not, it's not whole new sets of feelings. It's more like thoughts that flow from the feelings that I hadn't previously thought. That's, that's how I've experienced it.
Interesting.
So it is great. It is what you sort it along the lines of what you're saying it's sort of what you would expect that a theory of consciousness won't only have objective consequences it surely will also have subjective consequences I think that's what you were saying Mike.
Yeah.
Okay, in the last few minutes, then one other one other question I meant to ask you so so so we've and we've talked about this before and you mentioned towards the end of your, your talk here and Boston the artificial the artificial agent that you're building and that your team is building.
Is it going to sleep, either either by design or by or do you predict that as an emergent kind of feature. Is that is that something you guys have thought about.
Yeah, so the we're really at a very basic level so far, you know, so there's a there's a hell of a lot to come.
And as it happens that so we've gone through one phase of the of our study and we are now starting the second phase of it.
The one of the main things we're trying to do in the second phase is that making an environment which is better able to bring out and display the functionality.
And, and we also wanting to introduce multiple agents in the environment so where where the agents are having to predict each other's predictive models.
It's just so happens that the needs that we're building into this agent which which are, you know, they just ciphers one because we're dealing with this stage in entirely virtual agents.
The, the, the, it's, it has a need for energy supplies it has to find those resources in the environment and the environment keeps changing.
And it has to, while it's doing that it bangs into things, and it has to not bang into things because this is the equivalent of suffering a tissue damage, and it needs to rest in order for the tissue damage to repair which conflicts
and we need to find the energy resources sort of that. So those were the reasons why we, why we use those particular cyphers is we wanting to, we wanting to create a situation in which the one need competes with the other need.
We theoretically have introduced something that functions a little bit, I mean, I say theoretically, so more sort of like metaphorically or in some way we've created something which is analogous to a need to sleep but I don't think it begins to tap into the sorts of things
that we're getting at that, you know, because, for example, a crucial aspect of sleep is that the, the, the, the agent is no longer actually navigating an environment.
It's no longer actually having to test its predictions against a sensory consequences because they aren't any sensory consequences because it's not that you know they aren't any actions that can offer any sensory consequences.
That enables it now, being offline, being being exempted from having to do that sort of thing, it's able to do a whole lot of other sorts of things, including memory consolidation in other words including the task, the very introspective task for one to the
of attending to which of the new synaptic weightings
that I established today, am I going to retain?
And which am I not?
And this has increasing consequences
the deeper you go into the predictive hierarchy.
You know, if something happens today,
which was not predicted
by one of your most heartfelt beliefs,
you're not going to just give up your heartfelt belief
on the basis of one contrary bit of evidence.
You need to really consider.
And I think that this has a lot to do,
by the way, with dreaming,
which is why does consciousness intrude into sleep
if there's no problems to solve?
Because you're no longer predicting
anything about the world.
I think it's a kind of housekeeping
or mopping up exercise that's best done offline.
You know, because you don't have any here and now problems
to have to contend with,
you can do this kind of really very consequential business
of internally considering on the basis
of an internal precision modulation exercise,
whether to retain your old prediction or to update it
when it comes to deeper levels of the hierarchy.
So that kind of thing,
when you're dealing with a complex system
with a deep hierarchical predictive model,
I think that kind of thing is very important.
And it has consequences for a theory of consciousness
because of what I just said about dreaming.
So why must you lose consciousness at any point?
In other words, why must you sleep?
And why is sleep punctuated by dreams?
I think that those kinds of questions
can be addressed mechanistically
in an artificial agent that has proper sleep,
not the sort of toy sleep that all rests
that I was talking about earlier.
Yeah, well, the reason, I mean, one reason I ask is because,
so I have a postdoc that's very interested
in sleep and unconventional systems.
And so we've been thinking about how to recognize sleep
in things that don't have the typical,
the obvious hallmarks, right?
And so, in Plenaria and Paramecia, in our tissues,
of course there's circadian rhythms and things like that,
but the informational features that you're talking about now,
the consolidation and all that,
how do you, besides simple inactivity, right?
How do we recognize Xenobots, do Xenobots sleep?
How do we recognize that?
So I'm thinking about this whole spectrum
from the totally minimal age.
Well, since we are out of time,
it's a good moment to say we've come full circle
because my answer to that is one objective way
of ascertaining whether or not the organism is asleep
is if it's an organism which is capable
of the sort of thing we were talking about earlier,
generating novel behaviors,
then during sleep that should stop,
because that's an essential feature of sleep
in terms of what I was saying just now,
is you're no longer dealing with life's problems, external,
you're no longer dealing with the external environment,
you've withdrawn from it,
so you're no longer acting on the environment.
Really is in many different senses of the word,
definitional of sleep,
that you're not producing voluntary behaviors.
So I would say if you're talking about organisms
that do display the functionality
that we were speaking about earlier
and calling that voluntary behavior,
I would say when that stops,
if it stops for a phase of the diurnal cycle,
I would say that's its sleep phase.
It doesn't, of course, have to happen only once,
but if there's a protracted period like that,
that would look pretty much like sleep to me.
Yeah, yeah, cool, yeah, well, we'll be...
And isn't it nice?
Again, we see the benefits
of having a more objective mechanistic definition
rather than having to become the organism
to know whether you're asleep or not.
