But I wonder if that. So this I think this goes back to the to the Mary argument right about the blind, you know, color scientists and all that, in the sense that, I mean I think you can say quite a lot from from that objective sort of perspective.
I wonder if that's really everything in the sense that, you know, is it really is it really add nothing to then be the system, and, and say yeah that's you know so so so here's the system and we've made objectively all kinds of conclusions about
you're unhappy or whatever, and then magically you sort of become the system, and is your experience. Well yeah I already guessed all of that so nothing new here, or, or is it like, Oh, now it's sort of much more.
There's there's something at you know.
No, I don't agree with with that and I think that you don't either so let me just say it.
I think that to be.
We're talking. Let's say for example and we're engineering such a system. You know it's having artificial feelings about what is it like to run out of battery power.
It's been an inorganic system and I've never had a battery. So I can't, I can't imagine what it feels like to run out. I can, I can, I can't imagine but I can't know what it feels like to run out of battery power when you're a robot.
We've got this functional architecture, but that's not the same as Frank Jackson's knowledge problem, because in Frank Jackson's knowledge problem, Mary, the visual neuroscientist would, it's not only that she doesn't know what it's like to see.
It's that she heard knowledge of how vision works makes no predictions that it would be something that there would be something it is like to see there's no necessity for there to be something it is like to see you can understand the whole physical mechanism of vision and and leave out what it is like to see
and you've still got the whole mechanism so this is in some parallel universe, the what it is like to see it's it's neither necessary, nor predicted Mary would never have expected that there is something it is like to see.
In our case, the one we're talking about, we fully expect there will be something it is like from the viewpoint of the agent to be running out of battery power and we and we can say it will be unpleasant.
And we can say, you know that the the unpleasurable increase along this gradient that you can quantify because it has to do with the decreasing confidence in the policy as it leads towards critical values, and so on, but the exact quality you wouldn't know, but it's, it's, it's way different
from from this, the position that Mary was in.
And maybe Lewis Carroll or somebody was was saying that in order to to to transmit the feeling of the I think fear he was saying to do to make someone feel scared.
And the thing to do is not to describe what that's like but to actually provide some, some stimulus that actually scares them to, you know, say something that would actually scare them right.
And so, one kind of idea along this, this other minds thing is that the output of a proper, I mean, this is bothered me forever is like, what, what, what format should be in what format should be the output of a decent theory of consciousness.
Like if we had a golden let's say we had a gun, what the what does that output, right, because most of our theories output numbers and things about observable behavior but what does what does a theory of consciousness actually output.
And maybe what it outputs are is some sort of poetry or, or, or, you know, scary stories or whatever that that that literally try to put you in the same state that that they're that they're talking about.
Yeah. So, so some kind of stimuli some kind of scenario in which you get to experience at least to some extent, right as opposed as opposed to sort of a scientific objective story about you well you know this is what it's it's about this and it's linked to that and positive and negative and so on.
So, you know, that's that's why I was leaning on the on this sort of connection, because, because what what I think it would output is protocols protocols for you know you want to know what it's like to be this well here's a protocol for putting
yourself as close as you're going to get to the state that that this thing is in. And then you'll know.
So, I think, when you pose the question now to answers occurred to me in terms of the output of a theory of consciousness.
The first, the second one is closer to what you're talking about about Lewis Carroll's advice.
The first one is more remote. It's got to do with the numbers when you were saying well it won't be a system of numbers and so on. Some, it's something else, you know, because it's a theory of consciousness.
I was thinking, rather, actually, what it will produce is a system of numbers, its output will be, I mean, I'm speaking, poetically like you were you know it will be a formalism.
It will be as algorithms and formalisms which which which underwrite those algorithms and all of that. But those formalisms will describe both the behavior of the organism in other words the external observables, and it will describe the internal states.
But what is what is novel about what is what is the contribution of the output of a theory of consciousness is you'll have one formalism which will explain these tightly correlated variables, the physiological observables and the psychological
observables. That's, that's the yield, the scientific yield of a theory of consciousness. In other words, it does away with the mind body dualism. It gives us a dual aspect mechanism, which is not a metaphysical one but an actual
scientific one. But to come to the second answer which I think it gets closer to what you're getting at is that I think and I don't just think this. I've experienced it. As I was working out my own
uncertainty theory of consciousness. I started to have experiences that I hadn't had before. And those experiences were of the kind where I would suddenly realize oh that's why I'm feeling this now.
This is not just something I'm subject to. It's not just a thing that happened. I know why I'm feeling this in response to that and why my picture of the world is, I start noticing actually my picture of the world is not so continuous.
So I started to have different experiences which were yielded by the scientific account. But it wasn't that I started to have, it wasn't that I had feelings that I hadn't had before. It was more in the nature of I could understand my feelings in ways that I never had before.
And that has consequences. But it's not, it's not whole new sets of feelings. It's more like thoughts that flow from the feelings that I hadn't previously thought. That's, that's how I've experienced it.
Interesting.
So it is great. It is what you sort it along the lines of what you're saying it's sort of what you would expect that a theory of consciousness won't only have objective consequences it surely will also have subjective consequences I think that's what you were saying Mike.
Yeah.
Okay, in the last few minutes, then one other one other question I meant to ask you so so so we've and we've talked about this before and you mentioned towards the end of your, your talk here and Boston the artificial the artificial agent that you're building and that your team is building.
Is it going to sleep, either either by design or by or do you predict that as an emergent kind of feature. Is that is that something you guys have thought about.
Yeah, so the we're really at a very basic level so far, you know, so there's a there's a hell of a lot to come.
And as it happens that so we've gone through one phase of the of our study and we are now starting the second phase of it.
The one of the main things we're trying to do in the second phase is that making an environment which is better able to bring out and display the functionality.
And, and we also wanting to introduce multiple agents in the environment so where where the agents are having to predict each other's predictive models.
It's just so happens that the needs that we're building into this agent which which are, you know, they just ciphers one because we're dealing with this stage in entirely virtual agents.
The, the, the, it's, it has a need for energy supplies it has to find those resources in the environment and the environment keeps changing.
And it has to, while it's doing that it bangs into things, and it has to not bang into things because this is the equivalent of suffering a tissue damage, and it needs to rest in order for the tissue damage to repair which conflicts
and we need to find the energy resources sort of that. So those were the reasons why we, why we use those particular cyphers is we wanting to, we wanting to create a situation in which the one need competes with the other need.
We theoretically have introduced something that functions a little bit, I mean, I say theoretically, so more sort of like metaphorically or in some way we've created something which is analogous to a need to sleep but I don't think it begins to tap into the sorts of things
that we're getting at that, you know, because, for example, a crucial aspect of sleep is that the, the, the, the agent is no longer actually navigating an environment.
It's no longer actually having to test its predictions against a sensory consequences because they aren't any sensory consequences because it's not that you know they aren't any actions that can offer any sensory consequences.
That enables it now, being offline, being being exempted from having to do that sort of thing, it's able to do a whole lot of other sorts of things, including memory consolidation in other words including the task, the very introspective task for one to the
of attending to which of the new synaptic weightings
that I established today, am I going to retain?
And which am I not?
And this has increasing consequences
the deeper you go into the predictive hierarchy.
You know, if something happens today,
which was not predicted
by one of your most heartfelt beliefs,
you're not going to just give up your heartfelt belief
on the basis of one contrary bit of evidence.
You need to really consider.
And I think that this has a lot to do,
by the way, with dreaming,
which is why does consciousness intrude into sleep
if there's no problems to solve?
Because you're no longer predicting
anything about the world.
I think it's a kind of housekeeping
or mopping up exercise that's best done offline.
You know, because you don't have any here and now problems
to have to contend with,
you can do this kind of really very consequential business
of internally considering on the basis
of an internal precision modulation exercise,
whether to retain your old prediction or to update it
when it comes to deeper levels of the hierarchy.
So that kind of thing,
when you're dealing with a complex system
with a deep hierarchical predictive model,
I think that kind of thing is very important.
And it has consequences for a theory of consciousness
because of what I just said about dreaming.
So why must you lose consciousness at any point?
In other words, why must you sleep?
And why is sleep punctuated by dreams?
I think that those kinds of questions
can be addressed mechanistically
in an artificial agent that has proper sleep,
not the sort of toy sleep that all rests
that I was talking about earlier.
Yeah, well, the reason, I mean, one reason I ask is because,
so I have a postdoc that's very interested
in sleep and unconventional systems.
And so we've been thinking about how to recognize sleep
in things that don't have the typical,
the obvious hallmarks, right?
And so, in Plenaria and Paramecia, in our tissues,
of course there's circadian rhythms and things like that,
but the informational features that you're talking about now,
the consolidation and all that,
how do you, besides simple inactivity, right?
How do we recognize Xenobots, do Xenobots sleep?
How do we recognize that?
So I'm thinking about this whole spectrum
from the totally minimal age.
Well, since we are out of time,
it's a good moment to say we've come full circle
because my answer to that is one objective way
of ascertaining whether or not the organism is asleep
is if it's an organism which is capable
of the sort of thing we were talking about earlier,
generating novel behaviors,
then during sleep that should stop,
because that's an essential feature of sleep
in terms of what I was saying just now,
is you're no longer dealing with life's problems, external,
you're no longer dealing with the external environment,
you've withdrawn from it,
so you're no longer acting on the environment.
Really is in many different senses of the word,
definitional of sleep,
that you're not producing voluntary behaviors.
So I would say if you're talking about organisms
that do display the functionality
that we were speaking about earlier
and calling that voluntary behavior,
I would say when that stops,
if it stops for a phase of the diurnal cycle,
I would say that's its sleep phase.
It doesn't, of course, have to happen only once,
but if there's a protracted period like that,
that would look pretty much like sleep to me.
Yeah, yeah, cool, yeah, well, we'll be...
And isn't it nice?
Again, we see the benefits
of having a more objective mechanistic definition
rather than having to become the organism
to know whether you're asleep or not.
