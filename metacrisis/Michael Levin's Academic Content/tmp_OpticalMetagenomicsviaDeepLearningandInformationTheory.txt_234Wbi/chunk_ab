Which is inspired by I don't know if people heard about the needleman Wunsch algorithm, which is used for a sequence
alignment in traditional genomics
And and this is a dynamic programming algorithm. So
I developed an alternative version of it for
mapping between an image of a DNA molecule and the sequence
When we know what what's what the sequence patterns have been labeled and this equation the kind of
Defines the energy equation for the path for the alignment path and here we can see like an example
Alignment path that could be solved between
those two objects
So that's the alignment algorithm
Now
to improve the method further and beyond the
Twice accuracy that we showed above state-of-the-art
We also employed transformer models
For creating dimensionality reduction of both the images and the genome sequences
creating an embedding and then
Essentially creating your trivial so very similar to text retrieval or speech retrieval that is being used
Nowadays in commercial applications. It's inspired by the work from open AI, which is called the clip
learning transferable visual models from natural language supervision in which they
Improved opera upon the state-of-the-art of image classification. So let's say an image is a dog or an image is a cat
by just training on massive amounts of data in the wild
for pairs of an image and
and the caption that
That describes the image
So similarly to that
We can what we did is we trained on pairs of DNA molecule images
Versus genome sequences. So you can see the resemblance here
And we essentially trained two encoders. So here they had a text encoder and image encoder
We have an image encoder and a genome encoder
and they are trained so that
So that each
you give a batch of let's say 1,000 pairs of the image and
genome sequence and then
you train a model to predict that only the
Ones on the diagonal. So the matching genome sequences to the matching images should have a high similarity score
Let's say cosine similarity and look similar score for off diagonal entries. So
And and and and this way you can train the embedding to bring those
to first of all dimension alter do dimension alter reduction and bring
Related objects in the genome space and the image space closer and of course those are unrelated further apart
This is the loss that's also open a clip uses which is just cross entropy loss
on the similarity matrix
between the two modalities and the transformer model used was
Wavlam
developed by Microsoft research and they used it for a speech to text
applications in which
you have to either either retrieve text by speech or or just do speech to text
Tasks and this problem is kind of similar because we have a genome sequence and then and kind of analog
Noisy version of the genome sequence, which is the image
So there are parallels with speech and text in this case and it appeared that this model worked
much better than other models
Now that vantage of this embedding retrieval model over the localization alignment model
I presented before is a first of all
Computation speed the complexity of this algorithm is logarithmic in genome length compared to linear and genome length for
localization alignment model and that's because when you create a dimension alter reduction and you embed both the genome and
The image into Euclidean space of low dimension. You can use algorithms such as research
Katie tree and others to
essentially find a query in the reference in the logarithmic time
and this translates for example for a thousand X faster when we test it on human genome data and
Compared to like it can take 24 hours for the human genome for 200 X coverage
also an experiment we did so
Another another advantage is that it learns from real data compared to simulated data for the for the previous model
Which is an advantage because it utilizes all unknown physical chemical factors in OGM, which which we even don't know
About but it just learns it from the data
And the simulated data of course doesn't take those effects into account. So it potentially will provide a much higher accuracy
But it can overfit because data is limited. So you depend on a lot of data you have to
Tens of thousands of DNA molecules have to be imaged
So and of course in simulated data you have infinite data. That's the comparison
Now another work with it is as optimization of the experimental parameters themselves. So except
Then improving the algorithm for decoding those DNA molecule images. We want to optimize the labeling pattern itself
So we want to choose the right enzymes to optimize
the process
So there is a white playground with labeling patterns
so
In most experiments we did work with this CTTAG labeling enzyme, which which labels this pattern on the genome, but
Essentially a bacteria have developed so many restriction modification systems and we can use those enzymes from those systems and they recognize
Thousands of different four letter six letter seven letter patterns. So there is a white playground
And but to understand actually which one to choose we want to understand which labeling patterns extract the most information and to quantify information
We use information theory because information theory
Allows for the analysis of the information as the object contrary to other theories in physics that talk about mass and other quantities
Information theory deals about the quantity of information
I won't go into details because I'm sure not not everybody is familiar with this
I'll just say that Claude Shannon developed it in 1948 and it allowed for breakthroughs in communication
storage and
Everything important for us in technology today
Because we could understand and quantify the the information capacity in communication channels, etc
So so we did an analysis of this for
Information theory analysis for this problem of optical genome mapping and the information theory model for for it in simple terms is
Defined like this. So you have the message which is transmitted over the communication channel, which is a genome position
In a genome sequence and in the end what we want to decode is the estimated genome position
So where essentially this DNA fragment which we imaged in a microscope
Where in the genome and which from which genome sequence came from so we want to decode that and there is a whole process involving noise and
Same as in communication channels cellular communication wired communication fiber optic communication
And
To simplify this analysis
What we assume is we can both divide the genome sequence into beans of 1000 kilobase pairs and the image itself and
Then count the number of fluorophores or appearances of the labeled pattern on the genome sequence or on the on the image
And then we derived an equation which estimates the probability of error
Depending on parameters which are
Manifest which essentially are dependent on the size of the DNA fragment
The size of the whole genome reference database we want to
Identify DNA molecules from so the number of bacteria the size of their genomes etc
And the noises that are inherent in the in the process
Which allows us to kind of estimate the capacity of this information capacity of this process
And then it actually gives you useful predictions
So so you can use that model to choose for the optimal
Enzyme out of thousands of potential enzymes to label the DNA molecules
So that you have the most information from this blurry images with dots right so you want to map them to genome sequences
You want the most information possible
And here in this
In this chart I plot
The probability of error
Predicted by the theory for
Thousands of different enzymes of different labeling patterns actually here the blue dots are all the possible six letter combinations
That could be labeled on genome sequences and this is shown both for the human genome and some subset of selected bacterial genomes which are common in pathogens
And highlighted are also some common patterns which are used commercially and
You can you can actually by the prediction of the model you can get to up to 10 times and even maybe 100 times
Lower error probability by just choosing the right labeling pattern for the experimental conditions for the specific genome target
Yes, and and and we're looking forward also to test this experimentally and look at those very special
Patterns and maybe try to label them and and see if it improves the experimental results of optical genome mapping considerably and making it actually useful
So to conclude the research impact for optical genome mapping
Is from that we did is from two two sides one is the deep learning models that improve the accuracy up to 2x for
Short 50 kilobase fragments. It's actually allows for working with shorter DNA fragments. It simplifies the protocol the DNA extraction protocol it reduces cost
It's much more robust to experimental conditions such as non-uniform stretching of DNA fragments different noise factors happening in the images so you can also simplify the devices and etc.
And a significant reduction in computation time which is also important especially when you want to do fast pathogen detection in hours or minutes potentially
And the the information theory analysis allows for the choice of of optimal labeling enzymes which reduces can potentially reduce up to 10x the error rate
It also explains the effect of different noise factors on the accuracy allowing for choosing and optimizing the experimental parameters so now we can understand the problem you know so we can understand which parameters more important
For example, what's the effect of the genome database size is it logarithmic is it linear in that parameter and also allows for
Essentially knowing in advance before you're doing an experiment or building a device which does optical genome mapping, you can estimate its accuracy. So that's the use usage of the theory for that.
Those are two papers we published in bioinformatics and one more should come soon.
Yeah, so I think my home Institute of Technion. So thank you very much for listening and open to questions.
