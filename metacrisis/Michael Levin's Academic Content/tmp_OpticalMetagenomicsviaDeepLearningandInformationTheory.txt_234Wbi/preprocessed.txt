Thank you very much for inviting me to talk. I'll talk about my PhD research
Yeah, it's a
Been hard times in last last days for Israel. I'm sure everybody heard
but I'm happy to
To to give this talk in any case despite the hard the hard times
so
Yeah, so optical metagenomics very deep learning and information theory. This is the this is the topic of my PhD research
and
under the supervision of your chef man professor of chef man from techno and
You buy a bench day and tell we've university
collaboration with a tiny a copy in your vineyard near vine burger from computer science in techno and many other
Many other wonderful people that help with this research
so
I'll I'll talk about first of all, let's define
Metagenomics
Which is the analysis of the entire genomic information in a sample from an environment
Okay now
That that is that is in contrast to
You know like genomic traditional genomics, which takes and amplifies a specific
section of a genome or for specific organs
now
the motivation
For this research for me personally was cultivation free bacteria identification. I'll talk about it
But there are many other uses of the technology and in general of metagenomics
I'll talk about the technology itself about optical genome mapping
I'll talk about
the computational advancements we did using deep learning and information theory for actual optimization of
the method
Right, so feel free to stop me if something isn't clear
So for the motivation
bacterial infection diagnostics nowadays is pretty slow
And sadly many people die because it takes days to cultivate bacteria from blood samples, for example
Until you can actually identify the bacteria and
Prescribe the correct antibiotic despite all the antibiotic resistance that
Happened today with many bacteria
Identification today is done by either phenotyping with different growth media
mass spectrometry on the proteins inside the bacteria
By PCR directed to specific species or in very rare cases also sequencing
But all those require cultivation
So you have because they're not sensitive enough to actually just either analyze the very few
genomic material of the pathogen in the sample directly or
Deal with the low quantity of cells of the pathogen inside
so so something else should be
to be done and
for this I I looked at optical genome mapping as a technology because it provides a way to
observe single molecules of DNA and
Map them to genome references and that way we can actually take a single molecule of DNA and identify which organism it came from
So the definition of the technology is mapping optical images of
linearized and label DNA fragments to reference genome sequences
Okay, and here you can see an example image of
the of a DNA molecule
With which so essentially this is a double strand which we can't see of course in a microscope
But we do see fluorophores that are touched along that the double strand
it's tenons of microns in length and
Multiple thousands of base pair in in length
So it has been demonstrated by different research groups for either structural variation detections in the human genome for
Translocations reverse cells insertions in the chromosomes and of course for microbial species and indication
but
The technology is limited in accuracy and in actual utility today
Because of challenges I'll present both on the experimental side and the computational side and
My aim in my PhD research was to
Make those like solve those challenges and make this technology useful and maybe being used for
Things such as pathogen identification and diagnostics
So advantages of this technology over sequencing
Is first of all extremely long genome maps which
For applications in human genomics. It's very important for study of structural variation, etc
much higher sensitivity potentially
because
of a very simple protocol
Compared to the old process that is needed
For for sequencing by synthesis by
When when you have to essentially do shotgun sequencing at certain
So that's the reason it's a single molecule technique. So that's that's the main advantage
So how to do optical genome mapping first of all we have to linearize DNA
so
There are two ways
That we tried in our lab one on the left you can see that we take
Activated glass surface. Here is me holding a pipette and dragging a droplet
containing
DNA of a bacteria of E. Coli in this case
dragging it on on on a glass surface and
On the left you can see a microscopy image we took and you can see the DNA molecules
It's just stretching in straight lines on the glass due to the motion another way is a chip we used from banana genomics
It it contains thousands of nano channels
Into which DNA molecules can go and linearize
By the application of pressure and electric field. So that's two ways by which you can linearize DNA molecules
But also to create those images you have to label them because otherwise
You won't see anything and you have to reflect information about genome
So there are multiple ways to label DNA for
optical genome mapping
One on the left you see as the enzyme based labeling so and we'll focus on this kind of labeling in the stock mainly
So one way is to
Is to use a nicking enzyme which looks for four or six letter patterns on the
genome sequence
Nicks one of the strands and then a DNA polymerase incorporates a fluorophore there a fluorescent base pair
base and
Another way is to use material transferases which just search for a pattern and with a modification of a cofactor introduce a floor for
add specific sequence patterns another way is affinity based labeling by which you can for example
Stain only GC base pairs by
denaturation, which is
GC or AT selective or by using some other interpolating
Molecule which prevents which has affinity for AT base pairs and this way fluorophores
Only stain the GC base pairs those GC base pairs
so that that's another way to reflect information about the genome and
Image it in a fluorescent regular fluorescence microscope, right?
So the resulting OGM data we have
consists of
Microscopy images
Like this one. So we take a field of view and we see many DNA molecules each line here is a DNA molecule of
Tens of kilobase potentially hundreds of kilobase
length and the dots are the fluorophores that attach at the pattern we
Wanted to label and this is a zoom into one of those molecules
And
On the right and the data also consists of reference genome sequences
And there are tens of thousands of bacteria beings that already has been sequenced by humanity and in general hundreds of thousands of organisms
So we have plenty of data now. We have to match between
Between these images and genome sequences
Also, we have the knowledge of the the sequence pattern we labeled for example here
We see okay, let's if we label the pattern CTT AG on the double-stranded DNA
Then we can actually essentially build a database of reference positions in each of those sequences
So we can map between those images to to those sequences
Yeah, but there are many challenges
so
on the experimental side and
Total internal reflection microscopy is needed
Because the signal is very low. I don't know if you're familiar with this
but but that's that's a macroscopic method that allows to filter out the noise from the background and
Only capture light from very close to the cover sleep
And that's important which induces some experimental challenges the labeling enzymes
have
sometimes low efficiency and and we need them to label only what we are looking for and not
Anything we're not looking for and this could be a challenge. So the this has to be developed
for each enzyme and
Long enough DNA fragments are needed because we're imaging them with light and fluorescence microscopy
and
The linear the linearization of the DNA on the surface also has questionable yield
So you have to make sure that all of them or many of them actually stick and linearize and that's also a challenge
um
On the computational side, which is the main thing I focused in my
research project
Accuracy nowadays for OGM before our advancements is too low for short DNA fragments
And and and noise from different biochemical and photo physical effects
Like enzyme labeling efficiency and floor for
So inter
Inter-effect between neighboring floor force etc. Are not accounted for in the algorithms and
This reduces accuracy
Also, non-uniform stretching of DNA molecules is also a big problem
Which makes it hard to map them to genome sequences and the computation time is pretty significant
especially when you are imaging a
large volume of
DNA and you want to map against let's say the whole
The genome sequences of all known organisms. So I addressed all those challenges
Um
Yeah, let me know up the knife anything is not clear about like the method or introduction
Anybody has any questions?
Alright, so let's go on
so
The first fall we developed a deep learning model that
allows this mapping problem of
Images of DNA to genome sequences
Alright, so our goal is to map an image of DNA molecule to a genome sequence
So we have the reference genome
We know where the positions of the label the pattern. Let's say I'd labeled the but six letter pattern CTT
AAG and
And I know the positions now I get the image. I have to map it
Go over all the possible genome sequences
I know and and match it to the one which is it's most matching to and in the right position. So both ways
and we train this
convolutional neural net to
predict the positions from the images and
Then another algorithm that aligns and I will talk about it that aligns it to the reference gene and
The resulting accuracy here, you can see the comparison
We get essentially so the important line is this one and the blue line and we get twice more
Accuracy versus the state of the art for 50 kilobase
fragments
Which is significant?
Yeah, and so how did we make this model? How do you train it?
So first of all, we considered the simulated data first of all
In which all the different effects
That we could model were incorporated a labeling efficiency of the enzyme off-target labeling
point-spread function meaning
The optical spread of each fluorophore is significant. You see it's not just a dot. It's not just one pixel. It's it's a block
right because
resolution is limited, of course
And dependent on the on the wavelength of flight, of course and the numerical aperture of the objective
And the stretching of the DNA molecule so all those things were simulated and
and
The neural net learned to predict the exact
locations of
each fluorophore in such an image and
so the in green are the ground truth and the blue are the predicted positions which
Is pretty accurate
Given the fact that you hear for example, you have a fluorophore which is actually two fluorophores
overlapped on the same location
but from the shape of the of the
Resulting a blob of light and the neural net is able to discern that there are actually two fluorophores there
Which is very important for increasing the accuracy and extracting as much more information from
the genome from the from the DNA molecule to the genome sequence
The last function I will not go into details unless anybody is really interested and can go back into that and explain the loss function used
And then then we have the alignment algorithm
Which is inspired by I don't know if people heard about the needleman Wunsch algorithm, which is used for a sequence
alignment in traditional genomics
And and this is a dynamic programming algorithm. So
I developed an alternative version of it for
mapping between an image of a DNA molecule and the sequence
When we know what what's what the sequence patterns have been labeled and this equation the kind of
Defines the energy equation for the path for the alignment path and here we can see like an example
Alignment path that could be solved between
those two objects
So that's the alignment algorithm
Now
to improve the method further and beyond the
Twice accuracy that we showed above state-of-the-art
We also employed transformer models
For creating dimensionality reduction of both the images and the genome sequences
creating an embedding and then
Essentially creating your trivial so very similar to text retrieval or speech retrieval that is being used
Nowadays in commercial applications. It's inspired by the work from open AI, which is called the clip
learning transferable visual models from natural language supervision in which they
Improved opera upon the state-of-the-art of image classification. So let's say an image is a dog or an image is a cat
by just training on massive amounts of data in the wild
for pairs of an image and
and the caption that
That describes the image
So similarly to that
We can what we did is we trained on pairs of DNA molecule images
Versus genome sequences. So you can see the resemblance here
And we essentially trained two encoders. So here they had a text encoder and image encoder
We have an image encoder and a genome encoder
and they are trained so that
So that each
you give a batch of let's say 1,000 pairs of the image and
genome sequence and then
you train a model to predict that only the
Ones on the diagonal. So the matching genome sequences to the matching images should have a high similarity score
Let's say cosine similarity and look similar score for off diagonal entries. So
And and and and this way you can train the embedding to bring those
to first of all dimension alter do dimension alter reduction and bring
Related objects in the genome space and the image space closer and of course those are unrelated further apart
This is the loss that's also open a clip uses which is just cross entropy loss
on the similarity matrix
between the two modalities and the transformer model used was
Wavlam
developed by Microsoft research and they used it for a speech to text
applications in which
you have to either either retrieve text by speech or or just do speech to text
Tasks and this problem is kind of similar because we have a genome sequence and then and kind of analog
Noisy version of the genome sequence, which is the image
So there are parallels with speech and text in this case and it appeared that this model worked
much better than other models
Now that vantage of this embedding retrieval model over the localization alignment model
I presented before is a first of all
Computation speed the complexity of this algorithm is logarithmic in genome length compared to linear and genome length for
localization alignment model and that's because when you create a dimension alter reduction and you embed both the genome and
The image into Euclidean space of low dimension. You can use algorithms such as research
Katie tree and others to
essentially find a query in the reference in the logarithmic time
and this translates for example for a thousand X faster when we test it on human genome data and
Compared to like it can take 24 hours for the human genome for 200 X coverage
also an experiment we did so
Another another advantage is that it learns from real data compared to simulated data for the for the previous model
Which is an advantage because it utilizes all unknown physical chemical factors in OGM, which which we even don't know
About but it just learns it from the data
And the simulated data of course doesn't take those effects into account. So it potentially will provide a much higher accuracy
But it can overfit because data is limited. So you depend on a lot of data you have to
Tens of thousands of DNA molecules have to be imaged
So and of course in simulated data you have infinite data. That's the comparison
Now another work with it is as optimization of the experimental parameters themselves. So except
Then improving the algorithm for decoding those DNA molecule images. We want to optimize the labeling pattern itself
So we want to choose the right enzymes to optimize
the process
So there is a white playground with labeling patterns
so
In most experiments we did work with this CTTAG labeling enzyme, which which labels this pattern on the genome, but
Essentially a bacteria have developed so many restriction modification systems and we can use those enzymes from those systems and they recognize
Thousands of different four letter six letter seven letter patterns. So there is a white playground
And but to understand actually which one to choose we want to understand which labeling patterns extract the most information and to quantify information
We use information theory because information theory
Allows for the analysis of the information as the object contrary to other theories in physics that talk about mass and other quantities
Information theory deals about the quantity of information
I won't go into details because I'm sure not not everybody is familiar with this
I'll just say that Claude Shannon developed it in 1948 and it allowed for breakthroughs in communication
storage and
Everything important for us in technology today
Because we could understand and quantify the the information capacity in communication channels, etc
So so we did an analysis of this for
Information theory analysis for this problem of optical genome mapping and the information theory model for for it in simple terms is
Defined like this. So you have the message which is transmitted over the communication channel, which is a genome position
In a genome sequence and in the end what we want to decode is the estimated genome position
So where essentially this DNA fragment which we imaged in a microscope
Where in the genome and which from which genome sequence came from so we want to decode that and there is a whole process involving noise and
Same as in communication channels cellular communication wired communication fiber optic communication
And
To simplify this analysis
What we assume is we can both divide the genome sequence into beans of 1000 kilobase pairs and the image itself and
Then count the number of fluorophores or appearances of the labeled pattern on the genome sequence or on the on the image
And then we derived an equation which estimates the probability of error
Depending on parameters which are
Manifest which essentially are dependent on the size of the DNA fragment
The size of the whole genome reference database we want to
Identify DNA molecules from so the number of bacteria the size of their genomes etc
And the noises that are inherent in the in the process
Which allows us to kind of estimate the capacity of this information capacity of this process
And then it actually gives you useful predictions
So so you can use that model to choose for the optimal
Enzyme out of thousands of potential enzymes to label the DNA molecules
So that you have the most information from this blurry images with dots right so you want to map them to genome sequences
You want the most information possible
And here in this
In this chart I plot
The probability of error
Predicted by the theory for
Thousands of different enzymes of different labeling patterns actually here the blue dots are all the possible six letter combinations
That could be labeled on genome sequences and this is shown both for the human genome and some subset of selected bacterial genomes which are common in pathogens
And highlighted are also some common patterns which are used commercially and
You can you can actually by the prediction of the model you can get to up to 10 times and even maybe 100 times
Lower error probability by just choosing the right labeling pattern for the experimental conditions for the specific genome target
Yes, and and and we're looking forward also to test this experimentally and look at those very special
Patterns and maybe try to label them and and see if it improves the experimental results of optical genome mapping considerably and making it actually useful
So to conclude the research impact for optical genome mapping
Is from that we did is from two two sides one is the deep learning models that improve the accuracy up to 2x for
Short 50 kilobase fragments. It's actually allows for working with shorter DNA fragments. It simplifies the protocol the DNA extraction protocol it reduces cost
It's much more robust to experimental conditions such as non-uniform stretching of DNA fragments different noise factors happening in the images so you can also simplify the devices and etc.
And a significant reduction in computation time which is also important especially when you want to do fast pathogen detection in hours or minutes potentially
And the the information theory analysis allows for the choice of of optimal labeling enzymes which reduces can potentially reduce up to 10x the error rate
It also explains the effect of different noise factors on the accuracy allowing for choosing and optimizing the experimental parameters so now we can understand the problem you know so we can understand which parameters more important
For example, what's the effect of the genome database size is it logarithmic is it linear in that parameter and also allows for
Essentially knowing in advance before you're doing an experiment or building a device which does optical genome mapping, you can estimate its accuracy. So that's the use usage of the theory for that.
Those are two papers we published in bioinformatics and one more should come soon.
Yeah, so I think my home Institute of Technion. So thank you very much for listening and open to questions.
