When Josh was visiting our lab, we had two, among many other things, we had two interesting points that we talked about that I thought, you know, Chris could talk about as well, which one was about error correcting codes.
So, maybe we can, we can start with that and then the other one had to do with with proto cognitive capacities at the bottom of the scale, you know, particles and things like that so.
So, I don't know, maybe, maybe we start with the error correcting codes. You guys want to talk about error correction and how you see it. Maybe Josh, did you want to go first and just summarize what you were saying.
Yes, very briefly put, the reason why there's something rather than nothing is probably because the universe is not impossible. So if you're based reality nothing precedes the universe to make it not happen it's going to have an existing in the non existing branch.
And the non existing branch you don't get observers that complain so you basically have existence for free.
Now we need to figure out why some things are happening in the existing branch and others are not happening.
And the easiest answer to that is that simply everything is happening because there's nothing proceeding universe to select operators.
The universe is the superposition of all operators, which means it's a multivase system.
And you yourself being computed by it don't know which pass you are so the universe for you is going to have the appearance of being into domestic, which means that if you zoom in at the lowest level near the vacuum you'll see lots of random fluctuations that usually don't amount to anything.
In the structure, one part of the universe needs to what another part of the universe is doing. So sometimes you get the circumstances where you have a pattern that remains stable, statistically stable.
You will not be able to figure out whether you are in the past where the universe is going to the photon is going to the right or left slot, but you know what a million photons are going to look like.
Okay, you don't know which operator is going to happen next, but you know what the superposition of operators is going to look like when you have enough of them, and certain starting state.
And sometimes, basically you get a situation where the universe is going to perform our correction.
Before I think is you sit in a bus tub you move your hand you see on the surface of the bus tub lots of patterns emerging lots of waves, most of which are just dissipating not amounting to anything.
But sometimes you get a particle which means a vortex, and the vortex has this property that it's basically circle so it's shoving the information around in the same volume of space and doesn't dissipate until friction makes it disappear or until it bumps into something.
Water in the bus tub wants to produce those vertices more than other things is just what's left over.
And I suspect that the same thing is true for all the particles that basically they are error correcting codes, and the simplest one are control systems that act in the present on the present state or just the immediate state after that.
And slightly more complex control systems have a certain degree of elasticity like molecules can squish them and they bounce back into shape.
And cells are controllers for future states their agents they basically make models of future past so you can remain stable over much more complicated circumstances and in this way you could say that life itself is an error correcting code that allows to keep particle
integration stable that otherwise wouldn't control regions of the universe that wouldn't. And of course it doesn't stop at the level of sales but also their systems above the levels of sales like organisms societies and so on, where you find the replication of those control principles.
But that all sounds good to me.
Next.
So the conclusion from from that train of thought is that one way of thinking about mental representations is to look at them from the perspective of our correction to look at them as quasi particles, a difference between heat and sound is not that one is not bouncing molecules
so it's not that both are, but sound is information preserving.
It's even quantized. If you look at phonons, it's a quasi particle that is emergent over the activity of the molecules it's what's left over after you dissipate the heat, in some sense, in the same way if our brain is we can be seen maybe as an ether where the individual neurons are
that are passing on signals from neighboring nodes in the topology.
And then, if that thing is stable information preserving so you can compute with it. It's basically representations activation waves are something like quite a particles moving so that's thing.
Yeah, so.
Yeah, are you familiar with Princeton's work on free energy.
Yeah, okay. So, I mean, I guess I interpret the, or I see the free energy principle in its simplest form.
As saying something much along these lines.
That any system that persists that remains stable has to do something or other to keep its its boundary intact.
It has to effectively be auto poetic.
Has to maintain some level of structure.
But not so whatever the system looks like. Pardon me.
It needs to keep its boundary and time and tech, not in space. Right. It is needs to write itself into the future.
But it can be something that is actively colonizing as long as nothing invades it.
Yeah, it has to write some bounded set of values of some bounded set of degrees of freedom into the future.
But for instance life doesn't need to keep its boundary intact in the sense that it needs to be worried about being invaded by nothingness.
Because you can just itself invade chaos, but it needs to keep its boundary and time intact.
Basically, it's frontier by which it writes itself into the future needs to preserve it.
Okay, okay. Yeah, by boundary, I mean state space boundary.
Yes.
So, yeah, put everything in a Hilbert space or something. That's the boundary that's of interest in the in the free energy principle.
One, I think in translate what you just said into free energy principle terms without too much trouble. It's certainly consistent.
With what the free energy principle says, although the, the FEP community doesn't seem to talk about error correcting codes very much.
They talk about persistent structure instead. But since they fundamentally think of systems as informational structures, maintaining a consistent structure.
If you're an informational structure just is being an error correcting code.
So I think that hangs together quite nicely.
I guess another reflection, I think off of what you just said is the Zurich quantum Darwinist picture, which you probably are familiar with also in which state components of some system are being selected by its environment for stability.
And again, it's a very boundary preservation sort of idea.
If you fix the boundary between the system and its environment, then you fix the interaction at that boundary and given given an interaction for the whole system.
And then the stability of that interaction is effectively has to be preserved by the system and its environment working together.
And in that case, the eigenvalues of the interaction are essentially being preserved by an error correcting code, the code space being the internal dynamics of both the system and its environment.
And I guess the third kind of perspective on that is sort of the more more general perspective of error correcting codes that's operable and things like ADS CFT, where the state of of some volume is coupled to the state of its surface.
So, again, and a question of, although, although there in that community they don't talk explicitly about observation they take that for granted.
So there is something or other going on in the surface that's describable by some theory.
I suspect that the ADS CFT conformance is is a bit of a red herring.
I think that the universe fundamentally is automata.
Right, so it does is not a volume or not a surface because it's geometry geometry is only what happens when you squint at automata at scale.
Yeah, no, so of course you need to have some kind of conformance like this if you have a field description of the universe and the graph description and you need to map between them.
This time is based at a set of locations that we can make out and the trajectories that information can take between them, and that puts some constraints on what spaces you can observe if they give rise to systems like you.
Yeah, terrific.
Yeah, I completely agree about space and being a merchant.
In fact, Mike and I have been talking for some time about it.
I don't really know what it exactly describes if you look at the LLM that seems to be a radically Fristonian machine in that it's trying to minimize the prediction error.
And you could say that it's this free my energy minimization going on at some level.
But for organisms, it's happening in the limit, like for all life on Earth or something like that. But individual organisms look for more concrete things, because they are occupying only a small region in the space of behaviors.
Right, in some sense, humanity is is not an independent species it's more an organ in the entire dance of the cell on Earth.
And our particular role in this organ is just to burn the oil apparently to
No, our role is to really provide walking apartment houses for bacteria.
That's true for all large organisms.
I think they are the only organism that is able to basically collect this particular entropy the accidentally fossilized biocarbon that was turned into trees before there were enough insects to take the trees apart.
And so our role is to basically put it back into a circulation so that I can make new organisms and they don't know whether guy are already planned for us to wake up the rocks.
I think if that happens, there is the next stage of evolution, but it's going to be awesome because we are going to have substrates to run consciousness on that are magnitudes faster.
Which leads me to this question what consciousness is nice suspect that it's not in a prediction error minimizer but a coherence maximizer.
It's basically an operator that is not working like the transformer does and the alarm minimizing some prediction error, but it's a colonizing operator that takes suitable trainable, but somewhat cardiac substrate, and then imposes an
operation on it that is locally coherent so it acts in such a way as if it was a single agent.
Is there any operational difference between those two.
Because we see that the LLM is becoming coherent in the limit after you train the entire internet into it and use almost all of the compute on the planet it's almost coherent.
But it does lack the creativity right now to.
Yeah, I mean you can say the same thing about people right most of us are not coherent all the time.
But, but that's also because I think humans are evolved to be domesticated.
We are not the smart home in it we are the programmable home in it we can walk a lockstep.
I see a lot of humans when they are confronted with the opportunity to be truthful or to be in synchronization with the environment, pick the letter.
And you can see why that is evolutionary beneficial under the conditions where the ancestors go up.
I also think that there are a lot of humans which lack that limitation and they are basically generally intelligent vision the limitations of the substrate which are severe but if you put them into a different substrate, there is nothing that they cannot do.
Be interesting to test.
I mean history tries to test it but we can't replicate those those experiments.
I'm not I'm not sure as you know how similar it is to to surprise minimization all that but but I do but I really like this notion of sense making as the function as the as the right as the primary goal of it and I've been thinking about this recently in the context of
memory and the idea that you know what you get in these n grams is an incredibly compressed, very sparse representation of things that happened before, and at any given time since you don't have access to all of that which have access to is just the n gram.
The process of sort of re expanding it back out is very creative it isn't, you know you don't have a real allegiance to what did this mean before all you have is well what do I do with this now right what sense can I make of this now, and this like, you know,
ongoing dynamic property of a process of trying to understand what your own memories mean, and I almost feel like, you know, Mark, Mark solmes has this has this phrase about consciousness being palpated uncertainty about the environment.
I think maybe it's more palpated uncertainty about your own memories it's this constant process of, you know, here you are now with whatever traces the past is left in your in your brain and body, and you're constantly trying to make a coherent story out of this you know it's a, and a lot of it I
think you have to be creative you have to bring stuff to it that is not in the in the n gram itself because you've, you know of course compressed that you've thrown away all the, you know, all the, all the correlations and everything else and so a lot of it I think is that that that confabulatory kind of process where it doesn't matter what it used to
what matters is what can I do with it now, you know, and and we have in biology we have we have examples of this like the fact that the, the fact that the butterfly retains memories of the caterpillar, right, even though the brain is refactored during metamorphosis and all that but
you know to me the most interesting part of that is that the actual memories of the caterpillar are of no use to the butterfly whatsoever, you can't you can't use the the exact memories but what you can do is re and remap some of that information onto a completely new body new preferences,
new motion, motion control. Now you live in a 3D world you're a hard-bodied creature now instead of a, you know, a crawler and 2D.
You can't use the memories directly but you can re reinterpret them in ways that make sense to you now, you know, and do things with them now. So, I like I like that that emphasis on this, you know, the kind of the active construction aspect of it.
You know, Mike, I think we could tell exactly the same story you've told here about percepts from the external world as opposed to memories which we kind of tend to think of as percepts from the internal world.
Although lots of them are actually written on the external world as some sort of stigmatic record as we've discussed a lot, but any, you know, take it down to the cellular level, some receptor becomes activated and kicks off some pathway.
It doesn't actually matter to the cell what the previous event that kicked off that receptor was. What matters to the cell is what do I have to do now, given what I just sensed.
And that sensation or percept, if you want to put it in some sort of conceptual terms, can be external or internal. It doesn't really matter. It's still information that has to be coped with in the present somehow or other.
And this is this is why I think that Carl's emphasis on appropriate action is a nice way to think about coherence.
I mean, we've, we've sort of adopted previously in papers, Bateson's notion of differences that make a difference. And what do they make a difference for? They make a difference for doing something.
And coping in real time with whatever information is coming in. And from that point of view, the question of, I have this information, what do I do? What do I do next? Is a prediction problem?
And the question is, you make a prediction, you do something, right? You act on the environment in some way or other. And the result of that action is either good for you or bad for you.
And so maybe you cease to exist, or maybe some food appears, or maybe maybe something very threatening appears, who knows.
So in that sense, this is a very simple sense, of course, of testing predictions. You're just acting and seeing what happens. But there was some model that drove the selection of that action.
And I think that's really all that Carl's talking about. You can, you can, you can make it sound very cognitive, but you don't have to.
The good side of the transformer is that, in my view, the serialization of only adjacent events has limitations.
When you want to model the structure of text, you cannot use the same as for images or images, you can use convolutional networks because they basically embody this bias that adjacent pixels are semantically related, which works relatively well in the visual world.
But for text, this doesn't work because you are going to miss all the long range connections in the text. If you look at ngrams, it's very difficult because the alphabet is too large to basically make an ngram model that is bridging over large distance in the text.
So now you basically need to find structure that is freeform, so to speak, that is deserializing the text into a scene and operate on that scene. And you need to have a process that is actively constructing your working memory contents.
Not as a sequence of events where you decide what to do next, but where you're modeling the entire future space of possibilities at once and then sort that space somehow.
It seems just a funny thing, Sam Gershman wrote a couple of days ago, you're about to computer code and kind of keeping it up to date and everything, and he said, your most important collaborator is you six months ago, and he's not answering emails.
And I thought that was pretty funny, you know, in terms of, in terms of, you know, the messages, interpreting the messages and where they come from.
But I think in, you know, Chris, to your point a minute ago, when you get this, when this event happens, I think maybe one important piece of metadata you might want is whether that event was caused by you, whatever that is, or by something else, right?
Because wouldn't you want to know if something is like, am I being hacked? Am I being trained? Am I learning or am I being trained? You know, if there's a particular event, biologically anyway, it would seem like there's some value to understand, did I just do that?
Or was this somehow triggered from the outside? It seems like there would be evolutionary pressure to have ways of figuring that out, no?
Well, we certainly have very complicated machinery that tries to answer that question, that this whole kind of cognitive ownership and the emotions associated with cognitive ownership and all of that stuff that goes wrong in particular ways and particular unfortunate people.
And yeah, getting that sort of thing systematically wrong is debilitating. But it's all essentially heuristic, right? It's a heuristic solution to an undecidable problem. It's like our frame problem, you know, heuristics.
So, yeah, I think that sort of heuristic metaprocessing is extremely important. Let me remark also for a moment on language. When we're thinking about language and parsing and non-local relations and all of that, I think one way,
one thing we have to think about in terms of chunking input into bigger pieces that are then analyzed as units is that that's a time-windowing kind of process where we allow some input to come in.
And then we say, okay, you mentioned working memory capacity. I mean, now we're going to put this in working memory, and we're going to do a bunch of stuff to it, and then we're going to allow the frame to advance and do a bunch of stuff to the next chunk and compare those two results for consistency, et cetera.
So, we are working at this larger delta t, but I think at that larger delta t, we're still answering what do I do next question. You know, what do I do with this sentence? What do I do with this paragraph?
How can I put this in a representation that I feel like I understand? And feeling like I understand it means feeling like there's something I can do with it. There's some inference I can make from it, for example.
So it's a very good point that you make about the difference between language and images. But I think it's mainly a scaling point as well. It's like the geometry point you made earlier.
You know, we're constantly imposing these sorts of geometries on information that we encounter. And the geometry may or may not be there. We always treat it as there, right? We say, oh, that geometry is in the external world, but it's really us that's putting that geometry under the input.
I think a lot of our actions only make sense when we recognize that you're not just deciding what to do now. Like some of us start us like a control system in the present best, but the agency is the control of future states.
And what I observe in humans that they seem to be temporarily more coherent than, for instance, the present AI video models, which have more real-time coherence in the sense that they're coherent in space, that when you generate a picture, all the features in the picture seem to be more
coherent than the picture is coherent in two seconds into the future.
So basically the information preservation across time is something that's difficult to discover for the present models. And humans are much, much better at this, recognizing that the volume that you are looking at needs to remain constant over very many frames into the future, for instance.
For the spatial cohesion of the objects that we're looking at.
And that means that we have to build a model that is more compressed than the ones that the LLMs and the foundation models are currently building.
Yeah, no, I can, I can certainly see that. I mean, in a sense, you're making Mike's point about memory, except in the other direction.
We have this notion of, not only a past notion of, but a future expectation about the preservation of identity, but what counts as identity gets fuzzier and fuzzier as the planning horizon increases.
So if I'm planning my future, I could think, okay, well, I'm still going to exist a year from now, maybe. But who knows what I'll be like, right? I don't have, I don't have details about that.
In the same way that I don't have details about last year.
So I think this, this business of coarse graining is very important when we think about temporal coherence.
Yeah, I think that identity is ultimately always instrumental to a credit assignment.
And if you remove that, then identity is going to dissolve. It's not going to form in the first place.
But if you don't have a reason to discover yourself as something that is idiosyncratic, for instance, behaving specifically based on your protocol memory.
And then it makes very little sense to assign identity to yourself, making a self model.
One thing that's interesting to me is the question of what it means to train another system and observe that cats are usually most stupid than dogs, but they are much better at training people.
And as a result, they're also less trainable.
It seems they basically have evolved to train others.
As opposed to be trained, which dogs are evolved to be similar to a government.
They sometimes joke that government is the principle to recursively bully people.
And it's invented in many, many societies independently and once government is discovered it's going to colonize until it meets the boundary of another government with a shorter logistics chain.
And so up to a certain size of the system governments tend to be singletons.
They suspect the same is true for consciousness in the brain, for instance, across brains, but the protocol changes and the degree of cohesion changes the bandwidth changes.
And so it's much easier for other systems to work up to this boundary than you for yourself to spread over that boundary.
The bandwidth is no longer large enough to do that. A similar thing is happening when 120 people are controlling the Indian subcontinent you will be British East India company until the local systems are figuring out how this algorithm works and that they have a shorter information chain and are pushing out the occupants.
So basically I wonder what it means to colonize an environment to entrain yourself visit and to build it into structure that extends you.
That's nice. That's, I think that's a nice way to think about things. Yeah.
More sociology applied to biology and physics.
Except normally they decide distinguish two types of sociologists, those who understand social power and those who don't. And the former are called economists.
At ultimately it's an economical problem.
So what the dictators call energy is actually compute credits that you get computed by the substrate when you're competing with other agents that want to be computed by the same sub spreads in your society of mind, for instance, or more generally in nature.
Yeah, jump back for a second back back to just one last thing on this error correction business. So would you, would you say then that the notion of error correction presupposes a distinction between what's an error and what isn't.
In other words, like, you know, chemistry doesn't make mistakes so chemistry just does what chemistry does, but developmental biology which in theory people think is driven by chemistry, definitely makes mistakes.
Right, you have birth defects and things like that. And so, so the distinction. So there's a distinction there because you make errors relative to expectation of some other observer expectation of yourself about what you were trying to do expectation with respect to some
like, you know, you want it to hit whatever but in in in the in the way that you guys are using this term is there any notion of error as distinct from, you know, enough non error or or is this something else.
So, first of all, there are in variances that we observe. And, for instance, I told you I became an animus in recent years.
And that's not because I think that physicalism is wrong, but because I think that the invariance that you're looking at when you look at living things is the software that runs on the region of physics, not the mechanism itself.
The current dominant perspective in Western science is that the world is mechanical, and we need to explain it in terms of mechanisms.
Yet we also understand that, for instance, money is not mechanical.
It's a software that is only apparent when you have a certain course when you put on to the world to interpret the world, but also important aspects of the world make no sense if you don't use money as the experiment.
And because money itself is the invariant pattern that inscribes itself onto the physical reality there.
And the same way minds are invariant pattern or software is an invariant pattern that we care about we don't care about the number of transistors that implement the software.
We don't care about the specific neurons that implement the mind, because the mind is able to recruit other neurons if some of them fail.
And so the invariance is actually the software pattern.
And the reason why these invariances exist is because they compress reality and such a way that becomes controllable.
And according to the good regulator theory, if you want to control something you need to implement a model of what you control.
And I suspect that's the reason why the universe is so surprisingly learnable because all the structure that you observe is probably control structure.
And that means that the universe was able to miss its means on every level was able to discover models.
And particles are probably just vortices. So it's not much to discover.
And the atoms and molecules are the emergent patterns over the particle dynamics.
And, but for sales, it's much more complicated because for sales, you need to have some kind of control system and this that is looking into the future.
And here the complexity becomes so large that you need to stay in the realm of discoverable models.
And that means that you need to highly compress what the agent is doing. So you describe it as a single system with a single main concern and all the other concerns of the system being subservient to it.
You know, our discussion about thoughts and thinkers Mike.
Yeah.
Yeah, I broadly agree with that.
And I would say to have to have a usable error correcting code.
You need to have.
You need to also have classical communication.
And what counts as an error then is a discrepancy between what some system sees and what it expects to see based on its communications with some other system.
So, if I'm
And this is basically Shannon information theory.
If I'm sending a message across some channel and you're receiving it.
Then the message is noise unless we've already shared some sort of information like a language, for example, or a language plus some semantic box that the message is supposed to be about.
And if we share enough, then all I need to do is send a bit.
And for safety, I send three bits to give you some error correction capacity.
But those three bits only mean anything if we've communicated before about what the question is that's being answered.
And the, I mean, the same considerations go over into quantum codes.
When you talk about classical communication. What alternative is that classical communication.
Well,
I think the, I think the question here is, if you have a bunch of interactions, which are defining using quantum theory or information flow or whatever you want to use.
The question becomes, what do you have to assume to call some of them classical.
And you're making some assumption about thermodynamic, the reversibility to call something classical that I, I actually said these words, not some superposition.
And you actually hear some words, not some superposition.
Which, which gets back to what you were saying at the very early, very early on in the discussion about encountering superpositions but imposing order on the based on
regularities of some sort.
The regularities are classical.
The regularities are kind of what we mean by classical information.
Yeah, the superpositions are not very actionable. You need to figure a focus on those aspects that are not in superposition.
Well, the very notion of action is classical. Right. I did this, not that.
Yeah, exactly. So I think in order to conceptually itself itself as an observer with memories and so on the system needs to be able to maintain a classical model of itself.
And the collapse of the wave function is the point in your past beyond you fail to pretend that the universe is classical.
Listen, I suspect the correct interpretation of Copenhagen, if you want to take that one is not that consciousness is causing the collapse but consciousness can only exist in collapse timelines.
Yeah, what one could say an observer can only be an observer, if it considers itself to be an observer, right, to exist for more than just an instant to have some sort of identity over time.
Yeah, right. Isn't isn't that have a Markov blanket, whatever. Yeah.
So you could fundamentally then what you're what you have to do is define some time time period where if if dirt, you know, during that minimal.
It's like a frame rate or something where during that time, if you had multiple multiple thoughts multiple models multiple whatever you were doing, they sort of are in a superposition, if that whole period is what you're what you're seeing
is not infinitely thin, where you have to say was that was it this or was it that, if, if that time period of yourself is your self let is wide enough, then there might have been multiple different things going on and as far as you're concerned it is a superposition because
you can't cut it any further. Isn't that that what we're saying that there's a, you know whatever goes on within that minimal timeframe those are all superpositions but between timeframes, you've got to settle it was you know it was either this or that.
Yeah.
Yeah, if you think about any sort of model of, of an observed process.
Right, think of, think of digitizing what's going on in your laptop.
Right, we, we observed the laptop at the, you know, basically nanosecond scale.
We don't observe it at the femtosecond scale.
So, between those observations.
We don't care what happens.
We just think of these classical state transitions between effectively nanosecond wide slices.
And within, you know, with between those boundaries that we could look with our resolution.
We don't care some sort of quantum nonsense is going on but we don't care about that.
We don't care about the duration of the event we care about how many events are taking place during a duration that we integrate over to see them as one event.
Right, so even if there was a transistor interaction that one would only take a femtosecond be very much to care if it is affecting the thing that we are going to observe the nanosecond scale or on the second scale.
We didn't care about it theoretically the point that my point was just we can't care about it observation.
I just mean that it depends on how much energy you're moving in that femtosecond.
Right, so if that femtosecond is large enough to let your computer go up and smoke very much care.
That's true, but we can't localize it in time.
Yes.
We can't. I mean, we probably can if you really care so I imagine if you were to live in the universe where all the relevant events are happening in femtoseconds which seconds between to build computers you probably need to figure out equipment to deal with those femtoseconds.
Yeah, I mean if we were protons, then we naturally care about that time scale.
Maybe we are protons, we just forget.
So, have you read Italo Calvino's book Cosmic Comics.
Yeah, but long ago, long ago. It's wonderful. Yeah, he he tells us the story of the evolution of the universe from every point of view from that of a proton or something up.
I haven't I haven't seen it. So it's a wonderful very thin little book you can read it in an hour.
Amazing. So since okay since since since we got there.
Can we talk for a minute about your kind of your view on the status of the sort of smallest components.
What what can or should we say about the kind of how much of this kind of stuff, the sort of the the protocognitive kind of perspectives that we've been using how much of that applies at the at the lowest scale.
What can you say about particles versus their environment and so on.
Are you asking Chris or me. Let's start with you, Josh.
I suspect that the lowest level is not yet exerting interesting control but it's just mechanical in the same way as the vortex that emerges in the bathtub doesn't require intelligence to form it simply what's left over after you look at all the patterns and only retain those that are self propagating.
So it's basically some selection process them evolutionary process if you want that is selecting all the non error correcting patterns from reality.
And only those that are error correcting get to stay around with a certain probability.
And once they are over the threshold you can pretend that they're more less static for a given time and then you build structure above them.
So I suspect that at the lowest level, you don't have intelligence yet.
The intelligence only forms at the level that you have persistent particles that can be linked into symmetry breaking multi stable structures.
And then exploit make entropy gradients. So they're basically what life is doing it performs controlled chemical reactions that out compete dump chemical reactions.
And what's the, you know, give me a ballpark of what what do you think is the first slash simplest thing that was doing that.
That's a very tricky question. I think the smallest structure is the cell.
And it's not necessarily the simplest one because the cell is incredibly complicated. I could imagine that you have could have very, very large systems, say, storm systems on Jupiter, that wake up into general intelligence before the first cell
does because the first cell is so complicated I don't know what the assembly complexity of a cell really is how likely it is for this first cell to randomly emerge.
I could imagine that it's this probability is lower than the probability of a generally intelligent agent to emerge at much much larger scales, of course.
So, in a sense, I don't know whether you will find spontaneously formations of agency at the planetary scales.
You, of course, would miss from our sales and perspective.
But I think that it's probably impossible to build intelligence, simply only form elementary particles this at the subatomic scale, because atoms are the first level or molecules where you break symmetries.
What would you be looking for so let's say the Jupiter spot or something like that. What would you be looking for.
I think that the time scales on Jupiter would be so large that it's almost pointless to figure this out. Ultimately, you would need to simulate it over very, very long time spans.
And basically see the simulation deviates from in system that is energy optimizing in such a way that you need to assume that it has memory and is making models of the future to explain what you're observing.
Gotcha.
Interesting.
I agree that
one needs a certain amount of complexity.
To think about planning for the future.
I don't know where I wouldn't want to try to put a finger on where that complexity actually lives.
I think the kind of characteristic scale for say elementary particles not bound together in atoms that characteristic energy scale is very large.
And so what we at our scale and our observational capabilities call a particle.
Is it at that scale.
Some kind of complicated mess.
Right.
Think of drawing the first several orders of Feynman diagrams or something. There's a lot going on.
And
whether any of that stuff at its own scale could be
interacting with other entities like that at that scale.
It's hard for me to think about.
Certainly it's it's not something that we can observe with any technology that we have.
So we're
doing this business of
calling things objects
at a resolution that we can observe them.
And then building a theory that
maintains that sense of object hood.
Down to arbitrary scales.
And that theory is predictive of what we can observe at our scale.
But
I think it's more dangerous to say it's it's really describing what's going on at this at the scales that are really that are of interest there.
But certainly from the point of view of what can we observe.
What are the kinds of intelligence that
we can get a handle on.
Well there's some set of coupled pathways that are that are bounded in some way is probably the smallest entity that we can think about.
And maybe they're very large entities that have lower complexity.
Again the question is how would we observe such a thing.
There's some really interesting work with minimal matter so like droplets and
things like this that are only only a few chemicals but but they have some pretty rich behaviors and I think we need to we need to start developing
sets of sets of tools and you know kind of criteria for for because you know just just observations doesn't doesn't do the trick well obviously have to do some kind of perturbative experiments but we really need to start trying to understand it because
some certainly look like there's a lot of potential for finding some of these things and we need to we need a suite of tests that can start giving us a clue that that something like that is going on.
And the last the last set you know they were you know the running around a maze and they're coming together in in multi sort of you know in higher order structures, and I asked him how, how long did you have to search there.
It's a total of I believe three chemicals that they're made of, and I said how long did you have to search before before you found the chemicals that do this and he said it's basically the first thing I picked up off the shelf.
So, my guess is that it might not be super rare, this kind of thing.
And, you know, it might be it might be pretty natural but but it's unclear yet.
You know, I think I think the field the field of diverse intelligence still doesn't really have agreed upon criteria like what to look for in these kind of systems, even for the ones that are tractable I mean obviously you know, they talk about the
the economics markets in the spot of Jupiter in the cosmic web and all these things we can't do experiments there but but for some of these these minimal minimal active systems we can.
So, I think we need to.
Yeah, we need to settle on some some some experimental criteria.
No, that's very interesting.
But ultimately, you might have to resort to simulations where you try to figure out the behavior of a system from first principles, and then run the simulation to see how your observations deviate and identify criteria and which you need to ascribe more levels of control to the system that you're
observing.
Yeah, I mean we see this kind of very simple kinds of things in terms of going around barriers and you know kind of delayed gratification where it has to go backwards in the gradient in order to get gains later on and things like this in extremely simple systems.
You know that it's that that that emerges that capacity emerges in even very, you know we we studied it in simple sorting algorithms, you know this ability to go backwards to, to make gains later on you don't have to change anything you know very
simple algorithms have can have that those behaviors. So I think I think we're going to end up with a lot of surprises but we need to we need to start looking for that stuff.
Ideally, one would have a way of looking at the system's ability to operate on its own environment for its own future benefit in some way.
I mean in a sense with your algorithm experiments the environment is us.
Yeah.
Yeah, I mean I mean there are these very minimal.
There's a there's a system which actually kind of mimics what real cells do which is it put it constantly generates a self repellent.
And what that does is if you're traversing a maze and you end up at a dead end, you sort of sit there for a little bit but eventually the self repellent builds up and chases you out of there.
And in practical terms it means that you're never stuck in a dead end for you know for any length of time, it pulls you out of pulls you out of these, these kind of cul-de-sacs where you can't do anything else and it's super simple and you can say that what you've
done is you can, you can look at it as a as a stigmatic effect on the environment because you're basically dumped a bunch of messages that, you know, 10 minutes later are going to say, don't be here go go go somewhere else.
So it is it is a you know kind of a very weak version of niche construction from that in that in that sense.
Yeah, that sounds like a good limit to be pushing.
Well, anyway, it's how to build a self organizing system that is generally intelligence basically what is the, the minimal pattern for this colonizing seed that you observe in our own mind.
So basically I suspect that consciousness is an operator that induces coherence.
And I don't have a proper formalization for coherence that is properly tested.
And I suspect of it currently as minimization of constrained violations across models would think of it as a consensus algorithm that is forming in the mind and that is trying to find one interpretation where basically you maximize the number of simultaneously true statements in your
And out of this would fall the notion that there must be an observer that is in any act of observation but I suspect that the terminology of self reflectivity comes from the fact that consciousness to work needs to be self organizing and to do that it needs to be self stabilizing and
self observing.
And so basically I wonder what is this minimal self observing observer that keeps itself stable and is colonizing the environment what is the invariance and how can we formalize it in the substrate agnostic way.
You want the informational analog of a microbial mat.
No of an organism. I want to have something that is more than a biofilm I want something that imposes structure on itself maybe the microbial mat is the precursor.
Basically it's a sufficient substrate where you have self interested local notes that can attainable into performing the computations that you want.
And they actually are on their way to what you're talking about because in girl so else work he shows that to these some of these biofilms have very brain like potassium waves that ensure that the whole thing can eat.
It synchronizes right the metabolism across the network so that the ones on the outside don't get all the food and then the inside starts it starts it has this like large scale, these large scale potassium waves that look at me one of the papers was called you
brain like signaling and bacteria maps it's it's sort of on its way right it's kind of like they're working an organism. Yeah, yeah they're working up to a whole that's doing something different than the parts would do.
So what is holding systems back.
And right elephants are not able to be creative apparently they don't draw images they only replicate throwing stroke by stroke it seems.
Elephants despite having a long childhood do not achieve the same level of generality as we do is this the result of a need for tuning the system in a particular way that is tricky for evolution, or is there a Goldilocks size to brains that they can only integrate
information up to a certain scale deeply.
I don't know. The only thing I know about elephants is that we had we had a guest speaker once years ago that at my comfort in my seminar series where he said that he makes these giant xylophones and he drops them off in the forest of in the
jungle of I think Thailand he said for wild elephants, and he said that wild elephants when they come across these xylophones immediately figure out what the deal is and they start to play them.
And they just like I mean and I said is it good he said not the music's terrible but but but they like it and they you know and they will just not presumably not having heard it before they they like it and they just start to start playing so I don't know there's some kind of creative
why is it not good they have larger brains than us.
Well it's probably good for them. Exactly right that maybe you just don't like it. And I don't know I would suspect that what we consider to be good or not is not so much a question of cultural habits but it's the question of the sophistication of the structure that is being encoded.
Actually, that's interesting to there's all kinds of videos on YouTube where you can see some guy will take a violin out in the woods and just start playing it and all these wild animals come you know foxes will come and listen and you know these these wild animals will just come and listen I always thought that was really
interesting why why does it sound good to them to you know.
I mean I think part of the answer to your question could have to do just with the form of the body.
But we've, we've got these things that are really interesting.
And they don't.
They have a good good.
Pardon me. They have very good noses. Yeah, the elephant. Yeah, yeah.
So they basically have something that's almost as good as we do.
I mean, maybe that's an equally good toy for the brain to play with in its early development.
But I don't know.
Also people that go up without arms are not necessarily cognitively diminished.
They're growing up in a cognitively rich environment.
It's created by lots of other people for in the same way and build these models so basically the brains are evolved to discover this is we have sophistication and abstract modeling.
So under recently the elephant trunk requires babbling in the just just like, just like, you know, babies when they learn to use their arms at the beginning they have no control over it at the beginning just waves around, you know, crazy.
Eventually they get eventually they get some control over it.
But interesting, but interesting right not the legs. So, so the legs they stand up pretty early. So, so the legs are good to go almost almost immediately, but the one with a lot of degrees of freedom takes a while to get to get going.
