an actionable, biorealistic controller,
that it can use to get where it needs to go.
I mean, I think that's amazing.
And I think there are so many applications of this.
I think that's super.
Awesome.
Yeah, sounds cool.
Thank you.
Yeah.
I like the way the vision of the internal map, I guess.
I mean, I mean, it's, you know, it's, I think, you know,
the speculating a little bit about what will be the internal map of mice is a bit too much,
I guess.
But for us trying to, let's say, my goal usually was to understand what you can manipulate,
let's say, in the environment, such that you can satisfy, let's say,
other different constraints.
So, and then you can see how that can help.
The actual map maybe doesn't even matter.
What it matters is that we have this controller that we can modify.
Yeah.
Yeah.
I mean, forget the mouse, but I think easily you could produce a minimal,
you know, a minimal agent in a virtual world that has some needs,
that it would like some variables that it would like to be in a certain level.
Right. So, you can either go with a true false thing, or you could do fuzzy logic and say,
I want my hunger level maximized, but this other thing, I want my temperature in between this
and this.
So, you have some statements of things you prefer, and literally use that Hopfield network as a
controller and watch the Asian navigate.
I mean, you can actually watch it navigate its world.
I think this is incredibly powerful for linking,
because what you started with is a kind of,
no, this is not verbal, it's not right, but like an explicit description of what
the agent might prefer.
And from that, you got to a dynamical actual controller that might implement it.
And that's really, I mean, I could think of all sorts of model systems where that would be useful.
So, I'm also thinking, I'm also thinking in reverse, the thing you said at the end about
trying to read the map at the end, right?
So, that sounds to me like a neural decoding task where the idea is that,
so neuroscience is what they want to do.
They want to scan the brain.
And from that, they want to recover the propositional information that's there.
So, they want to read a bunch of electrophysiological signals and say,
you remember having breakfast like this and you believe in this and you've never seen this
and this is, your preference is that.
So, right, they want to recover those things.
So, this also, and we do this all the time, but we do it in outside the brain.
So, we will look at an early embryonic tissue and we will say,
okay, what shape do you believe you need to be?
Because those are the kinds of things like literally when we recover these bioelectrical
pre-patterns, we can actually recover goal states, anatomical goal states.
And to us, this is, again, just a navigation of anatomical state space where we say, okay,
I see where you're trying to get to is this particular region where you've got five fingers
and you've got this size and whatever.
So, and so we try to do that decoding too, like look at the bioelectrics and say, okay,
so what do you think you are?
Are you a zebrafish?
Are you a frog tail?
What are you?
And I really like this example too, your model, because that's like a simplified model where
you can practice that.
And you can say, can we look at the controller and the time profile of the controller
and extract from that the logical statements, basically read the mind of the creature.
What do you want to be true?
What are your goals?
We cannot do that yet, but I mean, I think maybe it's possible.
Yeah, no, no, I understand.
I mean, it's certainly in the neural context, it's also extremely hard, but this is a simplified
version where we can maybe test some techniques because there are some, obviously, there are
some AI tools that are coming online for inverting some of these things.
I mean, it's an inverse problem, right?
This is what you're trying to do.
Yeah, I think that's incredibly interesting.
Do you know, and so just a related thing that might be interesting to you, do you know the
work of Patrick Grimm on the dynamical systems of view of logical statements?
No, not, and I would be happy if you can forward me a paper too.
I'll send you, yeah, it's very interesting, and it has a number of papers.
So basically, just kind of briefly, it goes like this.
So Grimm is a philosopher in New York, and in the 90s, he developed the following thing.
Consider the simple liar paradox.
Okay, so you've got a sentence as this sentence is false.
The reason it's a paradox is that if you insist on a single unchanging truth value,
then you're stuck, right, because that doesn't work.
However, Grimm did two things.
One is he allowed it to have time, which means that what you can actually do is just
view it as an oscillator, because as you go through the sentence, right, it's true,
but then it's false.
So you've got the simple oscillator that just goes up and down.
And so he gave it time, which immediately removes the paradoxical nature of it,
because now there's no problem, it becomes a dynamical system.
And then the next thing he did was to just go to fuzzy logic and say that, okay,
sentences could have any value from zero to one.
And now you can have groups of sentences that go like this.
Sentence A might say, I am 80% as true as sentence B is false.
And sentence B might say, I am 30% true.
Okay, there's two sentences and they refer to each other.
And so now what you can do now that you view them as dynamical systems, you can plot them.
And you can either plot them as a function of time and you get some sort of time dependent
behavior, or you can use the extra dimension and just make a static shape out of it.
And so now what happens, right?
And so they become, depending on your sentences, they become, some of them are fractal,
some of them have weird attractors that are, some of them stabilize, right?
Some groups of sentences stabilize, some don't, some have all kinds of attractors and so on.
So what it does is that again, which is what I saw here, a very cool connection between
logical statements and sort of propositional content and dynamical behavior.
And getting behavior out of logic, getting spatial, and again, spatial, I mean,
the grim thing too is a kind of spatial navigation in some sort of truth space or something.
You know, I think, I think is extremely interesting.
So we're doing a bunch of work on that now, looking at the different types of sentences,
right? So the liar paradox is a basic oscillator, all it does is, you know, sort of other things
are even more boring as a simple statement that has one truth valid just like sits there.
This thing oscillates, other systems have all kinds of complex behaviors.
And once it becomes a dynamical system, then you can try to do something really crazy.
And this is, I don't have any results on this yet, but just, you know,
as we've shown with models, dynamical systems, models of gene regulatory networks,
they're often trainable. In other words, they have six different kinds of memory that they can
form from inputs. And so, including Pavlovian conditioning, you can literally train these
things. And so, yeah, and biological networks are much more trainable than random networks.
So I think evolution actually likes that property. So now you can think about doing
something completely nuts, which is to train groups of logical sentences.
And so if that's true, I think what happens is that you're not really training the sentences,
you're training a virtual, a simple and extremely simple virtual mind that contains those sentences
as its cognitive commitments. That's what you're training, even though, even though, you know,
you're sort of neglecting the rest of it, we're not modeling the rest of the organism,
we're just modeling that. So, yeah, so I, you know, I think I need to think about this more,
but I think that's one of the things that you have here is an amazing connection between
the propositional content of an agent's mind and a behavior, a dynamical system
controller that guides its behavior in space. I think that's pretty wild.
I like the way you phrase it. Maybe that would be the next paper.
Pretty wild. Yeah, yeah.
Yeah, I don't have a lot of, at what you said, I mean, that's a very nice
thinking about it on a more kind of a meta level. Yeah, I would love to think about if you're
interested, we can maybe do it together. The next problem that the agent will be solving and then
try to implement in this model, I would be happy to do it. Absolutely. Yeah, that'd be great. Yeah,
no, I'd love to collaborate. Yeah, we have a number of virtual, simple virtual systems that we use
them for making hybrids, so where we have neurons, you know, neurons. I mean, yeah, so one of the
things that we have is living cells that are instrumentized to inhabit a virtual world. So
by the signals they put out that moves them around in a virtual environment and then they
get rewards and punishments for, you know, they're basically, you know, it's basically a brain in
a vat living in this virtual environment. And one thing we can do is if we work a little bit on
the that inverse problem that we talked about, maybe we could actually apply it and pull out
pull out the cognitive commitments of that of that organism. So like imagine, you have a set of
cells, they learn to navigate an environment where certain regions are good or bad. And then we use
the reverse of the technique you just showed to extract a set of sentences where the thing literally
tells you, I hate this region over here, I like this region here. And this is the region where
all my needs are met. This is the region I like. We might even we might be able to actually extract
that, you know, those those statements, very simplified, you know, like, just a couple of
dimensions, three dimensions or something. Yeah, I think I understand what you would like to do.
I mean, right now I feel, you know, if you look at the weight matrices at the end, they're very
simple. They're like rank one, basically. So lots of information is gone. So I wonder how would that
need to be modified to, let's say, if you want to remember more than one statement or more than
things like that, then maybe it needs to get a bit more complex. But yeah, yeah, yeah, yeah. Well,
and there's also, you know, there's also the issue of, of course, there's going to be more than one.
Is that true? I think that's true. There's going to be more than one set of statements that are
consistent with any given matrix. So we're not recovering one unique, you know, mental snapshot
that belongs to each of these things. We're going to discuss maybe the simplest one, which is,
which is okay, right? That's, you know, kind of when we do theory of mind and age and age and
interaction. Again, we never know exactly what anybody's thinking, but you sort of say what,
okay, what's the simplest model of my conversation partner that explains what's, what's going on
here. So I think, I think that's fine. I think that's, that's, that's perfectly reasonable.
Yeah. Yeah, that's interesting. Okay, cool. So, so yeah, let me, let me think about this
some more and come out. Yeah, yeah, this is great. And I'll come up with a couple of simple
scenarios for things we can actually model, you know, simple things we can actually model in
this way. Sounds fantastic. Thank you. Yeah. Awesome. Thank you so much. Yeah. Any, any other
thoughts or maybe do, Werner, Mark, anything, anything you guys want to? Just reflecting on
what you're saying. I mean, it's a, something we're kind of looking at in a different domain,
let's say at the moment, from a kind of behavioral design perspective and trying to say,
well, you know, propositional content is maybe one dimension, but you also have say,
the kind of motivations and say the sets of capacities and skills from a kind of more embodied
perspective. And then before this of the environments, I mean, ultimately, if we can get
to a point where we're kind of modeling the interaction between all of those elements,
that would be fantastic, right? But simple is probably a good idea. Yeah. Yeah. And so the,
and so one way to add that. So, so I've been thinking about this for our case, because
if you have a group of sentences that refer to each other, to each other, right, in the, in the
kind of grim diagrams that I was talking about, I mean, that, that's all well and good. And you
get a shape, but, but if they only refer to each other, they're not tethered to any kind of real
world, right? They're sort of self-contained. They only refer to each other. But you can imagine
having atoms in there that do refer to a real world of, or at least a virtual world for their
embodiment. And you can say, you know, sentence A could be I am 50% as true as sentence B times
however true a sensor one is, right? And sensor one is some aspect of the, it's reading some
aspect of the outside world. And so now that system is not just a self-contained logical system
that sort of cycles in and of itself, but it's actually linked to the outside world, right?
And, and you can even then, then you can imagine multiple agent interactions where
here's a group of sentences, A through C, and here's one, you know, C through F, and they have
C in common, which means there's a kind of chemistry that can be made when they come together,
they have something in common, and they, you know, they might end up being incompatible,
or they might be compatible that knocks them into a particular region of the, of the space.
So you can imagine multi, multi-agent interactions, and you can absolutely tether them to the real
world by including variables in your sentences that, that correspond to some kind of affordance in
the, in the real world. So I think you could. So something else that seems to be interesting
there, like what you're describing really does map to the kind of phenomenology of,
let's say the development of behavior, right? We have a bunch of commitments and then we're
trying to work out where, where are the kind of congruences between them. But actually often,
like we fail to do that, right? As, as human beings, we kind of get stuck at times and we get kind
of good, so I don't know where the kind of congruences here, and that leads to ambivalence,
and it leads to ultimately times to disorder, right? So maybe there's actually the possibility
in here to start like, okay, well, I have all these sets of things, and there is some tension,
and maybe it seems kind of incompatible. Can we have a model that helps us understand how you
kind of resolve those tensions eventually? I have maybe one comment on what you said earlier about
an agent and trying to satisfy their desires or needs or whatever. If you, if you model that,
