and self, and finds a global solution, hence optimization.
Now this is a simple example.
We used to show that indeed the SL model can solve set problems, and that the learning
improves the chance for arriving at a global solution.
However, a more interesting scenario to examine is what happens when there is no solution
to the problem.
So for this, we use the map coloring problem.
So here we have the map of South America, and the goal is to cover all regions by distinct
colors such that no two bordering regions have the same color.
So we only want two colors with two colors, so this is not a problem, that's what I meant
by that.
You need four colors for this specific map.
But we find this example interesting, and I'll explain why.
So first of all, we represent the state of the map by a matrix.
Now here S is a matrix where Sij equals one denotes that region i has colored j.
This is a bit more complex notation, but it allows us to generalize the problem to the
cases when we want to use more than two colors.
So this is why we chose to use this notation.
And this problem can be represented by these three requirements.
So each region must be colored, a region cannot be two distinct colors at the same time.
Because with this notation, computationally, numerically, we can have a state like that
where the same region has different colors.
It doesn't make sense, but it can be allowed computationally, but this is a requirement
that we need to have.
So it cannot be two distinct colors, and the third requirement, regions that share a border
should have a different color.
So this is how we represented logic-wise.
You don't need to remember it.
I'm just showing it.
It's a bit more complex than the liar's problem.
And so these are three different sets of constraints, and our propositional formula would be basically
a conjunction of those three sets.
And here with what we did, we also added weights to the sets of causes.
So these two constraints are kind of our preferences, and I'll explain what it means.
So if we don't add any weights, let's just take in these three sets of constraints.
They all have the same weight.
What you see here is how that weight matrix of map-cowering problem of South America would
look like.
But again, as I explained, we might have a situation where a certain region doesn't have
a proper color.
So in order to avoid it, we also tried to put more weight on the first two sets of constraints,
meaning that each region, first and foremost, must be a proper color.
Another way we also tried to do this is instead of putting more weights on the first two set
constraints, we thought, OK, we can adjust the weight on the third set of constraints
by the border length of a country, meaning that adjacent regions are, sorry, the border
between two countries.
So meaning that adjacent regions are less forced to be covered differently if they have
a shorter border.
So if we look, for example, again, on the map, if we look at Peru and Chile, we see
that they share a very short border between them.
So if they're covered the same color, it's not that bad.
However, if we color Chile and Argentina in the same color, since they share such a long
border between them, that would be a bit more problematic.
So this is what this third set of weights means.
So here are the results.
So here, again, plot A and plot B show how the weight matrix representing the problem
looks before and after the simulation.
And then D basically shows the final product of the simulation, the converged state.
And so here, dark blue and light blue basically are proper colors, and orange is non-color.
So I know it's a bit hard to visualize what is non-color.
So this is how we visualize it.
So we see that there are three regions that are not a proper color, and the energy for
that is free.
So that's correlated.
So there's three clauses that were not satisfied.
However, in this situation, there's no point talking about borders between countries because
when you have non-color, so this is basically why we did this additional weights.
So in the second scenario, where we put weights now on making sure that all the regions have
proper color, now everything color properly as anticipated, however, what we can see here
is that it may be hard to see, but if you squeeze your eyes, there are eight borders
that were violated between the different regions, and the corresponding energy is eight.
And the third one with the borders one that we tried out, it's even worse energy-wise,
but we can do that, we wanted to try it out and see what it gets us.
So just to kind of have a short summary right now, so far I was discussing the results from
a satisfiability angle.
We saw that the SL model can solve some problems and we examined how we can manipulate our preferences
through our different solutions when we have this scenario where there is no correct solution.
What I would like to do now is to switch to talk about particularly interesting result,
and that is what we see here in the last plot in the corner.
If we zoom on it, we can see the region of Costa Rica in the specific run has the same
color as Panama.
Now, since nothing is attached to Costa Rica in this map, in principle, it should not cost
anything to the system to flip the color of Costa Rica to be distinct from the one Panama has.
However, this is not the case.
So what this example shows is how heavy learning can dramatically modify the weight matrix
that encodes the problem of the constraints such that some of these constraints are no
longer contained within it.
So in this example, in the unmodified weight matrix, the color of Costa Rica can be trivially
changed to satisfy the constraint on having opposite colors across borders.
However, a state with that color flipped is no longer an energetic minimum of the learned
weight matrix.
And this is quite interesting because this would never happen in the traditional set
solver.
Of course, this is not a good solution, but this is obvious since this problem has no
solution.
This is a worse solution, but that's not the point.
The point is to try to investigate how breaking of constraints happens through learning.
And that is hard to do in abstract problem because you can't understand what is a particular
state means in relation to the original weight matrix.
And with concrete problems like this, we can explore domains that were previously inaccessible.
So the next step would be to try to basically investigate different problems and see how
you can use this knowledge to explore different types of domains.
So to just summarize again, some limitations in future work, the SO model is much slower
than the state-of-the-art set solver.
But yeah, we were not trying to beat the set solvers.
The SO model has some potential for parallelization, and it can be tweaked to work faster.
But we find that it might be interesting for engineers that are interested in biological
possibility, let's say, since it is much more biologically realistic than a set solver.
Another thing with it is that, as I showed, the SO model, we did a max to set problems.
And the SO model in principle cannot handle set instances with more than two liters per
cross, that's what you would think.
However, we know that any case set problem can be reduced to a free set problem, and
any free set problem can be reduced to a max to set problem in both cases at the expense
of linear number of new variables.
So this means that in principle, any case set problem can be reduced to max to set problem,
and then the SO model can be used to solve it.
So it will have just much more, let's say, hidden variables.
About future work, so this specific topic, now that we know that we have this kind of
connections between initial constraints and what happens to them after learning,
we can ask, what did the system learn?
And maybe, for example, if we look at the learned weight matrix,
can we convert back to the logic and see what causes were eliminated, for example?
That's one idea for how the future work can go.
So finally, I would like to acknowledge my co-authors, Werner Koch and Ozan Erdem,
of course, Tom, and this is our lovely unit.
And thank you for listening.
Well, thanks so much.
That's super interesting.
I have a couple of thoughts, and I don't know, maybe this is obvious,
and you guys have already thought about it, but I'm just cluing into this now.
So the first thing is what occurs to me is that the initial set of requirements,
the logical statements that you're modeling, you could think of it as a set of beliefs or
preferences by an active agent of some sort.
And what then you're doing is you're producing a network that can act as a controller for that
agent that would navigate some space in a way that's consistent with its beliefs or preferences.
So I hadn't thought about this before, but I think it's a very interesting
map because I can tell you some related things that we're doing to this.
It's like you're moving from a statement of what the system either believes to be true
or would like to be true, and now you've got an actual
bio-realistic controller that seeks to implement it to you.
So by satisfying it, what you're really doing is navigating a space to a region where the
things that the agent is looking for are in fact true.
Does that make sense to you or have I misunderstood?
I mean, it seems like this is an agent navigation task here.
Well, I love the way you think about it as an agent navigation task.
So, you know, so far my angle on this was always more technical.
So I kind of always look for what is this agent in kind of a verb,
but then I would love to try it out.
But then how would you describe the problem, let's say?
Well, the problem is, so, okay, so I'm, and when I say agent, this could be a cell
or an autonomous vehicle or whatever it's going to be.
I am in a world and I have, this is something Mark Solms talks about a lot,
is this idea of having multiple constraints or multiple drives or needs.
And your goal is to have all of them met.
And sometimes it's impossible to have them all met.
But, you know, so my drive is that I want, in your initial case,
you know, this one is a liar, that one's not a liar.
So I want this one, I want to be in a region of my space where this thing is true
and this thing is also true.
And this other thing is not true.
I want to not be hungry.
I want to reproduce and I want to be out of the danger zone in my environment.
Right.
So those are three things you want.
And there are many areas where you can have one of the three,
but not the other two or two of the three.
And maybe if you're lucky, there's a region of the space
where all of those three things can be met and that's where you would like to go.
So this sounds to me like a typical problem of drive satisfaction for an agent
that has a multi-dimensional set of things that it wants to maximize or optimize.
And what you're looking at here.
And so now, okay, so that's the thing you want.
Now, the question is how do you get there?
So you've got this landscape.
How do you find the place that's going to meet all your requirements?
And it sounds like you've got a mechanism here,
which is actually a bio-realistic controller of how you navigate that landscape.
So what I love about this is that one way to think about the spectrum of agency
is to really think about what the world looks like from the perspective of that agent.
If you're dealing with a bowling ball on a landscape,
all you need to know is your vision as a third party observer,
as your vision of the landscape.
And that tells you the whole story of what's going to happen.
But if you've got a mouse on the landscape,
your view of the landscape doesn't matter that much.
What you want is to know,
you want to know the mouse's internal representation of that landscape.
Where has he been rewarded, punished, what are the valences?
Because you're not going to be able to predict his motion from your view of the landscape.
You need to know what his view is.
So what I see here is a really interesting formalism for specifying
what are the drives and the goals and the preferences of an organism.
And then you're basically producing a vision of the internal map of that organism.
How does it feel about that space and where are the places where it's going to be happiest?
And where it's going to be happiest is where all the constraints are met.
But at least having some of them met is better than none.
So that's super cool.
And now you've shown a map of the landscape
and the internal, kind of a first person perspective,
the agent's view of the landscape and an actual controller,
