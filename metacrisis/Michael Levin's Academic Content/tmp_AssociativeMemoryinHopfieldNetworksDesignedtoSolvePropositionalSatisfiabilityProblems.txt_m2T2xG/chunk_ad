I think a critical problem is how you weigh the different desires, like if they all have the same
weight and there's, it's set up such that you cannot satisfy all constraints simultaneously.
How do you, what sort of weights do you put in? Or ideally, you would want a system to have a way
to determine that itself, because in the, in the map coloring problem, it's, it's very difficult to,
to have the different sets of constraints. You cannot use the same weights, because then it will,
in a problem where no solution exists, it will most likely find a solution that doesn't even
have proper colors, which is, which is why you should have weights attached to that in one form
or another. But it's not obvious how you would let the system decide that itself. And I think if you
have an agent with their desires trying to be met, you have, you run into the same problem. How do
you decide that? If you had some physical model that would lead to weights automatically, then,
okay, that's an option. But if it's just the system in an abstract space, and I don't think it's
immediately obvious how you would do that. But ideally, you would have to fit the system self-adjusting.
No, it's a great question. And I think I'm going to, I'm going to show this to Mark Holmes,
because he's been thinking about these things a lot. He builds minimal agents with drives,
with orthogonal drives. You know, I think, I think one way to do it would be to introduce a time
aspect. So like, so, so you're an agent, and you have a bunch of drives. And the first thing is
that's going to kick in is thirst. And that kicks in before hunger. And you're not really going to
be worried about hunger if you're really thirsty. But at some point, at some point, they both become
sort of critical, right? So some, some things have a, you know, you can almost imagine, I mean,
this is like me imagining is on the fly. So maybe nonsense. But, but, but there's almost like,
like a, like a gradient sharpness, you know, like a, like a, like a slope, where certain, certain,
certain desires ramp up very quickly, and others ramp up slowly. And so these weights
might change in a real organism that's probably set to some extent anyway, by evolution or by
prior experience. Here, you know, you could let it have control over those weights. So for example,
some of the, some of the values of your nodes in your network could actually
feed back to tweak some of the, some of the, some of the weights, right? So I mean,
I can't even imagine what would happen if you did that, but, but that's easy enough to,
easy enough to check. And it's also
that's, that sounds like something that would easily become unstable. Like, I mean, the
Hopfield network is, it's stable, right? Like it converges to some state. But if you, if you
have feedback from the state into the weight matrix directly, I mean, the heavy and learning
does that to a certain extent, right? That is feedback. But it's, it's stable if the learning
rate is low enough. Yeah. If you start introducing direct feedback from the state into the weights,
then I can see this spiraling out of control very quickly. And then you, you have the same
problem on a meta level that you as the experimenter need to twist knobs on the algorithm to keep
stability. Maybe, maybe you as the algorithm, or maybe we have a second network, like a
metacognitive component, right? I could imagine ways to play with it. It may even be a model of
psychological, the kind of thing where you're, you have some incompatible drives that can't
all be true at once. And then eventually you just decide that, you know what, this one thing,
it's not worth it. It's killing all the other stuff, right? It's like, it's like, yes, you know,
I really like that chocolate cake, you know, in the middle of the night, but it's just,
you know, it's messing up my other stuff. And so I'm just, I'm just going to decide that that's
it. That one's out now. And, and so some ability to adding that metacognitive loop where there's,
there's some ability to reweight them based on how things are going. If your experience is that,
you know, I could, I could make nine out of 10 happen. It's just that 10th one that just always
who would like, I'm just like, I'm not paying attention at anyone. So I could, you could imagine
in biology, some of that works. I mean, you're never going to be able to cross off the things
like, you know, thirst and so on off your list. But there are other things you can, if it's really,
and I bet there's, I bet there are experimental data with, you know, probably rats or something
where after a while they just decide that within this environment, there's one thing that they
need to give up and then life gets better. So I don't know, I, it seems like it seems like we could,
we could try various on top of the system you showed today, various kinds of
changes that'll, that'll make that fly. Yeah, that's a very simple model. So I think there's
many ways you can make it more complex and break our head of trying to make it stable.
Cool.
Okay. Well, fabulous. Thank you so much. That was really interesting. Yeah, let me ponder
it some more and we'll be in touch. I would be really interested in working together on this.
Thank you so much. Nice talking to you.
