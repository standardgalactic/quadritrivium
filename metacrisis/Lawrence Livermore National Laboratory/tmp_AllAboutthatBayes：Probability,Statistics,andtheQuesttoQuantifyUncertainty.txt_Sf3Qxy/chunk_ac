turns out you can and that is called a confidence interval and I don't really have time to explain
how it works but this is how you calculate it in this case I'm going to figure out what outcomes
of my hypothetical experiment are as or more consistent with values in my interval as the one
I saw in this case that means anything that produces 12 or more ones out of 50 trials so now
I'm going to take every possible theta every possible probability of rolling a one and I'm
going to calculate the probability of getting 12 or more ones out of 50 trials and that's what that
looks like that is not a probability distribution that is just a probability calculated for a bunch
of different values of theta which is varying over the x-axis I'm going to identify all of the
probabilities up there that have less than a five percent chance of producing 12 more ones and I'm
going to boot those out of my interval and the reasoning behind that is because if I really
had a probability that small I don't think I would have gotten the data I would I that I actually
saw which was exactly 12 ones and then I can say with 95 confidence the probability of rolling a one
is at least 14 and a half percent and this this procedure does in fact have that 95 coverage
probability which is to say for 95 of experiments that you carry out this way the interval will
capture the real parameter value but of course you don't know if you've captured it for any
particular experiment it's worth pointing out I pretty much got the same answer twice I mean both
of them say that the probability of rolling a one is greater than 15 percent ish with uncertainty
whatever that means a five percent it was a lot harder to explain how the frequentist did that
and I would like to leverage my experience in teaching frequentist statistics to tell you
it takes a very very long time to get people to understand where probability comes into play
in frequentist procedures that you have this coverage probability for confidence interval
that when you're doing a hypothesis test you do not actually have a probability that the hypothesis
is true. Bayesian inference can give you that but frequentist inference can't because a hypothesis
is not random a parameter value isn't random and it takes a while to sort of wrap your head around
that and that is one of the primary Bayesian critiques of frequentist inference. So why did
this catch on and boy howdy did it ever catch on the advent of statistics as being a key piece of
the toolkit for science really was the advent of frequentist statistics they're much better at
marketing than the Bayesians and all of a sudden even though objective probability is it's inflexible
and it requires these sort of ancillary randomized experiments before you can you can calculate
results you've gotten rid of this problem of the prior which is twofold the first is how do I come
up with a prior that really represents my beliefs if I'm not a statistician and I therefore don't
think in probability and second of all how do I deal with this situation where if I have one prior
and you have a different prior we get different answers oh no so by getting rid of the prior they
got rid of this problem and they put statistics in the hands of people who don't think in probability
now this made statistics enormously popular it's also led to a lot of the problems that
we're experiencing in statistics today with misinterpretation of results if you want my
take on on frequentist statistics please see my other talk everything wrong with statistics and
how to fix it now available on youtube but rightly or wrongly when this caught on the Bayesians had
to find a way to respond to it and to respond to the implicit critiques of Bayesian inference
that were sort of embedded in the popularity of frequentist inference and you'll be shocked to
hear they couldn't agree on how to do that so there is one school of Bayesians I'll call them
the subjective Bayesians who just doubled down on subjective probability they said using these
ancillary randomized experiments is silly when you can just directly use probability to quantify
your state of knowledge for an experiment but they did pay attention to the critique where it's
hard to come up with a good prior when you don't think in probability so they invested a lot of
effort and a lot of research time into figuring out how do we do that better how do we really
represent people's state of knowledge using a probability distribution and they came up with
a concept called expert elicitation which is how do you pair up someone who thinks in probability
with someone who knows about science and find a way to write down a state of knowledge of science
using probability some subjective Bayesians took it a little bit farther and they denied
the existence of objective probability at all probably the most famous of these is Bruno
de Finetti who'd like to say probability does not exist he did not mean mathematical probability
he meant that probability is not representative of any real world phenomena it was sort of a call
back to Laplace's view the only reason we can't predict things is because we're ignorant about
them so the only viable description of uncertainty the probability can have is as a description
of ignorance rather than some description of randomness de Finetti never actually worked with
quantum physics might have changed his mind so on the other side you had the objective Bayesians
so the objective Bayesians were receptive to a different part of this critique of Bayesian
statistics by the way don't be fooled by the name they still use subjective probability all Bayesians
do but objective Bayesians said all right how do I come up with a procedure that's going to give
the same answer no matter how who uses it how do I just let the data speak instead of worrying about
different people's prior knowledge and so they invested a lot of time and effort in coming up
with what are called non-informative priors and these are priors that are expressive of ignorance
in some mathematically defined way so for example that that smiley face prior I showed you a few
slides ago that is a non-informative prior and it's actually less likely to bias your data
than a flat uniform prior is every time somebody just uses a a finite uniform prior
then an objective Bayesian sheds a single tear there are more rigorous ways of expressing that
concept some objective Bayesians take it a step farther in the objective stage where they will
actually calculate the frequentist properties of a Bayesian procedure so remember that that
Jeffrey's prior base credible interval that I calculated for the the probability of rolling
a one it has a 95 subjective probability of containing my parameter value that's great
it's also a 95 confidence interval which means they've tested it using simulation and they have
found that in 95 of cases where you do a random experiment and you follow that procedure it will
capture the real parameter value so this is basically saying to to the practitioner okay
you're not quite so comfortable with subjective probability as we are you don't have to be this
procedure is going to work under the objective probability framework as well and you can guess
what the subjective Bayesians thought of that there are other styles of Bayesians as well for
example the empirical Bayesians they use their data to inform their prior which depending on which of
those gentlemen you ask is either slightly cheating or a step away from human sacrifice
something that I hope you've picked up on from the way I've been describing statisticians in this
talk is that we take the interpretation of probability very very seriously because we have
to statisticians deliver probabilistic products anytime you get a statistical a statistical interval
or a result of a statistical hypothesis test that is either explicitly a probability or a
function of a probability if you don't start with a probability distribution that means something
and then maintain that meaning very carefully through your various mathematical permutations
then you end up with a number at the end which you can't use because you don't know what it
represents what it stands for so this is why this is a big deal to statisticians be they
Bayesian or frequentist not everyone uses probability that way some people use probability
as an intermediate step in some other procedure and in that case since the probability is a
means to an end they are a little bit less sensitive to maintaining a really good interpretation for
that probability one of the examples of that is Bayesian search and one of the best examples and
one of the earliest examples I have of Bayesian search is the search for the USS scorpion so the
scorpion was a skipjack class nuclear submarine that was lost with all 99 crew aboard on May
22nd 1968 when I say that she was lost this was not just a tragedy which of course it was it also
presented a very practical problem because the last known contact that the scorpion was on May 21st
and she was off the coast of Europe she was supposed to return to her home port in Norfolk
Virginia on May 27th but she never appeared which means that somewhere in the Atlantic Ocean you
have a missing submarine and needle in a haystack has gotten nothing on that so eventually the US
Navy was able to use acoustic signals which they believe are from the accident where scorpion sank
to localize somewhat the location of the wreck of the submarine however their search box was still
well over 100 square miles it was 400 miles from land and it was a part of the ocean that was up to
two miles deep so this is a non-trivial problem on top of that they have other sources of uncertainty
they couldn't precisely locate the search ship that they were using to to check the ocean floor
for scorpion they didn't have GPS they had both false positive and false negative probabilities
for the sensors that they were using to sweep the ocean floor and they could not figure out how do
we fold together all of this information to try and conduct the search in a credible way and they
went to a man named Dr. John Craven who was a mathematician who was very interested in Bayesian
search and Craven and his team said all right well first of all we need a prior distribution
and they came up with one the they were working with a number of experts from the Navy about
where might an incident have taken place what would be the heading the speed the depth of the
scorpion have been what were scenarios that might have caused the scorpion to sink and they rolled
all of this together into a probabilistic prior they sampled that prior and they fed it through a
simulation to say if the accident had occurred in this way where would the wreckage of the
scorpion now be found and the Navy had divided up their search box into grid squares and the
prior is actually the count of the number of times the simulation placed the wreckage in one
particular square just divided by the total number of 10,000 simulations on top of this
Craven's team came up with an update procedure similar to what we saw with the the table bachi
example where based on the false negative and false positive probabilities for the various
sensors they were using to try and find the wreckage they could update this probability
distribution based on where they'd already looked now it took a little bit of time to
fold all of this together but eventually they appeared literally on the search ship in august
of 1968 with this prior and this update procedure and you will all be shocked to hear that the
Navy did not let the math people drive this is because they thought they'd already found the
scorpion they had had a a magnetometer contact at that location right there and they were now
searching repeatedly this grid box and they were not able to get a a photograph of the wreckage
with their camera which was their sort of gold standard for up there of their search sensors
and so the first job that the mathematicians had was a futility calculation was basically saying
based on our false negative and false positive probabilities when have we searched here long
enough that we would have seen the scorpion if she were there and eventually they concluded
that they would have had an 80% chance of seeing the ship if she'd actually been there so they
moved on from that search box concluding it had been a false positive eventually they did let
the mathematicians drive and they you know promptly went to that darkest blue box up in the middle
and they searched and searched and searched and they didn't find anything and they concluded that
they needed to refine their model a little bit like for they needed to integrate their location
uncertainty they needed to understand their sensors a little bit better with false positives and
false negatives and they went out on a calibration cruise to to better understand their mathematical
model and promptly found the submarine such as life so when I say that this was not an exquisite
Bayesian model what do I mean well the biggest problem with it is this discretization of their
probability distribution and the problem with discretizing your distributions is you can bias
your results depending on where you draw those boundaries and you can actually see where that
happened here scorpion is 300 yards away from the highest probability box each of these boxes is
about three quarters of a mile square if they had drawn their boxes in a different place they
might have found the ship sooner so that's my my warning about discretizing your distribution
but I'm not saying that they did anything wrong for two reasons number one they found the submarine
and they found it faster than they would have if they had not been using Bayesian methods so we
have to call it a success on that front and second of all they did not have a choice they were on a
ship in the middle of the ocean literally working 24 hour shifts they had to be able to update this
distribution in real time and that means they needed a distribution they could actually compute
with so if we had used a fully rigorous continuous Bayesian distribution they almost certainly
would not have been able to update in time in fact at this time they probably wouldn't have been
able to work with it at all so they took the mathematical shortcuts that were necessary
to be able to work on this problem and if you are ever asked to do a probability calculation in the
1960s on a ship in the middle of the ocean you can take shortcuts too otherwise consider not doing
it that way so I have just alluded to an issue that I think some of you are probably wondering
about which is why do people think Bayesian statistics is new it's not it's hundreds of
years old in fact it's older than its most popular competitor so why is it you didn't start hearing
about it until fairly recently and the answer is computational remember that that constant
of integration that I kept kind of glossing over earlier on in the slides that's actually a huge
problem it turns out you cannot arbitrarily combine a prior distribution with a likelihood
and expect to be able to compute with it you almost certainly can't write it in closed form
and Bayesian inference for hundreds of years was limited to this very small family of priors and
likelihoods that were compatible and computable there's a reason you saw the beta binomial model
three times during this talk so that changed in 1990 with the publication of a paper by
Gelfand and Smith on something called Markov chain Monte Carlo what MCMC lets you do is sample
from a distribution even if you don't know the constant of integration so now the Bayesians can
go to town they can put together any prior and any likelihood that they want they can sample
from it they can do numerical integration and they can do inference on those samples of the
population and this is what caused the Bayesian Renaissance it was the combination of the availability
of these algorithms with the availability of increasingly sophisticated computing hardware
and this is what allows for the the use of very elaborate hierarchical style Bayesian models
today national laboratory connection the MCMC algorithm is a direct line descendant of something
called the metropolis algorithm that came out of Los Alamos in the 1950s and our old friend Edward
Teller was a co-author on that paper so prepare yourselves it's time for the big reveal what
kind of statistician am I I am all of the above I have used every style of inference
that is presented in this talk in full honesty I was raised frequentist as so many of us are
and to this day my instinctive understanding of probability is a frequentist understanding
it's probability as a representation of random events that said when frequentist inference
can't do something I want to do I go Bayesian a good example is my dissertation is actually
a set of Bayesian models and one of them is a very elaborate model over protein folds where
it's probabilistic distribution over the structures that proteins can take and this model draws
information from adjacent regions of the same protein from families of closely related proteins
when that is not available the prior goes out farther afield and draws information from a
broad class of general proteins I could never have written that model in a frequentist way
so my take on inference is that if it makes sense to you for your problem and if you are up front
about why you made the modeling decisions that you made it's all good and if somebody disagrees
with you that's fine they can analyze it their way maybe they get the same answer maybe they don't
and then you get to learn something so I've mentioned that the frequentists are better at
marketing than the Bayesians I meet a lot of people who are hesitant about Bayesian inference
because of the word subjective in terms of subjective probability subjective does not mean
arbitrary subjective does not mean non-rigorous it means dependent on human judgment and everything
we do in science is dependent on human judgment when we decide what hypotheses we're going to test
how we set up our experiments how we're going to take measurements when you do your statistical
analysis your choice of likelihood is subjective so if you introduce a well-considered prior
distribution that's not going to somehow contaminate your beautiful objective experiment in fact sometimes
clinging to mathematical objectivity can lead you down the wrong path and I have an all frequentist
example for you remember Ron Fisher he was one of the early advocates for the use of randomization
and experiment and one of the reasons for that is that if you use randomized controlled experiments
that can help you establish causality in a mathematically rigorous way in fact it's probably
the only thing that can the early evidence linking smoking with lung cancer was all
based on observational studies so Ron Fisher went on world tour literally saying correlation
