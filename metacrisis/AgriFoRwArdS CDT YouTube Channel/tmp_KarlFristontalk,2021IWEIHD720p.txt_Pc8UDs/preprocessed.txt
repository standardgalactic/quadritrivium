So with that, Carl, if you're here, hopefully you're here, please feel free to unmute yourself
and share your screen. Welcome. Thank you very much. Thank you for another introduction. Let me
share my screen.
Right. Well, it's a great pleasure to be here. And also nice to see so many, so many old friends
as well. That's a perverse gift of COVID. And I was torn about what to talk about in the next
half hour between looking at Markov blankets as one way of studying emergentism and embodiment
when the internal states synchronize with the external states or focusing on the intelligence
part of embodied intelligence. But I've elected to go for the intelligence part and pick up on
one of the themes that the first keynote introduced, this notion of intrinsic motivation
and a theme which came back a number of times, particularly in John Tarniz and Andy Clark's
presentations, the notion of planning and temporal depth and an understanding of the imperatives for
behavior and active engagement with the world that looks into the future. So I'm going to
focus on the intelligence bit of embodied intelligence. And I'm going to cast that in
terms of self-evidencing, briefly rehearse what I mean by that in terms of normative models of
action and perception and possibly consciousness and cognition, how it relates to or how self-evidencing
brings to the table some things that would be useful for understanding
embodiment in artificial intelligence. And then if we have time, just use a toy simulation just
to unpack how the principles work or some of the fundamentals of the behavior that ensues.
So I'm taking my lead from the notion that all we need to know in terms of explaining
and understanding self-organized behavior of a sentient and possibly intelligent sort
is that everything in our brains or in our body and in particular those bits of the body,
that change the way that the brain and the body samples its sensorium and its eco-niche,
everything is just in the service of maximizing the evidence for a artifact or a creature's
generative model of its world, M here. So I'm just articulating that mathematically,
everything is in the service of maximizing probability of some observable outcomes and
sensed outcomes given a model of those outcomes. And I'm just portraying here a number of different
takes on that would depend upon your commitments and your training. You could interpret the log
of the probability of those outcomes that you're trying to solicit or behave in a way to
maximize the probability of those outcomes being elicited as value, the valuable outcomes.
And you could look at that from the point of view of the enforcement learning and optimal control
theory or indeed if you're an economist expected utility theory. I've cast value in terms of a
negative potential that is in fact the self-information and information theory,
so sometimes referred to as surprise or surprise. And that's nice because if we try to maximize
value or have a normative take on value maximization then what we're also saying is
that we also have an information theoretic understanding of the imperatives that
are evinced by self-evidencing in terms of principles of minimum redundancy or maximum
efficiency and indeed the free energy principles. So just noting that the free energy that some
people have mentioned provides a bound on surprise or value when cast in this way.
In term that is also nice because the time average of surprise or self-information is entropy.
So what we're saying is in self-evidencing then we are just trying to resist the dispersion according
to the second law which of course is the holy grail of self-organization particularly in things
like cybernetics and synergetics. And of course if you're a physiologist it's just homeostasis
keeping physiological outcomes or observable states of the body within viable physiological
bounds. I'm going to actually take another perspective though on this quantity which is the
statisticians perspective named with the probability of some outcomes or observables given a model
constituting the marginal likelihood of that model or the model evidence and then one can spin
off the Bayesian brain hypothesis, evidence accumulation, predictive coding and the like.
So that's where I'm coming from. I want to now just drill down on what that means mathematically
and just relate it to a couple of things that we've already been talking about. So why self-evidencing?
Well this quantity constitutes the sort of you know the core part of this free energy in machine
learning this is known as an evidence lower bound. Why? Well because there's a bound here which can
never be less than zero which means that if I maximize F I am necessarily maximizing a bound
on the log evidence hence self-evidencing. This quantity is going to be interesting later on
but let me just focus on the first instance of why it will be useful from the point of view of say
artificial intelligence to maximize the evidence for a model of the world and implicit in having a
model is an explainability. So the quantity the objective function that I'm trying to optimize
is an explicit functional of a model that renders it inherently explainable. So we have a transparency
and explainability and indeed interpretability for free. That leads on to something quite important
which is optimal Bayesian design and of course the interpretation of self-evidencing in terms of a
Bayesian brain or a prediction machine, a constructive organ making inferences about its
environment and of course when you embody or put that in an active context we have a degree of
self-assembly, auto-poesis and self-teaching or to didactic
capacitors which leads to a principled way of understanding data foraging and epistemics
and possibly animal or generalized artificial intelligence. But I want to just rearrange
that equation which is the way that somebody machine learning might understand this free
energy bound. I'm just moving the terms around just to express it in a way that a statistician
would be more familiar with it which is basically the imperative to maximize the accuracy for your
explanations or predictions of sensed outcomes whilst at the same time minimizing the complexity
where the complexity is the degree of belief updating or the amount you have to change your
mind in relation to new data, the degrees of freedom you're using up in terms of providing
an accurate explanation and I'm going on about that because I was intrigued by the discussion of
complexity either in the brain or in the body and simplicity and the like and this is a central
notion in terms of self-evidencing where if not most of but certainly a substantial part of the
drives to maximize this evidence lower bound is in getting the simplest most parsimonious
explanations implicit in your generative model that you can. So that's just across the statement
of Occam's principle and you see that emerging in structure functional relationships in terms of
things like modular factorizations in computers or indeed the brain where it's known as functional
specialization efficient coding it also provides an interesting take on bounded rationality that
you know it's not a question of just being rational and very accurate it's a way it's a special kind
and optimal maximum evidence way of being rational where you're trying to find the simplest explanations
so it easily accommodates the notion of heuristics via prize inherent in the generative model namely
the probability of some states of affairs in the world generating these outcomes here.
It also practically means that when the artifact is behaving in an optimal way in relation to
free energy or marginal likelihood there's some simple physics which says it should be doing it
in the most thermodynamically efficient way so you can invoke the Jyninsky equality and Landau's
principle to say that if you've got the right solution or your artifact is self-evidencing
optimally then it will be consuming the least amount of electricity it will be as small and
as simple as possible. So that's the basic if you like motivations for self-evidencing cast in
terms of maximizing model evidence or marginal likelihood what I want to do though is ask what
would that look like if I had to choose how to use my body to maximize the free energy in the future
and essentially what I'm going to be saying is that if I condition the outcomes yet to be observed
on some plan then accuracy and complexity translate into two things which we will all
immediately recognize basically they're going to translate into an optimal design
and an optimum decision-making so what the story I'm telling here is that self-evidencing just is
a mixture of Bayes optimal design in the way that we go and solicit evidence from the world
evidence for our own existence for our generating model under prior beliefs about the kind of thing
that I am so here in this instance the Bayes optimality is of a decision theoretic sort that
if I choose to do this then the outcomes are going to be close to my prior beliefs about the kind of
outcomes a thing like me will expect to encounter so I'm just going to briefly rehearse that story
by comparing and contrasting conventional RL like approaches to optimality versus
this more general self-evidencing approach and I normally start this lecture
by asking you to imagine that you're a bird of prey and you're hungry and then I ask the audience
normally what are you going to do and they normally say quite correctly I'm going to
search and fly around until I find my prey and then I'm going to eat my prey and in that answer
there is buried an important or several important takes on the kind of functions or
functionals that you would use to try and describe good behavior so we could either assume the
existence of a value function of some states of the world that are contingent upon some
action or control variable u here and if this value function of states of the world existed
then I could optimize the value by selecting the best state action policy pi policy by selecting
the action u for every given state that maximize this value function and this is known as the
Bellman optimality principle however just in saying I'm going to search for my food before I eat it
you're saying something quite profound because searching is a action that resolves uncertainty
and uncertainty is an attribute of a probabilistic or a Bayesian belief and therefore the thing that
you are optimizing can't be a function of unknown states of the world it has to be a function of
beliefs about states of the world so in in the sort of Bayesian interpretation I'm denoting
posterior or approximate posterior beliefs about states of the world by q so what we're saying is
the optimal action depends not upon states of the world but on beliefs about states of the world
and we get this sort of functional form here the other thing that the sort of the owl example
shows is that the order matters it matters whether I first of all search for my prey and then eat it
or try to eat it and then search for it which means that there does not exist an optimal state action
policy but more an optimal sequence of behaviors given what I currently believe about the state
of the world at this point in time and we can articulate that in terms of a path or a time
integral of this quantity which is in fact going to be an expected free energy that Andy and a
number of other people have already mentioned and leading us to an optimization of a time integral
of an energy function which in physics is an action and I'm phrasing it like that because
what we've basically got here is a choice between the bellman optimality principle that would seem
things like optimal control theory or dynamic processing deep reinforcement learning Bayesian
decision theory on one hand or on the other hand we have Hamilton's principle of least action
or stationary actions more strictly that is exemplified by things like active inference
and subsumes things like artificial curiosity and intrinsic motivation that I'm going to cast in
terms of optimal Bayesian design that optimally resolves uncertainty about my high hypotheses
about how my body works and how the world works it also entails sequential policy optimization
and the distinction between observations of a certain world and the partially observed worlds
so here's the basic architecture that one has in mind when invoking and we've seen lots of lovely
examples of this for example Jantani's illustration this morning so you know the basic theme the
embodiment gets into the act here in terms of this sort of reciprocal engagement this circular
causal link between the inside and the outside that Andy was talking about
mathematically what one way in which one can describe this is that you get these outcomes supplied
by the environment and you use these outcomes to maximize a functional of those outcomes and
your beliefs under a journey model about states causing those outcomes to form posterior beliefs
about states of the world out there you then use these to roll out effectively into the future
under a number of potential policies so this is exactly the poise over possible actions or an
action space the action space here is explicitly encoded in terms of sequences of actions or
moves on the world denoted by pi and each pi you can roll out and estimate the expected free energy
which I've here decompose into risk and ambiguity and we'll drill down in that in that in the next
slide at the moment we just need to know now we've got a way of scoring each way forward each one of
these possible action possible policies possible plans can be scored in relation to our beliefs
about the kinds of things that we expect to encounter and the kinds of things that we do
with our bodies and that score can then be used to find the most likely policy with selected action
from that belief about the most likely policy it changes states in the world the world supplies a
new outcome and off we go again in this perception action like cycle that has this planning and
policy selection as an integral part of it and so this is the bit that sort of links the notion of
simplicity and complexity to the more prospective aspects of action and planning so what I've done
here is just rewrite down the evidence lower bound the free energy here in terms of accuracy
and complexity and remember the complexity is the difference between your post areas and your
priors the amount you've had to change your mind which is the energy hungry part of it and the accuracy
here and just by moving the terms around we can reconstitute or recover the the expression
that lends its elbow acronym the evidence lower bound because this quantity is always greater than
zero and here's the expected value has a very very similar form the only difference is that we're
now conditioning upon some possible action or possible sequence of actions and interestingly
what happens now is of course you don't have the observations that would be used to evaluate the
free energy because they're now random variables in the future so you now have to take your expectation
over the prediction of the outcomes in the future given I'm going to do that and hence expected free
energy but the form is the same and I like this because it's really quite revealing so the complexity
becomes risk the accuracy becomes on the negative accuracy becomes ambiguity which means that we're
going to choose those things if we are choosing those things that maximize expected free energy
in the future we are necessarily going to be minimizing our risk and minimizing our ambiguity
and I explain what that means in a second or we can carve up this expected free energy in another
way in terms of the intrinsic and the extrinsic value and this is the intrinsic motivation so we
have basically both the complexity and the intrinsic motivation or the expected complexity
and intrinsic motivation as different ways of carving up this underlying imperative for
self-evidencing but self-evidencing where you're maximizing the expected evidence in the future
given a particular action just to unpack this a little bit for those people in different fields
if I ignore the extrinsic value at the moment and by which I mean I've got some prior beliefs about
the kind of outcomes that I as a creature will expect to encounter if I have no particular
preferences prior preferences then we're just left with this thing here which is the expected
evidence bound and this quantity here is just the difference in my posterior beliefs with and without
observations that I would get if I pursued this policy and of course this is the basis of
the mutual information between the consequences and their causes states here that will be expected
under that particular policy if you do visual search or active sensing active vision then you
may recognize this as the expected base in surprise basically the amount of information the epistemic
affordance attributed or afforded by this particular policy the reduction in uncertainty about states
of affairs in the world out there in the future given observations and evidence for those states
in the future if I committed to that particular policy if I take uncertainty of the various kinds
off the table we're actually going to get going to get back to reinforcement learning or expected
utility but let me just do that carefully let's first of all assume that we've got a simple world
in which we can sense the states of the world so we remove the uncertainty about that's inherent
in the partially observed aspects of certain models so that we magically our outcomes now become
the hidden states of of the world we can see everything in this instance the s's become the
o's and we're just left and the ambiguity goes where there is no more ambiguity in the world we
know exactly what's going on and and that just leaves the risk so what is a risk well it's the
against the KL divergence between what I expect to happen under this policy and my prior preferences
so when I minimize risk what I'm doing is I'm choosing those policies that will lead to states
of the world or their outcomes the a priori I prefer so I'm trying to minimize the degree of
divergence or difference between again and posterior about future outcomes and prize over
future outcomes so I'm very very closely related to complexity not the complexity cost but now
we're talking about the future and if I remove the final kind of uncertainty from the table
what we're left with is effectively and the final kind of uncertainty is that the
the uncertainty due to different actions relative uncertainty just disappears I'm just left with
this thing here which I set up in the first slide as being really a version of expected utility
theory where the the expected or extrinsic value is just the expected value or the log
probability of preferred outcomes under what I anticipate to ensue given a particular
well given all policies or actions here so what we've done here is is derive from the
simple notion of self-evidencing the service of garnering evidence for your own existence
as implied by your model your embodied model of the world and carving it up into two different
aspects of Bayesian good behavior first of all I'm going to commit to those behaviors that would
or choices that have the greatest expected value and in statistics that's known as
making Bayes optimal decisions under some loss function but here we're casting
the loss function or the utility function in terms of a prior preference and then
which is adding to that effectively the information gain that ensues from pursuing a particular
policy and together they constitute the expected free energy or the expected evidence afforded
by a particular policy so I'm going to close now just by very quickly taking you through some
embarrassingly simple simulations I mean look at the beautiful simulations this morning
we are nowhere near hand able to scale up to that level of sophistication however I think
they're still illuminating you know in the sense that you know there's a certain simplicity in
you know in what I'm saying even though some of the maths may look a bit complicated
I'm not going to go through this in any detail I just want to show you the
kind of behavior the sort of behavior that is mandated under this optimization of expected
free energy we normally use Markov decision processes sort of discrete state space models
of a world graphically cast in terms of transitions from one state to the next point
in time generating outcomes of the likelihood mapping where the transitions are dependent upon
a policy be usually coded encoding the probability transitions and then we have interesting
well the policy clearly depends on the prior preferences encoded by this preference or
cost function here we also have in many of the simulations the precision of our beliefs
about policies and I just mentioned that because that's the W in John's talk that sort of
tunes the hierarchical depth or the ability to learn versus to teach depending upon you
the confidence the a priori you place in your your inactive policies when interrogating or
querying querying the row the world a couple of hyper parameters about initial states and when we
plug this very generic form for the geratin model into standard
elbow or free energy optimization schemes we get a a series of belief updating
equations that look very much like what the brain does and indeed we know that because of all the
all the simulations and interpretations process theories that we've already
witnessed this morning and in the literature and so you know there are many unknowns here
we just don't we don't know about the state of the world we don't know what we're doing and we
don't know how confident we are in what we're doing and we actually also don't know the parameters of
the of the of the model itself you know the the likelihood mappings and the and the transition
matrices so those can be learned so we just have perception which is resolving uncertainty or
inferring states of the world policy selection so having the right poise if that's correct abuse of
and its philosophical term on the space or possible actions and and also having a kind of second order
belief about our beliefs about those actions in terms of the confidence or the precision
that incidentally looks as if it's encoded by dopamine in the human brains then we have learning
of the unknown structure of the world that would be a particular interesting from the point of view
of the sort of you know the the embodiment part of this equation and then action selection is
selecting an action from my beliefs about what I do and when I simulate that and I'm going to simulate
that by applying those equations to a little mouse who can has to solicit a baited reward
from the right or the left arms it's only very it only has two moves and has to stay
in one or the other arm once committing to one of those arms but it has a choice of using up one
of its moves to go and get an instructional cue that will tell it definitively whether the reward
is there or there so this little rat has the choice of taking a gamble and it could be 50
percent correct and staying there or what spending one of its exploitative moves in an exploratory
sense to go and resolve uncertainty about the world and then go and secure the the cheese and
of course this is set up this sort of two-step paradigm is set up to demonstrate even though
the expected utility of both policies is identical on average that the self-evidencing
creature will clearly go and resolve its uncertainty responding to the epistemic affordance or the
intrinsic motivation go and get the cue and then go and enjoy and exploit its knowledge
by going to the arm that is baited and indeed that's what the agent does but what this simulation
is showing is is a systematic and principled and completely non-deterministic
this is a deterministic optimization of expected free energy which means that
if you actually introduce a predictability and a structure and a consistency to the world which
we've done here just by leaving the reward on the left arm perpetually after the first couple of
moves then the creature the rat will come to learn this and therefore the salience or the novelty
of the uncertainty resolving cue slowly wanes and there is no uncertainty after a while because
the rat knows that the reward is over on the side and therefore the epistemic affordance the
the explorative drives the intrinsic motivation for going to the condition stimulus or the
instructional cue starts to disappear and at some point when the intrinsic and the extrinsic well
well the intrinsic value or motivation falls below the pragmatic or the extrinsic value it'll
just switch to a different kind of behavior and this is incredibly systematic and you see this
in many many simulations and indeed in real world situations where you know your first imperative
is to resolve uncertainty about this particular world this eco niche this body and when you've
done that you start to then turn to your prior preferences because there's no more uncertainty
to hoover up so by move 20 the rat just goes straight there quite happily and will do so
forever if nothing else changes this is the final slide and I've got 30 seconds left so
I put this here just to remind myself about that initial ambivalence about whether to talk about
sort of the emergence of lifelike behavior from first principles or whether to talk about the
sort of the mechanics of planning and deep jota models of my own action it will be nice to
to actually connect the two perspectives and I'm just putting this up as an outstanding challenge
and the frame in which I think that challenge can be met I won't go into this but other than to
if anyone's interested provide a sort of starting point for discussion so here's what we've just
been basically saying we can understand the right kinds of behavior in terms of selecting the right
possible actions and what are the right ones where the other ones that underwrite our self
evidencing that minimize risk and ambiguity and they have this functional form there's another way
of understanding the universe in terms of non-equilibrium steady states that are normally
described in terms of integral fluctuation theorems if you did chemistry or physics and they
have a very similar structure in terms of articulating the probabilistic development of
states and outcomes consequence upon different states in terms of kaon divergences to provide
bound or inequalities underwrite much of statistical thermodynamics and interestingly
you can use the integral fluctuation theorems to derive this if you make one particular assumption
or commitment one small move here and all I'll say about this is it hasn't actually been done yet
but it does involve some probabilistic relationship about between the probability
of a path of action into the future and the ambiguity or the negative entropy of outcomes
given their causes so again linking action and uncertainty in an intimate way which may be the
key for understanding the kinds of systems i.e creatures like us that distinguish ourselves
from things like viruses or other self-organizing systems that don't plan anyway I'll leave the
last word with Einstein everything should be made as simple as possible but not simpler
and all that remains is for me to thank those people whose ideas I've been talking about and
of course thank you for your attention thank you very much indeed I'll stop my sharing that
thank you very much Carl I'm sure the 348 participants if they could they would unmute
and join me in thanking you there's a couple of questions arriving in the discord so before we
get to one or two questions I just like to ask the organizers if they could help connect Professor
Friston with the discord if he's not there already there's a question from Dana Damian
beautiful theory thank you does this theory also explain the emergence of belief in the first place
in an elemental way yes so that would be part of the story that I couldn't tell so
if you just take any system that has self-organized some kind of non-equilibrium steady state so it
persists there are recognizable characteristics and then you partition that system into inside
states that constitute the inside of a system and the environment and then they're enshrouded by
blanket states this is a Markov blanket partition then you can use the necessary properties of
non-equilibrium steady states solving the density dynamics to show that there is an information
geometry where the physical inside states come to parameterize Bayesian or probabilistic beliefs
about the outside states and that licenses then an information geometry which is a geometry of
belief states so if you if you permit me to interpret beliefs as Bayesian or probabilistic
beliefs conditional probability distributions then mathematically you're saying is there
an information geometry there and why I would say yes there is under the condition you've organized
to a non-equilibrium steady state great thank you again more questions arriving there's a question
here in zoom chat from from Andy if minimizing expected free energy delivers simple planning
what delivers deliberate planning that seems different yet related
I think the distinction is a very subtle one and we don't have time to get into it because
it's a really interesting question so notice that I was talking I was just carving this
functional this sort of expected free energy model evidence into two bits that linearly decompose
because we're in long space into sort of an information gain effectively you know a pragmatic
or expected utility term that added that would could be resolved independently
so you could argue well look yeah that's fine but that's not really resolving uncertainty about what
matters to me in relation to my prior preferences yeah so yeah I'm just resolving all uncertainty
irrespective of you know what what what I actually think I should be the kinds of things I'm going to
I'm going to solicit so if Andy's asking that question to ask well when does it get more interesting
one could algorithmically say well you have to move to a sophisticated inference where you
actually have to um in so just resolving uncertainty you have to actually encode what would I believe
if I knew that and what would those new beliefs mean as I work towards my preferences it's not
difficult to do it's basically a probabilistic generalization of the the bellman recursion
um however I'd actually say you don't probably don't need to do that because remember although these
two bits of the the good ways to choose your policies are linearly separable they're linearly
separable in long space which means you're actually multiplying so what you are saying in
probably in terms of probabilities I am choosing those um policies that have the greatest
epistemic affordance given that they afford um outcomes which I prefer so I think there's
actually built in for free a certain sort of um deliberation there in the sense that you know
conditioned upon resolving uncertainty there has to be an aspiration to the goals that are written
into the prior preferences great thank you um we have some time for discussion at the end of the
plenary session so I'm going to uh use my host privileges here and ask you one last question
from discord um how does free energy deal with uh groups of agents and a common belief among the
group will it promote agreeing uh with the group over further exploration of the environment
yes yes it does it promotes um cooperation and generalized synchrony so mathematically you know
that generalized synchrony basically affords a predictability of two agents in a diode or in
fact a multi-agent context so if the whole game is to minimize surprise then the best solution
is to basically um make you exactly like me and make me exactly like you and then our discourse
will be a perfect synchrony we're both mutually predictable and our joint free energy will fall
so it's all about finding harmony in groups um a shared narrative a common ground um and from
that you can spin up all sorts of hermeneutic interpretations and indeed you know the imperatives
for for language in terms of cultural niche construction under this sort of variational
variational frameworks and that would have been a third lecture which I which you can't do everything
in half an hour certainly can't thank you so much professor fristen on behalf of all the
participants here for a for a fascinating and thought-provoking presentation
