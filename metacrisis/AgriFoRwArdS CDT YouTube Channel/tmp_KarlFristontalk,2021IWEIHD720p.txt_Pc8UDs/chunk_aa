So with that, Carl, if you're here, hopefully you're here, please feel free to unmute yourself
and share your screen. Welcome. Thank you very much. Thank you for another introduction. Let me
share my screen.
Right. Well, it's a great pleasure to be here. And also nice to see so many, so many old friends
as well. That's a perverse gift of COVID. And I was torn about what to talk about in the next
half hour between looking at Markov blankets as one way of studying emergentism and embodiment
when the internal states synchronize with the external states or focusing on the intelligence
part of embodied intelligence. But I've elected to go for the intelligence part and pick up on
one of the themes that the first keynote introduced, this notion of intrinsic motivation
and a theme which came back a number of times, particularly in John Tarniz and Andy Clark's
presentations, the notion of planning and temporal depth and an understanding of the imperatives for
behavior and active engagement with the world that looks into the future. So I'm going to
focus on the intelligence bit of embodied intelligence. And I'm going to cast that in
terms of self-evidencing, briefly rehearse what I mean by that in terms of normative models of
action and perception and possibly consciousness and cognition, how it relates to or how self-evidencing
brings to the table some things that would be useful for understanding
embodiment in artificial intelligence. And then if we have time, just use a toy simulation just
to unpack how the principles work or some of the fundamentals of the behavior that ensues.
So I'm taking my lead from the notion that all we need to know in terms of explaining
and understanding self-organized behavior of a sentient and possibly intelligent sort
is that everything in our brains or in our body and in particular those bits of the body,
that change the way that the brain and the body samples its sensorium and its eco-niche,
everything is just in the service of maximizing the evidence for a artifact or a creature's
generative model of its world, M here. So I'm just articulating that mathematically,
everything is in the service of maximizing probability of some observable outcomes and
sensed outcomes given a model of those outcomes. And I'm just portraying here a number of different
takes on that would depend upon your commitments and your training. You could interpret the log
of the probability of those outcomes that you're trying to solicit or behave in a way to
maximize the probability of those outcomes being elicited as value, the valuable outcomes.
And you could look at that from the point of view of the enforcement learning and optimal control
theory or indeed if you're an economist expected utility theory. I've cast value in terms of a
negative potential that is in fact the self-information and information theory,
so sometimes referred to as surprise or surprise. And that's nice because if we try to maximize
value or have a normative take on value maximization then what we're also saying is
that we also have an information theoretic understanding of the imperatives that
are evinced by self-evidencing in terms of principles of minimum redundancy or maximum
efficiency and indeed the free energy principles. So just noting that the free energy that some
people have mentioned provides a bound on surprise or value when cast in this way.
In term that is also nice because the time average of surprise or self-information is entropy.
So what we're saying is in self-evidencing then we are just trying to resist the dispersion according
to the second law which of course is the holy grail of self-organization particularly in things
like cybernetics and synergetics. And of course if you're a physiologist it's just homeostasis
keeping physiological outcomes or observable states of the body within viable physiological
bounds. I'm going to actually take another perspective though on this quantity which is the
statisticians perspective named with the probability of some outcomes or observables given a model
constituting the marginal likelihood of that model or the model evidence and then one can spin
off the Bayesian brain hypothesis, evidence accumulation, predictive coding and the like.
So that's where I'm coming from. I want to now just drill down on what that means mathematically
and just relate it to a couple of things that we've already been talking about. So why self-evidencing?
Well this quantity constitutes the sort of you know the core part of this free energy in machine
learning this is known as an evidence lower bound. Why? Well because there's a bound here which can
never be less than zero which means that if I maximize F I am necessarily maximizing a bound
on the log evidence hence self-evidencing. This quantity is going to be interesting later on
but let me just focus on the first instance of why it will be useful from the point of view of say
artificial intelligence to maximize the evidence for a model of the world and implicit in having a
model is an explainability. So the quantity the objective function that I'm trying to optimize
is an explicit functional of a model that renders it inherently explainable. So we have a transparency
and explainability and indeed interpretability for free. That leads on to something quite important
which is optimal Bayesian design and of course the interpretation of self-evidencing in terms of a
Bayesian brain or a prediction machine, a constructive organ making inferences about its
environment and of course when you embody or put that in an active context we have a degree of
self-assembly, auto-poesis and self-teaching or to didactic
capacitors which leads to a principled way of understanding data foraging and epistemics
and possibly animal or generalized artificial intelligence. But I want to just rearrange
that equation which is the way that somebody machine learning might understand this free
energy bound. I'm just moving the terms around just to express it in a way that a statistician
would be more familiar with it which is basically the imperative to maximize the accuracy for your
explanations or predictions of sensed outcomes whilst at the same time minimizing the complexity
where the complexity is the degree of belief updating or the amount you have to change your
mind in relation to new data, the degrees of freedom you're using up in terms of providing
an accurate explanation and I'm going on about that because I was intrigued by the discussion of
complexity either in the brain or in the body and simplicity and the like and this is a central
notion in terms of self-evidencing where if not most of but certainly a substantial part of the
drives to maximize this evidence lower bound is in getting the simplest most parsimonious
explanations implicit in your generative model that you can. So that's just across the statement
of Occam's principle and you see that emerging in structure functional relationships in terms of
things like modular factorizations in computers or indeed the brain where it's known as functional
specialization efficient coding it also provides an interesting take on bounded rationality that
you know it's not a question of just being rational and very accurate it's a way it's a special kind
and optimal maximum evidence way of being rational where you're trying to find the simplest explanations
so it easily accommodates the notion of heuristics via prize inherent in the generative model namely
the probability of some states of affairs in the world generating these outcomes here.
It also practically means that when the artifact is behaving in an optimal way in relation to
free energy or marginal likelihood there's some simple physics which says it should be doing it
in the most thermodynamically efficient way so you can invoke the Jyninsky equality and Landau's
principle to say that if you've got the right solution or your artifact is self-evidencing
optimally then it will be consuming the least amount of electricity it will be as small and
as simple as possible. So that's the basic if you like motivations for self-evidencing cast in
terms of maximizing model evidence or marginal likelihood what I want to do though is ask what
would that look like if I had to choose how to use my body to maximize the free energy in the future
and essentially what I'm going to be saying is that if I condition the outcomes yet to be observed
on some plan then accuracy and complexity translate into two things which we will all
immediately recognize basically they're going to translate into an optimal design
and an optimum decision-making so what the story I'm telling here is that self-evidencing just is
a mixture of Bayes optimal design in the way that we go and solicit evidence from the world
evidence for our own existence for our generating model under prior beliefs about the kind of thing
that I am so here in this instance the Bayes optimality is of a decision theoretic sort that
if I choose to do this then the outcomes are going to be close to my prior beliefs about the kind of
outcomes a thing like me will expect to encounter so I'm just going to briefly rehearse that story
by comparing and contrasting conventional RL like approaches to optimality versus
this more general self-evidencing approach and I normally start this lecture
by asking you to imagine that you're a bird of prey and you're hungry and then I ask the audience
normally what are you going to do and they normally say quite correctly I'm going to
search and fly around until I find my prey and then I'm going to eat my prey and in that answer
there is buried an important or several important takes on the kind of functions or
functionals that you would use to try and describe good behavior so we could either assume the
existence of a value function of some states of the world that are contingent upon some
action or control variable u here and if this value function of states of the world existed
then I could optimize the value by selecting the best state action policy pi policy by selecting
the action u for every given state that maximize this value function and this is known as the
Bellman optimality principle however just in saying I'm going to search for my food before I eat it
you're saying something quite profound because searching is a action that resolves uncertainty
and uncertainty is an attribute of a probabilistic or a Bayesian belief and therefore the thing that
you are optimizing can't be a function of unknown states of the world it has to be a function of
beliefs about states of the world so in in the sort of Bayesian interpretation I'm denoting
posterior or approximate posterior beliefs about states of the world by q so what we're saying is
the optimal action depends not upon states of the world but on beliefs about states of the world
and we get this sort of functional form here the other thing that the sort of the owl example
shows is that the order matters it matters whether I first of all search for my prey and then eat it
or try to eat it and then search for it which means that there does not exist an optimal state action
policy but more an optimal sequence of behaviors given what I currently believe about the state
of the world at this point in time and we can articulate that in terms of a path or a time
integral of this quantity which is in fact going to be an expected free energy that Andy and a
number of other people have already mentioned and leading us to an optimization of a time integral
of an energy function which in physics is an action and I'm phrasing it like that because
what we've basically got here is a choice between the bellman optimality principle that would seem
things like optimal control theory or dynamic processing deep reinforcement learning Bayesian
decision theory on one hand or on the other hand we have Hamilton's principle of least action
or stationary actions more strictly that is exemplified by things like active inference
and subsumes things like artificial curiosity and intrinsic motivation that I'm going to cast in
terms of optimal Bayesian design that optimally resolves uncertainty about my high hypotheses
about how my body works and how the world works it also entails sequential policy optimization
and the distinction between observations of a certain world and the partially observed worlds
so here's the basic architecture that one has in mind when invoking and we've seen lots of lovely
examples of this for example Jantani's illustration this morning so you know the basic theme the
embodiment gets into the act here in terms of this sort of reciprocal engagement this circular
causal link between the inside and the outside that Andy was talking about
mathematically what one way in which one can describe this is that you get these outcomes supplied
by the environment and you use these outcomes to maximize a functional of those outcomes and
your beliefs under a journey model about states causing those outcomes to form posterior beliefs
about states of the world out there you then use these to roll out effectively into the future
under a number of potential policies so this is exactly the poise over possible actions or an
action space the action space here is explicitly encoded in terms of sequences of actions or
moves on the world denoted by pi and each pi you can roll out and estimate the expected free energy
which I've here decompose into risk and ambiguity and we'll drill down in that in that in the next
slide at the moment we just need to know now we've got a way of scoring each way forward each one of
these possible action possible policies possible plans can be scored in relation to our beliefs
about the kinds of things that we expect to encounter and the kinds of things that we do
with our bodies and that score can then be used to find the most likely policy with selected action
from that belief about the most likely policy it changes states in the world the world supplies a
new outcome and off we go again in this perception action like cycle that has this planning and
policy selection as an integral part of it and so this is the bit that sort of links the notion of
simplicity and complexity to the more prospective aspects of action and planning so what I've done
here is just rewrite down the evidence lower bound the free energy here in terms of accuracy
and complexity and remember the complexity is the difference between your post areas and your
priors the amount you've had to change your mind which is the energy hungry part of it and the accuracy
here and just by moving the terms around we can reconstitute or recover the the expression
that lends its elbow acronym the evidence lower bound because this quantity is always greater than
zero and here's the expected value has a very very similar form the only difference is that we're
now conditioning upon some possible action or possible sequence of actions and interestingly
what happens now is of course you don't have the observations that would be used to evaluate the
free energy because they're now random variables in the future so you now have to take your expectation
over the prediction of the outcomes in the future given I'm going to do that and hence expected free
energy but the form is the same and I like this because it's really quite revealing so the complexity
becomes risk the accuracy becomes on the negative accuracy becomes ambiguity which means that we're
going to choose those things if we are choosing those things that maximize expected free energy
in the future we are necessarily going to be minimizing our risk and minimizing our ambiguity
and I explain what that means in a second or we can carve up this expected free energy in another
way in terms of the intrinsic and the extrinsic value and this is the intrinsic motivation so we
have basically both the complexity and the intrinsic motivation or the expected complexity
and intrinsic motivation as different ways of carving up this underlying imperative for
self-evidencing but self-evidencing where you're maximizing the expected evidence in the future
given a particular action just to unpack this a little bit for those people in different fields
if I ignore the extrinsic value at the moment and by which I mean I've got some prior beliefs about
the kind of outcomes that I as a creature will expect to encounter if I have no particular
preferences prior preferences then we're just left with this thing here which is the expected
evidence bound and this quantity here is just the difference in my posterior beliefs with and without
observations that I would get if I pursued this policy and of course this is the basis of
the mutual information between the consequences and their causes states here that will be expected
under that particular policy if you do visual search or active sensing active vision then you
may recognize this as the expected base in surprise basically the amount of information the epistemic
affordance attributed or afforded by this particular policy the reduction in uncertainty about states
of affairs in the world out there in the future given observations and evidence for those states
in the future if I committed to that particular policy if I take uncertainty of the various kinds
off the table we're actually going to get going to get back to reinforcement learning or expected
utility but let me just do that carefully let's first of all assume that we've got a simple world
in which we can sense the states of the world so we remove the uncertainty about that's inherent
in the partially observed aspects of certain models so that we magically our outcomes now become
the hidden states of of the world we can see everything in this instance the s's become the
o's and we're just left and the ambiguity goes where there is no more ambiguity in the world we
know exactly what's going on and and that just leaves the risk so what is a risk well it's the
against the KL divergence between what I expect to happen under this policy and my prior preferences
so when I minimize risk what I'm doing is I'm choosing those policies that will lead to states
of the world or their outcomes the a priori I prefer so I'm trying to minimize the degree of
divergence or difference between again and posterior about future outcomes and prize over
future outcomes so I'm very very closely related to complexity not the complexity cost but now
we're talking about the future and if I remove the final kind of uncertainty from the table
what we're left with is effectively and the final kind of uncertainty is that the
the uncertainty due to different actions relative uncertainty just disappears I'm just left with
this thing here which I set up in the first slide as being really a version of expected utility
