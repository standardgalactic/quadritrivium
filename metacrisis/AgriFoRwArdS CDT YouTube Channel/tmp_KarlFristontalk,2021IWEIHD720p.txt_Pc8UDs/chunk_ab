theory where the the expected or extrinsic value is just the expected value or the log
probability of preferred outcomes under what I anticipate to ensue given a particular
well given all policies or actions here so what we've done here is is derive from the
simple notion of self-evidencing the service of garnering evidence for your own existence
as implied by your model your embodied model of the world and carving it up into two different
aspects of Bayesian good behavior first of all I'm going to commit to those behaviors that would
or choices that have the greatest expected value and in statistics that's known as
making Bayes optimal decisions under some loss function but here we're casting
the loss function or the utility function in terms of a prior preference and then
which is adding to that effectively the information gain that ensues from pursuing a particular
policy and together they constitute the expected free energy or the expected evidence afforded
by a particular policy so I'm going to close now just by very quickly taking you through some
embarrassingly simple simulations I mean look at the beautiful simulations this morning
we are nowhere near hand able to scale up to that level of sophistication however I think
they're still illuminating you know in the sense that you know there's a certain simplicity in
you know in what I'm saying even though some of the maths may look a bit complicated
I'm not going to go through this in any detail I just want to show you the
kind of behavior the sort of behavior that is mandated under this optimization of expected
free energy we normally use Markov decision processes sort of discrete state space models
of a world graphically cast in terms of transitions from one state to the next point
in time generating outcomes of the likelihood mapping where the transitions are dependent upon
a policy be usually coded encoding the probability transitions and then we have interesting
well the policy clearly depends on the prior preferences encoded by this preference or
cost function here we also have in many of the simulations the precision of our beliefs
about policies and I just mentioned that because that's the W in John's talk that sort of
tunes the hierarchical depth or the ability to learn versus to teach depending upon you
the confidence the a priori you place in your your inactive policies when interrogating or
querying querying the row the world a couple of hyper parameters about initial states and when we
plug this very generic form for the geratin model into standard
elbow or free energy optimization schemes we get a a series of belief updating
equations that look very much like what the brain does and indeed we know that because of all the
all the simulations and interpretations process theories that we've already
witnessed this morning and in the literature and so you know there are many unknowns here
we just don't we don't know about the state of the world we don't know what we're doing and we
don't know how confident we are in what we're doing and we actually also don't know the parameters of
the of the of the model itself you know the the likelihood mappings and the and the transition
matrices so those can be learned so we just have perception which is resolving uncertainty or
inferring states of the world policy selection so having the right poise if that's correct abuse of
and its philosophical term on the space or possible actions and and also having a kind of second order
belief about our beliefs about those actions in terms of the confidence or the precision
that incidentally looks as if it's encoded by dopamine in the human brains then we have learning
of the unknown structure of the world that would be a particular interesting from the point of view
of the sort of you know the the embodiment part of this equation and then action selection is
selecting an action from my beliefs about what I do and when I simulate that and I'm going to simulate
that by applying those equations to a little mouse who can has to solicit a baited reward
from the right or the left arms it's only very it only has two moves and has to stay
in one or the other arm once committing to one of those arms but it has a choice of using up one
of its moves to go and get an instructional cue that will tell it definitively whether the reward
is there or there so this little rat has the choice of taking a gamble and it could be 50
percent correct and staying there or what spending one of its exploitative moves in an exploratory
sense to go and resolve uncertainty about the world and then go and secure the the cheese and
of course this is set up this sort of two-step paradigm is set up to demonstrate even though
the expected utility of both policies is identical on average that the self-evidencing
creature will clearly go and resolve its uncertainty responding to the epistemic affordance or the
intrinsic motivation go and get the cue and then go and enjoy and exploit its knowledge
by going to the arm that is baited and indeed that's what the agent does but what this simulation
is showing is is a systematic and principled and completely non-deterministic
this is a deterministic optimization of expected free energy which means that
if you actually introduce a predictability and a structure and a consistency to the world which
we've done here just by leaving the reward on the left arm perpetually after the first couple of
moves then the creature the rat will come to learn this and therefore the salience or the novelty
of the uncertainty resolving cue slowly wanes and there is no uncertainty after a while because
the rat knows that the reward is over on the side and therefore the epistemic affordance the
the explorative drives the intrinsic motivation for going to the condition stimulus or the
instructional cue starts to disappear and at some point when the intrinsic and the extrinsic well
well the intrinsic value or motivation falls below the pragmatic or the extrinsic value it'll
just switch to a different kind of behavior and this is incredibly systematic and you see this
in many many simulations and indeed in real world situations where you know your first imperative
is to resolve uncertainty about this particular world this eco niche this body and when you've
done that you start to then turn to your prior preferences because there's no more uncertainty
to hoover up so by move 20 the rat just goes straight there quite happily and will do so
forever if nothing else changes this is the final slide and I've got 30 seconds left so
I put this here just to remind myself about that initial ambivalence about whether to talk about
sort of the emergence of lifelike behavior from first principles or whether to talk about the
sort of the mechanics of planning and deep jota models of my own action it will be nice to
to actually connect the two perspectives and I'm just putting this up as an outstanding challenge
and the frame in which I think that challenge can be met I won't go into this but other than to
if anyone's interested provide a sort of starting point for discussion so here's what we've just
been basically saying we can understand the right kinds of behavior in terms of selecting the right
possible actions and what are the right ones where the other ones that underwrite our self
evidencing that minimize risk and ambiguity and they have this functional form there's another way
of understanding the universe in terms of non-equilibrium steady states that are normally
described in terms of integral fluctuation theorems if you did chemistry or physics and they
have a very similar structure in terms of articulating the probabilistic development of
states and outcomes consequence upon different states in terms of kaon divergences to provide
bound or inequalities underwrite much of statistical thermodynamics and interestingly
you can use the integral fluctuation theorems to derive this if you make one particular assumption
or commitment one small move here and all I'll say about this is it hasn't actually been done yet
but it does involve some probabilistic relationship about between the probability
of a path of action into the future and the ambiguity or the negative entropy of outcomes
given their causes so again linking action and uncertainty in an intimate way which may be the
key for understanding the kinds of systems i.e creatures like us that distinguish ourselves
from things like viruses or other self-organizing systems that don't plan anyway I'll leave the
last word with Einstein everything should be made as simple as possible but not simpler
and all that remains is for me to thank those people whose ideas I've been talking about and
of course thank you for your attention thank you very much indeed I'll stop my sharing that
thank you very much Carl I'm sure the 348 participants if they could they would unmute
and join me in thanking you there's a couple of questions arriving in the discord so before we
get to one or two questions I just like to ask the organizers if they could help connect Professor
Friston with the discord if he's not there already there's a question from Dana Damian
beautiful theory thank you does this theory also explain the emergence of belief in the first place
in an elemental way yes so that would be part of the story that I couldn't tell so
if you just take any system that has self-organized some kind of non-equilibrium steady state so it
persists there are recognizable characteristics and then you partition that system into inside
states that constitute the inside of a system and the environment and then they're enshrouded by
blanket states this is a Markov blanket partition then you can use the necessary properties of
non-equilibrium steady states solving the density dynamics to show that there is an information
geometry where the physical inside states come to parameterize Bayesian or probabilistic beliefs
about the outside states and that licenses then an information geometry which is a geometry of
belief states so if you if you permit me to interpret beliefs as Bayesian or probabilistic
beliefs conditional probability distributions then mathematically you're saying is there
an information geometry there and why I would say yes there is under the condition you've organized
to a non-equilibrium steady state great thank you again more questions arriving there's a question
here in zoom chat from from Andy if minimizing expected free energy delivers simple planning
what delivers deliberate planning that seems different yet related
I think the distinction is a very subtle one and we don't have time to get into it because
it's a really interesting question so notice that I was talking I was just carving this
functional this sort of expected free energy model evidence into two bits that linearly decompose
because we're in long space into sort of an information gain effectively you know a pragmatic
or expected utility term that added that would could be resolved independently
so you could argue well look yeah that's fine but that's not really resolving uncertainty about what
matters to me in relation to my prior preferences yeah so yeah I'm just resolving all uncertainty
irrespective of you know what what what I actually think I should be the kinds of things I'm going to
I'm going to solicit so if Andy's asking that question to ask well when does it get more interesting
one could algorithmically say well you have to move to a sophisticated inference where you
actually have to um in so just resolving uncertainty you have to actually encode what would I believe
if I knew that and what would those new beliefs mean as I work towards my preferences it's not
difficult to do it's basically a probabilistic generalization of the the bellman recursion
um however I'd actually say you don't probably don't need to do that because remember although these
two bits of the the good ways to choose your policies are linearly separable they're linearly
separable in long space which means you're actually multiplying so what you are saying in
probably in terms of probabilities I am choosing those um policies that have the greatest
epistemic affordance given that they afford um outcomes which I prefer so I think there's
actually built in for free a certain sort of um deliberation there in the sense that you know
conditioned upon resolving uncertainty there has to be an aspiration to the goals that are written
into the prior preferences great thank you um we have some time for discussion at the end of the
plenary session so I'm going to uh use my host privileges here and ask you one last question
from discord um how does free energy deal with uh groups of agents and a common belief among the
group will it promote agreeing uh with the group over further exploration of the environment
yes yes it does it promotes um cooperation and generalized synchrony so mathematically you know
that generalized synchrony basically affords a predictability of two agents in a diode or in
fact a multi-agent context so if the whole game is to minimize surprise then the best solution
is to basically um make you exactly like me and make me exactly like you and then our discourse
will be a perfect synchrony we're both mutually predictable and our joint free energy will fall
so it's all about finding harmony in groups um a shared narrative a common ground um and from
that you can spin up all sorts of hermeneutic interpretations and indeed you know the imperatives
for for language in terms of cultural niche construction under this sort of variational
variational frameworks and that would have been a third lecture which I which you can't do everything
in half an hour certainly can't thank you so much professor fristen on behalf of all the
participants here for a for a fascinating and thought-provoking presentation
