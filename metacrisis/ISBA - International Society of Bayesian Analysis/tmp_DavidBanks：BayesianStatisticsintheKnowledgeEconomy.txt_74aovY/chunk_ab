Causal inference, of course, has lots of things, but the most trivial example is can you predict
whether somebody is going to click on an ad as a function of their demographics and whatever
else you know about them, and trust me, Google knows quite a lot about your age, gender, physical
location, marital status, economic status, lots of things.
Time series modeling pops up because, well, go back to the pizza example.
If you type pizza into your browser at three o'clock in the morning, all the pizza places
are closed.
You're not really going to be purchasing a pizza right away.
Time series modeling can help determine what is the potential gain from an ad that is shown
at different times of the day, and then build that into the bidding system so that somebody
typing pizza at six o'clock in the evening is going to be worth more and get a higher
bid than somebody who is typing in pizza at three o'clock in the morning.
Computational advertising also engages with text incentive and analysis, dynamic network
analysis, probabilistic ad contracting, spatial temporal processes, sensor data analysis,
and many other topics.
Text incentive and analysis I think is fairly obvious.
If you're advertising for McDonald's hamburgers, you might find a website that has a lot of
keywords like hamburgers and so forth on it, but if it's a pizza website, then sentiment
analysis will probably tell you that that's not a good place to put ads for McDonald's.
Dynamic network analysis pops up in lots of ways.
I'll refer you to a JASA paper that Mike West, nine, a couple of people wrote that looks
at how that applies in the context of computational advertising.
If ad contracting is a new field, one might imagine that there's a contract with a demand
side platform to show an ad for, I don't know, Donald Trump to 200,000 male eyeballs in California
between the ages of 18 and 80 before the end of the year.
There's another contract that gets written to show Joe Biden campaign ads to 100,000
males in Southern California between the ages of 18 and 65 before the end of July.
The same people could fulfill, some of the same people could fulfill both of those ads,
and so you want to be able to figure out which ads to show to which people, and since you
pay a forfeit, if you come to the end of the contract and you've not completely fulfilled
the contract, you have to return some of the money to your advertiser, then there becomes
this weird dynamic programming problem in which you're trying to assign people to ad
campaigns in order to maximize your expected revenue.
There are batch of ethical issues that get tied up in that, but that's something to consider.
Sensor data, spatial type of world stuff, all of that arises in ways that we can probably
talk about, but I don't want to spend too much time on that right now.
I do want to note that these problems are an amazing opportunity for academic research.
I have former students who are now working at Google, Anthropic, LinkedIn, Amazon, and
they tell me that they don't have time to do academic research.
They run from problem to problem, putting out fires, and then move on to the next thing.
They are also not really encouraged to publish because it can eliminate a competitive advantage.
Academics have the time to think through a problem from a principal standpoint and publish
on it, so I encourage academics as well as practitioners to get involved in this topic.
Recommender systems are, I think, an especially exciting component of computational advertising.
They determine all sorts of things.
They help the demand side platforms decide which ads to bid upon for display.
They make movie music and dating recommendations, and the main methods are Bayesian.
The 2008 paper by Selikudinov, which opened up a huge territory that many people have
been publishing on since, and a key aspect of computational advertising is that it's
really much more like data engineering than it is like data science.
It turns out that recommending a science fiction book is different from recommending a murder
mystery, and so Amazon doesn't use one of the main Bayesian methods.
They do a hybrid of multiple methods, and trying to automate methodologies that do the
fine tuning is, I think, an interesting research challenge.
The two starting points for recommender systems, theoretically, are collaborative filtering
and content-based filtering.
Both of them start with a very sparse ratings matrix R, and you can imagine the rows of
the matrix R users, and the columns are items, and the entries in the matrix are ratings
that the users assign to an item, and that's probably a little misleading because the rating
may not be a numerical rating that somebody gives for a movie or a restaurant.
The rating could be how long you sat at a website looking at a movie or a book before
you moved on to something else.
So the concept of a ratings matrix is actually pretty general.
If you're doing collaborative filtering, the recommender system algorithm looks for
people who like the same movies that you liked and also like the movies that you haven't
rated or haven't seen, and so it looks for people who are like you and then infers your
taste.
Content-based filtering makes your recommendation based on the features of an item, so it may
be that the movie is an action movie, or maybe the movie is a drama, and so it will try and
learn what are the features of the movies, and it will learn what the features an individual
particularly likes.
Some individuals like comedies, some individuals follow directors, some individuals follow
actors, and so it's going to try and learn what things you like, what content tags you
respond to.
And one area that I think is especially fun are active recommendation systems.
If somebody asks you to recommend a book or a movie, a human being asks them, what sort
of books do you like?
What sort of movies do you like?
Tell me some of the last five movies that you really enjoyed, and based on that information,
the human being can make a pretty good recommendation, and I can imagine that someday in the future
I will log into Amazon and they will say, David, no matter what, we're going to make
book recommendations for you, but if you let us ask you five questions, we'll do a much
better job of recommending books that you actually like.
And so that's pretty cool.
Amazon in principle has a prior distribution of the movie, yes.
I spoke, I didn't hear who it was.
Maybe it was a misfire.
Okay.
In principle, Amazon has a prior over repurchasing any item that they have on offer, and they
would like to ask the questions that will do the best job of narrowing down their recommendations.
And ideally, they would ask questions that have a 50-50 chance of me saying yes or no
according to their prior.
But there are difficulties with that because of their complexity constraints and there's
some non-standard feature selection that I'd like to unpack a little bit.
And in principle, one might have a personalized proximity matrix for books or movies or music
because things that are near for me in book space may not be near for Antoinette in book
space.
We have different tastes in things in response to different types of things.
And I will point out that the new large language models could be transformational if hooked
up to this type of problem.
So in principle, Amazon could calculate a distribution over repurchasing any book.
And this nearness should be complex and personally tailored because some people follow genres,
others follow the New York Times book review section, other people do different things.
Data stations know about non-Euclidean spaces and we can use isomap or paramap as tools
to try and map these spaces.
And now Amazon needs to decide which questions it wants to ask me.
And unfortunately, the question that is most likely to have sort of a 50-50 chance of me
saying yes or no is something that I cannot think about.
It's on the whole, do I like these 100 books more than I like this other list of 150 books,
which I cannot cognitively process.
So Amazon is going to need to put a complexity penalty on the questions that it asks.
And defining such a penalty is a challenge.
It's a kind of statistical regularization.
My sense is that you cannot ask more than three conditionals.
Do you like murder mysteries written by women before 1950?
That's a question that I can probably answer.
I got the Christie, Dorothy Sayers, Niall Marsh.
But if I add more conditionals on that, then it probably becomes very difficult for most
people to process that question.
So ideally Amazon wants to have a 50-50 chance of me answering yes or no, but there's a hidden
optimization problem here.
It may be the first question in the tree that Amazon asked has a pretty close to a 50-50
split.
But depending on the answer I give, the subsequent questions might be mostly 90-10 splits, which
are not very informative.
So one wants to optimize the question tree.
You want to have lots of near 50-50 questions in the follow-ups.
So maybe an initial question of the 60-40 split is better because it has lots of 60-40
questions underneath it in the tree.
One wants to maximize the average expected learning rate in the series of questions that
Amazon asks.
I am not aware of any previous literature that addresses these kinds of problems, but
Bayseans think a lot about optimal learning, and this seems like a nice area for us to
become involved with.
I'm going to move on now and talk about autonomous vehicles.
And I think this is transformational technology that has got huge potential.
They're going to revolutionize how goods are moved.
They're going to have significant impacts on insurance, just-in-time manufacturer, logistics.
I think they're going to spawn new kinds of businesses and support multimodal integration
of transportation.
And in terms of people, it should improve safety, reduce pollution, solve congestion,
and make our lives better.
Here are some statistics.
With the U.S. Department of Transportation, in 2018, Americans drove 3.4 trillion miles.
According to the CDC, the risk of dying in a vehicle injury are 1 in 77.
That is also the risk of dying from firearms, which includes suicide, and it's comparable
to falls and suicide and other things.
According to the Environmental Protection Agency, motor vehicles account for 75 percent
of carbon monoxide pollution, one-third of all air pollution, and 27 percent of greenhouse
gases in the United States.
And the U.S. Census Bureau says that the average commute time is 52.2 minutes today,
basically almost an hour in the car.
And autonomous vehicles can improve all of those numbers.
The potential gains from autonomous vehicles include safety, AI driving systems are never
distracted or impatient.
They have much better sensors than we do.
And so driving at night, they'll see the black cow that is wandered under the road because
they have better sensors.
In terms of the environment, better safety means that we can have lighter vehicles.
If you get safety good enough, then you can build vehicle bodies out of canvas and we
would no longer have to carry around a ton and a half of steel to protect us from other
bad drivers.
Instead, we can have much more fuel-efficient lightweight cars that are just as safe.
In order to achieve that safety, you want to have joint control and you'd like to have
electric vehicles for the environment.
But if all the vehicles are sort of under joint control, networked, then you really almost
never need to break the car.
And breaking, of course, is another inefficient use of fuel.
If cars are under joint control, congestion is solved.
One can get seven times as many vehicles on the road when all the cars know what each
other is doing and they don't have to space out for safety.
In terms of quality of life, AVs would be tremendous.
Your commute time would be time to work or time to take a nap or time to read a book.
Independence would matter a lot.
Seniors move into assisted living facilities and surrender their independence when they're
no longer able to drive to the grocery store and back home.
And similarly, children could be dropped off at school by an AV.
And the economic efficiency of moving goods and people would improve.
That's going to lower costs and create new economic markets.
The 2050 problem refers to the fact that in about 26 years, the world population will
reach its maximum, which is going to be between 9.8 and 10.2 billion people.
We are currently at 8.1 billion, and the carrying capacity of the planet is around 1 billion.
So we've overshot the carrying capacity.
If we want to have anything like a middle-class lifestyle, the population is going to have
to come down.
People warming is harder to forecast than demography, but the climate scientists say
that in 2050, parts of North Africa, the Middle East, India, and South Asia will regularly
experience summer temperatures between 120 and 125 degrees.
That is not survivable unless you have lots of potable water and high technology, and
most of the countries in those regions don't have that.
Autonomous vehicles are one of the few technologies on the horizon that actually has the potential
to make meaningful reduction in carbon emissions while maintaining relatively high standards
of living, and so I think that we need to be paying a lot of attention to autonomous
vehicles.
I'm getting to the point where Bayesian statistics are involved, but I'm not quite there yet,
so be patient and bear with me.
There are many legitimate concerns that people have about moving to autonomous vehicles.
There are some people who declare that people will not give up control to an AV.
They just won't trust them.
There will be a mixed fleet period that will be suboptimal and require continual risk analysis
