Okay, so let me just introduce Amit, he's a affiliated researcher at the Active Inference
Lab and he's building a tech and product at Paper Plane.
His presentation is entitled Evolution of Latent Model for Collective Cognition.
Hello, introduction. I'm Amit and I've been studying some sort of inference within
research as well as with how it applies in real life. So I'm just going to explain
how I've come across these inferences and how I've built my understanding of the latent model
that I'll be describing later. So okay, so this is just an intro, let's start with.
And there are three levels to it, layered on top of each other, first one being the knowledge
base, where it's just a random information within no context and the thing of that sort,
which is being prepared by individuals, agents acting in the latent space. So from there,
I built on to something called niche. A niche is more of a community level where people have
different sort of ordinances just to communicate and a latent model that transfers over time within
the latent space it has occupied. So these are three outliners of my talk and I'll be
delving deep into each one with focus on the top one which negotiates itself through time. So yeah.
All right, so let me start with the knowledge base first. A good analogy that I have with
knowledge base is a fractured city. So on the right, you can just see some sort of thermographs
of a organized complex society that already functions to some extent, but it's not really
useful, just a compilation of facts. It's never useful. So one other thing that has to be
incorporated within models and within frameworks that we are defining through time is where and
how those things come into function, more of the metaphysics of the ontology that is being defined.
And so how do we come about overcoming these certain like gaps in transitions from one
point to another? And so I'm just introducing a concept of translation. What happens is we're
just translating from one embedding to another one and that's done through the ease of translation
between two different points. So first of all, we started T0 with none of the agents interacting
and none of it making any sense. So there's no context identified. There's no interaction between
the individuals in the collective. So we start with different embeddings of it and just start
traversing and connecting the dots between whatever exists. All right, now another concept has become
and resonated more towards interacting agents and how they feel towards each other and things
like that. So this function that you see on the left bottom is just how agent navigates
metaspace given its own intrinsic biases or intrinsic motivations given in a volume and
that's how we define a probability density of a different volume to start with. So after
any iterations and being any arbitrary number, we will find some sort of preferences that an agent
really converges down to. So it won't be just some random initializations that will start with
but rather more of the intrinsic initial variables that an agent has and
producting that with the traversal volume it has, it will come along and form those
ghettos in the city as it's more analogous to that kind of analogy. So yeah, now moving on.
Well, it's obviously really good to find some sort of like interest or a niche within
a collective but it's not never enough until you find new affordances and something to build on
top of them. So right now what I'm trying to do is some sort of social experiments that I've been
trying with a few of the market spaces, one of them being an empty market space where I'm trying to
find collaborative indexes between different artists and how they reach along their everyday
day abouts and how do they communicate and build new affordances on top of what they already have. So
one thing that I really admired about that approach is how they build and really communicate
within their own niche that they have built over time. So it's not just them expressing the thoughts
and language, it's more about abstract ideas that they have in common with art and stuff like that.
So this being one of the vertical that I have tried to work on and build a recommender engine
about it, second one being the practicing medical physicians and this being a completely
polar opposite in terms of creativity when you compare that to the artists lane that I had just
worked on. This doesn't provide me much of the wiggle room but really explains and deals well
with how medical physicians navigate through technical niches and stuff like that. So
that's something I'm trying to really go through with these social experiments.
So there are certain assumptions and certain metrics that I'm going about just to measure
these two on same scale. First of all being path and path is nothing but just a communication
and knowledge transfer between two different nodes. So if you just have A and B traversal,
that is from A to B, you'll just find a likelihood of that traversal and multiply
indoor product with what we already had in the intrinsic assumptions section. So
we also have within this formula applied the likelihood as well as the intrinsic motivation
that the agent already has and that's how I'm going about molding this stuff. So once you
have reached a niche, all of the game turns towards how you really build new affordances on
top of it and it really turns really in line with your interests and how you actually communicate
with the people in your niche. So this is just a peak of the first two that I described,
the knowledge base and the niche. I'll just try to share the latent one. Unfortunately,
I don't have that in this PPT format. So I'll just switch it over here. What's the origin
of a latent model? This being the most time I've spent on any concept in my research,
it really is one of the things that really excites me and really inspires me to build multi
task models on top of what we already have. So it's usually inspired by the neuroplasticity
of brain and how it adapts to new tasks depending on the latent structure of the previous tasks
that it has already learned. So it's more about negotiating uncertainty with the certainty it
builds over with the latent modeling and stuff like that. So it's just first of all,
the task really becomes estimating those meta parameters and then transitioning
your task according to those meta parameters. So I'll just go about explaining
how I actually do that. All right. So another assumption that I had was your mind really
does not really form or does not really intake the whole paradigm of the whole world and the
whole contrast of it. It rather forms a very narrow model of the world and goes on about
forming inferences on top of it. So what I did was found a few tasks that I could represent in
coordinate space and laid them down on a different simulator and observed how they interact with
really forming a lot of those things. So the moving gif that you see on the bottom is just
embedding of different tasks. And as the time goes on, you'll just find different
intersections between those tasks. And it's really evident that learning a task really gives you some
sort of grasp on another task if you have some sort of coordinate space in common and really goes on
about doing that. So yeah. All right. How does that learning really happen? Once you have a latent
space of tasks and observations that you have, you need that to be continuous and differentiable
just for the traversal to occur. And that's really, really feasible through the Black Label
divergence that you can really form those kind of structures and a world model that's you
locally, linearly and interpolating towards different sort of different tasks. So it's not
just you forming those latent models, that's enough. But traversing between the latent model
is also something that's really necessary. One of the examples and one of the most intriguing
factors that I discovered about human intelligence is how it negotiates time with knowledge. So
metaphor and implicit knowledge transfer mechanisms, like just explaining how
things take time through metaphors like just wrong moves and built in a day, just describes
the same latent meaning over time to different civilizations, to different people and maybe
to different languages as well. So it's really that sort of elasticity that I'm looking for in
machine learning models as well so that we can just approximate some sort of meta space of tasks
and interpolate between those tasks and then just use a generative model that generates
parameters from the meta space parameters that we had already constructed. So it's just a linear
interpolation between the meta parameters and then a generation process that generates new
parameters based on the previous meta parameters that we had. Okay, so this is just one of the
implementations that I had tried for a similar model. It did not have really that sort of
implication that I expected but found myself really engrossed in it. So as I described earlier,
just finding a linear interpolation between your meta space, the image that you see
here, yeah, on the mid-bottom is just a local interpolation between the meta space and how
you generate from those meta parameters to the actual ones that really require you to
solve the task is done through a generative modeling process that's really familiar to
machine learning folks through VAEs and through generative processes in cognitive sciences and
in different domains as well. So once you've done that, it's just about traversing that space
and how you build on top of those things. So that's being done through a lot of
extrapolation in higher dimensions but already that's nested in interpolation that we did on
the meta space that was described and well established within the locally linear regions
that you can see on the bottom bed. So yeah, so that's how I'm going about describing these latent
processes and how these latent processes are really different from the first two inferential
processes that I described first one being knowledge base and second being a niche. So
in knowledge base, you do not have any sort of context nor did you have any sort of
inference based on the facts you had in the knowledge base. So it was just a collection
of facts rather a map and what a niche provides is you finding a weird and a compass to the map
rather so that your intrinsic alignments need you to those verticals and those verticals when
established really form a higher level of abstractions in forms of implicit knowledge.
That implicit knowledge could be through metaphors that knowledge could be through
more of those implicit cognitive mechanisms and these cognitive mechanisms are not really
context dependent so what a mind does is forms a generative model from those
latent abstractions just to adapt to your particular task and that's how a learning
machine maybe could be constructed it's just one of the proposal and rather a framework that I'm
presenting it's not something that I have tried on particular tasks. The reason being if you want
to build a general enough model for machine learning you wouldn't really make a task specific or
really test it on really a couple of tasks or the end tasks that you can really try on
yourself so the modeling process here is more of a framework and the generative process is the
observer so yeah that's all I've tried doing with the latent part of the model now circling back
to what I had already introduced first through my social experiments and how those things imply
in the latent context or the latent modeling side of things. Since we have formed verticals with
artists or doctors we can't really find any intersection between those until
those affordances or affordances at that level are constructed so once you go to type into one
vertical you won't really find any bridges that really connected with another one therefore we
need some sort of latent abstraction some sort of latent embedding between those two verticals
that is common to both and that's how they negotiate the information between those so
first of all we do not have any knowledge structure from the knowledge base we formed
a very niche vertical knowledge structure in the second phase and now just to negotiate
between the two I think it becomes a sort of necessity to form high-level abstractions like
language like culture and a lot of high-level abstractions that helps navigating the agent
or agents in two different verticals just to negotiate different entities or just to
negotiate different ideas yeah I think that's all I have for this
yeah I'll just test it out
okay all right so that's all for the experiments as well as the theory that had
underpinning to the experiments but as time goes on I'll just find out how will they align with
what I have just because of the fact that has some sort of theory and math behind my intuition
does not really imply how will it translate into the real life society and and the complex
dynamics of of these niches like artists and medical physicians and stuff like that so
what I'll do is keep on expanding on those niches and while in those niches maybe try to form some
sort of latent model that helps them negotiate and if if that's true I think the research project
and the research initiative that I'm just trying to confer and really build on top of is true as
well so it's not just a paper or just a thought process that I have in mind it's more of a proof
of concept that I want from the real world so establishing some sort of feedback through
research and development that I've come through to this standstill I'll just be trying to bridge
these to the development and research as time goes on and find out how my feedback loop performs
and really build on top of what I already have so yeah thank you for listening I think I'm done with
my presentation sorry hi thanks for your talk I'm not sure if Mahal is here to to to allow
you to any questions but but um yeah I couldn't follow all the formalism is I'm not too great at it
but um it definitely seemed very interesting and I agree that I share your interest in this whole
question of the origin of a spaces and its relation to transfer and earning um I'm very curious how it
exactly maps onto the sorry the um vanilla active infant stuff because the so so the
metadata space sounded really interesting and I'm curious if you've explored how it exactly
maps on to the formalism in active inference for how higher levels um contextualize the lower
level beliefs or models um is it like the same or are there interesting nuances to maybe explore
would love to hear that yeah yeah sure so the way I've explored active inferences through
a surprising minimization most of the time so what happens is if you don't really know how
the world works everything is a surprise for you so finding some sort of a meta space
is a good start just to build your theories on top of what you already know from the previous tasks
is is a good way to start from so I think that's where a bit of the component from active inference
comes in which is just uh like minimizing your surprise from a random set of tasks or random
set set of actions you can do to uh confining yourself to those uh meta space parameters
and so would you say that the uh what would be the order of constructing models like for example
you have uh uh uh incomplete incomplete or imprecise models at the at one level and you
create a meta latent space and that will also facilitate the exploration of the models below
right so you have this you know flexibility you just go to the level at which um the surprise
of minimization or range of minimization would be most feasible I would assume and then because of
the meta or the lower higher relation you get this transfer learning for free I would assume
yeah I think so uh it's more about the feedback that you get from the environment that that matters
so once you're traversing those layers as you described um now I don't think uh settling on a
layer that has zero uh deviance from expectations is a good choice no no other way around right so
yeah exactly because the layer where the um the gains information would be most optimal not just
the gain of information or learning in uh in one level but also taking into account the degree to
which learning at one level would transfer to other levels right yeah yeah that's cool that's
cool I mean that's what I'm trying to do as well like it's not just about uh the layers that I'm
taking feedback in it's more of the um teamwork and the model that I've described uh that I'm trying
to take feedback from um so it's it's not really a concrete model up till this point as I would
describe it so uh what this needs is more of a feedback from a real society so it's not really
digging deep into academia and really finding out uh how value theory theories perform and
how it goes down it's more about how it really really integrates with the real world that's why
I transitioned a bit towards um uh building my products and and a bit towards engineering side
just to come complete the feedback loop wait but but when you say real world do you mean like
brains or do you mean you want to apply this in machine learning for actual data analysis
like actual systems okay I mean it could be applied to machine learning systems for sure but uh
uh like but since studying a bit of neuroscience and getting into active inference I have like
sort of uh become inclined towards studying natural intelligence and finding out how it
translates to machine intelligence so uh in in in this way I'm just trying to uh find out how the
complex societies uh function uh between these two niches and maybe get some sort of valuable
feedback from those and then build on uh from there okay all right cool thank you okay thanks so much
