So, now I'm going to introduce
the next speaker. I encourage Jonas and George to come on to the stage. So, our next two speakers,
as I just said, are Jonas, who is a Jonas Magel, who is a PhD student in Muir Sciences at McKell
University, and George Dean, which is a PhD student in the School of Philosophy at the University of
Edinburgh. So, without further ado, I'll let them start their talk. Yeah, thank you for those great
talks up to this point, and thank you for having us at this conference, both George and me. George,
in fact, just finished his PhD, so congrats on that, George. That's been a while now.
And let's dive right into our talk. So, before we start, maybe we're saying that this is work with
a bit of a bigger team. Last Sunday, Smith, Christopher Timmerman, Robin Carteris, Maxwell
Ramsted, and Michael Liftschitz are all on the team and have all inspired this work with us.
So, shout out to all of them. All right, so in our talk, we will talk a bit about the phenomenology
of self and agency and think about how active inference can allow us to model these things,
though we will push it a bit into the spiritual domain of things. So, so far we've talked a lot
about self and agency and active inference model for like kind of individuals and social dynamics,
but under certain, in certain kind of spiritual or paranormal states, people report things of
feeling somewhat of connection to the universe or the world, or more interestingly, feel the
presence of something like a god who's speaking through people or the presence of elves or ended
like other entities or spirits. And most particularly, this phenomena are prevalent when
people take rather high doses of psychedelic substances, but also under meditation and other
kind of collective rituals and charismatic Christian prayer, these things are also prevalent.
And so, what we try to do is to take the active inference framework to get a bit of a grip of
how these phenomenologies can come about and to kind of be able to give a bit more of a kind of
theoretical footing, we will focus on psychedelics as the cause of these experiences in particular.
All right, just to give it a bit of a sense, what we talk about when we say that there are these
entity encounters, these things are strikingly human-like and have all the qualities that humans
have. So, when people report this data, I think people reporting entity encounters under the,
or induced by DMT, these entities that are encountered are often conscious, intelligent,
benevolent and sacred. Furthermore, the qualities by which these entities are encountered,
that's by kind of vision, hearing, touch, taste, smell. So, all these things that are kind of common
for human interaction, but there's also an extra sense, the extra sensory kind of level by which
we interact with these agents. And the last thing is these encounters are incredibly meaningful
and some have argued that a big therapeutic effect lays in the phenomenology of the psychedelic
experiences, including these entity encounters. So, therefore, we deem it valuable to understand
what is happening in these moments. We've given this talk before, and this is an ongoing project,
and the novel thing for the talk today is that we have a rich computational framework to express
our thoughts. And the way how we kind of go through this talk is that we have these entity
encounters that you want to explain, and we kind of build it up from the bottom. So, we start talking
about allostatic control and kind of some minimal itself. So, how do we usually model and monitor
our own physiological kind of a system to then think about how do we model kind of interpersonal
dynamics, and that's the first and first in paper, it's mentioned in many talks so far,
and building up from personal self to kind of interpersonal selfhood.
Up to the Rebus model, and the Rebus model basically outlines a breakdown of this hierarchical
structure. And once we have the Rebus model established, we will look into a computational
model to think about if the computational structure fails in certain dynamics, what will
be the implications for the larger system at all. And our goal is then to say that that can
wrap up and kind of link back to these entity experiences. We assume quite a lot of background
knowledge for this because we don't have time to go through all the modeling, but here's a very
quick summary based on a paper by Lars Sondred-Smith, and now talking about metacognition. And this
basically is a summary of much of the progress that this community has done on active inference,
especially on discrete time-space modeling. First is the idea that systems use some form of
inference to understand the world. Then we kind of build it up and say that this inference process
can be guided by a precision parameter. And then we build it up once more to say this inference
is embedded in an action framework where kind of the agent can act to change its own in the world
states and from moment to moment to moment. And at that stage kind of we have this first
active inference because like active as like comes from the action part in this model.
So this is kind of the basics of the model that allows us to kind of interact with the world
while still doing inference. There's a whole lot to understand in these diagrams and I'm
quite sad that we don't have the time to go into those, but I can highly recommend the
Sandwich method out paper to kind of get an introduction to these things. Where we start is
we start on this last panel, on this very panel, which is basically the hidden states or the
multimodal percents, which is about what am I perceiving? And that includes different levels
of perception, the introceptive, the proprioceptive, and the extraceptive sense. And then
we have the actions which are across time with pi here, which is the question what shall I do
given these percents across time. And now the cool thing that Maxwell has done a lot of work on
is that these things can be nested in one another. So we can just put another level on top that
basically monitors the performance on the slower level. So while we have these multimodal percents
on a very rudimentary level, we can have a next higher level which is the
some form of minimal self, some form of tracking of what kind of sense we have, what kind of sense
we percepts are going on, and how well am I as a system doing in kind of monitoring these different
sense parameters. And that's what we then call the deep generative model of mental action or
hidden mental states. And that's where Lars has done a lot of work to say this can answer
question of what am I paying attention to, what am I trying to pay attention to.
And then justice on the lower level, on this higher level that can kind of unfold over time.
So that basically means that I'm able to kind of monitor my performance on the sense level across
time. And so why this is pretty interesting is that this gives us like a first complete model
of a system to control itself. And that basically is what in the cognitive sciences often referred
to as allostatic control. And because this is the foundation of what our more complex model will
build on, we will spend the next few slides on diving in depth into allostasis really. And yeah,
over to you, George. Great. Yeah, thank you, Jonas. So I'm going to kind of give a
kind of conceptual overview or way of thinking about how to understand the
pre-reflective or minimal sense of self. And I've described it as an allostatic control model,
allostasis being defined as, it's not to work by Peter Sterling on this concept, but basically
predictive regulation. So rather than allostasis being confined to closed loop control in the
present, allostasis is anticipating this allostatic outcomes and acting to avoid them before they
arise. So yeah, we can think about active inference as a formalization of this idea,
so with action being a process of inference and organisms inferring actions, which
have the least expected free energy or the most self evidencing. So basically those actions,
which are conducive to remaining in a species specific window of viability or
phenotype congruence states are given the kind of organism you are, you have kind of
phylogenetically endowed expectations of the kind of states you should be in.
Yes, expected free energy can be decomposed into epistemic and pragmatic value.
So this is the idea of learning about the environment while realizing prior preferences,
which could be either genetically endowed, or could be honestly genetically endowed, or
like Natalie's talk earlier, could start thinking about how does culture shape prior preferences.
So yeah, the starting point here is that the notion of self is implicit in the active inference
framework. In one sense, because action itself involves systematic misrepresentation, so representing
being in another state than to what you're currently in. So attenuating, so you can think of
physiological attenuation, so you're attenuating evidence that my arm isn't moving in order to
bring about a desired movement where the higher precision is afforded to expected consequences
of the action. So to kind of unpack this a little bit conceptually, yeah, Yanis, could you flip the
slide? So this is kind of, it draws upon lots of different areas of literature and kind of past
accounts, but on the one hand, you've got this notion of control, or the system inferring itself
as an endogenous cause. So that's the story that Jacob Hoey and John Michael have told,
this was anybody should have a self. And then also these stories about sense of agency,
the sort of correspondence of the expected consequences of actions and the actual consequences,
the actual outcomes, that being associated with a sense of agency. I'll talk more about that in
a second. And then on the other side, these grounding sense of self in the body, so this stuff,
Anno Seth's done a lot on, and other people, these are fellow members relating self-modeling
on the sense of self to interception. So the central claim here about the, yeah,
my central claim is the sense of self originates from inference about endogenous control of
self-evidencing outcomes, where self-evidencing can be understood in terms of staying within a
species specific window of viability. Yeah, can you go on? So yeah, so just to kind of unpack
this notion of control. So this is an example taken from Hoey and Michael, but if you imagine a very
simple creature, I mean, probably much more simple than this sea slug. So this creature
knows this, the world periodically goes back, and it doesn't know why. And it's kind of,
you know, really basic question is like, is this due to exogenous causes? Is this something in the
world, or am I causing this? And the idea is that once it knows it's the correspondence between,
oh, the world goes back, black periodically, when, you know, this certain motor command occurs,
then the system starts to then, okay, so this motor command is correlated with
what's going black, it's probably due to an endogenous rather than exogenous cause.
And so it's attributed to self rather than others. It's kind of really basic,
yeah, basic distinction of self being related to endogenous causes, and other being related to
exogenous causes, and the predictability of the sensory consequences of actions being the key thing
there. Yeah, do you want to, can you flip? Yeah, so one way of illustrating this, imagining being in a
wearing a virtual reality headset, if the, you know, if your virtual hands, the movement of your
virtual hands corresponds to the actual movements that your arms are doing, you'll have a feeling of
agency and authorship over the, over the outcomes of the action, over what you're seeing. But if
they're, you know, behaving randomly, or they don't correspond, it doesn't feel like you and it lacks
this feeling of agency and authorship. So this is the idea here that the system is constantly
tracking its control of sensations. And, you know, this dates back to
the literature on corollary discharges and efference copies, and thinking about like refining motor
commands. Yeah, and so we're already used to start thinking about how this is related to
sensory attenuation. So the physiological sensory attenuation being attenuating where my arm currently
is, in favor of where I'm expecting it to be, and also perceptual sensory attenuation. So
canceling out the self-generated consequences of actions.
And yeah, as we'll see, failure of sensory attenuation
is, underlies quite a lot of the phenomena we're going to talk about.
Yeah. So yeah, this is where it connects to the body. So the idea is that the system's not kind
of dispassionately just inferring, you know, what's, how can I control sensation via action, but it's
really, you know, there are particular things that needs to control. And that is kind of most
fundamentally perhaps keeping essential variables or states of the body within certain bounds. So
perhaps like staying a certain body temperature, because these are the things which are most
fundamental to continued existence and survival. So they're the things that in terms of prior
preferences, they're going to be further genetically endowed, high precision on prior
preferences for certain intercepted states. And we can also think about, you know, so
how this extends over time. So with a deeper temporal model with, you know, increasingly
abstract outcomes on longer time scales. Yeah, the richer and deeper the model, the more the
system's going to have prior preferences subtending longer time scales. And we could also think about
how those are acquired over ontogeny or something like social status or
things like that. More abstract things going further away from the kind of states of the body,
but are also important for continued existence. So yeah, so the essential idea there is that the
this inference about our static control, so this permeates like
the perception of the world such that things appear salient according to, you know, your
inference about static control. And this also relates to, so if you're hungry, you know, food
appears salient, but it can be also controlling prior preferences on longer time scales. This
also relates to tracking, so tracking prediction error reduction over longer time scales. So
thinking about affect and
yeah, kind of realizing prior preferences on those longer time scales. The system also wants
to keep track of that. So think about kind of most invariant aspects of the self model.
Yeah, do you want to move on with this?
So this is the story about
inferring states of oneself, but the moment we don't have any kind of story about inferring
states of other people. So at the moment, we've just talked about the minimal self and you might
imagine that, you know, quite simple creatures have this kind of minimal self model, but they lack
representing the self as a self and they lack in representing other agents as other agents.
So the idea here is that, you know, well, and that's how static control model allows
the system to infer states of itself. You can also use this self model in order to,
you know, the goal here being to better predict the sensor on
various time scales. You can use the same self model to predict the behavior of others. So
you can think about, so kind of the essential story here is what state would I be in in order
to perform that behavior? So if you see someone going to the fridge, what state would I be in to
perform that behavior? And then you infer that the state is, you know, perhaps they're hungry,
but then you can think about more complex things or have something like facial expressions being
quite an interesting example. So in inferring someone else's effective state, my way of doing
that is to simulate their face expression. So yeah, if you want to do slide, Eunice.
So yeah, this goes back to like simulation theory connects to early literature. The idea is
using your own self model, but kind of environmental action, bending it into states of
the agent you're observing. So if you take, if you're seeing someone putting a disgusted facial
expression, micro muscle movements to mimic that facial expression to then infer what state would
I have to be in in order to display this behavior, the idea being you'd have to be in, you know,
and then on that basis infer, okay, this other agent is experiencing disgust.
Yeah, so this connects to what we heard a little bit about earlier. So this do it for one idea. So
using your own generative model and either attenuating your kind of self model when attending to
others. So another speaker, for instance, and then
or attenuating the sensory evidence in order to speak. So then focusing back on the cells.
So flipping between modeling yourself and others by kind of changing the
precision on the, on the self model. So this is like what the other static control or minimal
self model idea, you know, some talk about the, this in terms of a computational model in a second,
and then getting this extra kind of matter cognitive layer, which is attributing is this
self or other. Yeah, do you want to do slightly on us?
Yeah, I'll hand it over to Janis now.
I need to unmute myself. All right. Thank you, George. So what I will try to do now in the last bit of
this presentation, I will talk all the rich work that George has just presented and in fact, George
has been working on for many years by now, and try to put it in this in a computational language.
And I'll pick up again where I left off last time. So I introduced this basic kind of generative
model of this active influence scheme. And then we said that we can embed this
in the next higher level that represents this minimal self and that that extends over time.
And now the last point that George made is to say, if we model other agents using the same
generative model, we need a higher level that is able to distinguish what of those states that
we infer belong to ourself, which of those belong to other agents that we interact with.
