So, yeah, I want to talk a little bit about a phenomenon that I call reification in mathematical
modeling.
Oh, what does that sound mean?
Nothing, Mel, just keep going, it's the chat.
Just keep going.
Oh, Jesus Christ.
I can't make that stop, can I?
Yes, you can.
If you go back to the app and you click on the chat, there is a little cogwheel at the
top left and you just have to remove the notifications.
Did you find the cogwheel, Mel?
Yes, in theory.
The one that's on the top left in the chat function?
Oh, there.
It should say.
Okay, I think I did it.
Okay, cool.
Sorry about that.
So, I want to talk about a phenomenon that I call reification in mathematical modeling.
And I want to extend my discussion of that to the case of machine learning, which I consider
to be a use case of mathematical modeling.
And basically what that is, is what I've called mistaking the math for the territory.
And I want to discuss some of the potential harms that can come from that sort of reification,
especially in the domain of machine learning.
So I'm going to cover a couple of preliminaries on scientific modeling.
I'm going to describe machine learning as a variety of scientific modeling and I'm going
to then characterize what I mean by reification in mathematical modeling, give three examples
of that and then discuss briefly how it applies to actually inference the free energy.
So the basics of scientific modeling, models are taken to be abstract structures which
enable investigation into real world systems called target system.
And the analogy here is that model is to target as map is to territory in the sort of Korzybsk
in the depiction of the map of the territory, right?
Modeling aims to give us predictive, interventional or explanatory purchase or leverage.
It aims to give us grip on some target phenomenon nature.
These being entities, forces, processes or causal relationships in the world that we're
aiming to have predictive or causal or explanatory purchase over, right?
Now on the sort of contemporary mainstream account of scientific modeling, models are
taken to be interpreted structure.
So what a model consists in is a physical, mathematical or computational structure plus
a scientist's interpretation or construal of that structure which relates it to target
system.
This is perhaps the easiest seen in the case of model transfer.
So there's a famous model called or two models called the Locke-Covolterra model, right?
This is a system of nonlinear differential equations and originally these were proposed
in physical chemistry by Locke and then they got brought into ecology in the study of predator
prey dynamics.
So originally these differential equations were describing concentrations of chemistry,
right, of chemicals and how those were a sort of oscillator, right?
And then in the biological sphere these came to represent the sort of interplay between
predator and prey dynamics, population dynamics, right?
So it's the same math but the interpretation is making them different models because they're
describing different things in nature, they've been used to model different things in nature.
Now machine learning, right, a computational technique for big data analysis that generates
statistical model of the data, it's trained on given the instructions of an algorithm,
right?
I think this is pretty straightforwardly, usefully considered a type of modeling in
the same sense that we are discussing modeling in the field side literature.
So this, I don't want to take this to be a metaphysical claim right now.
I'm not saying that the ontology of ML is that it's the same sort of thing as the models
that scientists use in general, anywhere and always, but I think at least pragmatically
it's going to be useful for us to consider ML as a type of modeling in the same sense
that we're discussing scientific modeling in general, right?
Now, reification I take to be, in the case of mathematical modeling, a mistake, a confusion
of an abstract non-real or non-referring mathematical element of the model or its results for something
that's real concrete and in the world.
So anywhere that we're doing mathematical modeling and we mistake the math for the territory
as it were, we mistake the element of math, some element of math, some, you know, sign
flip, some variable, some constant, what have you, some correlation and we take it to be
something real.
We take it to be the target phenomenon of interest, right?
That's reification.
The first case of this is when we, and I'm not saying that these are necessarily exhaustive.
I'm sure you could, if you thought even a little you could come up with cases that fall
outside of the, the sort of scope of these, but I thought it would be best to illustrate
these with some examples.
The first one I consider is when we have a dummy or a nuisance variable, we have a constant
or some otherwise bookkeeping device or non-referring term in the mathematical model and we take
that to be a genuine entity or force or process or cause relationship in the world.
But as we take it to be an element that exists in the target system and is of interest to
us, let's say, I mean, so, so we might say that for this, that we have some, some bit
of math in a model that's, that's merely a bookkeeping device and we say, oh, that's
a particle.
And then we go out and we go and look for that particle.
Well, it's not a real particle.
It was just, we needed that, we needed that W there in order to make everything else check
out, right?
It's, there's a, there's a mathematically contingent reason why that, why that element
of the mathematical model is there and it's not because it's, it's telling us something
about some concrete thing out of the world.
Another case is when the history of model transfer leads us to erroneously associate
a particular interpretation or construal that is a particular, you know, mapping between
the structure, the mathematical structure and the target system.
That isn't licensed under the current application of that model, right?
So we might say that interpreting entropy and information theory as, you know, thermodynamic
entropy is such a case, right?
We have this strong, strong physical association with the notion of entropy.
So we just take it to mean, you know, physical distribution of, of states instead of our
physical dispersal states or something like that, or, or something by the equilibration,
right?
When it doesn't mean that we're using it in strictly statistical or information theoretic
sense, right?
Another case of this is ergoticity under the free energy principle, right?
Under the FEP.
There have been two papers that came out this past year alone.
To Paulus, the Colombian, that both really harp on the notion of ergoticity under the
FEP, insisting on interpreting these as, as the physical notion of ergoticity, right?
When the FEP is, I've gone to great pains to argue, it's not physics.
It's not a, it's not a, it's not a description at the level of physics, right?
To the extent that it can be taken as a description of systems of nature or a model of systems
of nature, it's not operating at the level of physics, right?
It's not, this is not stat mech, right?
So, so that's, that's a reification, right?
And it's a reification specifically because, right, the formalism of the FEP is one that
some of it originated in physics.
It went through ML and then it went into, you know, modeling in bio and neuro and, and
coxide, right?
And then it got, you know, now it's getting brought back into ML, but it's, it's had this
sort of long winding labyrinth and path of, of model transfer and that's led us to be
sort of confused about the way we're interpreting the terms after it, right?
So it's kind of easily understandable why we would, why we would make these mistakes,
but nonetheless it's a mistake, right?
Ergoticity under the FEP is not the ergoticity of physics.
Ergoticity in case number three, and this is sort of to, I'm sort of stretching it to
be two cases, but, but I think that they're, they're really fundamentally the same phenomenon,
the same mistake, right?
And this is the case in which brute force statistical techniques, e.g. ANOVA, analysis
of variance, right?
Multi-correlation, regression, factor analysis, principle component analysis, right?
These are generating us raw correlational data from, from the data sets to the run-on,
and then these correlational results are interpreted as, in some sense, causal.
They're, they're, they're given a, a realness and a concreteness that's totally unwarranted,
given, given what data they've been fed, right?
And this is first diagnosed by Luonten and Levenson Luonten, and this is the original
case for the characterization of readification.
So, so there hasn't been a, a real formal characterization of readification.
This is the first, this is my first stab, and also the, the literature's first stab at a,
at a sort of formal characterization of what we mean by readification and modeling.
But Luonten is using this notion of readification modeling, at least going back to the 70s,
and this is the use case that he's referring to here.
So he says, I feel pardoned the quotes.
I think that they're, they're powerful, and I think you, you'll see the analogy to the ML case
here pretty readily.
The most egregious examples of readification are in the use of multiple correlation and
regression and of various forms of factor and principle components analysis by social scientists.
Economists, sociologists, and especially psychologists believe that correlations
between transformed orthogonal variables are revelation of the real structure of the world.
Biologists are apparently unaware that in constructing correlation analysis itself,
they impose a model on the world. Their assumption is that they're approaching the
data in a theory-free manner and that data will speak to them through the correlation analysis.
If, however, we examine the actual relationship between dynamic systems and correlations,
it becomes clear that correlation can create relationships that do not exist.
Because the methodology of correlation is intrinsically without theoretical content
about the real world, that is thought to be its greatest virtue.
Any statements about the real world must come from the content imported into the analysis.
Further, of course, every investigator will repeat endlessly that correlation should not
be confused with causation, but such a disclaimer is disingenuous. No one would bother to carry out
the correlation analysis if they took seriously the caveat that correlations are not causations.
After all, what is the use of the analysis except to make inferences about causation?
And this is particularly the case when we want to intervene on a system.
We cannot intervene on correlations if there's no causal relationship there.
If we are talking about systems where we want to intervene, for example,
medicine or policing or the judicial system, these are cases where we want to intervene.
So there's no point in running statistical analyses there unless we're in some sense
latching onto something causal that we're able to effect in that system.
So I think that this analysis that Levinson-Lewanson give is very straightforwardly just the same
case of what's going on in the most egregious examples of ML gone awry, automated decision
making systems, in socially sensitive positions, making disastrous decisions. In the United States,
the most egregious cases of this have been in predictive policing and in generating risk scores
for recidivism rights. But you can see this all over the place too. You can see this in
predictions of who gets a loan and in predictions of who gets priority treatment for
hospital rights, for triage rights, for the vaccination distribution, all sorts of cases.
We're using ML to govern really important and by that I mean like life or death
situations for people, for real people. And what we're doing is we're finding correlational
relationships and we're then acting on those as though those are real causal relationships,
those are really getting at some fundamental feature of the system in nature of interest and
we're acting on it as such. And the results are disastrous because we've actually latched onto
something, we've latched onto a statistical relationship that's very far afield from what
we think that we're modeling. So in predictive policing and in the recidivism predictions,
what we're looking for is actually, we don't say this, we're never going to say this,
but what we're looking for is a bad guy metric or a bad neighborhood metric. We're looking for
the real causal factors there that lead certain people to be more bad than others,
to be more prone to crime than others or the occupants of certain neighborhoods to be more
prone to crime than others. Why? Because we want to send the cops to the neighborhoods that are
prone to crime. We want to do parole supervision on the guys who are more prone to crime or we want
to give them longer sentences. Unfortunately, what these methods are latching onto is
erroneous correlations and these are correlations that are actually picking up
more on things like the race bias or the class bias already inherent in parole supervision or
in sentencing or in policing rather than who's actually a bad guy or what neighborhood is actually
a bad neighborhood and so much as those notions can even make sense. The problem here is that we
end up reifying things like the bad guy or the bad neighborhood when what we've actually picked up on
is nothing to do with that. And the results are disastrous. The results are drastically
increasing existing inequities in these systems and they should be of, I think, real urgent moral
concern to us as well as the case being of an obvious epistemic concern because we're making a
kind of category error in what we're attributing to what. In the case of active inference, I was
a bit concerned to read the description on the website because the description of this
conference on the website brings up, brings up the cases of predicting recidivism rates.
And it attributes that to the decisions of black box networks and then the claim seems to be something
defective. That's a vulnerability that active inference, the FEP, are not vulnerable to,
utilized in machine learning. One of the main attributes of active inference in machine learning
is that it is transparent. The models at play are fully explicit and specify the causal factors
that generate data. This is an incredible advantage over black box networks that can
lead to the perpetuation of social problems when they draw from e.g. historical data. So
it's not saying that there's not an explicit claim that use of active inference in ML is not
capable of perpetuating social problems. But it's strongly suggested by this, right? It's
strongly suggested that when we're doing generative modeling, we're really getting at
the causal mechanisms that give rise to the data we observe, right? That the generative
process, the causal mechanism of the generative process are actually being captured by the generative
model for certain. And that because that's the case and because everything's made explicit
under generative models, that the perpetuation of social problems shouldn't be a concern
under use of active inference in ML. And I think this is really badly mistaken. I think it's
possible to take this in a bad direction. And that's of some concern to me. So
for one, this notion of reification, right? This absolutely occurs in the absence of opaque
models, right? We don't need model opacity, right, to see reification. When we're talking about,
you know, when Luontun is talking about the reification, the initial cases of reification
that he observes, right, in these big statistical techniques. The problem is not that we don't
know what's going on, that we can't open the black box of the model, right? The problem is that
we are bent on reading reality or causality into the results of our statistical modeling,
even when that's not licensed, even when that's not there, right?
And it's also a concern to me because generative modelling demonstrably does not deliver the
true causes of the generative process, right? We'd like that to be the case. That's the ideal case,
right? And certainly, active inference, any kind of generative modeling has some sort of
a priori benefits over other methods for certain particular use cases. However,
that doesn't mean that all the other ML techniques are just trying to get that real causality and
for some feature of the formalism is enabling active inference
to really latch onto the true causes, right? That's not the case. All machine learning methods,
like all statistical techniques more broadly, really are trying to give us analyses that give us
the causes of the data we observe, right? That's the goal of all of this. And some methods are
better than others at doing that, but we need real care to bring that about and simply asserting
that our method delivers true causes unlike the other methods is, I think, dangerous. I think it's
itself a real vacation. So, yeah, I wanted to be a bit, that's it. That's the end of my talk. I wanted
to be a bit harsh there. I wanted to give a little bit of a prod there to be careful in the way we
address this stuff, so. Okay, thank you, Mel. Now, if anyone has questions, you can line up at the
Q&A podium and you can start asking your questions for Mel. I think Marco is slowly edging towards
the, oh, and so is Maxwell. So, Maxwell, if you want to start asking your question, go ahead, step into
the orange circle. Hey there. Howdy. How's it going? Pretty good. Hope you're doing well.
Yeah, I mean, I enjoyed your talk. Thanks very much. So, I mean, I think you were also not
characterizing necessarily the proponents of active inference as suggesting that there were no ethical
concerns or implications, automatic ones. I think the kind of minimalistic or modest claim with
respect to the ethical benefits of explainability is that at least, in theory, you can understand
why, or rather, you have an increased grip on, you know, why your model arrived at the decision
that it did if you compare it to, for example. In principle, in principle, it's possible. Yeah,
sir, sir, if you've got, if you've got, if you've got a proprietary model, right, you've got a
proprietary model that's making socially sensitive decisions in the public sphere, or you've got a
fully opaque model, a model that isn't explainable, that isn't transparent, that's making socially
sensitive decisions, right, then there is in principle no way that we can really understand
what's going on there, right? So, of course. I don't know if I would, I would agree there in the
sense that at least with an explicit generative model, you have an understanding of the knowledge
structures that you're bringing to bear. So, there is at least a possibility for a self-critical
appraisal. If you've got an opaque model or you've got, for example, a proprietary one, there are a
lot of reasons that certain models are in principle unavailable for our analysis of them. And when you
have a model, like, you know, when we're doing generative modeling, that's in principle available
to our analysis. Yeah, that's worlds better, right, because it's still, you know, it's in
principle possible to do the analysis that helps us understand what's really going on there. Right.
But then the analysis still has to come. It's not enough that it's transparent. We still have to
carry out the analysis. And as, you know, as I was hoping to motivate with this talk, it's
totally still possible to fuck up that analysis. In fact, in fact, our history of doing these
analysis is one of fucking it up, right? Right. And, you know, I think all these questions are
absolutely critical and stuff. And I think you're bringing attention to them in a great way. But
I just want to point out the claim is a lot less like maximalist, right, or whatever. The claim is
very minimal. And it's just meant to say that this is potentially a tool that we can use to help
elucidate all of these things in a way that's more traditional methods. But I don't want to
take all the questions. I'm sorry. Thank you for your talk. Yeah, absolutely. Point taken,
absolutely. The thing for me is we often wave around for quite some time now we've been waving
around transparency and fairness as though, you know, we get a statistical fairness metric or we
get some sort of out of the can definition of transparency. And we get some certification
of our model like our RAI is transparent RAI is is is fully explicable RAI is is fair according to
this one, you know, fairness method that fairness analytic that we came up with. And that's almost
never enough because it's it's it's never it's never context sensitive, right? It's never it's
never enough. And I worry that we're starting to go there with causality, right, that we're
that we're starting to say, Oh, our methods, you know, our methods specify causes in some sense,
that's, you know, that really is a separate point, though. I mean, I do think that the active
inference stuff and the type of the fact that like the structure of the formalism is about
hypothesis testing, at least lends itself to causal investigation, right?
We're going to have to try it out, unfortunately, because I think other people wanted to ask
questions and there's really only four minutes left. So I know that Marco skipped his term,
but Alex is still in line. Alex, do you want to ask your question?
Or, hey, Mel, how are you doing? Good to see you. Yeah, for real.
Well, so I had a kind of philosophical question that does not lend itself to like two minutes,
but I guess I'm just wondering. Yeah, I guess I'm just wondering how if you think that there's
any form of modeling or or scientific practice or like cognition that that would do better or
like cognition vastly increase the problem space, didn't they? Yes, true. I'm not sure. Okay. Well,
you can decide to take or leave that that parameter. But like, do you think there's any
form of modeling, let's say, that that would that would count as like delivering the true
generative process? Or is that just like a transcendent ideal of some kind?
I mean, certainly, certainly, there's going to come in in
vast shades, right? Right. I want to say there's a there's a, you know, huge gamut of
potential variation there. But yeah, I think I think it may be like
the full, the full, you know, the model captures all aspects of the course. I mean,
actually, I think I think actually, I want to I want to say that that's no longer modeling,
right? Like in order in order to to be a model in order to to constitute a process of modeling,
we need to be abstracting or idealizing or discarding variables, right? Something is not
a model of something. If it is, if it is engendering all the, the, you know, causes that it's
a model, it's not a model anymore, if it's, if it's got all the causes, if it's slash not all
the causes, right? Cool. I'm really happy with that answer, actually. Cool. One for one.
One for two. Okay. So we may have time for Steven's question after us. Steven, can you go
ahead and ask a question? But after we move on to Julia's presentation.
Thanks. Thanks, Mel. Yeah, great talk. I was just wondering. So the machine learning sort of comes
out of maths and some of that work. And then you've got science coming to being brought to bear
under the free energy principle. So I suppose in post normal science, they're talking about,
you know, how you can't and complexity, you can't necessarily a causality. So you,
you can only have decision support rather than decision proof. So I'm wondering, like,
is there a danger, like, because it's got this scientific underpinning that people, I mean,
even if it's not within the active inference community, people can say it's scientific,
therefore, and it's also mathematical, therefore, and then the reification starts to
be enforced upon it because it makes it a good sales pitch. I don't know if that's
something that you're concerned with or. So like undo, undo, undo importing of, of epistemic
sort of authority, because of the science label. Um,
yeah, certainly that's the case. And in particular, I think, I think, like, machine learning methods
today are bound with, with physics concepts that actually aren't the physics concepts,
but just formal analogs. And that's one case where we're totally, we're totally letting
the, the, the authority of the, the fundamentality of physics, you know,
attempt to, to bring in sort of undo epistemic authority there. But I think, I think there's
more of a benefit there than there is, than there is a trade off. Why? Because I think, I think that
if we treat ML as a type of scientific modeling, then we need to hold it to the same standards of,
of scientific models. And I think that's, that's worth more than, than the potential for that kind
of epistemic authority slip picture. Thanks, Mel.
