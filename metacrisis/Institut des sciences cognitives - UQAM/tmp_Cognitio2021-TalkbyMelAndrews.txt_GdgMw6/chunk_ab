that generate data. This is an incredible advantage over black box networks that can
lead to the perpetuation of social problems when they draw from e.g. historical data. So
it's not saying that there's not an explicit claim that use of active inference in ML is not
capable of perpetuating social problems. But it's strongly suggested by this, right? It's
strongly suggested that when we're doing generative modeling, we're really getting at
the causal mechanisms that give rise to the data we observe, right? That the generative
process, the causal mechanism of the generative process are actually being captured by the generative
model for certain. And that because that's the case and because everything's made explicit
under generative models, that the perpetuation of social problems shouldn't be a concern
under use of active inference in ML. And I think this is really badly mistaken. I think it's
possible to take this in a bad direction. And that's of some concern to me. So
for one, this notion of reification, right? This absolutely occurs in the absence of opaque
models, right? We don't need model opacity, right, to see reification. When we're talking about,
you know, when Luontun is talking about the reification, the initial cases of reification
that he observes, right, in these big statistical techniques. The problem is not that we don't
know what's going on, that we can't open the black box of the model, right? The problem is that
we are bent on reading reality or causality into the results of our statistical modeling,
even when that's not licensed, even when that's not there, right?
And it's also a concern to me because generative modelling demonstrably does not deliver the
true causes of the generative process, right? We'd like that to be the case. That's the ideal case,
right? And certainly, active inference, any kind of generative modeling has some sort of
a priori benefits over other methods for certain particular use cases. However,
that doesn't mean that all the other ML techniques are just trying to get that real causality and
for some feature of the formalism is enabling active inference
to really latch onto the true causes, right? That's not the case. All machine learning methods,
like all statistical techniques more broadly, really are trying to give us analyses that give us
the causes of the data we observe, right? That's the goal of all of this. And some methods are
better than others at doing that, but we need real care to bring that about and simply asserting
that our method delivers true causes unlike the other methods is, I think, dangerous. I think it's
itself a real vacation. So, yeah, I wanted to be a bit, that's it. That's the end of my talk. I wanted
to be a bit harsh there. I wanted to give a little bit of a prod there to be careful in the way we
address this stuff, so. Okay, thank you, Mel. Now, if anyone has questions, you can line up at the
Q&A podium and you can start asking your questions for Mel. I think Marco is slowly edging towards
the, oh, and so is Maxwell. So, Maxwell, if you want to start asking your question, go ahead, step into
the orange circle. Hey there. Howdy. How's it going? Pretty good. Hope you're doing well.
Yeah, I mean, I enjoyed your talk. Thanks very much. So, I mean, I think you were also not
characterizing necessarily the proponents of active inference as suggesting that there were no ethical
concerns or implications, automatic ones. I think the kind of minimalistic or modest claim with
respect to the ethical benefits of explainability is that at least, in theory, you can understand
why, or rather, you have an increased grip on, you know, why your model arrived at the decision
that it did if you compare it to, for example. In principle, in principle, it's possible. Yeah,
sir, sir, if you've got, if you've got, if you've got a proprietary model, right, you've got a
proprietary model that's making socially sensitive decisions in the public sphere, or you've got a
fully opaque model, a model that isn't explainable, that isn't transparent, that's making socially
sensitive decisions, right, then there is in principle no way that we can really understand
what's going on there, right? So, of course. I don't know if I would, I would agree there in the
sense that at least with an explicit generative model, you have an understanding of the knowledge
structures that you're bringing to bear. So, there is at least a possibility for a self-critical
appraisal. If you've got an opaque model or you've got, for example, a proprietary one, there are a
lot of reasons that certain models are in principle unavailable for our analysis of them. And when you
have a model, like, you know, when we're doing generative modeling, that's in principle available
to our analysis. Yeah, that's worlds better, right, because it's still, you know, it's in
principle possible to do the analysis that helps us understand what's really going on there. Right.
But then the analysis still has to come. It's not enough that it's transparent. We still have to
carry out the analysis. And as, you know, as I was hoping to motivate with this talk, it's
totally still possible to fuck up that analysis. In fact, in fact, our history of doing these
analysis is one of fucking it up, right? Right. And, you know, I think all these questions are
absolutely critical and stuff. And I think you're bringing attention to them in a great way. But
I just want to point out the claim is a lot less like maximalist, right, or whatever. The claim is
very minimal. And it's just meant to say that this is potentially a tool that we can use to help
elucidate all of these things in a way that's more traditional methods. But I don't want to
take all the questions. I'm sorry. Thank you for your talk. Yeah, absolutely. Point taken,
absolutely. The thing for me is we often wave around for quite some time now we've been waving
around transparency and fairness as though, you know, we get a statistical fairness metric or we
get some sort of out of the can definition of transparency. And we get some certification
of our model like our RAI is transparent RAI is is is fully explicable RAI is is fair according to
this one, you know, fairness method that fairness analytic that we came up with. And that's almost
never enough because it's it's it's never it's never context sensitive, right? It's never it's
never enough. And I worry that we're starting to go there with causality, right, that we're
that we're starting to say, Oh, our methods, you know, our methods specify causes in some sense,
that's, you know, that really is a separate point, though. I mean, I do think that the active
inference stuff and the type of the fact that like the structure of the formalism is about
hypothesis testing, at least lends itself to causal investigation, right?
We're going to have to try it out, unfortunately, because I think other people wanted to ask
questions and there's really only four minutes left. So I know that Marco skipped his term,
but Alex is still in line. Alex, do you want to ask your question?
Or, hey, Mel, how are you doing? Good to see you. Yeah, for real.
Well, so I had a kind of philosophical question that does not lend itself to like two minutes,
but I guess I'm just wondering. Yeah, I guess I'm just wondering how if you think that there's
any form of modeling or or scientific practice or like cognition that that would do better or
like cognition vastly increase the problem space, didn't they? Yes, true. I'm not sure. Okay. Well,
you can decide to take or leave that that parameter. But like, do you think there's any
form of modeling, let's say, that that would that would count as like delivering the true
generative process? Or is that just like a transcendent ideal of some kind?
I mean, certainly, certainly, there's going to come in in
vast shades, right? Right. I want to say there's a there's a, you know, huge gamut of
potential variation there. But yeah, I think I think it may be like
the full, the full, you know, the model captures all aspects of the course. I mean,
actually, I think I think actually, I want to I want to say that that's no longer modeling,
right? Like in order in order to to be a model in order to to constitute a process of modeling,
we need to be abstracting or idealizing or discarding variables, right? Something is not
a model of something. If it is, if it is engendering all the, the, you know, causes that it's
a model, it's not a model anymore, if it's, if it's got all the causes, if it's slash not all
the causes, right? Cool. I'm really happy with that answer, actually. Cool. One for one.
One for two. Okay. So we may have time for Steven's question after us. Steven, can you go
ahead and ask a question? But after we move on to Julia's presentation.
Thanks. Thanks, Mel. Yeah, great talk. I was just wondering. So the machine learning sort of comes
out of maths and some of that work. And then you've got science coming to being brought to bear
under the free energy principle. So I suppose in post normal science, they're talking about,
you know, how you can't and complexity, you can't necessarily a causality. So you,
you can only have decision support rather than decision proof. So I'm wondering, like,
is there a danger, like, because it's got this scientific underpinning that people, I mean,
even if it's not within the active inference community, people can say it's scientific,
therefore, and it's also mathematical, therefore, and then the reification starts to
be enforced upon it because it makes it a good sales pitch. I don't know if that's
something that you're concerned with or. So like undo, undo, undo importing of, of epistemic
sort of authority, because of the science label. Um,
yeah, certainly that's the case. And in particular, I think, I think, like, machine learning methods
today are bound with, with physics concepts that actually aren't the physics concepts,
but just formal analogs. And that's one case where we're totally, we're totally letting
the, the, the authority of the, the fundamentality of physics, you know,
attempt to, to bring in sort of undo epistemic authority there. But I think, I think there's
more of a benefit there than there is, than there is a trade off. Why? Because I think, I think that
if we treat ML as a type of scientific modeling, then we need to hold it to the same standards of,
of scientific models. And I think that's, that's worth more than, than the potential for that kind
of epistemic authority slip picture. Thanks, Mel.
