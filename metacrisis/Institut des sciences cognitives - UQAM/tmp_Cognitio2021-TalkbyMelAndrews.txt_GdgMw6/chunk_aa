So, yeah, I want to talk a little bit about a phenomenon that I call reification in mathematical
modeling.
Oh, what does that sound mean?
Nothing, Mel, just keep going, it's the chat.
Just keep going.
Oh, Jesus Christ.
I can't make that stop, can I?
Yes, you can.
If you go back to the app and you click on the chat, there is a little cogwheel at the
top left and you just have to remove the notifications.
Did you find the cogwheel, Mel?
Yes, in theory.
The one that's on the top left in the chat function?
Oh, there.
It should say.
Okay, I think I did it.
Okay, cool.
Sorry about that.
So, I want to talk about a phenomenon that I call reification in mathematical modeling.
And I want to extend my discussion of that to the case of machine learning, which I consider
to be a use case of mathematical modeling.
And basically what that is, is what I've called mistaking the math for the territory.
And I want to discuss some of the potential harms that can come from that sort of reification,
especially in the domain of machine learning.
So I'm going to cover a couple of preliminaries on scientific modeling.
I'm going to describe machine learning as a variety of scientific modeling and I'm going
to then characterize what I mean by reification in mathematical modeling, give three examples
of that and then discuss briefly how it applies to actually inference the free energy.
So the basics of scientific modeling, models are taken to be abstract structures which
enable investigation into real world systems called target system.
And the analogy here is that model is to target as map is to territory in the sort of Korzybsk
in the depiction of the map of the territory, right?
Modeling aims to give us predictive, interventional or explanatory purchase or leverage.
It aims to give us grip on some target phenomenon nature.
These being entities, forces, processes or causal relationships in the world that we're
aiming to have predictive or causal or explanatory purchase over, right?
Now on the sort of contemporary mainstream account of scientific modeling, models are
taken to be interpreted structure.
So what a model consists in is a physical, mathematical or computational structure plus
a scientist's interpretation or construal of that structure which relates it to target
system.
This is perhaps the easiest seen in the case of model transfer.
So there's a famous model called or two models called the Locke-Covolterra model, right?
This is a system of nonlinear differential equations and originally these were proposed
in physical chemistry by Locke and then they got brought into ecology in the study of predator
prey dynamics.
So originally these differential equations were describing concentrations of chemistry,
right, of chemicals and how those were a sort of oscillator, right?
And then in the biological sphere these came to represent the sort of interplay between
predator and prey dynamics, population dynamics, right?
So it's the same math but the interpretation is making them different models because they're
describing different things in nature, they've been used to model different things in nature.
Now machine learning, right, a computational technique for big data analysis that generates
statistical model of the data, it's trained on given the instructions of an algorithm,
right?
I think this is pretty straightforwardly, usefully considered a type of modeling in
the same sense that we are discussing modeling in the field side literature.
So this, I don't want to take this to be a metaphysical claim right now.
I'm not saying that the ontology of ML is that it's the same sort of thing as the models
that scientists use in general, anywhere and always, but I think at least pragmatically
it's going to be useful for us to consider ML as a type of modeling in the same sense
that we're discussing scientific modeling in general, right?
Now, reification I take to be, in the case of mathematical modeling, a mistake, a confusion
of an abstract non-real or non-referring mathematical element of the model or its results for something
that's real concrete and in the world.
So anywhere that we're doing mathematical modeling and we mistake the math for the territory
as it were, we mistake the element of math, some element of math, some, you know, sign
flip, some variable, some constant, what have you, some correlation and we take it to be
something real.
We take it to be the target phenomenon of interest, right?
That's reification.
The first case of this is when we, and I'm not saying that these are necessarily exhaustive.
I'm sure you could, if you thought even a little you could come up with cases that fall
outside of the, the sort of scope of these, but I thought it would be best to illustrate
these with some examples.
The first one I consider is when we have a dummy or a nuisance variable, we have a constant
or some otherwise bookkeeping device or non-referring term in the mathematical model and we take
that to be a genuine entity or force or process or cause relationship in the world.
But as we take it to be an element that exists in the target system and is of interest to
us, let's say, I mean, so, so we might say that for this, that we have some, some bit
of math in a model that's, that's merely a bookkeeping device and we say, oh, that's
a particle.
And then we go out and we go and look for that particle.
Well, it's not a real particle.
It was just, we needed that, we needed that W there in order to make everything else check
out, right?
It's, there's a, there's a mathematically contingent reason why that, why that element
of the mathematical model is there and it's not because it's, it's telling us something
about some concrete thing out of the world.
Another case is when the history of model transfer leads us to erroneously associate
a particular interpretation or construal that is a particular, you know, mapping between
the structure, the mathematical structure and the target system.
That isn't licensed under the current application of that model, right?
So we might say that interpreting entropy and information theory as, you know, thermodynamic
entropy is such a case, right?
We have this strong, strong physical association with the notion of entropy.
So we just take it to mean, you know, physical distribution of, of states instead of our
physical dispersal states or something like that, or, or something by the equilibration,
right?
When it doesn't mean that we're using it in strictly statistical or information theoretic
sense, right?
Another case of this is ergoticity under the free energy principle, right?
Under the FEP.
There have been two papers that came out this past year alone.
To Paulus, the Colombian, that both really harp on the notion of ergoticity under the
FEP, insisting on interpreting these as, as the physical notion of ergoticity, right?
When the FEP is, I've gone to great pains to argue, it's not physics.
It's not a, it's not a, it's not a description at the level of physics, right?
To the extent that it can be taken as a description of systems of nature or a model of systems
of nature, it's not operating at the level of physics, right?
It's not, this is not stat mech, right?
So, so that's, that's a reification, right?
And it's a reification specifically because, right, the formalism of the FEP is one that
some of it originated in physics.
It went through ML and then it went into, you know, modeling in bio and neuro and, and
coxide, right?
And then it got, you know, now it's getting brought back into ML, but it's, it's had this
sort of long winding labyrinth and path of, of model transfer and that's led us to be
sort of confused about the way we're interpreting the terms after it, right?
So it's kind of easily understandable why we would, why we would make these mistakes,
but nonetheless it's a mistake, right?
Ergoticity under the FEP is not the ergoticity of physics.
Ergoticity in case number three, and this is sort of to, I'm sort of stretching it to
be two cases, but, but I think that they're, they're really fundamentally the same phenomenon,
the same mistake, right?
And this is the case in which brute force statistical techniques, e.g. ANOVA, analysis
of variance, right?
Multi-correlation, regression, factor analysis, principle component analysis, right?
These are generating us raw correlational data from, from the data sets to the run-on,
and then these correlational results are interpreted as, in some sense, causal.
They're, they're, they're given a, a realness and a concreteness that's totally unwarranted,
given, given what data they've been fed, right?
And this is first diagnosed by Luonten and Levenson Luonten, and this is the original
case for the characterization of readification.
So, so there hasn't been a, a real formal characterization of readification.
This is the first, this is my first stab, and also the, the literature's first stab at a,
at a sort of formal characterization of what we mean by readification and modeling.
But Luonten is using this notion of readification modeling, at least going back to the 70s,
and this is the use case that he's referring to here.
So he says, I feel pardoned the quotes.
I think that they're, they're powerful, and I think you, you'll see the analogy to the ML case
here pretty readily.
The most egregious examples of readification are in the use of multiple correlation and
regression and of various forms of factor and principle components analysis by social scientists.
Economists, sociologists, and especially psychologists believe that correlations
between transformed orthogonal variables are revelation of the real structure of the world.
Biologists are apparently unaware that in constructing correlation analysis itself,
they impose a model on the world. Their assumption is that they're approaching the
data in a theory-free manner and that data will speak to them through the correlation analysis.
If, however, we examine the actual relationship between dynamic systems and correlations,
it becomes clear that correlation can create relationships that do not exist.
Because the methodology of correlation is intrinsically without theoretical content
about the real world, that is thought to be its greatest virtue.
Any statements about the real world must come from the content imported into the analysis.
Further, of course, every investigator will repeat endlessly that correlation should not
be confused with causation, but such a disclaimer is disingenuous. No one would bother to carry out
the correlation analysis if they took seriously the caveat that correlations are not causations.
After all, what is the use of the analysis except to make inferences about causation?
And this is particularly the case when we want to intervene on a system.
We cannot intervene on correlations if there's no causal relationship there.
If we are talking about systems where we want to intervene, for example,
medicine or policing or the judicial system, these are cases where we want to intervene.
So there's no point in running statistical analyses there unless we're in some sense
latching onto something causal that we're able to effect in that system.
So I think that this analysis that Levinson-Lewanson give is very straightforwardly just the same
case of what's going on in the most egregious examples of ML gone awry, automated decision
making systems, in socially sensitive positions, making disastrous decisions. In the United States,
the most egregious cases of this have been in predictive policing and in generating risk scores
for recidivism rights. But you can see this all over the place too. You can see this in
predictions of who gets a loan and in predictions of who gets priority treatment for
hospital rights, for triage rights, for the vaccination distribution, all sorts of cases.
We're using ML to govern really important and by that I mean like life or death
situations for people, for real people. And what we're doing is we're finding correlational
relationships and we're then acting on those as though those are real causal relationships,
those are really getting at some fundamental feature of the system in nature of interest and
we're acting on it as such. And the results are disastrous because we've actually latched onto
something, we've latched onto a statistical relationship that's very far afield from what
we think that we're modeling. So in predictive policing and in the recidivism predictions,
what we're looking for is actually, we don't say this, we're never going to say this,
but what we're looking for is a bad guy metric or a bad neighborhood metric. We're looking for
the real causal factors there that lead certain people to be more bad than others,
to be more prone to crime than others or the occupants of certain neighborhoods to be more
prone to crime than others. Why? Because we want to send the cops to the neighborhoods that are
prone to crime. We want to do parole supervision on the guys who are more prone to crime or we want
to give them longer sentences. Unfortunately, what these methods are latching onto is
erroneous correlations and these are correlations that are actually picking up
more on things like the race bias or the class bias already inherent in parole supervision or
in sentencing or in policing rather than who's actually a bad guy or what neighborhood is actually
a bad neighborhood and so much as those notions can even make sense. The problem here is that we
end up reifying things like the bad guy or the bad neighborhood when what we've actually picked up on
is nothing to do with that. And the results are disastrous. The results are drastically
increasing existing inequities in these systems and they should be of, I think, real urgent moral
concern to us as well as the case being of an obvious epistemic concern because we're making a
kind of category error in what we're attributing to what. In the case of active inference, I was
a bit concerned to read the description on the website because the description of this
conference on the website brings up, brings up the cases of predicting recidivism rates.
And it attributes that to the decisions of black box networks and then the claim seems to be something
defective. That's a vulnerability that active inference, the FEP, are not vulnerable to,
utilized in machine learning. One of the main attributes of active inference in machine learning
is that it is transparent. The models at play are fully explicit and specify the causal factors
