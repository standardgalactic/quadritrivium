of response. So if we map this onto our perception action loop, we're asking the participants to
try and infer this hidden state of which square was the me square. They can act to intervene on
that hidden state through their mouse movements. We can see their internal model and their current
hypothesis about which square they control through the eye tracking data. And we also get
information about what their sensory input is. So this is focused on the square that they were
controlling. So that square with the green line around it and is not necessarily the same as
this hidden state that they're trying to infer. So we get this really rich data set and participants
have a lot of freedom in this experimental design, which I think are both really interesting additions
to the existing literature. Another really interesting addition I think that this experiment
allowed us to do was to measure prediction error as the trial progressed. So we can see a sensitive
and dynamic prediction error measure. It looks like an EEG signal over the whole trial. Importantly,
especially for this audience, this is a behavioral proxy for prediction error. So we don't measure
any neural signals. I mean, you could think of eye movement sometimes as neural signals,
but we don't measure anything from the brain. So this doesn't take into account the participants'
internal model of environmental uncertainty. We would expect participants to change their
predictions and their priors depending on the uncertainty condition. This measure doesn't take
that into account. But let me show you how we measure it. So we take the square that the
participant's looking at and wherever they move the mouse, we take that as the expected location
of the square that they're looking at. So this is kind of the prior expectation of the square's
movement based on lots of experience of moving mouses and seeing stuff move on screen. But of
course, our squares, even the MEE square, doesn't move directly to this expected location. Instead,
we add this variability. So the MEE square will just have this variability. And if they're looking
at the wrong square, if it's not the MEE square, then they'll also have this offset, which is that
number, the white number that was in the middle of the distractor screens squares. So when we add
this offset and variability, the actual square moves to some other location, the square that
they're looking at. And we just very simply take the difference Euclidean distance between the
expected location and the actual location as a dynamic measure of prediction error over the trial.
Now, like I said, this experiment gives a lot of power over to the participants. And one thing
that they have control over is the magnitude of this prediction error. So how can participants
control the prediction error in this experiment? The first way is that they control their speed
of movement. So if they don't move at all, then they get zero prediction error, but they also get
no information about which square they're controlling. Because the distance here is zero,
and the distance here is zero, and you just end up staying in the same spot. If participants move
slower, this triangle is smaller, and so the prediction error is smaller. If participants
move faster, then the triangle gets bigger, and the prediction error is faster. But also,
participants can control the magnitude of prediction error just by choosing a better
hypothesis. So if they're choosing the MEE square, you eliminate all of this offset,
and just look at the variability. So that's much smaller prediction error.
You can also have better and worse distractor squares, which have more and less prediction error.
So those are two ways that the participant can control the prediction error in this experiment.
So because we can measure prediction error, we can also look at how prediction error changes
across the trial in different conditions. So we take the average prediction error for all of
the trials for a participant, and look at, in different conditions, how the minimization of
prediction error changes. So for example, we can look at trials where participants judged that they
did not have agency versus trials where participants picked a square at the end, and we can fit a line
to these averages for each participant, which look at the change in prediction error over time.
And because we're particularly interested in prediction error minimization,
we want to look at the slope of this line that we fit for each participant.
So that's what I'll show you in this graph. Because we're looking at the slope of the line,
a steeper negative slope is more prediction error minimization, whereas a positive slope
is no prediction error minimization. There's prediction error accumulating.
We're going to look at not-agent judgments on the left or trials where the participants judged
that they were not the agent versus agent on the right, and I've also split them by accuracy in this
graph. So what the results show is that when participants judge that they have agency,
they minimize prediction error faster regardless of the accuracy of their judgment.
So these ones are lower. They have lower prediction error minimization. They have steeper
negative prediction error minimization than the ones on the left here. And importantly,
it doesn't matter whether they were right or wrong. There's a main effect of agency here.
And so this indicates to us that participants might be using something like the prediction
error minimization or the speed of prediction error minimization to judge at the end of the trial
whether or not they had agency. We can also look at the prediction error pattern around
particular actions of the agent, so instead of looking at the trend across the whole
trial, we can time lock to particular actions that the participant has taken.
So in this experiment, I want to focus on the hypothesis switch. So this is when a participant
chooses to change their hypothesis from one square to another square based on that eye-tracking data.
So we can look at, for an individual trial, we get some measure of prediction error over the trial,
and we take all of the moments that they switched which square they were looking at,
and we average an epoch around each of these events, just like an ERP analysis for anyone who
does EDG research. So we can see the average pattern of activity around these actions that
the participant's taking. And remember that the timing of these is totally up to the participant.
So what I'm going to show you here is like an ERP, we're going to time lock everything to the
hypothesis switch, look at 500 milliseconds leading up to it and 500 milliseconds after it,
and we're going to see prediction error on the y-axis. And what we're interested in here is whether
the hypothesis switch can be used or is being used as a policy for prediction error minimization.
So if this were the case, you'd see rising prediction error, then the participants would
act which should make the prediction error drop. And that's exactly what we do see. So we get this
really nice peak at the time of the hypothesis switch, which is preceded by increasing prediction
error and followed by decreasing prediction error. So this was really exciting and not the sort of
thing that we've ever seen measured in behavioral data before. So we could see that participants
seem to be using this action of hypothesis switching as a policy for prediction error minimization.
So two things I want you to take away from this experiment. The first is that participants seem
to be using the rate of prediction error minimization to make agency judgments. So when
the prediction error is going up, they tend to decide that it was not them. Whereas when the
prediction error is going down, that's when they decide that it was them, whether or not they were
right about this decision. And the second thing is the participants are acting to counteract
rising prediction error. And in this case, it was through switching their hypothesis about
which square they could control. So in the second experiment I want to tell you about,
it was another squares task. This is newer, a newer paper. And they were deciding between
four squares, which one they controlled. But in this experiment, we were interested in how
participants used the structures in different environments in order to infer agency, whether
they could capitalize on different structures and different environments in order to inform their
agency judgments. So if participants moved the mouse straight up, there were two environments on
the screen. And the participants couldn't see this boundary, but they knew that one half of the
screen was water and one half of the screen was sand. And they were trained a little bit or
instructed as to what the difference between the two was. So in the sand environment, moving up
and up straight, we add to that unpredictable variability. So just like in the last experiment,
on every frame, there was some distribution around the trajectory of the mouse movement.
And we jittered the square around that trajectory that was input into the system.
So this is just like the last experiment's jitter and variability. The difference in the water
environment is that there was some hierarchical deeper structure in the pattern of the variability.
So the direction of the variability was predictable. So for example, in this illustration, we have
two frames where the participant was pushed to the left of where they moved. Then two frames
where they were pushed to the right and then left and then right. So if the participants
properly modeled these waves or the pattern, then they could gain something or minimize more
uncertainty about the consequences of their movement. But by our prediction error measure,
the behavioral prediction error measure, the two environments were equivalent because the
difference between where they expected to be and where they actually ended up
was the same. It was just the sign that became predictable. So we were looking at the policies
that participants used to infer agency in this kind of more structured environment setup.
So if we map this onto our action perception loop, there were two things that we changed
from the last experiment. The first is obviously the addition of this environmental structure,
which affects the movement of the squares. And the second thing we changed was that the
current hypothesis was now measured using a button press. And I can talk more about that
in question time if you're interested. But basically, the pandemic happened, we had to
collect data online. So we changed the current hypothesis measurement to a button press instead
of the eye tracking. So we were looking at environment related policies. And we had these
two environments, one with predictable variability that required more model complexity in order
to make use of the prediction predictableness. And the other, which was the sand environment,
which had this unpredictable variability, had more irreducible uncertainty, but didn't require as
complex a model for the participants to navigate it. So we were what I'm going to focus on in this
talk is whether participants use switching between environments, just mere switching,
as a policy to minimize prediction error. So whether if we do that same kind of analysis,
where we look at the prediction error over time, and we time lock it to the times where they cross
that environmental boundary either through the middle or around the outsides, whether we also
see that same pattern of prediction error minimization around the environment switches.
So in this graph, I'm going to show you the true boundary crossings, which is the same kind of graph
I showed you before in blue. And we've got a control in this version of the experiment,
which is a null permutation. And I can talk a bit more about that in question time also.
So we've time locked everything to the environment switch this time. This is the policy that we're
interested in. We look at the frames before and after this is the same temporal duration as the
last experiment. And we've got prediction error on the y axis. So we're looking to see whether if
there's rising prediction error, the participants seem to act to change it to reduce the prediction
error. And again, the participants had full control over when they switched environments.
So we could really look at the pattern leading up to and following this kind of free decision.
So again, we see this really nice peak at the time of the action, which is significantly different
from this control condition. So the switching environments crossing from one environment to
the other is preceded by an increase in prediction error, unfollowed by a dramatic decrease.
And of course, because a lot of my work is about autism and understanding autism through a
predictive processing lens, in both of these experiments, we had a measure of autism traits.
So this is just a questionnaire. Low AQ indicates that participants had fewer autism traits,
and high AQ indicates the participants had more autism traits. And we can split those nice peaks
that we saw around hypothesis switches and environment switches by the participants autism
traits score. So let's see what that looks like. Now this was really exciting when I found it.
We see this really nice differentiation at the time of the action, particularly
between participants with different autism traits scores. So this shows that participants with low
autism traits across both these different experiments and both these different policies
seem to act at a higher prediction error, whereas participants with high autism traits
act when the prediction error is lower. So what this means, because we've timelocked everything
to the action, you can think of it instead as autistic people or people with high autism traits
acting earlier in response to rising prediction errors. So if we think of the prediction error as
increasing, the time at which the participants with high autism traits act is earlier than the
time when low autism traits participants act. So it's like they're tolerating less uncertainty
before they intervene to try and minimize it. This is really cool because there were lots of
differences between these two experiments. So even though we saw this really striking similarity
in the pattern of the data, remember that the action that we were measuring was different.
It was hypothesis switches versus environment switches. The way we measured prediction error
was different. We had this button press, which was much more intentional and manual and disjointed
versus the eye tracking, which was much smoother, had a lot more kind of subconscious processes going
on, but also that we could measure this at home and in the lab with eye tracking.
So I think it's usually really exciting when you see kind of replications like this,
but it was especially exciting to me because we had changed so much and it seems like a really
general finding. So if we go back to fidgeting as an active inference puzzle, you've probably
forgotten all about that by now, but I asked you in the beginning if all action is explained as a
way to minimize prediction error, how do we explain actions like fidgeting that don't give us more
information about the world or reward pragmatic gain? And what I would say is that fidgeting is
just another one of these actions that we perform when uncertainty is rising, when we're getting
rising prediction error. So the idea is that when the rate of prediction error is not what we expect,
then we get rising uncertainty about our model. So we no longer believe that our model is doing
an adequate job at minimizing prediction error. And so we act in order to try and minimize that
prediction error, we do these self-stimulatory repetitive actions. And the reason we choose these
is because we've learned across all sorts of contexts in our lives, they do the same thing,
they result in the same kind of expected sensory consequences. The kinds of actions that we choose
have a very tight causal loop. And so there's not lots of other hidden causes that are interacting
with our actions. We're usually like touching our own hair or a pen that we find very reliable
or twiddling our fingers. So these are all things that are not usually messed with by things in the
outside world. And doing these actions gives us strong predictable evidence for a continued
existence or model accuracy. So the purpose of the fidgeting action in my view is to minimize
rising prediction error. It's not going to solve the problem that you're having out in the world,
but it will temporarily reassure you that your model is still accurate in some sense, or at
least the deeper stuff. So what I want you to take away from today first is that participants
use the rate of prediction error minimization to make agency judgments. I've got some QR codes on
the right if you're interested in the details, but also that participants act to counteract
rising prediction error across a range of these different experiments and theoretical ideas about
action. So hypothesis switches, environment switches, and fidgeting. That autistic people
might act earlier in response to rising prediction error. And that fidgeting under active inference
might be best understood as a self-evidencing policy. So I just want to thank the organizers who
invited me here today. I'm really honored to have given this keynote. And also my co-author is Jacob
Hovey, Jonah Robinson, Becky Lawson, and Shana Jamadar, who without him I couldn't have done any
of this research. I really appreciate their support. Thank you. Okay, what an amazing talk.
So if anyone would like to ask questions, please go to the Q&A podium. Should I leave the slides up
to if people need to see things, or is it better to take them down? I'll leave them up. And if I'm
looking over here, it's because I'm looking at you not at my screen with my slides. So I'm not being
rude. We're not trying to be anyway. Hi. That was amazing. That's terrific. I almost feel like I'm
at an actual conference. That's how excited I got. Is it not an actual conference? As close as we can
get. Well, first of all, is it New York? Are the results like really clean? That's like, that's,
it's great, right? That's amazing. Yeah, and we had a lot of participants. So in the first
experiment, there were 40 final participants. In the second one, there were 80. And we had to,
especially in the second one, had to throw out a lot of participants for data quality issues. So I
think the final data set is much cleaner than the kind of initial, and there are lots of participants
so you end up with kind of cleaner results. Yeah. And so, so I was, sorry, it's a lot to take in.
