What I have been working on is the free energy principle as a theory of living systems.
My hope is that thinking through what sense free energy minimization does or does not
provide a sort of necessary principle for a single organism can be informative for thinking
about the kind of collective dynamics that we should or should not expect to emerge when
they interact.
OK, so as everyone at this conference likely knows, active inference has proven to be a
strikingly useful modeling tool that has been applied across a wide range of phenomena across
temples, spatial scales, neural dynamics to, that sort of slide, to interpersonal and collective
coordination.
But what I want to ask is why does it work so well?
And I think that's a particularly important question to ask when it comes to theories
that are being applied in the cognitive and social domain.
So Carl Friston nicely laid out the kind of two different ways that you could approach
active inference in a chapter on the book Clarke and his critics called Beyond the Desert
Landscape.
And one way he suggests you can approach active inference is by what he terms the high-ride.
So the slides are being kind of strained for a second.
OK, for high-ride, you can just treat it as a description of a computational function
that allows us to perform a particular task, namely controlling both the environment and
yourself to achieve a mutually stable state.
And the key insight would be that we can re-describe this overall control task in inferential terms
as minimizing the surprising of the state of the self-environment system.
That is, inferring some limited subset of states that can be stably maintained and so
easily predicted.
We can't do this directly for two reasons.
The state of the environment isn't directly observable, and true survival is typically
intractable to compute.
So free energy minimization serves as a computational level description of an approximation to that
function, achieved by minimizing the difference between the state of observable consequences
and a prediction about these drawn from a simplified set of more tractable probability
distributions, the variational model, which internal states encode.
And the details can be fleshed out by the selection of the particular class of variational
models and any fleshed out further in specific algorithmic implementations of that, like
the big process.
But whether or not a particularly cognitive, behavioral, social phenomena will be one model
by active inference will depend on whether it has the performance of that control task
as its function, which is something that we determine on a case-by-case basis.
The other way of approaching active inference is via what he calls a high-road, which as
Risen describes it, stands in for a top-down approach that starts by asking fundamental
questions about the necessary properties things must possess if they exist.
On this approach, surprising minimization is not just a way of describing a computational
task that we might sometimes need to approximate, it's a necessary principle, a theory of everything
that can be distinguished from other things in a statistical sense.
One that ties existence to one simple imperative, avoid surprises and you will last longer,
which as Preston puts it, is a principle so basic there is no need to recourse to any
other principles.
And what can make this claim sometimes a bit difficult to interpret is over the past
decade or so, I think the way the free energy principle has got fleshed out has changed
significantly.
Both in terms of the scope of the different types of systems that are claimed to fall
under this imperative and in terms of the claims made as to its explanatory power and
ontological fidelity.
This is a small but a kind of representative sample of literature on this and what I think
is interesting is that if you look at the years of publication, it looks like there's
been a link to me along both these axes in terms of increased scope going roughly but
not unerringly, hand in hand with a more modest assessment of frame once fidelity and explanatory
power.
So in the earliest introductions at the sort of top left hand side of that little graph
thing, it's presented as an explanation of how a brain-equipped creature can overcome
the interactability of true Bayesian influence and it's specifically that interactability
that motivates the principle is an explanation of how we can get around that problem.
So it's presented in a real sense as something the brain actually does on things that have
been encoded but at this point the focus is still on the brain.
Then with reasons like to be known at paper, it's appreciated that if all organisms need
to avoid spying situations then this needs to be a theory of more than just the brain.
So it gets generalized sort of non-year old operations of living organisms.
And following that around I guess the turn of this decade, there's been an increasing
question of how much the free energy principle actually explains.
And so we find people like Epiphobe arguing that it's best understood not as an explanation
of how biology systems persist but rather as a way of redescribing these systems as
a conceptual and mathematical analysis of self-organization.
Now in generalizing away from the specific structure of the brain fire, the marker blanket,
it turns out that free energy minimization can be predicated to both living and non-living
systems alike.
And as Balthieri, Buckley and Brunerberg described in the case of the war governor, but it's
also been applied to rocks or oil drops or as Friston puts it in a particular physics
to everything that is distinguishable from anything else in a statistical sense.
So there's some disagreement as to whether this principle can actually define self-organization
or whether it's just an idealized model that might be useful in studying self-organizing
systems.
And most recently, Tricia Palacios and Matteo Colombo have addressed this board a scope
of the free energy principle and argued that few concrete physical systems living or otherwise
actually strictly correspond to the descriptions that they give to the system.
As a result of the issues with finding concrete systems to which the free energy principle
actually applies, some more recent papers, both more critical and more sportive, have
begun to suggest that it's best to attach from particular applications and approach
as Matt Andrews puts it, as a formal modeling structure, a preemopathical formulas in whose
applicability in a particular type of system is a separate question to be turned empirically
on a case-by-case basis, which kind of brings us back to a principle that can influence.
A lot of things can be described as doing free energy minimization.
In some cases, they can be described as active and thoroughs, but the utility of this building
models that identify the mechanistic workings of the system get determined on a case-by-case
basis.
And my impression is the general view of the free energy principle is perhaps more towards
a more bottom right quadrant, and I think that makes a lot of sense, the more general
the description is, the less it will be able to explain the specifics that differentiate
one type of thing from another.
Once you've sort of cordoned the modeling structure off like this, what I want to ask
is how well, as a modeling structure, does it actually capture properties of living systems?
A set thing that some idealization is inevitable in a model, I won't ask how good an idealization
is, and does it extract how many important features, and how important are the features
that it discards or distorts?
Okay, so first of all, what does it look like to apply the free energy principle's living
system and systems, and why would we think there is an inscription of that behavior?
Well, the stems from the idea that the minimization of long-runs prysal, which is the minimization
of long-run free energy approximates, describes something that is essential to biological
existence, and the strongest statements of this idea come from those papers that I showed
on the top left-hand side of my graph, where we find Frank said by Kotlin-Pristan stating
that the free energy principle is a mathematical formulation that explains from first principles
the characteristics of biological systems that are able to resist decay and persist
every time.
And as Pristan explains this, that's because the defining characteristic of biological
systems is that they maintain their states and form in the face of a constantly changing
environment.
This maintenance of order distinguishes biological from other self-organizing systems.
Indeed, the physiology of biological systems can be reduced almost entirely to their heavy
states.
So reasons to think that the free energy principle latches onto key features of organisms that
surprise minimizes is that the surprise minimization describes many states.
And this definition of biological stability in terms of maintaining the stability of key
variables is a move that the free energy principle shares with the early side of the
system, namely across ASCII.
And I think this is a key idealizing move in both approaches.
And just to flag up, one appeal of this from the perspective of cognitive science is that
if homeostasis is this basic goal or imperative for living systems and if free energy minimization
provides a good description of this, then the way it translates homeostasis into inferentialist
terms could potentially allow us to connect that kind of basic or basic intentionality
up to ground the intentionality of more complex cognitive operations.
And well, so organisms are homeostatic, but why think that that's a fundamental principle?
And in particular, as the free energy principle has been extended to non-biological systems,
why think that this can describe all kinds of forms of existence?
Well, I think the idea here, and it's an idea that's also in ASCII, is that homeostasis
is kind of just a biologically specific term for a general requirement of all sorts of
existence, namely that any system that we think of as existing over time must have something
very measurable characteristic states or sets of states so that we can re-identify it over time.
And as the free energy principle adds to this, it must have some sort of boundary so that we can
demarcate what is the system and what is its environmental backdrop.
And first thing gives a neat sort of summary of this definition of a system and that there's a landscape
chapter where he says, technically, we are saying that if things exist, they must possess market
ground blankets, i.e. possess boundaries or surfaces that demarcate them from other things,
and they must be ergodic at least over a period of time.
With this existential dyad and an ergodic marker blanket, everything of interest about life in the
universe can be derived. So I take this definition of a system or one of the many roughly equivalent
variations of it to be the cost kind of idealizing things made under the free energy framework.
We're at this definition of a system and the necessity of surprise or minimization
for continued existence of it.
So the requirement of ergodicity has, I think, more recently aroused some controversy as it would entail
the strong claim that a system's long-term trajectory is going to be sensitive to its initial conditions
so that the average state of any one instance of that system over time will be the same as the average
state of multiple different instances captured at a single point.
A classic example being the idealized model of gas, molecules, and a box where after the system is
involved for enough time, the trajectory of a single particle will fill out the same reason of space
as a snapshot of lots of those particles at a single point.
And while ergodicity is often assumed in idealized models, as Palacios and Colombo argue,
it's quite hard to find concrete systems that can satisfy it in a time scale which will be
exponentially useful at least. And moreover, it ignores the kind of differentiation in individual
trajectories that seem to characterize biological systems specifically.
As such a more recent example, the emphasis is going to be to the requirement of the system
converges to some steady state. That allows different iterations of the system with different
initial conditions to settle into different average behaviors, with the requirement only being
that they eventually settle into some stable regime, not necessarily the same one.
So this was a nice illustration from the Guinness World Records spinning cops, and after some wandering
around, each of these spinning cops eventually settled into a kind of stable oscillation around a fixed
region of their disk. So they settle into a steady state. But due to the different initial spins,
they do not all end up orbiting the same region of their disk.
And that really works because the furniture principle, as I understand it, doesn't necessarily need
the relation between the average every time and the ensemble average over multiple different
iterations that ergodicity gives you. What it needs is that a particular iteration has some
stable average behavior over time on its own. That is to say the system must exhibit the
property of having a stationary probability distribution over most likely states.
And steady state gets you that kind of, the most likely state for that system now is the most
likely state for in the future, and ongoing as long as it exists.
Sorry, my computer's struggling a bit to keep up with the slides.
So to say that this is a steady state, is to say that it must be a steady state.
If we interpret this as a principle as a description of an existential imperative,
rather than just an idealization.
And this implies that as Kristen puts it, things only exist over time scales within which
they're ergodic, or in a sub-dated firm, or things only exist over time scales
within which they can be described by a stationary probability distribution.
To say that a system must remain at the same steady state in order to continue its existence,
it's time to understand that it cannot spend this proportionate amount of time in high
surprising states, but in states it hasn't regularly visited before, because if it did,
that would violate that requirement and change the probability distribution of a state that it's
likely to be in, which, under the free energy principle, would correspond to that system
ceasing to exist. So if we take the temperature of the human body, this can fluctuate over the course of the day,
it always stays within a set range that can be described by a fixed probability distribution.
If you go outside this range, it will be to end your state with high surprising, and to stay there
will be incompatible with the probability distribution that is described in a slightly body temperature
up till now. The breakdown of that probability distribution
corresponds to the phase transition of the cool depth. So this stable probability distribution
has two phases. One of them is a description of the characteristic states in the system
that allows us to re-identify ahead of time. The other is that it gives
us a target, one of the key components for active influence, which is a target probability density
to be approximated in variational free energy minimization.
And it's this stationary distribution entailed by the system's stable dynamics that allows
us to talk about it as instantiating or tailing the generator model.
So I want to be careful. I'm not saying that stationarity in an appointment in a system that's steady state is
necessary for active influence models, but that in the current formulation of the free energy principle, it's this idea
of what defines a system is used to guarantee that all systems can be described as
surprising minimizes.
