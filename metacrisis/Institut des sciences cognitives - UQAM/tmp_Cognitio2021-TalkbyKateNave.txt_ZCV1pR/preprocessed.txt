What I have been working on is the free energy principle as a theory of living systems.
My hope is that thinking through what sense free energy minimization does or does not
provide a sort of necessary principle for a single organism can be informative for thinking
about the kind of collective dynamics that we should or should not expect to emerge when
they interact.
OK, so as everyone at this conference likely knows, active inference has proven to be a
strikingly useful modeling tool that has been applied across a wide range of phenomena across
temples, spatial scales, neural dynamics to, that sort of slide, to interpersonal and collective
coordination.
But what I want to ask is why does it work so well?
And I think that's a particularly important question to ask when it comes to theories
that are being applied in the cognitive and social domain.
So Carl Friston nicely laid out the kind of two different ways that you could approach
active inference in a chapter on the book Clarke and his critics called Beyond the Desert
Landscape.
And one way he suggests you can approach active inference is by what he terms the high-ride.
So the slides are being kind of strained for a second.
OK, for high-ride, you can just treat it as a description of a computational function
that allows us to perform a particular task, namely controlling both the environment and
yourself to achieve a mutually stable state.
And the key insight would be that we can re-describe this overall control task in inferential terms
as minimizing the surprising of the state of the self-environment system.
That is, inferring some limited subset of states that can be stably maintained and so
easily predicted.
We can't do this directly for two reasons.
The state of the environment isn't directly observable, and true survival is typically
intractable to compute.
So free energy minimization serves as a computational level description of an approximation to that
function, achieved by minimizing the difference between the state of observable consequences
and a prediction about these drawn from a simplified set of more tractable probability
distributions, the variational model, which internal states encode.
And the details can be fleshed out by the selection of the particular class of variational
models and any fleshed out further in specific algorithmic implementations of that, like
the big process.
But whether or not a particularly cognitive, behavioral, social phenomena will be one model
by active inference will depend on whether it has the performance of that control task
as its function, which is something that we determine on a case-by-case basis.
The other way of approaching active inference is via what he calls a high-road, which as
Risen describes it, stands in for a top-down approach that starts by asking fundamental
questions about the necessary properties things must possess if they exist.
On this approach, surprising minimization is not just a way of describing a computational
task that we might sometimes need to approximate, it's a necessary principle, a theory of everything
that can be distinguished from other things in a statistical sense.
One that ties existence to one simple imperative, avoid surprises and you will last longer,
which as Preston puts it, is a principle so basic there is no need to recourse to any
other principles.
And what can make this claim sometimes a bit difficult to interpret is over the past
decade or so, I think the way the free energy principle has got fleshed out has changed
significantly.
Both in terms of the scope of the different types of systems that are claimed to fall
under this imperative and in terms of the claims made as to its explanatory power and
ontological fidelity.
This is a small but a kind of representative sample of literature on this and what I think
is interesting is that if you look at the years of publication, it looks like there's
been a link to me along both these axes in terms of increased scope going roughly but
not unerringly, hand in hand with a more modest assessment of frame once fidelity and explanatory
power.
So in the earliest introductions at the sort of top left hand side of that little graph
thing, it's presented as an explanation of how a brain-equipped creature can overcome
the interactability of true Bayesian influence and it's specifically that interactability
that motivates the principle is an explanation of how we can get around that problem.
So it's presented in a real sense as something the brain actually does on things that have
been encoded but at this point the focus is still on the brain.
Then with reasons like to be known at paper, it's appreciated that if all organisms need
to avoid spying situations then this needs to be a theory of more than just the brain.
So it gets generalized sort of non-year old operations of living organisms.
And following that around I guess the turn of this decade, there's been an increasing
question of how much the free energy principle actually explains.
And so we find people like Epiphobe arguing that it's best understood not as an explanation
of how biology systems persist but rather as a way of redescribing these systems as
a conceptual and mathematical analysis of self-organization.
Now in generalizing away from the specific structure of the brain fire, the marker blanket,
it turns out that free energy minimization can be predicated to both living and non-living
systems alike.
And as Balthieri, Buckley and Brunerberg described in the case of the war governor, but it's
also been applied to rocks or oil drops or as Friston puts it in a particular physics
to everything that is distinguishable from anything else in a statistical sense.
So there's some disagreement as to whether this principle can actually define self-organization
or whether it's just an idealized model that might be useful in studying self-organizing
systems.
And most recently, Tricia Palacios and Matteo Colombo have addressed this board a scope
of the free energy principle and argued that few concrete physical systems living or otherwise
actually strictly correspond to the descriptions that they give to the system.
As a result of the issues with finding concrete systems to which the free energy principle
actually applies, some more recent papers, both more critical and more sportive, have
begun to suggest that it's best to attach from particular applications and approach
as Matt Andrews puts it, as a formal modeling structure, a preemopathical formulas in whose
applicability in a particular type of system is a separate question to be turned empirically
on a case-by-case basis, which kind of brings us back to a principle that can influence.
A lot of things can be described as doing free energy minimization.
In some cases, they can be described as active and thoroughs, but the utility of this building
models that identify the mechanistic workings of the system get determined on a case-by-case
basis.
And my impression is the general view of the free energy principle is perhaps more towards
a more bottom right quadrant, and I think that makes a lot of sense, the more general
the description is, the less it will be able to explain the specifics that differentiate
one type of thing from another.
Once you've sort of cordoned the modeling structure off like this, what I want to ask
is how well, as a modeling structure, does it actually capture properties of living systems?
A set thing that some idealization is inevitable in a model, I won't ask how good an idealization
is, and does it extract how many important features, and how important are the features
that it discards or distorts?
Okay, so first of all, what does it look like to apply the free energy principle's living
system and systems, and why would we think there is an inscription of that behavior?
Well, the stems from the idea that the minimization of long-runs prysal, which is the minimization
of long-run free energy approximates, describes something that is essential to biological
existence, and the strongest statements of this idea come from those papers that I showed
on the top left-hand side of my graph, where we find Frank said by Kotlin-Pristan stating
that the free energy principle is a mathematical formulation that explains from first principles
the characteristics of biological systems that are able to resist decay and persist
every time.
And as Pristan explains this, that's because the defining characteristic of biological
systems is that they maintain their states and form in the face of a constantly changing
environment.
This maintenance of order distinguishes biological from other self-organizing systems.
Indeed, the physiology of biological systems can be reduced almost entirely to their heavy
states.
So reasons to think that the free energy principle latches onto key features of organisms that
surprise minimizes is that the surprise minimization describes many states.
And this definition of biological stability in terms of maintaining the stability of key
variables is a move that the free energy principle shares with the early side of the
system, namely across ASCII.
And I think this is a key idealizing move in both approaches.
And just to flag up, one appeal of this from the perspective of cognitive science is that
if homeostasis is this basic goal or imperative for living systems and if free energy minimization
provides a good description of this, then the way it translates homeostasis into inferentialist
terms could potentially allow us to connect that kind of basic or basic intentionality
up to ground the intentionality of more complex cognitive operations.
And well, so organisms are homeostatic, but why think that that's a fundamental principle?
And in particular, as the free energy principle has been extended to non-biological systems,
why think that this can describe all kinds of forms of existence?
Well, I think the idea here, and it's an idea that's also in ASCII, is that homeostasis
is kind of just a biologically specific term for a general requirement of all sorts of
existence, namely that any system that we think of as existing over time must have something
very measurable characteristic states or sets of states so that we can re-identify it over time.
And as the free energy principle adds to this, it must have some sort of boundary so that we can
demarcate what is the system and what is its environmental backdrop.
And first thing gives a neat sort of summary of this definition of a system and that there's a landscape
chapter where he says, technically, we are saying that if things exist, they must possess market
ground blankets, i.e. possess boundaries or surfaces that demarcate them from other things,
and they must be ergodic at least over a period of time.
With this existential dyad and an ergodic marker blanket, everything of interest about life in the
universe can be derived. So I take this definition of a system or one of the many roughly equivalent
variations of it to be the cost kind of idealizing things made under the free energy framework.
We're at this definition of a system and the necessity of surprise or minimization
for continued existence of it.
So the requirement of ergodicity has, I think, more recently aroused some controversy as it would entail
the strong claim that a system's long-term trajectory is going to be sensitive to its initial conditions
so that the average state of any one instance of that system over time will be the same as the average
state of multiple different instances captured at a single point.
A classic example being the idealized model of gas, molecules, and a box where after the system is
involved for enough time, the trajectory of a single particle will fill out the same reason of space
as a snapshot of lots of those particles at a single point.
And while ergodicity is often assumed in idealized models, as Palacios and Colombo argue,
it's quite hard to find concrete systems that can satisfy it in a time scale which will be
exponentially useful at least. And moreover, it ignores the kind of differentiation in individual
trajectories that seem to characterize biological systems specifically.
As such a more recent example, the emphasis is going to be to the requirement of the system
converges to some steady state. That allows different iterations of the system with different
initial conditions to settle into different average behaviors, with the requirement only being
that they eventually settle into some stable regime, not necessarily the same one.
So this was a nice illustration from the Guinness World Records spinning cops, and after some wandering
around, each of these spinning cops eventually settled into a kind of stable oscillation around a fixed
region of their disk. So they settle into a steady state. But due to the different initial spins,
they do not all end up orbiting the same region of their disk.
And that really works because the furniture principle, as I understand it, doesn't necessarily need
the relation between the average every time and the ensemble average over multiple different
iterations that ergodicity gives you. What it needs is that a particular iteration has some
stable average behavior over time on its own. That is to say the system must exhibit the
property of having a stationary probability distribution over most likely states.
And steady state gets you that kind of, the most likely state for that system now is the most
likely state for in the future, and ongoing as long as it exists.
Sorry, my computer's struggling a bit to keep up with the slides.
So to say that this is a steady state, is to say that it must be a steady state.
If we interpret this as a principle as a description of an existential imperative,
rather than just an idealization.
And this implies that as Kristen puts it, things only exist over time scales within which
they're ergodic, or in a sub-dated firm, or things only exist over time scales
within which they can be described by a stationary probability distribution.
To say that a system must remain at the same steady state in order to continue its existence,
it's time to understand that it cannot spend this proportionate amount of time in high
surprising states, but in states it hasn't regularly visited before, because if it did,
that would violate that requirement and change the probability distribution of a state that it's
likely to be in, which, under the free energy principle, would correspond to that system
ceasing to exist. So if we take the temperature of the human body, this can fluctuate over the course of the day,
it always stays within a set range that can be described by a fixed probability distribution.
If you go outside this range, it will be to end your state with high surprising, and to stay there
will be incompatible with the probability distribution that is described in a slightly body temperature
up till now. The breakdown of that probability distribution
corresponds to the phase transition of the cool depth. So this stable probability distribution
has two phases. One of them is a description of the characteristic states in the system
that allows us to re-identify ahead of time. The other is that it gives
us a target, one of the key components for active influence, which is a target probability density
to be approximated in variational free energy minimization.
And it's this stationary distribution entailed by the system's stable dynamics that allows
us to talk about it as instantiating or tailing the generator model.
So I want to be careful. I'm not saying that stationarity in an appointment in a system that's steady state is
necessary for active influence models, but that in the current formulation of the free energy principle, it's this idea
of what defines a system is used to guarantee that all systems can be described as
surprising minimizes.
So I want to emphasize that the requirement of stationarity in the probability distribution over
a most likely state does not preclude the system changing.
There can be various alterations and changes in the development towards a steady
state. There can be temporary fluctuations away from the most likely state as long as
they're counteracted by a surprise or minimizing flow back to more likely states
to preserve probability distribution.
So you can occasionally visit states you haven't visited before, you just can't stay there too long
because then it becomes so long that I can describe them as surprising and a probability distribution changes.
There can also be cycles and periodic changes around a set of equivalently likely states
which gets described by this sort of optional solenoidal component, the system's flow, which is that curved
arrow around that landscape.
And it's been suggested that it can be used to formalize a system that's at
non-equilibrium and might be what differentiates living from non-living systems.
So we can have all sorts of wandering around and temporary excursions
from that peak of most likely state. But as Friston, Hobson and Moisey describe it,
a personal particle is never off this manifold. The kind of novelty that would correspond
to a permanent phase transition from one stable track from landscape to another amounts
on this formulation of a system as that system ceases to exist.
The dissolution of the attractor and the potential emergence of a new regime corresponds to the depth of the system
as it's failure to adequately deal with environmental perturbations.
Okay, so to get Friston's reasonable table
it's not enough just to have a stable probability distribution. You also need to split a system
into parts so we can talk of internal states as minimizing Friston with respect to
the sensory ones to control external ones. In a case of brain bound
for humanization, we can just take the sensory mode to interface
but to generalize this to all systems involves formalizing the idea of a boundary
for perception and action. This is done by the Markov-Blinder
which decomposes the system into internal, external and blanket variables
based on the requirement that the probability of internal variables being in particular state is independent
of the safe external variables conditional on knowing the blanket states
and vice versa. So the second presumption of the Frenzy framework
is that any system can be factorized in terms of these kind of conditional statistical
dependencies between internal and external states.
And while Markov-Blinder gets some directive graphs like this are typically constructed based on statistical information
not causal information directly, these graphs can be and then the Frenzy framework
typically are presented as certain patterns of causal influence and examples of things that could be
Markov-Blinder gets a physical boundary such as the membrane of a cell or the states and center of the area.
There are some issues with that but I don't want to get into the sort of
the weeds of the links between statistical and causal influence here.
What is important to me is this claim which is that as a consequence
of requiring a fixed Markov-Blanket is the requirement of stable
patterns of interaction between the parts of the overall system and its environment.
So as Tristan puts it, it does not easily accommodate the fact that the particles that constitute a
Markov-Blanket can over time wander away or be exchanged or we need.
I can only call an example here being a blanket state of a counterplane whose constituent particles
molecules of gas are in constant flux. Because of this flux,
Tristan says, a counterplane cannot be a modelist possessing a Markov-Blanket.
So to kind of summarize
I think the two core moves of the Frenzy principle
that get you this requirement of surprise minimization is to describe the system
in form of two sorts of stability. Is there a requirement of stationarity
that the probability density of the states of the system is invariant every time?
Whatever state the system is most likely to be in today is the state it is most likely to be in
as long as it exists. And the requirement of a Markov-Blanket. So the patterns
of interaction between parts of the system are stable enough. We can identify some fixed
subset separating the system into fixed internal and external parts
from this little on that blanket. And there are some
different ways of formulating this but I take the most of them amount to different ways of
surprising these key stability requirements. So what we have is not an
excuse of how a system biological or otherwise avoids death or ceasing to exist
but as I have described it a formal analysis, a potential formal analysis
of what avoiding extinction is so as to how this is achieved can be described by
specific crisis theories. If we define a system in these terms then the minimization
of surprise will indeed be a necessary principle that they continue to exist
every time. Which are some extra requirements that allow you to model
that system as an active influence agent. But what I want to argue is that
while these requirements may characterize stable physical systems relatively well
they're at odds with the very characteristics that distinguish living ones.
So the first issue of this
formulation is that as it applies to living systems is that not all phase transitions
in an organism correspond to that system dying or ceasing to exist. Organisms
typically undergo many transitions from one steady state regime to another ever before so the lifespan.
Examples like embryogenesis and development can be interpreted as stages on the
path towards a steady state that characterizes the adult organism. But even
once adult form has been achieved organisms are liable to argue further phase transitions that don't
on an ordinary understanding amount to their death. And these midlife crises are not
unique to complex highly cognitive approaches like ourselves and nor do they only occur
at the population level of natural selection. Sometimes reference gets made
to a caterpillar turning into a butterfly is an interesting exception to our surprise
and encouraging. But these phase transitions are ubiquitous. There's the cases of short horned
grasshoppers that went over crowded turns locusts or they apparently
have 500 different species of fish that change sex under a variety of different circumstances.
And we might break these down as parts and treat them as the result of competition between different
sub-organisms each with their own steady state behavior. But even the simplest single cell
organism like an E.coli bacterium can deliberately engage in what can
be observed to survive behavior that
causes a transition from one steady state regime to another. Typically it's flailing
around between glucose molecules to keep its intake stable
could be characterized by the frame principle of steady state formants. When glucose levels drop
and lactose levels rise it can switch its behavior to begin metabolizing lactose instead.
That means a change in the probability distribution characterizing the E.coli's behavior
from a low probability of lactose metabolism to a higher one. And now
the E.coli bacterium looks like it's trying to do is minimize surprise and loss of the lactose intake levels instead.
And so long as lactose remains available, an E.coli bacterium can do very well in its
new regime. So it doesn't seem well characterized as a temporary deviation away from steady state
that needs to be counted. It's this sort of adaptability and phenotypic
plasticity that is not well described by the idea that a system
wanders around if it's attracted. And the analysis of the system's characteristic
states in terms of the stationary probability distribution fails to characterize the continuity
of the organism throughout this transition from one steady state regime to another.
So
that can be made less of a problem if we drop the idea that a stationary
distribution has to characterize the entire existence of the system. And instead
take it to characterize the stability of a temporary phase of the system. An able
process of life is a process of transitioning between steady states.
And we could put that in CUNY in terms to say that the phrase principle might be a good description of how the organism maintains
itself during the normal science phase but not of the paradigm system of that patient.
Still, this leaves perhaps the most interesting properties of organisms unexplained.
As a number of authors have argued, both as criticisms of and independently of the
phrase principle, it's precisely the preservation of a system's identity
who changes in its phase space on the capacity for permanent cumulative change that this
unlocks. It distinguishes the biological from the nearly physical.
An emphasis on this kind of historical change is, as De Palo and Thompson and Bayer recently noted,
the hallmark of the inactive approach to the system. And some of the potential limits of
inequality between these and the phrase principle. Moreover, just as not all phase transitions
are equal-depth, neither are all steady states equal. And the fact that a state
can be stably maintained does not make it optimal that we even buy it.
When a bacterium like E. coli transitions to lactose metabolism in a lactose-raised environment,
it seems like a pretty optimal new steady state. But an alternative response
to new cheap deficiency in other species of bacteria is to enter a dormant
endosporesate. And that dormant state can be maintained for millions of years. While we might
have discovered that E. coli, Charon, Garm's, lactose is thriving in its knees, that doesn't seem
apt for the endospore state. And similarly, the transition from life to
being frozen in psorogenesis is a transition between steady states. But the likely
coming of vegetarianism in this new steady state isn't a viable one for organisms like you and me.
So what we want, and what is not there in the phrase principle, is an explanation of which
steady state is conducive to organisms' survival and which others are not, and why.
So the second stability requirement
is that of a stable boundary. And if we approach that as an idealization,
and I apologize for the A-level biology diagrams here, I just thought they were quite nice and simple.
If we approach that as an idealization, like the requirement of stationarity,
it wouldn't require that the particles constituting the marker blanket never change.
But they are at least stable on long enough time scales to be treated as invariant.
And that their turnover isn't of significance in understanding the features of the system that we're interested in.
The problem for living systems is that this means ignoring the distinctive
side in which they're not only a continuous material turnover on time scales shorter than
the life span, but the ways in which that kind of material turnover is actually
constitutive of the kind of behavior that we see to explain.
For specific examples, we can take the cell membrane, which is often presented as the archetypal example
of a stable marker blanket between the interior of the system
and its exterior. This may be more stable than the candle flame,
but in all cells it will still be in a kind of molecular flux throughout the cell's life cycle.
There's the continual endosatases and exotases of this membrane
for regeneration, for growth and for particle transport in and out of the cell.
And this destructs in a regeneration of the band, which is not an accidental quirk,
but a constitutive part of how the cell imposes itself. In an example, particularly like
of the solenoid slime mold, the regeneration of this membrane could be key to locomotion
as opposed to taking in the membrane from the trailing edge and then regenerating it back out at the forward edge,
which would result in complete turnover of the entire membrane in the space of just a few minutes.
And that turnover of the membrane actually enables the slime mold to move
that material turnover is part of the behavior.
So as in the case of the candle flame, this flux of molecular parts can't easily be
accommodated, and a model that requires fixed relations between stable parts.
But the point isn't just that the Fray and New Principles definition of a system abstracts away
from some specific behaviors of living systems, but that these behaviors
are the way in which biological systems somehow preserved through this material turnover
that are, I'd argue, exactly the properties that are key to understanding the unique
sub-organism behavior of life.
So material turnover is not just a coincidental
quirk of some organismic operations and parts like the membrane, but
constitutive of living systems as metabolic systems that need to continuously regenerate themselves.
Living systems aren't only open and out of equilibrium with the
environment, not just amenable to the possibility of incorporating flows
of matter and energy. They are systems that depend on such a way for their ongoing existence.
At this point about the priority of metabolic turnover in characterizing living systems,
I think that's made by the philosopher Hans Jonas around 70 years ago,
and this could mean he could have asked me cybernetic formulations to be able to just frame, as I said,
in very similar terms with Fray and New Principles. And as Hans Jonas argues, organisms are
distinguished by standing in a kind of needful freedom to matter, both liberated from
any specific physical base, but in need of continual flows to reconstitute themselves.
The Fray and New Principles description of a system describes a stable
coupling between feedback mechanisms very nicely, but as Vanishing Kirschhoff,
who, I don't know if that's the name, but Vanishing Kirschhoff, Michael Gerger, Thomas Van Essen,
and Michael Kirschhoff, there we go, have recently noted, it's a modelling structure
that's not well suited to capturing this precariousness. And this is not due to
the requirement of a stable marker blanket, but a more basic consequence of a general modelling framework
that describes systems in terms of variables and influences of quantity to them to maintain a stable set of states.
And this is, all of this isn't to say that there's no
way to characterize some sort of biological identity that remains invariant
to the chains of both typical states of material substrate.
Pre-theoretically, we do it from the time. It's just that relations of influence between
atemporal fixed networks and variables are not able to capture it.
It's a model of macabolic turnover, as the theorem in biology is Matteo Massio,
Harvard and Reno, and Mayo Montevil describe, and they're building the work of strict government.
What is needed is not just relations of statistical or causal influence between variables,
but relationships of existential dependence between precarious constraints
and catalysts, that both depend on and enable the processes of production that regenerate them.
In altruistic and activist terms, an organism is not just an operationally
closed system, defined by a set of cycle of operations to a fixed set of characteristic states,
but an organizationally closed one. Here, its component
is not constrained by a characteristic state or physical instantiation, but only the role
it plays in the year of all network. And as long as that role of constraint or regeneration is maintained,
as long as its closure is preserved, the parts of the network may exchange and adapt
and evolve into whatever variety, new forms, states, and characteristic behaviors
are compatible with that closure.
So, completely. For energy minimization,
it's a necessary condition only for systems defined by the stability of the probability
distribution over the states and the stability of interactions between the parts.
Organisms may continually meet these requirements at points, but
neither is necessary. The very thing that distinguishes them
is their freedom from and ability to persist through transformations
of both. Their relationship to the environment does not reduce to responses that
preserve a stable coupling, but an asymmetric dependence upon
the environment gives the continual flows of matter and energy.
So, it is not just that the Fraynida principle is too general to describe living systems and needs supplementation
by additional requirements, such as temporal depth, mason length, or non-equilibrium,
but that the features it highlights are the ones that living systems are actually defined by the violation
of, and so it may ironically be much better at describing non-living physical objects,
a stable coupled systems, and biological organisms.
So, if the most basic goal of an organism is to preserve its continued existence,
then this protonormativity, if you want to treat it in a locus of protonormativity,
is not well formalized by, and can be directly opposed to, the preservation of a steady state
and an upper blanket. Fraynida humanisation itself cannot describe
the extensual imperative that organisms must follow.
But despite this, I'm not saying that modelling systems, both living and otherwise,
as Fraynida humanisers, isn't useful. As we've seen, like an extensive amount of
work in active inference, it demonstrates otherwise. A lot of organismal behaviour,
although far from all of it, is describable as hermousases of steady state.
But if these kinds of stability aren't necessary survival, or survival, then why would this
work so well? Well, as the Seville-Longo and Stuart Kaufman, among others,
argue, it's this openness of organisationally-closed networks,
the cumulative historical chains, and the difficulty of pinning down and identifying
invariant features that can make it particularly difficult to predict their evolution.
If not impossible, then at least inordinately more difficult than for physical systems.
But it's not only us as external observers that want to predict
what will be compatible with a system's ongoing existence. Living systems themselves
need to do this. If Fraynida humanisation is an idealisation,
and it still makes sense as one that we, as living systems, can actually implicitly use
to estimate what will be good for us in terms of a kind of conservative stick
with what you know and strategy. As a living system, your identity may only be
contingently associated with a particular physical form and pattern of behaviour.
But given that this particular institution has worked for you up till now, and served your
mind to what needs it till now, it's able to maintain it in the hope that it will continue to do so in the future.
It's not a bad strategy. It's just not a software necessity.
Okay, and that is my talk.
Thank you so much for listening.
That was absolutely brilliant.
I'm going to let Kier take the questions now.
Thank you, Kate, for the presentation. I find it reassuring that
Max Infanterist takes seriously some of the best
inactivist points, which you did, so thank you.
I have the whole question, which is
so, in your opinion,
the Fraynida principle is a potentially useful idealisation
for a living system, if I understand.
Do you think it is explanatory if it is in what sense
and of what?
So I think basically it's useful for modelling
how you respond to modelling
response to perturbations to remain stable.
But not useful for modelling adaptations when it comes to
a strategy or a particular way of responding to perturbations.
They all go working for you and they need to move to a new regime. And it's not useful for modelling,
I don't think, metabolism, which I kind of take it to be
what homeostasis, what the maintenance of stability subserves,
so that metabolism is prior, homeostasis is for metabolism,
and surprise minimisation, described by Fraynida minimisation
can describe what being homeostatic looks like.
But it's not going to get at the difference between the homeostasis of a biological system where
that's important for the preservation of existence in a different way from which
stability is important for the preservation of other physical systems.
If you could just repeat his question very briefly to us so that we know what
you're answering to. Sure, so I guess the kind of key thing that I'd like to point out was the
question about, we can describe outer body systems
using the Fraynida minimisation framework and how do I
think that whether or not that's an observer-dependent kind of projection
and the difference between when it's an observer-dependent projection and when something's really
doing, whether or not it makes sense that something's really minimising Fraynida
versus it's useful for us to describe it that way. Is that
even a one key point?
So I think that's exactly what I'm concerned about with this talk, because
I didn't talk much about handjournism, but I think for me
to start talking about things as having intentions as trying to do things
as being agents, that is all going to be grounded in
a system that has the precarity of being a metabolic system, a system that
whose continued existence depends on its own activity
in a way that I think metabolism characterises. I don't think Fraynida minimisation does
because you can describe all sorts of systems Fraynida minimises that don't have
that kind of precarious relationship to their own activity.
So the difference from what I think would be that you can talk about
a system trying to do things, trying to preserve itself, following imperatives
and all that kind of thing, only once it's a metabolic system.
And then the imperative that it follows might be to minimise surprise, but it's
only genuinely trying to do that because its existence depends on it
in a way that a metabolic system does, and in a way that things like couple pendulums
go. Are there any other questions?
Stephen?
Thanks, yeah, I was just going to ask
what you think about the difference between a thing and a system
because a thing doesn't have to
make a system per se.
And also where
using
variations
in the process of minimising free energy might be enough to get information
to do stuff, even if it's not.
The stuff isn't always about directly going to a minimum
of free energy at that time. I wonder what your thoughts are on
the difference between a thing and a system where maybe it's important to distinguish
and where it's not always about minimising but using
the process of minimising to get information.
Yeah, so in terms of the distinction of a thing and a system
that's not something I've been thinking about, but in terms of
when you think about how you describe systems, you sort of partially describe them in terms of their
dynamics as a process and partially in terms of their constituents
and where their system is defined as a process or
the thing that realises the process is something that I've
been thinking about. I don't really have, I think
a whole lot to say about it, except that
I think when you're characterising properties like closure
or self-production, what you're really characterising is a process.
So to say that a system is closed in a particular
way is never really to say that the thing that realises that process
is closed in that way. So when you say that, for example, a living
system is one that is, or an operation closed system, is one
closed to petivations that would take it outside its characteristic set of states,
it's not that the thing that you're describing is isolated or invulnerable to those,
it's that the thing only realises that particular system
as long as that's true of it. So I guess I'm
interested in the idea that when we talk about things as systems, we're talking about the more as processes
than as substance, I suppose, as physical
realises, but I don't have much more to say on it than that.
I can't remember the other part of your question.
Yeah, I mean the other part is about whether minimisation
has to be the main
route to everything. There might be a minimisation process, but in minimising
you might just get a route of variations
which might then give you a route to something else as opposed to it being
I suppose that starts to bring in the process theories a bit more,
but you're focused mostly on the focus
on minimising, isn't it? I'm getting to a minimum of free energy.
Would you say that what if it's not always
it's just using that process to do something else?
Is that what you mean by metabolism?
Yeah, I think that's what I want to say that
if we think of maintaining
a characteristic set of states as definitive of
some physical object existing over time,
then that can be described by minimisation of surprise level, minimisation
of energy as an approximation, but when an organism does that
that's for something else, that's for
maintaining the kind of intake that it needs to support its metabolism
So yeah, it's like
a useful tool to achieve a different goal. It's not that the minimisation
of energy is the fundamental imperative,
it's sort of like a useful way to do something that's what you really, really do want to be doing.
Nice, thank you, that helps.
Anyone else?
If there are no more questions
we're about at the end of time.
Thank you, Kay. It was a great talk.
