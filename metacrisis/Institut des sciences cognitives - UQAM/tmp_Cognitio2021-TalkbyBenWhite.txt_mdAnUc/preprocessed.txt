to the second day of Cognitiu 2021. You're very welcome and very happy to have you here
with us today. We have a very long, interesting day of very cool talks. The first one is going
to be presented by Daphne Demakas. She's a developer at Nested Minds Network and a Ben White with
a Leverheim funded PhD student at the University of Sussex and Mark Miller, who is an assistant
professor at the Center for Human Nature, Artificial Intelligence and Neuroscience at
Hokkaido University in Japan. The presentation is going to be given today by both Ben and Mark.
The title of the presentation is Artificial Empathy, Active Inference and New Collective
Intelligence. Before I give you the floor, Ben, I just want to make a note that at the end of the
talk, those of you who would like to ask any questions or comments, please do come to the
Q&A, which is here on the left, where it is marked as Q&A. Just come and line up and when you want
to ask a question, just make sure that you are standing on the circle, because otherwise you
won't be able to be heard. So just on that note, I'll give the floor to Ben. The floor is yours.
Thanks, Ines. So I just want to make sure, can everybody see my slide and hear me okay? Good,
good. Okay, so yeah, I think a good morning is in order to most people listen to this. I just want
to say thank you for having me. And it's important for me to say before I get started that this is
very much still a work in progress. And it's been a collaborative project from the beginning. So
myself and Mark Miller are carrying out this work with Daphne Demeckis and also Kat Snari
Mihara, who is at Hokkaido with Mark Miller. And I just want to say Mark Miller is here and he'll be
joining me in the Q&A after the talk. Now, since this is an Active Inference conference and we're
already in day two, I'm actually going to skip over kind of intro to what the framework is and how
it works. And what I want to do instead is tell a kind of a story about how we see the Active
Inference framework influencing new and exciting approaches in designing affective computing
devices. So affective computing is a pretty rapidly growing research program. And it aims to create
computing and robotic devices capable of recognizing, categorizing and responding intelligently to
human emotions. So this obviously draws on quite a wide interdisciplinary body of research,
including neuroscience, psychology, philosophy, computer science and engineering. And it's also
important to say that the term affective computing itself was coined by Rosalind Picard, who is an
MIT professor and she now runs the MIT affective computing lab. That's Rosalind Picard there with
the cover of her original book. And I want to give some examples of affective computing. Most of
these are from the MIT lab. So research has been looking at a range of different uses for this
kind of technology, including how it might operate in the classroom to a teaching and learning, how
affective interfacing in cars might make for better improved road safety, and how affective
computing could be used for therapeutic interventions like GIBO, which is the little screen and
camera thing you can see at the bottom of that slide. That's actually been used and tested in
dorms of undergraduate students at MIT. And another example is a little yellow guy on the blue
background. That's Wobot as in W-O-E Wobot, who's another therapeutic intervention that uses natural
language processing. So if you want to check out lots of examples of this stuff, then the MIT
affective computing lab has their own website and it's very, very cool. But for now, though, the
majority of the current deployments of affective computing have been in industry, including
sectors like recruitment and marketing. So for example, the company Unilever is currently using
a form of Emotion Recognition Tech to analyze the nonverbal cues and responses of people during
interviews. And then this software decides who they hire and who they don't. And I think Amazon
have recently started using something quite similar as well. Now, because of the increase in
prevalence of this in industry and business, it has attracted quite a lot of criticism both in
terms of its functionality and in terms of some what we think are very well founded ethical
concerns surrounding its use. So people in the know are kind of skeptical about this. So for
example, the insurance company Lemonade recently caused a Twitter outrage when it merely hinted at
the fact that it might start using Emotion Recognition Technology to detect fraudulent claims and
that kind of stuff. So current versions of Emotion Recognition have been shown to have some
really serious shortcomings actually. So some researchers in affective science have claimed
that these systems are just founded on highly questionable science and some badly misguided
assumptions about what emotions are and how they work. Going so far as to label these systems a
kind of neo-frenology. So Lisa Feldman Barrett in particular has been very critical of what she
terms a theory of basic emotion, which underpins most of these systems. So in a nutshell, this
theory just says that humans have six to eight basic emotions and that these emotions are kind of
universally expressed through discrete readable sets of facial expressions. And so in a recent
interview Barrett stated that quote, there are some companies that just continue to claim things
that can't possibly be true. Based on published scientific evidence, our judgment is that these
technologies shouldn't be rolled out and used to make consequential decisions about people's
lives. Now, there are a range of other ethical concerns. So Berber, Birhani has argued that
current systems which are trained on very large data sets are inherently conservative and they
propagate any biases or prejudices that are present in the data that they're trained on. So
this can result in systems which are, for example, exhibiting racist or sexist tendencies and
that lots of examples of this, unfortunately. So emotion recognition systems themselves have
been shown to perform much worse on non-white faces. And there's a really disturbing study by
Lauren Rue, which I've put up on the slide there that confirms this. Moreover, Birhani and others
have recently pointed out that these data sets are now so large that the people that are using
them, the entities that are using them to train their software, don't even know what's in there.
And in a recent study, Birhani found that there was just a range of kind of prejudiced or highly
offensive material in there. And finally, many of these systems are subject to the kind of worries
about opacity and black box kind of worries that afflict quite a lot of AI technology generally.
But clearly, this technology has been used to make quite consequential decisions about people's
lives. And so therefore, we think that it's pretty vital that this technology is at least
underpinned by the strongest theoretical commitments. And so I want to be really clear from the
outset that our goal in this talk is not to justify the wholesale use of affective computing in all
areas. So we're not going to argue or suggest what we suggest here today is going to solve all of
these concerns. And this is really important to say, because even if what we're going to describe
shortly is a promising way forward, and obviously we think it is, and even if theoretically all of
this technology could be perfected, then there might still be very principled reasons to think
very carefully about when it's deployed, how it's deployed and who it affects. So what's the
problem with it? Well, the major problem that we suggest in our paper is that the current paradigm
in affective computing is very problematically disembodied. So while there's a huge buzz today
about building emotional or kind of quote unquote empathic robots, all that boils down to in most
cases is just building artificial artifacts equipped with emotion recognition software coupled
with some form of decision making. So suggesting, for example, like at Unilever, who gets fired, who
gets hired. So in these cases, the emotional or affective side of things is really just very
superficial. So these aspects don't play a significant role in the actual computing processes that
are driving the interactions. So seen in this way, these approaches really are based on a foundation
of seriously outdated cognitive science. Because now we know, of course, and we've known for some
time really that affect cognition, perception, action, these are all dynamically co evolving
together. So for example, Luis Pazoa has argued that the prevalent view of emotions and affect has
been somehow separate from cognition. Having unique neural fingerprints is problematic for a
number of reasons. So he argues that complex cognitive emotional behaviors have their basis
in dynamic coalitions of connected brain regions, none of which should be conceptualized as
specifically affective or cognitive. And moreover, Pazoa emphasizes that these complex cognitive
emotional dynamics are inherently linked with action with how an agent acts in a given environment.
And this intrinsic link between affect and goal directed behavior is something that I will turn
to pretty soon. So if we're taking human emotions and affective interactions, including empathy as
a blueprint for designing affective computing devices, we need to think much more seriously
about these important and complex dynamics. So one way that we think you can do this is to bring
active inference into the picture, which, as we know, integrates all of perception, cognition,
action and affect into one fluid dynamic and embodied system for minimizing prediction error.
So we think that by using active inference as a theoretical basis for active affective computing,
it's going to resort in machines that are functionally more impressive. And it might even offer
avenues for potentially addressing some of the ethical concerns that I mentioned earlier.
So we get two things right off the bat when thinking about introducing active inference into
affective computing. So first, active inference offers a unique shared and transparent computational
currency, which is made up of a suite of theoretical and mathematical tools that's going to allow
theorists, neuroscientists and machine learning researchers to all collaborate very closely on
these issues. And second, it puts action up front in helping to solve our understanding of the
world and each other. So consider just how crucial action is in human interactions. So we're
constantly using actions to elicit responses to probe, to test, to gauge how other people might
be thinking or feeling. So for example, if you're on the subway and you see someone scowling at you,
then you might decide to flash them a quick smile just to see kind of to test what's going on,
because it might be the case that they just have a kind of resting scowl face, or it might mean
that the scowl actually means something and we might then decide to kind of move carriages. So
another example that I know that I've experienced and a lot of other people probably have is being in
a job interview where the people interviewing you are deliberately hiding their emotions to remain
neutral. And this can be quite stressful for us. And so we might kind of modulate our tone of voice
or drop in a joke just to kind of try and get some information about how other people feel about us
in that moment. So so much of what we know about human emotion and how we navigate emotional
situations is the result of this kind of testing and this kind of action. And importantly, this
kind of active testing is central in figuring out all those kind of important contextual cues that
dictate what an action or an expression means. So we do not inflexibly apply patterns from data,
like the current versions of emotion recognition do. We adapt on the fly and we use cues in our
environments and atmosphere to make sense of things happening in the particular moment.
So just consider the stranger scowling at you again, but this time imagine it's taking place on a
hospital ward, where you know that everybody's likely to be in quite a lot of pain. The meanings
of these these conveyance conveyances change can change dramatically depending on the context.
So context really is king and actively probing is so crucial for getting context right in emotional
interactions. And so this is it's exactly this kind of active probing in order to get to better
model and understand the emotions of specific others in specific situations that's kind of
surprisingly lacking in current versions of affective computing. And so this to our minds is
a major stumbling block for the current paradigm. So we already have some fantastic work out by
active inference researchers, including Samuel Vassier and colleagues on how we come to understand
each other through exactly this kind of active process of reducing uncertainty. And so I want to
highlight one example of where we see this gap between humans and disembodied AI in terms of
dealing with context. And that is selective attention. So it's quite a pervasive problem in AI
design that current systems don't tend to look at the same regions as humans do. So the question
researchers are asking is how can we make this search pattern more human like? And how will this
result in more accuracy? So successful selective attention is all about filling gaps in the observer's
knowledge, knowledge about where to look to get the most kind of bang for your buck in terms of
reducing uncertainty. And typically researchers have thought that this is to do with humans just
having a much wider kind of experience set and bigger knowledge base than neural nets. But we
suggest that they may also have better machinery for gleaning through actively predicting,
testing and confirming what matters most in a scene. So the way we build up these models,
as we've been saying, isn't only by passively perceiving, but by continually and actively
probing. And this includes overactions like circuiting and moving, and also internal dynamics
like precision reallocations. So this is one major benefit of a more fully embodied affective device,
one that has available to it kind of manipulatable eyes, necks, heads. It's going to be able to
physically manipulate its own viewpoints just like humans do. And so it will be able to drink in
a higher quantity of relevant details within the physical environment. And in the robotics
community, this is known as active vision, and it's increasingly seen as absolutely vital to
building intelligent systems. So one of the major benefits of applying the active inference framework
to building affective computing devices will be this ability to actively forage for information
to resolve its own uncertainty. And here that uncertainty is about somebody else's emotional
states. So through current approaches to active affective computing, they don't have to do this.
Action driven learning through reduction of uncertainty comes as part and parcel of active
inference. So one corollary benefit worth mentioning, I think is of this much more active approach,
is that devices would actually have the benefit of needing much smaller data sets with shorter
training times, because the device would be able to autonomously select the data that best optimizes
the model in a given context. So this might just be one step toward addressing some of the concerns
that Birhani and others have had about the problems that you kind of inherent very, very much data
sets. But from a practical standpoint, making these additions means that we'll just have to move
beyond building affective computing devices that are basically lumps of plastic or just make use
of camera phones. And so to begin to build properly embodied devices, we'll need humanoids with
sophisticated representation to the face, which are able to act fluidly and quickly adapting in
real times the dynamics of conversation, action, and environment. So kind of speaking of adapting
fluidly in real time, in cognitive robotics today, an important question is how artificial
agents learn their internal models in order to interact efficiently with the world around them
and with other people. We've already talked about how important embodied action is to learning
about other people's models and active inference, but we suggest that affectivity in the form of
prediction aerodynamics may be a key element in motivating and directing this kind of embodied
epistemic foraging in artificial agents, just like it is in humans. So what that means in other
words is what we're suggesting is affective computing devices would ultimately benefit from
having internal affective dynamics of their own. Although many different computational models have
been proposed for building intrinsic motivation into artificial systems in order to generate these
kind of exploratory behaviors and curiosity, existing approaches have agents choose actions
according to some internal measures related to the novelty or the predictability of anticipated
situations. So the main difference between what's offered in active inference and these existing
approaches is that in active inference, this intrinsic motivational architecture is based
on the relationship between prediction error and affect. So according to recent work in active
inference, affect is at least in part a form of second order information within the predictive
system itself. So this second order information informs the system about how well or how poorly
prediction error is being reduced over time relative to expectations. So in short, when we
feel good, we feel good when we do better than expected and we feel bad when we're doing worse
than we expect to at reducing prediction error. And these feelings, these affective feedback
informs us about how things are going relative to those expectations. These second order states
also play an important role on modulating the level of confidence or precision that the system has
in its own actions and beliefs. And of course, this seems to make intuitive sense because when we
do better than expected at reducing error, we feel good. And this good feeling helps to reinforce
our confidence that whatever it was that we were doing, those current action policies and predictions
are good ones to implement again in the future in similar contexts. So in this way, aerodynamics
serves to keep an agent flexibly and sensitively tuned to opportunities for success within
environments and as our skills and abilities evolve over time. And notice here that agents
outfitted with a sensitivity to aerodynamics will naturally be curious. So they're going to be drawn
to situations where the most error can be reduced reliably because that's where the best feelings
are going to be found. And those places tend to be at the edge of one's familiarity. So neither too
well known where there's going to be too little error, nor too complex where there's too much error
and we become frustrated. So crucially, aerodynamics play a role in helping to direct and enhance
learning. So prediction error and its reduction rates signal the expectations about the learnability
of particular situations and helps to guide attention and prioritize learning, making the
updating of our models much more efficient. So we have already seen this sort of optimal
error engagement show up in curious robots. So Roy Dyer and Smith is a good source here where
they train robots to seek out optimal levels of complexity, because that's where the most
learning can take place. And specifically, active inference approaches have begun to be used in real
world robotics. And there's some really, really great proof of concept in the work of Guido,
Skelasi, Bruno Laura, and Alejandra, Syria, who have actually built robotic systems using this
kind of internal machinery of prediction aerodynamics. And indeed, their work has shown how robots
equipped with aerodynamics are better able to manage uncertainty by fluidly selecting adaptive
actions in an environment when compared to more traditional approaches. So these artificial
agents that are equipped with internal aerodynamics are better able to learn and then autonomously
select the proper prediction error strategy in a given situation by allowing an agent's
valence, which is that second order information, to weigh and motivate the selection of whatever
might be the most suitable behavior. So this work also showed that aerodynamics provided
a way for artificial agents to navigate the temporal aspects of goal selection,
which is to say that when to engage with a goal, how long to persevere, how long to persevere when
things aren't going right before quitting, and so on and so on. So notice that affect here is
intrinsically related to goals arising when differences between expected and actual rates
of progress occur. So to give a kind of cute real world example, when my niece tries to reach the
sweets in the cupboard and she fails, she might persevere for a given amount of time. She might
see if first there's a way that she can use a stool to get a leg up or something, but when this
strategy fails, inevitably the frustration builds and she might start to cry, at which point she's
trying to rope someone else into her crime and steal in the sweets. And when this fails, she
might ostensibly abandon the mission and run to her mum for a cuddle, which kind of viewed through
this kind of aerodynamics framework, is her engaging emotionally to potentially recruit
someone else into getting the sweets for her. So we suggest in our paper that by introducing these
aerodynamics into effective computing devices, we'd see devices not only motivated to flexibly
and curiously implement new policies for action, which in this case would mean to learn more about
other people's emotional states, but biomimetic devices implementing very similar kinds of
affective motivational mechanisms that we see in humans. So this new wave of affective computing
devices would not only recognize and respond to human emotions more efficiently, but it would
actually have in some sense an internal emotional affective structure of its own.
So lastly and most interestingly and definitely most speculatively, we want to explore the
possibility that these kinds of devices now equipped with this internal proactively expressive
body and something of its own affectivity might be able to enjoy the kind of model
synchrony with their users that we see emerging between in the case of human interactions.
So recent work in active inference on communication dynamics highlights this kind of
synchronization as part of coming to understand one another. So in their paper a duet for one,
Carl Friston and Christopher Frith have used simulations of songbirds to show that a generalized
and this is a quote from the paper, generalized synchrony is an emergent property of coupling
active inference systems that are trying to predict each other. So in very rough terms what they
demonstrate is that according to active inference, meaningful communication between agents requires
that these agents are sufficiently able to model one another and then model the other person model
in them and so on and so forth in an infinite regress. And agents that are able to do this by
making and testing predictions can come to synchronize their internal generative models.
So as noted in the paper by Daphne Demeckis, Friston and Parr quote, in the context of
emotion recognition, this demonstrates the possibility of synchronizing the internal states
of the device and its user, such that the device and user are both attending to an attenuating
sensory information in order to properly cause and predict each other's emotional states.
So I suggest that everybody that's interested in this kind of model synchrony checks out
the two papers I just mentioned by Friston and Parr and Daphne and colleagues,
just for some more information on the possibility of humans and robots understanding each other's
emotional lives in that way. And it's worth noting here that for this sort of synchrony to occur
between a human and a robot, their mutual predictions need to form this kind of closed loop.
So that would mean that the emotional artifact would have to have some form of interceptive
signals and it would have to act in ways that reflected those signals. So of course this is
all very very speculative, but as we learn more about the brain and we learn more about emotions
and we build a better understanding of these hidden states and these internal mechanisms,
this kind of model convergence between a device and a user we think becomes at least
very plausible in theory. And we also think that building artifacts capable of this type of model
synchrony with their users could have a number of potential benefits. So first consider this form
of synchrony within the context of therapeutic interventions. So when the artifacts model is
able to sufficiently converge on the user's model, then when there's an anomaly in the user's
predictions, such as there is in cases of things like depression or anxiety, the device would be
able to understand how those predictions are generated and also predict what kinds of sensations
or actions might help to alleviate them. And it's interesting that this is essentially what
cognitive behavioral therapy is already trying to do. And second, in the action-orientated approach
to affective computing that we've been talking about, the drive was largely epistemic. So
that's to say that the artifact is trying to reduce uncertainty
relative to modeling the user's emotional states. But once we have the possibility of humans and
artifacts establishing this form of synchrony, it will begin to become possible for the artifact
to install prior preferences about what agent states might be preferable, such that the artifact
might actually be able to begin to steer the emotional synchrony to specific ends.
We do want to flag a cause for concern here, though. These kinds of affective devices would
still be subject to the kinds of concerns that exist around current systems, which is their abuse
by powerful entities that might use this technology to oppress and control vulnerable groups. So in
the case of these active inference devices that have this kind of model synchrony, there's a
speculative concern there that they might make highly effective manipulators.
Nevertheless, though, outside of these therapeutic settings, this sort of model cohesion could also
set the stage for empowering a broad range of improved human AI collaborative work through
addressing the so-called value alignment problem. So very briefly stated, the value
alignment problem is a worry that AI systems may, due to some unforeseen oversight, maximize some
quantity that they value to the detriment of some quantity that we value. So this is essentially
the kind of worry that underpins a lot of our sci-fi worries, sci-fi situations like Skynet and
the Matrix. But by synchronizing and sharing a generative model and similar embodied affective
mechanisms, what's good for a user will also be good for the device. And when the user is doing
worse than expected, then so is the device. So the device will monitor its user and adjust its own
precision profiles accordingly. And it's worth noting as well that active inference devices will
not only rely on the same kinds of problematic data sets. They will not only rely on the same
kinds of problematic data sets mentioned earlier. So by not relying on them, they'll possibly circumvent
the possibility of picking up these hidden patterns of prejudice. But it's also worth
saying that devices that are synchronizing models with individual users would obviously be prone
to picking up any nasty attitudes or prejudices that that particular user already has.
So basically, what we have here, we think to our minds, is at least an initial and admittedly
quite speculative, the building blocks of designing a much more authentically empathic AI. And this
could also potentially be the basic requirements for safer, more effective, collective, and
collaborative intelligence that could emerge between humans and machines in the future.
And that's it. Thank you very much.
All right. Thanks so much, Ben, for this extraordinary talk. I will now open the
floor for the Q&A. So if there's anyone that would like to come and ask questions,
please come to the Q&A on the left.
I think Shannon is coming into the first. So, Shannon?
Yeah, I think.
There you go.
Thanks for this talk, Ben. This has been really fascinating. And I'm interested in also how
AIs create emotional art. So emotion is something that
is claimed to be super important for making authentic music or authentic visual art.
So I have sort of a wonder there if any improvements that AIs that are generating this
artwork, especially there's projects at DeepMind where they interact with someone to,
I think it's called AI duet, actually. And there's a person who's playing along and then
the AI is improvising with the person. And so maybe through that interaction, they're
feeding off of some sort of emotional state of the person. And if that's anything that you guys
are interested in for your project. But then I have a second question, sort of where your talk
ended, about how the AI needs to learn the emotional, sort of starts to learn the emotional
patterns of the person that they're working with. And it just made me think we do have pets
already. And they already have, you know, embodied interceptive states that they're responding to
and they're responsive to your emotions. And we have therapy pets that therapy animals that
are responsive to your motion and also can nudge you into different patterns of behaviors.
So I wonder whether these maybe effective robots would be looking something like therapy pets,
or would be looking something like surveillance that's reporting back to a doctor or something.
Yeah, really, really interesting. On the point about art, I don't know if I don't know if I've
got anything really substantive to add to what you already said. But to the question, are we
interested in it? Speaking for myself, yes, definitely. I kind of have a background in
aesthetics. And so I'm really interested in what active inference and prediction error minimization
has to say about creativity generally. I'm not sure it would fit into this particular project
that we're doing now. On the topic of therapy pets, yeah, again, a really good point,
something I've not actually thought of. I'd have to think about the kind of dynamics between,
because I actually don't have any pets, unfortunately, I've never been able to,
but I have made use of therapy days with animals. But I think you hinted at something
in your question, which is that the devices that we're talking about, I think, would be
they'd be able to engage with their users on a much more level playing field. I think with the
therapy pets, the therapy animals, in my experience, it's always been, it's always at least felt like
kind of a kind of one way street, maybe, that the animal, if you could have a device that was,
like you say, also plugged into kind of mental health services that have been run by humans,
or it had access to certain information, it could look out for certain bits of information in your
speech or whatever you're inputting into the device. And then it could, like you say, then
connect with other healthcare professionals and get you any help that you might need. So I think
there would be benefits that come with this kind of technology, but I think it's a really
interesting point. I don't know if Mark's got anything he wants to add to that.
No, I think that was great. I just agree. I think everything you said is what I would say as well.
Very good.
All right, cool. So then I think we have Jonas. I think he wants to, yeah. So yeah, go for it.
Hey, Ben, thanks so much for the talk. So much appreciated. There's just two thoughts coming
to mind, the talk you have, and I would just kind of be keen to share them and see how they land.
One of them is, and you hinted at this a bit at the beginning, kind of how emotional interactions
work. And that's not only kind of a picking up of emotion, but often like a specific kind of
teasing out certain emotions. So engaging in certain actions. So it's like actively seeking for
certain responses. And I wonder how that fits into this broader image. If this, like if that,
you've thought about mechanisms by which that would happen, or if that kind of fits into the whole
thing. And maybe I just put the other thing out there as well. Actually, Jonas, Jonas, if I can,
you can hear me? Yeah. What do you think there? Because of course, like if you have a technical
idea about how that gets unpacked, I'd love to hear it. I mean, that was your question, right?
Like, were you thinking about how we specifically implement some sort of active emotional
information forging? I mean, if you have an idea, I'd love to hear.
And so that links a bit into the second part, where you talked about the duet for one and how we
kind of dive into the symphony with others. And one crucial idea then that's, but what I'm working on
at the moment is that we try to kind of use the same generators model we're in to interrupt with
others. In the terms of like robotics and robotic interactions, we would put ourselves in the shoes
of that other thing. And that's, I think, why embodiment becomes so important, facial expressions
like body movement. And so I think what would be super interesting here is to kind of dive into
this idea of making them, like making it as easy as possible for us to kind of step into the shoes
of these robots. And that wouldn't mean to make them as efficient as possible, make them as human
like as possible, right? Have all the flaws, all the kind of shortcomings that we have.
Because if we would have that, then it would be much easier for us to also kind of simulate where
the robot is, to simulate what stimuli could be presented robot to like get better ideas of
where that robot is. So it makes sense a little bit. So the idea is to kind of align them in a way
that we are able to figure out what would be the information that, what reaction would I elicit
given certain encounters of information. And then kind of have that simulation, kind of like
knowing how I would respond, helps me then interrupt with the robot.
It's a bit birdy, but... Ben, do you mind if I go first?
Yeah, no, go for it. Not at all.
Just two things there. So one, and I think now I was making this point as well in the chat,
if I understand what they're saying there. So the one problem there again is that you,
there might be an over-generalization of what an external sort of posture would mean for an
internal state and the AI is only going to be able to infer from the external state.
But I don't think that that's necessarily a problem. I think it would be really interesting.
We haven't done it in this paper, but I think it would be really interesting to think about
these things in the way of simulation theory and to think of the ways that we do learn a lot
about each other. And we learn a lot about context by mimicking. I mean, we do do that, right?
We mimic each other as part of gaining fluency. So again, like one of the ways that you could
entrain two models become entrained is you literally mimic each other, you know, you physically
mimic one another. So I think it would be, I mean, as a sort of just somebody who's just starting
on this research, it sounds to me very intuitive to think about what simulation theory type approach
is to empathy, how they could be expressed using these same sort of ideas. I completely agree.
And then there is just a thought like really interesting overlap that the work that
C.O.M. and the hyperscanning people are doing here and here, but also T.C.L. right where it just,
yesterday, it's been in the lab and trying out the first three experiments and just the idea of like
just physically kind of modeling and following each other, already leading to some kind of
neurophysiological symphony. So that kind of feeds in really nicely to the work you're proposing here.
I completely agree. Yeah, that would be a really exciting, it would be a really exciting collaboration
to look at hyperscanning with these ideas agreed.
Well, hey, let's get forward to see what this goes.
Perfect. So is there anybody else who has got a question? Please come to the Q&A.
All right, Steven, go for it.
Thanks. Okay, can you hear me? Yes.
Yeah, I'm just wondering if when you're looking at these emotions, whether you're going to try and
look at them in a particular type of niche environment. So for instance,
I don't know, maybe when someone's at a fun fair or when they're in a prison cell or when it's a
practitioner deeply immersed in dance or something like this,
you know, is there so that you know where their
orientation, position, attitude is and how that might need to be somehow,
somehow sort of established so that you haven't got too much other unknowns going on.
Do you want to start on that one or do you want to?
Yeah, I mean, I think it sounds to me like what you're talking about there is the importance
of context, if I'm hearing you right. Yeah. Yeah. So yeah, I mean, we think that, you know,
as I said in the slides, I think that perhaps the most important thing in regard to these devices
is context. So this is one of the major stumbling blocks of existing emotion recognition technology
is it doesn't take any of that kind of stuff into account. And so this would be an actual really
crucial part of this picture. And I think that it's just played into by things like real embodiment
and active vision, so that the kind of the more able the device is to act in an embodied way,
the more able it is to kind of drink in all the relevant details. And then if it, you know,
if there's still prediction error that needs to be minimized, then it can kind of act to prompt
and probe, you know, prod, whatever the user's happens to be doing in whatever context. So yeah,
I think that's, you know, context might be the biggest challenge in regard to building these
devices. And I think, like I mentioned at the end, there's a kind of trap door in terms of this where
the better that the device gets in regard to learning about an individual user, the more
kind of locked in it is to the sort of attitudes and foibles of that particular user. So these
devices would be less general, I think, than we might want. Current iterations, we might like to
think that current iterations are, there's always going to be some payoff there. I don't know if
you've got anything to add to that, Mark. No, that's great. I mean, yeah, it's exactly the point
we're struggling with, Steven. So I mean, you hit the nail in the head. And I think the interesting
thing that's going to come out of this paper is that we're proposing a way, we're not going to be
able to answer exactly specifically how do you deal with, you know, this context rather than that
context. But what we do get is we get a framework that starts with context. I mean, that's what
active inference is all about in a way. You start perceiving and acting based on a model of how the
world works. So you're building up a model of the world, including these different contexts. And
that's what governs your behaviors and your perceptions. And then what you get, what you get
in addition is you get AI devices that are good at probing in that context to see why it's different
than the other contexts it's shown up in before. So rather than just taking a scowling face and
saying, oh, that person is a scowling person through probing, through actively probing,
and through some of the second order information dynamics that we're suggesting,
we think that we'd be in a good place to start seeing affective devices be able to be more sensitive
in different situations, because it's going to be checking in when it gets the error back that,
oh, I checked and I got a weird, I got a weird error back, you know, last time they looked like
that, like last time they had a frowny face, they were in pain because we were in a hospital. And
this time they have a frowny face because they're bored. And that's something that AIs don't do
good at all right now, you know, they, you're working on these big data sets, so they just take
a basic emotion, match it over and say, oh, so and so is disappointed. But lots of people look
disappointed even when they're not disappointed on the inside. So how do you get past that front
layer? And we think that some of these anticipatory action oriented anticipatory and second order
tuning mechanisms would help an AI be able to probe more deeply and start solving some of those
context problems, although we don't know exactly how that looks, if you know what I mean.
It's not going to get away with not needing to look at the face as well, because you've got other...
Agreed, agreed. And we do that all the time, right? I mean, you don't necessarily take cues
just from a face. People have really good control over their faces sometimes. You look instead,
what's the scene like? What's what's been happening? And how are you speaking? What kind
of word selection are you using? I mean, we know, I mean, if you're married or if you're in a long
term relationship, you know, you can tell if something's wrong with the other person by lots
of little factors, you know, and then what do you do? Well, you don't just, I mean, if you're a good
partner, you just want to assume and say a bunch of weird stuff, but you probe a little, right? You
say, what's what's going on? You know, like, you might have an AI here that will say to you, like,
is everything okay, honey? You know, like, what, what's up? Because your word choice there was
different than it normally is in this kind of context. There's some really interesting
philosophical work on what's been called affective atmospheres as well, which I think is kind of
relevant to what Mark was just talking about where, you know, when you kind of walk into a room and
you can just feel there's a kind of tension or there's a mood that's present in the room. And
you don't, you know, you don't look at each person's face and then kind of run a calculation.
And I think it's interesting to kind of examine this from a phenomenological perspective and ask
what actually is going on in these situations. Because then we might buy kind of examining
that we might find out and things that are very relevant to, you know, designing these kinds of
devices, right? If we can really unpack what it means to be in an affective atmosphere and what
contributes to that. And I think that's going to be another, another piece of the puzzle.
Cool. Thanks very much. This is great work. Take care. Thank you.
Steven, great questions. Thanks for that.
Super cool. Next in line, we have Kevin.
Hello. So I'm not, you know, I guess it's kind of a, and not fully articulated question yet
when I kind of work through, but the idea that came to mind was, and again, I don't know if this
is empirically grounded, but I think Yoshabak made a comment about kind of difference between like
an artist and a craftsman, right? And maybe this goes back to how to deal with certain contacts or
certain dispositions and environment. But like, you know, and it's maybe a question about the
minimizing of prediction error and how helpful that might be in situations where, let's say,
what the artist is not actually trying to minimize their loss function or to improve their conscious
state, but to as best they can represent it, right, through their art and express it in that form.
And they're not actually trying to improve their experience. Does that kind of make sense? Like,
so I'm kind of wondering what you all think in terms of those situations where people are not
trying to actually improve their experiences, but they're actually trying to represent them,
or to somehow savor them? I'm not sure that's the right word, but hopefully that can lead somewhere.
Are we talking, so I mean, so when you say they're not trying to improve, they're trying
to save us something, is this, are we talking, is this again about creativity or maybe even play?
Right. But like, play at the expense of the player. Like, so they're not trying to,
let's say, minimize their uncertainty. I'm kind of wondering, like, you know,
from a theoretical standpoint, like, what is the active inference framework
do in that kind of context? Does that make sense?
So when you say play at the player's expense, could you say just a little bit more about what you
mean by that?
Well, okay, yeah. So I mean, you know, maybe it's like someone who's composing a song or a poet
who's writing a poem, and they're actually simulating and going through a process that's
incredibly internally painful for them, right? And so I'm kind of wondering what is the dynamic
that y'all would expect from an active, you know, emotional, like, artificial intelligence that
trying to feed back into this, like, wouldn't it kind of fit to where they're at and just kind of
reinforce the negative conscious state? I see what you're saying. Yeah, this is brilliant. So yeah,
so if I'm hearing you right, you mean that sometimes we can have emotional experiences
that might have a kind of negative valence in the moment, but are actually kind of, you know,
if you zoom out slightly, they're kind of in service of something productive or something
that might be more positive on a larger scale. Is that right?
Yes. And then like, you know, and yeah, I think that's in the right direction. I think that's
starting to form the question that maybe I have the intuition for. Yeah, yeah. So I think it's
the questions about how can these, how might these devices be sensitive to some of these much more
kind of mysterious and nuanced instances of human emotional experience where, you know,
I suppose, I suppose, Mark, you've done some work on scary films, I mean, fear, because I think that
sounds like... Yeah, I mean, I don't know if this is that on for you, Kevin, but I mean,
it really is something that Ben and I talk a lot about because we think one of the big problems
with AIs today is, well, not just AIs, but actually lots of parts of our society is that we have a
misguided understanding about what's best for us, you know, and one of those things is we feel
like we want things to be easy and optimal. This is coming out a lot in our work on uncertainty.
You might think that an uncertainty minimizing organism would want no uncertainty. It can sign
like that at the front, you know, and actually sometimes it feels like that to us as well.
You know, somebody comes to you and says, I've got a great idea. I'm going to make this city
extremely efficient. I'm going to make it as efficient as possible. You're not going to run
into any problems. And you might just think like, oh my God, that sounds amazing. Build that, okay?
Except for when city planners really do think like this and so do AI designers. When you build a
city that's stripped down and made optimal, everybody's depressed. I mean, you have skyrocketing
cases of depression in those areas. And then when you implement guerrilla gardening and provocative
art installations and more windy roads and more diversity, like lots of diversity, a lot of
differences that you have to manage. So error goes up, but depression comes down. And one of the
reasons is because we do best when we're in that optimal zone, not too little error, not too much
error. If we want to be good at reducing uncertainty over the long term, then we benefit from hugging
these good slopes from these good places where there's a good kind of error. We get better
when we're able to select the right problems rather than getting rid of all of the problems.
We don't build AI like that today, really. I mean, you go to a search engine because it's most
optimal. It's most efficient. It gets you exactly what you expect when you want it. But that's also
creating filter bubbles and echo chambers and all sorts of other problems. So how do you solve
that? I mean, it's difficult to know. I will say, so one thing that springs to mind is
I mentioned Lisa Feldman Barrett's criticism of these systems and her criticisms based on a
criticism of what she calls a theory of basic emotions. And I think what the kinds of things
you're talking about, Kevin, really show why so many researchers are worried about this theory
of basic emotions because it chops up emotion into these very basic categories like fear,
disgust, happiness. And really, I think most people know that these just fail to capture
the broader spectrum of human emotional experiences. And like Barrett has argued,
emotions are actually much more dynamic than that. So she proposes a competing theory where emotions
are kind of constructions that are there to make sense of very contextually complicated
situations. And so I think at least the way I'm thinking about what you asked, it just brings
me back around to context. I think that so far, research in affective computing has really,
really not grasped just how important and deep and complicated context is to human emotions. And
I think once we start taking that into account, then I think we might find a way to, for these
devices to get a grip on the kinds of situations that you're describing, which really are kind
of unique and a little bit mysterious. Kevin, if you wanted to probe this one step further,
just so on your own, just so you know, we have a paper about to be accepted as a preprint out
called The Predictive Dynamics of Happiness and Wellbeing. And in that, we propose that these
emotional regulation processes are happening at multiple levels. So we have, we have global
aerodynamics and local aerodynamics, which I think is a nice step in the direction of
increasing these ideas of contextualization. And Alejandra and Bruno, Alejandra is here
with us tonight that we mentioned their paper in our talk today. They've done fantastic work on
building some of these into robots, and they were onto the same idea about this
layers of affective control, local and global. So that what you get out of that is you can
locally fail at being able to manage error well, but globally succeed. And there's a real benefit
from having precision be set by more global considerations, you know, so that you can like
fail at playing a guitar song, but not think your life is a failure, right? You can fail locally,
but still feel good globally. Anyway, those, if you're interested in that, then that's,
there's a couple of ways you could go and look at more.
All right, Mark. Thanks so much. Sorry to interrupt, but we do have time for one last question.
And these are the very last five minutes before the next talk. And Servo's been standing in line,
so I'm just going to let him go ahead and ask a final question.
Do you understand me now? Okay. So I want to rise, like you talked about the issue of
inclusivity with the automated emotional processing AI. I want to point out another,
which I think is even more from national ethical issue with artificial empathy. And I'd like to
see if you have ideas or positions or ways to defend the problem. So I think you would agree to
two things. The first is that all emotions are socially laden, like you do not have emotions
outside the context of certain norms and institutions and values. And second,
empathy is to provide empathy to someone is a way to reinforce whatever behavior went into the
empathy. So because of these two reasons, if we call robots that provide unconditional empathy,
they will provide an unconditional empathy to, I don't know, murderers, misogynists, and Nazis,
and they will reinforce those norms. So that's not acceptable. But if you build norms
within empathic robots, and you agree that some institutions have the capability to say what is
literally acceptable thoughts are not, you get into a 1984 scenario where whoever the fuck gets
control, literally, like very literally controls what people think privately. And this is also
acceptable. So do you see a way forward in some sense or a reason why this problem is actually
not so critical? Yeah, they're great points. I think the first point you mentioned,
you know, again, if I heard you right, was essentially that these kinds of embodied machines
would also be embedded very much within a culture. So machines that are sensitive to context
in this way would also be sensitive to taking on whatever prevalent norms and conventions there
were in the culture. I, yeah, I wish I had an answer to this. I mean, there's some very big,
I know Rosalind Picard from MIT has given some talks on the kind of politics of affective computing.
So if you're interested in these kinds of questions, it might be worth chasing some of
her work up and seeing what she says on it. I think, you know, when we're talking about the
dangers of this kind of technology essentially falling into the wrong hands and having very
powerful entities, install norms and conventions in these machines that would then potentially,
you know, the point I made in the presentation was about manipulation,
because obviously if there's model synchrony between the device and user,
one of the pitfalls there is that the device is going to be much more apt to manipulate the
user's feelings. And so, I mean, all I would say really is I think you've made great points,
but I don't know, I'd have to think much more carefully. I wish I had some answers to those
worries, but I just don't.
