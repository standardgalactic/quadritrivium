you know, is there so that you know where their
orientation, position, attitude is and how that might need to be somehow,
somehow sort of established so that you haven't got too much other unknowns going on.
Do you want to start on that one or do you want to?
Yeah, I mean, I think it sounds to me like what you're talking about there is the importance
of context, if I'm hearing you right. Yeah. Yeah. So yeah, I mean, we think that, you know,
as I said in the slides, I think that perhaps the most important thing in regard to these devices
is context. So this is one of the major stumbling blocks of existing emotion recognition technology
is it doesn't take any of that kind of stuff into account. And so this would be an actual really
crucial part of this picture. And I think that it's just played into by things like real embodiment
and active vision, so that the kind of the more able the device is to act in an embodied way,
the more able it is to kind of drink in all the relevant details. And then if it, you know,
if there's still prediction error that needs to be minimized, then it can kind of act to prompt
and probe, you know, prod, whatever the user's happens to be doing in whatever context. So yeah,
I think that's, you know, context might be the biggest challenge in regard to building these
devices. And I think, like I mentioned at the end, there's a kind of trap door in terms of this where
the better that the device gets in regard to learning about an individual user, the more
kind of locked in it is to the sort of attitudes and foibles of that particular user. So these
devices would be less general, I think, than we might want. Current iterations, we might like to
think that current iterations are, there's always going to be some payoff there. I don't know if
you've got anything to add to that, Mark. No, that's great. I mean, yeah, it's exactly the point
we're struggling with, Steven. So I mean, you hit the nail in the head. And I think the interesting
thing that's going to come out of this paper is that we're proposing a way, we're not going to be
able to answer exactly specifically how do you deal with, you know, this context rather than that
context. But what we do get is we get a framework that starts with context. I mean, that's what
active inference is all about in a way. You start perceiving and acting based on a model of how the
world works. So you're building up a model of the world, including these different contexts. And
that's what governs your behaviors and your perceptions. And then what you get, what you get
in addition is you get AI devices that are good at probing in that context to see why it's different
than the other contexts it's shown up in before. So rather than just taking a scowling face and
saying, oh, that person is a scowling person through probing, through actively probing,
and through some of the second order information dynamics that we're suggesting,
we think that we'd be in a good place to start seeing affective devices be able to be more sensitive
in different situations, because it's going to be checking in when it gets the error back that,
oh, I checked and I got a weird, I got a weird error back, you know, last time they looked like
that, like last time they had a frowny face, they were in pain because we were in a hospital. And
this time they have a frowny face because they're bored. And that's something that AIs don't do
good at all right now, you know, they, you're working on these big data sets, so they just take
a basic emotion, match it over and say, oh, so and so is disappointed. But lots of people look
disappointed even when they're not disappointed on the inside. So how do you get past that front
layer? And we think that some of these anticipatory action oriented anticipatory and second order
tuning mechanisms would help an AI be able to probe more deeply and start solving some of those
context problems, although we don't know exactly how that looks, if you know what I mean.
It's not going to get away with not needing to look at the face as well, because you've got other...
Agreed, agreed. And we do that all the time, right? I mean, you don't necessarily take cues
just from a face. People have really good control over their faces sometimes. You look instead,
what's the scene like? What's what's been happening? And how are you speaking? What kind
of word selection are you using? I mean, we know, I mean, if you're married or if you're in a long
term relationship, you know, you can tell if something's wrong with the other person by lots
of little factors, you know, and then what do you do? Well, you don't just, I mean, if you're a good
partner, you just want to assume and say a bunch of weird stuff, but you probe a little, right? You
say, what's what's going on? You know, like, you might have an AI here that will say to you, like,
is everything okay, honey? You know, like, what, what's up? Because your word choice there was
different than it normally is in this kind of context. There's some really interesting
philosophical work on what's been called affective atmospheres as well, which I think is kind of
relevant to what Mark was just talking about where, you know, when you kind of walk into a room and
you can just feel there's a kind of tension or there's a mood that's present in the room. And
you don't, you know, you don't look at each person's face and then kind of run a calculation.
And I think it's interesting to kind of examine this from a phenomenological perspective and ask
what actually is going on in these situations. Because then we might buy kind of examining
that we might find out and things that are very relevant to, you know, designing these kinds of
devices, right? If we can really unpack what it means to be in an affective atmosphere and what
contributes to that. And I think that's going to be another, another piece of the puzzle.
Cool. Thanks very much. This is great work. Take care. Thank you.
Steven, great questions. Thanks for that.
Super cool. Next in line, we have Kevin.
Hello. So I'm not, you know, I guess it's kind of a, and not fully articulated question yet
when I kind of work through, but the idea that came to mind was, and again, I don't know if this
is empirically grounded, but I think Yoshabak made a comment about kind of difference between like
an artist and a craftsman, right? And maybe this goes back to how to deal with certain contacts or
certain dispositions and environment. But like, you know, and it's maybe a question about the
minimizing of prediction error and how helpful that might be in situations where, let's say,
what the artist is not actually trying to minimize their loss function or to improve their conscious
state, but to as best they can represent it, right, through their art and express it in that form.
And they're not actually trying to improve their experience. Does that kind of make sense? Like,
so I'm kind of wondering what you all think in terms of those situations where people are not
trying to actually improve their experiences, but they're actually trying to represent them,
or to somehow savor them? I'm not sure that's the right word, but hopefully that can lead somewhere.
Are we talking, so I mean, so when you say they're not trying to improve, they're trying
to save us something, is this, are we talking, is this again about creativity or maybe even play?
Right. But like, play at the expense of the player. Like, so they're not trying to,
let's say, minimize their uncertainty. I'm kind of wondering, like, you know,
from a theoretical standpoint, like, what is the active inference framework
do in that kind of context? Does that make sense?
So when you say play at the player's expense, could you say just a little bit more about what you
mean by that?
Well, okay, yeah. So I mean, you know, maybe it's like someone who's composing a song or a poet
who's writing a poem, and they're actually simulating and going through a process that's
incredibly internally painful for them, right? And so I'm kind of wondering what is the dynamic
that y'all would expect from an active, you know, emotional, like, artificial intelligence that
trying to feed back into this, like, wouldn't it kind of fit to where they're at and just kind of
reinforce the negative conscious state? I see what you're saying. Yeah, this is brilliant. So yeah,
so if I'm hearing you right, you mean that sometimes we can have emotional experiences
that might have a kind of negative valence in the moment, but are actually kind of, you know,
if you zoom out slightly, they're kind of in service of something productive or something
that might be more positive on a larger scale. Is that right?
Yes. And then like, you know, and yeah, I think that's in the right direction. I think that's
starting to form the question that maybe I have the intuition for. Yeah, yeah. So I think it's
the questions about how can these, how might these devices be sensitive to some of these much more
kind of mysterious and nuanced instances of human emotional experience where, you know,
I suppose, I suppose, Mark, you've done some work on scary films, I mean, fear, because I think that
sounds like... Yeah, I mean, I don't know if this is that on for you, Kevin, but I mean,
it really is something that Ben and I talk a lot about because we think one of the big problems
with AIs today is, well, not just AIs, but actually lots of parts of our society is that we have a
misguided understanding about what's best for us, you know, and one of those things is we feel
like we want things to be easy and optimal. This is coming out a lot in our work on uncertainty.
You might think that an uncertainty minimizing organism would want no uncertainty. It can sign
like that at the front, you know, and actually sometimes it feels like that to us as well.
You know, somebody comes to you and says, I've got a great idea. I'm going to make this city
extremely efficient. I'm going to make it as efficient as possible. You're not going to run
into any problems. And you might just think like, oh my God, that sounds amazing. Build that, okay?
Except for when city planners really do think like this and so do AI designers. When you build a
city that's stripped down and made optimal, everybody's depressed. I mean, you have skyrocketing
cases of depression in those areas. And then when you implement guerrilla gardening and provocative
art installations and more windy roads and more diversity, like lots of diversity, a lot of
differences that you have to manage. So error goes up, but depression comes down. And one of the
reasons is because we do best when we're in that optimal zone, not too little error, not too much
error. If we want to be good at reducing uncertainty over the long term, then we benefit from hugging
these good slopes from these good places where there's a good kind of error. We get better
when we're able to select the right problems rather than getting rid of all of the problems.
We don't build AI like that today, really. I mean, you go to a search engine because it's most
optimal. It's most efficient. It gets you exactly what you expect when you want it. But that's also
creating filter bubbles and echo chambers and all sorts of other problems. So how do you solve
that? I mean, it's difficult to know. I will say, so one thing that springs to mind is
I mentioned Lisa Feldman Barrett's criticism of these systems and her criticisms based on a
criticism of what she calls a theory of basic emotions. And I think what the kinds of things
you're talking about, Kevin, really show why so many researchers are worried about this theory
of basic emotions because it chops up emotion into these very basic categories like fear,
disgust, happiness. And really, I think most people know that these just fail to capture
the broader spectrum of human emotional experiences. And like Barrett has argued,
emotions are actually much more dynamic than that. So she proposes a competing theory where emotions
are kind of constructions that are there to make sense of very contextually complicated
situations. And so I think at least the way I'm thinking about what you asked, it just brings
me back around to context. I think that so far, research in affective computing has really,
really not grasped just how important and deep and complicated context is to human emotions. And
I think once we start taking that into account, then I think we might find a way to, for these
devices to get a grip on the kinds of situations that you're describing, which really are kind
of unique and a little bit mysterious. Kevin, if you wanted to probe this one step further,
just so on your own, just so you know, we have a paper about to be accepted as a preprint out
called The Predictive Dynamics of Happiness and Wellbeing. And in that, we propose that these
emotional regulation processes are happening at multiple levels. So we have, we have global
aerodynamics and local aerodynamics, which I think is a nice step in the direction of
increasing these ideas of contextualization. And Alejandra and Bruno, Alejandra is here
with us tonight that we mentioned their paper in our talk today. They've done fantastic work on
building some of these into robots, and they were onto the same idea about this
layers of affective control, local and global. So that what you get out of that is you can
locally fail at being able to manage error well, but globally succeed. And there's a real benefit
from having precision be set by more global considerations, you know, so that you can like
fail at playing a guitar song, but not think your life is a failure, right? You can fail locally,
but still feel good globally. Anyway, those, if you're interested in that, then that's,
there's a couple of ways you could go and look at more.
All right, Mark. Thanks so much. Sorry to interrupt, but we do have time for one last question.
And these are the very last five minutes before the next talk. And Servo's been standing in line,
so I'm just going to let him go ahead and ask a final question.
Do you understand me now? Okay. So I want to rise, like you talked about the issue of
inclusivity with the automated emotional processing AI. I want to point out another,
which I think is even more from national ethical issue with artificial empathy. And I'd like to
see if you have ideas or positions or ways to defend the problem. So I think you would agree to
two things. The first is that all emotions are socially laden, like you do not have emotions
outside the context of certain norms and institutions and values. And second,
empathy is to provide empathy to someone is a way to reinforce whatever behavior went into the
empathy. So because of these two reasons, if we call robots that provide unconditional empathy,
they will provide an unconditional empathy to, I don't know, murderers, misogynists, and Nazis,
and they will reinforce those norms. So that's not acceptable. But if you build norms
within empathic robots, and you agree that some institutions have the capability to say what is
literally acceptable thoughts are not, you get into a 1984 scenario where whoever the fuck gets
control, literally, like very literally controls what people think privately. And this is also
acceptable. So do you see a way forward in some sense or a reason why this problem is actually
not so critical? Yeah, they're great points. I think the first point you mentioned,
you know, again, if I heard you right, was essentially that these kinds of embodied machines
would also be embedded very much within a culture. So machines that are sensitive to context
in this way would also be sensitive to taking on whatever prevalent norms and conventions there
were in the culture. I, yeah, I wish I had an answer to this. I mean, there's some very big,
I know Rosalind Picard from MIT has given some talks on the kind of politics of affective computing.
So if you're interested in these kinds of questions, it might be worth chasing some of
her work up and seeing what she says on it. I think, you know, when we're talking about the
dangers of this kind of technology essentially falling into the wrong hands and having very
powerful entities, install norms and conventions in these machines that would then potentially,
you know, the point I made in the presentation was about manipulation,
because obviously if there's model synchrony between the device and user,
one of the pitfalls there is that the device is going to be much more apt to manipulate the
user's feelings. And so, I mean, all I would say really is I think you've made great points,
but I don't know, I'd have to think much more carefully. I wish I had some answers to those
worries, but I just don't.
