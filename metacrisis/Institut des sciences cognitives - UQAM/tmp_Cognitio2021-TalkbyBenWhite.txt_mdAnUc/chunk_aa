to the second day of Cognitiu 2021. You're very welcome and very happy to have you here
with us today. We have a very long, interesting day of very cool talks. The first one is going
to be presented by Daphne Demakas. She's a developer at Nested Minds Network and a Ben White with
a Leverheim funded PhD student at the University of Sussex and Mark Miller, who is an assistant
professor at the Center for Human Nature, Artificial Intelligence and Neuroscience at
Hokkaido University in Japan. The presentation is going to be given today by both Ben and Mark.
The title of the presentation is Artificial Empathy, Active Inference and New Collective
Intelligence. Before I give you the floor, Ben, I just want to make a note that at the end of the
talk, those of you who would like to ask any questions or comments, please do come to the
Q&A, which is here on the left, where it is marked as Q&A. Just come and line up and when you want
to ask a question, just make sure that you are standing on the circle, because otherwise you
won't be able to be heard. So just on that note, I'll give the floor to Ben. The floor is yours.
Thanks, Ines. So I just want to make sure, can everybody see my slide and hear me okay? Good,
good. Okay, so yeah, I think a good morning is in order to most people listen to this. I just want
to say thank you for having me. And it's important for me to say before I get started that this is
very much still a work in progress. And it's been a collaborative project from the beginning. So
myself and Mark Miller are carrying out this work with Daphne Demeckis and also Kat Snari
Mihara, who is at Hokkaido with Mark Miller. And I just want to say Mark Miller is here and he'll be
joining me in the Q&A after the talk. Now, since this is an Active Inference conference and we're
already in day two, I'm actually going to skip over kind of intro to what the framework is and how
it works. And what I want to do instead is tell a kind of a story about how we see the Active
Inference framework influencing new and exciting approaches in designing affective computing
devices. So affective computing is a pretty rapidly growing research program. And it aims to create
computing and robotic devices capable of recognizing, categorizing and responding intelligently to
human emotions. So this obviously draws on quite a wide interdisciplinary body of research,
including neuroscience, psychology, philosophy, computer science and engineering. And it's also
important to say that the term affective computing itself was coined by Rosalind Picard, who is an
MIT professor and she now runs the MIT affective computing lab. That's Rosalind Picard there with
the cover of her original book. And I want to give some examples of affective computing. Most of
these are from the MIT lab. So research has been looking at a range of different uses for this
kind of technology, including how it might operate in the classroom to a teaching and learning, how
affective interfacing in cars might make for better improved road safety, and how affective
computing could be used for therapeutic interventions like GIBO, which is the little screen and
camera thing you can see at the bottom of that slide. That's actually been used and tested in
dorms of undergraduate students at MIT. And another example is a little yellow guy on the blue
background. That's Wobot as in W-O-E Wobot, who's another therapeutic intervention that uses natural
language processing. So if you want to check out lots of examples of this stuff, then the MIT
affective computing lab has their own website and it's very, very cool. But for now, though, the
majority of the current deployments of affective computing have been in industry, including
sectors like recruitment and marketing. So for example, the company Unilever is currently using
a form of Emotion Recognition Tech to analyze the nonverbal cues and responses of people during
interviews. And then this software decides who they hire and who they don't. And I think Amazon
have recently started using something quite similar as well. Now, because of the increase in
prevalence of this in industry and business, it has attracted quite a lot of criticism both in
terms of its functionality and in terms of some what we think are very well founded ethical
concerns surrounding its use. So people in the know are kind of skeptical about this. So for
example, the insurance company Lemonade recently caused a Twitter outrage when it merely hinted at
the fact that it might start using Emotion Recognition Technology to detect fraudulent claims and
that kind of stuff. So current versions of Emotion Recognition have been shown to have some
really serious shortcomings actually. So some researchers in affective science have claimed
that these systems are just founded on highly questionable science and some badly misguided
assumptions about what emotions are and how they work. Going so far as to label these systems a
kind of neo-frenology. So Lisa Feldman Barrett in particular has been very critical of what she
terms a theory of basic emotion, which underpins most of these systems. So in a nutshell, this
theory just says that humans have six to eight basic emotions and that these emotions are kind of
universally expressed through discrete readable sets of facial expressions. And so in a recent
interview Barrett stated that quote, there are some companies that just continue to claim things
that can't possibly be true. Based on published scientific evidence, our judgment is that these
technologies shouldn't be rolled out and used to make consequential decisions about people's
lives. Now, there are a range of other ethical concerns. So Berber, Birhani has argued that
current systems which are trained on very large data sets are inherently conservative and they
propagate any biases or prejudices that are present in the data that they're trained on. So
this can result in systems which are, for example, exhibiting racist or sexist tendencies and
that lots of examples of this, unfortunately. So emotion recognition systems themselves have
been shown to perform much worse on non-white faces. And there's a really disturbing study by
Lauren Rue, which I've put up on the slide there that confirms this. Moreover, Birhani and others
have recently pointed out that these data sets are now so large that the people that are using
them, the entities that are using them to train their software, don't even know what's in there.
And in a recent study, Birhani found that there was just a range of kind of prejudiced or highly
offensive material in there. And finally, many of these systems are subject to the kind of worries
about opacity and black box kind of worries that afflict quite a lot of AI technology generally.
But clearly, this technology has been used to make quite consequential decisions about people's
lives. And so therefore, we think that it's pretty vital that this technology is at least
underpinned by the strongest theoretical commitments. And so I want to be really clear from the
outset that our goal in this talk is not to justify the wholesale use of affective computing in all
areas. So we're not going to argue or suggest what we suggest here today is going to solve all of
these concerns. And this is really important to say, because even if what we're going to describe
shortly is a promising way forward, and obviously we think it is, and even if theoretically all of
this technology could be perfected, then there might still be very principled reasons to think
very carefully about when it's deployed, how it's deployed and who it affects. So what's the
problem with it? Well, the major problem that we suggest in our paper is that the current paradigm
in affective computing is very problematically disembodied. So while there's a huge buzz today
about building emotional or kind of quote unquote empathic robots, all that boils down to in most
cases is just building artificial artifacts equipped with emotion recognition software coupled
with some form of decision making. So suggesting, for example, like at Unilever, who gets fired, who
gets hired. So in these cases, the emotional or affective side of things is really just very
superficial. So these aspects don't play a significant role in the actual computing processes that
are driving the interactions. So seen in this way, these approaches really are based on a foundation
of seriously outdated cognitive science. Because now we know, of course, and we've known for some
time really that affect cognition, perception, action, these are all dynamically co evolving
together. So for example, Luis Pazoa has argued that the prevalent view of emotions and affect has
been somehow separate from cognition. Having unique neural fingerprints is problematic for a
number of reasons. So he argues that complex cognitive emotional behaviors have their basis
in dynamic coalitions of connected brain regions, none of which should be conceptualized as
specifically affective or cognitive. And moreover, Pazoa emphasizes that these complex cognitive
emotional dynamics are inherently linked with action with how an agent acts in a given environment.
And this intrinsic link between affect and goal directed behavior is something that I will turn
to pretty soon. So if we're taking human emotions and affective interactions, including empathy as
a blueprint for designing affective computing devices, we need to think much more seriously
about these important and complex dynamics. So one way that we think you can do this is to bring
active inference into the picture, which, as we know, integrates all of perception, cognition,
action and affect into one fluid dynamic and embodied system for minimizing prediction error.
So we think that by using active inference as a theoretical basis for active affective computing,
it's going to resort in machines that are functionally more impressive. And it might even offer
avenues for potentially addressing some of the ethical concerns that I mentioned earlier.
So we get two things right off the bat when thinking about introducing active inference into
affective computing. So first, active inference offers a unique shared and transparent computational
currency, which is made up of a suite of theoretical and mathematical tools that's going to allow
theorists, neuroscientists and machine learning researchers to all collaborate very closely on
these issues. And second, it puts action up front in helping to solve our understanding of the
world and each other. So consider just how crucial action is in human interactions. So we're
constantly using actions to elicit responses to probe, to test, to gauge how other people might
be thinking or feeling. So for example, if you're on the subway and you see someone scowling at you,
then you might decide to flash them a quick smile just to see kind of to test what's going on,
because it might be the case that they just have a kind of resting scowl face, or it might mean
that the scowl actually means something and we might then decide to kind of move carriages. So
another example that I know that I've experienced and a lot of other people probably have is being in
a job interview where the people interviewing you are deliberately hiding their emotions to remain
neutral. And this can be quite stressful for us. And so we might kind of modulate our tone of voice
or drop in a joke just to kind of try and get some information about how other people feel about us
in that moment. So so much of what we know about human emotion and how we navigate emotional
situations is the result of this kind of testing and this kind of action. And importantly, this
kind of active testing is central in figuring out all those kind of important contextual cues that
dictate what an action or an expression means. So we do not inflexibly apply patterns from data,
like the current versions of emotion recognition do. We adapt on the fly and we use cues in our
environments and atmosphere to make sense of things happening in the particular moment.
So just consider the stranger scowling at you again, but this time imagine it's taking place on a
hospital ward, where you know that everybody's likely to be in quite a lot of pain. The meanings
of these these conveyance conveyances change can change dramatically depending on the context.
So context really is king and actively probing is so crucial for getting context right in emotional
interactions. And so this is it's exactly this kind of active probing in order to get to better
model and understand the emotions of specific others in specific situations that's kind of
surprisingly lacking in current versions of affective computing. And so this to our minds is
a major stumbling block for the current paradigm. So we already have some fantastic work out by
active inference researchers, including Samuel Vassier and colleagues on how we come to understand
each other through exactly this kind of active process of reducing uncertainty. And so I want to
highlight one example of where we see this gap between humans and disembodied AI in terms of
dealing with context. And that is selective attention. So it's quite a pervasive problem in AI
design that current systems don't tend to look at the same regions as humans do. So the question
researchers are asking is how can we make this search pattern more human like? And how will this
result in more accuracy? So successful selective attention is all about filling gaps in the observer's
knowledge, knowledge about where to look to get the most kind of bang for your buck in terms of
reducing uncertainty. And typically researchers have thought that this is to do with humans just
having a much wider kind of experience set and bigger knowledge base than neural nets. But we
suggest that they may also have better machinery for gleaning through actively predicting,
testing and confirming what matters most in a scene. So the way we build up these models,
as we've been saying, isn't only by passively perceiving, but by continually and actively
probing. And this includes overactions like circuiting and moving, and also internal dynamics
like precision reallocations. So this is one major benefit of a more fully embodied affective device,
one that has available to it kind of manipulatable eyes, necks, heads. It's going to be able to
physically manipulate its own viewpoints just like humans do. And so it will be able to drink in
a higher quantity of relevant details within the physical environment. And in the robotics
community, this is known as active vision, and it's increasingly seen as absolutely vital to
building intelligent systems. So one of the major benefits of applying the active inference framework
to building affective computing devices will be this ability to actively forage for information
to resolve its own uncertainty. And here that uncertainty is about somebody else's emotional
states. So through current approaches to active affective computing, they don't have to do this.
Action driven learning through reduction of uncertainty comes as part and parcel of active
inference. So one corollary benefit worth mentioning, I think is of this much more active approach,
is that devices would actually have the benefit of needing much smaller data sets with shorter
training times, because the device would be able to autonomously select the data that best optimizes
the model in a given context. So this might just be one step toward addressing some of the concerns
that Birhani and others have had about the problems that you kind of inherent very, very much data
sets. But from a practical standpoint, making these additions means that we'll just have to move
beyond building affective computing devices that are basically lumps of plastic or just make use
of camera phones. And so to begin to build properly embodied devices, we'll need humanoids with
sophisticated representation to the face, which are able to act fluidly and quickly adapting in
real times the dynamics of conversation, action, and environment. So kind of speaking of adapting
fluidly in real time, in cognitive robotics today, an important question is how artificial
agents learn their internal models in order to interact efficiently with the world around them
and with other people. We've already talked about how important embodied action is to learning
about other people's models and active inference, but we suggest that affectivity in the form of
prediction aerodynamics may be a key element in motivating and directing this kind of embodied
epistemic foraging in artificial agents, just like it is in humans. So what that means in other
words is what we're suggesting is affective computing devices would ultimately benefit from
having internal affective dynamics of their own. Although many different computational models have
been proposed for building intrinsic motivation into artificial systems in order to generate these
kind of exploratory behaviors and curiosity, existing approaches have agents choose actions
according to some internal measures related to the novelty or the predictability of anticipated
situations. So the main difference between what's offered in active inference and these existing
approaches is that in active inference, this intrinsic motivational architecture is based
on the relationship between prediction error and affect. So according to recent work in active
inference, affect is at least in part a form of second order information within the predictive
system itself. So this second order information informs the system about how well or how poorly
prediction error is being reduced over time relative to expectations. So in short, when we
feel good, we feel good when we do better than expected and we feel bad when we're doing worse
than we expect to at reducing prediction error. And these feelings, these affective feedback
informs us about how things are going relative to those expectations. These second order states
also play an important role on modulating the level of confidence or precision that the system has
in its own actions and beliefs. And of course, this seems to make intuitive sense because when we
do better than expected at reducing error, we feel good. And this good feeling helps to reinforce
our confidence that whatever it was that we were doing, those current action policies and predictions
are good ones to implement again in the future in similar contexts. So in this way, aerodynamics
serves to keep an agent flexibly and sensitively tuned to opportunities for success within
environments and as our skills and abilities evolve over time. And notice here that agents
outfitted with a sensitivity to aerodynamics will naturally be curious. So they're going to be drawn
to situations where the most error can be reduced reliably because that's where the best feelings
are going to be found. And those places tend to be at the edge of one's familiarity. So neither too
well known where there's going to be too little error, nor too complex where there's too much error
and we become frustrated. So crucially, aerodynamics play a role in helping to direct and enhance
