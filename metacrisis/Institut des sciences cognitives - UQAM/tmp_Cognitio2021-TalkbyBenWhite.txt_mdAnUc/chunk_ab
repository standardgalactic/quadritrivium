learning. So prediction error and its reduction rates signal the expectations about the learnability
of particular situations and helps to guide attention and prioritize learning, making the
updating of our models much more efficient. So we have already seen this sort of optimal
error engagement show up in curious robots. So Roy Dyer and Smith is a good source here where
they train robots to seek out optimal levels of complexity, because that's where the most
learning can take place. And specifically, active inference approaches have begun to be used in real
world robotics. And there's some really, really great proof of concept in the work of Guido,
Skelasi, Bruno Laura, and Alejandra, Syria, who have actually built robotic systems using this
kind of internal machinery of prediction aerodynamics. And indeed, their work has shown how robots
equipped with aerodynamics are better able to manage uncertainty by fluidly selecting adaptive
actions in an environment when compared to more traditional approaches. So these artificial
agents that are equipped with internal aerodynamics are better able to learn and then autonomously
select the proper prediction error strategy in a given situation by allowing an agent's
valence, which is that second order information, to weigh and motivate the selection of whatever
might be the most suitable behavior. So this work also showed that aerodynamics provided
a way for artificial agents to navigate the temporal aspects of goal selection,
which is to say that when to engage with a goal, how long to persevere, how long to persevere when
things aren't going right before quitting, and so on and so on. So notice that affect here is
intrinsically related to goals arising when differences between expected and actual rates
of progress occur. So to give a kind of cute real world example, when my niece tries to reach the
sweets in the cupboard and she fails, she might persevere for a given amount of time. She might
see if first there's a way that she can use a stool to get a leg up or something, but when this
strategy fails, inevitably the frustration builds and she might start to cry, at which point she's
trying to rope someone else into her crime and steal in the sweets. And when this fails, she
might ostensibly abandon the mission and run to her mum for a cuddle, which kind of viewed through
this kind of aerodynamics framework, is her engaging emotionally to potentially recruit
someone else into getting the sweets for her. So we suggest in our paper that by introducing these
aerodynamics into effective computing devices, we'd see devices not only motivated to flexibly
and curiously implement new policies for action, which in this case would mean to learn more about
other people's emotional states, but biomimetic devices implementing very similar kinds of
affective motivational mechanisms that we see in humans. So this new wave of affective computing
devices would not only recognize and respond to human emotions more efficiently, but it would
actually have in some sense an internal emotional affective structure of its own.
So lastly and most interestingly and definitely most speculatively, we want to explore the
possibility that these kinds of devices now equipped with this internal proactively expressive
body and something of its own affectivity might be able to enjoy the kind of model
synchrony with their users that we see emerging between in the case of human interactions.
So recent work in active inference on communication dynamics highlights this kind of
synchronization as part of coming to understand one another. So in their paper a duet for one,
Carl Friston and Christopher Frith have used simulations of songbirds to show that a generalized
and this is a quote from the paper, generalized synchrony is an emergent property of coupling
active inference systems that are trying to predict each other. So in very rough terms what they
demonstrate is that according to active inference, meaningful communication between agents requires
that these agents are sufficiently able to model one another and then model the other person model
in them and so on and so forth in an infinite regress. And agents that are able to do this by
making and testing predictions can come to synchronize their internal generative models.
So as noted in the paper by Daphne Demeckis, Friston and Parr quote, in the context of
emotion recognition, this demonstrates the possibility of synchronizing the internal states
of the device and its user, such that the device and user are both attending to an attenuating
sensory information in order to properly cause and predict each other's emotional states.
So I suggest that everybody that's interested in this kind of model synchrony checks out
the two papers I just mentioned by Friston and Parr and Daphne and colleagues,
just for some more information on the possibility of humans and robots understanding each other's
emotional lives in that way. And it's worth noting here that for this sort of synchrony to occur
between a human and a robot, their mutual predictions need to form this kind of closed loop.
So that would mean that the emotional artifact would have to have some form of interceptive
signals and it would have to act in ways that reflected those signals. So of course this is
all very very speculative, but as we learn more about the brain and we learn more about emotions
and we build a better understanding of these hidden states and these internal mechanisms,
this kind of model convergence between a device and a user we think becomes at least
very plausible in theory. And we also think that building artifacts capable of this type of model
synchrony with their users could have a number of potential benefits. So first consider this form
of synchrony within the context of therapeutic interventions. So when the artifacts model is
able to sufficiently converge on the user's model, then when there's an anomaly in the user's
predictions, such as there is in cases of things like depression or anxiety, the device would be
able to understand how those predictions are generated and also predict what kinds of sensations
or actions might help to alleviate them. And it's interesting that this is essentially what
cognitive behavioral therapy is already trying to do. And second, in the action-orientated approach
to affective computing that we've been talking about, the drive was largely epistemic. So
that's to say that the artifact is trying to reduce uncertainty
relative to modeling the user's emotional states. But once we have the possibility of humans and
artifacts establishing this form of synchrony, it will begin to become possible for the artifact
to install prior preferences about what agent states might be preferable, such that the artifact
might actually be able to begin to steer the emotional synchrony to specific ends.
We do want to flag a cause for concern here, though. These kinds of affective devices would
still be subject to the kinds of concerns that exist around current systems, which is their abuse
by powerful entities that might use this technology to oppress and control vulnerable groups. So in
the case of these active inference devices that have this kind of model synchrony, there's a
speculative concern there that they might make highly effective manipulators.
Nevertheless, though, outside of these therapeutic settings, this sort of model cohesion could also
set the stage for empowering a broad range of improved human AI collaborative work through
addressing the so-called value alignment problem. So very briefly stated, the value
alignment problem is a worry that AI systems may, due to some unforeseen oversight, maximize some
quantity that they value to the detriment of some quantity that we value. So this is essentially
the kind of worry that underpins a lot of our sci-fi worries, sci-fi situations like Skynet and
the Matrix. But by synchronizing and sharing a generative model and similar embodied affective
mechanisms, what's good for a user will also be good for the device. And when the user is doing
worse than expected, then so is the device. So the device will monitor its user and adjust its own
precision profiles accordingly. And it's worth noting as well that active inference devices will
not only rely on the same kinds of problematic data sets. They will not only rely on the same
kinds of problematic data sets mentioned earlier. So by not relying on them, they'll possibly circumvent
the possibility of picking up these hidden patterns of prejudice. But it's also worth
saying that devices that are synchronizing models with individual users would obviously be prone
to picking up any nasty attitudes or prejudices that that particular user already has.
So basically, what we have here, we think to our minds, is at least an initial and admittedly
quite speculative, the building blocks of designing a much more authentically empathic AI. And this
could also potentially be the basic requirements for safer, more effective, collective, and
collaborative intelligence that could emerge between humans and machines in the future.
And that's it. Thank you very much.
All right. Thanks so much, Ben, for this extraordinary talk. I will now open the
floor for the Q&A. So if there's anyone that would like to come and ask questions,
please come to the Q&A on the left.
I think Shannon is coming into the first. So, Shannon?
Yeah, I think.
There you go.
Thanks for this talk, Ben. This has been really fascinating. And I'm interested in also how
AIs create emotional art. So emotion is something that
is claimed to be super important for making authentic music or authentic visual art.
So I have sort of a wonder there if any improvements that AIs that are generating this
artwork, especially there's projects at DeepMind where they interact with someone to,
I think it's called AI duet, actually. And there's a person who's playing along and then
the AI is improvising with the person. And so maybe through that interaction, they're
feeding off of some sort of emotional state of the person. And if that's anything that you guys
are interested in for your project. But then I have a second question, sort of where your talk
ended, about how the AI needs to learn the emotional, sort of starts to learn the emotional
patterns of the person that they're working with. And it just made me think we do have pets
already. And they already have, you know, embodied interceptive states that they're responding to
and they're responsive to your emotions. And we have therapy pets that therapy animals that
are responsive to your motion and also can nudge you into different patterns of behaviors.
So I wonder whether these maybe effective robots would be looking something like therapy pets,
or would be looking something like surveillance that's reporting back to a doctor or something.
Yeah, really, really interesting. On the point about art, I don't know if I don't know if I've
got anything really substantive to add to what you already said. But to the question, are we
interested in it? Speaking for myself, yes, definitely. I kind of have a background in
aesthetics. And so I'm really interested in what active inference and prediction error minimization
has to say about creativity generally. I'm not sure it would fit into this particular project
that we're doing now. On the topic of therapy pets, yeah, again, a really good point,
something I've not actually thought of. I'd have to think about the kind of dynamics between,
because I actually don't have any pets, unfortunately, I've never been able to,
but I have made use of therapy days with animals. But I think you hinted at something
in your question, which is that the devices that we're talking about, I think, would be
they'd be able to engage with their users on a much more level playing field. I think with the
therapy pets, the therapy animals, in my experience, it's always been, it's always at least felt like
kind of a kind of one way street, maybe, that the animal, if you could have a device that was,
like you say, also plugged into kind of mental health services that have been run by humans,
or it had access to certain information, it could look out for certain bits of information in your
speech or whatever you're inputting into the device. And then it could, like you say, then
connect with other healthcare professionals and get you any help that you might need. So I think
there would be benefits that come with this kind of technology, but I think it's a really
interesting point. I don't know if Mark's got anything he wants to add to that.
No, I think that was great. I just agree. I think everything you said is what I would say as well.
Very good.
All right, cool. So then I think we have Jonas. I think he wants to, yeah. So yeah, go for it.
Hey, Ben, thanks so much for the talk. So much appreciated. There's just two thoughts coming
to mind, the talk you have, and I would just kind of be keen to share them and see how they land.
One of them is, and you hinted at this a bit at the beginning, kind of how emotional interactions
work. And that's not only kind of a picking up of emotion, but often like a specific kind of
teasing out certain emotions. So engaging in certain actions. So it's like actively seeking for
certain responses. And I wonder how that fits into this broader image. If this, like if that,
you've thought about mechanisms by which that would happen, or if that kind of fits into the whole
thing. And maybe I just put the other thing out there as well. Actually, Jonas, Jonas, if I can,
you can hear me? Yeah. What do you think there? Because of course, like if you have a technical
idea about how that gets unpacked, I'd love to hear it. I mean, that was your question, right?
Like, were you thinking about how we specifically implement some sort of active emotional
information forging? I mean, if you have an idea, I'd love to hear.
And so that links a bit into the second part, where you talked about the duet for one and how we
kind of dive into the symphony with others. And one crucial idea then that's, but what I'm working on
at the moment is that we try to kind of use the same generators model we're in to interrupt with
others. In the terms of like robotics and robotic interactions, we would put ourselves in the shoes
of that other thing. And that's, I think, why embodiment becomes so important, facial expressions
like body movement. And so I think what would be super interesting here is to kind of dive into
this idea of making them, like making it as easy as possible for us to kind of step into the shoes
of these robots. And that wouldn't mean to make them as efficient as possible, make them as human
like as possible, right? Have all the flaws, all the kind of shortcomings that we have.
Because if we would have that, then it would be much easier for us to also kind of simulate where
the robot is, to simulate what stimuli could be presented robot to like get better ideas of
where that robot is. So it makes sense a little bit. So the idea is to kind of align them in a way
that we are able to figure out what would be the information that, what reaction would I elicit
given certain encounters of information. And then kind of have that simulation, kind of like
knowing how I would respond, helps me then interrupt with the robot.
It's a bit birdy, but... Ben, do you mind if I go first?
Yeah, no, go for it. Not at all.
Just two things there. So one, and I think now I was making this point as well in the chat,
if I understand what they're saying there. So the one problem there again is that you,
there might be an over-generalization of what an external sort of posture would mean for an
internal state and the AI is only going to be able to infer from the external state.
But I don't think that that's necessarily a problem. I think it would be really interesting.
We haven't done it in this paper, but I think it would be really interesting to think about
these things in the way of simulation theory and to think of the ways that we do learn a lot
about each other. And we learn a lot about context by mimicking. I mean, we do do that, right?
We mimic each other as part of gaining fluency. So again, like one of the ways that you could
entrain two models become entrained is you literally mimic each other, you know, you physically
mimic one another. So I think it would be, I mean, as a sort of just somebody who's just starting
on this research, it sounds to me very intuitive to think about what simulation theory type approach
is to empathy, how they could be expressed using these same sort of ideas. I completely agree.
And then there is just a thought like really interesting overlap that the work that
C.O.M. and the hyperscanning people are doing here and here, but also T.C.L. right where it just,
yesterday, it's been in the lab and trying out the first three experiments and just the idea of like
just physically kind of modeling and following each other, already leading to some kind of
neurophysiological symphony. So that kind of feeds in really nicely to the work you're proposing here.
I completely agree. Yeah, that would be a really exciting, it would be a really exciting collaboration
to look at hyperscanning with these ideas agreed.
Well, hey, let's get forward to see what this goes.
Perfect. So is there anybody else who has got a question? Please come to the Q&A.
All right, Steven, go for it.
Thanks. Okay, can you hear me? Yes.
Yeah, I'm just wondering if when you're looking at these emotions, whether you're going to try and
look at them in a particular type of niche environment. So for instance,
I don't know, maybe when someone's at a fun fair or when they're in a prison cell or when it's a
practitioner deeply immersed in dance or something like this,
