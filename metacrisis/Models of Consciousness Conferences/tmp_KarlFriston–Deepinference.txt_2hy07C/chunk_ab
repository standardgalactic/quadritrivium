just like these this graphical model out in terms of the equations and the interesting thing is
if you can write down a probabilistic graphical model there is always a conjugate kind of model
then there's a factor graph I won't go into the details all we need to know here is that the
factor graph specifies the message passing that underwrites the belief updating so there's always
a message passing scheme for any given generative model the key distinction between the graphical
model and the factor graph is that the edges become the unknowns and the nodes become the
the the factors here and we can interpret this in terms of Bayesian belief updating so this kind
of generative model leads to this kind of message passing that I've just annotated with the edges
or the messages that are passed between different nodes in this factor graph this is nice because
what it does is it provides an off the shelf technology to articulate the belief updating
in terms of a mechanics of message passing and belief updating that you can then see well are there
artifacts out there that do this kind of thing and have this computational architecture and indeed
there are the brains are probably the most compelling example of this so what I've done here
I just lifted the under this kind of generative model you get this kind of what's called variational
message passing that looks very very much like neuronal dynamics and plasticity and learning
and action selection in a brain so you know all that we need to do is just to look at well what
don't we know what are we making inferences about we just don't know states in the world
under any given policy we don't know the policy that we're pursuing I've also equipped this model
with a precision parameter which is not really central from the current arguments so the belief
updating about states just becomes a nonlinear function that you can think of as a sort of
neuronal activation function of linear mixtures of observable outcomes in states in the past
and states in the future and the policy selection inferring the policy I am pursuing akin to planning
or controllers inference is just a standard classical softmax function of my expected free
energy weighted by my inverse precision here learning transpires to be or formally identical
to associated plasticity or heavy and plasticity just basically counting co-occurrences of things
and building up parameters connections in your generative model that underwrite the message
passing as you accumulate and learn the structure of your world simply by outcomes and causes co-occurring
or the state now and the subsequent states co-occurring allowing you to update all sorts
of parameters here these are just the initial states and we've talked about action selection
as just selecting the action that is most likely under the policy that has the greatest posterior
probability and you can play all sorts of interesting games about assigning different parts of the
generative model to different parts of the world here's one particular one where we've assigned
policy selection to the striatum and the precision control to the dopaminergic system
but let me now conclude by just giving you a really simple work example of the emergent
behaviours under this mechanics the example uses a sort of very elemental two-step task imagine
that we have a subject here a little mouse who's put in a tea maze and the mouse has two moves
and it wants to secure bait or reward that can be either in the left or the right arm
and it's in this instance got this interesting opportunity to do a bit of searching in the sense
that there's an instructional cue here that tells it whether the bait is on the left or the right
hand side so this presents an interesting choice for the little mouse it can either waste one of its
moves and go to the instructional cue and then go and enjoy for the last move the reward because it
knows exactly where the reward is or it can take a chance and go either to the right or the left
arm noting that once you go into one of these absorbing states you have to stay there so
half the time it could spend twice two moves with a reward on half the time two moves without
so from the point of view of the bell and optimality principle these have the same affordances
but clearly from the point of view of the the principle of least action that has this uncertainty
reducing aspect then going to the instructional cue has the has clearly the the greatest expected
free energy and will therefore attract or determine that kind of behaviour and indeed
that's what happens so here's the results of a simulation that just I'm presenting now
irrespective of the particular design or generated model this is the kind of behaviour that one sees
here described in terms of the initial states the reward is on the left on the right
this is the policy that was selected this is the outcomes and the
expected utility and this is the learning about the initial states which was
here whether the reward was on the left hand side or the right hand side and what we basically see
is in the mouse inevitably goes for the epistemic policies infers that it should be and
therefore pursues an epistemic policy that maximises this intrinsic value this sort of
curiosity driven uncertainty reducing epistemic or explorative policy in in the first instance
but what we've done here is after the first couple of trials we've always left the reward
on the same side so that as time goes on the mouse learns that the reward the context is most
likely to be on the right on the left hand side and that has the interesting consequence but the
epistemic affordance of resolving uncertainty by going to the instructional cues starts to wane
over time because there's less uncertainty to resolve the animals becoming more familiar with
its environment and what that means is at some point that extrinsic value that utilitarian
pragmatic goal directed as opposed to information directed seeking behaviour certainly now has the
greatest expected free energy and there's a switching behaviour to more exploitative behaviour
where it goes straight to the reward there's a natural consequence of this trade-off or splitting
of the expected evidence of free energy into this pragmatic and epistemic parts
so just to illustrate the that epistemic part that is underwritten by having the right kind
of generative model of your of your world I'm going to end by demonstrating
an influence under a deep generative model so this is a slide we saw before where we have a
generative model in terms of a graphical model and the accompanying factor graph that proscribes for us
gives us the the message passing that will do the belief updating and and what I've done here
is simply put one of these generative models on top of another one to create a deep model
the first the other move that's been made here is that the probability transitions in time if you like
evolves more slowly at the deeper or the higher model than the the lower parts of the model
so technically what's happening here is that there's a slow part of the generative model that's
generating states or contexts that change slowly and at each point or each transition
there are some initial states for a much quicker faster generative model at the lower level
doing a little if you like mini epoch or trajectory for every cycle of the changes at the higher level
and I can write that down very simply this is a hierarchical extension of the previous one
this is the corresponding factor graph with all the messages sort of evidence about the current
context coming from the lower part of the model while there are reciprocal messages that provide
if you like prior constraints or contextualize the dynamics and the the sequence of behaviors
and inference perceptual inference at the lower level here and this is simple to engineer and to run
these message passing schemes to emulate inference under these deep and in virtue of the separation
of temporal scales di chronic generative models here there's a sort of very simple model of
reading it is very simple in this instance if we just focus on the lower level of the model
here are causes and here are consequences so the agent can basically see something in the
center of its vision which can be thought of as a letter in terms of little icons here
and it also feels where in the world the word it is it is currently looking and these outcomes
are generated by hidden states which essentially is where am I looking and what's what am I looking
at in terms of a word that has a composition of different letters in iconic form here so this
are the labels of these combinations of letters here fleet feed and weight so if I knew what the
word was I knew where I was looking I could generate exactly what I would see and where I'd
feel myself looking so this will be the first level and I could refer what word I was looking at
by scouting around the world at a higher level though we can now put in the sentence from
which that word came or in another way the sentence that generated the word so now we equip
the outcomes with basically which sentence am I looking at and if I can generate or if I have
a generative model of sentences say fleet weight feed and weight if I knew which sentence or what
sentence and where on the page that sentence was I can now generate the initial conditions for
what word I'm currently looking at and I can now generate the outcomes so we have this kind of
deep generative model that we can write down for this very simple task and it vins the following
kind of behaviour so this is a result of a typical simulation here where this little agent was
reading the sentence fleet weight feed and weight so just to put a bit of anthropomorphic semantics
on this you know if there's a cat next to the bird you run away although if there's nothing next
to the bird you do nothing if there's some seeds next to the bird you feed so that's the sentence
and this is depicts where the agent acted now action here is just basically where am I going
to look next and it's driven purely by this intrinsic motivation intrinsic value this uncertainty
reducing aspect of maximising expected free energy and what we see is very much characteristic
of real eye movements during reading so what we have at the bottom are renditions of the
posterior beliefs about this little agent about the current letter and the ensuing word and then
beliefs about the actual sentence that is being accumulated over time and the key thing from
this simulation is the the agent skips what it doesn't need to know and moves on to the next
letter or the next word once she has assured herself with sufficient confidence that she
knows what the letter must be so as soon as she sees a cat letter she knows she knows that there
must be the word must be flee and she can move to the next word and so I'm very very efficiently
harvest the right information in order to resolve uncertainty about what is generating this both
the first and the second level so that by the end there is an accrual or accumulation of evidence
for the hypothesis that this was the best sentence the best context that explains this sequence or
trajectory of observations active observations in the sense that they're accompanied by action
this is the same results but I'm just writing them out in a different way here in terms of
the different probabilities ascribed to different sentences by the higher level of the
generative model and the different probabilities assigned to the different words at the lower level
and I've cartoon this separation of temporal time scales here simply with this line as it moves
through over time showing the slow accumulation of evidence for the first sentence which is only
resolved at the end because it's in the last word that discriminates between these two sentences
and the fast switching of beliefs posterior beliefs about the word that's currently being
perceived or assimilated that's passing evidence up accumulating evidence for the word here.
I've used this format and the final format I've used here is just to sort of take this
if you like synthetic or simulation of neural activity and then I've filtered it using the same
filters that people in electrophysiology might use and the reason I've done that is just to show
that this has a degree of validity in relation to empirical neural responses in terms of
pre-secalic neural activity in the prefrontal cortex that when filtered looks very much like
peri-secalic field potentials during active vision in animal experiments.
So I show that just again to try and establish a point of validity or constant validity with empirical
neurobiology and message passing this is a final slide. All I've said is that there is a mechanics
of sentience available should you want to subscribe to this if one did and then you need to
write down the generative model which means that all the questions and all the different
flavours of sentient behaviour really inherit from the kind of generative model
and now I'm trying to make this relevant for a conversation about consciousness
and this is a sort of a serious idea that came out of discussions with an application to the
Templeton Foundation to try and compare and contrast active inference with
integrative information theory. So just very briefly the different kinds of generative models
may usefully map to different kinds of distinctions in a very common sensical way
in consciousness and so for example the difference between unconscious inference and conscious inference
may rest upon having deep generative models of the consequences of action that necessarily
entail counterfactuals, they entail agency and autonomy in the sense that it is me acting
on the model that is acting and action clearly speaks to some degree of embodiment.
The next level of questions well what kind of generative models would you need
to actually have qualitative experiences and to infer that you are seeing something
and usually the story unpacks along the lines of there being some kind of mental
or covert action that depends upon the precision or the confidence that you ascribe two particular
beliefs during the belief updating. So it's beliefs about the nature of the belief updating
that inherits from the uncertainty and then finally questions about self-awareness and
minimal selfhood speak to different parts of a generative model where the self now becomes
a part of the generative model that is just a hypothesis or a hidden state that provides
an accurate explanation and a simple explanation for all of the belief updating and mechanics
that are going on underneath or at lower levels or more shallow levels of the generative model.
So I've been talking for 44 minutes so I'm going to finish now with Einstein's famous tic tac.
Everything should be made as simple as possible but not simpler and that is certainly true
in terms of self-evidencing in the sense that you want to maximise the accuracy whilst
maintaining the simplicity of your explanations and with that all that remains is for me to thank
the people whose ideas I've been talking about and of course to thank you for
attention, thank you very much indeed. Oh thank you so much for your talk, that's absolutely
wonderful and so we've now come to the point where we open up the floor to see whether there's
some questions, I'm sure there will be people who want to dig deeper and we've got some time
to do so and say it's a great opportunity with Carl here to ask him about the free energy principle.
So I got some hands up already. Okay so Johannes would you like to start?
Yeah I would very much like to start, thanks very much so that was quite interesting, thank you.
So I was just wondering whether you could contrast the proposal which you had on the last slide
in particular like the difference between conscious and unconscious inference
and that being somehow tied into a deep generative model, if you could contrast that a little bit
with you know the usual higher order thought kind of idea, you know some higher order thing which
somehow has to process some lower order thing. Yeah I think it's a slightly simpler distinction,
I think sort of higher order thought theories are really much more about sort of
a metacognitive, a representation of your inference that practically you have to do
during the inversion of any generative model in the sense that you have to assess your confidence
in your beliefs or your inferences and there's nothing magical about this, we do it every day
when making inferences with things like t-tests or certainly in hierarchical generative models
of within and between subjective pets, we have to estimate the precision of our beliefs about
the uncertainty in terms of the degrees of freedom and like, so as soon as you start to actually
estimate the degrees of freedom in a model then you have a kind of metacognition in play that
you know could be read as you know you as agent gathering data and gathering these number of
observations that is much simpler than just doing the t-test by pressing the sort of the button on
your SPSS or your software package. So the first distinction I was talking about was actually
slightly simpler than that, you can run this kind of model, you can develop very very simple
generative models that don't have this higher order or hierarchical structure that would be
perfectly fit for purpose say to simulate a thermostat or a virus that wouldn't have this
higher order aspect but would at some point over the hood have an implicit model of the
consequences of action, so a thermostat by its engineering or if it was a very sophisticated
thermostat by its wiring would know that or you could reverse engineer it, that it has an
implicit generative model that things will change in terms of its thermo receptors if it switches
on a heater or not. So it was that sort of prospective depth that I was talking about,
you're having a generative model of the consequences of actions because usually
in machine learning and statistics and also in much of sort of perceptual
research and say vision research people have very very beautifully crafted generative models
that are fit for purpose for scene classification and scene construction and the like or recognising
things. What we're talking about here is generative models that include action as part of the unknown
causes of the sensor of the sensorium and that kind of model has obviously so evidently
has to include the future so it was that very simple move from a generative model of the present
