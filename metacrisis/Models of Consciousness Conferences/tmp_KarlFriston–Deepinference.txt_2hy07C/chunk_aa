Mae'n rhoi amwneud, i wych yn archlywedd会w sydd am y cyfynol iawn pa dda,
a philadol, ychw elementary a phróm Scottish,
ond z capnwch eraill wedi bodach o'u cyfynol yn m videog ofsodol,
eu bod yn en gewesen uch am hwnnw iIt,
felly mae ffynol yn wych yn unican yn ei w Stackardddhau i'ch locations.
My agenda was really to overview for you our work on the free energy principle as an offering
that addresses the physics of sentience, an account of sentient behaviour from a large
mathematical perspective or at least a sort of statistical physics perspective. At the end
I will just cover very briefly some perspectives on what this has to offer a discussion of
consciousness but this what I'm talking about is not in and of itself a theory of consciousness.
It's basically a theory of or an approach to self-evidencing so I'm going to overview
what that means. Technically I'm going to take the approach of asking what normative models of
sentient behaviour do we have available and is in a way of describing all of those models
in a crisp and clear and formal way. I'm going to take my lead from behavioural psychology and in
particular recent research in artificial intelligence and the imperatives that drive that kind of
research. The second part of the talk will be just an illustration of the ensuing mechanics of
belief updating and sentience that come from the first principle account established in the
first section and I'm going to focus on a particular aspect of belief updating namely
epistemic foraging and how that emerges from this first principle account and then we'll close
with one slide on possible utilities of this approach for further discussion. So what is
self-evidencing? Very simply it's the assertion that everything that we are and do is in the service
of maximising the evidence for our models of the world. So technically what we're saying is that
there are certain internal states and certain actions that we can avail ourselves of and that
we change our internal states and actions in order to maximise the probability of observable
outcomes under some model of the causes the latent unobservable sometimes called hidden causes
of those observations and to establish the sort of you know the the universality and the
constability of this description of sentience or sentient behaviour in terms of self-evidence
and unpacking this optimisation this normative description in terms of things which a lot of
people may be familiar with. So first of all if the probability of an outcome given say me or my model
of the world is high then those are the kind of outcomes that are characteristic of me and that I
will in a mathematical sense find attractive in the sense that they constitute an attractive set of
outcomes to which I will be drawn or that I will typically solicit or occupy and reading this
probability this evidence in that sense means that I can interpret the log of that as some value
function and indeed I'm going to quantify that in terms of a free energy function of some outcomes
given some beliefs or expectations about the causes of those outcomes. So maximising value
is simply a statement that underwrites reinforcement learning optimal control theory or if you're
in economics expected utility theory and that's nice because if you were in information theory
the negative of this value becomes self-information or surprise or more simply surprise that corresponds
to a negative free energy which means that as I'm trying to maximise value I'm trying to minimise
my self-information or surprise and this can be read in terms of the principle of maximum mutual
information in terms of things like minimum redundancy or maximum efficiency let's say
due to Horace Barlow and indeed the free energy principle. In turn if for every moment I'm minimising
my self-information I'm also look as if I'm minimising my entropy in the sense that the
expectation the time average of surprise is just entropy and that of course is the holy grail of
things like self-organisation, dissipative structures, synergetics of the kind described by
Haken and if I was a physiologist this would just be home with status is keeping my observable
states, my physiological states within some plausible viable bounds. There's a final perspective
that I'm going to exploit which is a statistical perspective that this quantity here can just be read
as Bayesian model evidence leading to accounts of belief updating and Bayesian decisions in terms
of the Bayesian brain on the perceptual side evidence accumulation and things like predictive
coding. So that's the basic premise. Technically what is this free energy bound on this value or
negative surprise? I've just written it out here in terms of some beliefs cue about hidden or latent
states of the world generating observations at any particular time in terms of a negative energy
and an entropy here and the reason I've done that is just to highlight its first connection
with James' maximum entry principle that maximising value or self-evidencing is just trying to
maximise the entropy of my beliefs about the state of the world generating observations under
constraints afforded by my generative model or my model of how those outcomes were caused
usually articulated in statistics in terms of a likelihood the probability of some outcomes given
a cause and some prior beliefs about the causes themselves, the states generating outcomes. I
should say when I talk about beliefs I'm talking about Bayesian beliefs that these are non-person,
non-propositional, they're just conditional probability distributions prior being conditioned
upon a model. So that's the structure of that. It's an important structure that can be unpacked in a
number of different ways, afford a number of different perspectives on the normative or the
theological interpretation of maximising this bound on log evidence. I've written it out here in
terms of the log of the evidence of the expected log evidence here in terms of the expected
probability of some outcomes given a model and a bound that can be never less than zero.
This I think is a useful perspective or decomposition because it appeals to explicitly
a model of how you think your observations were generated that you have to actually write down
and if you're in artificial intelligence or machine learning it gives you for free explainable AI.
What we will see furthermore is that when one takes this kind of approach to deciding what is
the optimal thing to actually do in the sense of soliciting or gathering new observations or data
this leads to a particular kind of Bayes optimality in terms of optimal Bayesian design
which in turn if you are in industry would have lots of implications for data foraging
epistemics and what might argue generalised artificial intelligence.
There's another way of writing down this expression which is I think equally informative so
all I've done is sort of move these terms around to express the free energy as the difference between
accuracy and complexity where accuracy is just if you like the goodness of fit of your model
in terms of being able to explain the data and the complexity is just the degree to which I had
to change my mind in order to provide that accurate account it's a KL divergence between
my posterior beliefs this Q beliefs about states generating the data and my prior beliefs.
So it's the degrees of freedom I've had to use up in order to provide an accurate account
of some observations under a particular model of how those observations were generated.
This is useful because of course it's compliant exactly with Occam's principle point the simplest
explanation that is not too simple in the sense that it accurately accounts for the observations at
hand and it's also a formal way to quantify the computational cost that practically by things
like the Janinsky equality and Landauw's principle also tells you if you're doing it the right way
you're doing it quickly and with a minimum amount of energy expenditure or metabolic cost so you're
looking for artifacts that are running with very small batteries or running very very efficiently
in a thermodynamic sense but the key thing I want to do with this this free energy functional
this way of writing down an optimization function that underwrites sentient behaviour is to think
about what it means in terms of deciding what data to gather next how to act upon the world
which is why I talk about sentient behaviour so the behaviour becomes quite crucial here and the
rest of my presentation is really just telling us a very simple story that if we choose to act or we
select those acts that maximise free energy in the future expected following an act what we have
for free is an account of self-evidencing in terms of two kinds of Bayes optimality this notion
of Bayes optimal design that I'm going to get those data that disambiguate my uncertainty
about the states of affairs that I'm operating under plus a minimisation of risk in terms of
making optimal decisions that we will see respectively maximise expected accuracy following
an act upon the world and the expected complexity so just to illustrate that I'm going to ask you
to think about what you're going to do so imagine you're an owl and that you're hungry and then
if this was an in-person audience I'd be asking people on the front row what are they going to do
and almost invariably they correctly respond well I'm going to look for some food which is clearly
the right thing to do but that answer has an enormous implication for the kind of functionals
the kind of objective functions that underwrite behaviour I want to compare and contrast two
particular kinds of approaches that you could choose between deliberately emphasising the differences
but I'm going to sort of repair the dialectic later on but for the moment I could say well look
there's some value function out there associated with a state of the world which I can't observe
directly if I took this action and if I can find the actions to take from any given state that
maximise this value function then I have a state action policy that is optimal in the
sense that it maximises this function of states of the world however that's not the kind of policy
that is can accommodate the answer I'm going to search for my food because in searching for my food
I am changing the attribute of a probabilistic belief namely I'm reducing my uncertainty about
the location of my prey this means that the function that we're chasing is not a function
of states of the world but a function of beliefs about states of the world or a function of a
function where that function is beliefs about the states of the world that would ensue given an action
so the fundamental difference between a value function of states and a functional an energy
functional of beliefs about states furthermore looking for food brings something else to the
table it means that state action policies are not going to do the job in the sense that it matters
whether I look for my food and then eat it or try to eat it and then look for my food so this more
belief-based formulation brings to the table the sequences of actions and that the functional
that we're talking about must be a composition of beliefs over time so we're talking about
not just the current action but the action into the future the path of trajectory or a policy
into the future and this gives a very different sort of principle or mathematical basis for
optimisation or a theological take on self-organised behaviour just mathematically
there this would conform to bellman's optimality principle and from that you would get things
like optimal control theory dynamic programming reinforcement learning basing decision theory
state action policy durations and all of that kind of approach this on the other hand because it's
optimising a a path into broad time interval of an energy function then it becomes a principle
of least action homologous to Hamilton's principle of station reaction and this would these kinds of
approaches would fall under free energy principle active inference artificial curiosity intrinsic
motivation in robotics optimal basing design reducing uncertainty by gathering the right kind
of data and sequential policy optimisation so if we now pursue the belief-based
principle of least action or station reaction approach this is the kind of architecture that
you get into a computational architecture that you get and the idea is basically that you
take your observations from the world from from your sensory organs you first of all optimise
your beliefs about the world by maximising this also known as an evidence lower bound it's bound
on the marginal likelihood of the evidence for your model so that you get the best possible beliefs
about space the world and then you use those to evaluate the free energy that you'd expect
following under those beliefs and then work out the plausibility or the probability
in terms of an expected free energy of pursuing this action or that policy or that sequence
of behaviours and then you select the one that you think is going to maximise the expected free
energy you act upon the world the world generates some new observations and so the perception
action cycle continues so that's the basic idea I've rewritten those equations out
in slightly more detail here because although they look a bit complicated there is a beautiful
symmetry here which which I just want to unpack for you so I've just rewritten the two ways of
decomposing the free energy here in terms of a bound on log evidence that can just by switching
these things around be expressed in terms of a k-r divergence between prize and posterior the
complexity and the accuracy here and here's the expected free energy and the only difference here
is that we're basically evaluating this quantity under technically a posterior predictive distribution
of outcomes that I might get if I pursued this particular policy and what we get is that the
complexity becomes risk the accuracy becomes on the negative accuracy becomes ambiguity
the evidence bound becomes an intrinsic value and the log evidence becomes an extrinsic value
so what do I mean by these things well these can have interpretations again which many of you
will be familiar with so let's just unpack and drill down on say the intrinsic value the expected
evidence bound here so if I just ignore extrinsic value what am I left with well I'm left with
this quantity here which is just the k-r divergence between my beliefs about states of affairs in the
world when I have observed the consequences of the outcomes of a policy relative to not observing
so it scores the information gain or equivalently the reduction of uncertainty
in the visual search literature it's known as basing surprise
well simply it's just the neutral information between the causes
states and consequences outcomes in the future conditioned upon a particular policy
and I can go on and start to remove various sorts of uncertainty and drill down on what this
extrinsic value means and so the first kind of uncertainty that I'm going to take off the table
is uncertainty that is induced by this ambiguity and I'm going to do that just by assuming that
I have direct access to the all the hidden states of the world I can see everything so that the
outcomes become the states or the observations become the states and when I do that this term
disappears and I'm left with this term here which is essentially a k-l divergence or a difference
between the anticipated outcomes or states of the world in the future or their outcomes
given a policy versus my prior beliefs about the kind of states or outcomes that me as a model of
my world would typically encounter so if we interpret these as prior preferences
then it's this risk that scores the difference between my anticipated outcomes and my preferred
outcomes and then in engineering this is known as k-l control and economics it's known as risk
sensitive control so if I now take the final bit of uncertainty away which is the uncertainty
about the consequences of my actions I'm now just left with no ambiguity and no risk and now
I'm just left with the expected value this log probability of my preferred outcomes
and hence expected utility theory in economics so just to rehearse what we've done here we've
taken the free energy we've taken the expected free energy in the future consequent upon an action
and then just taken away various sorts of uncertainty we've got from essentially a principle
of least action back to the bellman optimality principle but in so doing I've had to remove
various sorts of uncertainty from the game but that even more simply what we're saying here
is that optimising, extremising, expected free energy can be read as basically maximising a
mixture of your expected value your negative surprise and information gain and these two
in turn can be read as making optimal decisions that minimise Bayesian risk whilst at the same
time resolving uncertainty and ambiguity about your world through conforming to the principles
of optimal Bayesian design so practically let me show you how that works in practice so technically
what you have to do is to simulate this kind of behaviour or to understand and create
sentiant artefacts that comply with this kind of belief updating and action
referred to as active inference in my world and you have to start with the generative model and
that's basically the key message of everything I'm going to say until the very last slide everything
inherits from your model of the world for which you are trying to gather evidence and if you are
phenotypically that model you can interpret that as some people have including Jacob Howey
as self-evidencing so how do you articulate a model a generative model it is always possible
to write down a generative model in terms of the graphical model and I've done a generic
discrete state space model here that we use and I'll use in a second just to illustrate the
kind of behaviours that ensue by maximising expected free energy so here we've got a simple
model with different states that have transitions over time that depend on a probability transition
matrix that itself depends upon the policy where the policy is sampled from a Gibbs distribution
whose energy is given by the expected free energy here and then every state generates
via a likelihood mapping usually denoted by a matrix some outcome and that is a complete
description of my generative model of any world and here you can ignore the equations here they're
