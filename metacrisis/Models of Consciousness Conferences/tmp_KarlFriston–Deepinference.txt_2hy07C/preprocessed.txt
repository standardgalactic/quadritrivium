Mae'n rhoi amwneud, i wych yn archlywedd会w sydd am y cyfynol iawn pa dda,
a philadol, ychw elementary a phróm Scottish,
ond z capnwch eraill wedi bodach o'u cyfynol yn m videog ofsodol,
eu bod yn en gewesen uch am hwnnw iIt,
felly mae ffynol yn wych yn unican yn ei w Stackardddhau i'ch locations.
My agenda was really to overview for you our work on the free energy principle as an offering
that addresses the physics of sentience, an account of sentient behaviour from a large
mathematical perspective or at least a sort of statistical physics perspective. At the end
I will just cover very briefly some perspectives on what this has to offer a discussion of
consciousness but this what I'm talking about is not in and of itself a theory of consciousness.
It's basically a theory of or an approach to self-evidencing so I'm going to overview
what that means. Technically I'm going to take the approach of asking what normative models of
sentient behaviour do we have available and is in a way of describing all of those models
in a crisp and clear and formal way. I'm going to take my lead from behavioural psychology and in
particular recent research in artificial intelligence and the imperatives that drive that kind of
research. The second part of the talk will be just an illustration of the ensuing mechanics of
belief updating and sentience that come from the first principle account established in the
first section and I'm going to focus on a particular aspect of belief updating namely
epistemic foraging and how that emerges from this first principle account and then we'll close
with one slide on possible utilities of this approach for further discussion. So what is
self-evidencing? Very simply it's the assertion that everything that we are and do is in the service
of maximising the evidence for our models of the world. So technically what we're saying is that
there are certain internal states and certain actions that we can avail ourselves of and that
we change our internal states and actions in order to maximise the probability of observable
outcomes under some model of the causes the latent unobservable sometimes called hidden causes
of those observations and to establish the sort of you know the the universality and the
constability of this description of sentience or sentient behaviour in terms of self-evidence
and unpacking this optimisation this normative description in terms of things which a lot of
people may be familiar with. So first of all if the probability of an outcome given say me or my model
of the world is high then those are the kind of outcomes that are characteristic of me and that I
will in a mathematical sense find attractive in the sense that they constitute an attractive set of
outcomes to which I will be drawn or that I will typically solicit or occupy and reading this
probability this evidence in that sense means that I can interpret the log of that as some value
function and indeed I'm going to quantify that in terms of a free energy function of some outcomes
given some beliefs or expectations about the causes of those outcomes. So maximising value
is simply a statement that underwrites reinforcement learning optimal control theory or if you're
in economics expected utility theory and that's nice because if you were in information theory
the negative of this value becomes self-information or surprise or more simply surprise that corresponds
to a negative free energy which means that as I'm trying to maximise value I'm trying to minimise
my self-information or surprise and this can be read in terms of the principle of maximum mutual
information in terms of things like minimum redundancy or maximum efficiency let's say
due to Horace Barlow and indeed the free energy principle. In turn if for every moment I'm minimising
my self-information I'm also look as if I'm minimising my entropy in the sense that the
expectation the time average of surprise is just entropy and that of course is the holy grail of
things like self-organisation, dissipative structures, synergetics of the kind described by
Haken and if I was a physiologist this would just be home with status is keeping my observable
states, my physiological states within some plausible viable bounds. There's a final perspective
that I'm going to exploit which is a statistical perspective that this quantity here can just be read
as Bayesian model evidence leading to accounts of belief updating and Bayesian decisions in terms
of the Bayesian brain on the perceptual side evidence accumulation and things like predictive
coding. So that's the basic premise. Technically what is this free energy bound on this value or
negative surprise? I've just written it out here in terms of some beliefs cue about hidden or latent
states of the world generating observations at any particular time in terms of a negative energy
and an entropy here and the reason I've done that is just to highlight its first connection
with James' maximum entry principle that maximising value or self-evidencing is just trying to
maximise the entropy of my beliefs about the state of the world generating observations under
constraints afforded by my generative model or my model of how those outcomes were caused
usually articulated in statistics in terms of a likelihood the probability of some outcomes given
a cause and some prior beliefs about the causes themselves, the states generating outcomes. I
should say when I talk about beliefs I'm talking about Bayesian beliefs that these are non-person,
non-propositional, they're just conditional probability distributions prior being conditioned
upon a model. So that's the structure of that. It's an important structure that can be unpacked in a
number of different ways, afford a number of different perspectives on the normative or the
theological interpretation of maximising this bound on log evidence. I've written it out here in
terms of the log of the evidence of the expected log evidence here in terms of the expected
probability of some outcomes given a model and a bound that can be never less than zero.
This I think is a useful perspective or decomposition because it appeals to explicitly
a model of how you think your observations were generated that you have to actually write down
and if you're in artificial intelligence or machine learning it gives you for free explainable AI.
What we will see furthermore is that when one takes this kind of approach to deciding what is
the optimal thing to actually do in the sense of soliciting or gathering new observations or data
this leads to a particular kind of Bayes optimality in terms of optimal Bayesian design
which in turn if you are in industry would have lots of implications for data foraging
epistemics and what might argue generalised artificial intelligence.
There's another way of writing down this expression which is I think equally informative so
all I've done is sort of move these terms around to express the free energy as the difference between
accuracy and complexity where accuracy is just if you like the goodness of fit of your model
in terms of being able to explain the data and the complexity is just the degree to which I had
to change my mind in order to provide that accurate account it's a KL divergence between
my posterior beliefs this Q beliefs about states generating the data and my prior beliefs.
So it's the degrees of freedom I've had to use up in order to provide an accurate account
of some observations under a particular model of how those observations were generated.
This is useful because of course it's compliant exactly with Occam's principle point the simplest
explanation that is not too simple in the sense that it accurately accounts for the observations at
hand and it's also a formal way to quantify the computational cost that practically by things
like the Janinsky equality and Landauw's principle also tells you if you're doing it the right way
you're doing it quickly and with a minimum amount of energy expenditure or metabolic cost so you're
looking for artifacts that are running with very small batteries or running very very efficiently
in a thermodynamic sense but the key thing I want to do with this this free energy functional
this way of writing down an optimization function that underwrites sentient behaviour is to think
about what it means in terms of deciding what data to gather next how to act upon the world
which is why I talk about sentient behaviour so the behaviour becomes quite crucial here and the
rest of my presentation is really just telling us a very simple story that if we choose to act or we
select those acts that maximise free energy in the future expected following an act what we have
for free is an account of self-evidencing in terms of two kinds of Bayes optimality this notion
of Bayes optimal design that I'm going to get those data that disambiguate my uncertainty
about the states of affairs that I'm operating under plus a minimisation of risk in terms of
making optimal decisions that we will see respectively maximise expected accuracy following
an act upon the world and the expected complexity so just to illustrate that I'm going to ask you
to think about what you're going to do so imagine you're an owl and that you're hungry and then
if this was an in-person audience I'd be asking people on the front row what are they going to do
and almost invariably they correctly respond well I'm going to look for some food which is clearly
the right thing to do but that answer has an enormous implication for the kind of functionals
the kind of objective functions that underwrite behaviour I want to compare and contrast two
particular kinds of approaches that you could choose between deliberately emphasising the differences
but I'm going to sort of repair the dialectic later on but for the moment I could say well look
there's some value function out there associated with a state of the world which I can't observe
directly if I took this action and if I can find the actions to take from any given state that
maximise this value function then I have a state action policy that is optimal in the
sense that it maximises this function of states of the world however that's not the kind of policy
that is can accommodate the answer I'm going to search for my food because in searching for my food
I am changing the attribute of a probabilistic belief namely I'm reducing my uncertainty about
the location of my prey this means that the function that we're chasing is not a function
of states of the world but a function of beliefs about states of the world or a function of a
function where that function is beliefs about the states of the world that would ensue given an action
so the fundamental difference between a value function of states and a functional an energy
functional of beliefs about states furthermore looking for food brings something else to the
table it means that state action policies are not going to do the job in the sense that it matters
whether I look for my food and then eat it or try to eat it and then look for my food so this more
belief-based formulation brings to the table the sequences of actions and that the functional
that we're talking about must be a composition of beliefs over time so we're talking about
not just the current action but the action into the future the path of trajectory or a policy
into the future and this gives a very different sort of principle or mathematical basis for
optimisation or a theological take on self-organised behaviour just mathematically
there this would conform to bellman's optimality principle and from that you would get things
like optimal control theory dynamic programming reinforcement learning basing decision theory
state action policy durations and all of that kind of approach this on the other hand because it's
optimising a a path into broad time interval of an energy function then it becomes a principle
of least action homologous to Hamilton's principle of station reaction and this would these kinds of
approaches would fall under free energy principle active inference artificial curiosity intrinsic
motivation in robotics optimal basing design reducing uncertainty by gathering the right kind
of data and sequential policy optimisation so if we now pursue the belief-based
principle of least action or station reaction approach this is the kind of architecture that
you get into a computational architecture that you get and the idea is basically that you
take your observations from the world from from your sensory organs you first of all optimise
your beliefs about the world by maximising this also known as an evidence lower bound it's bound
on the marginal likelihood of the evidence for your model so that you get the best possible beliefs
about space the world and then you use those to evaluate the free energy that you'd expect
following under those beliefs and then work out the plausibility or the probability
in terms of an expected free energy of pursuing this action or that policy or that sequence
of behaviours and then you select the one that you think is going to maximise the expected free
energy you act upon the world the world generates some new observations and so the perception
action cycle continues so that's the basic idea I've rewritten those equations out
in slightly more detail here because although they look a bit complicated there is a beautiful
symmetry here which which I just want to unpack for you so I've just rewritten the two ways of
decomposing the free energy here in terms of a bound on log evidence that can just by switching
these things around be expressed in terms of a k-r divergence between prize and posterior the
complexity and the accuracy here and here's the expected free energy and the only difference here
is that we're basically evaluating this quantity under technically a posterior predictive distribution
of outcomes that I might get if I pursued this particular policy and what we get is that the
complexity becomes risk the accuracy becomes on the negative accuracy becomes ambiguity
the evidence bound becomes an intrinsic value and the log evidence becomes an extrinsic value
so what do I mean by these things well these can have interpretations again which many of you
will be familiar with so let's just unpack and drill down on say the intrinsic value the expected
evidence bound here so if I just ignore extrinsic value what am I left with well I'm left with
this quantity here which is just the k-r divergence between my beliefs about states of affairs in the
world when I have observed the consequences of the outcomes of a policy relative to not observing
so it scores the information gain or equivalently the reduction of uncertainty
in the visual search literature it's known as basing surprise
well simply it's just the neutral information between the causes
states and consequences outcomes in the future conditioned upon a particular policy
and I can go on and start to remove various sorts of uncertainty and drill down on what this
extrinsic value means and so the first kind of uncertainty that I'm going to take off the table
is uncertainty that is induced by this ambiguity and I'm going to do that just by assuming that
I have direct access to the all the hidden states of the world I can see everything so that the
outcomes become the states or the observations become the states and when I do that this term
disappears and I'm left with this term here which is essentially a k-l divergence or a difference
between the anticipated outcomes or states of the world in the future or their outcomes
given a policy versus my prior beliefs about the kind of states or outcomes that me as a model of
my world would typically encounter so if we interpret these as prior preferences
then it's this risk that scores the difference between my anticipated outcomes and my preferred
outcomes and then in engineering this is known as k-l control and economics it's known as risk
sensitive control so if I now take the final bit of uncertainty away which is the uncertainty
about the consequences of my actions I'm now just left with no ambiguity and no risk and now
I'm just left with the expected value this log probability of my preferred outcomes
and hence expected utility theory in economics so just to rehearse what we've done here we've
taken the free energy we've taken the expected free energy in the future consequent upon an action
and then just taken away various sorts of uncertainty we've got from essentially a principle
of least action back to the bellman optimality principle but in so doing I've had to remove
various sorts of uncertainty from the game but that even more simply what we're saying here
is that optimising, extremising, expected free energy can be read as basically maximising a
mixture of your expected value your negative surprise and information gain and these two
in turn can be read as making optimal decisions that minimise Bayesian risk whilst at the same
time resolving uncertainty and ambiguity about your world through conforming to the principles
of optimal Bayesian design so practically let me show you how that works in practice so technically
what you have to do is to simulate this kind of behaviour or to understand and create
sentiant artefacts that comply with this kind of belief updating and action
referred to as active inference in my world and you have to start with the generative model and
that's basically the key message of everything I'm going to say until the very last slide everything
inherits from your model of the world for which you are trying to gather evidence and if you are
phenotypically that model you can interpret that as some people have including Jacob Howey
as self-evidencing so how do you articulate a model a generative model it is always possible
to write down a generative model in terms of the graphical model and I've done a generic
discrete state space model here that we use and I'll use in a second just to illustrate the
kind of behaviours that ensue by maximising expected free energy so here we've got a simple
model with different states that have transitions over time that depend on a probability transition
matrix that itself depends upon the policy where the policy is sampled from a Gibbs distribution
whose energy is given by the expected free energy here and then every state generates
via a likelihood mapping usually denoted by a matrix some outcome and that is a complete
description of my generative model of any world and here you can ignore the equations here they're
just like these this graphical model out in terms of the equations and the interesting thing is
if you can write down a probabilistic graphical model there is always a conjugate kind of model
then there's a factor graph I won't go into the details all we need to know here is that the
factor graph specifies the message passing that underwrites the belief updating so there's always
a message passing scheme for any given generative model the key distinction between the graphical
model and the factor graph is that the edges become the unknowns and the nodes become the
the the factors here and we can interpret this in terms of Bayesian belief updating so this kind
of generative model leads to this kind of message passing that I've just annotated with the edges
or the messages that are passed between different nodes in this factor graph this is nice because
what it does is it provides an off the shelf technology to articulate the belief updating
in terms of a mechanics of message passing and belief updating that you can then see well are there
artifacts out there that do this kind of thing and have this computational architecture and indeed
there are the brains are probably the most compelling example of this so what I've done here
I just lifted the under this kind of generative model you get this kind of what's called variational
message passing that looks very very much like neuronal dynamics and plasticity and learning
and action selection in a brain so you know all that we need to do is just to look at well what
don't we know what are we making inferences about we just don't know states in the world
under any given policy we don't know the policy that we're pursuing I've also equipped this model
with a precision parameter which is not really central from the current arguments so the belief
updating about states just becomes a nonlinear function that you can think of as a sort of
neuronal activation function of linear mixtures of observable outcomes in states in the past
and states in the future and the policy selection inferring the policy I am pursuing akin to planning
or controllers inference is just a standard classical softmax function of my expected free
energy weighted by my inverse precision here learning transpires to be or formally identical
to associated plasticity or heavy and plasticity just basically counting co-occurrences of things
and building up parameters connections in your generative model that underwrite the message
passing as you accumulate and learn the structure of your world simply by outcomes and causes co-occurring
or the state now and the subsequent states co-occurring allowing you to update all sorts
of parameters here these are just the initial states and we've talked about action selection
as just selecting the action that is most likely under the policy that has the greatest posterior
probability and you can play all sorts of interesting games about assigning different parts of the
generative model to different parts of the world here's one particular one where we've assigned
policy selection to the striatum and the precision control to the dopaminergic system
but let me now conclude by just giving you a really simple work example of the emergent
behaviours under this mechanics the example uses a sort of very elemental two-step task imagine
that we have a subject here a little mouse who's put in a tea maze and the mouse has two moves
and it wants to secure bait or reward that can be either in the left or the right arm
and it's in this instance got this interesting opportunity to do a bit of searching in the sense
that there's an instructional cue here that tells it whether the bait is on the left or the right
hand side so this presents an interesting choice for the little mouse it can either waste one of its
moves and go to the instructional cue and then go and enjoy for the last move the reward because it
knows exactly where the reward is or it can take a chance and go either to the right or the left
arm noting that once you go into one of these absorbing states you have to stay there so
half the time it could spend twice two moves with a reward on half the time two moves without
so from the point of view of the bell and optimality principle these have the same affordances
but clearly from the point of view of the the principle of least action that has this uncertainty
reducing aspect then going to the instructional cue has the has clearly the the greatest expected
free energy and will therefore attract or determine that kind of behaviour and indeed
that's what happens so here's the results of a simulation that just I'm presenting now
irrespective of the particular design or generated model this is the kind of behaviour that one sees
here described in terms of the initial states the reward is on the left on the right
this is the policy that was selected this is the outcomes and the
expected utility and this is the learning about the initial states which was
here whether the reward was on the left hand side or the right hand side and what we basically see
is in the mouse inevitably goes for the epistemic policies infers that it should be and
therefore pursues an epistemic policy that maximises this intrinsic value this sort of
curiosity driven uncertainty reducing epistemic or explorative policy in in the first instance
but what we've done here is after the first couple of trials we've always left the reward
on the same side so that as time goes on the mouse learns that the reward the context is most
likely to be on the right on the left hand side and that has the interesting consequence but the
epistemic affordance of resolving uncertainty by going to the instructional cues starts to wane
over time because there's less uncertainty to resolve the animals becoming more familiar with
its environment and what that means is at some point that extrinsic value that utilitarian
pragmatic goal directed as opposed to information directed seeking behaviour certainly now has the
greatest expected free energy and there's a switching behaviour to more exploitative behaviour
where it goes straight to the reward there's a natural consequence of this trade-off or splitting
of the expected evidence of free energy into this pragmatic and epistemic parts
so just to illustrate the that epistemic part that is underwritten by having the right kind
of generative model of your of your world I'm going to end by demonstrating
an influence under a deep generative model so this is a slide we saw before where we have a
generative model in terms of a graphical model and the accompanying factor graph that proscribes for us
gives us the the message passing that will do the belief updating and and what I've done here
is simply put one of these generative models on top of another one to create a deep model
the first the other move that's been made here is that the probability transitions in time if you like
evolves more slowly at the deeper or the higher model than the the lower parts of the model
so technically what's happening here is that there's a slow part of the generative model that's
generating states or contexts that change slowly and at each point or each transition
there are some initial states for a much quicker faster generative model at the lower level
doing a little if you like mini epoch or trajectory for every cycle of the changes at the higher level
and I can write that down very simply this is a hierarchical extension of the previous one
this is the corresponding factor graph with all the messages sort of evidence about the current
context coming from the lower part of the model while there are reciprocal messages that provide
if you like prior constraints or contextualize the dynamics and the the sequence of behaviors
and inference perceptual inference at the lower level here and this is simple to engineer and to run
these message passing schemes to emulate inference under these deep and in virtue of the separation
of temporal scales di chronic generative models here there's a sort of very simple model of
reading it is very simple in this instance if we just focus on the lower level of the model
here are causes and here are consequences so the agent can basically see something in the
center of its vision which can be thought of as a letter in terms of little icons here
and it also feels where in the world the word it is it is currently looking and these outcomes
are generated by hidden states which essentially is where am I looking and what's what am I looking
at in terms of a word that has a composition of different letters in iconic form here so this
are the labels of these combinations of letters here fleet feed and weight so if I knew what the
word was I knew where I was looking I could generate exactly what I would see and where I'd
feel myself looking so this will be the first level and I could refer what word I was looking at
by scouting around the world at a higher level though we can now put in the sentence from
which that word came or in another way the sentence that generated the word so now we equip
the outcomes with basically which sentence am I looking at and if I can generate or if I have
a generative model of sentences say fleet weight feed and weight if I knew which sentence or what
sentence and where on the page that sentence was I can now generate the initial conditions for
what word I'm currently looking at and I can now generate the outcomes so we have this kind of
deep generative model that we can write down for this very simple task and it vins the following
kind of behaviour so this is a result of a typical simulation here where this little agent was
reading the sentence fleet weight feed and weight so just to put a bit of anthropomorphic semantics
on this you know if there's a cat next to the bird you run away although if there's nothing next
to the bird you do nothing if there's some seeds next to the bird you feed so that's the sentence
and this is depicts where the agent acted now action here is just basically where am I going
to look next and it's driven purely by this intrinsic motivation intrinsic value this uncertainty
reducing aspect of maximising expected free energy and what we see is very much characteristic
of real eye movements during reading so what we have at the bottom are renditions of the
posterior beliefs about this little agent about the current letter and the ensuing word and then
beliefs about the actual sentence that is being accumulated over time and the key thing from
this simulation is the the agent skips what it doesn't need to know and moves on to the next
letter or the next word once she has assured herself with sufficient confidence that she
knows what the letter must be so as soon as she sees a cat letter she knows she knows that there
must be the word must be flee and she can move to the next word and so I'm very very efficiently
harvest the right information in order to resolve uncertainty about what is generating this both
the first and the second level so that by the end there is an accrual or accumulation of evidence
for the hypothesis that this was the best sentence the best context that explains this sequence or
trajectory of observations active observations in the sense that they're accompanied by action
this is the same results but I'm just writing them out in a different way here in terms of
the different probabilities ascribed to different sentences by the higher level of the
generative model and the different probabilities assigned to the different words at the lower level
and I've cartoon this separation of temporal time scales here simply with this line as it moves
through over time showing the slow accumulation of evidence for the first sentence which is only
resolved at the end because it's in the last word that discriminates between these two sentences
and the fast switching of beliefs posterior beliefs about the word that's currently being
perceived or assimilated that's passing evidence up accumulating evidence for the word here.
I've used this format and the final format I've used here is just to sort of take this
if you like synthetic or simulation of neural activity and then I've filtered it using the same
filters that people in electrophysiology might use and the reason I've done that is just to show
that this has a degree of validity in relation to empirical neural responses in terms of
pre-secalic neural activity in the prefrontal cortex that when filtered looks very much like
peri-secalic field potentials during active vision in animal experiments.
So I show that just again to try and establish a point of validity or constant validity with empirical
neurobiology and message passing this is a final slide. All I've said is that there is a mechanics
of sentience available should you want to subscribe to this if one did and then you need to
write down the generative model which means that all the questions and all the different
flavours of sentient behaviour really inherit from the kind of generative model
and now I'm trying to make this relevant for a conversation about consciousness
and this is a sort of a serious idea that came out of discussions with an application to the
Templeton Foundation to try and compare and contrast active inference with
integrative information theory. So just very briefly the different kinds of generative models
may usefully map to different kinds of distinctions in a very common sensical way
in consciousness and so for example the difference between unconscious inference and conscious inference
may rest upon having deep generative models of the consequences of action that necessarily
entail counterfactuals, they entail agency and autonomy in the sense that it is me acting
on the model that is acting and action clearly speaks to some degree of embodiment.
The next level of questions well what kind of generative models would you need
to actually have qualitative experiences and to infer that you are seeing something
and usually the story unpacks along the lines of there being some kind of mental
or covert action that depends upon the precision or the confidence that you ascribe two particular
beliefs during the belief updating. So it's beliefs about the nature of the belief updating
that inherits from the uncertainty and then finally questions about self-awareness and
minimal selfhood speak to different parts of a generative model where the self now becomes
a part of the generative model that is just a hypothesis or a hidden state that provides
an accurate explanation and a simple explanation for all of the belief updating and mechanics
that are going on underneath or at lower levels or more shallow levels of the generative model.
So I've been talking for 44 minutes so I'm going to finish now with Einstein's famous tic tac.
Everything should be made as simple as possible but not simpler and that is certainly true
in terms of self-evidencing in the sense that you want to maximise the accuracy whilst
maintaining the simplicity of your explanations and with that all that remains is for me to thank
the people whose ideas I've been talking about and of course to thank you for
attention, thank you very much indeed. Oh thank you so much for your talk, that's absolutely
wonderful and so we've now come to the point where we open up the floor to see whether there's
some questions, I'm sure there will be people who want to dig deeper and we've got some time
to do so and say it's a great opportunity with Carl here to ask him about the free energy principle.
So I got some hands up already. Okay so Johannes would you like to start?
Yeah I would very much like to start, thanks very much so that was quite interesting, thank you.
So I was just wondering whether you could contrast the proposal which you had on the last slide
in particular like the difference between conscious and unconscious inference
and that being somehow tied into a deep generative model, if you could contrast that a little bit
with you know the usual higher order thought kind of idea, you know some higher order thing which
somehow has to process some lower order thing. Yeah I think it's a slightly simpler distinction,
I think sort of higher order thought theories are really much more about sort of
a metacognitive, a representation of your inference that practically you have to do
during the inversion of any generative model in the sense that you have to assess your confidence
in your beliefs or your inferences and there's nothing magical about this, we do it every day
when making inferences with things like t-tests or certainly in hierarchical generative models
of within and between subjective pets, we have to estimate the precision of our beliefs about
the uncertainty in terms of the degrees of freedom and like, so as soon as you start to actually
estimate the degrees of freedom in a model then you have a kind of metacognition in play that
you know could be read as you know you as agent gathering data and gathering these number of
observations that is much simpler than just doing the t-test by pressing the sort of the button on
your SPSS or your software package. So the first distinction I was talking about was actually
slightly simpler than that, you can run this kind of model, you can develop very very simple
generative models that don't have this higher order or hierarchical structure that would be
perfectly fit for purpose say to simulate a thermostat or a virus that wouldn't have this
higher order aspect but would at some point over the hood have an implicit model of the
consequences of action, so a thermostat by its engineering or if it was a very sophisticated
thermostat by its wiring would know that or you could reverse engineer it, that it has an
implicit generative model that things will change in terms of its thermo receptors if it switches
on a heater or not. So it was that sort of prospective depth that I was talking about,
you're having a generative model of the consequences of actions because usually
in machine learning and statistics and also in much of sort of perceptual
research and say vision research people have very very beautifully crafted generative models
that are fit for purpose for scene classification and scene construction and the like or recognising
things. What we're talking about here is generative models that include action as part of the unknown
causes of the sensor of the sensorium and that kind of model has obviously so evidently
has to include the future so it was that very simple move from a generative model of the present
or static outcomes to a model of controlled outcomes and then you put on top of that hierarchical
depth and sort of you know an HOT-like or metacognitive or hierarchical inference
that then takes it takes it takes you to the next level. Does that make sense? Yes, thank you very much.
That's great, it's great to still help cashmere cashmere and so it can actually
be. Sorry, you said it very well. Thank you very much for giving me the chance,
Professor Frist and thank you very much for the talk it was very illuminating.
My question might be quite not directly related to the modelling but in several different papers
of yours and people who work on the free energy principles there is this emphasis or there is
the references to the embodiment and inaction and that has been caused certain level of at least to me
confusion when I want to understand free energy principle from the mathematical perspective
and the way that has been taken by let's say inactivism for instance and the embodiment
and I personally feel there is a little bit of a confusion at the same time for fetchness
towards how this embodiment and agency has been actually attributed from that perspective.
Would you please mind to have some words on that? Yeah, I think the simplest thing to say
is that active inference is quintessentially inactive. As soon as you talk about
an agent or a system or anything that's separable from its environment in exchange with the environment
from the physics perspective you have to talk about trajectories and paths and dynamics and that
you're necessarily entails a reciprocal coupling between the agent and the environment and that
from the environment to the agent so that you can't get away from the inactive, the situated
aspect of that exchange that underwrites the belief updating in a sense that's exactly the
distinction that in response to the previous to Johannes' question you know that this is just
taking generative models into the world of actively exchanging with the source of data basically
so then all the mechanics of that of the kind of data that you can solicit what is the world
you're trying to model? Of course 99% of the world you're trying to model from the point of view
of a neonate is going to be your body. Did I cause that? Did mum cause that? The very fact that I
can control things may be a very late hypothesis that can be confirmed that first rests upon building
a generative model of your motor plant in your body and not just the motor aspects but also the
interceptive autonomic, the gut feelings. All of these things have to be have to be modelled so that
they can be suitably predicted so you can roll out into the future under the hypothesis of fantasy
that I'm in control of that or I'm not in control of that. It doesn't matter these are hypotheses
that you will come to learn by optimising the model so I would put this I put active inference
certainly as a corollary of the sort of physics version of the free energy principle
very very clearly in the inactive camp, in the situated camp and you know when you start to get
very practical about it in terms of the actual plant that is the interface between the belief
updating and the thing that your belief updating about that then you have to become very practical
in body and you have to write down very very carefully for example simulating simple things
like eye movements you really have to think very carefully about how quickly can you move your eyes
you know so these things become very much centre stage once you know once is that the kind of
distinction or was there something more philosophical that are you talking about radical inactivism?
That's exactly that makes me quite confused when I think about it I cannot actually have my head
around it. So I won't speak to that because people on both sides of the fence
you know a lot of my friends people like Sean Gallagher and Jacob Howe and they're like this
they try to sort of dissolve that sort of any tension between active inference and inactivism
but they always leave radical activism undone with so certainly I don't radical inactivism would
have no place in this mechanics. There's something quintessentially representational about beliefs
well so you can write down a belief about hidden states of affairs out there generating your data
unfortunately or fortunately you're committing to a mathematical representationalism which radical
inactivists won't let you do so it's very difficult to talk to radical inactivists. Thank you very much
you definitely clarified that at that point that it was actually looking for an answer.
Yeah thank you very much. Yeah that was a great question I was going to ask a similar question.
So Nguyen would you like to ask yours? Yeah thank you very much for your very clear and impressive
talk on the basis of your large work. One question is about metacognition. So you highlighted
that metacognition is a candidate for conscious experiences. From the animal investigations a metacognition
was shown by Hampton and others in chimpanzees which seem to have a very implicit unconscious
sensitivity how good their memory is. It seems to be that a lot of metacognitive processes remain
unconscious. Why do you think that metacognitive processes in your model are closely connected
to conscious processes and if so can you relate that to any evolutionary benefit or how is your
theory related to evolutionary benefits? That's an excellent question which I haven't got time to
answer in completely but just to deal with the evolutionary aspects. From the maths perspective
natural selection is rewritten as Bayesian model selection using exactly the same free energy
function. So the evidence or the free energy now becomes the adaptive fitness. This is the
probability that this model is there or certainly the outcomes, the physical outcomes,
experienced by this model is realised in a population. So evolution is read as
optimising the structure and the form of the genetic model in the sense of structure learning as
you get in say radical constructivism. Why would part of that structure learning entail
different levels of metacognitive like processes? I think the answer is very very simple. It's just
that when you're encoding or parameterising beliefs there are at least two attributes. One is
if you like the sort of the sufficient statistic that locates your probability mass over this content,
you know it's big or small, nice or nasty, fast or slow and then there's another
sufficient statistic which is the uncertainty of the dispersion and to completely optimise your
belief structure you have to do both which brings to the table a whole if you like other anatomy
which is not just about recognising or detecting this edge or this person or this mood. It's actually
also estimating the uncertainty about it and I think it's that operation of estimating the
uncertainty in my world, the inverse uncertainty would be the precision that has to operate at
every hierarchical level and I think you're absolutely right that you know we have to
in indeed in simulations and indeed in terms of analysing data you have to estimate the precision
of the data at very low levels which will be a long long way away from anything that was
personal or conscious and we do that every day again you know when doing a t-stastic. When we
estimate the standard error we are estimating a belief about the amplitude of random effects,
our uncertainty about our inference and our uncertainty about the data. If one takes that
kind of precision prediction or precision estimation so from the predictive processing
point of view we're now talking about not the predictions of content but the predictions of
the precision of the content and if one that takes that up to a particularly high level in a
hierarchy and associates it with an internal hypothesis that it is me estimating the precision
then it is that kind of metacognitive processing that I thought might be a candidate for
possibly not self-awareness but certainly you know there is self-hood implicit in that that might
have a certain kind of consciousness associated with it so that's what I've talked about before
about mental action that you know estimating the precision of various beliefs encoded at low in
the hierarchy means that you're acting on the belief updating lower down to optimise it as you
have to and that that kind of very high level precision optimisation may be the kind of covert
action internal action action upon your belief updating that might be sufficient to house a
conversation about consciousness. Thank you if I'm allowed to make only one remark I think it's
very convincing that this brings in some self-component but this self-component may be separated from the
conscious dimension but that's in a further debate. Thank you very much.
