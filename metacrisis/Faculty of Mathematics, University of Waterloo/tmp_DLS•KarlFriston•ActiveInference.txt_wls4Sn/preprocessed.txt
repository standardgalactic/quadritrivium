setting of active influence. The special focus on belief propagation under deep generative models
and a further focus not just upon optimising outcomes with respect to some cost function
but the importance of reducing uncertainty that I'm referring to in terms of epistemic foraging.
So what is self-evidencing? So the argument here is that everything can be regarded as
optimising or extremising a single quantity and by everything I mean more specifically
beliefs cue about latent or hidden states of the world that are generating outcomes or observations
here given me that I will be in fact a model of the way that these outcomes are generated by
these hidden or latent states. I'm also including not just beliefs about states of the world
generating my observations but also beliefs about how I am soliciting those observations,
how I am acting upon the world, how I'm palpating that world and the argument is that both
these beliefs are trying to maximise this quantity here which is simply the probability
of those outcomes given the kind of outcome solicitor that I am and I'm motivating that from
a number of different perspectives which I'm sure you'll all be familiar with. So for example
were I able to or were I to take the logarithm of this probability to create it into a potential
function then the kind of outcomes that I would expect to encounter are those that define my
attracting set if you like or the things that I would expect to observe given I am who I am
and therefore they become intrinsically valuable to me in the sense that they are the characteristic
outcomes that I will encounter. So if I read this quantity the log of the probability of outcomes
given me as value then this extremisation can be thought of as a statement of the things that
underwrite through enforcement learning things like optimal control theory in engineering
and in economics expected utility. I've also introduced a lower boundary of free energy
functional of these beliefs given the outcomes that plays the role of this value. These quantities
here are nice because if I take the negative value I can now tell another story in terms of
information theory that's because the negative log probability of some outcome given me is an
information theory learned as self-information or surprise or more simply surprise which means
that maximising this quantity means I'm minimising surprise or surprise and I can read that as a
statement of the principle of mutual maximum mutual information or the informatics principle
the principle is a minimum redundancy due to horizontal and indeed the free energy principle
where the free energy refers to this evidence lower bound here. In turn the average or the time
average path interval of this surprise is entropy so that means that maximising this
means that on average it will look as if I am minimising the entropy of my exchanges with the
world and of course that's the holy grail of things like self-organisation synergetics and
laser physics and beyond and if I was a physiologist this would just be a statement of homeostasis
it's just keeping those outcomes that matter within viable physiological bounds the characteristic
states that I physiologically would expect to find myself in and I use the word expectation
there because that brings me to the final reading of this quantity which if I were a statistician
I would call a model evidence and now me becomes a model my model of the way that these outcomes
or data sensory data are generated and that is also known as the marginal likelihood
or just more simply the evidence and hence self-evidence in here and that motivates the story
in terms of the Bayesian brain evidence accumulation things like predictive coding
in engineering and in the neurosciences so that's the idea let me just focus on this the
practical objective function that you would actually use in a machine learning setting
and that many people think may be an apt description for neuronal dynamics as a gradient flow on this
objective function so it is in machine learning an evidence lower bound and we'll see why that
is the case in a second but first let me just try and motivate where this comes from
and it's simplest you can probably think of it as an expression of James's maximum entropy
principle where the entropy pertains to the beliefs about the latent states that I cannot
observe so I'm trying to not commit to a very precise belief about the causes of my data under
constraints so what are those constraints well those constraints are provided by what I've labeled
here as a negative energy a potential that usually decomposed into a likelihood and a prior
together they constitute a generative model the likelihood is simply the probability of some
outcomes given their causes and the prior beliefs about those Bayesian beliefs so for that about
those causes or those states at any given time so this formulation has some nice interpretation
so I'm just going to take the the same sort of arrangement here where I'm writing out now
the free energy explicitly as a lower bound on the log of the evidence and it becomes a bound
in virtue of the fact that the free energy can be expressed as the evidence noting that oh
the arguments of this p here do not include s so we can ignore this it's just the the log
evidence minus a quantity that can never be less than zero and that makes it into a bound
by Jensen's inequality and this construction I think is really useful from a number of points
of view first of all if you're in artificial intelligence or say deep learning and you're
worried about explainability and interpretability then all of those worries are dissolved if you
commit to a particular generative model that is necessary to define the evidence because you know
or this becomes your hypothesis about how these data were generated and furthermore you can
using this formulation appeal to certain kinds of Bayes optimality and we'll look at that later on
both in terms of Bayesian decision theory but also optimal Bayesian design in the sense of you know
what evidence do I go and get in terms of supplying the most evidence that I am that I essentially
my model is a good model of the world and if I am my model then I exist in that world
and this leads from a technical point of view that sort of optimal Bayesian design perspective
to the notion not unrelated to things like David McKay's active learning that there is the best
way to go and get dated to explore the world and that's certainly going to be the focus of this talk
another way of arranging this equation reveals another very interesting set of interpretations
so all I've done I've just switched around some of the terms and it's now written in a way that
a statistician might understand this free energy functional it is a mixture of accuracy and complexity
where the accuracy is just the expected log likelihood under my beliefs about the the parameters
or the latent states generating my data with a complexity penalty that is simply the divergence
between my posterior beliefs q and my prior beliefs p so this scores the degree to which I
have to change my mind on seeing some new data in other words the degrees of freedom that I'm using
up in providing an accurate account of the data at hand and clearly that's just a statement of
Occam's principle that I aspire to or the best explanations for some observable outcomes are
those which are the simplest as possible from the point of view of computer science this is also
directly the computational cost of a belief update it's a very repeat the degree to which
I have to change stuff or move my beliefs in some information space or belief space
in order to accommodate new data and if you're in engineering and this matters literally
in terms of the amount of electricity or the amount of energy it costs in order to do that
belief updating via the Jeninsky equality or Landauers principle so we've got the right kind
of structure here that has a lot of if you like faith or constructility in relation to a number
of different perspectives and on good behavior and good data sampling this what I want to focus on
though is what it means to sample something because that's a very active thing you know the
data that I get depends upon the moves that I make and the actions that I take and where I look for
example so essentially all I'm going to be talking about is can we use this objective function this
elbow of variational free energy as the basis to define what would be the optimal way to act
and to cut a long story short what we're going to see is that if I can evaluate
the free energy consequent upon a move on the world consequent upon an act or a sequence of
acts of policy then if I can evaluate the expected free energy I can in principle select
that action that has the greatest expected free energy where in the same way we can decompose
the evidence into accuracy and complexity the expected free energy that scores the goodness
of an act becomes the expected accuracy and the expected complexity and the story is going to be
that this expected accuracy is effectively the objective function that underwrites optimal
Bayesian design and the expected complexity underwrites optimal Bayesian decisions under
some beliefs about the kind of outcomes that I encounter so that's the story and I usually
introduce this this this account by posing a question to the audience but as I'm not actually
there with you I'll have to guess what you might have said so the question is
imagine you're an owl and that you're hungry so what are you going to do and I usually pick on the
person in the front row and they say quite correctly well I'm going to look for my food
I'm going to search for my prey and indeed this is an owl searching for her prey here
and that answer has I think a number of profound implications for the kind of calculus that you
might bring to the table to score the goodness of a particular act or policy and I'm going to
deliberately contrast two approaches you will recognize either one or the other side of the
coin that I'm going to present I'm deliberately introducing a dialectic I will repair it later
and show that one is a special case of the other but for the moment I just want to emphasize
the differences and the differences are between the assumption that there exists some value
function of the states of the world that would ensue if I were to take this action denoted by you
at time t and if I knew what this value function was and I could work out the consequences of my
action then I could maximize this value function and define an optimal state action policy and
I'm denoting that state action policy as a policy which is a function of any state at any one particular
time however remember that what you're actually going to do is search you're going to search for
your food searching is a euphemism for reducing uncertainty about the location in this instance
of your prey and certainty is an attribute of a belief about states of the world which means
that in searching you're not optimizing a a function or a value function of the states of
the world you're optimizing a function function of a function of a function of beliefs about
states of the world and furthermore when you're searching for food it really matters what order
you do things in in the sense that if I tried to eat my food before I had found it I would get a
very different outcome from first looking for my food and then eating it so that calls into question
a very different kind of policy which is not a state action policy it's a policy a sequential
policy that entails a sequence of actions as we roll out into the future which means that if
optimal action is scored by a functional of beliefs and denoting that functional by g which
we'll see in a second is expected for the energy of beliefs that ensue from a particular action
then sequences of action have to be scored by a path integral or a time integral as I roll out
into the future and I'm using the word path integral deliberately because of course from the
point of view of physicists that's the the accumulation of an energy function which is
just an action so what we can do is we can regard this kind of class of optimization procedures or
formulations as committing to Bellman's optimality principle and you'll be familiar with these in
optimal control theory dynamic programming deep reinforcement learning such as utility
economics based decision theory and the like whereas this class of optimization rests upon
Hamilton's principle of station reaction and the free energy principle is one such example
in the context of unpacking that in terms of action and sequential policy optimization
we've referred to that as active inference in newer robotics it would be the approach taken
by people interested in artificial curiosity and intrinsic motivation and it is the approach
taken by people wanting to design the best kind of experiments through the principles of optimum
based in design to resolve uncertainty to quantify uncertainty in an optimal way
leading to sequentially policy optimization and partially observe the markoff decision processes
so that's what I want to pursue and the game will be trying to get back to the bellman
optimality principle and ask well what would have to be the case to make the bellman optimality
principle conciliant with Hamilton's principle of station reaction so I've just cartooned here the
if you like computational architecture that one has in mind from the point of view of these
evidence bounds or variational free energies the outcomes are sampled from the world they come in
they are used to optimize beliefs about hidden or latent states of the world
in terms of this functional the free energy functional here in order to provide an accurate
account of the outcomes in terms of unobservable states out there whilst at the same time minimizing
complexity and once we've optimized those beliefs we can then roll out into the future
under plausible policies and then we can score those in terms of the expected complexity and
accuracy which we'll see on the next slide basically corresponds to a minimize or choosing
those policies or the actions implied by each policy that minimize risk and ambiguity
so just a little bit more formal detail again you don't really need to understand the maths if
you don't if not familiar with it but there's a beautiful symmetry here between the the objective
function for good inference in the moment and the objective functions for what I should do next
in terms of the expected outcomes or the expected in this instance of free energy so I've just
written down the free energy and its two forms just by swapping the order here just to reveal
or evince the decomposition to accuracy and complexity and as we've just rehearsed
noting that this functional is abound on the log evidence which is the thing that we want to
maximize and all I've done here is now take the expectation of this under technically the posterior
predictive density over the outcomes that would happen if I took this policy pi here
and so this is just this quantity but expected under the most likely outcomes given my beliefs
so in you can see immediately that the functional form is preserved exactly with the exception of
conditioning upon a policy so the expected complexity now becomes risk and the expected
accuracy becomes ambiguity the expected bound becomes something I've called an intrinsic value
and the expected accuracy sorry log evidence now becomes an extrinsic value so what do I mean by
those well let's just go through bit by bit what these things mean first of all let's simplify
things by assuming I have no preferences I have no prior beliefs about the kind of outcomes that I
expect to sample so that just leaves me with these terms here the expected bound
but because now I'm taking an expectation over thing observations in the future these outcomes
now become random variables and now what I'm trying to do is to maximize this KL divergence
and what is that KL divergence well in the visual search literature would be known as an expected
Bayesian surprise is essentially the difference between my beliefs about states of the world but
unobserved given some future outcomes under a given policy relative to those beliefs without those
outcomes so it scores the relative entropy or the information gain afforded by those outcomes
that I would get under that policy pie and if you for those people who know information theory
you will recognize this immediately as very simply the the mutual information between causes s
states of the world and outcomes o that are observed under a particular policy pie so we're
trying to maximize the mutual information between stuff that can and cannot be cannot be seen
and let me now take focus on risk by removing ambiguity from the game so what is ambiguity
well it's just the uncertainty about the outcomes given their causes or the states here so let's
imagine as is often imagined in many aspects of machine learning that I can see directly
all the hidden states of the world so there's no ambiguity in my observations in relation to
the causes of those observations which means that basically the o's become the s's and this term
disappears just leaving us with this term here so what's this term here it's just the difference
between what I think will happen either in terms of states or outcomes because they're
effectively now the same relative to what I preferred would happen so it scores the difference
between the anticipated or expected outcomes if I pursue this policy and what I would actually
prefer or expect it to happen so in that sense it becomes risk and indeed it's this kind of functional
can be found in economics and the under the rubric of risk sensitive control in engineering
this is KL control minimizing the the distance between on the apologies the divergence between
controlled outcomes and a prior preferred outcomes so what I'm going to know is take the
last kind of uncertainty off the table which is that due to
due to the risk inherent in the intrinsic value so now it doesn't really matter
I'm assuming that there's no uncertainty if I commit to this action then the outcomes
are effectively known and that just now leaves the expected value that we
entertained in the in the first slide so notice what we've done here by taking successive
sorts of uncertainty off the table we've now got back from a generic formulation in terms
of risk and ambiguity or intrinsic and extrinsic value we've now got back to the thing that the
Bellman optimality principle would pertain to simply optimizing an expected value or a function
of the outcomes that ensued from certain states so this is the repair of the bellman
optimality principle and the Hamilton's principle of stationary action but in repairing that we've
had to ignore lots of sources of uncertainty and so in summary what we have is a notion of
and Bayes optimal behavior that has in its most general form two bits to it and I've
cartooned that here in terms of optimizing an expected free energy extremizing or choosing
the policies pie that give you the extremize the free energy which is just maximizing this
mutual information on this information gain here in the future whilst at the same time
maximizing my expected value defined in terms of my prior preferences and this can be read
basically as a combination of making Bayes optimal decisions in terms of policies I
pursue and Bayes optimal design in terms of soliciting the right kind of data which together
comprise active inference operation defined through the extremization of this expected free
energy so what I'm going to do now is try and unpack that with a simple example and then turn
to a slightly more complicated example which I think is interesting and highlights the importance
of both that intrinsic value that sort of the thing that underwrites the artificial curiosity
or the expiration and at the same time appeals to how far you can get when you consider sophisticated
or deep generative models of the world but let's start off simply so I've made a big thing about
generative models underwriting all this that you have to have a generative model before you can
evaluate the free energy have to have a free energy before you can evaluate the expected free energy
so what kind of generative models do people normally use in choice behaviour or decision
making usually people use Markov decision processes so here's a simple rendition of
a Markov decision process but it's now equipped with this universal objective function here
the expected free energy so how does this work well it's very simple we've just got different
states of the world that can generate observable outcomes and that probabilistic mapping is denoted
by likelihood mapping usually in in in our work a tensor or matrix A and then these states of the
world change over discrete time steps to other states of the world via a probability transition
matrix which we usually don't by B but crucially those probability transitions those ways in which
the world changes for example where my eyes are pointing or you know the the motion of an object
in the visual scene some of them depend upon the policies that I commit to and the policies in term
are sampled from or determined by the expected free energy so that's why we've got this slightly
augmented MDP partially observable Markov decision process here. Ignore the equations
they're just defining the functional form and the probability distributions in this graphical model
the reason I'm showing the graphical model in this format is that there's a nice conjugate
or sister kind of graphical model known as a normal style factor graph or fawny style
factor graph which tells me immediately if I can write down one of these then I can write
down one of these and if I can write down one of these I can simply and very quickly read off
the requisite message passing between different nodes over edges that would allow me to invert
this model given some observations and by inverting I simply mean mapping from the consequences back
to the causes so the forward model the geratin model can be regarded as a forward mapping from
from cause to consequence and then the inversion mediated by these message this message passing
message passes here corresponds to the inversion getting the causes from the consequences
and if I just take off the shelf belief update equations from this kind of factor graph
those people are interested there's a curious twist here that the the random variables now
or at least the sufficient statistics of random variables in this graphical model are now assigned
to the edges and the messages passed along those edges whereas the nodes now become the factors in
my probabilistic graphical model hence factor graph but the key thing from our point of view is
that there is off the shelf math to work out the message passing for any given graphical model so
I can write down the generative model I can always simulate belief updating active inference under
that generative model interesting from the point of view of neuro science and psychology this message
passing look remarkably similar to what the brain is doing so remember all we're trying to do is get
the best beliefs about what we don't know so what don't we know well we don't know the hidden state
of the world s in the past or in the future under different policies pi and we don't know the policies
pi that we might pursue in the future and I've actually equipped this probability density here
with a precision parameter beta but the key things we don't know are basically the states
and what we're doing and we also don't know the parameters of the likelihood and the initial
states d and the transition matrices b and so we have to update those so just looking at those
off the shelf message passing or belief update belief propagation or solutions to the belief
propagation and you can see that the expected state of the world under a given policy is a
nonlinear function of a linear mixtures of outcomes and beliefs or expectations about the
past and the future that has the functional form very much like we'd use in discrete updates in
neural networks with us when neuronal networks with us and voltage current activation function
applied to some connectivity matrix pooling together various expectations or activities
neural activities the policy selection is the standard softmax response function where the value
of policy is given in this instance by a precision or inverse temperature weighted free energy
functional that plays a role of if you like the value of that particular goodness of that policy
and interestingly the the temperature parameter itself can be updated and looks very much like
a reward prediction error that's another story the learning is really really trivial and looks
very much like a sensitive plasticity or heavy plasticity in the sense that you're just accumulating
coincidences or occurrences technically as Dirichlet counts or concentration parameters
in exactly the same way that people write down the sensitive plasticity rules in the brain
with an associative and a decay term here and then action you just sample the action
from the most likely policy or the policy that has greatest expectation so that's
the mechanics and the the the scheme that we use it has a degree of biological plausibility but from
I think from your perspective that's less interesting more interesting is how does it behave
and so I'm now going to illustrate that using a very simple as simple as we could get it
two step task that involves very much reducing uncertainty in searching for food
or in this instance a little red reward here so this is a game so this little mouse can make
two moves in a teammate and it prefers to be sitting with its reward so it doesn't know if
the reward is on the right or the left so it could take a gamble and I should say that if it
commits to one arm or the other it has to stay there these are absorbing states so it could take a
gamble and either be with its reward for two moves or not for two moves but this paradigm in this
paradigm the mouse has another option there's an instructional cube at the bottom of the lower arm
here that tells it definitively or at least probabilistically where the reward is so it's
now got the choice of wasting one of its moves and it could have spent with the reward where it defined
the reward by going and getting the instructional cube and then going directly to the reward so
the expected utility is exactly the same for two for the two policies but clearly the uncertainty
reduction and the information gain afforded by the instructional cube should make this policy
going to look at the instruction the sign and then exploit that knowledge by going to secure the
reward should make it much much more appealing and have much greater greater affordance these
are just the specification of this generative model in terms of the probability transition
matrices and the likelihood mapping here and here are the prior preferences and that basically mean
that you know this this animal just prefers to be sitting on on her reward and it doesn't matter
where whether it's on the left or the right hand side so very simple to write down very simple to
iterate and integrate and look at the ensuing behavior this is an illustration of behavior
where I've now marked the initial states whether the reward was on the left or the right here
with the colored circles here the posterior density or distribution over the various policies
do I stay and then move down do I stay and move down do I stay and move down do I go to the right
arm and stay there do I go to the left arm stay there do I go down then go to the right arm
all the different policies it could entertain it evaluates in terms of the expected free energy
as we have described and then samples an action from that and then the trial repeats for 32 trials
here and here are the outcomes in terms of reaction times in compute time and the the expected
value of of the different actions here I've also shown the parameters of the initial states the
context of each of these trials here is the reward likely to be on the right hand side
or the left hand side and I've done that because I played a trick on this animal I have after the
first couple of moves always left the reward on the left hand side so that as time progresses
there is the opportunity for this self-evidencing creature to accumulate knowledge about
the contingencies and the consequences of action and I've done that to illustrate a very generic
kind of the evolution of behavior as things become more familiar with their environment
and move from a mandated exploration to an exploitation so as anticipated in the first few trials
this policy is basically going look at the the queue and then go to the left hand side
because we know where the rat sorry the mouse knows where the reward is but as time goes on
there is less and less epistemic affordance or information gain afforded by this
instructional queue or condition stimulus here and that's because the the mouse knows
that the reward is always going to be or more likely to be on this side and at some point
this the intrinsic value falls below the extrinsic value of going directly to get the reward and
staying there for two moves and that's what we see here we see this discrete transition from
exploratory to exploitative behavior simply because there is no more information gain
or epistemic affordance from doing any exploration and then deterministically we move
to an exploitative behavior where we go and sit on our reward and stay there for the two trials
so that's the basic behavior I'm now going to close by giving you another example of exactly that
kind of epistemic behavior but using a slightly more sophisticated generative model of the kind
that you might use for language understanding or in this instance reading where there are no
preferences the only thing driving behavior an active sampling of the sensorium is the desire
to resolve uncertainty and the reason becomes if you like slightly slightly more sophisticated
that we're going to be using a deep generative model and a deep generative model with the twist
so here is the same graphical illustration of the generative model we've just been using
in its graphical form probabilistic graphical form and the accompanying
factor graph and all we're going to do is take one of these mdp structures and put one on top of
the other but the twist is I'm going to do it with a separation of temporal scales
and hence this diachronic here and what does that mean well what I'm going to assume is that most
worlds evolve at multiple temporal scales and the fastest scales are always contextualized
by some state transitions of a contextual nature at the level above and then some more contextual
states are contextually contextualized those contexts with an even slower timescale so how
would you write that down in terms of the graphical model well you just have a slow mark-off decision
process with transitions from one state to the next over an extended period of time which is
extended simply because this provides an initial context for a fast unfolding of states at the
lower level that are actually generating the sampled observations so from the point of view
of the generative model what we're seeing here is a slow fluctuation in some abstract contextual
state that provides the initial priors if you like empirical priors or constraints on the starting
point for a fast trajectory during one cycle of the higher state on the point of view of the
factor graph and the message passing what that means is that you know if you were to put this
generative model into that belief propagation or variational message passing scheme what you
would see is it would look as if the agent or the belief updating is accumulating evidence
at a very fast scale for the context that is a best explanation for the evidence secured or
accumulated very very quickly and then you'd have a slow accumulation of evidence for
an inference about the context in which you're operating technically what we're doing here is
basically destroying the Markovian aspect of a mark-off decision process and rendering a semi-Markovian
Markovian by introducing this separation of temporal time scales which incidentally introduces
all sorts of interesting issues about the scheduling of compute times and from the point of view of
the computation. So here's the example I'm going to use an example of reading but made very very
simple and using not letters but little visible icons so this is basically what this agent can
see she can basically foveate certain letters pile of seeds cat and the bird and she also has
access to where which which letter that she's actually looking at so what would I need what
hidden latent states will I need to generate these kinds of icons well first of all I'm going to need
to know what letters are available to look at and where I'm actually looking so if I knew for
example that there was the word feed that has these two letters bird and seed and I knew where
I was looking at this word then I could generate exactly the thing that would be observed
by foveating on that on that letter so all I need to know is the what and the where basically
what's the scene I'm looking at and where am I sampling that scene to generate a visual outcome
I've introduced here just for fun a font change by flipping the these iconographic words
around the horizontal and vertical axes so that would be an apt generative model to generate
outcomes like the probability transition matrices in terms of the different words that I might be
sampling and what would they come from well they would come from a deeper context they would come
from the sentence that I was reading and the particular word that I would need to generate
in order to generate the letter to generate the outcome would determine would depend upon where
in the sentence which word was I looking at and what was the sentence so again we have this what
and where separate hidden state factors determining the context which determines the the quicker
unfolding succession of outcomes or observed outcomes that depend upon where I look so here
I've just got six different contexts or six different sentences flee wait feed wait a succession of
these words here that ensue and I can sample by looking at the different looking at different
words within the sentence in terms of selecting the right policies in terms of jumping to the next
word having foraged around the current word so when we integrate that scheme or we solve those
belief update equations this is a kind of behavior that you see so effectively we're trying to we're
trying to reproduce natural reading behavior and what we see at the top are the is the sentence
actually shown to the synthetic subject with the four words each with two two letters in it
and these are her saccadic eye movements that she chooses through this process of active infants or
self-emotencing to resolve uncertainty to find to infer not only what am I looking at but what
is the context in which she has been looking at things many building beliefs about the sentence
or the context at hand this is a movie rendition of the posterior beliefs that reflect that evidence
accumulation in terms of the first and the second level inferences basically what what letter am I
looking at or is she looking at and what is the what is the sentence and the crucial thing that I
wanted to illustrate with this simulation is that the way in which information is sampled
is very very akin to the way that you and I would actually sample our visual world for example
in the sense that we don't hang around when there's no more information to secure from looking there
and we jump to the most informative location so notice as soon as she sees a cat she knows exactly
what the word has to be because a cat can only appear in the word flee so there's no epistemic
affordance in staying on this letter and she jumps immediately to the next letter the first
outcome the first location she chooses to look is a bit ambiguous there's nothing there that could
be a couple of things so she just checks by looking down here to resolve uncertainty introduced by the
font inversion and correctly concludes that this is a weight output a weight word and then
jumps to the next a bit of ambiguity it could be one of two things so she checks as soon as
she resolved uncertainty goes to the final outcome so to my mind you're a compelling illustration
of how to most efficiently design your sensory experiments to resolve uncertainty about your
hypothesis so this is effectively what a scientist does in terms of designing experiments but in
this context it is just simply making sense of sensory data these are the same results but using
a very different display format what i've done here is show or color code expectations about
hidden states of the world at the first and the second or deeper level here so remember these
expectations what's the word in play going to be one of three so these all sum to one what's the
sentence in play compared to one of six and these all sum to one so as the synthetic agent
accumulates by reading the text she is able to very confidently infer that the first set is
flea moves to the next letter and that is weight moves to the next letter feed and slowly
accumulate beliefs about the sentence that is generating this particular actively sample
sequence of of outcomes and because sentences one and four share the first three words she
has to wait right till the end before she can resolve uncertainty about the final sentence
and this kind of belief updating is very reminiscent again of what you see empirically
so if you actually look in the brain using sort of invasive electrophysiology
you can imagine that the unit activity here recorded in during the delay period
pre-soccalic delay period in the prefrontal cortex you can imagine that it looks very much like
the this synthetic belief updating when your activity or spiking is representing the most
likely on the expected hidden state at a high level of the generative model if I were to take
these time series here and then just filter them using the frequencies that people use in
EEG research then I get these fluctuations in synthetic local field potentials which again
look remarkably similar in many details to those observed empirically and earlier and later
visual cortex during or while measuring periscadic field potentials during active vision
and their similarity some certain aspects of this are very interesting from the point of view
of the belief updating and the sort of the compute in the sense that a slow accumulation of
evidence at the highest scale translates into slightly lower slower frequencies in terms of
belief updating that is seen both in terms of the simulated simulations but also in terms of
the empirical signatures on your own correlates of this kind of belief updating so that's it
I'll stop there just by reminding myself and everybody else that all of this ensues either
in the moment or in expectation consequent upon an action on an appreciation that evidence that
you know if you like the evidence that exists the most important existential objective function
can always be decomposed into simplicity and accuracy thereby everything that I think and do
should be basically trying to provide an accurate account but as simply as possible
and with that it only remains for me to thank those people whose ideas I've been talking about
and of course to thank you for your attention thank you very much indeed
okay thank you
Carl um great fascinating stuff and you have time for questions
seem to be getting to echo here but uh I think that the way we can do this is people
can either just pipe up with a question or can type something in the chat
um so while we're waiting for some questions to come in uh I will go ahead and ask one of my own
that um you sort of briefly skipped over the the sort of connections to the neuroscience and I do
know there's a quite some number of neuroscientists on this uh listening to the lecture so I wonder
if you could just give us a brief sort of view of that connection and why why is this uh these
Bayesian models thought to be somehow or how accurately do they they come to modeling what's
really going on in the human brain is that amazing excellent question uh yeah I deliberately took out
the the neurophysiology for time but also to emphasize the computer science part of it but
that the biological possibility and using this as an observation model of neuronal dynamics and
assistive processity is an important motivation for this work and specifically
how that could go wrong in certain conditions like schizophrenia or depression so it's a really
important question. The brief answer is everybody including me has been very impressed by how similar
the functional forms of the updates are in when just deriving the updates from a first principal
account and the sort that I've given you um and those neuronal dynamics and um connectivity
weight update um equations that have been used in computational neuroscience and this model
electrophysiology remarkably similar at a core strain level of analysis um the um further to that
there's people that's one one so that is um the kind of message passing you get when you deal with
geriatric models with continuous time and continuous state spaces so when you implement
that this sort of modeling version for those continuous time and state models what you get
out is a Bayesian filter um which um from the point of view of an engineer or a neuroscientist
would be known as predicted coding so predicted coding is the special case of this kind of Bayesian
mechanics um a geriatric model that deals with continuous states and time of the sort you might
want to use for early vision or very early acoustic processing for example and there's a lot of
biological plausibility um that attends uh writing down the dynamics in the Bayesian filter or a
predicted coding scheme and a neuronal dynamics it's basically a fast gradient flow on uh a free
energy or an evidence lower bound or a marginal maximum marginal likelihood uh and please for
those people interested it is always the case that you can um express the gradients of that
variational free energy functional as a prediction error um it could either look like a classical
prediction error in terms of the the arithmetic difference between my sensory data and my predictions
of those data or it can be the same kind of arithmetic difference but um read now as a
KL divergence in terms of differences for long probabilities in both instances you could always
write down these dynamics this gradient flow as a gradient on a prediction error so you can always
express these schemes in terms of minimizing prediction error um and I'm emphasizing the
predictive coding is to contrast it with another kind of belief update you get when you don't use
continuous state space models but you use discrete state space models and discrete updates in time
you've got a very different kind of message passing schemes the same maths and you still use
a gradient flow or a gradient um descent on the uh the negative free energy or the prediction errors
inherent in that negative free energy so you still go very plausible account of neural dynamics
but it now has a sortatory or episodic nature every 20 usually every 250 milliseconds you have
these dynamics and then you've supplied new data and then you you do get your descent to get to the
next um next point which is your the solutions that I showed you with that brief propagation before
the next bit of data comes along you do get your descent and what when you interpret those
schemes and notice that you know it's important that there's a dynamics and a gradient descent
that if you like um scheduled usually every 250 milliseconds um what that is um our data that
looks very much like what we see um in in in neuroscience um that implicitly have things
like theta gamma coupling because the the every 250 milliseconds is a theta rhythm and then the
fast gradient descent is giving faster frequencies um you can reproduce all sorts of things using
that little two maze rat or stem I gave you could you can look at you can play cells you can find
you can simulate this much negativities you can simulate heavy plasticity you can
simulate that theta gamma coupling all of that good stuff just falls out for free from this um
this this belief updating under under a discrete journal model so technically um then you
choose the propagation um to solve this in the brand you'd use um variational message passing
and form it in continuous time so that now it looks like a a dynamics a gradient flow
so and that um my point of view having done your science and and the computational aspects
I have to say that the discrete bits look more like really empirical data than the continuous bits
but it's more than like it the brain uses both kinds of message passing so the the point it has
to interface with continuous things like motion or you know fluctuations frequency lights and
sound processing it's likely that that's all advertised um pretty decoding or amazing filtering
and then when you get deeper into the hierarchy you start to have a quantized of discrete representation
that you're like grandmother cells and that you know discrete notions I am in the kitchen as
opposed to being x y z in relation to some reference or landmark point um point then the
the better explanation for empirical um both in terms of the computational architecture the
hierarchical architecture but also the dynamics seems to be um a discrete space space model
with the sort of um discretized updates every 250 milliseconds that corresponds to um whenever I
you know articulate a phoning or whatever you know the frequency with which I paint my visual
scene if I was a mouse the frequency which I'm whisking so most of this sort of discretized
sample of the world seems to appear at about four. Okay um Carl can you your microphone I think has
some we're getting a lot of static on the line I wonder if you're uh you could just try muting
and unmuting if that might help us I don't know um it may be coming from somewhere else uh so
we do have another question so uh uh Christoph do you want to ask your question directly or do
you want me should I you can go ahead and just ask uh Carl your question directly if you'd like
or I can read it so he's uh say okay uh he's he's uh we have a question about type one
processes so it's reads you have focused on type one processes in the the dual process theory style
I wonder how these principles can be applied to a more holistic model that integrates type one and
two processing so I think this is related to our what we were just talking about I guess the
how high up does this continue to go can we get this kind of what type two or deliberative
processing uh working in these kinds of models
how's my how's my study gone away yeah it's gone away yeah thank you
come close to the microphone I was yeah I'd like to say that's a great question
I'm afraid I so little at what a type one type two is so quickly tell me what they are now
I'll attempt to attempt to answer them
I guess okay I'm gonna end with myself yeah this is just referring to um you know this this idea
that um you know there's the the uh sort of unconscious uh fast processes
oh versus the system dual process theory basically and you know kind of we're talking
about that uh you know the higher levels you have more and more kind of discrete processing
you know kind of wonder how how some of these models might explain for how you acquire sort of
the higher level knowledge or how you decide to allocate I don't know mental um capacity for the
for the sequential uh you know type two processes right
so I right I think I understand that well I think just as already intimated that this
this this separation I think um corresponds to whether you're at low levels of um active
inference which which can be read as planning as inference so um in machine learning panning
as inferences basically means that choosing what to do next is an inference problem and
then I apply all the normal um inference machinery to that and then choose the best thing so that
must effectively one way of looking at planning as inference so if you had a very shallow model
um now a lot of that can be automized um technically people like to talk about amortization but
we can regard that as sort of habitizing um something that was previously deliberately
deliberately inferred um by having a slightly higher hierarchical model so
if you want to simulate this distinction what generally happens is it might be slightly
counterintuitive but um generally what happens is um that the the fast automatic which I presume is
is a type one but please forgive me I've ever got this got this wrong way around uh the type one
processing um inherits from um first of all engaging in more deliberative planning um
in explicit goal directed behavior so put very simply what we do is say with that simulation of
the um of of the mouse in the two step maze um we would um it would basically and we kept the um the
two um the two uh rewards um randomly allocated to the two arms then what would happen is that she
would or the simulated mouse would would pursue this very deliberative explorative planned behavior
she's going through um explicitly a sequence of policies and always identify this epistemic
policy this explorative policy um plan as you can read policy for plan as the best thing to do so
that would be if you like a you know um a minimal version of planned um uh motivated behavior where
the key thing is you are choosing among a number of different counterfactual policies after a period
of time though that epistemic policy can itself now become habitized or if it's the case that the
context does um does not change and the reward is always found on one side on the other side
then what emerges as we saw in the simulations is that the uncertainty about which of these
counterfactual plans to evaluate and then commit to resolves so ultimately there's a
priori just one plan at which point there is no more planning and you can immediately commit to
that um commit to that particular policy which effectively now becomes automatized or or
habitized and do you no longer have any if you like volition not that these simulations have
any volition but there's no choice at hand you just do this now notice what happens is
if now the environment changes and you've a priori now assume that the uh the environment
doesn't change but you've got these habits it's very difficult to actually get back from that habit
to the uh the active inference with uh by uh unless you relax your priors over the plans at hand so
it speaks for all sorts of interesting issues about wide devaluation for example um is a natural
and emergent property of this sort of hierarchical inference that can become habitized um with
sufficient exposure to a non-volatile environment was that the kind of difference between type 1
and type 2 that you had in mind? Yeah that makes sense it was just just flipped in a sense but
and I guess the question is like you know what type of representations uh you need that um you
know the the higher level of your mdp hierarchy versus lower where for example I guess at the
higher level we you have to think about your information source and how do I um you know
I guess explore information sources or that I take into account this this possibility it's
probably a it's a different type of uh knowledge that that uh that's exploited there versus the
habitual level yeah like that. Yeah absolutely yeah so so you know that as soon as you habitize
or automatically or your priors get sufficiently precise that they dominate all the other bits of
the expected free energy by definition that those epistemic imperatives or those epistemic
aspects are suppressed so or by construction in your in this formulation that's absolutely right
that you know you have learned that there is no value in intrinsic value in going to find out
any more information about this and you just go you do you just do do the right thing um so you're
I think that phenomena you would you'd see at all you're at all levels of planning I guess your
point is that you you clearly have to have a sufficiently deep and sophisticated generative
model to actually entertain a non-trivial number of policies all of which will be
scored in terms of you know what will I learn how much information would I get if I did that
and is that worth the payoff that comes from the prior preferences or the more utilitarian
extrinsic value value part of it so all of this does depend upon the sufficiently deep
generative model the equivalent I don't know if you're interested in in in in sort of the
steel process theory as it relates to physiology but the equivalent in physiology has been written
very about very artfully by Giovanni Pazzullo in terms of the distinction and people like
class Stefan and the distinction between allostasis and homeostasis so instead of worrying about
sort of motor choice behavior you just think about autonomic behavior interceptive inference
then the distinction I think you're talking about is whether you have an immediate homeostatic
response or whether you go and search out for the context the optimal context that enables the
homeostasis so instead of releasing insulin because my blood sugar is low I might suspend that
automatic response and then go and prepare some food to eat so that I don't need to release my
insulin so that theme of putting context upon context upon context with deep generative models
I think you'll find is endemic and it has to be in the sense that it's a key attribute of these
generative models and the generative models underlies all the Bayesian mechanics and the
behaviors that we're trying to explain. Thank you. Okay we have another question here
I'm going to just read it there's Arti Mahoutros asking about the dark room problem but I'm
going to suggest we save that for the student meeting unless you want to address that Carl
but there's another question here on the role of long-term memory on policy selection
maybe you can just asking for your thoughts on the role of long-term memory on policy selection
but she's also asking about the dark room problems if you want to explain what that is and then try
to get into that in 10 minutes we can do that here or we could save it for the for the student
meeting. We'll briefly resolve it in one minute and then turn to the other question then we can
revisit it at the student meeting so the dark room problem is the notion that if I want to minimize
my surprise I should go into a very dark room and stay there forever because I know exactly what's
going to happen to me and on the surface of it it seems very plausible as a consequence of
minimizing surprise but it just doesn't work it's one of these false thought experiments and
indeed you can see that immediately in relation to what we're just talking about in terms of
resolving uncertainty so the first thing that I do when I go into a dark room to switch on the light
why because that resolves my uncertainty about what is causing what's in that room I think
probably more tellingly the dark room problem just fails to account for action
the dark room problem is only good for cesare creatures or computers that don't have to decide
which data to gather as soon as you are moving to an active frame of reference and think about
the dynamics of what the consequences of what I'm going to do then you have to work out what's going
to lead to the least expected surprise a mathematically expected surprise is a uncertainty
so it's always those moves that reduce uncertainty and the first thing I do in a dark room is to
switch the light on. Yeah I think I've heard you say before that the creature that you would find
happily living without surprise in the dark room would be a very different creature than a human
being. Yeah and they exist and they are the troglophites I mean the dark creatures are not
surprised by being by not being surprised by bright things but I'm sure they have a
wonderful sensory ability to predict and be surprised about it. Okay so Artie your second
question was about the role of long-term memory on policy selection so you can you do you have
thoughts on that? Oh sorry right I was essentially yeah yes so long-term memory so again I'm always
referring to the simulations because I find that you know to understand the mechanics of this
it's much simpler to dismiss your intuitions and just see what these things do when you put them
in play and it is certainly the case you can get some fundamental changes in behaviour through the
belief updating applied to the parameters of the gerative model so the example in that two
that two-step maze task was basically accumulating the leafs effectively in connection strengths
about the context so that what you could regard that as encoding is a long-term memory that in
the past half hour from the point of view of the mouse I basically I'm remembering that the
reward is largely on the left hand side and this and I'm slowly learning and I'm remembering that
now the question comes so in a coarse-grained simple sense that for me would be the long-term
memory but at what temple scale so now you start to think well what would happen now if yes I left
the reward on the left the 32 moves and then moved it to the right you know how quickly would it
relearn and of course if it's accumulated all these beliefs and you know technically Dirichlet
parameters physiologically a really consolidated connection strength between the sort of you know
the populations of pre-personaptic specializations it's going to take a long time for it to reverse
its belief in the context of a volatile environment so then you get this higher time scale that now
has to optimize the impressionability of this long-term memory so to the extent that this is
really procedural memory I'm talking about here that has to be now contextualized you get really
interesting games about then well what kind of slow generative model mark of decision process
if you like would you have to have in order to model a volatile environment or simply moving
from one environment to another environment at a particular time scale so the really interesting
issues and fairly straightforward issues that have a base optimal and technical solution
when it comes to procedural memory episodic memory is much more interesting and it speaks to the
notion that you've actually got a hypothesis entailed in your generative model that I have been
here before which means you have to recognize I have sorry in order to recognize I have been here
before either overtly or covertly then you need to have a representation of I have been here before
and to recognize it covertly as the best expression what's going on you have to actually
remove all the sensory evidence so you have to be sort of in a mind wandering state or a sleep state
technically just turn down the precision of the like and my things to make it into
an agent who is more mindful not mindful I use that word and I shouldn't because it's not mindful
it's the opposite of mindfulness it's you're actually generating fantasies so I think the
notion of episodic memory is much more delicate and has yet to be properly simulated I have a
colleague Laura Comatino who's very interested in doing this and a number of other colleagues
working with Laura works with Neil Burgess but in principle you can do but you do need
you do need a high level and because of this complexity constraint it has to be
coarse grained continuously in an optimal way so you've got to forget all the instances of
I've been here before we're here now does not stand in for this particular event but this kind
of event this canonical or platonic kind of event or narrative so you have to forget the
details optimally but retain what can be conserved when you next come across this kind of event and
say oh I've been here before and when that happened I did this and then that becomes literally
instrumental in your planning is inference and your active inference and then you can sort of
plan well if I go to that corner of the room or I push that button even though I've never used
this kind of keyboard before or this iPhone before my memory of these kinds of things the last time
I did it then this happened then you start to have again in a minimal sense a formalism that
might account for some aspects of episodic memory and these have to be learned so they
can only be instantiated through repeated experience through accumulation of these
Dirichlet parameters or the learning the likelihood and the prize implicitly probability
transition matrices the things are under the narratives thank you okay we we only have a
minute left so there's there is one more question here in the in the chat which I'll just try and
summarize a question that has to do with the match between you mentioned a few times that the
the empirical observations are similar to the simulations that you see and and the question
is about how do you measure that similar similarity so for example if you're talking about words
that are being learned then how can we actually find evidence for that by looking at the
at the brain I mean can we actually measure those words in in some sense is that hope I'm
reading that question right Nick well if you're really right it was an excellent question
so at the moment what people tend to do when trying to bring together these hypothetical
architectures by committing to a specific generative model and then applying standard
brief updating schemes bring that together with empirical usually electrophysiology and
they go for certain characteristic responses that that they have survived peer review
literature for at least two decades so I'm talking about things like
mismatched negativities or p300s for example in EEG research so what you're trying to do is to
demonstrate that these again canonical or platonic electrophysiological phenomena are emergent
properties of your Bayes optimal scheme and in fact interestingly I didn't show them I'm now
regretting not doing this in the neurobiology side of this presentation but that last illustration
using that deep model of iconic reading was designed in order to elicit both mismatched
negativity and p300 and then 400 responses in the sense that we had the hypothesis that these
would be emergent properties of any inference machine that had a deep diachronic structure
or specifically the surprising outcomes which we did by changing the font would only be registered
at the lower level whereas a change in the context by changing the semantics that was violating
beliefs at the high level about the sentence I'm reading that provides a prediction about
or an expectation about the next word would account for both the hierarchical level
where you find things like the p300 p3b specifically the mismatched negativity and in
time in terms of peristimus time and indeed you know it was it was what came out of the simulations
was almost exactly what you see empirically that you've got this sort of fast surprise prediction
error like response at the lower level of the hierarchy and you've got this more protracted
contextual revision of context that you would beliefs about context at the high level of the
sort that people normally associate with the p3 at least the posterior parts of the p3 so that
would be one example and ultimately I think the best way to do it though is is just to have your
in silico brain and the real brain doing exactly the same task and then under ideal
based assumptions you can just use the active inference scheme to simulate the belief updating
in real time with exactly the same inputs and outputs as the as the as the subject and then
just look for correlations between the two so you know in principle that would be much more if you
like that that would be a more formal way to test the hypothesis that this belief this hypothesis
about this the belief updating scheme is apt to describe real neural dynamics you can basically say
I can predict the neural dynamics and vice versa from the the belief updating scheme and then you
can sort of look at this both in space and time where in the brain the lower level is where in
the brain the high level is what is more contextual and longer term and what is faster and shorter
term and in the same sense you can do that with the learning the changes in the connectivity
and the learning and we we do that anecdotally using things like dynamic causal modeling to
to measure the effective connectivity as time progresses and say an EEG paradigm and then
ask is that the kind of learning that we're seeing when we simulate the same paradigm in the computer
okay so I think we'll end it there
