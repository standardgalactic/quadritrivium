the dynamics of what the consequences of what I'm going to do then you have to work out what's going
to lead to the least expected surprise a mathematically expected surprise is a uncertainty
so it's always those moves that reduce uncertainty and the first thing I do in a dark room is to
switch the light on. Yeah I think I've heard you say before that the creature that you would find
happily living without surprise in the dark room would be a very different creature than a human
being. Yeah and they exist and they are the troglophites I mean the dark creatures are not
surprised by being by not being surprised by bright things but I'm sure they have a
wonderful sensory ability to predict and be surprised about it. Okay so Artie your second
question was about the role of long-term memory on policy selection so you can you do you have
thoughts on that? Oh sorry right I was essentially yeah yes so long-term memory so again I'm always
referring to the simulations because I find that you know to understand the mechanics of this
it's much simpler to dismiss your intuitions and just see what these things do when you put them
in play and it is certainly the case you can get some fundamental changes in behaviour through the
belief updating applied to the parameters of the gerative model so the example in that two
that two-step maze task was basically accumulating the leafs effectively in connection strengths
about the context so that what you could regard that as encoding is a long-term memory that in
the past half hour from the point of view of the mouse I basically I'm remembering that the
reward is largely on the left hand side and this and I'm slowly learning and I'm remembering that
now the question comes so in a coarse-grained simple sense that for me would be the long-term
memory but at what temple scale so now you start to think well what would happen now if yes I left
the reward on the left the 32 moves and then moved it to the right you know how quickly would it
relearn and of course if it's accumulated all these beliefs and you know technically Dirichlet
parameters physiologically a really consolidated connection strength between the sort of you know
the populations of pre-personaptic specializations it's going to take a long time for it to reverse
its belief in the context of a volatile environment so then you get this higher time scale that now
has to optimize the impressionability of this long-term memory so to the extent that this is
really procedural memory I'm talking about here that has to be now contextualized you get really
interesting games about then well what kind of slow generative model mark of decision process
if you like would you have to have in order to model a volatile environment or simply moving
from one environment to another environment at a particular time scale so the really interesting
issues and fairly straightforward issues that have a base optimal and technical solution
when it comes to procedural memory episodic memory is much more interesting and it speaks to the
notion that you've actually got a hypothesis entailed in your generative model that I have been
here before which means you have to recognize I have sorry in order to recognize I have been here
before either overtly or covertly then you need to have a representation of I have been here before
and to recognize it covertly as the best expression what's going on you have to actually
remove all the sensory evidence so you have to be sort of in a mind wandering state or a sleep state
technically just turn down the precision of the like and my things to make it into
an agent who is more mindful not mindful I use that word and I shouldn't because it's not mindful
it's the opposite of mindfulness it's you're actually generating fantasies so I think the
notion of episodic memory is much more delicate and has yet to be properly simulated I have a
colleague Laura Comatino who's very interested in doing this and a number of other colleagues
working with Laura works with Neil Burgess but in principle you can do but you do need
you do need a high level and because of this complexity constraint it has to be
coarse grained continuously in an optimal way so you've got to forget all the instances of
I've been here before we're here now does not stand in for this particular event but this kind
of event this canonical or platonic kind of event or narrative so you have to forget the
details optimally but retain what can be conserved when you next come across this kind of event and
say oh I've been here before and when that happened I did this and then that becomes literally
instrumental in your planning is inference and your active inference and then you can sort of
plan well if I go to that corner of the room or I push that button even though I've never used
this kind of keyboard before or this iPhone before my memory of these kinds of things the last time
I did it then this happened then you start to have again in a minimal sense a formalism that
might account for some aspects of episodic memory and these have to be learned so they
can only be instantiated through repeated experience through accumulation of these
Dirichlet parameters or the learning the likelihood and the prize implicitly probability
transition matrices the things are under the narratives thank you okay we we only have a
minute left so there's there is one more question here in the in the chat which I'll just try and
summarize a question that has to do with the match between you mentioned a few times that the
the empirical observations are similar to the simulations that you see and and the question
is about how do you measure that similar similarity so for example if you're talking about words
that are being learned then how can we actually find evidence for that by looking at the
at the brain I mean can we actually measure those words in in some sense is that hope I'm
reading that question right Nick well if you're really right it was an excellent question
so at the moment what people tend to do when trying to bring together these hypothetical
architectures by committing to a specific generative model and then applying standard
brief updating schemes bring that together with empirical usually electrophysiology and
they go for certain characteristic responses that that they have survived peer review
literature for at least two decades so I'm talking about things like
mismatched negativities or p300s for example in EEG research so what you're trying to do is to
demonstrate that these again canonical or platonic electrophysiological phenomena are emergent
properties of your Bayes optimal scheme and in fact interestingly I didn't show them I'm now
regretting not doing this in the neurobiology side of this presentation but that last illustration
using that deep model of iconic reading was designed in order to elicit both mismatched
negativity and p300 and then 400 responses in the sense that we had the hypothesis that these
would be emergent properties of any inference machine that had a deep diachronic structure
or specifically the surprising outcomes which we did by changing the font would only be registered
at the lower level whereas a change in the context by changing the semantics that was violating
beliefs at the high level about the sentence I'm reading that provides a prediction about
or an expectation about the next word would account for both the hierarchical level
where you find things like the p300 p3b specifically the mismatched negativity and in
time in terms of peristimus time and indeed you know it was it was what came out of the simulations
was almost exactly what you see empirically that you've got this sort of fast surprise prediction
error like response at the lower level of the hierarchy and you've got this more protracted
contextual revision of context that you would beliefs about context at the high level of the
sort that people normally associate with the p3 at least the posterior parts of the p3 so that
would be one example and ultimately I think the best way to do it though is is just to have your
in silico brain and the real brain doing exactly the same task and then under ideal
based assumptions you can just use the active inference scheme to simulate the belief updating
in real time with exactly the same inputs and outputs as the as the as the subject and then
just look for correlations between the two so you know in principle that would be much more if you
like that that would be a more formal way to test the hypothesis that this belief this hypothesis
about this the belief updating scheme is apt to describe real neural dynamics you can basically say
I can predict the neural dynamics and vice versa from the the belief updating scheme and then you
can sort of look at this both in space and time where in the brain the lower level is where in
the brain the high level is what is more contextual and longer term and what is faster and shorter
term and in the same sense you can do that with the learning the changes in the connectivity
and the learning and we we do that anecdotally using things like dynamic causal modeling to
to measure the effective connectivity as time progresses and say an EEG paradigm and then
ask is that the kind of learning that we're seeing when we simulate the same paradigm in the computer
okay so I think we'll end it there
