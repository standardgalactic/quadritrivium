setting of active influence. The special focus on belief propagation under deep generative models
and a further focus not just upon optimising outcomes with respect to some cost function
but the importance of reducing uncertainty that I'm referring to in terms of epistemic foraging.
So what is self-evidencing? So the argument here is that everything can be regarded as
optimising or extremising a single quantity and by everything I mean more specifically
beliefs cue about latent or hidden states of the world that are generating outcomes or observations
here given me that I will be in fact a model of the way that these outcomes are generated by
these hidden or latent states. I'm also including not just beliefs about states of the world
generating my observations but also beliefs about how I am soliciting those observations,
how I am acting upon the world, how I'm palpating that world and the argument is that both
these beliefs are trying to maximise this quantity here which is simply the probability
of those outcomes given the kind of outcome solicitor that I am and I'm motivating that from
a number of different perspectives which I'm sure you'll all be familiar with. So for example
were I able to or were I to take the logarithm of this probability to create it into a potential
function then the kind of outcomes that I would expect to encounter are those that define my
attracting set if you like or the things that I would expect to observe given I am who I am
and therefore they become intrinsically valuable to me in the sense that they are the characteristic
outcomes that I will encounter. So if I read this quantity the log of the probability of outcomes
given me as value then this extremisation can be thought of as a statement of the things that
underwrite through enforcement learning things like optimal control theory in engineering
and in economics expected utility. I've also introduced a lower boundary of free energy
functional of these beliefs given the outcomes that plays the role of this value. These quantities
here are nice because if I take the negative value I can now tell another story in terms of
information theory that's because the negative log probability of some outcome given me is an
information theory learned as self-information or surprise or more simply surprise which means
that maximising this quantity means I'm minimising surprise or surprise and I can read that as a
statement of the principle of mutual maximum mutual information or the informatics principle
the principle is a minimum redundancy due to horizontal and indeed the free energy principle
where the free energy refers to this evidence lower bound here. In turn the average or the time
average path interval of this surprise is entropy so that means that maximising this
means that on average it will look as if I am minimising the entropy of my exchanges with the
world and of course that's the holy grail of things like self-organisation synergetics and
laser physics and beyond and if I was a physiologist this would just be a statement of homeostasis
it's just keeping those outcomes that matter within viable physiological bounds the characteristic
states that I physiologically would expect to find myself in and I use the word expectation
there because that brings me to the final reading of this quantity which if I were a statistician
I would call a model evidence and now me becomes a model my model of the way that these outcomes
or data sensory data are generated and that is also known as the marginal likelihood
or just more simply the evidence and hence self-evidence in here and that motivates the story
in terms of the Bayesian brain evidence accumulation things like predictive coding
in engineering and in the neurosciences so that's the idea let me just focus on this the
practical objective function that you would actually use in a machine learning setting
and that many people think may be an apt description for neuronal dynamics as a gradient flow on this
objective function so it is in machine learning an evidence lower bound and we'll see why that
is the case in a second but first let me just try and motivate where this comes from
and it's simplest you can probably think of it as an expression of James's maximum entropy
principle where the entropy pertains to the beliefs about the latent states that I cannot
observe so I'm trying to not commit to a very precise belief about the causes of my data under
constraints so what are those constraints well those constraints are provided by what I've labeled
here as a negative energy a potential that usually decomposed into a likelihood and a prior
together they constitute a generative model the likelihood is simply the probability of some
outcomes given their causes and the prior beliefs about those Bayesian beliefs so for that about
those causes or those states at any given time so this formulation has some nice interpretation
so I'm just going to take the the same sort of arrangement here where I'm writing out now
the free energy explicitly as a lower bound on the log of the evidence and it becomes a bound
in virtue of the fact that the free energy can be expressed as the evidence noting that oh
the arguments of this p here do not include s so we can ignore this it's just the the log
evidence minus a quantity that can never be less than zero and that makes it into a bound
by Jensen's inequality and this construction I think is really useful from a number of points
of view first of all if you're in artificial intelligence or say deep learning and you're
worried about explainability and interpretability then all of those worries are dissolved if you
commit to a particular generative model that is necessary to define the evidence because you know
or this becomes your hypothesis about how these data were generated and furthermore you can
using this formulation appeal to certain kinds of Bayes optimality and we'll look at that later on
both in terms of Bayesian decision theory but also optimal Bayesian design in the sense of you know
what evidence do I go and get in terms of supplying the most evidence that I am that I essentially
my model is a good model of the world and if I am my model then I exist in that world
and this leads from a technical point of view that sort of optimal Bayesian design perspective
to the notion not unrelated to things like David McKay's active learning that there is the best
way to go and get dated to explore the world and that's certainly going to be the focus of this talk
another way of arranging this equation reveals another very interesting set of interpretations
so all I've done I've just switched around some of the terms and it's now written in a way that
a statistician might understand this free energy functional it is a mixture of accuracy and complexity
where the accuracy is just the expected log likelihood under my beliefs about the the parameters
or the latent states generating my data with a complexity penalty that is simply the divergence
between my posterior beliefs q and my prior beliefs p so this scores the degree to which I
have to change my mind on seeing some new data in other words the degrees of freedom that I'm using
up in providing an accurate account of the data at hand and clearly that's just a statement of
Occam's principle that I aspire to or the best explanations for some observable outcomes are
those which are the simplest as possible from the point of view of computer science this is also
directly the computational cost of a belief update it's a very repeat the degree to which
I have to change stuff or move my beliefs in some information space or belief space
in order to accommodate new data and if you're in engineering and this matters literally
in terms of the amount of electricity or the amount of energy it costs in order to do that
belief updating via the Jeninsky equality or Landauers principle so we've got the right kind
of structure here that has a lot of if you like faith or constructility in relation to a number
of different perspectives and on good behavior and good data sampling this what I want to focus on
though is what it means to sample something because that's a very active thing you know the
data that I get depends upon the moves that I make and the actions that I take and where I look for
example so essentially all I'm going to be talking about is can we use this objective function this
elbow of variational free energy as the basis to define what would be the optimal way to act
and to cut a long story short what we're going to see is that if I can evaluate
the free energy consequent upon a move on the world consequent upon an act or a sequence of
acts of policy then if I can evaluate the expected free energy I can in principle select
that action that has the greatest expected free energy where in the same way we can decompose
the evidence into accuracy and complexity the expected free energy that scores the goodness
of an act becomes the expected accuracy and the expected complexity and the story is going to be
that this expected accuracy is effectively the objective function that underwrites optimal
Bayesian design and the expected complexity underwrites optimal Bayesian decisions under
some beliefs about the kind of outcomes that I encounter so that's the story and I usually
introduce this this this account by posing a question to the audience but as I'm not actually
there with you I'll have to guess what you might have said so the question is
imagine you're an owl and that you're hungry so what are you going to do and I usually pick on the
person in the front row and they say quite correctly well I'm going to look for my food
I'm going to search for my prey and indeed this is an owl searching for her prey here
and that answer has I think a number of profound implications for the kind of calculus that you
might bring to the table to score the goodness of a particular act or policy and I'm going to
deliberately contrast two approaches you will recognize either one or the other side of the
coin that I'm going to present I'm deliberately introducing a dialectic I will repair it later
and show that one is a special case of the other but for the moment I just want to emphasize
the differences and the differences are between the assumption that there exists some value
function of the states of the world that would ensue if I were to take this action denoted by you
at time t and if I knew what this value function was and I could work out the consequences of my
action then I could maximize this value function and define an optimal state action policy and
I'm denoting that state action policy as a policy which is a function of any state at any one particular
time however remember that what you're actually going to do is search you're going to search for
your food searching is a euphemism for reducing uncertainty about the location in this instance
of your prey and certainty is an attribute of a belief about states of the world which means
that in searching you're not optimizing a a function or a value function of the states of
the world you're optimizing a function function of a function of a function of beliefs about
states of the world and furthermore when you're searching for food it really matters what order
you do things in in the sense that if I tried to eat my food before I had found it I would get a
very different outcome from first looking for my food and then eating it so that calls into question
a very different kind of policy which is not a state action policy it's a policy a sequential
policy that entails a sequence of actions as we roll out into the future which means that if
optimal action is scored by a functional of beliefs and denoting that functional by g which
we'll see in a second is expected for the energy of beliefs that ensue from a particular action
then sequences of action have to be scored by a path integral or a time integral as I roll out
into the future and I'm using the word path integral deliberately because of course from the
point of view of physicists that's the the accumulation of an energy function which is
just an action so what we can do is we can regard this kind of class of optimization procedures or
formulations as committing to Bellman's optimality principle and you'll be familiar with these in
optimal control theory dynamic programming deep reinforcement learning such as utility
economics based decision theory and the like whereas this class of optimization rests upon
Hamilton's principle of station reaction and the free energy principle is one such example
in the context of unpacking that in terms of action and sequential policy optimization
we've referred to that as active inference in newer robotics it would be the approach taken
by people interested in artificial curiosity and intrinsic motivation and it is the approach
taken by people wanting to design the best kind of experiments through the principles of optimum
based in design to resolve uncertainty to quantify uncertainty in an optimal way
leading to sequentially policy optimization and partially observe the markoff decision processes
so that's what I want to pursue and the game will be trying to get back to the bellman
optimality principle and ask well what would have to be the case to make the bellman optimality
principle conciliant with Hamilton's principle of station reaction so I've just cartooned here the
if you like computational architecture that one has in mind from the point of view of these
evidence bounds or variational free energies the outcomes are sampled from the world they come in
they are used to optimize beliefs about hidden or latent states of the world
in terms of this functional the free energy functional here in order to provide an accurate
account of the outcomes in terms of unobservable states out there whilst at the same time minimizing
complexity and once we've optimized those beliefs we can then roll out into the future
under plausible policies and then we can score those in terms of the expected complexity and
accuracy which we'll see on the next slide basically corresponds to a minimize or choosing
those policies or the actions implied by each policy that minimize risk and ambiguity
so just a little bit more formal detail again you don't really need to understand the maths if
you don't if not familiar with it but there's a beautiful symmetry here between the the objective
function for good inference in the moment and the objective functions for what I should do next
in terms of the expected outcomes or the expected in this instance of free energy so I've just
written down the free energy and its two forms just by swapping the order here just to reveal
or evince the decomposition to accuracy and complexity and as we've just rehearsed
noting that this functional is abound on the log evidence which is the thing that we want to
maximize and all I've done here is now take the expectation of this under technically the posterior
predictive density over the outcomes that would happen if I took this policy pi here
and so this is just this quantity but expected under the most likely outcomes given my beliefs
so in you can see immediately that the functional form is preserved exactly with the exception of
conditioning upon a policy so the expected complexity now becomes risk and the expected
accuracy becomes ambiguity the expected bound becomes something I've called an intrinsic value
and the expected accuracy sorry log evidence now becomes an extrinsic value so what do I mean by
those well let's just go through bit by bit what these things mean first of all let's simplify
things by assuming I have no preferences I have no prior beliefs about the kind of outcomes that I
expect to sample so that just leaves me with these terms here the expected bound
but because now I'm taking an expectation over thing observations in the future these outcomes
now become random variables and now what I'm trying to do is to maximize this KL divergence
and what is that KL divergence well in the visual search literature would be known as an expected
Bayesian surprise is essentially the difference between my beliefs about states of the world but
unobserved given some future outcomes under a given policy relative to those beliefs without those
outcomes so it scores the relative entropy or the information gain afforded by those outcomes
that I would get under that policy pie and if you for those people who know information theory
you will recognize this immediately as very simply the the mutual information between causes s
states of the world and outcomes o that are observed under a particular policy pie so we're
trying to maximize the mutual information between stuff that can and cannot be cannot be seen
and let me now take focus on risk by removing ambiguity from the game so what is ambiguity
well it's just the uncertainty about the outcomes given their causes or the states here so let's
imagine as is often imagined in many aspects of machine learning that I can see directly
all the hidden states of the world so there's no ambiguity in my observations in relation to
the causes of those observations which means that basically the o's become the s's and this term
disappears just leaving us with this term here so what's this term here it's just the difference
between what I think will happen either in terms of states or outcomes because they're
effectively now the same relative to what I preferred would happen so it scores the difference
between the anticipated or expected outcomes if I pursue this policy and what I would actually
prefer or expect it to happen so in that sense it becomes risk and indeed it's this kind of functional
can be found in economics and the under the rubric of risk sensitive control in engineering
this is KL control minimizing the the distance between on the apologies the divergence between
controlled outcomes and a prior preferred outcomes so what I'm going to know is take the
last kind of uncertainty off the table which is that due to
due to the risk inherent in the intrinsic value so now it doesn't really matter
I'm assuming that there's no uncertainty if I commit to this action then the outcomes
are effectively known and that just now leaves the expected value that we
entertained in the in the first slide so notice what we've done here by taking successive
sorts of uncertainty off the table we've now got back from a generic formulation in terms
of risk and ambiguity or intrinsic and extrinsic value we've now got back to the thing that the
