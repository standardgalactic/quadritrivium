Bellman optimality principle would pertain to simply optimizing an expected value or a function
of the outcomes that ensued from certain states so this is the repair of the bellman
optimality principle and the Hamilton's principle of stationary action but in repairing that we've
had to ignore lots of sources of uncertainty and so in summary what we have is a notion of
and Bayes optimal behavior that has in its most general form two bits to it and I've
cartooned that here in terms of optimizing an expected free energy extremizing or choosing
the policies pie that give you the extremize the free energy which is just maximizing this
mutual information on this information gain here in the future whilst at the same time
maximizing my expected value defined in terms of my prior preferences and this can be read
basically as a combination of making Bayes optimal decisions in terms of policies I
pursue and Bayes optimal design in terms of soliciting the right kind of data which together
comprise active inference operation defined through the extremization of this expected free
energy so what I'm going to do now is try and unpack that with a simple example and then turn
to a slightly more complicated example which I think is interesting and highlights the importance
of both that intrinsic value that sort of the thing that underwrites the artificial curiosity
or the expiration and at the same time appeals to how far you can get when you consider sophisticated
or deep generative models of the world but let's start off simply so I've made a big thing about
generative models underwriting all this that you have to have a generative model before you can
evaluate the free energy have to have a free energy before you can evaluate the expected free energy
so what kind of generative models do people normally use in choice behaviour or decision
making usually people use Markov decision processes so here's a simple rendition of
a Markov decision process but it's now equipped with this universal objective function here
the expected free energy so how does this work well it's very simple we've just got different
states of the world that can generate observable outcomes and that probabilistic mapping is denoted
by likelihood mapping usually in in in our work a tensor or matrix A and then these states of the
world change over discrete time steps to other states of the world via a probability transition
matrix which we usually don't by B but crucially those probability transitions those ways in which
the world changes for example where my eyes are pointing or you know the the motion of an object
in the visual scene some of them depend upon the policies that I commit to and the policies in term
are sampled from or determined by the expected free energy so that's why we've got this slightly
augmented MDP partially observable Markov decision process here. Ignore the equations
they're just defining the functional form and the probability distributions in this graphical model
the reason I'm showing the graphical model in this format is that there's a nice conjugate
or sister kind of graphical model known as a normal style factor graph or fawny style
factor graph which tells me immediately if I can write down one of these then I can write
down one of these and if I can write down one of these I can simply and very quickly read off
the requisite message passing between different nodes over edges that would allow me to invert
this model given some observations and by inverting I simply mean mapping from the consequences back
to the causes so the forward model the geratin model can be regarded as a forward mapping from
from cause to consequence and then the inversion mediated by these message this message passing
message passes here corresponds to the inversion getting the causes from the consequences
and if I just take off the shelf belief update equations from this kind of factor graph
those people are interested there's a curious twist here that the the random variables now
or at least the sufficient statistics of random variables in this graphical model are now assigned
to the edges and the messages passed along those edges whereas the nodes now become the factors in
my probabilistic graphical model hence factor graph but the key thing from our point of view is
that there is off the shelf math to work out the message passing for any given graphical model so
I can write down the generative model I can always simulate belief updating active inference under
that generative model interesting from the point of view of neuro science and psychology this message
passing look remarkably similar to what the brain is doing so remember all we're trying to do is get
the best beliefs about what we don't know so what don't we know well we don't know the hidden state
of the world s in the past or in the future under different policies pi and we don't know the policies
pi that we might pursue in the future and I've actually equipped this probability density here
with a precision parameter beta but the key things we don't know are basically the states
and what we're doing and we also don't know the parameters of the likelihood and the initial
states d and the transition matrices b and so we have to update those so just looking at those
off the shelf message passing or belief update belief propagation or solutions to the belief
propagation and you can see that the expected state of the world under a given policy is a
nonlinear function of a linear mixtures of outcomes and beliefs or expectations about the
past and the future that has the functional form very much like we'd use in discrete updates in
neural networks with us when neuronal networks with us and voltage current activation function
applied to some connectivity matrix pooling together various expectations or activities
neural activities the policy selection is the standard softmax response function where the value
of policy is given in this instance by a precision or inverse temperature weighted free energy
functional that plays a role of if you like the value of that particular goodness of that policy
and interestingly the the temperature parameter itself can be updated and looks very much like
a reward prediction error that's another story the learning is really really trivial and looks
very much like a sensitive plasticity or heavy plasticity in the sense that you're just accumulating
coincidences or occurrences technically as Dirichlet counts or concentration parameters
in exactly the same way that people write down the sensitive plasticity rules in the brain
with an associative and a decay term here and then action you just sample the action
from the most likely policy or the policy that has greatest expectation so that's
the mechanics and the the the scheme that we use it has a degree of biological plausibility but from
I think from your perspective that's less interesting more interesting is how does it behave
and so I'm now going to illustrate that using a very simple as simple as we could get it
two step task that involves very much reducing uncertainty in searching for food
or in this instance a little red reward here so this is a game so this little mouse can make
two moves in a teammate and it prefers to be sitting with its reward so it doesn't know if
the reward is on the right or the left so it could take a gamble and I should say that if it
commits to one arm or the other it has to stay there these are absorbing states so it could take a
gamble and either be with its reward for two moves or not for two moves but this paradigm in this
paradigm the mouse has another option there's an instructional cube at the bottom of the lower arm
here that tells it definitively or at least probabilistically where the reward is so it's
now got the choice of wasting one of its moves and it could have spent with the reward where it defined
the reward by going and getting the instructional cube and then going directly to the reward so
the expected utility is exactly the same for two for the two policies but clearly the uncertainty
reduction and the information gain afforded by the instructional cube should make this policy
going to look at the instruction the sign and then exploit that knowledge by going to secure the
reward should make it much much more appealing and have much greater greater affordance these
are just the specification of this generative model in terms of the probability transition
matrices and the likelihood mapping here and here are the prior preferences and that basically mean
that you know this this animal just prefers to be sitting on on her reward and it doesn't matter
where whether it's on the left or the right hand side so very simple to write down very simple to
iterate and integrate and look at the ensuing behavior this is an illustration of behavior
where I've now marked the initial states whether the reward was on the left or the right here
with the colored circles here the posterior density or distribution over the various policies
do I stay and then move down do I stay and move down do I stay and move down do I go to the right
arm and stay there do I go to the left arm stay there do I go down then go to the right arm
all the different policies it could entertain it evaluates in terms of the expected free energy
as we have described and then samples an action from that and then the trial repeats for 32 trials
here and here are the outcomes in terms of reaction times in compute time and the the expected
value of of the different actions here I've also shown the parameters of the initial states the
context of each of these trials here is the reward likely to be on the right hand side
or the left hand side and I've done that because I played a trick on this animal I have after the
first couple of moves always left the reward on the left hand side so that as time progresses
there is the opportunity for this self-evidencing creature to accumulate knowledge about
the contingencies and the consequences of action and I've done that to illustrate a very generic
kind of the evolution of behavior as things become more familiar with their environment
and move from a mandated exploration to an exploitation so as anticipated in the first few trials
this policy is basically going look at the the queue and then go to the left hand side
because we know where the rat sorry the mouse knows where the reward is but as time goes on
there is less and less epistemic affordance or information gain afforded by this
instructional queue or condition stimulus here and that's because the the mouse knows
that the reward is always going to be or more likely to be on this side and at some point
this the intrinsic value falls below the extrinsic value of going directly to get the reward and
staying there for two moves and that's what we see here we see this discrete transition from
exploratory to exploitative behavior simply because there is no more information gain
or epistemic affordance from doing any exploration and then deterministically we move
to an exploitative behavior where we go and sit on our reward and stay there for the two trials
so that's the basic behavior I'm now going to close by giving you another example of exactly that
kind of epistemic behavior but using a slightly more sophisticated generative model of the kind
that you might use for language understanding or in this instance reading where there are no
preferences the only thing driving behavior an active sampling of the sensorium is the desire
to resolve uncertainty and the reason becomes if you like slightly slightly more sophisticated
that we're going to be using a deep generative model and a deep generative model with the twist
so here is the same graphical illustration of the generative model we've just been using
in its graphical form probabilistic graphical form and the accompanying
factor graph and all we're going to do is take one of these mdp structures and put one on top of
the other but the twist is I'm going to do it with a separation of temporal scales
and hence this diachronic here and what does that mean well what I'm going to assume is that most
worlds evolve at multiple temporal scales and the fastest scales are always contextualized
by some state transitions of a contextual nature at the level above and then some more contextual
states are contextually contextualized those contexts with an even slower timescale so how
would you write that down in terms of the graphical model well you just have a slow mark-off decision
process with transitions from one state to the next over an extended period of time which is
extended simply because this provides an initial context for a fast unfolding of states at the
lower level that are actually generating the sampled observations so from the point of view
of the generative model what we're seeing here is a slow fluctuation in some abstract contextual
state that provides the initial priors if you like empirical priors or constraints on the starting
point for a fast trajectory during one cycle of the higher state on the point of view of the
factor graph and the message passing what that means is that you know if you were to put this
generative model into that belief propagation or variational message passing scheme what you
would see is it would look as if the agent or the belief updating is accumulating evidence
at a very fast scale for the context that is a best explanation for the evidence secured or
accumulated very very quickly and then you'd have a slow accumulation of evidence for
an inference about the context in which you're operating technically what we're doing here is
basically destroying the Markovian aspect of a mark-off decision process and rendering a semi-Markovian
Markovian by introducing this separation of temporal time scales which incidentally introduces
all sorts of interesting issues about the scheduling of compute times and from the point of view of
the computation. So here's the example I'm going to use an example of reading but made very very
simple and using not letters but little visible icons so this is basically what this agent can
see she can basically foveate certain letters pile of seeds cat and the bird and she also has
access to where which which letter that she's actually looking at so what would I need what
hidden latent states will I need to generate these kinds of icons well first of all I'm going to need
to know what letters are available to look at and where I'm actually looking so if I knew for
example that there was the word feed that has these two letters bird and seed and I knew where
I was looking at this word then I could generate exactly the thing that would be observed
by foveating on that on that letter so all I need to know is the what and the where basically
what's the scene I'm looking at and where am I sampling that scene to generate a visual outcome
I've introduced here just for fun a font change by flipping the these iconographic words
around the horizontal and vertical axes so that would be an apt generative model to generate
outcomes like the probability transition matrices in terms of the different words that I might be
sampling and what would they come from well they would come from a deeper context they would come
from the sentence that I was reading and the particular word that I would need to generate
in order to generate the letter to generate the outcome would determine would depend upon where
in the sentence which word was I looking at and what was the sentence so again we have this what
and where separate hidden state factors determining the context which determines the the quicker
unfolding succession of outcomes or observed outcomes that depend upon where I look so here
I've just got six different contexts or six different sentences flee wait feed wait a succession of
these words here that ensue and I can sample by looking at the different looking at different
words within the sentence in terms of selecting the right policies in terms of jumping to the next
word having foraged around the current word so when we integrate that scheme or we solve those
belief update equations this is a kind of behavior that you see so effectively we're trying to we're
trying to reproduce natural reading behavior and what we see at the top are the is the sentence
actually shown to the synthetic subject with the four words each with two two letters in it
and these are her saccadic eye movements that she chooses through this process of active infants or
self-emotencing to resolve uncertainty to find to infer not only what am I looking at but what
is the context in which she has been looking at things many building beliefs about the sentence
or the context at hand this is a movie rendition of the posterior beliefs that reflect that evidence
accumulation in terms of the first and the second level inferences basically what what letter am I
looking at or is she looking at and what is the what is the sentence and the crucial thing that I
wanted to illustrate with this simulation is that the way in which information is sampled
is very very akin to the way that you and I would actually sample our visual world for example
in the sense that we don't hang around when there's no more information to secure from looking there
and we jump to the most informative location so notice as soon as she sees a cat she knows exactly
what the word has to be because a cat can only appear in the word flee so there's no epistemic
affordance in staying on this letter and she jumps immediately to the next letter the first
outcome the first location she chooses to look is a bit ambiguous there's nothing there that could
be a couple of things so she just checks by looking down here to resolve uncertainty introduced by the
font inversion and correctly concludes that this is a weight output a weight word and then
jumps to the next a bit of ambiguity it could be one of two things so she checks as soon as
she resolved uncertainty goes to the final outcome so to my mind you're a compelling illustration
of how to most efficiently design your sensory experiments to resolve uncertainty about your
hypothesis so this is effectively what a scientist does in terms of designing experiments but in
this context it is just simply making sense of sensory data these are the same results but using
a very different display format what i've done here is show or color code expectations about
hidden states of the world at the first and the second or deeper level here so remember these
expectations what's the word in play going to be one of three so these all sum to one what's the
sentence in play compared to one of six and these all sum to one so as the synthetic agent
accumulates by reading the text she is able to very confidently infer that the first set is
