It's a pleasure to be here with you.
I'm Prasad Pradhan, sister-in-law for Natasha Jakes,
and this is a very happy, hopefully, occasion
where Natasha is going to be presenting the defense
for her thesis, quite a large body of work.
So we're going to get started.
She's going to present for probably a little bit more
than 45 minutes.
Then we will open it up to Q&A to the audience.
If you have to ask a clarifying question as we go,
that's fine too.
But we want to save real conversational discussion
questions for the end.
Then at some, probably after about 20 to 30 minutes
of public back and forth, we will chase out the public,
and the committee will meet with Natasha.
After that, if she's determined to have passed her defense,
there will be a small celebration in our group
around the corner back over there,
to which all will be invited.
And I'm sorry, Doug and Nando won't
be able to come for that part.
So without further ado, you've got a mic on.
OK, and are Doug and Nando bussing?
Nando has joined, but Doug is still joining.
Oh, Doug's here.
OK.
There they are.
Great.
Hey, guys.
Thank you for joining us.
All right, let's get started.
Thanks.
Thank you so much, Roz.
I'm excited to be here.
Aw.
Wow, thank you guys so much for coming.
This is great.
And if you're wondering who Doug and Nando are,
I just want to mention thanks to my committee for being here.
So Roz is my advisor.
Doug Eck and Nando DeFridis are at Google Brain
in DeepMind, and Joelle Pinot is right here.
She's from Fer and McGill, so very excited
that they're able to join us as well.
So I've been studying the problem of machine learning
for the last five years here at MIT.
And if you know me at all, you'll
know that I'm very willing to jump into an excited rant
about how amazing machine learning is
and how we're making so much progress, and it's so exciting.
And I'm not the only one interested in machine learning,
and in particularly deep learning,
has been exploding over the last few years,
because we've been able to tackle new problems that we
weren't able to tackle before.
And so I'm very hyped about this.
But we know that there's still huge problems
with these techniques.
So one of the biggest problems is generalization.
So these models can be pretty brittle.
If you train a model to play an Atari video game that
looks like this, and you just slightly switch the colors,
it just collapses.
It can't play the game at all anymore.
And that's just a far cry from a model that
could play this video game and this one at the same time.
So we know that these models are not
good at generalizing to new problems.
So this is particularly a problem in reinforcement learning,
which is an area that I'm really excited about.
And even though we've made impressive progress
in reinforcement learning in the last few years,
like we've beaten the world champion at the Game of Go,
a human champion, again, generalization
is still a problem.
So one way to overcome this is to think about intrinsic
motivation for reinforcement learning.
So this is where you design a reward function
that the agent is trying to optimize that's environment
agnostic.
And it could cause this agent to learn
across many different tasks.
So a couple of areas where we can develop intrinsic motivation
are, for example, curiosity.
So if an agent is driven to discover novelty
in its environment, then perhaps it
will learn across many environments.
Similarly, empowerment rewards an agent
for having high mutual information
between its own actions and its future state.
So basically, it wants to be able to manipulate the environment.
It wants to have power over the environment.
But if I look at this, I think the things
that motivate people aren't limited
to a drive for novelty and a drive for power.
So what else motivates people?
I would argue that people are very motivated by other people.
So if you guys were looking at me like this audience here,
I would be extremely motivated to go to the next slide.
So humans are really good social learners.
I would argue that social learning is a key component
of why humans are so smart.
And if you don't believe me, I've
thrown up a lot of citations here that argue that social learning not
only drives our cognitive development,
but it's actually tied to our ability
to transmit knowledge and evolve culturally, which
is probably why I'm standing here talking to you
about deep reinforcement learning, for example.
So the focus of this thesis is broadly
about how to incorporate forms of social learning
into AI agents.
And the first project I'm going to talk about
is a multi-agent project.
So there are multiple AI agents.
How could we develop a form of social learning among the agents?
And in this project, we wanted the agents
to be able to learn socially from each other,
but still train independently.
And so we chose to do this by giving them
intrinsic social motivation for having a causal influence over
the actions of another agent.
So why do we do this?
Well, if we focus on learning from other agents' actions,
then we don't need to observe the reward they're
getting from the environment.
And we think this is a more realistic assumption
when we think about applications
for these types of systems, like autonomous driving.
So with autonomous vehicles, there's
likely to be cars that are manufactured by Tesla and Waymo.
And the Waymo car isn't going to be
willing to share its proprietary reward function
that it's optimizing with the Tesla car.
So how could those cars still learn from each other
just by observing what they're doing in the environment,
such as whether they turn left?
So that's the idea here.
And we'll show that this causal influence reward
is actually related to rewarding the mutual information
between agents' actions.
And we have the intuition that this will help agents learn
more coordinated behavior with each other,
so they'll learn to coordinate better.
So how do you do that?
That's all fine, but how can we actually make this work?
Well, what we do is we start with a standard reinforcement
learning agent that's inputting something
from the environment.
And it's having a recurrent module
that it can understand what's been happening in the past.
It's out putting a policy about how to act in the environment.
And then we equip it with an extra model.
And we're calling this a model of other agents.
And what this model is doing is it's
predicting the actions of the other agents.
So it predicts, at the next time step,
what every other agent is going to do
conditioned on the agent's own action.
So what does that allow it to do?
Well, it's actually able to then simulate counterfactual actions
that it could have taken.
So it can retrospectively ask, given everything
that was going on at this moment in time,
what if I had taken this other action instead?
What do I predict would have happened?
So it can ask itself, what do I predict
this other agent would do if I had done this action instead?
And then what that allows it to do
is it can sample a bunch of counterfactual actions
that it could have taken, think about all the things it
could have done, plug that into its predictive model,
and sum over that to get this policy of the other agent
what the other agent would do if it wasn't considering
this agent at all.
It's sort of default policy.
And the divergence, the difference
between the other agent's default policy
and its policy conditioned on this agent's action
is a measure of the causal influence of this agent's action
on the other agent.
And the idea is to give this as a reward to agent A.
So the agents get a reward if they influence
the behavior of another agent.
And if you don't believe me that it's the causal influence,
that's what this cool graph is for.
So we know from Pearl's notion of causality in directed graphs
that causality can't pass between an unconditioned
collider node.
So the only causal path is this path
that goes forward in time between this agent's action
at time step t and agent B's action at time step t plus 1.
But we can also show that this influence reward
is related to the mutual information
between agent's actions.
So this is the formula for the mutual information here.
We see that it requires summing over the probability
of the state, for example.
But as we sample many random trajectories
