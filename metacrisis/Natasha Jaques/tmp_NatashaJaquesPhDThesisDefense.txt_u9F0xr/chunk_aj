to integrate extra information,
and you can use other techniques like unsupervised learning.
That's also in the dialogue project.
We first learn unsupervised, a decent model of conversation.
Well, decent is a...
We're stretching the term there, but, you know, we try.
And so I'm not wed to that,
but I do think RL is really promising,
and I'm really excited about it,
and basically for two reasons.
So the first is that RL is a super-broad paradigm
that basically describes the scenario
anytime an agent is interacting with the environment,
taking actions, and getting feedback.
So it's this idea of, like, not static learning,
but actually doing this interaction.
And so it's so broad that it seems like it's inevitably
going to describe whatever we want to do with AI,
whether it's a conversational AI,
whether it's a robot that's in the environment,
it's going to be interacting,
and you want it to be able to learn from that interaction.
And the second thing about this is that...
I don't know if you've seen this very well-replicated work
that shows that dopamine neurons in the human brain
are actually encoding a temporal difference signal.
Actually, Nando pointed me to this.
They're encoding a temporal difference signal
that's a fundamental component of RL.
And so I think that's a really promising clue
that we're kind of on the right track.
And yeah, just this optimization function
of estimating total expected future reward,
long-term payoff.
I do this action now, but I don't want to be greedy.
Like, how will this actually work out in the long run?
That just seems like a really promising framework
for something that our agents are going to want to know
about the world.
So that's why I like RL.
Thank you. Thanks.
Thanks also. It was a great presentation.
The question I have...
You mentioned about categorizations of different agents
and personalizations of them.
So I'm wondering if you can also frame that
in the problem of mobile-based
and how you can find the optimal policy
when I know what is the difference
between different agents or characteristics
of different agents?
How I can have optimal interactions with them?
Have you considered that or what you perceive?
Yeah, I think that would be super promising.
So if you had some model-based approach
where you basically could distill people
into some latent characteristics of people
and then use those latent characteristics,
for example, to condition your RL model
so it has a sense of who it's talking to
and how it can adapt to that person,
I think that's really promising.
And then you could even do something more interesting
where your RL model observes some feedback
that actually that person didn't like it,
so you miscategorize them.
So could it update your extraction of the latent factors?
That could be really cool.
You guys are giving me so many good ideas.
Thank you. Let me ask a few questions.
Yes.
And first start by saying it's been an absolutely pleasure
to see the progress of this research
and in particular seeing you engage more and more deeply
with some of the technical models
and technical questions arising.
Still not losing perspective
of the application side of things.
So from that point of view,
I think in maybe reaching back
to one of the previous questions,
some of your contributions
are around this concept of intrinsic motivation
and what that means in terms of reward function.
And you go through an interesting mechanism
and you're not the only one,
a lot of the literature does too,
to sort of take intrinsic motivation
and turn it into explicit reward.
In this case, by labeling different emotions
and essentially giving a valence
positive and negative to these.
And so I'm left a little bit
with this interrogation of should we be skeptical
that there is a useful notion
of intrinsic motivation, right?
Isn't it the case that we may be labeling
some of these things intrinsic,
but at the end of the day,
there's no learning without a loss function
and in RL, there's no reinforcement learning
without some notion of a reward
and the only way to get to that
is to have human input on that definition.
Yeah, that's a really good point.
And I think there's kind of a blurry line
where you could get into a situation
where you're actually doing quite a lot of reward engineering
and still calling it intrinsic motivation.
So maybe broadly, people do have an intrinsic motivation
to get along with other people or have empathy,
but at what point does it actually just get into
hand engineering something for a particular domain?
And so yes, that's a valid criticism.
Do I think there is actually a concept
of intrinsic motivation that is meaningful?
I would say yes.
I think the work in curiosity is really promising.
I do think humans have intrinsic motivations
like curiosity, like empathy that help drive us
to not only learn but coordinate with others.
I think the direction of self-supervised learning
is a promising one that does seem like
the agent has to have some intrinsic thing.
It's trying to optimize.
It's not necessarily an externally imposed goal,
but it's a way that it can learn.
It's a way that it benefits from learning
from interacting with the environment.
So do I think the concept is useful?
Yes.
Do I think every implementation of it
is deeply generalizable to many different environments?
Not necessarily, right?
Yeah.
And I'm going to ask another question
that deals a little bit with this notion of reward
but a little bit more with the work
that you've done on off-policy learning.
And so one thing that's interesting here
is the space of actions that you considered
for off-policy learning.
Am I right in understanding that it was actually
at the level of choosing the words?
Yes.
So this is very interesting because it's an action space
that is much higher dimensional than most of the action space
that are considered usually for off-policy learning.
Is there something we need to do differently
when we think of off-policy learning
in small action space versus large action space?
Because in your case, you sort of, you know,
you lined up there's these previous methods
and here's a few variants that you've explored
but you didn't necessarily lean into the complexity
that arises from having a very large action space
and what that means specifically for off-policy learning.
So what are your thoughts on that?
Yes.
So I think there is definitely something that you have to do
that's very different and one thing I mentioned
is that you have to have a reasonable pre-trained model.
Otherwise this overestimation issue
extrapolation area issue would just be catastrophic.
So you absolutely have to be bringing in some prior knowledge
to give you some way to start over which of the words you could emit
and so that is quite complicated.
And then the second thing I would say is that
it's not the only answer to doing this.
So we're currently looking into, as you saw it,
it's a hierarchical model, right?
So we're actually currently looking into controlling the model
at the upper level of the hierarchy,
like the sentence level of the hierarchy
because we think that could reduce some of the complexity
and allow the model to sort of choose conversational goals
that it's conditioning on.
So that's a future direction I'm thinking of, yeah.
Is there a notion at which that helps you get a better sense
of like the similarity between the spaces?
Because one of the things in off-policy learning
is right trying to relate the space explored
by your behavioral policy versus your target policy
and relate that in some representation.
Do you think that gets at the question
how to have a lower dimensional latent representation
between these things?
So I think the thing about doing this on the word level
is that it's going to be really hard to do credit assignment
and propagate back the correct choice
when you have so many words in a conversation
and each time you choose each word you have such a broad action space.
So doing credit assignment at the word level
is going to make less sense than doing credit assignment
at the sentence level because that's closer to getting at
like what is the conversational intention of what I'm trying to say.
So I think that makes sense.
And so that handles both the dimensionality of the action space
but also the sequence length
which is also probably longer in your cases
