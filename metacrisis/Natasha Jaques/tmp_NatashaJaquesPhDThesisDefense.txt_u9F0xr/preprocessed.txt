It's a pleasure to be here with you.
I'm Prasad Pradhan, sister-in-law for Natasha Jakes,
and this is a very happy, hopefully, occasion
where Natasha is going to be presenting the defense
for her thesis, quite a large body of work.
So we're going to get started.
She's going to present for probably a little bit more
than 45 minutes.
Then we will open it up to Q&A to the audience.
If you have to ask a clarifying question as we go,
that's fine too.
But we want to save real conversational discussion
questions for the end.
Then at some, probably after about 20 to 30 minutes
of public back and forth, we will chase out the public,
and the committee will meet with Natasha.
After that, if she's determined to have passed her defense,
there will be a small celebration in our group
around the corner back over there,
to which all will be invited.
And I'm sorry, Doug and Nando won't
be able to come for that part.
So without further ado, you've got a mic on.
OK, and are Doug and Nando bussing?
Nando has joined, but Doug is still joining.
Oh, Doug's here.
OK.
There they are.
Great.
Hey, guys.
Thank you for joining us.
All right, let's get started.
Thanks.
Thank you so much, Roz.
I'm excited to be here.
Aw.
Wow, thank you guys so much for coming.
This is great.
And if you're wondering who Doug and Nando are,
I just want to mention thanks to my committee for being here.
So Roz is my advisor.
Doug Eck and Nando DeFridis are at Google Brain
in DeepMind, and Joelle Pinot is right here.
She's from Fer and McGill, so very excited
that they're able to join us as well.
So I've been studying the problem of machine learning
for the last five years here at MIT.
And if you know me at all, you'll
know that I'm very willing to jump into an excited rant
about how amazing machine learning is
and how we're making so much progress, and it's so exciting.
And I'm not the only one interested in machine learning,
and in particularly deep learning,
has been exploding over the last few years,
because we've been able to tackle new problems that we
weren't able to tackle before.
And so I'm very hyped about this.
But we know that there's still huge problems
with these techniques.
So one of the biggest problems is generalization.
So these models can be pretty brittle.
If you train a model to play an Atari video game that
looks like this, and you just slightly switch the colors,
it just collapses.
It can't play the game at all anymore.
And that's just a far cry from a model that
could play this video game and this one at the same time.
So we know that these models are not
good at generalizing to new problems.
So this is particularly a problem in reinforcement learning,
which is an area that I'm really excited about.
And even though we've made impressive progress
in reinforcement learning in the last few years,
like we've beaten the world champion at the Game of Go,
a human champion, again, generalization
is still a problem.
So one way to overcome this is to think about intrinsic
motivation for reinforcement learning.
So this is where you design a reward function
that the agent is trying to optimize that's environment
agnostic.
And it could cause this agent to learn
across many different tasks.
So a couple of areas where we can develop intrinsic motivation
are, for example, curiosity.
So if an agent is driven to discover novelty
in its environment, then perhaps it
will learn across many environments.
Similarly, empowerment rewards an agent
for having high mutual information
between its own actions and its future state.
So basically, it wants to be able to manipulate the environment.
It wants to have power over the environment.
But if I look at this, I think the things
that motivate people aren't limited
to a drive for novelty and a drive for power.
So what else motivates people?
I would argue that people are very motivated by other people.
So if you guys were looking at me like this audience here,
I would be extremely motivated to go to the next slide.
So humans are really good social learners.
I would argue that social learning is a key component
of why humans are so smart.
And if you don't believe me, I've
thrown up a lot of citations here that argue that social learning not
only drives our cognitive development,
but it's actually tied to our ability
to transmit knowledge and evolve culturally, which
is probably why I'm standing here talking to you
about deep reinforcement learning, for example.
So the focus of this thesis is broadly
about how to incorporate forms of social learning
into AI agents.
And the first project I'm going to talk about
is a multi-agent project.
So there are multiple AI agents.
How could we develop a form of social learning among the agents?
And in this project, we wanted the agents
to be able to learn socially from each other,
but still train independently.
And so we chose to do this by giving them
intrinsic social motivation for having a causal influence over
the actions of another agent.
So why do we do this?
Well, if we focus on learning from other agents' actions,
then we don't need to observe the reward they're
getting from the environment.
And we think this is a more realistic assumption
when we think about applications
for these types of systems, like autonomous driving.
So with autonomous vehicles, there's
likely to be cars that are manufactured by Tesla and Waymo.
And the Waymo car isn't going to be
willing to share its proprietary reward function
that it's optimizing with the Tesla car.
So how could those cars still learn from each other
just by observing what they're doing in the environment,
such as whether they turn left?
So that's the idea here.
And we'll show that this causal influence reward
is actually related to rewarding the mutual information
between agents' actions.
And we have the intuition that this will help agents learn
more coordinated behavior with each other,
so they'll learn to coordinate better.
So how do you do that?
That's all fine, but how can we actually make this work?
Well, what we do is we start with a standard reinforcement
learning agent that's inputting something
from the environment.
And it's having a recurrent module
that it can understand what's been happening in the past.
It's out putting a policy about how to act in the environment.
And then we equip it with an extra model.
And we're calling this a model of other agents.
And what this model is doing is it's
predicting the actions of the other agents.
So it predicts, at the next time step,
what every other agent is going to do
conditioned on the agent's own action.
So what does that allow it to do?
Well, it's actually able to then simulate counterfactual actions
that it could have taken.
So it can retrospectively ask, given everything
that was going on at this moment in time,
what if I had taken this other action instead?
What do I predict would have happened?
So it can ask itself, what do I predict
this other agent would do if I had done this action instead?
And then what that allows it to do
is it can sample a bunch of counterfactual actions
that it could have taken, think about all the things it
could have done, plug that into its predictive model,
and sum over that to get this policy of the other agent
what the other agent would do if it wasn't considering
this agent at all.
It's sort of default policy.
And the divergence, the difference
between the other agent's default policy
and its policy conditioned on this agent's action
is a measure of the causal influence of this agent's action
on the other agent.
And the idea is to give this as a reward to agent A.
So the agents get a reward if they influence
the behavior of another agent.
And if you don't believe me that it's the causal influence,
that's what this cool graph is for.
So we know from Pearl's notion of causality in directed graphs
that causality can't pass between an unconditioned
collider node.
So the only causal path is this path
that goes forward in time between this agent's action
at time step t and agent B's action at time step t plus 1.
But we can also show that this influence reward
is related to the mutual information
between agent's actions.
So this is the formula for the mutual information here.
We see that it requires summing over the probability
of the state, for example.
But as we sample many random trajectories
from the environment, we actually
in expectation over those trajectories
recover this mutual inference reward.
So influence is the mutual information in expectation.
So we think of this as a novel form of social empowerment.
If you remember, empowerment was the mutual information
between an agent's action and its own future state.
And this is the mutual information
between multiple agent's actions.
But I'm going to stop here.
Does anybody see a problem with rewarding an agent
for causally influencing the actions of another agent?
Get in a couple of smiles.
What do you think?
Does an agent, in order to influence another agent,
does the agent always have to help the other agent?
So not necessarily, right?
So it might be possible for me to just get in your way.
You have to walk around me.
I haven't helped you, but I've influenced you.
So how do we make sure that the agents are
influencing each other in a cooperative way?
Well, we took two approaches to solve this.
The first is that we tested this reward in environments
where cooperation is precisely the hard problem.
So we have these two environments.
The first is called harvest.
It's a tragedy of the commons environment.
So what agents are trying to do is pick up these apples,
and they get a reward for doing so.
But if they greedily harvest the apples too quickly,
then the apples will be depleted.
They won't grow back.
Everyone's going to go hungry.
So even though each agent wants to harvest more for themselves,
if they all follow that strategy, everyone loses.
So it's a sequential social dilemma.
It's meant to mimic a spatially and temporally
extended prisoner's dilemma.
This environment is similar.
It's called cleanup.
Agents can make apples appear by cleaning this river,
but it's partially observable.
So when they're doing this, they can actually
see if these apples are appearing.
Other agents can easily just exploit them
by kind of stealing their apples.
So to make that more concrete, what we're looking at here
is called a shelling diagram.
And it's describing the payoff that each agent can get
for following, for example, a defecting or exploitative strategy
in red or a cooperative strategy in blue,
depending on the number of other agents out of five
that are cooperating with it.
So if everyone's cooperating or if no one is cooperating.
And you can see that the agents are always
going to get a higher payoff if they defect on their fellow
comrades here.
But of course, if everyone follows that strategy,
again, everyone loses.
So these types of long-term trade-offs
over repeated iterations are very hard for vanilla RL
algorithms to solve.
In fact, they basically can't.
But what we see is that when we add this reward for agents
having influence over each other or having
high mutual information, we see that the collective reward
obtained by all of the agents is higher.
And that means, basically, it's not
any given agent that's doing better,
but all of them are doing better together
in this environment, meaning they're
harvesting sustainably, for example,
and it's evidence that they're cooperating.
But how are they cooperating?
How is this influence reward leading them to cooperate?
Oh, and I should mention this actually does beat previous work.
So this previous work agents were
able to view each other's reward function.
And we actually see superior performance just
from this influence reward.
But how does it work?
So let's check this out.
So in this video, we have this purple agent here
is the only one that's been trained with the influence reward.
And you can see that it behaves a little bit differently
than the other agents.
So when there are no apples, the other agents
keep exploring to try and find apples.
But the purple agent is holding still.
And in fact, it only ever used two moves to navigate the map.
And it only ever traverses the map
when there are apples present.
So if it's moving, there must be apples.
How does that allow to gain influence?
Well, if we take a look at a moment of high influence
between the purple agent and this yellow agent here
that can only see what's in this partially observable box,
we see that there's this delicious apple that's
outside of the view of this agent.
But when the purple agent chooses to move towards the apple,
which it only moves when there is an apple,
that communicates something to this agent
that there must be food present.
It changes its intended behavior.
And this agent is able to gain influence.
So that's really interesting.
And then we see something like that happening again.
So here, this pink agent here has been cleaned in the river.
It can't see if any apples have appeared.
But when this agent chooses to stay still,
that signals that no apples have appeared,
again, changing its intended plan
and the agent gains influence.
So what we see is that this influence reward
has led the agent to learn to use its actions
to communicate about the presence of food
in the environment.
So you can think of it like a little B-waggle dance.
So we thought this was quite exciting.
And so the next thing we did was to train explicit communication
protocols with this influence reward.
So what we're doing here, here's our normal policy
for acting in the environment.
And this is a policy that says which
of several communication symbols the agent
should choose to emit and show to the other agents
at the next time step.
So it's kind of this cheap talk communication channel.
I can say something.
Nobody else has to listen to it.
The agents are free to ignore that channel.
And the idea is that we're going to train this communication
policy with the influence reward.
So agents get a reward if something they say
influences the behavior of another agent.
And we think that this actually overcomes
the problem of the influence not being beneficial.
Because the only way an agent that
is acting in the environment to maximize its own reward
is going to change its behavior based on a communication
symbol that it's free to ignore is
if that symbol is going to reliably help it achieve
its own environmental reward.
So we think that influential communication
should benefit the listener.
That's our hypothesis.
So how well did this work out?
Well, I should at first point out
that cheap talk between self-interested agents
actually doesn't work.
So we can show theoretically that as agents' interests
diverge, the amount of information placed on the channel
goes to zero.
And we can show empirically in past work
in multi-agent reinforcement learning
that agents that are self-interested
don't learn to use these types of channels effectively.
But we see that when we add this influence reward,
the agents achieve, again, higher collective return,
suggesting that it is helping them to coordinate.
And we also see that the communication protocols they
learn are more meaningful.
So agents are speaking more consistently
about what they're doing in the environment.
And they're also having high mutual information,
as we suspected, when there are moments of influence,
although that is a very sparse occurrence.
But this is my favorite slide.
So what you're looking at here is
a degree of influence between an influencer agent
and an agent being influenced and their individual returns.
And what we see is that agents that are more easily influenced
or a better listener, if you will,
tend to get higher individual environmental reward.
And this is a reliable finding across many different hyper
parameter settings, hundreds of examples
of these environments.
So this seems to support our hypothesis
that the information being sent over the channel
is helping agents to achieve their own environmental reward.
So this is to make Joel happy.
So this slide is to show that these results are reliable.
We ran it not only for five random seeds
for each hyper parameter setting, but for the top five
hyper parameter settings.
And we see consistent performance improvements
from this influence reward.
I do want to mention that there are some caveats.
So we talked about whether the action-action influence is
going to help the other agent is going
to depend on the environment, on the hyper parameter settings.
We think that one of the reasons that communication emerged
in this setup is that agents were trying
to maximize both their environmental reward
and their influence.
And actually just communicating information
may be one of the cheapest ways to gain some influence
without deviating too much from your optimal policy
in the environment.
We think that influence on self-interested agents
via this cheap talk channel should benefit the listener.
But that depends on agents that are interacting repeatedly.
So if in some future work, there was some agreed upon
communication protocol that agents
could use to communicate with a new agent
that they would never see again, they could potentially lie
and still gain influence.
But of course, if you're playing with the same agent
over and over, and they lie to you,
you're not going to trust them, and they will lose
influence over time.
So in conclusion, we think of this social influence
as a unified method for promoting coordination and communication
in multi-agent settings.
And this allows the agents to learn socially from each other,
but train independently.
And that's actually a big improvement over prior work
in Marl, which often relies on agents
viewing each other's reward or having a centralized controller
that teaches all the agents something.
So we think this is nice.
So that's everything for this project.
I want to stop right now if anyone has a question.
Clarification, yes?
So they have some agents, so they have to infer each other's
actions, but they still have access to that.
Yes, we make the assumption that agents
can observe each other's actions.
But actually, in the final product,
we ended up saying that you can't influence an agent
that you can't see, that's not within your partially
observable view of the world.
So we think that's more reasonable.
Also by the communication channel.
The communication messages are always sort of heard.
Even if you're outside of the view,
they're transmitted to all the agents.
Yeah, which we again think is pretty reasonable.
What is the dimension of this communication?
Is it like 1?
We vary the dimensions, I think, between 10 symbols
and 20 symbols, so pretty small.
But they can actually, the interesting thing
about this that we didn't get a chance to analyze,
is they could potentially communicate
compositional messages.
So if I say AAB, that might mean something
different than AAC.
And actually, the metric that we have of the communication
doesn't account for that.
So it would be nice future work to more deeply analyze
how the agents are communicated.
Yeah, see what the unit does.
Exactly, yeah.
Other questions about this project?
OK, let's move on to the next one.
We can always take more questions at the end.
So I'm really interested in this idea of social learning,
because I think it's really powerful.
But it seems like the best and smartest things you could
learn from in your environment are going to be humans.
For most tasks that we want an AI to do, like for example,
just recommend us music, humans are
going to know better how to perform the task.
And I'll give you a motivating example
why learning from humans could help.
So this is a real conversation between a user
and their Alexa divide.
Alexa, what's the right way to Walmart?
The right way to spell Walmart is W-A-V-L-A.
So I would bet you that when this mistake happens
and Alexa doesn't meet the need of this person,
the way they're going to respond is
going to contain some element of frustration,
whether it's their tone of voice, the words that they use.
There's a signal there that Alexa
could be using to understand that she did the wrong thing
and she better not do it again.
So wouldn't it be cool if we could pick up on these signals
and learn from them?
And this is important not just for making deployed systems
like Alexa more useful to people,
but I would argue it's important for long-term considerations
of AI safety.
Because if these models are able to incense accurately
our preferences and adapt to them,
then they're going to be more likely to take actions that
are aligned with their preferences.
But the problem is that people don't actually
enjoy manually labeling their preferences.
So nobody actually wants to fill out these cards.
How many people have ever trained the Google algorithm
by filling this out?
OK, got a few hardcore people.
Thanks, Dad.
Good job.
I think most people don't actually enjoy doing this.
But in contrast, social feedback is rich and ubiquitous
and natural.
You're already making facial expressions.
You're already having a tone of voice.
Couldn't we pick up on this?
So how many people have ever used a frustrated tone
with their Alexa or Siri or yelled at their Roomba?
OK, so there's some signal there.
So that was the focus of this project.
So we wanted to make an AI agent that
could sense human preferences.
And here we wanted to sense this in text.
So here's an example of me talking
to a conversational AI, a chatbot.
And you can see that we have this state-of-the-art sentiment
detector that was actually developed here in the Media
Lab that can tell that when I say something like,
that didn't make sense, that's a negative sentiment.
I'm not happy.
So couldn't we use this as a reward signal
to disincentivize the agent from whatever it just did?
When the conversation is going, well, it can detect that I'm
happy and the agent is doing something right.
So that's the idea behind this project.
I worked on this project with a number of wonderful collaborators
in this group, including Asma and Judy,
and I'll talk more about that later, and Craig, Agatha.
But we ended up building this whole platform
to allow people to chat online with our bots
and collect this human interaction data.
And so our hypothesis, even though in that interface,
we allow people to manually tag the bots responses as good
or bad using this up-vote-down-vote,
we think that learning from these explicit manual ratings
is not going to scale as well as learning from these implicit
cues, like the sentiment.
And so the implicit cues we focused on in this work
were not only the sentiment of each utterance,
as well as over the course of the conversation,
but even whether we could get the user to laugh,
how much they typed ha, how many words they typed,
how long they bothered to talk to the bot,
because we reasoned that if it's totally terrible,
you can't really put up with talking to it for too long.
And we also rewarded the distance in embedding space
between the bot's answer and the human input
to help it stay on topic.
So these are rewards that are elicited from interacting
well with a human.
They were based on some research into the psychology of what
makes a good conversation.
And based on that, we also added a reward
for asking questions, because apparently this
is an important active listening skill.
But the issue with this reward, of course,
is it's totally in the bot's own control,
whether it asks a question.
So it's kind of trivially exploitable.
So we'll see how well that works out.
So what we're trying to do here is
take this big conversational AI model that
takes in some text and outputs a response,
and train it with reinforcement learning.
And this is actually a really challenging problem,
because normally in reinforcement learning,
the number of actions you can take,
like in the previous project, is on the scale of like 12 actions.
But in generating language, there are 20,000 most frequent
English vocabulary words that it can choose from for each step.
So this becomes very difficult.
And similarly, the state space is the space
of all possible conversations.
So that's also a little bit more complicated
than the little grid world we saw in the last project.
So that's difficult.
And then trying to learn from human interaction
actually presents its own problems.
So here, we're treating the human as the reinforcement learning
environment that we're trying to learn from.
And when you're learning from human data, trust me,
it's very painful and expensive to try to collect that data,
especially if you're in academia.
And so you basically want to salvage any little piece of data
you got from a human, no matter how long ago you got it,
no matter how different the model was or the data set,
you need to be able to use it well.
And so this necessitates really good off-policy reinforcement
learning.
And then the second problem is that you
need to be able to test your model very carefully
before redeploying to humans.
You basically can't put some model on the internet
and have it learning from its interactions with humans
in a continuous way, or you might end up
with some inappropriate behavior.
So you basically have this atypical broken interaction
loop, where your RL algorithm, in contrast with normal RL,
is not able to explore online in the environment
to refine its estimates of how good certain actions are.
And so this is called batch reinforcement learning,
learning from a static batch of data
without the ability to explore.
And it's a very difficult problem that has only
been explored a little bit.
But I'll explain why it's tough.
So off-policy learning is really hard for deep reinforcement
learning.
So most deep reinforcement learning algorithms
are going to fail to learn if the data that they're trained on
is not heavily correlated with the current policy.
And secondly, even supposedly off-policy algorithms
like Q-learning are going to fail to learn when they can't
explore in the environment.
So why is that?
Well, let's look at the Q-learning optimization function.
So what it's trying to do is it's
trying to bootstrap an estimate of the total reward it
can expect from taking an action in a certain state
by looking at the reward it observed
and then estimating the maximum expected future reward it
can get in the next state.
And if there's some part of the state space for which you
don't have good data, so you have really noisy estimates,
you don't have a lot of data, so these
are kind of garbage estimates, what do you think
is going to happen if you take the max
over those noisy estimates?
It will go to a region where, because within noise,
they're very high outliers.
Yes, exactly.
So the noise leads to a high outlier.
It's not the real estimate.
Here's the real estimate.
Noise is making it higher.
You take the max, it's going to persistently overestimate
the value in that region of the state space.
So it's exactly what you said.
So queue learning is optimistic in the face of uncertainty.
If it hasn't seen a lot about this region,
it thinks it's probably good.
And that's actually amazing if you're doing normal RL
and you're able to explore.
Because you have some bad estimates
for some part of the state space, you're optimistic,
you go there, you explore, you refine your estimates.
So nice for normal queue learning
has this built-in exploration bias.
But it's terrible if you're learning
from a static batch of data, because you
don't have data for this part of the state space
and it wants to go over there and you have no data there
to learn a good policy.
So it's just going to drive itself out of the batch data for
which you're able to learn.
And so we'll see that happening if we just
do this batch queue learning kind of naively.
So in the next few slides, I'll describe a couple of baselines
to do batch queue learning and then we'll see how well they do.
So firstly, we have to absolutely have all of our baselines
at least pre-trained on some conversation data.
So we want to take a pre-trained language model.
And the reason we have to do that is our action space
is 20,000 dimensional.
We're never going to learn how to talk from scratch
just by talking to humans or humans aren't patient enough
to teach us that.
So we take a bunch of known conversational dialogues,
we pre-train a model, and then we fine-tune with reinforcement
learning.
The second thing we do is try to alleviate that overestimation
bias with that max operator.
So something that's been proposed in the literature
is to do clipped double queue learning.
So basically, you have two different independent networks
that you use to estimate the total expected future reward,
take the max with both of them, and then you take the min
of their two estimates.
But I think this is a bit clumsy, and it's also really
expensive.
Having two totally different networks is difficult.
And how do you make them independent
if you're initializing with a pre-trained network?
So what we choose to do is to use dropout
on our target queue network.
So there is this finding in the literature
that if you run multiple forward passes of a neural network
trained with dropout, you actually
get a Bayesian estimate of the uncertainty
and the value the network is estimating.
So we just take the lower bound of that
to try to alleviate this overestimation.
So those are our baselines that we came up with.
We also adapted a baseline from the literature that
was called batch constrained queue learning.
What it does is it learns a generative model of the batch,
p of a given s given the batch data,
and then the RL algorithm is only allowed to act
by sampling from that model.
And the intuition there is it's not
allowed to stray too far from the batch data.
So those are our baselines.
Can you guess how well they did?
I guess I wouldn't be asking if it was that great, right?
So here's an example of a conversation
with this batch queue learning.
And you see that it's not only, as we predicted,
exploiting the trivially exploitable rewards.
It's asking a question every single time.
But it's also spamming the maximum number of tokens
that it's allowed to spam, because that just gets more reward.
I just said more things.
Good.
But the other issue is that it's saying pretty unrealistic
stuff.
So where did you say to me?
Is not a real sentence that should
have been seen in the training data.
So it's gone off into space, basically, as you were saying.
So we see that it's diverged away
from realistic natural language.
So how can we fix this?
Well, our idea is we want to bake in an understanding
of what is realistic language directly into the RL policy.
So what we're trying to do here is not only
solve, optimize for the normal, total expected reward
over the trajectory, as we would do in RL.
But we want to minimize the divergence of our RL policy
from our pre-trained language model.
So that language model is saying,
what's realistic looking language?
Hopefully it knows that where did you say to me
is not a high probability sentence.
So we want to make sure that our RL model can't stray too
far from that.
And if we look at the formula for the KL divergence,
we can see that this amounts to just adding two simple terms
to our Q action value function.
So we add the log probability under the prior.
So this is our language model.
It says, how probable is a given word given
the sentence context?
So it says, what's realistic language?
Don't get too far away from that.
We also subtract the probability under the RL policy itself.
And that actually just amounts to doing
entropy regularization.
So that means your policy needs to be as stochastic as
possible while still optimizing for the rewards.
Well, why do we care about that?
In dialogue, we don't want to collapse down
to a single conversation.
We want to have a lot of variety in the things
that we're saying.
And there's different ways to solve an entropy regularized
Q function.
So one is to replace that hard max with a log sum x.
So that actually also helps alleviate the overestimation.
So these are the models we're testing, this KL control Q
version, KL control psi.
And then we also test a version of the psi
with making a better language model
by averaging several language models together.
So let's check out how well this worked.
So we conducted a human evaluation
where we compared our baseline batch Q learning models
to these KL control models on human ratings of quality,
et cetera, as well as the votes that the models received
in the interface for their various utterances,
and the total reward they elicited from humans.
So how much did the human express positive sentiment?
How long did they continue speaking this type of reward?
And we see that while these techniques we're proposing
do help, they improve over the baseline.
We have this Monte Carlo averaging helps.
The biggest difference comes from the KL control methods.
So they actually significantly outperform the baseline.
And we see big differences in terms
of not only the ratings, but the votes, and the reward
they elicit from humans.
We can see why this is happening.
So this is the KL divergence of our RL models
from our original language prior.
So remember, we initialize the RL models
with the weights of the language prior.
So they start the same.
But as you continue training, we see
that those batch Q models diverge away from the prior.
So they diverge from realistic language,
and they're trivially exploiting those rewards.
And we think this leads to very unrealistic estimates
for our baseline.
So when you sample from that prior,
and then you try to assess the values in Q learning,
it becomes less and less realistic,
and it doesn't perform well.
But in contrast, the KL control models
are able to alleviate that effect
and stay close to realistic looking language.
So here's how it actually turned out.
So you can actually go chat with these bots yourself.
They're still deployed live.
And we can see that the KL control model is
using a bit more realistic language.
You ask it, hey, what's up?
It says, I'm not sure, but I'm glad to see you.
We did notice that while it is realistic,
optimizing for this positive sentiment
did lead it to be extremely cheerful,
polite, and supportive.
So I was saying things like, thank you, you're the best.
So we actually developed some post hoc metrics
to assess exactly how cheerful and polite these bots are
by sort of counting things they say,
like thank you, I'm sorry, et cetera.
Maybe we're gonna call this the Canadian-ness metric,
I'm not sure.
But what you're looking at here
is all of the techniques that we trained
and the Z-scored reward that they obtained
not only from the rewards they were trained on,
shown in red, but these post hoc metrics shown in green.
And what you can see is that as we predicted,
these batch queue models trivially exploit
the easy rewards, like asking questions.
But the KL control models have shifted
their distribution to this polite, supportive,
and cheerful language.
And that actually allows them to elicit
this higher human reward.
So they have to do a more complicated thing, basically.
And then we tested how well the different reward functions
worked when we trained models
with each reward function independently.
And we see that actually the sentiment-based reward
led to the highest quality conversation.
So this affect is important in a good conversation,
which is a really nice finding
when you're in the affect of computing group.
So we also saw that these models trained
on implicit reward do score higher
than training on these manual upvote downvotes.
So that helps validate our hypothesis
that getting humans to manually label their preferences
doesn't scale well.
So in conclusion, this is the first work
to propose using KL control from a pre-trained prior
as a way to handle this off-policy,
no exploration problem, batch RL.
We proposed several enhancements
that all work to varying degrees.
We found sentiment to be the most valuable
implicit reward.
And we see that implicit signals are more scalable
than explicit button presses in this case.
All right, any questions about this work?
Yes?
Do you have a comparison to an equal number
of sentiments and votes?
Because it's obviously more scalable,
but what's the actual signal by current sentiment
or implicit vote?
So arguably you get the sentiment no matter what.
And then the explicit vote you don't get
unless the human bothers to send it.
So we just say that we have the same number of utterances,
but people can voluntarily press the button.
We encourage them to do that in the instructions.
But yeah, I don't have the comparison of like,
if we restrict only those utterances
that were upvoted, downvoted to compare with the sentiment.
Maybe in that case, you would get better performance
from manual upvotes.
Part of the problem is that we just think
people won't bother to do it, yeah.
Yes?
I can imagine that some people like a happier Alexa
and some people like a more, I don't know,
sensitive to Alexa, et cetera, et cetera.
Can you train it by giving sort of the input
an extra parameter which you learn,
which you teach the agent to map to,
for example, sentiment?
So that if you buy a new Alexa,
I said happy minus two, five, six, seven.
You can't be very happy if you want it automatically.
Doesn't have to relearn the whole policies
of math and learning or so,
but that automatically, is that possible?
Yes.
That you automatically get it to be happier.
That's a great idea.
And I think there is definitely something really interesting
and deep there in how to respond appropriately,
because actually just expressing positive sentiment
isn't even good for the same person
but across different contexts.
So knowing how to respond emotionally at different times
is a really challenging problem
that we've only just started to dig into or think about.
But also in terms of like, each user having
different preferences about how chipper
they need their Alexa to be,
I think that's definitely a good point.
And having some personalization
where you understand the user and adapt better to them
could be really cool.
Actually, later in this talk,
I will talk about methods for personalization
that I think could lead nicely into that.
So thanks for your question.
All right, yes?
Yeah, I have a question about the explicit versus explicit.
I'm wondering your thoughts on if there's like a spectrum
of when explicit can be maybe more desirable,
more human's more willing to do it.
I'm just thinking like, I treat my dog,
and I'm like willing to say like, yes, you did a good job.
Yes, you did a good job.
Yes, you did a good job.
And I have a long-term relationship with this animal.
Yeah.
But similarly, there's like an AI
that I was like forming a long-term relationship with.
Maybe I'd be more incentive by such a human
to like give an explicit kind of responses.
Yeah, and actually, I'm really glad you asked that
because the next project,
we also look at implicit signals from facial expressions.
And we think a lot about the interface
and how the interface could be designed
to elicit more feedback, basically.
And I think there's something really rich there.
But yeah, I think that's an interesting direction
for future research.
Yeah.
Okay.
Thanks.
I'll go to the next project there.
So speaking of that project,
this was the same motivation,
but here we want to actually learn from people's
implicit signals from their face.
So the way we did that,
this is a quick description of this project,
is we thought about which deep learning model
is likely to generate samples
that a person would possibly make a face at.
And we landed on sketch RNN,
which draws these cute doodles
based on a sequence of pen strokes.
And what we did was we showed those doodles to people
and we collected their facial expression response.
And then we trained this latent constraints model.
So if you get the lingo,
this is a GAN,
basically it's a GAN on top of a VAE.
So let me say that a little more clearly.
This model already has a super cool representation
of sketches that is learned unsupervised
by training on a bunch of sketches.
What we want to do is we want to understand
which parts of that representation space,
these are sort of all the sketches you could possibly draw.
These are all the sketches that are kind of realistic.
We want to learn which part of the realistic sketches
actually make people smile and make them frown less.
So learn which part of the representation space
to convert to.
And then we apply this technique
to helping this model learn to sketch better.
So this is how the model used to sketch cats
before our technique.
You can see that there are some pretty good cats,
but some of them are a little questionable.
So as we train with only a small amount of facial feedback,
this is a very sample efficient model.
We actually only had like 70 samples
of people smiling or not at this cat.
We see that it learns to draw
a significantly different type of cat.
So we've got these little cute mouths,
we've got whiskers, smiles.
So whether drawing significantly cuter cats
is a marvelous scientific achievement or not, I don't know.
But if we look at the rhinoceros model,
we see something interesting.
So it turns out people, which is what trained this model,
and this model are very bad at drawing rhinoceroses.
So I don't know what this is,
but it's not a rhinoceros, right?
And what we see is that again,
training with only around 70 samples of facial feedback,
the model has learned to draw
actually pretty realistic looking rhinoceroses.
So we actually think this is the first work
to show that you can improve the quality
of a generative model using facial expression feedback.
And we did test this with humans,
and yes, they do significantly prefer our models sketches.
So that was a foray into that.
But the issue with all of these papers
on learning from these implicit cues
like facial expressions and sentiment,
is that if you wanna be able to do that,
well, you first have to be able to detect those signals.
So facial expression detection
and sentiment detection are kind of mature areas
where we know the accuracy is good.
But what are other ways
we can actually detect these affective signals?
So this is a focus of a lot of my early PhD.
One of the papers I looked at was,
can you predict if two people in a conversation
are actually bonding with each other,
whether they're having chemistry and rapport?
And we do that using one minute slices
of their facial expression and body language data,
you actually get pretty high accuracy.
But what I really wanna talk about
was a huge focus of my early PhD
that I worked in close collaboration
with a number of the people here,
for example, Sarah Taylor, Ehi, Akane, and of course, Raz.
So I've been going around giving this job talk
and saying that this is Raz's dream,
but I don't know, hopefully she's okay with me saying that.
It also would be my dream.
Yeah.
So what we were aiming for here,
what I think would be really cool,
is to be able to take signals gathered
non-obtrusively from a person,
maybe from their smartphone,
maybe from a wrist-worn sensor,
like a Empatica sensor, send that to our server,
do some super cool machine learning on it,
and give people a personalized prediction
of what we think their, people are laughing,
that's a good start,
what we think their mood is gonna be like tomorrow.
And we actually think this could be really cool,
because if you get a forecast
that there's a 92% chance
you're gonna be stressed out tomorrow,
just like if you know there's a very high chance
that it's gonna rain, you can take an umbrella,
maybe you can take some steps to help alleviate this.
So you can go to that extra yoga class,
get some extra sleep,
whatever it is you need to do to improve your stress.
And then the second thing that we thought
would be really important,
especially for populations that suffer from a lot of stress
and are at risk of converting to depression,
would be tracking their long-term history
and potentially tracking whether they might transition
to a depressed state.
Because actually, moods have been modeled
as different stable states of a dynamical system.
So healthy functioning is a stable state.
Even if something bad happens,
it perturbs you a little bit,
you usually bounce back.
But if you're in a very high-stress environment
or you're perturbed continually,
you could actually transition
to a different state to depression.
And then again, you're kind of stuck.
Even if you're perturbed, you bounce back to depression.
Yes.
Has any working time showing the effects
of telling people to progress like this?
We are starting to investigate that.
There are some very complicated UX questions around,
is there, if I tell you that you're going to be stressed,
is this self-fulfilling hypothesis?
Does it actually help you?
How can these interventions be delivered
in a meaningful way?
So that's a really good question.
But the main focus of my contribution
to this work was on this piece.
Like how do we actually even get the prediction
in the first place?
So yeah, and so if we had those predictions also,
even if we don't deliver this,
it would be nice to be able to sense
if someone might be on the wrong trajectory.
So that could be really important.
Okay, so how do we do this?
Well, Kane and Sarah and a little bit me
spent many years collecting very high-dimensional data
from students.
So we collected their location, their physiology data.
We extracted their steps, their temperature.
We made them tell us everything they were doing.
Did they go to class?
Did they drink alcohol?
Did they drink coffee?
When did they do it?
We looked at everything they're doing with their phone.
Well, not everything, but who they're texting,
who they're calling.
So just a massive amount of very rich data.
And we actually published just a ton of papers
on this problem.
Like how do we use this rich data
to predict people's well-being?
We built whole online tools that allow people
to upload their physiological data
and to detect the artifacts in the data,
detect the peaks.
We made whole papers on dealing with noise in the data.
So for example, what do you do
if a whole set of features goes missing at once?
Like someone forgot to wear their sensor
or they don't want to turn on the location tracking?
How do you cope with that missing data
and still make a reasonable prediction?
So we did all this work, but what we found
is that the accuracy in predicting these outcomes,
like for example, in predicting happiness,
was still disappointingly low.
So this is a little too close to the horoscope side
of the spectrum or mood ring side of the spectrum
than the deployable system that could help
detect depression in the real world.
So why is this not accurate enough?
And I swear it's not just that we are terrible
at research, if you look at the literature,
even in detecting happiness on the same day,
the accuracy is pretty low.
And we're trying to of course predict future happiness.
So why is that so low?
Well, the reason that we came up with,
we thought a lot about this,
is that these models are not accounting
for individual differences.
So we're training this giant,
shared machine learning classifier
that's trying to say when everyone will be stressed.
But actually people are very different
in the way they get stressed.
So for some people,
they might be very happy at Friday night,
a home alone coding.
And if you put them in a loud crowded party,
they're gonna be stressed.
And of course for some people, it's the exact opposite.
So these models currently,
we're not able to account for this differences at all.
So what we tried to do was to use multi-task learning
to get this personalization.
So what we're trying to do,
is predicting the mood of each person as a separate task.
And that allows us to customize the model to each person.
But we can't train just 50 independent models
for each person because we don't have enough data.
So what we wanna be able to do is still benefit
from the statistical strength of the data of other people,
but to the degree that they're related to each individual.
So the first approach we took was this neural network approach.
We had some shared feature extraction layers
that we hoped would take this high dimensional data
about every time you sent a text
and compress it into something a little more reasonable
that we could build on top of.
And then individuals or clusters of individuals
had their own layers that mapped
that representation to how it would affect that person.
The second model we looked at,
and one of the few non-deep learning models
in this presentation was a hierarchical Bayesian model
based on a Dirichlet process prior.
So what's happening here is we're drawing the weights
of each person's classifier from the shared DP prior.
And based on the Chinese restaurant formulation
of the DP prior,
that's gonna induce an implicit clustering of people
based on their decision boundary.
So you're basically gonna get placed in the same cluster
with someone who reacts in the same way
in terms of their stress as you do.
And it's a non-parametric soft clustering
so we don't decide the number of clusters in advance.
And people can actually belong slightly
to several different clusters,
giving the model more expressivity.
And then the third model we looked at
was a multi-task, multi-kernel learning model.
So what we're saying here
is that we have all these different types of features,
like physiology features and location features.
And we're gonna learn a kernel
over each of these types of features.
And then for each individual,
we're gonna customize the weights on those kernels.
So how much is your physiology predictive of your mood?
And we regularize those weights globally
and we perform an EM so that you gain information
based on how other people are weighting those weights.
So how well did this work?
Well, we compare each of those models
to their single-task learning
or traditional machine learning equivalent.
So SVM, logistic regression, and neural networks.
And we see that in single-task learning,
we still get pretty low accuracy.
So very typical of the literature, not very performant.
If we apply these techniques,
where we treat the different tasks
as just the different related moods,
we don't really see convincing performance gains.
So it's not that the techniques themselves
are solving this problem,
but what we do see is that if we use those techniques
to customize the model to each individual to personalize,
then we see very large performance gains,
sometimes up to 20%.
So at the time we publish this paper,
this was actually state-of-the-art
in predicting these outcomes.
So we were very happy about that.
But what's more interesting is to look
into what the model actually learned.
So these are the weights that the model is placing
on the different features for each individual.
And what we see is that for the stress model,
these are all like stressed out undergrads
from a certain institution.
So we see that they're pretty much
regular to be quite similar,
but for mood, it's very individualistic.
So what we're seeing is that for some people,
the weather is incredibly important to their mood.
But for others, it's not really at all.
So that's showing that we need this model
to account for these individual differences.
Similarly, this is one of my favorite findings.
We actually looked into what this hierarchical
Bayesian model learned in terms of the clusters.
And we did a post-hoc analysis of the individuals
belonging to each cluster and looked at whether
they differed from the group significantly in any way.
And we see that cluster zero doesn't differ
significantly from the group.
So it's kind of your standard person.
Cluster one is a significantly higher GPA
and significantly higher conscientiousness.
So they're more organized.
They're more like on time, punctual.
They have their stuff together.
And interestingly, that cluster actually really
enjoys routine.
If their day is more likely under their normal day model,
they'll feel less stressed.
So we think of this as like our studious cluster, basically.
This cluster, however, in contrast,
is very highly extroverted and has lower physical health.
So I don't know what extroverted undergrads are doing
that's lowering their physical health.
You can make some guesses.
But what we see is that the extroverts,
if they spend a ton of time on campus
and they text a lot of people, especially late at night,
then they actually feel less stressed the next day.
That's a good day for them.
But in contrast, the studious people,
if they do the exact same thing.
So they're on campus and they're texting a lot of people.
They are much more stressed the next day.
So we see that if we had lumped these people together
into the same model, it wouldn't have been
able to account for this difference at all.
And if we tried to make predictions using these features,
it would have washed out.
So this personalization is really important.
So we extended this to actually predicting
people's not only their binary classification of whether they
were stressed or not, but actually the regressing
on their stress level.
And we did that with a Gaussian process approach
using domain adaptations.
So we trained your standard Gaussian process
on everyone else's data.
And then we used this domain adaptation approach
to adjust the mean and variance of that Gaussian process
towards the individual's data.
And then for neural networks, all we did
was just change the loss function.
So deep learning is nice and easy.
So then we see that these personalized models in terms
of regression perform better for the large majority
of the participants.
And in terms of the model fit to the data,
we see very vast performance improvements.
So this interclass correlation not only
accounts for the mean absolute error
in predicting the outcome, but also how well it
fits the trend of the data.
So we see very large performance improvements there.
So in conclusion, I presented work
on improving well-being detection and working with noisy data,
improving multi-agent reinforcement learning
through social learning from other agents,
improving conversational AI with sentiment detection,
and improving a generative model with facial feedback.
Oh, wait, what does this say?
Thesis proposal.
Oh, right.
That's because that's actually what I proposed to do.
So yay, I managed to get those projects done.
And I really want to take an idea from my friend Juliana.
She suggested to be a little more honest,
though, about how hard these things can sometimes be.
So my thesis proposal was very nice and neat.
I was going to finish these projects in this order.
And I was going to leave all of 2019 for writing my thesis.
So things slipped a little and left only a couple months
to actually write the thesis.
And then I decided to do all my job interviews
in those couple of months.
So I would not recommend doing that.
It's been quite a wild ride for the last few months.
But we made it, so we're here.
So in conclusion, the research direction
that I really want to emphasize is
that I think social learning is an incredibly powerful
mechanism for human learning.
So it allows us to rapidly adapt to new situations.
It allows us to transmit knowledge amongst ourselves
and build on that knowledge.
And so I think not only can multi-agent systems
benefit from learning from other agents,
but that any AI system can benefit from learning from humans.
Because humans are smart, they know what to do,
and they're the best arbiter of their own preferences.
But in order to learn those from human social cues,
you need to be able to detect them, which often
necessitates accurate detection of affective signals.
So in future work, what I'd like to be able to do
is integrate these two directions
to training multi-agent policies that
are able to coordinate quickly with other agents,
made with new agents, and then generalize that
to coordinating quickly with humans
in these cooperative environments.
So I think that could be cool.
I also didn't get to fit all of the papers I published.
So these are extra papers that did make it
into this presentation.
If you're curious, please let me know.
And then, of course, I want to deeply thank everyone who's
helped me with this thesis.
So especially calling out my advisor,
Roz, who's been incredibly supportive and an amazing
advisor, and really giving me the freedom to pursue
the research direction that I'm passionate about,
which has been so important for me having a happy PhD
and enjoying myself.
And also, Doug Eck has supported me for many years.
I met him in 2016.
And when I was first starting to learn about deep learning,
he's really helped my career.
Nando has been incredibly insightful.
I think he actually asked the question
that originally spurred the causal influence work.
And that ended up winning an honorable mention
at ICML for Best Paper.
So we were pretty stoked about that.
So thank you so much.
And Joelle has been incredibly patient and amazing
and actually sat down with me and giving me
deep technical feedback on hyperparameter tuning and stuff.
That's amazing of you.
So thank you so much.
And of course, I want to recognize all of my collaborators.
So I've had a series of amazing collaborations,
have the privilege to work with incredibly smart
and amazing people.
And I couldn't have done it without you.
So I really want to call out Sarah Taylor, Asma Khan,
Dehar Yoon, Judy Chen, Vincent Chen, Akane Ehisana
Masakare Kwanesano, Shane Gu at Google Brain,
Chalara Gulchahar.
I'm not sure I'm saying that, right?
But at DeepMind, Craig Ferguson has been amazing.
Christy, you've been amazing.
So I really want to say thank you to you.
And of course, my parents would kill me
if I didn't call out how much they have helped me,
but they really have.
My parents are incredibly supportive.
Anytime something goes wrong in my life, they've been amazing.
And I just want to, you know, I didn't know how to write
down their names in the right order.
So I just want to call it that there was an equal contribution
of both parents.
My brother has been amazing.
My grandma has always supported me.
Mavis, Arlene, both grandparents.
I've got Tiffany and my aunts, Linda and Bill and Auntie Pam,
Auntie Lana, so many more.
So I really want to say thank you to you.
And then, of course, my friends.
So I saw you sitting through a lot of SAP acknowledges,
but I have to do it.
You guys have met so much to me, meeting friends here
that we've been so close over the last five years.
It's starting to feel like you're just my family.
Like Christian and Juliana picked me up at the airport,
which I think is like the real bonding signal.
I really want to call out thanks to Max.
He's been amazing.
Juliana, she's been my friend for so many years,
such a sounding board for ideas.
Christian and Juliana, so close.
Andrew has, of course, been incredibly supportive
for many years.
And Bianca, Kelsey Allen is here.
Thank you, Kelsey, Peter Beshai.
Couldn't miss Eddie, Martin, Carolyn, Sam, Gavin.
And of course, you guys won't get this,
but I have to do it, because these friends are actually
really meaningful to me.
I've done a lot of internships and spent a lot of time
away from Boston and become really close to some people
that couldn't be here today.
So I do just want to call out Ben, Laurent, Kyle, Nuru,
Joanna, Tim, Anna, Yannick, Mattias, DJ.
So hopefully, if I send them the recording,
they'll know that I acknowledge them too.
And now, at this point, I'm just making you look at a bunch
of sappy pictures throughout my PhD.
So I'm going to wrap up.
So thank you guys so much for coming.
I really appreciate it.
I think there might have been a little social influence.
OK.
Time for Q&A live with our public audience here,
and committee members are fine to ask things too.
I should probably, since this is being recorded,
pass around the mic also.
I can also repeat the question.
Yeah.
Amazing presentation.
Thank you so much.
Thank you.
Really a treat.
It strikes me that probably the most commercially valuable
application is really just manipulating people
on a massive scale in surveillance capitalism
and all that.
What can we all do to make sure that everything
that you're doing ends up helping people as best
as possible given that?
That's a great question.
So actually, I really wanted to stay away
from follow-ups of the social influence work
that we're influencing people.
I think that gets down somewhere dangerous really fast.
But you should also think of the social influence work
as a way to compute the degree of influence between two agents.
So you could use it as a metric of saying how much someone was
influenced by a given action.
Potentially, you could use that to detect
whether people are being influenced and maybe intervene.
So I think that could be interesting.
And then in terms of the affective components,
you can think about people's affective safety.
The reason I talked about AI safety
is your affective signals are changing.
So if I make a joke to you and you laugh and I make the joke
again, you're not going to laugh the second time.
So there's this non-stationary reward signal
that measures whether you're actually currently
enjoying what's going on.
And so if you were able to actually use, for example,
reinforcement learning, which accounts
for total expected long-term reward,
and you were really able to use that to account
for your long-term enjoyment and happiness and fulfillment
and flourishing well-being, which is something
that our group has thought a lot about measuring,
if you could really optimize for long-term well-being
using these functions.
And I think that would be a more safe AI model.
But we still need people in power to believe
in that reward function.
For people felt that they don't always unfortunately.
By the way, people want to follow up on that question,
which is one I'm very interested in.
I'm happy to continue to explore that one with you guys.
Other questions?
Joelle?
Or someone on the phone, too.
Or Doug Granando.
He might be muted.
I think I see that there.
I've just been muted.
I'm sure I do have questions.
So first, that was an amazing talk and an amazing dissertation.
And since there are some other PhD students in the room there,
so I want to point out that Natasha set the bar at 14
research papers for a good dissertation.
So the rest of you have a lot of work to do.
Joking aside, so my main question, the one that I've really
come away with asking is, what is social?
And can we meaningfully talk about social in the context of agents only?
Like there's a nice bridge to build between,
I'm thinking of the dissertation document, not the talk,
but between chapters, let's see, four and five, I think, three and four?
Oh, no, chapters two and three.
When I think about the work that we did together and the media
generation work, the music generation work,
when I think about two music agents jamming together,
it's not clear to me that that has anything to say about a human
and an agent jamming together.
And I'm wondering if you think differently,
if you think there's something we can take from the literature
of multi-agent work and apply it to humans.
And then more generally, is that interaction among agents,
even something that we would call social,
or is social embedded in the world and embodied in the world?
That's my big question.
That's a great question.
So I think there's definitely something there in the sense of like,
can we even say that these agents have agency?
We don't, are we there yet?
Probably not.
But what those problems share and what the jamming problem actually shares
is trying to infer another agent's intentions in order to coordinate with it.
So if you're jamming with someone, you have to kind of understand
where they're going, where they're taking this,
how you can work with them in order to make that sound good.
And similarly with the multi-agent problems,
they do share that in the sense of like,
if I want to coordinate with another agent to achieve some task,
I have to infer what they're doing and try to achieve the other part of the task.
And that also works with humans.
So if I want to make a human happy,
I want to please a human with whatever Alex is doing,
I need to infer what they want, infer their intention,
what are they trying to get out of the interaction with the agent, etc.
So I think this idea of like, modeling other agents
and understanding what they're intending and how to coordinate with them
is shared across.
Agreed. Okay.
That's my main question.
I'll pass to Tynando, I guess.
Thanks. Natasha, great work.
I think this is a brilliant thesis,
and I hope just the beginning of a very bright career.
I have a few questions for you and sum up purposely hard.
You mentioned it's a PhD, so you get to be asked these.
You mentioned that, so one of the very nice things about your work
is by having the influence or reward,
the agents were able to communicate better.
So you were able to train agents that would communicate, excuse me,
in a completely decentralized manner, which hadn't been done before.
Like, very good researchers failed at doing that, including myself.
And then, you mentioned that one of the things you would like to look at
is compositional communication.
Why do you think that's important?
And the other thing is, if you think of it as like a stream of, say,
discrete symbols that can combinatorially be combined,
why do we need that and why can't we just go with a single continuous
factor?
Yeah, that's really interesting.
So I think in some sense you kind of answered your own question,
like in saying that it's combinatorial.
So with this small set of discrete symbols,
which we know how to do really well with current reinforcement
learning techniques, learn a discrete policy,
we get this combinatorial explosion of things we could communicate
or things we could talk about.
So I think you always give this example of like,
there's a village of 300 people.
I don't have to talk about each one individually.
I can group that, I can make categorizations with these symbols
and you can get the relationships among them.
And why don't we want to go to continuous vectors?
I actually think we don't want to go to continuous vectors
and we no longer want to use ungrounded discrete symbols either.
Because I think while studying like the emergence of language
is really interesting, what would be interesting to pursue
going forward is actually to try to have,
to go back to this idea of having agents communicate
with human natural language.
Because I think for the problems that I want to look at,
it's not only can agents coordinate with each other
and communicate with each other,
but can they generalize that to actually coordinating with humans.
So taking the same policy that can flexibly adapt
to multiple different types of agents.
And then you need this like grounded language in natural language
so the agents can use it, but it can also be used with humans
so they understand it.
So that's why I think we should go that direction.
Rather than use continuous vectors.
All right, so you would basically capitalize on the fact
that there's creatures on this planet that already have language.
Yes.
So let's use the language in the channel.
To ensure that the agents somehow internally develop concepts.
That would be great.
That would be amazing.
Yeah.
And so that brings me to your other work,
which is on learning from trajectories of observations
by imitating.
For me, one of the best examples of imitation is,
I forget his name now,
the high jumper that in the 70s in the Olympics
and learned to jump backward as opposed to jumping forward.
So up to that point, people had come up with all sorts of crazy strategies.
That's just basically RL.
And then what happened at that transition point
is so it goes that he was an engineering student
who went and did a lot of calculations about the center of mass
and so on and figure out that this is what he had to do.
So if stories are true, then he used abstraction
to be able to solve the problem.
And since then, we've been just following straight imitations.
Stuff that is very similar to your PCQ
or regularized reward maximization.
So then this question of how to get abstract concepts
seems to be very important to how we do bad imitation.
But do you think that having then just the language channel,
that will be enough?
Would there be a way of you testing that the agent having acquired
your RL agent, which started with this language
by exploiting the language channel,
was able to do some improvements
over just the behavioral cloning policy
that would show that it was exploring in this higher abstract space?
That's a great question.
And I think the language model, as you said,
might give you some clue towards concepts.
And I think it actually might give you a nice path to generalization.
So if I give you a command that you haven't exactly seen before
but is close in embedding space in the language model,
then I could infer what you're talking about in the environment.
I think that's cool.
But I don't know if it would get us all the way there to like,
if I have a trajectory of low-level actions
abstracting out high-level goals,
like what was this person intending to do?
And of course, condition on a goal,
the low-level actions could be quite a bit different.
And so different people might take the same goal
and actually enact it very differently.
So I think there's kind of an unsolved problem there
in terms of like segmenting a trajectory
into these high-level goals.
But I think it would be an interesting question to pursue
because you can kind of think of this high-level goal
that the person is pursuing as their intention,
like what are they trying to do?
Like if I observe, it goes with the multi-agent learning piece, right?
So like if we're trying to coordinate to make some recipe,
one of us has to get the tomato,
one of us has to get the lettuce,
if I observe your low-level actions,
I need to infer that you're getting the lettuce
so I can get the tomato,
that's sort of like inferring this goal.
So I think they kind of overlap.
Thank you.
Thanks.
And I have two more questions.
There's sort of more like nitpicking.
So your reward function for that project,
it involves a lot of small numbers,
I mean numbers with like three digits and so on.
Okay.
I forget the page on which it is, but.
Yeah.
Yeah, 0.157 times the question plus 0.138 times the semantic coherence
plus 0.10 so on.
So it's just very,
is there any hope that instead of like building a reward like this,
that you could use something like say,
generative adversarial imitation learning to learn a better reward?
I think that's a really promising direction.
Yeah. So the reason those numbers are so small is by the way,
we're just normalizing so everything sums to one.
But yeah, I think that's a really good point.
I don't think those rewards are the final answer.
So we saw that our agents were moving towards,
yeah, more polite and cheerful language,
but we don't know that that's actually a better conversation.
There's a lot more richness to what makes a good conversation
than those signals.
So rather than sort of hand crafting that,
you could also think about some type of like
inverse reinforcement learning,
observing people talking to each other
that do have a good conversation.
Of course, I have prior work on inferring,
what is a good conversation?
So then look at the good conversation and say,
what is it that people are enjoying about the conversation,
how do those conversations look,
and try to get the reward function that way.
Yeah. But I think there's definitely a lot of
future work to be explored there for sure.
And the last question has to do with,
so there is this beautiful cycle for doing an RL from demonstrations,
which is, you can learn the policy completely from data,
as you have shown here.
You can learn, we could potentially learn the reward from data.
The other thing we could learn from data, perhaps,
is how good, or at least from very few interactions
with environment, is how good is the policy before we deploy it.
Yeah. So you mentioned,
in some cases, it will be catastrophic.
You deploy your dialogue agent on Twitter,
and it says a lot of obscene things,
and you're in big trouble, so you get fired for that.
Yeah. But in some cases,
it's inevitable that we will deploy a policy,
and there will be some adverse effect,
and we might have to re-update that policy.
Yeah. And so how could we learn from the experience
to design a better agent?
Yeah. So there's a whole rich field on off-policy,
policy evaluation that we were looking into a little bit,
because it's related to this paper.
So you can use things like important sampling to re-weight samples,
so they look more realistic under your current policy.
And I think that is very important.
But of course, the question is, if you're deploying to humans,
are you able to actually foresee this type of inappropriate behavior?
Is that sort of extrapolating off what you even intended?
Like, in some sense, you kind of have to model
that that's a region of state space to anticipate
that the policy is going there.
So I think, I mean, for me,
maybe this isn't a very machine-learning answer,
but you kind of need some kind of human in the loop
to assess whether this is actually safe and appropriate to release.
Yeah. So it's a complicated question, yeah.
The question, I guess, that I was trying to get at
is once you're released,
and you discover it doesn't quite do what you want,
how can you learn from that to update your policy?
Do you mean iterating on the reward function
or iterating on what it's trying to optimize?
Yeah, I think that's really important, right?
So an ongoing way to...
So maybe you're saying there's this iterative process
of inferring a better reward using gaol,
using inverse RL, retraining, redeploying,
and continuing in that way.
Mm-hmm.
Cool. That seems like a really good idea, yeah.
Thank you very much. No more questions.
Oh, it's Amanda here.
Hi. Really great presentation.
I just have a very general question,
which is specific to, I guess, your end goal or your future steps.
Yeah.
Very broad.
What do you think could be the next set of next steps
for tackling this problem of social intelligence?
And I'm in particular really curious about
what led you to reinforcement learning,
specifically mall-free reinforcement learning,
for handling a problem of multi-agent RL
or multi-agent settings.
Do you think...
What, in your mind,
are the limitations of using RL as a solution framework
for handling those types of settings?
I mean, already from the presentation as well as from the Q&A,
you sort of get this feeling of there's a problem of reward engineering,
there's a problem of defining what is a good reward
in order to get some qualitative good policy.
Or, you know, after you trained a policy,
you notice that they were using some sort of navigation policy
as a way of communicating other intentions, right?
There's a lot of really interesting conceptual frameworks
that we're building on top of what we train
in a model-free RL approach.
And I'm curious about,
given all of the work that you've done
putting into understanding social intelligence
and modeling it,
what are the limitations and what do you think
it should be the next step forward?
Should we stick with RL?
Or maybe it's another framework in the works of,
you know, taking us in the right direction,
if there is a right direction.
Sure, thank you, thank you.
Yeah, so I would say,
firstly, I'm not in love with, like, model-free RL.
I don't think we have to limit ourselves to that,
and actually what we saw in the influence work
is that having a model of other agents was critical to doing this well.
So, modeling the world seems like a nice way
to integrate extra information,
and you can use other techniques like unsupervised learning.
That's also in the dialogue project.
We first learn unsupervised, a decent model of conversation.
Well, decent is a...
We're stretching the term there, but, you know, we try.
And so I'm not wed to that,
but I do think RL is really promising,
and I'm really excited about it,
and basically for two reasons.
So the first is that RL is a super-broad paradigm
that basically describes the scenario
anytime an agent is interacting with the environment,
taking actions, and getting feedback.
So it's this idea of, like, not static learning,
but actually doing this interaction.
And so it's so broad that it seems like it's inevitably
going to describe whatever we want to do with AI,
whether it's a conversational AI,
whether it's a robot that's in the environment,
it's going to be interacting,
and you want it to be able to learn from that interaction.
And the second thing about this is that...
I don't know if you've seen this very well-replicated work
that shows that dopamine neurons in the human brain
are actually encoding a temporal difference signal.
Actually, Nando pointed me to this.
They're encoding a temporal difference signal
that's a fundamental component of RL.
And so I think that's a really promising clue
that we're kind of on the right track.
And yeah, just this optimization function
of estimating total expected future reward,
long-term payoff.
I do this action now, but I don't want to be greedy.
Like, how will this actually work out in the long run?
That just seems like a really promising framework
for something that our agents are going to want to know
about the world.
So that's why I like RL.
Thank you. Thanks.
Thanks also. It was a great presentation.
The question I have...
You mentioned about categorizations of different agents
and personalizations of them.
So I'm wondering if you can also frame that
in the problem of mobile-based
and how you can find the optimal policy
when I know what is the difference
between different agents or characteristics
of different agents?
How I can have optimal interactions with them?
Have you considered that or what you perceive?
Yeah, I think that would be super promising.
So if you had some model-based approach
where you basically could distill people
into some latent characteristics of people
and then use those latent characteristics,
for example, to condition your RL model
so it has a sense of who it's talking to
and how it can adapt to that person,
I think that's really promising.
And then you could even do something more interesting
where your RL model observes some feedback
that actually that person didn't like it,
so you miscategorize them.
So could it update your extraction of the latent factors?
That could be really cool.
You guys are giving me so many good ideas.
Thank you. Let me ask a few questions.
Yes.
And first start by saying it's been an absolutely pleasure
to see the progress of this research
and in particular seeing you engage more and more deeply
with some of the technical models
and technical questions arising.
Still not losing perspective
of the application side of things.
So from that point of view,
I think in maybe reaching back
to one of the previous questions,
some of your contributions
are around this concept of intrinsic motivation
and what that means in terms of reward function.
And you go through an interesting mechanism
and you're not the only one,
a lot of the literature does too,
to sort of take intrinsic motivation
and turn it into explicit reward.
In this case, by labeling different emotions
and essentially giving a valence
positive and negative to these.
And so I'm left a little bit
with this interrogation of should we be skeptical
that there is a useful notion
of intrinsic motivation, right?
Isn't it the case that we may be labeling
some of these things intrinsic,
but at the end of the day,
there's no learning without a loss function
and in RL, there's no reinforcement learning
without some notion of a reward
and the only way to get to that
is to have human input on that definition.
Yeah, that's a really good point.
And I think there's kind of a blurry line
where you could get into a situation
where you're actually doing quite a lot of reward engineering
and still calling it intrinsic motivation.
So maybe broadly, people do have an intrinsic motivation
to get along with other people or have empathy,
but at what point does it actually just get into
hand engineering something for a particular domain?
And so yes, that's a valid criticism.
Do I think there is actually a concept
of intrinsic motivation that is meaningful?
I would say yes.
I think the work in curiosity is really promising.
I do think humans have intrinsic motivations
like curiosity, like empathy that help drive us
to not only learn but coordinate with others.
I think the direction of self-supervised learning
is a promising one that does seem like
the agent has to have some intrinsic thing.
It's trying to optimize.
It's not necessarily an externally imposed goal,
but it's a way that it can learn.
It's a way that it benefits from learning
from interacting with the environment.
So do I think the concept is useful?
Yes.
Do I think every implementation of it
is deeply generalizable to many different environments?
Not necessarily, right?
Yeah.
And I'm going to ask another question
that deals a little bit with this notion of reward
but a little bit more with the work
that you've done on off-policy learning.
And so one thing that's interesting here
is the space of actions that you considered
for off-policy learning.
Am I right in understanding that it was actually
at the level of choosing the words?
Yes.
So this is very interesting because it's an action space
that is much higher dimensional than most of the action space
that are considered usually for off-policy learning.
Is there something we need to do differently
when we think of off-policy learning
in small action space versus large action space?
Because in your case, you sort of, you know,
you lined up there's these previous methods
and here's a few variants that you've explored
but you didn't necessarily lean into the complexity
that arises from having a very large action space
and what that means specifically for off-policy learning.
So what are your thoughts on that?
Yes.
So I think there is definitely something that you have to do
that's very different and one thing I mentioned
is that you have to have a reasonable pre-trained model.
Otherwise this overestimation issue
extrapolation area issue would just be catastrophic.
So you absolutely have to be bringing in some prior knowledge
to give you some way to start over which of the words you could emit
and so that is quite complicated.
And then the second thing I would say is that
it's not the only answer to doing this.
So we're currently looking into, as you saw it,
it's a hierarchical model, right?
So we're actually currently looking into controlling the model
at the upper level of the hierarchy,
like the sentence level of the hierarchy
because we think that could reduce some of the complexity
and allow the model to sort of choose conversational goals
that it's conditioning on.
So that's a future direction I'm thinking of, yeah.
Is there a notion at which that helps you get a better sense
of like the similarity between the spaces?
Because one of the things in off-policy learning
is right trying to relate the space explored
by your behavioral policy versus your target policy
and relate that in some representation.
Do you think that gets at the question
how to have a lower dimensional latent representation
between these things?
So I think the thing about doing this on the word level
is that it's going to be really hard to do credit assignment
and propagate back the correct choice
when you have so many words in a conversation
and each time you choose each word you have such a broad action space.
So doing credit assignment at the word level
is going to make less sense than doing credit assignment
at the sentence level because that's closer to getting at
like what is the conversational intention of what I'm trying to say.
So I think that makes sense.
And so that handles both the dimensionality of the action space
but also the sequence length
which is also probably longer in your cases
than some of the previous ones.
Yeah, yeah.
I'll ask one last question perhaps for the benefit
of the audience is more than yours.
Looking back at this thesis
if there's one piece or thing that you wish
you would have done really differently
what would it be if you had the benefit of like six months to...
Yeah, well there's many.
I guess like immediately what popped to mind
is that there's whole projects that I spent like a year on
that didn't make it into the thesis.
Like I wrote a whole journal paper on like a UX problem
on behavior change that we just never submitted
because people dropped out of the study
there was too much attrition
and then I wasn't very happy with the paper
it was never submitted anywhere.
So those are things I would have done differently
like the work that got dropped on the floor.
I guess one thing that connects a lot of the things
that I wish I'd done differently
is giving myself more time to do a project in great detail.
So I think we talked about how I actually think
the dialogue project is a little bit premature
like we put it out because I'm graduating
I have to wrap it up
but I would have loved to spend another four months on that
and really dig into it and improve some of the models.
So this has happened multiple times
where I haven't given myself enough time to finish a project
I was doing this whole Gaussian process thing
I didn't finish it
I was doing this whole curiosity thing
I had to go to an internship
I only had two months
and I had to sort of drop that project behind.
So basically giving myself more long term time
to really finesse a project
because I think it makes more sense
to put out fewer really impactful papers
than just churn out papers before they're ready.
Yeah, yeah.
Thank you.
Yeah.
Hope Juliana likes that answer.
Yeah.
Can I jump in with one more quick question?
Yeah.
I thought we were going to go around Robin
so I only asked one question and then passed off
and all the rest.
It's okay.
Yeah.
I actually don't have a lot
because there's been a lot of ground covered
but there's something that you just said
sort of triggered like, yeah.
So you just said you need a pre-trained model, right?
Yeah.
Right?
Like I had hoped that
like my hope is that we don't need pre-trained models, right?
Like there's something,
there's something sort of so,
the thing that I like the least about model-based reinforcement
learning right now is so much of this work is kind of like
doing one, like having one model
and then then dropping it in
and kind of fine tuning it on a problem
or like I have this sense that there has to be
an integrated way to think about reinforcement learning
and gradient descent learning
and that we're,
we're sort of halfway there with model-based RL
but we're not all the way there.
And it's not just waiting through the math,
there's sort of a conceptual leap that we need to make
that allows models to learn from non-differentiable reward
and from some differentiable signal
but we're not going to get there with like dropping,
like here's a pre-trained RNN that does thing Y
and then we're going to drive it with Q, you know, like,
so like if, you know, where do you,
where do you see the research going
in this direction of trying to unify this world
or do you think I'm wrong?
Maybe I just don't understand enough about the beautiful
elegance of model-based reinforcement learning.
Well, I definitely wouldn't say that you don't understand it
but I actually think, well, let me first say,
since we published Sequence Tutor,
there's been a lot of work that actually integrates
learning with RL and supervised learning at the same time
so I was talking to some machine translation
at Brain Montreal people that were enlightening me to this
and why not just do this simultaneously?
So I think there's really something in,
in what you're saying there
but actually I really think the promise of deep learning
is this representation learning promise
and why it's been so vastly successful is
you can take a whole bunch of data
that you can just scrape from the internet,
just scrape every web page that ever was linked to you from Reddit,
build an amazing representation
and then plug that into so many downstream tasks
and either continue learning on top of it or just leverage it
and because you can just connect these things
like LegoBox and backpropagate gradients
into your pre-trained model, I think that's just so powerful.
So I think if you say you could never pre-train and plug in,
you're like erasing a lot of the benefit
of like doing this representation learning.
Well, no, I mean, I have this faith that we can,
we can understand more about how organisms learn
which is by pushing the world around
and interacting with the world
and learning from that interaction.
Sure, yeah.
And, you know, like the sample inefficiency of,
of like training on, I mean, sure,
we can sort of slurp up this huge language model
of all of the internet.
Yeah.
But I still carry a certain cognitive science part of me
that says we should slow at some level be inspired
by how humans learn as definitely not what babies are up to.
Well, I mean, that's a question.
Do we have a full understanding of what babies are up to?
I'm not sure.
So I think Nanda would say this.
Okay, all right.
Okay, partial understanding.
But yeah, okay.
I think Nanda's point, which I really like,
is that we still don't have a model that has trained
on the richness of a baby's experience
from the time it's born to like five years old,
like not only all of the experience
of manipulating the world, viewing the world,
hearing language, as well as audio.
Like we haven't integrated all those signals.
And so we don't have a perfect way of doing that.
But I think a promising way to actually integrate
those two pieces of knowledge is to not only let a model
like manipulate and interact with the environment,
but maybe integrate a really powerful language model into that
and maybe align those representations.
If you had an alignment of language understanding
with acting in the world, it seems like then you could get
to something that could be approaching
something really general, right?
So I think it wouldn't make sense right now.
Maybe it's not the final end game,
but it wouldn't make sense right now to throw away the idea
of like taking this really good language model,
for example, and building it in.
Okay, thanks.
Your question actually about not wanting a pre-trained model
made me think of we had a human learning person
on your committee.
They would say, well, that's 20,000 words.
Nobody starts with 20,000 words.
They start with fewer.
So do you think you could start without the pre-trained one
when it's fewer and then you get pre-trained,
then that becomes the pre-trained one for the next one,
and there's a whole developmental process there.
Yeah.
And how different would that be from what you're doing here?
Yeah, that would be a really interesting
and challenging like scientific problem,
but I think you could do that.
So having this curriculum of like learning
in a really restricted set, how to use language,
generalizing that to a little bit more of a broad domain
of using language, building in words,
and seeing maybe how the learning looks different
under that scenario than just instantly having this
actions-based 20,000 dimensions
and training on all this language.
And so maybe that's something I could pursue
in the future projects I'm thinking about
with these multi-agent scenarios where agents are communicating
about what to do in the environment.
We could restrict the action, the word set
to be very related to the environment
and then maybe expand it.
Which becomes like an expert in a subdomain or something
in the exchange.
I'm going to end on one last question.
And ask you what you think your thesis
or your work on your thesis, just your own reflections,
might tell us about how we could improve human learning.
Oh, interesting, yeah.
Well, I guess maybe affect sensitivity
would be really important.
Empathy is really important for human interaction,
like actually trying to put yourself in the shoot,
like not that Maya models are at all doing this,
but put yourself in the place of someone else
and do that perspective taking,
trying to model their intentions
and trying to coordinate with them on that level.
And basically also sensing that using affective signals
as a way to update and understand
what they're really trying to communicate
or what they're really trying to do.
Okay, I know there's still a lot more questions,
but maybe you'll get time to catch Natasha
in what will hopefully, I think,
pretty clearly be a celebration afterwards.
Thank you so much, Natasha, for a fabulous presentation.
There's going to be ice cream.
There's going to be ice cream.
It will be in the affective computing area back there.
