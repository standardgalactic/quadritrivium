from the environment, we actually
in expectation over those trajectories
recover this mutual inference reward.
So influence is the mutual information in expectation.
So we think of this as a novel form of social empowerment.
If you remember, empowerment was the mutual information
between an agent's action and its own future state.
And this is the mutual information
between multiple agent's actions.
But I'm going to stop here.
Does anybody see a problem with rewarding an agent
for causally influencing the actions of another agent?
Get in a couple of smiles.
What do you think?
Does an agent, in order to influence another agent,
does the agent always have to help the other agent?
So not necessarily, right?
So it might be possible for me to just get in your way.
You have to walk around me.
I haven't helped you, but I've influenced you.
So how do we make sure that the agents are
influencing each other in a cooperative way?
Well, we took two approaches to solve this.
The first is that we tested this reward in environments
where cooperation is precisely the hard problem.
So we have these two environments.
The first is called harvest.
It's a tragedy of the commons environment.
So what agents are trying to do is pick up these apples,
and they get a reward for doing so.
But if they greedily harvest the apples too quickly,
then the apples will be depleted.
They won't grow back.
Everyone's going to go hungry.
So even though each agent wants to harvest more for themselves,
if they all follow that strategy, everyone loses.
So it's a sequential social dilemma.
It's meant to mimic a spatially and temporally
extended prisoner's dilemma.
This environment is similar.
It's called cleanup.
Agents can make apples appear by cleaning this river,
but it's partially observable.
So when they're doing this, they can actually
see if these apples are appearing.
Other agents can easily just exploit them
by kind of stealing their apples.
So to make that more concrete, what we're looking at here
is called a shelling diagram.
And it's describing the payoff that each agent can get
for following, for example, a defecting or exploitative strategy
in red or a cooperative strategy in blue,
depending on the number of other agents out of five
that are cooperating with it.
So if everyone's cooperating or if no one is cooperating.
And you can see that the agents are always
going to get a higher payoff if they defect on their fellow
comrades here.
But of course, if everyone follows that strategy,
again, everyone loses.
So these types of long-term trade-offs
over repeated iterations are very hard for vanilla RL
algorithms to solve.
In fact, they basically can't.
But what we see is that when we add this reward for agents
having influence over each other or having
high mutual information, we see that the collective reward
obtained by all of the agents is higher.
And that means, basically, it's not
any given agent that's doing better,
but all of them are doing better together
in this environment, meaning they're
harvesting sustainably, for example,
and it's evidence that they're cooperating.
But how are they cooperating?
How is this influence reward leading them to cooperate?
Oh, and I should mention this actually does beat previous work.
So this previous work agents were
able to view each other's reward function.
And we actually see superior performance just
from this influence reward.
But how does it work?
So let's check this out.
So in this video, we have this purple agent here
is the only one that's been trained with the influence reward.
And you can see that it behaves a little bit differently
than the other agents.
So when there are no apples, the other agents
keep exploring to try and find apples.
But the purple agent is holding still.
And in fact, it only ever used two moves to navigate the map.
And it only ever traverses the map
when there are apples present.
So if it's moving, there must be apples.
How does that allow to gain influence?
Well, if we take a look at a moment of high influence
between the purple agent and this yellow agent here
that can only see what's in this partially observable box,
we see that there's this delicious apple that's
outside of the view of this agent.
But when the purple agent chooses to move towards the apple,
which it only moves when there is an apple,
that communicates something to this agent
that there must be food present.
It changes its intended behavior.
And this agent is able to gain influence.
So that's really interesting.
And then we see something like that happening again.
So here, this pink agent here has been cleaned in the river.
It can't see if any apples have appeared.
But when this agent chooses to stay still,
that signals that no apples have appeared,
again, changing its intended plan
and the agent gains influence.
So what we see is that this influence reward
has led the agent to learn to use its actions
to communicate about the presence of food
in the environment.
So you can think of it like a little B-waggle dance.
So we thought this was quite exciting.
And so the next thing we did was to train explicit communication
protocols with this influence reward.
So what we're doing here, here's our normal policy
for acting in the environment.
And this is a policy that says which
of several communication symbols the agent
should choose to emit and show to the other agents
at the next time step.
So it's kind of this cheap talk communication channel.
I can say something.
Nobody else has to listen to it.
The agents are free to ignore that channel.
And the idea is that we're going to train this communication
policy with the influence reward.
So agents get a reward if something they say
influences the behavior of another agent.
And we think that this actually overcomes
the problem of the influence not being beneficial.
Because the only way an agent that
is acting in the environment to maximize its own reward
is going to change its behavior based on a communication
symbol that it's free to ignore is
if that symbol is going to reliably help it achieve
its own environmental reward.
So we think that influential communication
should benefit the listener.
That's our hypothesis.
So how well did this work out?
Well, I should at first point out
that cheap talk between self-interested agents
actually doesn't work.
So we can show theoretically that as agents' interests
diverge, the amount of information placed on the channel
goes to zero.
And we can show empirically in past work
in multi-agent reinforcement learning
that agents that are self-interested
don't learn to use these types of channels effectively.
But we see that when we add this influence reward,
the agents achieve, again, higher collective return,
suggesting that it is helping them to coordinate.
And we also see that the communication protocols they
learn are more meaningful.
So agents are speaking more consistently
about what they're doing in the environment.
And they're also having high mutual information,
as we suspected, when there are moments of influence,
although that is a very sparse occurrence.
But this is my favorite slide.
So what you're looking at here is
a degree of influence between an influencer agent
and an agent being influenced and their individual returns.
And what we see is that agents that are more easily influenced
or a better listener, if you will,
tend to get higher individual environmental reward.
And this is a reliable finding across many different hyper
parameter settings, hundreds of examples
of these environments.
So this seems to support our hypothesis
that the information being sent over the channel
is helping agents to achieve their own environmental reward.
So this is to make Joel happy.
So this slide is to show that these results are reliable.
We ran it not only for five random seeds
for each hyper parameter setting, but for the top five
hyper parameter settings.
And we see consistent performance improvements
from this influence reward.
I do want to mention that there are some caveats.
So we talked about whether the action-action influence is
going to help the other agent is going
to depend on the environment, on the hyper parameter settings.
We think that one of the reasons that communication emerged
in this setup is that agents were trying
to maximize both their environmental reward
and their influence.
And actually just communicating information
may be one of the cheapest ways to gain some influence
without deviating too much from your optimal policy
in the environment.
