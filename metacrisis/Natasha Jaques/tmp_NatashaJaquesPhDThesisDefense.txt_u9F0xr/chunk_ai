Yes.
So let's use the language in the channel.
To ensure that the agents somehow internally develop concepts.
That would be great.
That would be amazing.
Yeah.
And so that brings me to your other work,
which is on learning from trajectories of observations
by imitating.
For me, one of the best examples of imitation is,
I forget his name now,
the high jumper that in the 70s in the Olympics
and learned to jump backward as opposed to jumping forward.
So up to that point, people had come up with all sorts of crazy strategies.
That's just basically RL.
And then what happened at that transition point
is so it goes that he was an engineering student
who went and did a lot of calculations about the center of mass
and so on and figure out that this is what he had to do.
So if stories are true, then he used abstraction
to be able to solve the problem.
And since then, we've been just following straight imitations.
Stuff that is very similar to your PCQ
or regularized reward maximization.
So then this question of how to get abstract concepts
seems to be very important to how we do bad imitation.
But do you think that having then just the language channel,
that will be enough?
Would there be a way of you testing that the agent having acquired
your RL agent, which started with this language
by exploiting the language channel,
was able to do some improvements
over just the behavioral cloning policy
that would show that it was exploring in this higher abstract space?
That's a great question.
And I think the language model, as you said,
might give you some clue towards concepts.
And I think it actually might give you a nice path to generalization.
So if I give you a command that you haven't exactly seen before
but is close in embedding space in the language model,
then I could infer what you're talking about in the environment.
I think that's cool.
But I don't know if it would get us all the way there to like,
if I have a trajectory of low-level actions
abstracting out high-level goals,
like what was this person intending to do?
And of course, condition on a goal,
the low-level actions could be quite a bit different.
And so different people might take the same goal
and actually enact it very differently.
So I think there's kind of an unsolved problem there
in terms of like segmenting a trajectory
into these high-level goals.
But I think it would be an interesting question to pursue
because you can kind of think of this high-level goal
that the person is pursuing as their intention,
like what are they trying to do?
Like if I observe, it goes with the multi-agent learning piece, right?
So like if we're trying to coordinate to make some recipe,
one of us has to get the tomato,
one of us has to get the lettuce,
if I observe your low-level actions,
I need to infer that you're getting the lettuce
so I can get the tomato,
that's sort of like inferring this goal.
So I think they kind of overlap.
Thank you.
Thanks.
And I have two more questions.
There's sort of more like nitpicking.
So your reward function for that project,
it involves a lot of small numbers,
I mean numbers with like three digits and so on.
Okay.
I forget the page on which it is, but.
Yeah.
Yeah, 0.157 times the question plus 0.138 times the semantic coherence
plus 0.10 so on.
So it's just very,
is there any hope that instead of like building a reward like this,
that you could use something like say,
generative adversarial imitation learning to learn a better reward?
I think that's a really promising direction.
Yeah. So the reason those numbers are so small is by the way,
we're just normalizing so everything sums to one.
But yeah, I think that's a really good point.
I don't think those rewards are the final answer.
So we saw that our agents were moving towards,
yeah, more polite and cheerful language,
but we don't know that that's actually a better conversation.
There's a lot more richness to what makes a good conversation
than those signals.
So rather than sort of hand crafting that,
you could also think about some type of like
inverse reinforcement learning,
observing people talking to each other
that do have a good conversation.
Of course, I have prior work on inferring,
what is a good conversation?
So then look at the good conversation and say,
what is it that people are enjoying about the conversation,
how do those conversations look,
and try to get the reward function that way.
Yeah. But I think there's definitely a lot of
future work to be explored there for sure.
And the last question has to do with,
so there is this beautiful cycle for doing an RL from demonstrations,
which is, you can learn the policy completely from data,
as you have shown here.
You can learn, we could potentially learn the reward from data.
The other thing we could learn from data, perhaps,
is how good, or at least from very few interactions
with environment, is how good is the policy before we deploy it.
Yeah. So you mentioned,
in some cases, it will be catastrophic.
You deploy your dialogue agent on Twitter,
and it says a lot of obscene things,
and you're in big trouble, so you get fired for that.
Yeah. But in some cases,
it's inevitable that we will deploy a policy,
and there will be some adverse effect,
and we might have to re-update that policy.
Yeah. And so how could we learn from the experience
to design a better agent?
Yeah. So there's a whole rich field on off-policy,
policy evaluation that we were looking into a little bit,
because it's related to this paper.
So you can use things like important sampling to re-weight samples,
so they look more realistic under your current policy.
And I think that is very important.
But of course, the question is, if you're deploying to humans,
are you able to actually foresee this type of inappropriate behavior?
Is that sort of extrapolating off what you even intended?
Like, in some sense, you kind of have to model
that that's a region of state space to anticipate
that the policy is going there.
So I think, I mean, for me,
maybe this isn't a very machine-learning answer,
but you kind of need some kind of human in the loop
to assess whether this is actually safe and appropriate to release.
Yeah. So it's a complicated question, yeah.
The question, I guess, that I was trying to get at
is once you're released,
and you discover it doesn't quite do what you want,
how can you learn from that to update your policy?
Do you mean iterating on the reward function
or iterating on what it's trying to optimize?
Yeah, I think that's really important, right?
So an ongoing way to...
So maybe you're saying there's this iterative process
of inferring a better reward using gaol,
using inverse RL, retraining, redeploying,
and continuing in that way.
Mm-hmm.
Cool. That seems like a really good idea, yeah.
Thank you very much. No more questions.
Oh, it's Amanda here.
Hi. Really great presentation.
I just have a very general question,
which is specific to, I guess, your end goal or your future steps.
Yeah.
Very broad.
What do you think could be the next set of next steps
for tackling this problem of social intelligence?
And I'm in particular really curious about
what led you to reinforcement learning,
specifically mall-free reinforcement learning,
for handling a problem of multi-agent RL
or multi-agent settings.
Do you think...
What, in your mind,
are the limitations of using RL as a solution framework
for handling those types of settings?
I mean, already from the presentation as well as from the Q&A,
you sort of get this feeling of there's a problem of reward engineering,
there's a problem of defining what is a good reward
in order to get some qualitative good policy.
Or, you know, after you trained a policy,
you notice that they were using some sort of navigation policy
as a way of communicating other intentions, right?
There's a lot of really interesting conceptual frameworks
that we're building on top of what we train
in a model-free RL approach.
And I'm curious about,
given all of the work that you've done
putting into understanding social intelligence
and modeling it,
what are the limitations and what do you think
it should be the next step forward?
Should we stick with RL?
Or maybe it's another framework in the works of,
you know, taking us in the right direction,
if there is a right direction.
Sure, thank you, thank you.
Yeah, so I would say,
firstly, I'm not in love with, like, model-free RL.
I don't think we have to limit ourselves to that,
and actually what we saw in the influence work
is that having a model of other agents was critical to doing this well.
So, modeling the world seems like a nice way
