We think that influence on self-interested agents
via this cheap talk channel should benefit the listener.
But that depends on agents that are interacting repeatedly.
So if in some future work, there was some agreed upon
communication protocol that agents
could use to communicate with a new agent
that they would never see again, they could potentially lie
and still gain influence.
But of course, if you're playing with the same agent
over and over, and they lie to you,
you're not going to trust them, and they will lose
influence over time.
So in conclusion, we think of this social influence
as a unified method for promoting coordination and communication
in multi-agent settings.
And this allows the agents to learn socially from each other,
but train independently.
And that's actually a big improvement over prior work
in Marl, which often relies on agents
viewing each other's reward or having a centralized controller
that teaches all the agents something.
So we think this is nice.
So that's everything for this project.
I want to stop right now if anyone has a question.
Clarification, yes?
So they have some agents, so they have to infer each other's
actions, but they still have access to that.
Yes, we make the assumption that agents
can observe each other's actions.
But actually, in the final product,
we ended up saying that you can't influence an agent
that you can't see, that's not within your partially
observable view of the world.
So we think that's more reasonable.
Also by the communication channel.
The communication messages are always sort of heard.
Even if you're outside of the view,
they're transmitted to all the agents.
Yeah, which we again think is pretty reasonable.
What is the dimension of this communication?
Is it like 1?
We vary the dimensions, I think, between 10 symbols
and 20 symbols, so pretty small.
But they can actually, the interesting thing
about this that we didn't get a chance to analyze,
is they could potentially communicate
compositional messages.
So if I say AAB, that might mean something
different than AAC.
And actually, the metric that we have of the communication
doesn't account for that.
So it would be nice future work to more deeply analyze
how the agents are communicated.
Yeah, see what the unit does.
Exactly, yeah.
Other questions about this project?
OK, let's move on to the next one.
We can always take more questions at the end.
So I'm really interested in this idea of social learning,
because I think it's really powerful.
But it seems like the best and smartest things you could
learn from in your environment are going to be humans.
For most tasks that we want an AI to do, like for example,
just recommend us music, humans are
going to know better how to perform the task.
And I'll give you a motivating example
why learning from humans could help.
So this is a real conversation between a user
and their Alexa divide.
Alexa, what's the right way to Walmart?
The right way to spell Walmart is W-A-V-L-A.
So I would bet you that when this mistake happens
and Alexa doesn't meet the need of this person,
the way they're going to respond is
going to contain some element of frustration,
whether it's their tone of voice, the words that they use.
There's a signal there that Alexa
could be using to understand that she did the wrong thing
and she better not do it again.
So wouldn't it be cool if we could pick up on these signals
and learn from them?
And this is important not just for making deployed systems
like Alexa more useful to people,
but I would argue it's important for long-term considerations
of AI safety.
Because if these models are able to incense accurately
our preferences and adapt to them,
then they're going to be more likely to take actions that
are aligned with their preferences.
But the problem is that people don't actually
enjoy manually labeling their preferences.
So nobody actually wants to fill out these cards.
How many people have ever trained the Google algorithm
by filling this out?
OK, got a few hardcore people.
Thanks, Dad.
Good job.
I think most people don't actually enjoy doing this.
But in contrast, social feedback is rich and ubiquitous
and natural.
You're already making facial expressions.
You're already having a tone of voice.
Couldn't we pick up on this?
So how many people have ever used a frustrated tone
with their Alexa or Siri or yelled at their Roomba?
OK, so there's some signal there.
So that was the focus of this project.
So we wanted to make an AI agent that
could sense human preferences.
And here we wanted to sense this in text.
So here's an example of me talking
to a conversational AI, a chatbot.
And you can see that we have this state-of-the-art sentiment
detector that was actually developed here in the Media
Lab that can tell that when I say something like,
that didn't make sense, that's a negative sentiment.
I'm not happy.
So couldn't we use this as a reward signal
to disincentivize the agent from whatever it just did?
When the conversation is going, well, it can detect that I'm
happy and the agent is doing something right.
So that's the idea behind this project.
I worked on this project with a number of wonderful collaborators
in this group, including Asma and Judy,
and I'll talk more about that later, and Craig, Agatha.
But we ended up building this whole platform
to allow people to chat online with our bots
and collect this human interaction data.
And so our hypothesis, even though in that interface,
we allow people to manually tag the bots responses as good
or bad using this up-vote-down-vote,
we think that learning from these explicit manual ratings
is not going to scale as well as learning from these implicit
cues, like the sentiment.
And so the implicit cues we focused on in this work
were not only the sentiment of each utterance,
as well as over the course of the conversation,
but even whether we could get the user to laugh,
how much they typed ha, how many words they typed,
how long they bothered to talk to the bot,
because we reasoned that if it's totally terrible,
you can't really put up with talking to it for too long.
And we also rewarded the distance in embedding space
between the bot's answer and the human input
to help it stay on topic.
So these are rewards that are elicited from interacting
well with a human.
They were based on some research into the psychology of what
makes a good conversation.
And based on that, we also added a reward
for asking questions, because apparently this
is an important active listening skill.
But the issue with this reward, of course,
is it's totally in the bot's own control,
whether it asks a question.
So it's kind of trivially exploitable.
So we'll see how well that works out.
So what we're trying to do here is
take this big conversational AI model that
takes in some text and outputs a response,
and train it with reinforcement learning.
And this is actually a really challenging problem,
because normally in reinforcement learning,
the number of actions you can take,
like in the previous project, is on the scale of like 12 actions.
But in generating language, there are 20,000 most frequent
English vocabulary words that it can choose from for each step.
So this becomes very difficult.
And similarly, the state space is the space
of all possible conversations.
So that's also a little bit more complicated
than the little grid world we saw in the last project.
So that's difficult.
And then trying to learn from human interaction
actually presents its own problems.
So here, we're treating the human as the reinforcement learning
environment that we're trying to learn from.
And when you're learning from human data, trust me,
it's very painful and expensive to try to collect that data,
especially if you're in academia.
And so you basically want to salvage any little piece of data
you got from a human, no matter how long ago you got it,
no matter how different the model was or the data set,
you need to be able to use it well.
And so this necessitates really good off-policy reinforcement
learning.
And then the second problem is that you
need to be able to test your model very carefully
before redeploying to humans.
You basically can't put some model on the internet
and have it learning from its interactions with humans
in a continuous way, or you might end up
with some inappropriate behavior.
So you basically have this atypical broken interaction
loop, where your RL algorithm, in contrast with normal RL,
is not able to explore online in the environment
to refine its estimates of how good certain actions are.
And so this is called batch reinforcement learning,
learning from a static batch of data
without the ability to explore.
