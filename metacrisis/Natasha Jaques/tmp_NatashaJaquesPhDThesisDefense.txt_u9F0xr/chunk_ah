Auntie Lana, so many more.
So I really want to say thank you to you.
And then, of course, my friends.
So I saw you sitting through a lot of SAP acknowledges,
but I have to do it.
You guys have met so much to me, meeting friends here
that we've been so close over the last five years.
It's starting to feel like you're just my family.
Like Christian and Juliana picked me up at the airport,
which I think is like the real bonding signal.
I really want to call out thanks to Max.
He's been amazing.
Juliana, she's been my friend for so many years,
such a sounding board for ideas.
Christian and Juliana, so close.
Andrew has, of course, been incredibly supportive
for many years.
And Bianca, Kelsey Allen is here.
Thank you, Kelsey, Peter Beshai.
Couldn't miss Eddie, Martin, Carolyn, Sam, Gavin.
And of course, you guys won't get this,
but I have to do it, because these friends are actually
really meaningful to me.
I've done a lot of internships and spent a lot of time
away from Boston and become really close to some people
that couldn't be here today.
So I do just want to call out Ben, Laurent, Kyle, Nuru,
Joanna, Tim, Anna, Yannick, Mattias, DJ.
So hopefully, if I send them the recording,
they'll know that I acknowledge them too.
And now, at this point, I'm just making you look at a bunch
of sappy pictures throughout my PhD.
So I'm going to wrap up.
So thank you guys so much for coming.
I really appreciate it.
I think there might have been a little social influence.
OK.
Time for Q&A live with our public audience here,
and committee members are fine to ask things too.
I should probably, since this is being recorded,
pass around the mic also.
I can also repeat the question.
Yeah.
Amazing presentation.
Thank you so much.
Thank you.
Really a treat.
It strikes me that probably the most commercially valuable
application is really just manipulating people
on a massive scale in surveillance capitalism
and all that.
What can we all do to make sure that everything
that you're doing ends up helping people as best
as possible given that?
That's a great question.
So actually, I really wanted to stay away
from follow-ups of the social influence work
that we're influencing people.
I think that gets down somewhere dangerous really fast.
But you should also think of the social influence work
as a way to compute the degree of influence between two agents.
So you could use it as a metric of saying how much someone was
influenced by a given action.
Potentially, you could use that to detect
whether people are being influenced and maybe intervene.
So I think that could be interesting.
And then in terms of the affective components,
you can think about people's affective safety.
The reason I talked about AI safety
is your affective signals are changing.
So if I make a joke to you and you laugh and I make the joke
again, you're not going to laugh the second time.
So there's this non-stationary reward signal
that measures whether you're actually currently
enjoying what's going on.
And so if you were able to actually use, for example,
reinforcement learning, which accounts
for total expected long-term reward,
and you were really able to use that to account
for your long-term enjoyment and happiness and fulfillment
and flourishing well-being, which is something
that our group has thought a lot about measuring,
if you could really optimize for long-term well-being
using these functions.
And I think that would be a more safe AI model.
But we still need people in power to believe
in that reward function.
For people felt that they don't always unfortunately.
By the way, people want to follow up on that question,
which is one I'm very interested in.
I'm happy to continue to explore that one with you guys.
Other questions?
Joelle?
Or someone on the phone, too.
Or Doug Granando.
He might be muted.
I think I see that there.
I've just been muted.
I'm sure I do have questions.
So first, that was an amazing talk and an amazing dissertation.
And since there are some other PhD students in the room there,
so I want to point out that Natasha set the bar at 14
research papers for a good dissertation.
So the rest of you have a lot of work to do.
Joking aside, so my main question, the one that I've really
come away with asking is, what is social?
And can we meaningfully talk about social in the context of agents only?
Like there's a nice bridge to build between,
I'm thinking of the dissertation document, not the talk,
but between chapters, let's see, four and five, I think, three and four?
Oh, no, chapters two and three.
When I think about the work that we did together and the media
generation work, the music generation work,
when I think about two music agents jamming together,
it's not clear to me that that has anything to say about a human
and an agent jamming together.
And I'm wondering if you think differently,
if you think there's something we can take from the literature
of multi-agent work and apply it to humans.
And then more generally, is that interaction among agents,
even something that we would call social,
or is social embedded in the world and embodied in the world?
That's my big question.
That's a great question.
So I think there's definitely something there in the sense of like,
can we even say that these agents have agency?
We don't, are we there yet?
Probably not.
But what those problems share and what the jamming problem actually shares
is trying to infer another agent's intentions in order to coordinate with it.
So if you're jamming with someone, you have to kind of understand
where they're going, where they're taking this,
how you can work with them in order to make that sound good.
And similarly with the multi-agent problems,
they do share that in the sense of like,
if I want to coordinate with another agent to achieve some task,
I have to infer what they're doing and try to achieve the other part of the task.
And that also works with humans.
So if I want to make a human happy,
I want to please a human with whatever Alex is doing,
I need to infer what they want, infer their intention,
what are they trying to get out of the interaction with the agent, etc.
So I think this idea of like, modeling other agents
and understanding what they're intending and how to coordinate with them
is shared across.
Agreed. Okay.
That's my main question.
I'll pass to Tynando, I guess.
Thanks. Natasha, great work.
I think this is a brilliant thesis,
and I hope just the beginning of a very bright career.
I have a few questions for you and sum up purposely hard.
You mentioned it's a PhD, so you get to be asked these.
You mentioned that, so one of the very nice things about your work
is by having the influence or reward,
the agents were able to communicate better.
So you were able to train agents that would communicate, excuse me,
in a completely decentralized manner, which hadn't been done before.
Like, very good researchers failed at doing that, including myself.
And then, you mentioned that one of the things you would like to look at
is compositional communication.
Why do you think that's important?
And the other thing is, if you think of it as like a stream of, say,
discrete symbols that can combinatorially be combined,
why do we need that and why can't we just go with a single continuous
factor?
Yeah, that's really interesting.
So I think in some sense you kind of answered your own question,
like in saying that it's combinatorial.
So with this small set of discrete symbols,
which we know how to do really well with current reinforcement
learning techniques, learn a discrete policy,
we get this combinatorial explosion of things we could communicate
or things we could talk about.
So I think you always give this example of like,
there's a village of 300 people.
I don't have to talk about each one individually.
I can group that, I can make categorizations with these symbols
and you can get the relationships among them.
And why don't we want to go to continuous vectors?
I actually think we don't want to go to continuous vectors
and we no longer want to use ungrounded discrete symbols either.
Because I think while studying like the emergence of language
is really interesting, what would be interesting to pursue
going forward is actually to try to have,
to go back to this idea of having agents communicate
with human natural language.
Because I think for the problems that I want to look at,
it's not only can agents coordinate with each other
and communicate with each other,
but can they generalize that to actually coordinating with humans.
So taking the same policy that can flexibly adapt
to multiple different types of agents.
And then you need this like grounded language in natural language
so the agents can use it, but it can also be used with humans
so they understand it.
So that's why I think we should go that direction.
Rather than use continuous vectors.
All right, so you would basically capitalize on the fact
that there's creatures on this planet that already have language.
