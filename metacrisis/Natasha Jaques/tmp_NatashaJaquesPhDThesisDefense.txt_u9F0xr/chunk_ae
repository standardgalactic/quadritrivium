using a bit more realistic language.
You ask it, hey, what's up?
It says, I'm not sure, but I'm glad to see you.
We did notice that while it is realistic,
optimizing for this positive sentiment
did lead it to be extremely cheerful,
polite, and supportive.
So I was saying things like, thank you, you're the best.
So we actually developed some post hoc metrics
to assess exactly how cheerful and polite these bots are
by sort of counting things they say,
like thank you, I'm sorry, et cetera.
Maybe we're gonna call this the Canadian-ness metric,
I'm not sure.
But what you're looking at here
is all of the techniques that we trained
and the Z-scored reward that they obtained
not only from the rewards they were trained on,
shown in red, but these post hoc metrics shown in green.
And what you can see is that as we predicted,
these batch queue models trivially exploit
the easy rewards, like asking questions.
But the KL control models have shifted
their distribution to this polite, supportive,
and cheerful language.
And that actually allows them to elicit
this higher human reward.
So they have to do a more complicated thing, basically.
And then we tested how well the different reward functions
worked when we trained models
with each reward function independently.
And we see that actually the sentiment-based reward
led to the highest quality conversation.
So this affect is important in a good conversation,
which is a really nice finding
when you're in the affect of computing group.
So we also saw that these models trained
on implicit reward do score higher
than training on these manual upvote downvotes.
So that helps validate our hypothesis
that getting humans to manually label their preferences
doesn't scale well.
So in conclusion, this is the first work
to propose using KL control from a pre-trained prior
as a way to handle this off-policy,
no exploration problem, batch RL.
We proposed several enhancements
that all work to varying degrees.
We found sentiment to be the most valuable
implicit reward.
And we see that implicit signals are more scalable
than explicit button presses in this case.
All right, any questions about this work?
Yes?
Do you have a comparison to an equal number
of sentiments and votes?
Because it's obviously more scalable,
but what's the actual signal by current sentiment
or implicit vote?
So arguably you get the sentiment no matter what.
And then the explicit vote you don't get
unless the human bothers to send it.
So we just say that we have the same number of utterances,
but people can voluntarily press the button.
We encourage them to do that in the instructions.
But yeah, I don't have the comparison of like,
if we restrict only those utterances
that were upvoted, downvoted to compare with the sentiment.
Maybe in that case, you would get better performance
from manual upvotes.
Part of the problem is that we just think
people won't bother to do it, yeah.
Yes?
I can imagine that some people like a happier Alexa
and some people like a more, I don't know,
sensitive to Alexa, et cetera, et cetera.
Can you train it by giving sort of the input
an extra parameter which you learn,
which you teach the agent to map to,
for example, sentiment?
So that if you buy a new Alexa,
I said happy minus two, five, six, seven.
You can't be very happy if you want it automatically.
Doesn't have to relearn the whole policies
of math and learning or so,
but that automatically, is that possible?
Yes.
That you automatically get it to be happier.
That's a great idea.
And I think there is definitely something really interesting
and deep there in how to respond appropriately,
because actually just expressing positive sentiment
isn't even good for the same person
but across different contexts.
So knowing how to respond emotionally at different times
is a really challenging problem
that we've only just started to dig into or think about.
But also in terms of like, each user having
different preferences about how chipper
they need their Alexa to be,
I think that's definitely a good point.
And having some personalization
where you understand the user and adapt better to them
could be really cool.
Actually, later in this talk,
I will talk about methods for personalization
that I think could lead nicely into that.
So thanks for your question.
All right, yes?
Yeah, I have a question about the explicit versus explicit.
I'm wondering your thoughts on if there's like a spectrum
of when explicit can be maybe more desirable,
more human's more willing to do it.
I'm just thinking like, I treat my dog,
and I'm like willing to say like, yes, you did a good job.
Yes, you did a good job.
Yes, you did a good job.
And I have a long-term relationship with this animal.
Yeah.
But similarly, there's like an AI
that I was like forming a long-term relationship with.
Maybe I'd be more incentive by such a human
to like give an explicit kind of responses.
Yeah, and actually, I'm really glad you asked that
because the next project,
we also look at implicit signals from facial expressions.
And we think a lot about the interface
and how the interface could be designed
to elicit more feedback, basically.
And I think there's something really rich there.
But yeah, I think that's an interesting direction
for future research.
Yeah.
Okay.
Thanks.
I'll go to the next project there.
So speaking of that project,
this was the same motivation,
but here we want to actually learn from people's
implicit signals from their face.
So the way we did that,
this is a quick description of this project,
is we thought about which deep learning model
is likely to generate samples
that a person would possibly make a face at.
And we landed on sketch RNN,
which draws these cute doodles
based on a sequence of pen strokes.
And what we did was we showed those doodles to people
and we collected their facial expression response.
And then we trained this latent constraints model.
So if you get the lingo,
this is a GAN,
basically it's a GAN on top of a VAE.
So let me say that a little more clearly.
This model already has a super cool representation
of sketches that is learned unsupervised
by training on a bunch of sketches.
What we want to do is we want to understand
which parts of that representation space,
these are sort of all the sketches you could possibly draw.
These are all the sketches that are kind of realistic.
We want to learn which part of the realistic sketches
actually make people smile and make them frown less.
So learn which part of the representation space
to convert to.
And then we apply this technique
to helping this model learn to sketch better.
So this is how the model used to sketch cats
before our technique.
You can see that there are some pretty good cats,
but some of them are a little questionable.
So as we train with only a small amount of facial feedback,
this is a very sample efficient model.
We actually only had like 70 samples
of people smiling or not at this cat.
We see that it learns to draw
a significantly different type of cat.
So we've got these little cute mouths,
we've got whiskers, smiles.
So whether drawing significantly cuter cats
is a marvelous scientific achievement or not, I don't know.
But if we look at the rhinoceros model,
we see something interesting.
So it turns out people, which is what trained this model,
and this model are very bad at drawing rhinoceroses.
So I don't know what this is,
but it's not a rhinoceros, right?
And what we see is that again,
training with only around 70 samples of facial feedback,
the model has learned to draw
actually pretty realistic looking rhinoceroses.
So we actually think this is the first work
to show that you can improve the quality
of a generative model using facial expression feedback.
And we did test this with humans,
and yes, they do significantly prefer our models sketches.
So that was a foray into that.
But the issue with all of these papers
on learning from these implicit cues
