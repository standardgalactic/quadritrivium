than some of the previous ones.
Yeah, yeah.
I'll ask one last question perhaps for the benefit
of the audience is more than yours.
Looking back at this thesis
if there's one piece or thing that you wish
you would have done really differently
what would it be if you had the benefit of like six months to...
Yeah, well there's many.
I guess like immediately what popped to mind
is that there's whole projects that I spent like a year on
that didn't make it into the thesis.
Like I wrote a whole journal paper on like a UX problem
on behavior change that we just never submitted
because people dropped out of the study
there was too much attrition
and then I wasn't very happy with the paper
it was never submitted anywhere.
So those are things I would have done differently
like the work that got dropped on the floor.
I guess one thing that connects a lot of the things
that I wish I'd done differently
is giving myself more time to do a project in great detail.
So I think we talked about how I actually think
the dialogue project is a little bit premature
like we put it out because I'm graduating
I have to wrap it up
but I would have loved to spend another four months on that
and really dig into it and improve some of the models.
So this has happened multiple times
where I haven't given myself enough time to finish a project
I was doing this whole Gaussian process thing
I didn't finish it
I was doing this whole curiosity thing
I had to go to an internship
I only had two months
and I had to sort of drop that project behind.
So basically giving myself more long term time
to really finesse a project
because I think it makes more sense
to put out fewer really impactful papers
than just churn out papers before they're ready.
Yeah, yeah.
Thank you.
Yeah.
Hope Juliana likes that answer.
Yeah.
Can I jump in with one more quick question?
Yeah.
I thought we were going to go around Robin
so I only asked one question and then passed off
and all the rest.
It's okay.
Yeah.
I actually don't have a lot
because there's been a lot of ground covered
but there's something that you just said
sort of triggered like, yeah.
So you just said you need a pre-trained model, right?
Yeah.
Right?
Like I had hoped that
like my hope is that we don't need pre-trained models, right?
Like there's something,
there's something sort of so,
the thing that I like the least about model-based reinforcement
learning right now is so much of this work is kind of like
doing one, like having one model
and then then dropping it in
and kind of fine tuning it on a problem
or like I have this sense that there has to be
an integrated way to think about reinforcement learning
and gradient descent learning
and that we're,
we're sort of halfway there with model-based RL
but we're not all the way there.
And it's not just waiting through the math,
there's sort of a conceptual leap that we need to make
that allows models to learn from non-differentiable reward
and from some differentiable signal
but we're not going to get there with like dropping,
like here's a pre-trained RNN that does thing Y
and then we're going to drive it with Q, you know, like,
so like if, you know, where do you,
where do you see the research going
in this direction of trying to unify this world
or do you think I'm wrong?
Maybe I just don't understand enough about the beautiful
elegance of model-based reinforcement learning.
Well, I definitely wouldn't say that you don't understand it
but I actually think, well, let me first say,
since we published Sequence Tutor,
there's been a lot of work that actually integrates
learning with RL and supervised learning at the same time
so I was talking to some machine translation
at Brain Montreal people that were enlightening me to this
and why not just do this simultaneously?
So I think there's really something in,
in what you're saying there
but actually I really think the promise of deep learning
is this representation learning promise
and why it's been so vastly successful is
you can take a whole bunch of data
that you can just scrape from the internet,
just scrape every web page that ever was linked to you from Reddit,
build an amazing representation
and then plug that into so many downstream tasks
and either continue learning on top of it or just leverage it
and because you can just connect these things
like LegoBox and backpropagate gradients
into your pre-trained model, I think that's just so powerful.
So I think if you say you could never pre-train and plug in,
you're like erasing a lot of the benefit
of like doing this representation learning.
Well, no, I mean, I have this faith that we can,
we can understand more about how organisms learn
which is by pushing the world around
and interacting with the world
and learning from that interaction.
Sure, yeah.
And, you know, like the sample inefficiency of,
of like training on, I mean, sure,
we can sort of slurp up this huge language model
of all of the internet.
Yeah.
But I still carry a certain cognitive science part of me
that says we should slow at some level be inspired
by how humans learn as definitely not what babies are up to.
Well, I mean, that's a question.
Do we have a full understanding of what babies are up to?
I'm not sure.
So I think Nanda would say this.
Okay, all right.
Okay, partial understanding.
But yeah, okay.
I think Nanda's point, which I really like,
is that we still don't have a model that has trained
on the richness of a baby's experience
from the time it's born to like five years old,
like not only all of the experience
of manipulating the world, viewing the world,
hearing language, as well as audio.
Like we haven't integrated all those signals.
And so we don't have a perfect way of doing that.
But I think a promising way to actually integrate
those two pieces of knowledge is to not only let a model
like manipulate and interact with the environment,
but maybe integrate a really powerful language model into that
and maybe align those representations.
If you had an alignment of language understanding
with acting in the world, it seems like then you could get
to something that could be approaching
something really general, right?
So I think it wouldn't make sense right now.
Maybe it's not the final end game,
but it wouldn't make sense right now to throw away the idea
of like taking this really good language model,
for example, and building it in.
Okay, thanks.
Your question actually about not wanting a pre-trained model
made me think of we had a human learning person
on your committee.
They would say, well, that's 20,000 words.
Nobody starts with 20,000 words.
They start with fewer.
So do you think you could start without the pre-trained one
when it's fewer and then you get pre-trained,
then that becomes the pre-trained one for the next one,
and there's a whole developmental process there.
Yeah.
And how different would that be from what you're doing here?
Yeah, that would be a really interesting
and challenging like scientific problem,
but I think you could do that.
So having this curriculum of like learning
in a really restricted set, how to use language,
generalizing that to a little bit more of a broad domain
of using language, building in words,
and seeing maybe how the learning looks different
under that scenario than just instantly having this
actions-based 20,000 dimensions
and training on all this language.
And so maybe that's something I could pursue
in the future projects I'm thinking about
with these multi-agent scenarios where agents are communicating
about what to do in the environment.
We could restrict the action, the word set
to be very related to the environment
and then maybe expand it.
Which becomes like an expert in a subdomain or something
in the exchange.
I'm going to end on one last question.
And ask you what you think your thesis
or your work on your thesis, just your own reflections,
might tell us about how we could improve human learning.
Oh, interesting, yeah.
Well, I guess maybe affect sensitivity
would be really important.
Empathy is really important for human interaction,
like actually trying to put yourself in the shoot,
