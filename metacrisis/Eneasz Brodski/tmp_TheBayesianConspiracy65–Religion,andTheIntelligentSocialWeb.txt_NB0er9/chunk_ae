I couldn't think of them.
He gave me a week, like she's crazy genius at this.
Dude, do you remember the question?
It was basically Sabine had said something and Julia had kind of asked back with another example
and said, how is that any different than like this weird thing that I'm,
this weird example that I'm generating on the fly.
And she had said, or then Sabine replies,
you know what?
You're right.
I didn't define it carefully enough.
And so like, she can, she can nail those down really well.
So it's valuable for that.
But I was just killing time until he found what you're looking at on your notes.
So, did you want me to start then?
Yeah, totally.
But I'm just saying everyone should listen to rationally speaking.
Yes.
Okay.
I need to set up some background for this post before we start.
Is that okay?
Totally.
Okay.
So the needed background for this is Newcombe's problem.
I'm pretty sure you're familiar and I know we talked about it once before on the episode,
or not on the episode, but on the podcast.
But that was a while ago.
People might not remember.
Some people may not have heard that episode.
So I'm going to go over Newcombe's problem again real quick.
I think that was one of our worst episodes ever.
That was the game theory one.
Oh, okay.
Wasn't it?
I don't remember.
Oh, well, it sounds like a game theory thing.
So anyway, let's go over Newcombe's problem.
Alrighty.
Newcombe's problem.
Hold on.
Sorry.
Quick introduction.
When I say it was one of the worst ones ever,
I meant that that particular podcast of ours,
we meant to have a guest for.
And I think you'd led well,
but I wasn't the least prepared for.
And I think we kind of all floundered
and we got feedback of like,
that was not a good coverage of game theory.
And I was like, yeah.
So anyway, if you haven't heard that one,
you're not missing out.
Yeah, cool.
So all the more reason to cover it briefly right now.
Please.
Okay.
Newcombe's problem is this.
You are approached by a predictor.
We will call this predictor Omega.
Omega has some access to your decision making process.
Perhaps you are an algorithm and it can look at your source code.
Perhaps you are a human and it's just really good at reading human psychology
or it can actually scan your brain or something.
It's a predictor.
It shows you two boxes.
One box is clear.
You can see that within it is a thousand dollars.
The other box is opaque.
You don't know what's in that box.
Omega says to you, I have made a prediction.
If I predicted that you would open only the black box,
then you will only get what is in the black box.
And I put a million dollars in the black box.
If I predicted that you will open both boxes,
I put zero dollars in the black box.
Here's the two boxes, make your choice, and then Omega walks away.
So at this point, you can open just the black box,
in which case the thousand dollars in the clear box gets incinerated or something.
You don't get it.
And you get either a million dollars or zero dollars
based on whether Omega predicted that you would open only one box or not.
Or you can open both boxes.
In which case, you get the thousand dollars in the clear box,
and you get either zero dollars or a million dollars in the opaque box
based on whether Omega thought that you would open only one or both of them.
And the question is, what do you do now?
Someone who hasn't heard this problem before,
one of the common replies might be,
why do I trust Omega's judgment?
What is Omega doing that makes me think that there's anything to their prediction?
We can say that he's done this a lot of times in the past
and has always been right before.
Cool.
Or at least 99.9% of the time he's been right.
And so whether or not that's through, like you said,
brain scanning, I'm a super intelligence, I'm literally God.
It's the point is, is that...
He's pretty darn good at this.
For the postulate of the thought experiment,
are they all knowing or just really fucking good?
Just really fucking good.
Okay.
But good enough.
Basically, yeah.
Mistakes are extremely rare.
Extremely rare as in one in a hundred or one in a hundred thousand.
At least one in a thousand, maybe less.
Okay.
Then yeah, I'm a one boxer.
I don't...
Okay.
But I'm sure that there are some people who disagree with me
and maybe because I don't know anything about decision theory,
but I hear this and I'm like,
this predictor tells me that they scan my brain
and if I'm a one boxer, I get a million dollars.
Well, then by God, I'm a one boxer because I'd love a million dollars.
Right.
So...
And the people who disagree with that say that that is backwards thinking
because the choice you make now cannot affect things that happened in the past.
Like whether Omega put the money in the box or not.
But Omega's probably making the decision based on your reaction
after they told you what happened when you walked into the room or something, right?
Well, they know they put the money in the box before you showed up.
Right.
But they're telling you this saying,
I know how you'll act after I tell you this.
Yes.
Because they've already set the room up.
Yeah.
Yeah.
Yeah.
It's considered a difficult problem by many
because of that backwards causality thing,
even though most rationalists have the same opinion you did.
You know, if I only choose one box, I get a million dollars.
So fuck you.
I'm only choosing one box.
Marginal cost of taking two boxes is another thousand dollars on top of a million.
To me, it's not worth it.
Right.
Maybe we should do something like make it 100,000.
I think that's how it usually is.
Yeah, it is.
It is a thousand.
But like, then the idea is like, well, it's a bonus thousand.
And...
Who cares about a bonus thousand if you're getting a million?
Well, no, no.
But the idea, I think the point still stands.
But to me, I'm willing to gamble away that thousand for saying I have a one and...
Or I have a 999,999 chance out of 100 or out of a million or whatever
of getting a million dollars.
What if they were both one million then?
I see what you're saying.
It would have to be more in the black box.
Right.
Yeah.
Some amount to make it worth going for the black box.
So three million then.
Three million and one million.
You're guaranteed one million.
You might get four million or you take a chance to get either three million or zero.
Take a really good chance to get three million though, right?
So it is.
It is.
Yeah.
Supposedly a really good chance.
I would still be a one boxer in that situation.
Okay.
All right.
It is often been compared to not often, at least one time.
I saw it compared to...
Often.
Once.
I think it just stuck in my brain really strongly, which is why I was like,
it's been often kind of like, no, it's been often in my head.
There you go.
Been compared to Calvinism.
John Calvin, I think his first name was John, was...
They're all named John.
Yeah, right.
Was a theologian who believed in predestination, i.e. when you are created,
God knows whether you're going to heaven or to hell,
and there's nothing you can do to change that.
So the common reply to that is, well, if I'm already either saved or damned,
regardless of what I do, what is my motivation for not going out and,
you know, killing, stealing, raping, whatever I want to do,
because it doesn't change what happens to me in the end?
And his answer or the common answer is, well, if you were going to heaven,
you wouldn't do those things.
That you are demonstrating what sort of person you are by doing those things and
making it obvious to everyone else where you're going to.
This strikes me as a clear example of him having this predestination theory where,
of course, he's one of the chosen and so are all of his best friends who supported his religion.
Maybe, I mean...
But then this was like a post-hoc reply.
This doesn't sound like the kind of carefully thought out,
like it was all written down at once, then shown to people that way, right?
