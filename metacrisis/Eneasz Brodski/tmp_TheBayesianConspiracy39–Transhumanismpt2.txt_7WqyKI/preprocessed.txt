Welcome to the Basin Conspiracy, I'm Inyash Brotsky, I'm Stephen Zuber, and with us today
we have once again, Shelly, who is here to talk with us some more about the transhumanism
because we had to wrap up a little early last time, and also since we have been neglecting
listener feedback for a while, jumping into some listener feedback.
So let's start off with that, and then maybe we will edit this into the back part of the
show, or maybe we will just do things backwards today.
Backwards.
It's a crazy backwards day.
And the transhumanism is future, everyone will do everything backwards.
Oooh.
Have you guys seen Memento?
Yeah.
Love Memento.
Right after it came out, we could probably cut this out later, but right after Memento
came out, there was like this rash of everyone doing everything backwards, right?
There was a backwards Jerry Seinfeld episode, yeah, and all that.
There was.
I remember that episode.
Oh yeah, there was a bunch of things that just did things backwards, and it got to be
like, I knew someone was like, I am just sick, I think Memento's stupid because they did
backwards for no good reason, now everyone's doing things backwards, I was like, no, no.
Everyone else is doing it because it's a fact because Memento did it so well, but Memento
had to tell the story backwards because it's the only way you as the audience get the experience
of not being able to remember what just happened because you haven't seen it yet.
So you're going through it the same way he has, where you don't know what just happened.
I never actually saw Memento.
Oh my God, it's so good, I'll get around to it.
And it's like a circular storyline, so the beginning is the ending, which is why we're
telling it backwards also works very well.
Gotcha.
So at the end of the Seinfeld episode, it's like Flash's 10 years earlier, and all who
cares actually.
Okay.
Okay.
I'm doing listener feedback at the beginning, we'll not have that cool effect, just.
No, no, sadly not.
Unless we like, air it backwards, we'll have to play it backwards to listen, and since
no one has like a tape or like a button that does that, then it would just be some secret
encoded message.
Well, I mean.
And you can probably cut all of this, I'm probably.
Audacity can play backwards, like there's probably a fair bit of programs out there that can
play things backwards now.
Not like a podcast app though.
Maybe some iPhone one, I guess it's not really a useful feature.
People generally don't put backwards podcasts out yet, okay, so we have from based and confused
speaking about our recreational drugs episode.
That's an awesome name by the way.
Yes.
I heard a few times the implication that the frame of mind one inhabits during a trip is
somehow less useful than the one we have when we're sober.
And they tried to draw an analogy, imagine that the parts of your cognition are an orchestra.
Now at first you start using your brain as a baby or just banging around and making noise,
but eventually you settle onto a type of thinking that you could consider like classical orchestral
music.
And the intricacy of what you can perform there is incredible.
But when taking a psychedelic suddenly all that sheet music you've been using is switched
out for experimental jazz.
And the first time you try it's going to be extremely uncomfortable, but at the same
time there's going to be moments where you have this new interplay of sound that you
never tried before and will come up with revelations that are perfectly valid and completely rational
but would not have been available to you with your old way of thinking.
If now the orchestra that plays classical music as I function in my daily and rational
life reaches a problem that is causing me a lot of stress, I can access a whole nother
set of reasoning skills that are just as viable and logical, but from a very different angle.
And I think that is a very useful and incredible ability.
Did they say they used psychedelics?
Yeah, I assume so.
I can only assume so.
I would hope so.
I mean, my limited experience, my experience was not like that.
I imagine if you do it for an often enough, a few times a year, maybe after a few years
you can get to a place where you're still functional and just having a different set
of orchestra directions on your brain.
I think it's more like a kind of a face shift in those sorts of games where you can swap
the gravity or something.
Maybe you want to start with this one?
I feel like I have not experienced the higher doses.
I think what I do is about the same as what you did, Stephen.
So it's hard for me to say at those higher doses where people do talk about having some
thoughts that seem pretty irrational to me.
If that is something that you could just get used to and then start thinking logically
in those states too, but I don't know.
Were you unable to think things well in those states?
I don't know.
I don't feel like I was illogical or, like, you know how sometimes I'm like, I'm like,
sometimes when you're half, we're just waking up, your brain is still like pretty illogical
just in that moment and then you're like, what, why did I think that stupid thing?
I was like, still having a dream.
That's pretty much my only experience of lucid irrationality, I guess.
And it's hard for me to imagine being learning to think properly in that state either.
Maybe he has magic superpowers.
But trained superpowers.
I just read the rest of that comment and yeah, they said they'd used, they have a pattern
that they use that they're actually experts and they're not talking like speculating.
Okay.
That works for you.
Go for it.
None of us here have that kind of experience, so maybe it does take some training to get
into, maybe like transitioning to smooth jazz.
So I mean, the first time, you can't just play experimental jazz, not your first time
anyway.
It sounds like crap.
You can't.
But honestly, I can't play any instruments, so yeah, I could play the Clackety plastic
guitar.
I can blow a mean jug.
Can you really?
I mean, maybe?
How hard can it be?
I mean, if you want to blow a mean jug, it's probably pretty tough.
It fights back.
No, I actually meant it metaphorically.
I know.
So do I.
I don't know.
I'm running out of ways to chase this.
So I mean, I think that the general resistance to it though is that there's less, I mean,
so you're right now, anyone's lucid perception of reality is only taken in through incomplete
or otherwise transmitted and scrambled and re-unscramble sense data, right?
Like you don't proceed the world directly.
You get photons bouncing off the walls to your eyes and that's all processed to paint
this picture in your head that's somewhat close to how things actually look.
Otherwise, you know, we'd be walking into stuff all the time, but it's not perfect.
I don't think that using psychedelics makes your, maybe the people who speak about it
like inhibiting you as a rational thinker or something, talk about it specifically like
in the empiricist sense that your, especially your senses of like sight and hearing are
radically altered if you're on psychedelics.
If you're an expert, it's different, but for the amateur, my perceptions of like sense
data were not at like stop representing reality in any meaningful way.
So I think that's what people talk about like when you're not being like it's not productive.
But I mean, I guess I'm not sure productive is the right word.
I want to argue with you because I was sitting you at the time and you were basically functional.
You could walk around and not fall into things.
You knew that, you know, what you were seeing was an artifact of the drug and you could talk
just fine.
You seemed like a normal person having a very interesting journey as opposed to my senses
are not working.
Hey, I took a modest dose and B, I think a lot of that came through from me pretending
to feel like, look like I don't know what I'm doing a lot of the time, like when I'm
not high.
So I think I just translate over really well.
I mean, so yeah, from the outside, I did know that things like I didn't lose the knowledge
that I'd taken something that was altering my perceptions.
If I'd taken three times what I did, I variable may have lost that that knowledge.
And so I would just be sitting there having these great experiences and I wouldn't again,
maybe if I practice, I want to keep caveatting.
But that would be it.
That would have been a very different state of affairs for me.
I'd be watching the walls moving and, you know, everything vibrating or whatever and
have really no idea that it was unusual or what was going on.
There were times where I tried to like break down that layer of perception when I was,
you know, having that day.
But I get this sense that it would have been more complete had I been able to get higher
on that trip.
Do you think it would have shut down your brain that much?
Yes.
Okay.
I can only speculate that from like the glimpses that I had and like report the experience
from other people.
Probably this one's aimed at you.
Sorry am I posted.
Someone said something like, I don't want to be more open.
I know it's easy to think about this while when I'm not in the middle of the conversation,
but I really wish someone would have asked, how come?
How come?
Oh, well, I'm just like, in general, a picky person, I like being picky, I want to be more
picky.
You want to be more picky?
Yeah.
Okay.
I generally more enjoy and also more improve of the idea of wanting something first and
then getting it and then liking it rather than being like, oh, some random thing happened
and it wasn't what I've always dreamed of, but it was cool.
Like, I mean, that's all right, but I much rather be like goal directed and joyer.
I think I can back you up there.
I like it when things that I like happen by accident, but I mean, the alternative is
just like throwing the dice and having a bunch of bad experiences and occasionally landing
on one.
Like, yeah, I guess I could do that again.
Yeah.
And I don't, like I value information because it's useful, but I don't really value just
experience for the sake of experience.
And I don't, like if I had to choose to go in one direction, I would go more in the direction
of being even more picky than I am.
I admit that there's such a thing as too picky, but I'm not anywhere close to that for
my tastes.
So, yeah, I think that being more picky is sometimes a really good thing, especially
if you tend to make dumb choices a lot, which you don't, but there are people who do.
But I really, when you, Steven, you said just like throwing random dice and most of the
time you get crap and every now and then you get something good, I disagree.
I mean, yes, you do most of the time get crap, but the fact is, if you didn't throw those
dice all those times, you wouldn't find those few good things every now and then.
And I've had just like the most random acts of serendipity where like, I do not know what
I want to do for this scene.
And then all of a sudden like, I hear something on the radio and I'm like, that's perfect.
It's going right there.
And that's great.
And I mean, those things happen.
I think maybe what I'm, assuming that Shelley and I are on the same page, I think what
I'm trying to get at anyway is that like, I prefer to do that like in a controlled way.
Like I found my favorite comedian through a random shuffle on Pandora.
Okay.
And so like that kind of thing is great.
But I mean.
You have to go through a lot of crap before you find the few you like.
Yeah.
And like, I don't like onions unless they're like, you know, prepared in like certain
ways, but like just like big slices of onions on sandwiches or something.
I hate that.
Yeah.
I didn't like that the last 10 times.
I won't like that tomorrow.
Right.
So like, I think that level of picky, like I'm just saying.
That's not really randomness though.
That's always an onion.
That's true.
So I guess I'm, I'm just defending general pickiness there.
Okay.
Also, if you actually were open to everything and we're cool and liked everything, that
would be almost just like the other end of being wireheaded and just loving everything
that you experienced.
Like if you have no preferences, then everything is just blah.
It's just a sea of neutral in a way.
I think there's a difference between being open to trying stuff and, you know, being
and not having preferences.
Right.
Yeah.
Like you can, you can try a bunch of things and hate most of them.
And like that's still a preference.
You're just still trying new stuff.
Yeah.
I'm just like, why would I waste the time?
I hate doing things.
Right.
Things and stuff.
That's right.
But in the specific context that we're talking about is about how people who do mushrooms
in particular was the study.
After doing psilocybin one time, they permanently became more open to experience, which is one
of those major personality measurements that exist.
So I'm not super familiar with that, like how you measure that, but if I had to imagine
what it's like to be more open to experience versus less, is it just like a greater risk
taking?
Like this might suck, but I'm willing to find out.
Is that what that is?
I would assume so.
It's more like a, let's see what happens.
More exploration, less exploitation.
I mean, that might be part of it.
I think the only way that, I mean, here I am trying to think of years ago, remembering
reading stuff and psych, I think most of that's like self-report.
Like I tend to try new things.
I like to go out, you know, do stuff like that's like, like those are the sort of questions
they ask you on a personality assessment.
So I guess that would be more of a, are you driven to explore things by yourself normally?
Cause like I tend to go out is a directed activity.
Right.
It's not like just I'm open to things randomly happening to me.
It's I enjoy going out and seeing if anything is different tonight.
I can't remember any of the specific questions.
I think the things like that, and they might cover both of those kinds of things, but I'm
not sure.
Cause it seems like you could just sum that up as being willing to take risks.
It's like, but what's the difference between nothing that I can think of, but then it wouldn't
spell ocean as a good acronym.
So like willingness to take risks if you replace the O, yeah.
Or like willingness to take risks in the particular domain of experiences, but I mean, everything's
experience.
Yeah, but there's a difference between being willing to try new experiences and being willing
to, I don't know, rush some, what is it, what is a risky activity driving really fast?
That is, that is very risk tolerant behavior or like going to a new club or something.
I don't know.
Yeah.
I guess the new club would be more of a openness thing, whereas a driving recklessly and fast
is more of a just not caring about risks.
Oh, sure.
Yeah.
That's a fair point.
Also, I dropped an acronym.
Ocean is spelled like it sounds.
It's the acronym for the big five personality traits that are easily Googleable.
Do you remember what they are offhand?
Openness, conscientiousness, extroversion, agreeableness and eroticism.
What's the N?
Eroticism.
Neuroticism.
Yeah.
Eroticism is one of the big five.
We can just drop a, what was the E, I can't remember.
Extroversion.
Yeah, we'll drop that and put eroticism.
Yeah.
The new big five, 2.0.
Better big five.
But you know, I am actually very open to experience when it comes to food.
I always want to try new foods, and yet I still have strong preferences regarding food.
It's weird.
I don't have very strong preferences regarding food, and I'm also not very open about new
food experiences.
I'm like, just give me the same thing I've always had, please.
It's worked well so far.
Interesting.
I'm kind of more in that camp too.
And it gets weird.
I rarely regret trying new things, even like if I hated it, I'm like, well, okay, I tried
that and knocked it off the list, but I hate the thought of doing new things.
Does that make any sense?
I don't think I'm quite lined up there.
My wire is across somewhere.
You don't really want to, but after you do it, you're kind of cool with it.
I'm kind of like that with literally everything.
I keep on starting a new TV show.
I'm like, all these new character names and plot stuff, and then I watched it.
I'm like, oh, I don't regret doing that for the most part.
Neophobia.
Yes.
Well, hopefully not super phobic, but neoversion.
This is not really a comment, but I wanted to bring up that after we recorded that episode
at Slate Star Codex, there was a blog post put up commenting on the weird inability of
the pharmaceutical industry to discover anything very groundbreaking over the last several
decades.
They just have little minor improvements here and there to the normal stuff.
The really big groundbreaking discoveries, the discovery that MDMA is good for treating
PTSD and ketamine, injections are worked really well for depression, were both found
out by random druggies using things to get high and just pointing out that maybe recreational
drug use has side effects in the positive benefits in the whole helping to treat mental
illness just through a bunch of people experimenting on themselves randomly, sort of thing.
Because of the pharmaceutical industry spent this much time and billions of dollars of
effort and can't find anything, but random hippies trying to get high finds it.
Maybe that's a good benefit to society.
It provides an incentive to explore if you get high.
Yeah.
The history of science has a great history of people who put their necks on the line
for science to see what happens if this happens.
This should work and then putting everything on the line to test it.
This is a little less intense than that, but I guess I don't know if I'm all for it.
That's probably too strong, but I have a soft spot for self-driven science, although be
careful of doing science in yourself.
Let's just...
Yeah.
It's a very inspirational story that Benjamin Franklin was like, hey, I think lightning
is electricity.
Let's see what happens if I fly a metal kite up there, but apparently one or two people
every year die trying to recreate that.
No kidding.
Yeah.
Probably a bad idea.
He got lucky.
He did get very lucky.
I think he also had some moderately okay precautions, but also very lucky.
That's amazing.
I didn't know people died still doing it.
What are they thinking?
Well, they're thinking this was a really awesome thing that Benjamin Franklin did, but I can
do it too.
There's a good Skeptoid episode on weird shit people do to themselves in the history of
science and some of them are way more dangerous and stupid than what Franklin did.
None of me is like, let's try that out.
They did it and they turned out okay.
The closest I might do just to see if I could attract a lightning bolt, I might tie something
on a kite and fly it in a storm, but then I would tape it and then leave and come back
and get the camera.
If the camera is blown up, I'm like, oh, I sure dodged a bullet there.
I have heard, I put a reasonably high amount of credence in this story that all of our
artificial sweeteners that we know of today, not like natural ones like Stevie or whatever,
but artificial sweeteners that are created in the lab were discovered by people having
poor lab safety practices and accidentally tasting the chemicals and be like, hey, this
is sweet.
Awesome.
A little terrifying if you work in a lab, I imagine.
I, Mr. Oliva, again, staying on the recreational drugs theme, apparently people were interested
in drugs.
Who would have thought?
Mr. Oliva says, I've always thought of music as a low key drug.
It's a way to make physical stimulus and use it to create a mental change or to take physical
stimulus and use it to create a mental change.
And I think I agree to that to some extent, but then Kelty Booty too, no, sorry, Kelty
Booty, too, says, yeah, I recently came to this conclusion.
I'm also worried.
What effects does music have in the long term?
Whomp.
You are the one person saving his brain, Stephen.
I listen to music.
No, you don't.
My model of view is forever the person who does not like music.
I came out less enthusiastic than the average person about music wants to, and he has in
his head like, you hate music, right?
You're like the Grinch of music.
That's right.
That's all the worst.
That's my real opinion.
I see the general thrust of what Mr. Oliva was getting at, but I sort of, I think we
talked about this.
There's that one word for drug that encompasses everything, apparently from music to like,
you know, 600 micrograms of LSD.
And that's just too broad, right?
So I mean, it's.
And if we want to stick with things that chemically affect your brain?
Well, I mean, at least, I don't know, it seems like having that one word to map onto all
those things just makes it kind of pointless, but I just, like the general point is there.
We use more extreme examples of like skydiving and people talk about like running and runner's
highs and, you know, you know, dwarfens are a thing that people get when they work out
and stuff.
So those are all like drugs in that same category.
It's an interesting point because yeah, when I think of drugs, I think of like chemicals
that you ingest to alter your biology, but a lot of people do say like, you know, endorphins
and adrenaline, those are both chemicals, just your own body produced them, right?
And people do BDSM specifically to get into an altered state, like I'm not going to say
everyone, of course, not everyone does that for that reason, but certain people do it
to get into the altered state.
Well, the altered state of.
For some people it's induced by pain or it's like a floaty, druggy state of being.
Sure.
Okay.
Yeah, I can see that.
Cause like, I mean, a lot of us do a lot of sex acts because they put in us in a state
of mind that we enjoy, right?
Yeah.
Yeah.
I guess they're.
There are a lot of old traditions of ex ex inflicting a lot of like non permanent damage
on yourself, just inflict pain in order to get like a spirit journey or whatever it is.
Yeah.
I don't know how self-flagellation became popular.
Yeah.
Yeah.
And some cultures have those like intense rituals like hanging yourself in hooks.
Yeah.
And I can imagine there's a lot of things going on there.
Just the, the knowing that you can get through the pain will probably gives you a great sense
of, you know, strength and, and resilience.
But also it probably does put a lot of people into this trans like spiritual feeling state.
This one I think was actually a good one to, to write about because we may have given
the wrong impression.
Not without incident says, I was really surprised to hear you all shitting on 12 step programs
for treating alcoholism just because people can moderate their drinking on their own.
Project match has been thoroughly discredited and more genetic contributions are contributors
are being discovered all the time.
We know now that some people respond better to different treatments and will have a very
hard time stopping without help.
I'm in no way trying to play down what Enias and others have accomplished by quitting smoking,
excessive drinking or other drugs.
It's super impressive.
And I would never suggest it was easy for you because of your genetics.
I just got a bit of anyone can quit drinking without therapeutic or pharmaceutical help
vibe.
And if that, and that was the argument I'd like to push back strongly.
And yes, I also, after he put it that way, I kind of felt like a huge elitist jerk because
obviously there's a lot of genetic contribute contributions and also just sociological,
you know, what support nets you have.
Oh, I, I really strongly do believe that a lot of people need treatment.
I just don't think 12 step is likely to be that great of a treatment for most people.
Yeah, I think I can dig up something on this and I will post it either way.
I remember either reading that the 12 step program as just as like the one they, what
is it, AA is less effective than similar programs that are like less religious.
Yes.
So it is super effective for a very specific type of mental architecture.
And for those people, it works wonders and they get really like deeply passionate about
it, but for most people, it is not the best way to quit at all.
Yeah.
So in that way, I'm, I'm against AA as like a required mandate.
Yeah, I'd be much prefer like a judge said, you have to go to some sort of program and
you'll figure it out.
Or the default was kind of harmful.
Yeah.
That's why I have a hard spot, I guess for AA, but that said, I, if we came off as anti
program for people, I'd certainly hope that wasn't a, or I'd like at least to correct
the record on that, that that's not, I think are any of our opinions.
If you need a program, you know, whatever works for you, go nuts.
And I will say that I believe 12 step does better than no treatment at all.
Like if it's a choice between those two, 12 steps is probably going to be the
winner, but compared to other treatments, it's hard to, to have, to find any evidence
that says that the improvements are due to the specific things that AA, you know,
the specific details of AA, rather than just that you're doing something.
Hey, I heard that for most, most types of mental architecture, it's not really that
much better than doing nothing at all.
And with AA, since they are both so overtly and strongly Christian or, or they
pretend they're not Christian and they're just some deity out there, but they're,
they're at least very strongly religious.
And most people can see, yes, it's Christian.
I sort of have an issue of that with that specifically.
I'm like, it seems like you are simply replacing the dependence on drugs with
dependence on your religion.
That the whole, I am worthless and I cannot do anything myself and I must give
up everything over to the higher power and they can save me is, I don't think very
healthy, again, for most people, for some people, it works great.
Yeah.
And even if it's not the most healthy thing, it's probably healthier than alcoholism.
So like, if that's what I'm just saying, if that's what helps you, then go for it.
You're right.
It is a specific kind of person would respond to that.
But that having been said, I, I take antidepressants and I have, you know, run
into some people that are like, stop taking antidepressants.
All you got to do is exercise and think good about yourself and that kind of
stuff.
I'm like, fuck you.
So there's, yeah, there is definitely both pharmaceutical and therapeutic
treatments that help people and that are very important.
And I don't want to say to anyone like, Oh, you just aren't, you know, pulling
yourself by your up by your bootstraps or anything.
As I, I did not mean to come across as giving that impression.
Those things are very valuable.
It's just that you don't always need them necessarily.
And I've heard about half of people end up not using them at all.
But then there's also, you know, the other half and the fact that it can help
people, even if they don't use them that much.
Many people, I was surprised by this, so I'm not going to quote any one person,
but there were a lot of people that wrote in to say basically along the line,
things along the lines of, uh, I have come to accept that I cannot really
perceive what's truly real.
And I recognize that my perceptions are just, uh, the only thing I have to go on.
I wasn't sure exactly how to reply to that or if we even want to get into it.
I think it could be.
I mean, is that like a whole, what's the question?
Like, I'm not being facetious.
Like what, what exactly is the topic that I'd dive into?
Like, yes, our perceptions aren't perfect.
But like I said, we're, they're not so wrong that we're constantly being hit by
cars, right?
So, but I've heard, I've heard some people say things along the lines of they're
so wrong that we have no idea at all what's out there.
Well, those people, I mean, throw something at their head and watch them dodge it.
Right.
Right.
Like, oh, you have something that is very, that is a very evolutionarily adaptive.
I can make it so that this cluster of atoms doesn't get hit in the head by things,
but are atoms even real and what's going on?
I don't know.
I don't know what to make of those sorts of arguments.
Like, obviously there's something that we can out there that we are interacting with.
And even if it's not the deep quantum state that everything runs on, it's there.
It's still a thing.
Yeah, I'm not super convinced by people who say things like that.
Like, I mean, they don't something, something making beliefs, pay rent,
something, something like, what, what are they possibly believing that has any
impact in the world with that?
Like, they, they could say these things, but they don't really do anything.
Do we want to throw open like a, if anyone wants to come on the episode and talk
to us about this particular belief thing?
Or at least, at the very least, right in a coherent version of, I'm not trying
to be an asshole, I just don't really understand what a steel man version of
that position looks like.
Do you have any?
I know, I don't.
Um, but maybe they're just pointing out the fact that it's not 100 percent.
Like there's always that slim infinitesimal chance that you're, you're totally
being deluded, but it is slim.
Yeah, I, that's probably, I mean, I think there's too many versions of this
argument to have some, have one response, like hit all of them.
But yeah, that's part of it.
That, you know, what if everything's like this, right?
Yeah, like what if you're a bullets in the brain?
Yeah.
Then like the answer to that is 1% doesn't mean that, like a 1% chance
something doesn't mean that it's reasonable to believe that thing or even
that it's permissible if you're trying to be rationally consistent to believe
that thing, right?
I mean, most UFOs that people see in the, in the night sky are, you know,
airplanes or shooting stars or something.
Um, I'm prepared to say all of them, but you know, since I can't prove that
whatever, uh, if I saw, you know, five planes fly by and then I see another one,
I'm like, I'll bet that one's a UFO.
Well, you can't prove that it's not.
There's, there's a non-zero chance, right?
Am I at all reasonable to believe that?
No, there's so much, uh, there's so many, so much burden of proof.
And there's so, that's such a grand claim to make that it just isn't
reasonable to put that forward.
Well, but when you're requesting the whole concept of proof at all, then
think things get a little shakier.
But what my response to that is just like, you have two choices, do the
best you can or give up on life, right?
And that's what most of the comments ended up saying is like, and I just
do the best I can, but I don't know.
I think back to Eleazar's post about, uh, having beliefs that are entangled
with reality, that when I believe that my shoelaces are untied, it's
because there's a physical thing down there, which a photon came from the
sun and interacted with it and then came shot off into my eyes and it
interacted with my eyes and those interacted by setting neurons, firing
into my brain.
And so there's this whole chain of causal events that is, you know, entangled
with the physical world around me.
So even if I'm not seeing the original quantum flux of things that are happening,
it's still a chain of events that is entangled with what is out there.
I think the, the extreme version of this question is that entanglement that like
there are no, I mean, could be like, are your shoes untied in the matrix?
Like yes and no, depending on how you want to answer that question.
So, I mean, again, I sort of find most of that just sort of uninteresting.
Not, again, not being mean, I'm not being dismissive, but that's what you
think and you're being chill about it.
That's cool.
I just, I don't find it, I don't know.
I think I went briefly through this period where I was thinking about that sort
of thing and then I just kind of was, I remember having this thing when I was a
teenager, I was trying to think of like, how do you persuade somebody that logic
and consistency matter, if they're doubting that logic and consistency matter?
And then I eventually realized, I think years after I like, just give up on
thinking, maybe months after I start thinking about that, that I was like, you
know what, there's nobody who actually operates that way.
Like, so that was, that was my feeling is I was trying to answer the wrong question.
I should have asked, do these people actually exist?
No one thinks that logic and, and consistency don't matter.
Cause like, they still look both ways of crossing the street, even if they
don't think that, you know, things are real.
So they, they're not, they're not being consistent if they're saying these things.
So that's why I think it's just these empty hanging beliefs that might have been
a bit rambly.
So going off the topic of recreational drugs and stepping back one to the
non new, new, new tropics, how do you pronounce it again?
New tropics, new tropics, new tropics.
Okay.
Gadbeb, I guess, I'm not sure how to pronounce the name, uh, on the forums at
slash R on Reddit slash R slash the basing conspiracy says, I figured some
folks might be interested in some information I've come across in my
further reading on Midefinal.
Here are the most common side effects, according to the British national
formulary, formulary, do you guys know what a formulary is?
Sounds, it sounds official in British.
So the British national formulary, uh, common or very common signed effects
include things like abdominal pain, anxiety, appetite changes, uh, constipation,
depression, dry mouth, nausea, sleep disturbances.
There's a number there.
Obviously sleep, sleep disturbances.
That's the whole point of the thing.
Headaches.
There's a list of, of, uh, common side effects.
And then there were some uncommon side effects listed too.
And, uh, it says the common ones occur in one out of 10 and between one and 10 and
100 administrations of the drug.
So, uh, I just wanted to put that out there, that there has been at least
some research on the topic of the side effects of Midefinal and there's
some data out there so that people will know, I guess, sort of what they're
getting into.
Have you ever experienced any of these?
Um, trouble sleeping, maybe, but I mean, even after like you come down, you're
not like, it's not really a come down effect.
It's just like your sleep might be a little less settled that night.
Well, I like it because it's like more vivid dreams, maybe, but, um, I just, when
I read this, I couldn't help but like be an asshole and, uh, just think about like
that, that, so the list, you know, I should only like a bridge to the list of
side effects from it.
I mean, here's the list of side effects from Bayer brand aspirin, rash, gastrointestinal
ulcerations, abdominal pain, upset stomach, heartburn, drowsiness, headache,
cramping, nausea, gastritis and bleeding and, uh, black, bloody or tarry stools.
These are the rare ones, but he listed common then rare or they did.
Uh, coughing up blood or vomit that looks like coffee grounds, severe, nausea,
vomiting, stomach pain, fever lasting longer than three days, swelling, pain
lasting longer than 10 days, heart problems ringing in your ears, upset stomach,
heartburn, drowsiness, et cetera.
Um, so I think I just, I, I found that list of, of side effects, super
uncompelling because that, I mean, if, if, if you're going to say a drug is safe,
it's aspirin for the most part, right?
When you use responsibly, that's sort of how I feel about.
Modafinil that, yeah, the list of side effects are scary, but like some of
those side effects for aspirin are horrifying, you know, coughing up things
that look like coffee grounds, that sounds like you're dying and you probably
are, but that's super, super rare, right?
So, I mean, these are ones that are supposedly common, but on the other hand,
a lot of them seem like things that are sort of psychosomatic, abdominal pain,
dry mouth, headaches, that kind of stuff.
They, they are things that I would be willing to attribute to other stuff
because they're so wide ranging.
It's not like consistently people who, who take this always get nausea.
It's just a large drowsiness.
Drowsiness is a side effect of Modafinil.
I think everything that keeps you awake can also, and some people make you
sleepy and vice versa.
Everything that makes you sleepy can, and some people make you unable to sleep.
The one thing, my partner's mom has that reaction, the opposite reaction to
like a, whatever that dryness agent in Benadryl is, it wires her up like coffee.
Some people are backwards, but like, like I said, half of those common ones
overlap with the, the common side effects from aspirin.
Yeah.
So I just, I mean, grain of salt.
The one thing he did say that I thought may be worth noting, and actually
he used the words worth noting, it's worth noting that it seems to reduce
the effectiveness of oral contraceptives, which is a kind of a big deal.
That is a big deal.
And I didn't know that.
And then actually I'm glad I was pointed out and that should be broadcast or at
least looked into and then broadcast.
This was it.
I think this was on the same episode still where we were talking about, uh,
making people crap.
Now I'm trying to get, getting our episodes mixed up, but, um, the, when
were we talking about, uh, raising people to new levels of doing better?
Uh, Mr.
Alivar says the marginal utility of rising to average is much higher in most
cases than the marginal utility of becoming better than average.
It's a difference between being independent and being very good at what you do.
What do you guys think about that?
Given that society is probably set up for the average case, you probably
are going to drop out of just standard functioning supported by society.
If you're below average, right?
Yeah.
So, yeah.
So on the personal level, probably there is huge marginal utility, but to be
able to get up to average, which is why people strongly support things to bring
people up to average, but not so strongly or support them to make them
better than average.
But that's, I think like Shelly said, that that's, that's just a by-product
of society being the way that it is, uh, that if like being below average is
punished less or being above average is rewarded more, or there was just like
less pressure by the way everything's built up.
Like you need to have some level of, I mean, some close to average level of
cognition, just to function in society, like to manage your budget or like go
shopping, those sorts of like, you know, day to day things and a million other
things, those are two really lame examples.
Yeah.
I think once that's taken care of though, then like getting bigger and
better is the logical path forward and desirable path forward.
Yeah.
Um, I would say that this really comes out when you talk about disability,
because like society is set up for people with certain mobility capabilities.
And to drop below that would be because of the way society is set up a really
big impact on your life.
Whereas if you can actually walk faster, right?
Or you're really good runner studies, not really going to reward you for that
unless you're like an Olympian or something.
That's a really good point.
Like if I couldn't walk, being able to walk would be a much bigger improvement
than being able to walk three times as fast and three times as long as I currently
can.
Yeah.
So yeah, I basically agree absolutely on the individual level, but I kind of
cheated on this by thinking about it a bit before I got here since I knew I was
going to read it.
And, uh, I think that on the societal level, it maybe flips because on society,
society as a whole can support a certain amount of people who are below average.
And it's not that big a deal.
It doesn't direct society down that much, but the real progress that pushes us
forward is done by the outliers, by like the super smart or super creative people
that come up with the new drugs or the, the, the Maxwell's equations governing
electromagnetism or something like that, that the, the frontiers that make us
better and, and push us to transhumanism are the really smart people.
So maybe it would be the other way around where it's more important to focus
on that high end and try to get people over into that higher edge, not for
anyone's individual good, but for the aggregate societal development.
I was going to say, as long as you don't like leave people behind, but then I
realized that that doesn't make any sense.
Like if, you know, if we could, if we could bring on the advent of, you know,
like a safe universe a hundred years quicker, if we lost a billion people now,
even though like I'm, this is not something I'd advocate.
That's the worst way to do it, but if that happened, it would be hard to say
that that was the wrong thing given how much suffering you prevented, right?
My, my inner utilitarian caught up with me before I could counter what you said.
I also think that the fact that standout individuals can advance science so
much is a artifact of the, the reality that we have science.
Yeah.
We have an in scientific institution.
We have the infrastructure of society.
Like could we, could we do any, I'm going to bring it back to agriculture, right?
We have, we have agriculture and we have like metal mines and you know,
all of that, those little details are what allow these intellectual leaps to come
to anything.
Imagine someone way back when he was a genius and like, what are they really
going to be able to, what kind of, what kind of advances or can they achieve?
So you're saying that the, the benefit to pushing above and beyond like the
average is only beneficial because like there's already an infrastructure in
place to make that useful.
Yeah, there's, there's an engine that can make use of it.
And if you were to lose, if society were to collapse, would, would 10 geniuses
be able to, oh God, no, no, the, the, the infrastructure is absolutely vital.
But on the other hand, I don't like what I'm about to say because it feeds
into the whole capitalist narrative of humans are just units of production and
are interchangeable, but if, if the whole point of institutions is that they
keep going on when the humans inside of them retire or die or whatever.
So humans are somewhat replaceable when it comes to things like working the fields
or working the mines.
One, one guy is just in terms of economic output, almost as good as any other.
Whereas it's very hard to replace someone like a Stephen Hawking or well,
not, you know, someone along on the leading edge right now.
It's, you can replace them, but there's a much smaller pool to choose from.
I also don't see Shelly's point as like necessarily a counterpoint to the
general position.
It's just that it's in contingency that's worth keeping in mind, but I don't
see it as like an argument against it.
Maybe it's not like a total argument against it, but just like the fact we
need, we need that, the average people in order for the, for the next 20 years
till robots are doing all these jobs.
So like you mentioned, you know, the average guy is, you know, can pick fruit
and dig, dig minds better than, you know, the same as the other average guy.
But, you know, we're about to be pacing us in that in the next few years.
And then, then.
And that's not to devalue the average guy.
I said before that every hero needs an entire support structure behind them.
Without, without the people there making the parts for his X-Wing, Luke is just a
guy swinging his dad's lightsaber around on a farm.
But it's also worth pointing out that like your self worth doesn't tie it up
like in your production to capitalism or to like any job, right?
It's easy to, having come from six months of unemployment, I can totally
relate that like, God, I feel worthless right now.
And I would like to think that if my jobs were, like if I lost my job to
automation, I would, you know, have, find some, and I, but I was still collecting
paycheck or something I could find meaning and I probably could if I wasn't
so worried about paying bills and stuff.
So that's sort of an aside, but I didn't, I think I earlier, I made it sound like
I was tying up people's self worth with their production and I didn't mean to do
that.
So, uh, I'm just going to go with M because emails are not public and I don't
know if this person wants their name revealed.
Uh, M says, it seems to me that the most effective cognitive booster is sleeping
regular hours and exercise, that unless you're already doing that and really
need more, it's a bad idea to move on to untested drugs.
Someone mentioned that exercise was too time consuming, but I feel like they
weren't considering the additional health benefits.
Maybe if you're an astronaut, nootropics are a good idea since you got limited
time in, in space, a limited time in space for your science stuff and you're
already operating on the edge of efficiency.
And, uh, yeah, I, I think we did bring that up a few times that really the best
by far things you can do is get enough sleep and, uh, get exercise.
Those just have such ridiculously outsized effects that it's hard to compare
them to anything else, but nootropics really are, are sometimes you don't get
enough sleep and you don't get enough exercise and there, there are a way to
cheat.
Yeah.
I mean, I don't know a lot of people who sleep and exercise and still drink
coffee and coffee is nootropic by any measure.
You make a good point.
Like, and there's all kinds of other benefits to exercising.
I don't think anyone's shitting on exercising.
It is time consuming, but like, it's not just for the alertness factor.
You have all the, the awesome side effects of like living longer and feeling
better all the time, like those things are, make it worth the time probably.
Looking a bit better too.
Oh yeah.
It's a nice side effect.
All right.
I only have two things left.
Let's go with the one from our, this was also from a private channel.
So won't say their full name.
I understand it's hard to keep up on topic with podcasting, airing delay, but
getting back to repugn conclusion.
How long ago's we talked about the Republican?
Really quick.
I think this is an internet handle and this person was a Patreon supporter.
Okay.
So I think we can say that their, their handle was Roman.
Okay.
I don't know if they, I mean, that's not a whole lot of self identifying
information, but it's there.
Thanks, Roman.
We appreciate your contribution and we're reading your question on the air.
Yay.
I already had an argument with a friend about this, but it seems like a
simplification error.
You simplify the real world too much in order to draw this conclusion, even if it
seems perfectly reasonable, it doesn't mean that it can be applied to the real
world.
In reality, you can't have a society where everyone is of equal happiness, nor
can you simply tell some, nor can you simply tell that someone is happy
enough if he hasn't killed himself.
Which is true.
Not only this person might be counted as unhappy, but he can also make a whole
bunch of other people miserable.
I've personally had the misfortune of encountering some pretty bad people and
can't help but hold a very strong belief that if those people were to
disappear, humanity's total utility would go up.
And I think it actually has a really good point about that because it is a bit
oversimplified to get to the point like where everyone is just above the level
of having a life worth living and then you introduce another person and they
will bring things down because happiness is not necessarily tied to how the
resources are distributed.
And people do vary greatly in their happiness.
There's probably people right now that would fall far below that line of
wanting to keep existing necessarily.
And so, yeah, to make it all flat and consider everyone even like that, I
think, makes the thought experiment a bit too simple, like thinking of a
spherical cow.
So I haven't actually read Derek Parfit's whole version of that, but I can only
imagine that the comeback to that would be something like, well, you take an average.
Right?
An average is just a nice point.
But I don't think the point of the thought experiment was to get to the
point where adding just a few more people makes everyone much more miserable.
And so that is why we shouldn't add more people in the more extreme conclusion
when there's only like a million people around.
Are you very familiar with Parfit's repugnant conclusion?
I'm not like an expert on it.
But I would say that one thing that you just said, Enya, that resources don't
directly translate to happiness is important when you think about actually
applying this to the real world.
Because real human happiness actually is affected by the number of people around them,
the number of people they get to interact with, the number feeling like they're lost
in the crowd and feeling like there's so many of us people that we're not noticed by maybe
some people who are more powerful or whatever.
I think numbers of communities and numbers of people that you are expected to take into
account, but maybe you can't take that many people into account directly does affect human
happiness in a way that doesn't directly translate to resources, as you said.
And therefore, given that it's so, there's such this feedback loop going on,
how could you even calculate it if you try to make a practical implementation of this?
I think basically that last sentence, how can you calculate it if you're trying to make an
implementation of this, is like you come back to like 50, 60% of all moral thought experiments.
But I think the point, it's less of this is an actualizable thing that we're trying to work
towards and more of like an intuition pump. And I think the point of Parfitt's analysis of this
is that like, no matter what position you take, none of them work out well. So clearly we don't
have a great picture of what we actually want and are trying to do. So I think it's less about
like trying to prove this point that this is, I guess what I'm trying to say is that it is overly
oversimplified and confusing, but even the simplified version draws up these contradictions
from like what our preferences are and what we're working towards. And so that's more what it's
about. It's just that like, we don't have a coherent extrapolated volition when it comes to
like happiness levels in population. Well, I agree about that. But I think
that the thought experiment is so, it has to map onto something that to make it make sense.
It's not mapping onto anything. It's like the trolley problem.
Well, it is just trying to say like, if you had to choose between A and B, which would you choose
to let you investigate what you really care about more A or B? Even though in the real world,
you probably won't have to choose between A and B.
Right. Well, I mean, I think that makes total sense for things like the trolley problem where it
actually maps onto things in reality. But with the repugnant conclusion, like if one more person
were to be born today, would they bring down or increase the total happiness? I mean, maybe...
Depends on how happy they are.
Exactly. So that has absolutely nothing to do with how many resources. Well, not absolutely
nothing, but they... Right. It depends on how happy they are. If we're right at that level of
subsistence, you introduce one more person, maybe they'll increase the happiness level,
because they'll be more happy than people in general. Or maybe right now where we have,
I will go out on a limb and say, a high amount of overall happiness in society,
you introduce another person and they may drag down things instead of...
You have a low amount of happiness in society.
You think? Oh, I think it depends on how you phrase the question. Like the most plentiful
countries tend to be the highest suicide rates, which is counterintuitive. But I mean,
we are enjoying pleasures that our ancestors, they spent 16 hours a day tracking down lunch
and dinner and hiding from tigers and shit. And we're... Our downside is like,
ah, my TV show got canceled. That is such an awesome problem to have when our ancestors were
scratching the dirt trying not to die. But also they just didn't have time to think about whether
their lives were a good thing to continue living or not, right? They're so busy, they didn't
think about it from the existential perspective. And now all this free time means that
we have time to reflect on that and come to the answer that like, oh, shit, it's pointless.
So I feel sorry for them because they didn't realize. Oh man, I'm glad my ancestors didn't die.
At least, you know, not before they did anyway. So most of my ancestors are dead.
I think I might have pointed this out when we were talking about repugnant conclusion,
it's been a while. But one of my favorite people, Will McCaskill, was on Sam Harris's podcast
months ago, maybe a year ago. And he talked about this for 10, 15 minutes, and it was awesome.
And so unless you want to dig up Parfait's essays, maybe Roman read them and found them
uncompelling. And if that's the case, I want to come back to this once I've had a chance to get
into it more, and I will. You know what? Next episode we do, do you want to do the repugnant
conclusion and economics, like specifically how the capitalist system treats people right now?
Because I've recently been reading up, not reading up on, I've been recently reading a blog that
is making a very interesting argument, which has been changing my mind quite a bit over the last
month. And I would like to talk about this. Yeah, well, your market, that sounds fun.
Okay, cool. That said, I'm plugging Will McCaskill's presence on Sam Harris's podcast,
Waking Up is the name of the show. Okay, I would check it out. Even whether or not you care about
Parfait's repugnant conclusion, it was a great interview slash discussion. I will listen to that.
And I do want to say that the concept of a practical implementation that takes into
account their pregnant conclusion could conceivably be a thing you need to start figuring out how to
calculate when, if there is like a friendly AI that's taking care of everything, and you
don't need to actually calculate people's happiness and determine how it's going to be affected
by having more people. Which is another thing that ties into our transhumanism, which we're
about to jump into. Well, you know what, this other thing I can probably bring up next time.
So that's some of the listener feedback we've been meaning to get around to,
and have been too busy to do lately. So sorry about that, but thanks for your patience.
And I know we miss some other stuff. So we will definitely be on this more in the future.
Yes. You know how a lot of people, you get into arguments with them and they're like,
well, it's unnatural to extend the human lifespan and we all need to die and that's what gives
life meaning and all that. I have the exact opposite intuition on things. I feel that it almost,
so this is coming from a religious background where I was literally told,
God created all people to live forever, which is why that is what we're going to get after Jesus
comes back. So this is at least influenced by crazy religious upbringing. But I get the intense
impression that if you wanted to create a race of beings that would be very happy living forever,
you would create something much like humans that are constantly changing and constantly growing
and finding things that they used to do boring and finding new things exciting and who find
richness in interacting with others and all the different permutations that can come from that.
And also that forget things over time. So who you were like 70 years ago is a very different
person from who you are right now, both due to changes in biology and just due to changes in
circumstance and life experience. And it just seems like such a damn waste to kill someone off when
they're 90 years old and they've just gotten started on everything. If you were to create a
being that just could go on forever, it would be something like humans that that's the kind of
being that doesn't get bored. It doesn't eventually run out of things to experience because it's
already experienced everything. It can come back to a guitar after three millennia and be like,
Oh yeah, this thing. Yeah, let's do this again. Because I forgot how to make it fun or how to make
it sing. I think that's a really good point. I'd never heard it put that way before. And I'm
still unpacking it. But I guess I have one quick thought, which is that you mentioned like, you
know, killing somebody at 90. And I do feel like it'd be a different conversation if people lived
on average to be 1000. I don't think that anyone arguing that, Oh, they should die at 90.
There are some people that do not, I mean, either far fewer. Yeah, not specifically,
we should kill people that make it to 90. But along the lines of, you know, four score and 10
is as much as any human really should have. And after that, shuffle along and make room for the
new generations. And also that it would make your life meaningless if you just kept living.
I think, I think people only say that because that happens to be ripe old age right now.
Yes, I agree. So like, if, if, like just if our ancestors lived to be 1000,
and that was how long everyone lived, then they'd be saying 100 score instead, right? So
how much does a score 20? Then whatever. It's like that scene with Dumbledore was like,
Yeah, but I wouldn't want to outlive my 280 years, Harry. He wants to live longer than that.
Right. The only thing is that you mentioned that some people say it's unnatural. I don't know
what you mean by that word or what you think they mean by that word.
I think what they implicitly mean, even though they haven't thought about it, is that it is
how long people live right now. I guess why is what's natural? Like what's wrong with doing
something natural then? Not my word. I mean, if you had to, I just, that's what's uncomfortable
to them. Like there were people who said organ transplants are unnatural when that was first
introduced. They're super unnatural. The idea that like I could be walking around with someone
else's heart pumping, my blood is really weird, but I don't think it's bad. Right. I think people
have an intuition, a Chesterton's fence intuition that maybe they couldn't explain as well as
Chesterton did. Do we want to quickly cover what Chesterton's fence is? Probably. Would you like
to be honest? Well, if you come across a fence out in the open and you don't know why this fence
is there and you're like, well, since I don't know what it's for, I'm going to get rid of it,
then you realize that it had a purpose. Like you get gored by a bull and you're like,
oh, it's keeping the bull away from me. The idea is that you can remove the fence, but only once
you actually know what was put there in the first place. And yeah, to say like, I don't see the
purpose of this, let's get rid of it is not wise. So I mean, the reason that the Chesterton fence
is there, I don't know if there's a reason for that, unlike when we die is because our cells
run out of the ability to duplicate reliably. And that's like, I mean, there's not really a
why behind it. I think they're misapplying the concept, but it is that intuition
that you shouldn't change things. And there's a sense of like people have an intuition of
being designed by some creator. And even if they don't specifically believe in that,
there's still this weird intuition that a lot of people have about evolution being in some sense
purposeful. And then so there'd be like, if we lived for much longer than we do, then there'd be
all these unforeseen things that we evolved neatly and usefully evolved to avoid all those
terrible things. And now we have to deal with them and we don't even know what they are.
That's fair. So yeah, even if you're not running with the God made purpose, the people have this
intuition that things aren't the way they are for no reason. And so we should figure out what those
reasons are before we start fucking with things. I guess I can see that. But I mean,
AM much more willing to quickly swallow that pill of things are the way they are for basically no
reason, or at least no good reason, especially stupid biological things. Yeah, those are all,
I mean, the reason for those I think are pretty well understood. And they're not, I don't think
there's any moral backing to them, right? I'm also not super convinced just by data that because
something is unnatural means that it's bad. I think my favorite one version of that that I heard
of what natural means is using something for its intended purpose. And in that sense, homosexuality
was unnatural because you're not using your reproductive organs for the way that evolution
or God intended. But then I'm like, the best comeback to that is opening a beer bottle with a
hammer, like morally wrong. I mean, it might not be the best use of a hammer or the best way to
open a beer bottle, but if it gets a job done, like, it's not morally wrong. Like, I don't think that
they may say that you are degrading the beer bottle on the hammer, but it's much worse. Like,
it doesn't matter to degrade objects, but degrading your own body is something that matters.
My own fucking body. That's what I might come back. Don't you need to be telling me what to do?
Yeah. My bottle and hammer. Something, something.
I'm going to dive into that. Kind of reminds me of the feedback we got,
which I didn't get to, but I think it was also by Mr. Oliva saying that he's mainly
against things like performance enhancing drugs for kids, because it would be like a barbaric
Roman gladiator sort of thing to see people just smashing into each other and destroying themselves.
And I'm like, yes, but on the other hand, it's when it's a kid, then obviously that's bad thing,
but it's our own bodies. We can do what we want to them. People are regularly risking their bodies
doing things like driving in, in sports races or, or mining or anything you do that risks,
has some risk of danger to you. It's your choice. It's your body, you know, use it up how you see fit.
Yeah. I mean, those are different situations to one of the kids and doing it yourself.
But I'm also like, I could spend 10 years working out to become like as athletic as an
Olympian, or if I could do it in two years with drugs, it's hard for me to see like,
as long as there's no serious drawbacks, why one's better than the other.
Well, the thing is that there often are serious drawbacks, but and yet there are perverse incentives
that motivate people to, to value, to be willing to sacrifice their health for an Olympic medal
or whatever it is. Yeah. If there were no like physical side effects to, to taking those huge
making drugs, but people did get torn up on the, out on the playing field doing this
for our entertainment, is that, is that still wrong in some sense?
Like there was that huge outcry about the concussions that NFL football players suffer.
And they're still playing football. They are still playing football, but they have changed
the rules a little bit to make the concussions to take that into account.
I mean, boxers are still wearing gloves and, you know, ruining their lives too.
Like, in fact, he met Muhammad Ali, right? I did. You met Muhammad Ali? I did.
I get the impression that he probably would have been a healthier old person and an older
old person if he hadn't boxed his whole life. Parkinson's. Might have been linked to his,
you know, repeated traumatic brain injuries. I guess I don't know that much about the UTL,
I was, of Parkinson's. How was he when you met him? Was he still functional?
He couldn't speak. Um, yeah. I didn't mean to like name drop for you, but I remember that that
was interesting. And it was, I always think of like boxers when I think, I mean, football,
because yeah, they've tried, you know, better padding, changed rules and stuff with boxing
or like really any fighting. Uh, that's, I mean,
Head trauma is the point. Yeah. It's partly like they're for their glory, but it's like,
it's only glory for, gloryful, glorifying for them, because it's fun for us, right?
I don't actually watch professional fighting, but, uh, like for some people it's a lot of fun.
And that's why they get glory out of it. If everyone thought it was barbaric and gross,
they wouldn't, fewer people would do it. Yeah.
And even in the Roman times, most gladiators were, uh, people who wanted to be gladiators,
because it brought a lot of glory and money and you had rich ladies coming over and wanting
to sleep with you. Cause fuck yeah, you just tore some shit up in the gladiator arena.
And it probably beat other kinds of being a slave. It was what?
It probably beat other kinds of slavery. Right. So, right. So that's one of the
perverse incentives that, that leads people to do things like that. Like your life is going to be
pretty shitty. So why not go out in the blades of glory?
I'm going to go ahead and drop stars, uh, the TV show network, uh, version of Spartacus. Very fun.
Yeah. Gladiators.
But we can all agree we don't want to have modern gladiator arenas, right?
Where people kill each other for the glory and our entertainment.
I mean, do we want to tell people what they can do with their bodies?
You know, I thought we just settled on that.
That's what I'm bringing up just now. Yeah.
I'd be, I'd be okay with it if people really weren't free and didn't have these, you know,
pressures coercing them into it. If people like made that choice in the vacuum of some, some sort.
If they didn't need the money to feed their family.
Yeah. Or yeah.
I mean, I don't know how you'd remove all those incentives and have it still be a desirable
thing to do. Like I'm sure like playing football is fun for people, but it's also fun because they
like people watching and sharing their name and stuff. And like, I don't know if that
incentive is perverse or not.
Let's imagine the, the transhumanist future where some people decide to tell the AI to,
to build some kind of risk of death into their
activities. And maybe they do it for the sake of an audience as well.
A common thing to find in transhumanist fiction is that sort of risky play.
In every novel that I've read, society has basically shrugged and said, okay,
I'm not sure if that's a realistic depiction of how things would go.
And I would hope that the people who do that and the people who like to watch that
could be like in a totally different part of reality than me. So I wouldn't have to talk to them.
I'm not, not talking to them because like you would be sad or.
Yeah, it's just kind of like, oh, you like that? Well, I'm going to be over here.
Would you try to stop them?
No, I'd be like, that's, that's your deal. And also your personality is just tasteful to me.
I think I'm in your camp, but I'm also just like contemplating this for the first time.
So like, the idea is that they would do the same crazy fun stuff that all of us want to do,
but they'd like to leave all their backups first and then film themselves or something, right?
Like, is that what sort of you're imagining?
Well, that's, that's a good enough, that's close enough.
Again, part of me is like, well, do whatever you want, but I would also think that they've
got some serious priority issues if they care more about like the applause of the crowd than
like living till tomorrow.
But maybe they also have some, they're doing it partially for themselves, I bet. I bet the,
the thrill of I could actually die is, is part of it. And then also the audience
is also another part of it.
There is some risky behavior that I'm attracted to purely because it is risky.
That's perverse and your personality is just tasteful to me.
Whoa.
Aw.
Yeah, I'm, I'm pretty low risk guy. So that none of that appeals to me or even really
makes intuitive sense.
I am for the most part, but there's just one or two things that it's like, oh,
this is going to be fun.
Okay.
You've got a loss to at least one of them.
No, I don't.
Okay.
I thought that might work.
So, I mean, I don't have a good answer to that. That's an interesting thing to consider.
I'm sort of at a loss.
Well, this kind of brings up the issue and there's a whole article about it in the fun
sequences, fun theory sequences. And I wish I had read it more recently so I can report
what it actually said, but it was about dangerous options.
I'm not sure what the, the title was something similar to dangerous options, but it wasn't that.
But the idea that in the future, we should not allow every conceivable,
like doable option to be available to people because they'll screw themselves and we should
somehow like nanny them out of it.
I'm sort of in favor of, I don't know, I need to give this some more thought. I tend to be,
when I'm first exposed to an idea, I tend to just agree with whatever the first thing I hear is.
I'm trying to, I'm trying to, that's something I'm actively working on is to correct that and
just reserve my position. I can see the benefits to that position. I can see why some people
find it compelling. That's as much as I'm willing to say right now.
I think I strongly disagree with that. I'm reminded of the metamorphosis of prime intellect,
which is available free online and it follows someone who is, they're just a legitimately
crappy person. They're mean, they hurt other people and they are very masochistic as well.
They hurt themselves a lot and they're upset that they are in a reality
that is run by an Evelyn AI that won't let them kill themselves.
She's a protagonist that you're talking about, right?
Yes, she's a protagonist.
I don't remember her being that mean. I remember her being very masochistic and in some instances
vindictive.
It's very hard to be terribly mean in that world because you can't hurt people if they
don't agree to it.
But also she seemed to care about people.
But she was extremely self-destructive and I feel like the story was meant a sort of
an extreme outlier as if someone was this self-destructive and hell-bent on hurting themselves,
should we stop them? I think I kind of come down on the point of view that I think the
author was advocating that no, go ahead and let them hurt themselves. It's their body and their
choice. They're an adult.
I think the point of that story was actually something else.
It was to raise the question of whether people's lives should at all be safe.
Even have the option of a safe life.
I think you should have the option.
Because remember this protagonist destroyed the AI and returned everyone to
like prehistoric technological level.
Spoiler alert.
Spoilers.
Second, I disagree. I don't think that happened. I think that the AI allowed her to think that
that's what she did and created a special simulation just for her where she could go
ahead and be as self-destructive as she wanted after that.
I haven't read the story, but I like your version because it means that the AI is
thinking at one level higher than this person. That's awesome.
That's what I want from a super intelligent major.
The last chapter was from the perspective of a child of the protagonist.
Which if you're going to actually give someone that level of freedom,
you kind of have to let them have children as well, right?
No. If you're going to go to the point of tricking them into thinking that this is reality,
why not trick them into thinking they have kids also and rather than condemning a
non-consenting person to live in that reality?
I want to ask a clarifying question. How did this person end up in this world?
Were they just born into it like an average person?
They were dying of terminal cancer at very old age.
They lived through the singularity.
Yeah. The AI basically took control as they were dying and saved her,
but she'd lived for many years in constant pain.
Interesting. I could say that makes someone bitter.
And then once she was in the perfect world, she hated how perfect it was.
She said there's no meaning to life if you don't have anything to struggle and
there's no fear of distraction.
Okay. Which I disagree with.
I was going to say it sounds to me that the AI that put this world together did a bad job
in letting people in of that predisposition, but they've brought in already-existent people
of whatever disposition they were and had some imperative against changing them as people,
which is probably a safe bet.
Okay. Oh, they covered that already, I guess. That's a good point.
So I tend to believe that it actually did happen because otherwise,
why is there this person who's perspective we are now seeing through who is the child of her?
I see your point. I still like my interpretation.
Well, that would be nicer for sure.
So the real question that we're trying to figure out is like-
Should we nanny people away from things they might do to themselves?
For instance, I would make my memory perfect and I would not be one of those humans that you
describe that forget things over time.
Oh, really?
But should I be prevented from this?
What if I remember every person I've ever been friends with and then as time goes on
and get bored with being friends with each of them and I eventually become friends with everyone
and then there's no new people for me to be friends with,
but then we have to create new people, but then we have to calculate
if we're approaching a repugnant conclusion.
So I can stop there at the second part of that ladder, though, that you would be friends with
everybody. You might have met everybody, but by the time you made friends with the one billionth
person, the first person to be very different. You can't maintain perfect relationships with
the billion people at once.
Maybe you can in the transhumanist future. You can make lots of clones of yourself and
reintegrate your memory.
Or you could even just have telepathic conversations with a thousand people at a time.
You don't need to be like separate minds or separate clones.
That could, are you even human anymore?
Well, if you're transhuman, that's the point.
Oh yeah, good point.
Yeah, are human brains safe for multi-threading?
Probably make it safe.
Yeah.
Should we not be allowed to do extreme multi-threading?
There's some big questions, guys.
I should have thought of these before I got here.
I should have known what we were talking about before I got here.
Whenever we get into the question of should we be allowed to do the thing,
I almost always default to...
Yeah, we should.
Yeah, you should, unless you're hurting someone else and then only with their consent.
I mean, I'm also in favor of nannying, though.
We used kids as an example of modern humans last time.
You nanny your kids out of the road because there's cars.
It would be nice if nature or some huge force could nanny us away from the edge of
metaphorical cliffs or whatever.
Super dangerous things.
Is it nannying to remove heroin from the earth or other things like that?
But is that a bad thing?
Or pick something worse than heroin, pick whatever crazy drug acts.
Or like...
Things that I would argue that drug addictions in a significant way actually decrease your autonomy.
So it's hard to say it's a violation of your autonomy to not allow me to destroy my own autonomy.
Those things could fall into a special...
But if it's what you want, we even have some of that today in the realms of kink,
like the people who like to be 24, 7 subs,
that they have willingly given away a certain bit of their autonomy, but they like that.
This is a totally different topic, but I'm going to say that they are
formally claiming to have given away their autonomy.
Oh, I see.
But it's not legally enforceable, so they still have...
Laws are no laws.
They haven't created...
Their minds are still capable of making decisions, and they're still more responsible for their own acts.
They haven't literally...
What do you call it when you lance out a part of the brain?
What's the word?
Lobotomy?
Thank you.
Lobotomized themselves, and they started with an L.
Yeah, so maybe in the case that you brought up, these evil consented and are, like you said,
formally forfeited their freedoms, but it's not like...
I'm assuming, I would hope, that if they woke up Tuesday morning next week and they're like,
I'm kind of done with this.
Like, no one's like, nope, sorry, you're here for life.
Like, then that's where it starts becoming a wrong thing to do, right?
So they haven't really given up their freedom any more than anybody who volunteers to go into a cave
for eight years.
That's very different than, like, being shoved in solitary confinement in prison for eight years,
right?
Yeah.
I mean, I'm brought back to...
I know I reference fiction a lot.
I'm sorry, because this is terrible, and I feel like...
That's what your brain is made out of.
How dare you be well-read, Jesus.
I don't know, I just...
It feels...
It feels...
I don't know.
Anyways, getting to the point.
You can't refer to an actual experience, because you don't have any.
Exactly, yeah.
But again, at the golden age by John C. Wright, who nowadays is a crazy man,
and even back then was a crazy man, but this book is really good.
There is a character in it for certain definitions of good.
There's a character in it who experiences life from a different perspective that is so amazing.
He cannot imagine going back to his own real life anymore.
And so he just puts himself in that simulation where he's living this other life again,
and erases his memories that he ever was the other person.
But the simulation isn't free.
It's very cheap, but it still costs some money to run.
And he eventually starts running low on money.
And so he gets pulled out of the simulation by the AI and being like,
hey, you're running low on money.
We're going to have to return you to real life so you can make some money.
He's like, no, you know what?
Make the simulation cheaper.
Make it run slower.
Make there be...
And after a month pulled out, hey, you're running low on money again.
Okay, make it so that the colors are more muted and there isn't as much vibrancy and sound.
And erase the parts of my brain that would notice that.
And then a few years after that, pulled back out, he's like,
make everything black and white.
Make Dunalog no longer render things in 3D.
Make everything 2D and delete the parts of my brain that would notice that's a weird thing.
And eventually is living this impoverished existence where he is still in his mind,
like living the ultimate best life as this wonderful person.
But in reality, it is this flat black and white, almost no sound sort of...
Hell.
He gradually tricked himself into wire heading.
Of course, maybe he wouldn't have chosen that at the beginning,
but over these steps, he got himself into that point.
I love that's the trend of like half the stories you bring up that like,
oh, that sounds great at the beginning.
And then it's like, oh, yeah.
But then this terrible daysy thing happens and then it gets awful.
I'm like, oh, fuck.
It's hard to say where to draw the line.
Like, I'd be okay losing, I don't know, rendering objects that I can't see.
That sounds like an obvious thing to...
Yeah, wire wasting resources on that.
But then, you know, where do you draw the line?
Do you draw it at color at 3D?
Yeah.
But is that a thing that it should be not legal for him to do?
I don't know, man.
I'm kind of hoping like, maybe I should have said that an hour ago.
Like, I don't know.
I'm prepared to say that I don't have a good answer to that.
Like, I'm prepared to say that if people no longer want to live,
they should be allowed to kill themselves.
Be like, eh, I've done everything I want to do and I'm no longer getting enjoyment in life.
Please terminate my processes now.
I think it depends on why you want to kill yourself, for sure.
Like, if you want to kill yourself because of a problem that's easily correctable,
then, you know, no, we're going to just go ahead and
help you through this problem that you're having.
And then, if you still want to, we can talk about it, you know?
You know, you could sort of trick people out of that.
Oh, man.
You could say, okay, we'll let you die, but will you agree to, in your will,
bequeath us a full copy of your memories?
And once they do, you just create a similar personality to theirs,
except that what doesn't want to die and have put in all the memories.
They probably only work once.
Then after that, people are like, I saw what you did to the last person that wanted to die.
I'm also pretty sure they'll be violating their wishes,
but it's hard to see, like, well, your wishes are to not exist.
If we want to make just a slightly better and different version of you exist,
like, what harm is it to you?
None, you're dead.
And they agree to give you their memories?
Yeah, if they agreed, I don't know.
Seems like a violation.
I feel like our intuitions aren't great at grasping questions like this,
and that these aren't like problems that we can,
like, maybe you guys have thought about them for longer than I have.
For the most part, I sort of just, like, hope things won't suck in the future,
and I guess, yeah, hope I could check out if things did suck.
But I'm kind of hoping that either really smart generations,
you know, generations worth of smart humans have worked out good solutions to these problems,
or, you know, in an afternoon, a superintelligence did for us, right?
And then we don't have all these weird quandaries,
be like, you know what, this is actually your CEV.
This will work out great.
This is humanity's CEV, right?
I don't have good intuitions on a lot of these questions.
I feel like that's something I should have said earlier.
Since you brought up CEV, coherent extrapolated volition,
I could be wrong, but I feel like this notion is somewhat strongly tied to
another notion of the psychological unity of humankind,
and the fact that's an article by Eliezer called the Psychological Unity of Humankind.
How do you feel about that?
Do you think humankind is as psychologically unified as he says?
No, and in fact, there was a counterpoint article written by Kais Sattala.
Which was called the Psychological... something else?
This unity?
Something like that, right?
And that one was more convincing to me.
I don't suppose you could lay it out.
It's been a while since I've read...
I might have actually read that other one too,
but I remember, I think, the vague outline of Eliezer's point,
which was like, we all have the same underlying brain architecture,
like one human's possible range of experiences
can't be that different from any others,
because we all run on the same substrate,
we all have the same meat, or we all have indistinguishably similar meat.
I think the phrase that I like is that we're, you know,
same model of cars with different color paint,
like there's under the hood, we're exactly the same.
I get that like the hardware and software are two very different things.
That's actually, I guess, when you run down that path, like you mentioned,
AA works for people of certain mental architectures.
I mean, you can make a mental architecture where someone's super happy
mutilating their six-year-old daughter, right?
I mean, that somebody's lived experience, probably, on Earth right now.
So I'm sort of of the opinion that that's not what they would want
if they had a sampling of better experiences and better architectures.
Like if they could try other things out,
they would have realized like, oh yeah,
that was a pretty fucked up way to do stuff.
I might be wrong.
Well, I'm just going to read the last sentence of Eliezer's article
before the article talks about evolution and why like evolution is reason for this,
and specifically sexual reproduction.
And then so the last sentence is, having been born of sexuality,
we must all be very nearly clones, because that's how that works.
But some things that are pointed out in Kai Satala's article were that,
I guess we should all have this machinery in our genes,
but it's very easy, even after you have evolved a complex structure
to just with one tiny mutation turn off that whole complex thing that was evolved,
or to just dial it down.
And is the author suggesting that that happens regularly with populations of humans?
Well, they made an analogy to dogs and dog breeds and how
domestication of dogs from wolves has happened in a very short amount of time.
And so that the argument that humans haven't had enough time to become that diverse doesn't really
curl it up when you think about dogs and, you know, compared to other evolutionary things.
I'm convinced that I could make people dislike that entire position
by pointing out some of the possibly repugnant implications of that line of thinking,
even though that wouldn't actually disprove it,
but it sounds so politically incorrect to say that like,
oh yeah, you can make people's behaviors different,
but then it turns out that like some isolated groups of humans are like
less smart than other groups of humans,
or like traits that we actually care about are less prominent in some of these places, right?
I don't feel it's that controversial to say that like
introversion, extroversion can be largely determined genetically, like things like that.
I mean, maybe, like, but there's a lot of kickback to the idea that like
things like intelligence or even like, I think there's less kickback because it's so obvious,
but like your sort of like baseline happiness level is pretty, pretty strongly genetic,
and that's sort of a bummer, but it's hard for me to see like why things like
intelligence wouldn't be also heritable to some strong, in fact, all the evidence,
a lot of evidence says that it is to large degrees.
I'm not sure exactly, okay, so I do know sort of who you're quoting about that,
but as far from what I've heard, and I'm not a person who hangs out in these
biological intellectual circles, so this is hearsay, but from what I've heard,
basically everyone in the field says things along the lines of
yes, intelligence is at least partially heritable, and it varies among groups.
The idea that it varies among groups is very politically incorrect,
and therefore like denied as a fact by a lot of people, and I think that to say that, oh yeah,
some people, you know, if we're, if some human populations are as distinct as like Chihuahuas
and Great Danes to take the dog analogy, it's hard to say that like some groups of people would have
like, I guess all the capacities that we care about as people and feel are important,
that those would, that those would remain magically untouched, right?
Right, but I mean, again, I'm not really making at this point actually invalidates that argument,
I'm just saying that I feel like I could make a dark arts argument that would convince some people.
Clearly humans are not as different as dog breeds, but it just shows that we could be
different enough to have different values, and that maybe even if you try really hard,
you couldn't find a CEV between everyone on the planet, I have a hard time believing that
even people who are closely related to me would necessarily have a CEV with me.
I'm quite sure that there are people who really find a lot of, I want to say almost moral
righteousness, and certainly definitely a feeling of this is the way things should be,
that people who are able to physically dominate should do so.
It's often part of the more risk behavior driven type of person that these are the
way things should be run. I don't know if they would ever have the same CEV as others,
as people who are more egalitarian and more chill and everyone should live together like
we tend to be, and by we I mean the three of us in this room.
I think part of my intuition for this, and maybe this will strip away the deeper you get,
but like, oh look at this, this person cares a lot more about praying and self-sacrifice,
whereas this person cares more about reading and going to school or something, very different
goals. What do they really care about? They care about being the best person that they think they
can be, and maximizing their path towards that, or maybe a better example would be...
Being the best person you can be is a very different thing for a Marine in the Army,
and for a programmer in Silicon Valley.
Not really, I mean they both, these are the kinds of things I want to do,
I'm going to do what it takes to get really good at them.
I mean they, I guess, the value of trying to become the best you you can be,
I don't think that their core values are that different in those two cases.
Maybe one really doesn't want to kill people and one really does, but I mean...
That's kind of a core value.
Well, that's what I was going to pick at. I don't think that, I mean aside from,
I think anomalies, a lot of people don't want to kill people, they're just ready to,
or they have like a threshold of like, okay, I'm going to put my life on the line to protect
myself and those I care about, that's way lower than other people's.
I think they would also want to see different societal structures.
I think one of those people would be much more on board with the let people do anything they
want, it's their body argument, and other types would be more along the lines of,
no, that's stupid, you don't let someone turn their life into a two-dimensional black and
white hellscape that they think they enjoy, you step in and you stop them regardless of what they want.
So why do those two people disagree? I'm asking like a real question. I think it's...
I'm interested in hearing your answers.
Maybe one person values autonomy more and the other person values someone being around
to be happy in the way that I think humans should be happy more.
So I think it would be easier if we picked an example that wasn't all the way to like,
the sci-fi example you read, we can pick two plausible current humans or current human positions.
Well, I have maybe a fundamental value difference in that you want to see the future
still have some chance of disaster that humans need to take care of and not leave it to the AI, right?
I would not want to be a nanny race or not a nanny race, but what is the opposite of nanny?
Nannied.
Nannied, yes.
You want to have some portion of the survival of the species depend on members of the species or
something like that?
Not necessarily. It has to depend on members of the species, but I guess that I sort of what it boils
down to that there is no parent figure that fixes everything for us. That we are still important
in some fundamental way.
Are we important now?
I'm not trying to be an asshole. I feel like I say that too much that maybe I am an asshole.
I think you're more used to hanging around the types of people that would require that caveat,
as opposed to like, we don't need that caveat. We both know you and we like to talk about the
sort of thing anyway.
Fair enough. I always need that caveat.
So keep saying that when it shows.
I'll say I'm being an asshole next time I feel like I'm being, or like if I'm trying to be an
asshole. So why, I mean, are humans important in the way that you care about now?
As far as I can tell, there is no greater power. So if we want to get off this planet
and colonize the stars, that's up to us.
And so in that regard, yes.
You want a super intelligence to help us build better rockets.
You know, I don't mind the concept of being helped by things. We're helped by a lot of things.
We're helped by dogs to keep ourselves sane, you know?
We're helped by machines to build buildings, but
if someone were to take over completely and just allow us to play in the corner where it's safe,
that's different than actually being able to have some measure of control over our destiny.
It's not necessarily that the AI saves us from
meteors destroying everything on our planet, but rather that it can.
We don't actually have the power. If it was really a super intelligence,
we wouldn't really have the power to do anything that it couldn't do.
Yeah, I think it's more like a bodily autonomy taken to a social level where if
we are not in ultimate control of our destiny, the AI decides whether or not this asteroid goes
here and we don't.
So does it just matter to you that it's humans or any particular human?
I don't have an ultimate destiny over not like nukes get launched tonight, right?
So or whether or not like if there wasn't an asteroid coming and NASA did have plans to
actually take care of it, I wouldn't be in charge of that.
But is it important to you that just humanity is?
Sort of, which is also weird because I understand that going forward,
we're not going to be human anymore. We're going to be transhuman.
So it's not like I'm necessarily married to the idea of a squishy meat bag
being in control of things, but I think it's the difference between like
if I were to live another 100 years and become the person that I am in 100 years,
there's a continuity there. Whereas if who I am right now was replaced tomorrow morning
by the meat from 100 years ago, that would not feel like the same sort of process.
Well, and that's answering a different question though, I think.
I mean, Steven, do you think that you meant 100 years from now, not 100 years ago, right?
Yes, did I say 100 years ago?
Yeah, I was just checking.
Sorry, I saw what you're getting at though.
I mean, does that I hate to like always, you know, with thought experiments like
pick either or, but would you rather die than just be replaced by you from 100 years from now
tomorrow morning?
I mean, I would say that I had died.
Would you say that you died completely?
I mean, there's some semblance of you and 100 years from now,
still have some of your memories, you know, care about some of the things you still care about.
If I went back and I took the place of 19 year old me,
I would feel like I had killed the person who existed between 19 and now.
I mean, maybe this is a failure of my imagination, but I'm imagining that me in 50 years will be
more similar to me now than me now is to me of 15 years ago.
Yeah, but 15 years ago, you were still a kid.
Well, I mean, 19, you were still basically a kid.
Yeah, but once you get past puberty, you don't change quite as quickly.
15 years ago for you right now is a huge difference, whereas 15 years from you now
is not going to be as huge of a difference.
I said 50.
Oh, you said 50.
Yeah, in the future.
So like even if you were or 150 even, I think I'll be more similar to myself in 150 years
from now than I was to myself 15 years ago.
I'll be bigger.
It'd be hard to compare the two.
I mean, but I guess I'm still trying to figure out exactly what the Yoshi is with your nanny,
like the nanny version of a like of a.
I would like to mention that you could murder me right now.
No, I couldn't.
That would be that would be wrong.
I mean, physically you could murder me right now and there's nothing I could do about it.
People you'd be punished after the fact and I could tell you like threaten you with punishment
after the fact, but I couldn't actually physically stop you if you had decided
that was a good thing to do.
I mean, you might be able to think things are never a hundred percent.
You're making a good argument in favor of private gun ownership.
I mean, just going to say you could just kind of like move out of the way and trip me.
I couldn't possibly fall down the stairs.
It is very unlikely that I could prevent you from murdering me right now.
Like if you're suddenly bloodlusted, I don't want to think about me murdering anyone.
This is not a comfortable and I still feel like the fact that you are so much more
capable of murdering me than I am of murdering you or whatever.
That doesn't necessarily reduce my autonomy just the fact that you have this power.
You using it would reduce my autonomy, but the fact that you have it.
I mean, so pretend I'm bigger and stronger than you are and for whatever reason,
I can try to think of some ridiculous example where something's falling and I'm able to catch it
like if I was Spider-Man and someone threw a bus at us and I could catch it and you would
die if I didn't. Would that take away your autonomy for me to catch that bus?
No, no, I'd be happy that you did that.
So you extrapolate to a meteor just like if the thing gets bigger.
But if you were like Uber Spider-Man who got to see the entire course of my life and decide
whenever you think I'm doing something too stupid not to allow me to do that and make
sure that everything that I decide to do is safe enough that I'm okay, that you're okay letting
me do that thing, I would have some sort of qualms about that.
That's a big difference from what you're saying earlier about like wanting humans in charge
of keeping humanity alive.
Right, well I mean that's because it's hard to explain I guess and I needed this metaphor to
help me with that. It's not just that I don't want an AI's help if an asteroid is coming to hit
Earth, fuck yeah I do, but more along the lines of the AI is the only thing that matters and make
sure that we only do the things that are okay and within our sandbox of safety.
Okay, I think you've changed a little bit because you've definitely before said that you wanted
humans to stop the asteroid. Whether on this episode or the last one I can't remember.
Okay, yeah the last one.
There's a distinction there, right? I do find Spider-Man saving me.
But you're right, if Spider-Man was following me around all the time making sure that I made
all the right choices, like Steven do you really want lucky charms for breakfast?
Like that's not the kind of Spider-Man I want in my life.
Man these conversations become a lot more fun and replace super intelligent Spider-Man.
Should we just call him Spider-Man for now on?
From now on Rationalist Taboo, AI's are actually Spider-Man.
But I was really attached to the, I'm imagining it as Machine Doggo.
Okay.
Who says things like, hi human, we are friends.
Are you sure you want to kill yourself today?
Dogs don't want you to do that.
I reminded a little bit of when you mentioned Machine Doggo,
what was that story that you linked to a couple weeks ago on Facebook?
Utopia Lowell?
Yes.
Oh my god, that was a fun story.
That was. I found it actually kind of, like you mentioned, it was kind of a roller coaster.
Like at first it just slapstick fun.
Yeah.
But then you get into it and you're like, oh, this is touching.
Yeah, we should link to it.
Probably too difficult to summarize in a way that would capture the essence of why it's worth reading.
But Utopia Lowell, fantastic story, very fun to read.
Very much in, I would say, the style of HPMOR even.
Yeah, well a little more slapstick, but it's also like what, half an hour read?
Yes, yes.
It took me maybe half an hour and I'm a slow reader, so.
Yeah, it's only 5,000 words.
It is a short story, highly recommended.
I was set to find out that.
It ended the way it did, kind of.
The AI was not the way that I would want the AI to be, that's it.
This part right here, we will put it on the next episode after people have had two weeks to read.
But go ahead, what were you upset about the AI?
Oh, okay, okay.
Well, we have the power to bend time.
It is inherent in computer technology.
We're going to spoil that story that we were talking about.
What was it called?
Utopia Lowell.
We're going to spoil Utopia Lowell.
I strongly encourage you to read it and then listen to it if you want to afterwards.
It's a really fun story.
It's worth getting it without the spoiler, and it doesn't take that long to read.
Cool.
And it's available free online.
Not only is it available free online to read, it is available in a podcast as well,
because Strange Horizon podcasts all their fiction.
Read by Anaela.
Who was also a voice in Harry Potter and the Methods of Rationality?
Cool.
Yeah.
Who did the voice in Methods?
She did, God.
It was a minor character.
I think it was the girl that snaped bribed with a large ruby in the dungeons in Confessor,
the interlude with the Confessor.
Have fun.
Chapter.
Yeah.
At this point, we talked about the story Utopia Lowell for a number of minutes.
We are removing it from this episode to avoid spoilers,
and we'll add it to the end of the next episode in two weeks.
However, if you want to hear it early,
it is available to our Patreon subscribers right now.
Otherwise, you can hear it in two weeks.
And maybe we should go back to the two topics that we were kind of in between.
One is, is the CEV even possible?
Can we even agree on what we want our transhumanist future to be like?
And two, like, in particular, what we made disagree on is the nanny-ing.
Yeah.
Right.
And I guess I also wanted to bring up the conundrum of, related to nanny-ing,
how much people should be able to alter themselves.
Maybe we can say that they should just have the autonomy to do whatever they choose,
but they should be warned strenuously.
For instance, like when I was talking about how maybe I want to expand my memory and brain
capacity to the point where I will get bored of my friends, even a whole lot of friends.
There are humans on earth who have, I forget what it's called,
super autobiographical memory, is what I call it.
I think there, I remember James MacGaha, I watched him give a talk about it,
like eight years ago, who is interesting.
It turns out it's different parts of the brain that remember different kinds of things,
like your declarative memory is different part of your brain than that's why people
in amnesia can still learn new things or why, you know, you can, etc.
But people with perfect autobiographical memories,
I don't think reported that problem.
They do report the problem of being less happy than the average person,
because they remember every bad thing that ever happened to them by somebody,
and every vindictive comment from every asshole in elementary school.
They also have, even with their enhanced memory,
they still have basically the same done bar number that an average person would have, I expect.
That's like 140 people.
I think it's like 150 around there.
I think it depends on the person, but I've heard between 100 and 200 with an average about 150.
For me, it's like 20.
We should really quickly define the done bar number for people who aren't familiar with it.
Right.
Would you like to, since you brought it up?
Okay, I will.
So the done bar number, or as I have heard it called,
and which I like to call it, because it is much more fun this way, the monkey sphere.
Yeah.
Oh my God, I've never heard of put that way before.
Oh, really?
It's awesome.
Okay.
Is the maximum number of strong personal connections that people can have with other people?
Is it strong personal connections though?
Because some people at the outside of your sphere are not strong connections,
but they're like the number of people you can kind of keep track of as being in your general
social vicinity.
Yeah.
Well, I mean, by strong personal connections, I think of people on Facebook who have 500 friends.
That's still sort of a social connection because this through Facebook, but it's not
one that I really consider as counting as strong.
Okay.
So I heard it like an ability to keep track of that many people in your mind of like,
oh yeah, that's Jim.
He's married to so-and-so and they like tennis, have that level of connection with people.
The friendly acquaintance level, perhaps.
Yeah.
There's probably an operational definition.
It's better than all three of these.
Yeah.
But think of someone that you know in real life as at least a friendly acquaintance level,
that about 150, with the one to 200 range being what most people can handle.
And after that, they top out and they just, they can't handle more people.
They have to lose some people.
And I can imagine that if everyone upgraded their Dunbar number, that could cause some
problems with socializing, perhaps.
Oh, what kind of problems?
Unforcing problems.
What kind of problems do you think?
Like people feeling lonely more easily if they're...
Oh, despite knowing more people?
No, well, because they now need more to have more relationships.
Do you think it changes how many relationships they would need to have?
I don't know, maybe.
Maybe that capacity is one of the things that affects our social needs.
Maybe just boredom, again, is one of the things.
If you can keep track of gazillion relationships, are you going to get bored with fewer?
I think that's an unknown cognitive question, because we don't have...
I mean, it's hard for me to speculate, because that's a very different kind of person.
I did have a good definition of Dunbar's number from Wikipedia.
Is a suggested cognitive limit to the number of people with whom one can maintain stable
social relationships, dash, relationships in which an individual knows who each person is
and how each person relates to every other person.
Yeah, I like it.
As far as whether or not having that number go up or down would make people happier or sadder,
I am struggling to imagine why it made people worse off.
I think it's not defined as the number of people that someone needs to know
to be happy and flourish.
It's just the number of people that you can just currently keep track of.
But if you didn't have...
I think dialing that knob a little bit wouldn't really ruin anything unless you'd turn it...
I mean, unless you didn't attune the adjacent knobs appropriately too, right?
You never know.
Maybe you'd need to have a complexity of a level where a certain percentage of that
is full in order for you to feel like you are living in a socially rich enough world.
Otherwise, the world seems bleak and empty.
And what if just increasing your IQ itself makes you more likely to get bored with shit?
Probably does, judging by the number of miserable artists that I hear about.
Yeah, and just in general, I think, isn't IQ slightly negatively correlated with happiness?
Probably.
I know that to the extent that religiosity and IQ anti-correlate, I know that...
I don't know what the nice way to put it.
People who would identify with smart groups like skeptics and atheists
tend to have higher rates of alcoholism and depression.
So that would seem to suggest some sort of relationship, at least loosely,
depending on how well you want to say those things are tied together very closely.
And it's hard to point, figure out which one is the causal factor.
Well, I don't know, being more depressed would make you smarter.
No, no.
But maybe religion makes you happier, and since being smarter means you're likely to be less
religious, that is where the unhappiness could come from rather than the smartness itself,
potentially.
As a non-religious person, I'm going to go ahead and just spell out the alternative,
where stupid people are more religious, and stupidness equals happiness.
I'm not saying that's true.
I just felt like the other side of the coin needed to be shown some light there for a second.
There is pretty strong evidence that religion itself is a causal factor.
Well, okay, I'm not going to say itself, but the social aspect of religion is...
Oh, sure.
Yeah, that's not, I mean, that's distinct enough from religion that you can get that in other places,
right?
Yeah.
Like we have our rationality meetups and stuff.
It is really hard though to get that kind of community bonding and social structure.
Yeah, if you're not going to threaten somebody with eternal torture,
it's hard to get them to do whatever you want, so come every week in tides.
But I've been thinking lately, I think the whole you have to come every Sunday,
sort of a good adaptation for once you get really busy and an adult.
Like when you're a kid or in your teens, early 20s, it doesn't really matter.
But once you start getting really busy and adult and you just don't have the time to get
together and socialize with other people, the being forced to show up every freaking Sunday
turns out to be a good thing.
Yeah, I guess I just sort of worry about what goes into it to make it forced, right?
Right, well, eternal damnation.
Right, so like I feel like that's probably a stick that isn't an ingredient in a good
mental architecture, right?
Sticks a little too big.
A little too big, a little too sharp.
Yeah.
Yeah, they're all kinds of things that I kind of worry about.
People having the options too, but then again, I just want them to be able to do it,
like wireheading.
I don't know if we've given a good operational definition of that.
Maybe we did last time, I can't remember.
Pretty sure we talked about it a few times.
Fair enough, maybe I'm, for some reason, unable to draw to mind exactly what wireheading,
I know qualitatively what you're getting at, but I don't have a good definition if someone asked me.
Well, I like to talk about wireheading on two levels.
One is a very basic, simplistic, like artificial stimulation of your pleasure centers,
and maybe even so much that you can't focus on anything else but pleasure.
But then there's sophisticated wireheading where you will receive not just pleasure, but like
feelings of importance, love, excitement, etc., all from sources that don't actually
have to do with all just directly stimulated, so to speak, or from sources that aren't what you
would normally associate those feelings with.
For instance, if you're talking to a stimulated NPC type person and you have this deep, meaningful,
loving relationship with them, but they are not a person and they do not have those feelings.
All they see is welcome to corn area.
Or transcendent love, I'm thinking San Francisco from Unsong.
I feel like they had to stretch what the cause of that was a little bit,
but everything's weird in that story, so it probably works out.
So the latter is undesirable because you want your states of minds to be tangled up
with the way things really are.
Yeah, because I have, and I think most people, but maybe not everyone,
has actual preferences about external reality and not just experience.
Agreed, and yet I was the one who got the look last time for being sentimental about meat space.
You mean like your experiences regarding other minds, not like stuff.
I don't tend to have strong preferences about stuff.
I have preferences about other minds.
The stuff is just mainly for the purpose of the experiences,
right, but the other minds, that's what...
I think that also ties back into the CEV thing quite a lot because
I think there are people who would disagree with you.
That minds are not the only thing that are important,
and it would be very hard for their future extrapolated volition to match up with yours.
For the record, I am a big fan of physical stuff,
but I wouldn't say that it matters in a way that is relatable.
I think matters is doing some equivocation there,
at least in my minds and probably somebody else's.
It doesn't matter in the sense that if I took this really cool fossil
and smashed it with a hammer that I did anything wrong to the fossil.
It doesn't matter in the sense that it cares about what happens to it,
or that it matters in the sense that it's important to things with minds.
It doesn't have any worth just at the bottom of the ocean right now.
It's only cool as the fact that it's potentially findable by things that would care to look at it.
That's the part of the way that I care about how things actually are.
If I was in simulation and I was finding cool fossils every day,
or even just spread out far enough to where I kept finding it interesting,
that would seem less rewarding to me knowing that they were put there
for us to find deliberately.
That would be like finding a mining node in World of Warcraft.
I was thinking in Diablo where the bonuses and those fountains that grant
double XP or stuff are spread out just far enough to make it fun to find them every time.
I remember us playing this once and Rachel was watching and she was like,
and I told her previously that some half dozen people had died playing this game from dehydration
or starvation or however they died.
It seemed like World of Warcraft and other games,
but she was like, I don't get how this is not fun to watch.
But watch this just for five minutes and see what's happening.
Enemies are spread out far enough to where I'm stressed but not overwhelmed.
I keep getting bonuses but not so often that they're unrewarding.
It's almost hitting that perfect pleasure center crack.
That's not like a digression, so let's see if I can make it relevant.
In a simulated world, I can imagine it feeling like that.
Playing Diablo doesn't matter.
All right, I'm sort of butchering that analogy.
But I guess it's not...
Maybe I'm not doing a good job of articulating where I'm coming from,
which might mean that I'm not coming from a place that makes any sense.
Maybe you're talking yourself out of liking video games.
No, I'm wondering if I'm talking myself out of liking reality or a meat space.
And I don't think I am, but I'm doing such a poor job defending it.
I think it's interesting in a way that I don't feel like a fake version could be.
If I had a thing, and I told you, when I met Muhammad Ali,
he touched this particular thing and he can touch it too now.
I mean, so my brother went to Prague, I don't know, 10, 12 years ago,
and I asked him to bring me back a cool rock.
And he brought me back a cobblestone, like the, I don't know, two square inches.
He didn't try it out of the street, did he?
I think he did, which is a drag for the street.
But it's really cool for me knowing that this was part of history.
We can't put it back.
So whatever.
Or like, I also have a friend who went to IS for a rock from her too.
And it was a piece of like what do you call recently dried lava.
So that was cool to me because that has a history that I can appreciate.
And it's a different kind of history than the rock from Prague, right?
I would ask the AI to scan it, extract all information from it, and destroy it.
See, you two could not have a coherent future together.
Well, but see, like one is just like it's less rewarding to me knowing that it's duplicate.
Like, why destroy it?
Because those atoms can be used for other things.
Oh, oh, if it was yours.
Yeah.
Okay, you wouldn't take away Stevens though.
No.
Okay, all right.
I was like, why are you destroying Stevens rock?
Because I don't want him to have it.
It's vital to fulfill a prophecy.
So this is this is purely just like, I don't know, biological essentialism,
like written, you know, projected onto stuff.
But I think it'd be really cool to own like Isaac Newton's original notebook that he took his notes
in while building calculus or, you know, theory of gravity or something, right?
Having a copy that was made and there's 20 million in production would be less rewarding
than having the original that he held and turned to his own hands, right?
Like, even though they're the exact same or close enough and they contain the same amount
of information or whatever, something that is just part of my brain finds it really cool
to know that this was handled by, you know, a big moment in history and by a big person in history.
Here's a fun question.
What if someone made a atom for atom exact replica and took the two and put them in a box
and shook them and you could never again in the future tell which one was the original one?
Would they have destroyed something of value?
Pause, because I'm thinking about this.
Like, there's a George Carlin quip of if famous paintings can be forged well enough
to fool experts, what makes the original so valuable?
And the answer is for the art, I don't know.
And like, so again, yeah, you're right, nothing matters about this book,
especially if there's, you know, a duplicate that looked the same.
And do you still have the original book?
You just don't know which one it is.
Right. I mean, there was actually a study done and I looked it up once,
because I mentioned it, I don't know, a few years ago, because I heard about it 10 plus years ago,
where they offered, I think this was like skeptics and atheists, like, hey, if we could
take your wedding ring and put it in this duplicator machine that destroyed the original
and gave you a copy and $1,000, would you do it?
And so would I, which is weird because it's just stuff.
But I think it's because only I care about it.
And like, it doesn't have like a history beyond like my scope.
But a lot of people said no, because they liked theirs.
And even though like they, and so that, that was used to kind of make the point that like,
they thought there's something more to it than their atoms.
And maybe that some people were making that mistake.
I'm not really sure.
I mean, I can see how people might have this inhibition, maybe grounded in the sense that
there's something else about it that we missed.
And so we need the original in case we missed something about it.
And then, you know,
I mean, like a rubbing on one of the pages that wouldn't come through,
like, and it just hands translation.
Or you could scan it for some radioactive signature that would teach you something
about science or whatever.
Yeah.
And that's, that's true.
But that, that is distinct from what the point,
from like the sense that I'm making.
Yeah.
It is sort of cheating the, the thought experiment.
The thought experiment is that there's, they're literally identical.
Yeah.
My own personal answer would be that I think something valuable was destroyed.
But the only thing that it would be is humanity,
psychological attachment to a specific object.
Which is a worthless attachment.
Let's get rid of it.
Which at least we can say is a sometimes overemphasized attachment.
Yeah.
I'm willing to see that much.
I will say there's a lot of things I wouldn't sacrifice for that attachment.
Like I wouldn't let a baby die for that attachment.
Would you let a kitten die?
A cat?
A dog?
The answer is yes.
We're not friends.
So you can just lie.
I'm going to take the fifth on that one.
It depends what you're talking about.
I mean, I mean,
But, but the fact that there is that psychological weight invested in things,
I don't think it's necessarily a bad thing.
Like you said, it was a stupid,
a stupid psychological thing and we should get rid of it.
And I think it's a lot of times overvalued,
but I don't think we should get rid of entirely,
it entirely because it makes humans a little more complex and that
is fun, more permutations of humans that you can get.
Also, if we're worried about the like cognitive fallout from
shifting the dials on Dunbar's limit,
yeah, we should also worry about like how much this could fuck with people.
If suddenly they didn't care about like
historic artifacts or things that they previously found very valuable,
just because even though they're things, right?
I agree with that.
I think that's probably tied to a lot of other stuff.
Yeah.
So that's not a good chest circumference not to cross,
at least until we fully understand what we're doing.
I would want the AI to really investigate all these things
so that we can have more control over our psychology.
All right.
I can get behind that.
I mean, certainly nothing's off the table of what's worth looking into.
Yeah.
I look forward to altering my psychology in many different ways.
I'd like to be less biased, but probably there are also some biases
that I wouldn't get rid of it.
I'm more biased, I'm going to say that.
You might be a lot less rational.
Right.
No, I agree.
I mean, that's one of the allures of transhumanism is that
there's no reason that these things are in principle off the table.
Like saying, I want to be happier right now means,
yeah, you can take this pill that will kill your sex drive
and make your hair fall out and whatever.
I'm not sure if that's actually a side effect,
but you can list off all the scary side effects of pills
that might also make you a little bit happier.
But if the idea that like, no, you could just put your average
hedonic whatever measurement on a scale of 1 to 100,
just bump that up by like 10.
If that was doable without any fallout,
or I think that is in principle doable without any fallout,
I want to have more motivation.
Like I want to suffer less from Eurasia,
where I eat things I shouldn't eat
because I'm trying to be healthier or something, right?
Little tweaks that you make to yourself
to make yourself much more psychologically happy and flourished.
I don't know if that's a word.
You demonic?
Yeah, thank you.
Those are the kind of things that I look forward to that.
And obviously like the hardware upgrades would be awesome too.
Uh, did you see my thing on Facebook?
I now have a short story collection that I have put out.
Yes.
Yeah.
All right.
Spoiler.
I bought one and I'm going to make you autograph it.
Oh, fantastic.
I even shelled out for the physical copy.
Cool.
So, well, yeah, that's awesome.
Yeah, and I haven't read all of them.
And I can bring back the one magazine
that has one of your stories in it since it'll be in that book.
Yes, yes it is.
Yeah.
So I have a collection out that people can buy as an e-book
or as an actual physical book
that has the five stories that I have published so far.
My collection is called Red Legacy and Other Stories.
And it is available at Amazon and various other places
where you can buy e-books and physical copy at Amazon as well.
Cool.
Okay.
But yeah, we've been going at this for a while.
So we should sign off.
We will mention to people that they can comment
if they want to on the subreddit slash r slash the Bayesian conspiracy
or at our website, thebayesianconspiracy.com.
Or if they don't want to comment publicly,
they can reach us at bayesianconspiracypodcastatgmail.com.
We want to give a mad shout out to all our Patreon supporters.
And of course to Kyle, our sound engineer,
who makes us sound great and saves us so much time every week.
Cool.
And if you don't have the ability to easily support the show
and you don't, whatever, for whatever reason aren't inclined to,
that's totally cool.
There are other ways to do it.
You can leave a review on iTunes or your other podcast apps.
You can share it on Facebook.
Share it on Facebook, on social media, Twitter.
You can trap a friend on a road trip
and force them to listen to an episode.
Hopefully one of the better ones.
Right.
Whatever you want to do.
Thanks for listening and we'll be back in a couple of weeks.
Bye.
Bye.
