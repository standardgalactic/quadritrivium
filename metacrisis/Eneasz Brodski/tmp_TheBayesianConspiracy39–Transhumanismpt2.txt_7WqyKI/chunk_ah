And then so the last sentence is, having been born of sexuality,
we must all be very nearly clones, because that's how that works.
But some things that are pointed out in Kai Satala's article were that,
I guess we should all have this machinery in our genes,
but it's very easy, even after you have evolved a complex structure
to just with one tiny mutation turn off that whole complex thing that was evolved,
or to just dial it down.
And is the author suggesting that that happens regularly with populations of humans?
Well, they made an analogy to dogs and dog breeds and how
domestication of dogs from wolves has happened in a very short amount of time.
And so that the argument that humans haven't had enough time to become that diverse doesn't really
curl it up when you think about dogs and, you know, compared to other evolutionary things.
I'm convinced that I could make people dislike that entire position
by pointing out some of the possibly repugnant implications of that line of thinking,
even though that wouldn't actually disprove it,
but it sounds so politically incorrect to say that like,
oh yeah, you can make people's behaviors different,
but then it turns out that like some isolated groups of humans are like
less smart than other groups of humans,
or like traits that we actually care about are less prominent in some of these places, right?
I don't feel it's that controversial to say that like
introversion, extroversion can be largely determined genetically, like things like that.
I mean, maybe, like, but there's a lot of kickback to the idea that like
things like intelligence or even like, I think there's less kickback because it's so obvious,
but like your sort of like baseline happiness level is pretty, pretty strongly genetic,
and that's sort of a bummer, but it's hard for me to see like why things like
intelligence wouldn't be also heritable to some strong, in fact, all the evidence,
a lot of evidence says that it is to large degrees.
I'm not sure exactly, okay, so I do know sort of who you're quoting about that,
but as far from what I've heard, and I'm not a person who hangs out in these
biological intellectual circles, so this is hearsay, but from what I've heard,
basically everyone in the field says things along the lines of
yes, intelligence is at least partially heritable, and it varies among groups.
The idea that it varies among groups is very politically incorrect,
and therefore like denied as a fact by a lot of people, and I think that to say that, oh yeah,
some people, you know, if we're, if some human populations are as distinct as like Chihuahuas
and Great Danes to take the dog analogy, it's hard to say that like some groups of people would have
like, I guess all the capacities that we care about as people and feel are important,
that those would, that those would remain magically untouched, right?
Right, but I mean, again, I'm not really making at this point actually invalidates that argument,
I'm just saying that I feel like I could make a dark arts argument that would convince some people.
Clearly humans are not as different as dog breeds, but it just shows that we could be
different enough to have different values, and that maybe even if you try really hard,
you couldn't find a CEV between everyone on the planet, I have a hard time believing that
even people who are closely related to me would necessarily have a CEV with me.
I'm quite sure that there are people who really find a lot of, I want to say almost moral
righteousness, and certainly definitely a feeling of this is the way things should be,
that people who are able to physically dominate should do so.
It's often part of the more risk behavior driven type of person that these are the
way things should be run. I don't know if they would ever have the same CEV as others,
as people who are more egalitarian and more chill and everyone should live together like
we tend to be, and by we I mean the three of us in this room.
I think part of my intuition for this, and maybe this will strip away the deeper you get,
but like, oh look at this, this person cares a lot more about praying and self-sacrifice,
whereas this person cares more about reading and going to school or something, very different
goals. What do they really care about? They care about being the best person that they think they
can be, and maximizing their path towards that, or maybe a better example would be...
Being the best person you can be is a very different thing for a Marine in the Army,
and for a programmer in Silicon Valley.
Not really, I mean they both, these are the kinds of things I want to do,
I'm going to do what it takes to get really good at them.
I mean they, I guess, the value of trying to become the best you you can be,
I don't think that their core values are that different in those two cases.
Maybe one really doesn't want to kill people and one really does, but I mean...
That's kind of a core value.
Well, that's what I was going to pick at. I don't think that, I mean aside from,
I think anomalies, a lot of people don't want to kill people, they're just ready to,
or they have like a threshold of like, okay, I'm going to put my life on the line to protect
myself and those I care about, that's way lower than other people's.
I think they would also want to see different societal structures.
I think one of those people would be much more on board with the let people do anything they
want, it's their body argument, and other types would be more along the lines of,
no, that's stupid, you don't let someone turn their life into a two-dimensional black and
white hellscape that they think they enjoy, you step in and you stop them regardless of what they want.
So why do those two people disagree? I'm asking like a real question. I think it's...
I'm interested in hearing your answers.
Maybe one person values autonomy more and the other person values someone being around
to be happy in the way that I think humans should be happy more.
So I think it would be easier if we picked an example that wasn't all the way to like,
the sci-fi example you read, we can pick two plausible current humans or current human positions.
Well, I have maybe a fundamental value difference in that you want to see the future
still have some chance of disaster that humans need to take care of and not leave it to the AI, right?
I would not want to be a nanny race or not a nanny race, but what is the opposite of nanny?
Nannied.
Nannied, yes.
You want to have some portion of the survival of the species depend on members of the species or
something like that?
Not necessarily. It has to depend on members of the species, but I guess that I sort of what it boils
down to that there is no parent figure that fixes everything for us. That we are still important
in some fundamental way.
Are we important now?
I'm not trying to be an asshole. I feel like I say that too much that maybe I am an asshole.
I think you're more used to hanging around the types of people that would require that caveat,
as opposed to like, we don't need that caveat. We both know you and we like to talk about the
sort of thing anyway.
Fair enough. I always need that caveat.
So keep saying that when it shows.
I'll say I'm being an asshole next time I feel like I'm being, or like if I'm trying to be an
asshole. So why, I mean, are humans important in the way that you care about now?
As far as I can tell, there is no greater power. So if we want to get off this planet
and colonize the stars, that's up to us.
And so in that regard, yes.
You want a super intelligence to help us build better rockets.
You know, I don't mind the concept of being helped by things. We're helped by a lot of things.
We're helped by dogs to keep ourselves sane, you know?
We're helped by machines to build buildings, but
if someone were to take over completely and just allow us to play in the corner where it's safe,
that's different than actually being able to have some measure of control over our destiny.
It's not necessarily that the AI saves us from
meteors destroying everything on our planet, but rather that it can.
We don't actually have the power. If it was really a super intelligence,
we wouldn't really have the power to do anything that it couldn't do.
Yeah, I think it's more like a bodily autonomy taken to a social level where if
we are not in ultimate control of our destiny, the AI decides whether or not this asteroid goes
here and we don't.
So does it just matter to you that it's humans or any particular human?
I don't have an ultimate destiny over not like nukes get launched tonight, right?
So or whether or not like if there wasn't an asteroid coming and NASA did have plans to
actually take care of it, I wouldn't be in charge of that.
But is it important to you that just humanity is?
Sort of, which is also weird because I understand that going forward,
we're not going to be human anymore. We're going to be transhuman.
So it's not like I'm necessarily married to the idea of a squishy meat bag
being in control of things, but I think it's the difference between like
if I were to live another 100 years and become the person that I am in 100 years,
there's a continuity there. Whereas if who I am right now was replaced tomorrow morning
by the meat from 100 years ago, that would not feel like the same sort of process.
Well, and that's answering a different question though, I think.
I mean, Steven, do you think that you meant 100 years from now, not 100 years ago, right?
Yes, did I say 100 years ago?
Yeah, I was just checking.
Sorry, I saw what you're getting at though.
I mean, does that I hate to like always, you know, with thought experiments like
pick either or, but would you rather die than just be replaced by you from 100 years from now
tomorrow morning?
I mean, I would say that I had died.
Would you say that you died completely?
I mean, there's some semblance of you and 100 years from now,
still have some of your memories, you know, care about some of the things you still care about.
If I went back and I took the place of 19 year old me,
I would feel like I had killed the person who existed between 19 and now.
I mean, maybe this is a failure of my imagination, but I'm imagining that me in 50 years will be
more similar to me now than me now is to me of 15 years ago.
Yeah, but 15 years ago, you were still a kid.
Well, I mean, 19, you were still basically a kid.
Yeah, but once you get past puberty, you don't change quite as quickly.
15 years ago for you right now is a huge difference, whereas 15 years from you now
is not going to be as huge of a difference.
I said 50.
Oh, you said 50.
Yeah, in the future.
So like even if you were or 150 even, I think I'll be more similar to myself in 150 years
from now than I was to myself 15 years ago.
I'll be bigger.
It'd be hard to compare the two.
I mean, but I guess I'm still trying to figure out exactly what the Yoshi is with your nanny,
like the nanny version of a like of a.
I would like to mention that you could murder me right now.
No, I couldn't.
That would be that would be wrong.
I mean, physically you could murder me right now and there's nothing I could do about it.
People you'd be punished after the fact and I could tell you like threaten you with punishment
after the fact, but I couldn't actually physically stop you if you had decided
that was a good thing to do.
I mean, you might be able to think things are never a hundred percent.
You're making a good argument in favor of private gun ownership.
I mean, just going to say you could just kind of like move out of the way and trip me.
I couldn't possibly fall down the stairs.
It is very unlikely that I could prevent you from murdering me right now.
Like if you're suddenly bloodlusted, I don't want to think about me murdering anyone.
This is not a comfortable and I still feel like the fact that you are so much more
capable of murdering me than I am of murdering you or whatever.
That doesn't necessarily reduce my autonomy just the fact that you have this power.
You using it would reduce my autonomy, but the fact that you have it.
I mean, so pretend I'm bigger and stronger than you are and for whatever reason,
I can try to think of some ridiculous example where something's falling and I'm able to catch it
like if I was Spider-Man and someone threw a bus at us and I could catch it and you would
die if I didn't. Would that take away your autonomy for me to catch that bus?
No, no, I'd be happy that you did that.
So you extrapolate to a meteor just like if the thing gets bigger.
But if you were like Uber Spider-Man who got to see the entire course of my life and decide
whenever you think I'm doing something too stupid not to allow me to do that and make
sure that everything that I decide to do is safe enough that I'm okay, that you're okay letting
me do that thing, I would have some sort of qualms about that.
That's a big difference from what you're saying earlier about like wanting humans in charge
of keeping humanity alive.
Right, well I mean that's because it's hard to explain I guess and I needed this metaphor to
help me with that. It's not just that I don't want an AI's help if an asteroid is coming to hit
Earth, fuck yeah I do, but more along the lines of the AI is the only thing that matters and make
sure that we only do the things that are okay and within our sandbox of safety.
Okay, I think you've changed a little bit because you've definitely before said that you wanted
humans to stop the asteroid. Whether on this episode or the last one I can't remember.
Okay, yeah the last one.
There's a distinction there, right? I do find Spider-Man saving me.
But you're right, if Spider-Man was following me around all the time making sure that I made
all the right choices, like Steven do you really want lucky charms for breakfast?
Like that's not the kind of Spider-Man I want in my life.
Man these conversations become a lot more fun and replace super intelligent Spider-Man.
Should we just call him Spider-Man for now on?
From now on Rationalist Taboo, AI's are actually Spider-Man.
