if we're approaching a repugnant conclusion.
So I can stop there at the second part of that ladder, though, that you would be friends with
everybody. You might have met everybody, but by the time you made friends with the one billionth
person, the first person to be very different. You can't maintain perfect relationships with
the billion people at once.
Maybe you can in the transhumanist future. You can make lots of clones of yourself and
reintegrate your memory.
Or you could even just have telepathic conversations with a thousand people at a time.
You don't need to be like separate minds or separate clones.
That could, are you even human anymore?
Well, if you're transhuman, that's the point.
Oh yeah, good point.
Yeah, are human brains safe for multi-threading?
Probably make it safe.
Yeah.
Should we not be allowed to do extreme multi-threading?
There's some big questions, guys.
I should have thought of these before I got here.
I should have known what we were talking about before I got here.
Whenever we get into the question of should we be allowed to do the thing,
I almost always default to...
Yeah, we should.
Yeah, you should, unless you're hurting someone else and then only with their consent.
I mean, I'm also in favor of nannying, though.
We used kids as an example of modern humans last time.
You nanny your kids out of the road because there's cars.
It would be nice if nature or some huge force could nanny us away from the edge of
metaphorical cliffs or whatever.
Super dangerous things.
Is it nannying to remove heroin from the earth or other things like that?
But is that a bad thing?
Or pick something worse than heroin, pick whatever crazy drug acts.
Or like...
Things that I would argue that drug addictions in a significant way actually decrease your autonomy.
So it's hard to say it's a violation of your autonomy to not allow me to destroy my own autonomy.
Those things could fall into a special...
But if it's what you want, we even have some of that today in the realms of kink,
like the people who like to be 24, 7 subs,
that they have willingly given away a certain bit of their autonomy, but they like that.
This is a totally different topic, but I'm going to say that they are
formally claiming to have given away their autonomy.
Oh, I see.
But it's not legally enforceable, so they still have...
Laws are no laws.
They haven't created...
Their minds are still capable of making decisions, and they're still more responsible for their own acts.
They haven't literally...
What do you call it when you lance out a part of the brain?
What's the word?
Lobotomy?
Thank you.
Lobotomized themselves, and they started with an L.
Yeah, so maybe in the case that you brought up, these evil consented and are, like you said,
formally forfeited their freedoms, but it's not like...
I'm assuming, I would hope, that if they woke up Tuesday morning next week and they're like,
I'm kind of done with this.
Like, no one's like, nope, sorry, you're here for life.
Like, then that's where it starts becoming a wrong thing to do, right?
So they haven't really given up their freedom any more than anybody who volunteers to go into a cave
for eight years.
That's very different than, like, being shoved in solitary confinement in prison for eight years,
right?
Yeah.
I mean, I'm brought back to...
I know I reference fiction a lot.
I'm sorry, because this is terrible, and I feel like...
That's what your brain is made out of.
How dare you be well-read, Jesus.
I don't know, I just...
It feels...
It feels...
I don't know.
Anyways, getting to the point.
You can't refer to an actual experience, because you don't have any.
Exactly, yeah.
But again, at the golden age by John C. Wright, who nowadays is a crazy man,
and even back then was a crazy man, but this book is really good.
There is a character in it for certain definitions of good.
There's a character in it who experiences life from a different perspective that is so amazing.
He cannot imagine going back to his own real life anymore.
And so he just puts himself in that simulation where he's living this other life again,
and erases his memories that he ever was the other person.
But the simulation isn't free.
It's very cheap, but it still costs some money to run.
And he eventually starts running low on money.
And so he gets pulled out of the simulation by the AI and being like,
hey, you're running low on money.
We're going to have to return you to real life so you can make some money.
He's like, no, you know what?
Make the simulation cheaper.
Make it run slower.
Make there be...
And after a month pulled out, hey, you're running low on money again.
Okay, make it so that the colors are more muted and there isn't as much vibrancy and sound.
And erase the parts of my brain that would notice that.
And then a few years after that, pulled back out, he's like,
make everything black and white.
Make Dunalog no longer render things in 3D.
Make everything 2D and delete the parts of my brain that would notice that's a weird thing.
And eventually is living this impoverished existence where he is still in his mind,
like living the ultimate best life as this wonderful person.
But in reality, it is this flat black and white, almost no sound sort of...
Hell.
He gradually tricked himself into wire heading.
Of course, maybe he wouldn't have chosen that at the beginning,
but over these steps, he got himself into that point.
I love that's the trend of like half the stories you bring up that like,
oh, that sounds great at the beginning.
And then it's like, oh, yeah.
But then this terrible daysy thing happens and then it gets awful.
I'm like, oh, fuck.
It's hard to say where to draw the line.
Like, I'd be okay losing, I don't know, rendering objects that I can't see.
That sounds like an obvious thing to...
Yeah, wire wasting resources on that.
But then, you know, where do you draw the line?
Do you draw it at color at 3D?
Yeah.
But is that a thing that it should be not legal for him to do?
I don't know, man.
I'm kind of hoping like, maybe I should have said that an hour ago.
Like, I don't know.
I'm prepared to say that I don't have a good answer to that.
Like, I'm prepared to say that if people no longer want to live,
they should be allowed to kill themselves.
Be like, eh, I've done everything I want to do and I'm no longer getting enjoyment in life.
Please terminate my processes now.
I think it depends on why you want to kill yourself, for sure.
Like, if you want to kill yourself because of a problem that's easily correctable,
then, you know, no, we're going to just go ahead and
help you through this problem that you're having.
And then, if you still want to, we can talk about it, you know?
You know, you could sort of trick people out of that.
Oh, man.
You could say, okay, we'll let you die, but will you agree to, in your will,
bequeath us a full copy of your memories?
And once they do, you just create a similar personality to theirs,
except that what doesn't want to die and have put in all the memories.
They probably only work once.
Then after that, people are like, I saw what you did to the last person that wanted to die.
I'm also pretty sure they'll be violating their wishes,
but it's hard to see, like, well, your wishes are to not exist.
If we want to make just a slightly better and different version of you exist,
like, what harm is it to you?
None, you're dead.
And they agree to give you their memories?
Yeah, if they agreed, I don't know.
Seems like a violation.
I feel like our intuitions aren't great at grasping questions like this,
and that these aren't like problems that we can,
like, maybe you guys have thought about them for longer than I have.
For the most part, I sort of just, like, hope things won't suck in the future,
and I guess, yeah, hope I could check out if things did suck.
But I'm kind of hoping that either really smart generations,
you know, generations worth of smart humans have worked out good solutions to these problems,
or, you know, in an afternoon, a superintelligence did for us, right?
And then we don't have all these weird quandaries,
be like, you know what, this is actually your CEV.
This will work out great.
This is humanity's CEV, right?
I don't have good intuitions on a lot of these questions.
I feel like that's something I should have said earlier.
Since you brought up CEV, coherent extrapolated volition,
I could be wrong, but I feel like this notion is somewhat strongly tied to
another notion of the psychological unity of humankind,
and the fact that's an article by Eliezer called the Psychological Unity of Humankind.
How do you feel about that?
Do you think humankind is as psychologically unified as he says?
No, and in fact, there was a counterpoint article written by Kais Sattala.
Which was called the Psychological... something else?
This unity?
Something like that, right?
And that one was more convincing to me.
I don't suppose you could lay it out.
It's been a while since I've read...
I might have actually read that other one too,
but I remember, I think, the vague outline of Eliezer's point,
which was like, we all have the same underlying brain architecture,
like one human's possible range of experiences
can't be that different from any others,
because we all run on the same substrate,
we all have the same meat, or we all have indistinguishably similar meat.
I think the phrase that I like is that we're, you know,
same model of cars with different color paint,
like there's under the hood, we're exactly the same.
I get that like the hardware and software are two very different things.
That's actually, I guess, when you run down that path, like you mentioned,
AA works for people of certain mental architectures.
I mean, you can make a mental architecture where someone's super happy
mutilating their six-year-old daughter, right?
I mean, that somebody's lived experience, probably, on Earth right now.
So I'm sort of of the opinion that that's not what they would want
if they had a sampling of better experiences and better architectures.
Like if they could try other things out,
they would have realized like, oh yeah,
that was a pretty fucked up way to do stuff.
I might be wrong.
Well, I'm just going to read the last sentence of Eliezer's article
before the article talks about evolution and why like evolution is reason for this,
and specifically sexual reproduction.
