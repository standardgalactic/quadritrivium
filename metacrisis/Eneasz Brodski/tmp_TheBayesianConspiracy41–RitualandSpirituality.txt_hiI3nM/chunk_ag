Only legible humans fit into modern economic markets, which is actually true. It's one of
the points that was made in those articles. Only legible humans can be factored into
utilitarianism. I thought that was an interesting comment. And only legible humans can participate
in science, which I think some of those may be true. Definitely the ones using the government
social technologies like one person, one vote, and strong property rights, you'd have to be
a legible human to the state for to make those work. Yeah, there may be more benefits to legibility
than we were letting on in that last episode. Well, certainly, like you get all the benefits
of having an organized system, right? And like we did talk about like, you know, the relative safety
of, you know, knowing that you could defend that you could depend on, you know, law enforcement
or something in many situations. Googleplex specifically said that science and utilitarianism
only become possible in individualistic societies. And those are only possible through legibility.
So I don't know, because like, even if you're in a non organized collective, you could still say,
well, I'm going to give my $5 to whatever system that would help distribute it evenly. But that
requires some legibility, right? Yeah, I guess you could still say I'm going to give my $5 to
this beggar and not my, you know, my friends just because I like him, right? So that's still more
utilitarian. Maybe you need to be like effective because you could also like do science in the
sense that, you know, you could measure what you're doing and tell people about it or whatever.
Or check your beliefs systematically against nature, which is my favorite definition,
even though it's kind of vague. Yeah, I don't think you necessarily need to be legible to the
state to do science. But it makes it easier for the state to fund science. But I don't think
they necessarily are the same thing. On the other hand, who but large states can fund things like
the LHC? Yeah, you have to have some sort of agreed upon vocabulary and system of, I guess you
could say rules to do proper science or to do formal science. So like, you know, you can't be
doing, you know what I mean? So like, that's why whatever the fake official language is Latin.
So, you know, because every scientist can write and read the right Latin words, not necessarily
fluently. So maybe that's what they were getting at. That's kind of that's really interesting.
GADBB, I think is the name, also from our subreddit, made a comment on when we had the
transhumanism podcast, and we talked about AIs controlling every aspect of our life because
they know this stuff better than we do and how that was like weird and icky and we didn't know
how we felt about that or I didn't anyway. You guys had different opinions. But GADBB says,
every time you talked about that, I am reminded of Scotland's laws on consent and autonomy,
specifically the adults with incapacity act, which in my opinion is a remarkable piece of
ethical lawmaking. And he didn't he has quite a long thing here, which I will not read all of it.
I would recommend people read the post. It seems like a wonderful model to use for
superhuman artificial intelligence things. But the law is has an emphasis on maximizing autonomy
and minimizing interference. And consent in the law is defined as being the expressed agreement
of a patient who understands the options, alternatives and their associated costs, risks
and benefits. And when someone can't do that, they the state, first of all, has to make a
reasonable effort to assist them to understand these things. But if they can't, the state
can make the decisions for them in some cases, but they have to be as non interfering as possible
and have very clear benefits. And the views of the person, what they would have wanted if they
were not incapacitated must be taken account, such as taking an account, you know, their
their past history or what their friends and families say about them. And if someone with
assistance can make, have an opinion on something, they are allowed to make the
wrong decision, even for terrible reasons. I thought these all seem like really cool things.
So being able to opt out was something that I think we kind of agreed on during the transition
during human episode. For the rest of it, I feel like that's sort of where we were at,
but the problem was like, how can you inform somebody or something that they can't really
understand? So whether it's, you know, you literally need to be as smart in relation to
or rather like you need to scale like from us to ants, but up again. And like, so how can I explain
anything to somebody like that? If, or how can I have anything explained to me other than being
told, look, trust me. But I mean, even like, I'm, I'm something like, so I like, I do like the
spirit of what that's getting at, but I, I'm thinking of like, you go to the doctor and they're
like, yeah, you're going to need surgery and here's all the great reasons. Well, like you're,
I mean, they could even list out your two alternatives, but, you know, you're not,
you're not, you don't have 20 years of medical experience to weigh those options. Like in
what sense do you really have informed consent? Like you, you just, there's too much to cover.
So I don't know if that solves the problem. Other thing gives like another way to look at it,
right? Maybe you have more time to explain stuff, you know, if, if for running in simulated time,
you'd be like, all right, here, step into my office. It'll take two days of outside time and
inside it'll be all the 50 years. I need to explain all this shit to you, but it won't be
uncomfortable. I don't know, but then there's trust them for that house 50 years in that room,
not be uncomfortable. Fuck. So it'll only be an hour. I don't think I would want to age 50
subjective years either. I was thinking in simulated time where it wouldn't be right,
but in simulated time, my mind is still ageing 50 years. Like I've gone through 50 years of life.
It's been 50 years since I saw that my loved ones so that I could make this stupid decision.
Even if only two days passed for everyone else. So then I'm just trying, I'm not trying to ruin
the point because I do like where it's getting at. I want to reemphasize that, but I'm thinking,
okay, well, then we can just Neo matrix this into your brain. Like, you know, he learns kung fu,
but how do you know if you want to know it or how if you'll change if you know it?
Like unless they can, you know, so at that point, you do just need to kind of trust the
super intelligence that you're dealing with and that like, well, look, I ran all these simulations
and you liked all of them. So I'm pretty sure it'll be fine. And when I'm pretty sure it's pretty
damn sure. I don't know what else he could go on. Yeah, I don't know. Okay. All right. Thank you for
feedback. Swindle says we talked about Dunbar's number in the last episode and actually this
episode too. Swindle says on our homepage, the basin conspiracy dot com, you touched briefly
on Dunbar's work in this episode. It's a bit more complex than you explained and a bit more
far reaching than a single number. So he just goes into a little more detail for us. Dunbar
correlated brain size with social group size. The short version is that yes, a post human of
some sort that will have a higher intelligence will almost certainly have a higher Dunbar number.
And also Dunbar's number is actually a set of many numbers that match up to various intimacy
levels. The levels basically started five and triple more or less from there. It's been five
years. So bear with me if this isn't 100% correct. But your initial five are basically people you
see every day and would openly sob at their funeral. Your next 15 are like close friends,
people you might put in your wedding or make your pallbearer. 50 is basically your community.
And 150 is all the people who are real people in your brain on an instinctive emotional level.
It goes out further, but the numbers get really nebulous after that point.
And he says he has a bunch of examples in the book compares them to military units and sports
team sizes. There was an interesting one about Amish communities and how they will split the
community on purpose to keep the population at a low level that can be controlled by peer pressure
alone, which is a good point. Can't control like a city of Denver by peer pressure. There's just
too many people, but you can get much smaller groups. Anyways, he ends with an infinitely
smart person that can potentially care deeply about an infinite number of people.
I like the whatever 150 ish is the number of people that you can model as actual people
instinctively. So that's actually a really interesting way to think about it, right? You're
way more bummed when even like somebody you knew in high school, you know, gets a divorce or gets
killed or something. Then you are hearing about a tsunami in Japan, right?
Definitely. I know, but the, I guess it depends on how close you were to people in high school.
Yeah. And I'm throwing out, because I don't know 150 people. So I'm trying to think of,
I have to go back. I can sometimes get really bummed about certain celebrities. If they were
celebrities that like I had a lot of emotional experiences as we were just talking about with
through their art. If it was someone who like their music really deeply touched me and I read
a lot about them. And I felt like even though I'd never met them, they don't know who I am.
They don't have any sort of connection with me, but I somehow managed to glom onto them a little
bit anyway. And it feels like, I don't know, maybe celebrity culture is invading our Dunbar
numbers. Oh yeah. I can feel that way. I can relate to that, at least like with authors and
stuff too, right? Yeah. So, you know, and that's, I mean, when did Ian Banks die?
Two years ago? Oh, 2010. I don't know. I was throwing up a number.
Some time ago. God, time passes fast. Well, like Derek Parfit just died last year or the year
before. So like, you know, but you know, part of the point of being an author is that you can get
your, your thoughts and stuff out to people well outside that limit. So I don't think that there's
anything, I don't think you were implying this either, but you know, there's nothing wrong with
like having that impact go one way, right? So. Well, aside from the fact that if there's only
150 people, we can feel emotionally instinctively that like they're real people, then having people
that you'll never interact with take up one of those slots is kind of a bad thing.
It's, it's a use, it's a poor, it's a suboptimal use of a limited resource.
Well, maybe, maybe. Yeah. I think it depends like, because I could get a lot more out of,
you know, maybe a, a musical artist or an author than I could out of the 150th person
that I can find in my circle, right? It's true. So it might be suboptimal for like the purpose
of maximizing my monkey sphere in the, you know, almost literal use of the word, well,
in the almost figurative use of the word monkey sphere. But it's optimal for like me in my life,
right? I guess it would also help a lot if you have those sorts of emotional connections with
people that are distant from you in cultures. Like if you have that emotional connection to
someone from Zimbabwe or, or someone in China, then it becomes, you become a more accepting,
non-racist sort of person, perhaps. Now we can wonder if you're wasting that slot,
if you're doing that with dead people, but probably not for the same reasons.
Because I'm thinking like, you don't lose anything by reading like Derek Parfit just
because he's dead. I mean, other than the fact that you can't get him to articulate if you
didn't write it anywhere else, right? But, you know, I was thinking of people having
emotional experiences thinking about Jesus dying or something, which some people do.
Well, they're not, that's a waste. You decide. I don't know.
That's interesting. If one billion people all have an emotional connection to Jesus
as one of the part of their 150, that sort of ties them together in a way.
Well, for a lot of them, he's like one of their five. Yeah.
That's kind of intense. Yeah. Really? Yeah. Wow. Okay.
I think he won the monkey sphere.
Wait, Muhammad might have won it. That's true. He's got more than a few years.
Muhammad made a huge comeback. Yes.
We're not comeback. I just, whatever, we're ending that sprint, but yeah.
That's all the listener feedback I got for this week.
I didn't look anything up, so I don't have really anything.
All right. Thank you for joining us. We'll be back next week, not next week.
We'll be back in two weeks. Bye. Thanks.
This is the part of the ritual that I don't like, is I always get a look
minyosh when I don't say bye to the microphone. Why don't you say bye?
Because I'm not talking to the, like, I'm not talking, I'm talking to the listeners
abstractly. I'm not saying bye, like, hey, see you later. That feels forced.
All right, fine. I will never ask you to say bye again.
No, no, I don't want to make you do anything you're uncomfortable doing.
All right. It's not so uncomfortable that it, I mean, does it have a value?
I kind of, if anyone writes in on this, that's basically what I'm going to go with.
So if there's two, try to just, try to agree so it's not an even split.
I don't insist that you do it, Steven. No, you don't got to do it.
I sometimes listen to this podcast. You prefer the bye?
What? You prefer the goodbye at the end or does it feel awkward and forced?
No, that makes sense. Bye.
It's a closing of the ritual because the entire...
And then you have the music play and then it makes sense.
Well, the entire podcast is a ritual.
I love that little arm.
And so you close the ritual by saying goodbye so everyone knows it's over and they can go on
to do the next thing.
I'll concede that by saying that the entire podcast is a ritual in the same way that you're
doing witchcraft.
Yeah. And with that, we can see you in two weeks.
And also, of course, big thanks to our sound engineer, Kyle Moore.
