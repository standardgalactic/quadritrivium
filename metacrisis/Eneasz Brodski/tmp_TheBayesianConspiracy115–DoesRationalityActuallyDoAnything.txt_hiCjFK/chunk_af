interactions, uh, and how to be more assertive and a lot of things. And then like getting into a
science field was sort of just me proving to myself that I could get into a science field,
because I've had been told when I was a little kid that like science isn't for girls, which
gross, but, um, also I'm not a girl, which, haha. Take that. You got them on both counts. And like,
I also, I also don't want to shit on it on accounting, but of the three of us, you're the
one whose career is making a positive impact on the world. So, um, I, I'll go ahead and, you know,
I have my, if my, if my company's product disappeared tomorrow, the world would be no
worse off for it. I guess, you know, the people who make money off of it would lose that money,
but it's not like the kind of thing that I think will make a lasting impact on the
man. Well, hold on. See, I mean, I completely agree with that, you know, Jason's having the
most impact and all, but like people give your company money because your company gives them
something of value. Like you're making someone's life better, which is why they're giving you
money. And they, they're actually getting money out of it. The customers are like, it's, it's, uh,
referral marketing tracking is what my company does. And so like you're either getting money
or something else, but like, uh, the, I always go to example is like that little share button
at the bottom of Hulu. Um, you know, if Hulu was one of our customers, uh, then it might be where
like, you know, it doesn't matter, but the point is, is like, you don't get, you know, Hulu doesn't
send you five bucks. They said you two free, free weeks, which is the equivalent of seven and a
half dollars, right? Um, so, you know, saving some people a little money down the road, but that
said, like I'm not building AI yet, but you know, I'm, I'm new into the, into my career. So
I put more stuck in growth mindset. Obviously the people that are trying to mitigate AI risk,
that's the bigger concern, I think, but, uh, that is totally an aside, but
No, I'll just make AI first and then, you know, we'll solve the problem of the, of the fallout
afterwards. I mean, uh, I, that's supposed to be, as far as, no, I, sorry, I processed it late
because I was like on a thought train of some kind. Oh, I was just making it clear. So no one
adds me over it. You know, Steven, that's really a bad idea. Um, we, one more thing. I'll finish.
I think particularly if I'm thinking about, uh, effect of altruism and like 80,000 hours,
the fact that I think the rationalist community has identified
what they think are the key existential risks and also put a lot of work into, uh,
well, philosophy and ethics. I mean, those fields haven't really made a lot of progress since,
I don't know, they've been around the, uh, for quite a while and we've entered new territory with
the atheist skeptics movement, uh, kind of letting people for the first time be publicly free of
superstitious beliefs, but then there's sort of a void left there and people are trying to define
like what's good. I think, I mean, EA is pretty incredible. Um, when I thought about like the
simple, the simplicity of the concept of just like, you know, giving 10% of your earnings,
whatever that is, or whatever you can afford to give based on how much you make and what your
expenses are to the people that are literally the most in need or the cause that needs the most,
you know, just like multiplying your efforts there. Um, yeah, we forgot to mention that EA
was birthed out of the rationalism. I was, when I was trying to think of, when I was talking about
the organizations that I like, I was like, you know, the one with the charities and the, yeah,
give well and et cetera. Yeah. Uh, even if that's all the rationality community contributed,
like, and then we just all died out because a meteor struck the SF Bay or something. Like,
still that's assuming like these organizations continue to move forward. I think, uh, 80,000
hours, giving those resources to people who have to drive to either work to give or to work in one
of the key fields, like, you know, researching pandemics, uh, going into politics, uh, AI alignment
problem. Uh, that's all stuff that humanity really needed. And no one else is really like
pushing real hard on these things. I mean, religions are still saying the things religions have
always said, which is do quote unquote good things and don't, and, you know, stop the bad
quote unquote things, which are just super updated. Um, and you'll get to heaven or whatever your
reward is. I think only like Buddhism that I can think of doesn't have some kind of reward.
Uh, gosh, now I'm going on like a big, long tangent. I was going to finish up real quick.
No, you're good. I'm just trying to, trying to think of how I could like wrap up my thoughts
about this, which are, I guess, I think we're already doing pretty good for a very young
sentient species as compared to the many other sentient species out there. God,
I should just stop. I'm going to stop. I think I've made my point already.
As a very, as a very young community.
Yeah, let's, let's go with that one.
What about you? And Yash, did you have any wrap up thoughts on this subject?
No, no, I am, I'm good. I think we've hit just about everything. How about you?
Um, I mean, I, I want to make sure I do the question justice. And like, I feel like saying
we don't have the answer or like don't have the data yet is a cop out, but I,
I hope I've given a good explanation to why I feel like it's, uh,
just the circumstances we're in and that it at the same time doesn't make it unreasonable to
believe that there's something here. Um, and I don't think that the type of people who,
who, uh, need the data to join are going to actually join. Like this is something
you do because it feels like it is making your life better. And yes, we would all like to have
that data and it is an ideal and a goal of the community to have that eventually, but
that's not what personally motivates people. Yeah. I mean, that's a good point. Certainly
not at this stage. Like if it did work out to where, you know, CFR 3.0 can guarantee, you know,
guarantee with like 96% success that you will see a 50% increase in your income a year after
leaving the workshop or something, that'd be great. Um, but I mean, yeah, like right now it's,
it's mainly anecdata that, you know, I can give from my own personal testimony as to why I'm here
and that I've heard from other people. Um, and, you know, part of it's the community aspect of
it. Like the, the friends that I've made in the community are awesome. Um, I don't like,
I mean, I didn't make any friends in the skeptic community. I was never, maybe because there's
no in person things, um, although there wasn't here really in Denver until I started them. So
like maybe if I tried in Fort Collins, it could have happened, but, um, for a skeptic meetup,
but, oh, you know what? There was actually a skeptic meetup and it was super boring. I remember
someone was trying. I went to one. Uh, yeah, I think I kept saying I was going to go and then
never made it out. I went to one. It was like an upstairs part of a, of a coffee shop and some guy
just came in and he's like, so I heard this was like a skeptic scheme. I wanted to ask you guys
about like, you know, it was some ESP or some bullshit. Yeah. And it was like, okay. So we spent
the whole time talking with him about ESP. Um, I mean, it was, it was fun and it's kind of exactly
what you want. If you go to a skeptic meetup, you want to sit there and tell somebody why they're
wrong, but uh, that's never why I was into it though. Like, and I guess that's a, that's why
you're here and not there. Well, yeah, yeah, exactly. I mean, um, what do you get together and
talk about? Like is that I still am sad about the fact that the skeptics movement kind of
seems to, I don't know what happened. I think, uh, God, you know, let's not get into culture war
stuff, but culture wars happened. Uh, yeah. I have a brief culture note, the, uh, culture
worry side note that I can make before we segue into discussing any other like rationality techniques,
because J.C. missed that episode and I wanted to get your, your take. Um, if you feel like you're
up for it, but the, uh, we might be running kind of long this time, actually. Yeah. And we still
got like a couple things that I wanted to hit kind of fast before we finish.
Fair enough. Well, we'll, we'll, we'll just have like, whenever we feel like it, or we need to
fill some dead air, we'll throw in a rationality technique that we like. How's that sound?
I almost want to do like a part two of that episode because I have so many things.
Okay. Yeah. We'll make it a dedicated thing then. Um, anyway, I wanted to mention that,
uh, like five hours ago as of the posting of this, the New Yorker, uh, posted a, uh,
an article called Slate Star Codex and the Silicon Valley Wars against the media.
And, uh, it was about as good as the title makes it sound. Um, again, it does that wordy thing
where it has to like, just, I don't quite get that style of writing.
An AI probably has their own style of writing.
Maybe. Yeah. No, I think it has a human author, but like, uh, speaking of styles of writing,
this is just to kind of get the, illustrate the point. Like, has anyone ever read a LinkedIn
post? Uh, like you have you? Not really.
Okay. Well, so, uh, like, if you, if anyone has a LinkedIn, go there and just scroll to the top
of your feed and look for one that's not an ad and just look at the weird way everyone talks.
It's all algorithmic. I'm telling you, this is, but, but it's human, it's human algorithmic.
It isn't. It isn't even, I have a friend. He was a Grimsman at our wedding,
talks like a perfectly normal human in real life. I see his LinkedIn posts once in a while
and it's like, you know, so happy to be working with the, you know, this best, you know, the best
team that were really driven. The synergy here is awesome, et cetera, et cetera. And I'm like,
why are you talking like this? Oh, cause you're on LinkedIn, but it's so anyway,
I bring that up because that's how I feel about reading like every news website.
But the New Yorker maximizing for eyeballs and it's, it's algorithmic. Uh, it's about keywords.
There may or may not be human analysts that are coming up with this stuff, but I was in
the app store optimization and search engine optimization.
Like, uh, that was part of my job when I was doing the social media marketing for mentioned one.
Definitely, uh, this is, I don't know, like that. I almost want to do a whole episode about the
YouTube algorithm because it's gotten really terrifying. That does sound like another interesting
kind of worms. I just meant that I've read ones that are written by humans who I flesh and blood
know. And they would never write that title. Yeah, they just put on a, no, I mean, it is
their title. Like they wrote the whole thing. If this was the way that they were talking to you,
they would never, yeah, phrase it that way. It's because they're optimizing for keywords.
Exactly. I think unconsciously even, I think it's probably pretty intentional.
I think people just talk different in different mediums. Like I talk with my parents differently
than I talk with you guys. And I think there's probably a similar thing with LinkedIn. I
definitely talk with my boss different than I do with, uh, my friends. One more. Yeah. Like
there's Tumblr speak. There's Reddit speak. Um, like that's just the thing. Anyway, I brought
all that up because I hate reading articles from places like the New Yorker and New York times
and vice and all that stuff. And yet I found articles from all those things, uh, while doing
this. And then I came across the New Yorker one for the, I just wanted to plug really quick and
I'll put it in the show notes. That's why I brought it up. Um, more recording on what the, I already
forgot the date because it's always the ninth. It's, if I haven't mentioned yet, we're in the
middle of a plague. And, you know, so when you're listening to this five years in the future, this
is during the first year of lockdown when everyone was still adjusting. So, um, that's a scary thought.
Um, yeah, by year three, we'll have this down. Everything will feel about, feel like it's normal.
Oh, I really love how the headline says that it's, uh, the Silicon Valley war on the media
instead of the other way around because fuck you, you assholes. By love, I mean hate in this case,
where, where they, yeah, no, they're like, we're the media. So obviously Silicon Valley declared
war on us. Also the derision, like you're expected to read the words Silicon Valley with a rolled
eye and a snare. Are you? I think so. I mean, anytime I see Silicon Valley mentioned, uh,
I think in 10% of cases it's to praise some innovation and 90% of cases it's to
like give the middle finger to those, those people, you know, those tech boroughs, those, but us.
I don't know. I say Silicon Valley a lot and I just mean it to refer to the place.
You do. I don't know. I didn't, I didn't realize that there was this negative connotation.
Maybe, maybe that's just if you read the New Yorker. No, I think it's a lot of media outlets.
It's kind of like using the word rich. Yeah, it's not even somebody could be wealthy,
but like that's almost a slur. It reminds me of the extent to which people hate Elon Musk.
I mean, in his defense, he does a lot to help fuel that. It sucks. He's like,
like the Batman that we all want, but we're not getting and he like so could be. He's almost
our universe is Tony Stark. But I mean, like, he just does some weird shit. I mean, doing weird
shit isn't something that should make people hate you. We do weird shit all the time.
Yeah, but we're not famous. No, it's like the fact that, oh, he thinks he's so much smarter than all
of us. But I even saw somebody was shitting on Elon Musk for attempting to come up with some kind
of COVID intervention. I don't remember exactly what it, oh, it was, he tried to get his company
to donate a bunch of a ventilation machines to hospitals and they sent the wrong machine
to a hospital. But I think actually I looked into it and they did send the correct machine to like
a large number of hospitals and maybe one or some percentage got the wrong machine.
And I think it was fixed after the fact, but it was like, go home, Tony Stark, like,
you know, stop trying to, and the same thing about like Bill Gates and the malaria foundation.
Like, I see hate from, yeah, like anyone involved in the EA too, but like, God forbid that you try
to say that there's a better way to do charity than you giving to like your neighborhood baseball
team or whatever. I don't understand what kind of a monster tries to help humans.
I think it's anyone that puts their goals in a position of saying that they're good at something
or like, like, you know, trying to be the best at this thing, then all the haters have to rise
up and tear them down. And of course, anything that feeds into that is going to be maximally eyeball
grabbing. So they try to really like push for that in the media. It's there's so much negativity
bias in the media. I can confirm that of the media slant about Elon Musk's respirators. I literally
only heard about the hospital that got the wrong ones. But like by the weird shit, I mean, like,
you know, when the those Taiwanese kids were stuck in that cave, and he like tried to build
that sub or something, and then they I think they ended up not using it. And then he called
like the guy who rescued them on Twitter, a pedophile, I think out of nowhere. So like,
that's the weird thing I'm talking about. Yeah, everyone's going to focus on that though,
like rather than the fact that he tried to do something about it. Oh, no, I'm the one singing
it. Don't go wrong. I was singing his praises. It just it's just like, sometimes he makes he
gives the enemy too much ammunition. But we're getting hung up on Elon Musk, but I get I get
the point. Yeah, I mean, I guess I don't think that he should have to be a politician. No, but
I also don't think he should be running around calling probably not but that's called give a
pedophile on Twitter, but you've got a lot of followers, you know, this is exactly why most
people should never ever get on Twitter. Well, he's not doing any worse than the president. Fair.
That's you know, if you're sitting the bar there, then we're all doomed. I think we're all doomed.
Oh, geez. Actually, like, all righty, now she said he had something to something else to talk
about before we all get to him. Yeah. Yeah, sure. Let's go on to the Twitter one then.
I used to have a Twitter account. I made it like many years ago, basically never used it
after about a week because I was like, this is dumb. Twitter is terrible. I'm out. I logged back
into Twitter for the first time in years to post a note of support for aila. Aila is a
rationalist and is probably most well known to people by her own admission as being the person
who dropped acid, a large dose of acid every week for I don't even know how long months at least,
maybe I think it was two years, years. Okay, cool. And yeah, we talked about her when we
did our psychedelics episode. We did. I just didn't remember how, how long this went on.
A long time. Yes, apparently. Anyways, she no longer does the weekly acid, but this is not
what this is about. So I'm going to get to what it is about. Aila, Twittered or tweeted, I guess.
Okay, boomer.
She texted to the tweet.com confession. I lurk in Christian only communities online.
And when someone posts about doubting their faith or being in pain over what their faith
requires them to do, I PM them sympathy and affirm their doubts. And like, first of all,
that is awesome. I logged on to reply and say that that's awesome and to thank her for helping
both improve those people's lives in the long run and strengthening society as a whole, because
that is wonderful. And I, you know, I kind of wish I had the time and the energy to do that
