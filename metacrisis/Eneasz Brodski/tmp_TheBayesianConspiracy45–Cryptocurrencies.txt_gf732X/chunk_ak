They have a solution. Imagine you're behind Rawls Vale of Ignorance and this city is explained
to you, knowing you have a large chance of living in a wonderful place or a small but
substantial chance of being tortured for 70 years in order to give that splendor to thousands of
people. Wouldn't you still accept it? Probably. I'm a gambler. Let's do this.
Plus, I like those odds, right? It's not one in five. It's one in 100,000s.
Well, probably not 100,000s, but one in thousands at least. I'm not sure exactly how big the
population center was, but at least several thousand seemed like.
Yeah, I think that I'd probably take the shot, although I think it's an important part of the
story that's heavily implied that the person being tortured wasn't given the option.
That they're just fueling some horrible machine. I think that's important. I also,
I don't know, that is a weird question though, because that sort of brings out the question
of the utility monster. Am I obligated to sacrifice my life to someone else if they
would get a lot of utility out of my sacrifice more than like, am I obligated to be someone's
slave if my 70 years of misery would make their lives much better than how miserable I am?
Yeah, that seems analogous, and that seems worse, so now I don't know where to go.
I mean, I'd like to think no, but that's the whole point of the utility monster is like, well,
good luck arguing with this because it seems to hit every point that you care about,
and yet this horrible thing happens. Yeah, I don't know. I don't have a good
re-bottle to the utility monster. I have me neither. I'm sure they exist. We can look it up.
I think that's one of the reasons it was a good story.
Bayes and Confused says that... That's a great username, by the way.
Almost everyone would agree that raising the sanity waterline for everyone and giving
rationalist techniques to everyone, even if it's only a little bit per person, would be massively
useful. I'm going to give an example from my own background. My family is an Egyptian,
but I've been a U.S. citizen since about a year old. I've been highly Americanized.
I went back to Egypt for the first time about a year ago, and the thing that struck me the most
is what happens to an entire society when the average education level is about second or third
grade. One of my younger cousins who was who's asking me what things I had learned that helped
me in life in America so he could try them in Egypt, I pointed him to the less wrong community
and the sequences. It's been over a year now, and we realize there is an interesting dynamic
at play between the effectiveness of an individual rational person if the sanity waterline is low
in their society. I'm sorry, I'm kind of screwing up the reading of this. Specifically, he's gotten
better at practicing even simple rationalist techniques like repeatedly asking himself why he
believed stuff and defaulting to looking to data instead of going with his assumption.
The results are really good with him. He's gotten multiple promotions and he's doing
better in school than he's ever done, significantly better than his classmates.
But now, he keeps running into issues of cooperation. Because he now thinks through
his plans a bit more slowly and deliberately and comes up with good plans, he wants to have
people who can help him and can follow that line of reasoning. But at a certain point,
he just can't find any other intelligent people and his progress stalls.
A formulaic way of describing the phenomenon on is to say that the bigger the difference
between the rationalist and the society, the faster they will climb upwards towards whatever
success or goal they have. But it is the overall sanity waterline in a culture that determines
how high you can possibly ascend. In Egypt, being a rationalist means that you can get to the top
really quickly, but because of where society is, that top is not nearly as high as it would be in
another educated society. I think that's why it's important to make rationality and ideas
related to it more popular as a whole. Yes, even to the soccer hooligans. Because if they can get
even 5% smarter, I think it moves up the maximum upper bounds significantly. And I thought that
was a really good point that if we can spread the whole rationalist thing a bit and move up the
sanity waterline just a bit, there's leaps and bounds more progress that we can make at the
highest levels. Because everyone else working with them and keeping society running is just
better at doing that. It's a multiplier effect almost. Yeah, that's an awesome story. And I
appreciate sharing that. I have a couple immediate thoughts on that one. Yes, I think that that
general point is probably true. So you can only get so high, but without cooperation,
there's no more edifice to keep building on, right? Because you can only build so high by
yourself. I have this thought every few months whenever something sparks it, but I'm pretty
sure Eleizer had many motivations in writing the last wrong sequences. And I think that's
probably one of them. There's only so much that I can do by myself. I need more people who are
interested in this kind of thing, kind of closer to where I'm at, so I can get more of a team on
things. And maybe that was an expressed goal, but that seems like the kind of thing. And that might
be a solution. I'm not sure exactly what problems your, was it cousin or nephew, whatever it was,
your cousin was facing. But man, if they have any friends that they want to get into this kind of
thing, then at least that way you have like a brain trust to people that if you want to really
think through a hard problem, you're not just doing it by yourself, right? You can be like Harry
Potter in methods of rationality, starting his own basing conspiracy with Draco to try to try to
get some more sane educated wizards on his side. That's like 70% of the reason I wanted to do the
local meetup here. Oh, really? Not just for like, you know, getting, well, let me rephrase that.
It wasn't for that exact reason. It was, it was for the reason that I preferred about like getting
a brain trust to people that thought enough like me that I could bounce ideas off of and we could,
you know, but they could provide original content back like I can't do by myself.
I find that super gratifying and super useful if I had a problem, but you know,
really, it hasn't been so much of like a, all right, brain trust is my problem.
Has it, has it worked out? Has it helped you to have the lesser on group?
I'm actually just thinking about that. And probably now that I think about it, because
I started, well, I didn't start the group, but I, well, I helped get the local community going.
And it was around the time that I was just out of sight of my current life trajectory at my,
where I was at professionally with my job and, you know, where that was going.
And I'm not, it was, I was ready to like take the dive and do something else, but like,
I didn't really know what to do. And it was these people who were like, you know what,
you should give this a look into and check this out. And I don't know if I'll love programming
in 15 years, but I like it now. And if it gets boring, like there's only a thousand things to
do in that field. So I feel like I could stay busy there for a while. So yes, now that I think
about it, that could be definitely a consequence of doing this. So yay, that's good. Awesome.
Yeah. I hadn't realized that before. I'm glad things have worked out. I feel like a warm glowy
right now. Well, I'm hoping that helps other people too, right? Like, I mean, that was,
that's the thing about like, you know, it could just be a Slack channel if I wanted just ideas
to bounce the people off of, right? So I think there's some special about meeting people in
meet space. Agreed. That's, that's why I like that. So when you meet people in real life, you get
like a, you know, a real community that feels more personal because it is. Yeah. You meet faces,
you meet names, you get stories, that sort of thing. So yeah, I'm hoping it's valuable. I'm
assuming it's valuable to other people than just me because people keep coming. So yeah.
Excellent. That's all I got. Yeah. I don't have any other feedback.
Okay. Cool. I mean, there's other people who wrote in and we, I think between us, we probably read
absolutely everything that, you know, comes in on Reddit or through the email or through the website.
There was an interesting back and forth on one of the Reddit's about someone explaining why Harry
Potter, Canon Harry Potter is very classist and pro capitalist. I was like, Oh, that's an interesting
point you make there. And I had never seen it because I'm just living in that water, you know.
This was on the, on the Bayesian conspiracy subreddit. Okay. I gotta find this. Yeah.
That sounds awesome. Because I'm curious what the argument was. Yeah. Okay. That's it. Thank you
everyone for listening. We'll be back in two weeks. Thanks. Bye.
