I can see why someone who's taken the Hippocratic oath might be,
might come to Hippocrates.
But yeah, I think anything done pre-science is like kind of not really fair.
But even so, the scientific, I don't remember who pointed this out,
but the scientific method was invented over two centuries ago.
And well, over two centuries before we started washing our hands.
Yes, right. It took, I mean, it's, when you think about it,
the scientific method is the weirdest fucking thing.
It was basically just some people, most usually guys,
because they had like the, the extra time and money to do this sort of thing.
Just some people who were like interested in nature and was like,
all right, let's fuck around with nature,
tinker a bit here and there and see what we can find.
Whoa, that was fucking weird, I'm writing that down, you know?
And they just did this in their spare time for 200 fucking years
before we've got anything of use out of it.
And that is, first of all, that is a hell of a project.
And I'm really admiring them and glad that they did this
for so many years with no results.
But on the other hand, it did take 200 years of groundwork
before we got anything actionable out of it.
Yes, Scott said, he's not sure that we can short circuit
the spent 2000 years flailing around and being terrible step,
which I think is one of my hypotheses also for why we're not dramatically winning.
We just haven't had enough time.
We're the first generation of rationalists,
the enlightenment to the extent that such a thing can be said to exist
didn't happen overnight.
And maybe the most important thing that we could do
as the first generation is just keep building momentum.
Like if you think of rationality as a martial art or school of philosophy,
how do these get passed down and built up through generations?
And like even Harry Potter for methods of rationality was overconfident
in his ability to figure out the science behind magic.
And he discounted things like entrenched systems and stronger players.
So maybe we do need to get better at epistemic rationality before we can start winning.
And maybe the legacy of our winning will be that it took us 50 years other than 200
because we were able to actually build on knowledge of human psychology that,
you know, A, I guess standing on the shoulders of giants kind of thing,
but B, just not fucking around for as long, you know, going at it with real gusto instead of...
Or achieving more in like the same amount of time.
I mean, hard problems do take time because you do have to do a lot of hard work.
It's not just clever insights.
Yeah.
It's like, you know, elbow grease.
Yeah, a lot of the low-hanging fruit was already grabbed.
I wonder how much faster rationalists would have grabbed it
if cognitive science was introduced in 1600 rather than 1950.
I don't know.
I also wonder how much of us failing to win is just that we've solved a lot of the low-hanging fruit
and that we need to find a really important problem to work on.
Well, and I think Scott Alexander goes on to point out a couple of those, you know,
artificial or friendly artificial intelligence, effective altruism.
Those are some wins that kind of came out of this community.
Yeah, I think those are like just the fact that we're noticing them and kind of
we got the rest of the world to start paying attention to.
For example, AI safety is kind of a big win.
I think so, yeah.
I mean, I'm not that old, but I remember everything about AI used to just be terminator and stuff
and that still as a joke is, but, you know, now...
There was that thing with Google recently where they said that they weren't going to
fund the project of AI based military technology.
Yeah.
And what would they have come to that same conclusion, you know,
if we hadn't been proselytizing about AI being dangerous?
Yeah, that's a good point.
And, you know, we're getting big players on board.
You know, Elon Musk has a big platform.
You know, Bill Gates, Bill Gates has a visit.
Yeah, he's been out on this.
Yeah.
I mean, even Stephen Hawking in the last years of his life.
Yeah.
Hawking, Sam Harris, isn't really up there with those guys,
but he's, you know, to his podcast of a million people, he's made...
He's, in his words, made noises about this for the last few years.
I found it kind of frustrating that we haven't gotten the credit for that.
Yeah.
We know we've got the credit for it.
All right.
But that's one of those things that could increase awareness of the rationality community
and that epistemic rationality is a good thing to learn.
It's true.
That's why I think instrumental rationality is important.
If you do help people get measurably better in some sense,
people will take notice and ask, why did you, you know, what happened?
What caused you to get so much better and then we get more people on board?
Yeah.
Although, like, focusing on finding artificial intelligence is kind of like another,
like a bigger example of me not losing is counting that as a win, right?
It's not a public win because like, oh, look, we didn't destroy the world.
And it's like, who do we give credit to for this?
And so it's less, less obvious, but it could be the kind of thing,
like in retrospect, we look back and be like, oh yeah, thank God,
these people were doing that stuff back then.
Because otherwise you might have done this differently.
And we can view all the timelines.
We're like, oh man, look at what we dodged there.
Yeah.
In fairness, I would rather that the world be saved and we not get the credit for it
than we get the credit and the world not be saved.
So that's, that is my main focus, but yeah, it would be nice.
And it almost makes me wonder if we as a community should target like smaller problems.
You know, I mean, I don't want to take momentum away from effective altruism
or developing friendly.
That's the problem.
But I mean, if we could pick certain things to work on,
and then we'd actually have some measurable, you know, success
that we could kind of wave around like a flag.
Do you have any suggestions?
No.
Oh, okay.
Maybe that'd be a fun, fun project to think of some, some smaller goals that are,
you know, achievable within a few years that you could say, look, we did it.
But, but even then people would ask, did you do it because you were determined and smart
or because you're rationalists?
Because it's probably not the rationalism thing.
Rationalists are a weird group of introverts.
But I think I am with you guys.
That, you know, focusing on big things like EA and FAI are, are big wins, even if like,
technically one of them is just not losing.
You know, I don't know, EA is more of a, I'm trying to think that's, it's not,
I think to some object, objectories that might not be like solving a problem.
It's like, great, now actually cure, and I actually fix world hunger or cure malaria or something.
I have to really commend 80,000 hours for actually identifying the problems on the
effect of altruism side and prioritizing them.
Oh, me too.
I was just saying that I'm trying to think of the, of a cynical objector
might say, look, yeah, you guys pointed out that this is wrong, but like now do better.
It's like, we kind of fucking are.
So not wasting our charity budget is sort of a huge deal.
And advocating, you know, getting, successfully getting lots of people on board.
I don't have figures, but I'm sure people like
McCaskill and Peter Singer do of people who have been influenced over the last 40 years
by people writing about the sort of stuff to, you know, instingers words do good better.
Or wait, no, McCaskill, that was his book, doing good better.
Yeah.
Um, Scott did say one thing I wanted, one last thing I wanted to pull out from his reply
is that he compared instrumental rationality to self-help, which is kind of not entirely
inaccurate.
I would say it's, it kind of feels like a self-help thing, right?
Uh, I think that's where I was kind of getting into whether or not we should taboo it or not,
because, you know, again, the baseball player wearing the dirty jersey is self-helping in
that way, right?
Where they're, they're tricking themselves.
If they have to believe this mumbo jumbo to hit a home run, then that's what they have to do.
But you can't sell that to somebody else.
And so that's the thing with a lot of self-help things that Scott points out,
that this, this advice isn't generalizable.
And if it is, it sounds like, like woo, right?
Yeah.
He says that once you start becoming a self-help community, you start developing all sorts of
norms that help you be a self-help community.
And you attract the sorts of people who are attracted to self-help communities.
And then 10 years later, when someone asks, Hey, shouldn't we go back to being pure truth
seekers?
It's a very different community that discusses the answer to that.
I don't know.
Another one of my hypotheses as for why we're not winning is just that we kind of fail to work
effectively in groups.
Someone brought that up in the comments on Sailor Vulcan's post too.
Yeah.
Research shows that effective groups of average workers outperform geniuses,
which is kind of the whole idea behind why rationality could help someone succeed.
Geniuses who can't work effectively with other geniuses or with average workers in groups
actually perform worse than the effective average groups.
And we demonstrated that with AIs too, where in games, AIs beat humans.
But if you combine a human and an AI, make them work together, they outperform the solo AIs.
So I think if we want to win, that we do need to get better at this.
And I don't think I wouldn't want to just throw the whole self-help angle under the bus.
Is that a problem unique to our community?
Or is that just like in general, something that we've identified as a weakness,
and that we have the capacity as rationalists to work on?
Because I mean, a lot of people, I think a lot of industry suffer from like the,
no, I'm better at this because I'm the smart one.
Leave me alone.
Coordination is classically an incredibly hard problem.
And people have been trying to solve it for millennia.
I think we might be a little worse than most groups at trying to solve it though, because we're
lots of thing oriented people.
Yeah, we're more introverted than the norm.
And we're more like...
Well, we have a lot of geniuses and we have a lot of people that work solo
and don't like to work in groups.
And I think a lot of that comes from maybe the atheist community,
people that have deconverted from group thinking type groups,
are just naturally suspicious of wanting to do team building and form a community that's kind of church-like.
And I understand that.
But I think that we shouldn't let that come between us and performing effectively in groups,
considering that performing in groups is a really major hack.
Yeah, I think I agree with everything you just said that
this is something that we've identified as a failure of our community.
And we're smart enough to realize it.
We should be smart enough to do something about it.
It's hard to know what to do.
I mean, we are comprised of a lot of thing oriented people.
And someone at one point on the last rung had mentioned that was unfortunate phrasing,
but we really need more stereotypically feminine women.
In our community.
But what they mean is we need more people oriented people.
