Adventure time, come on, grab your friends, go to the very distant lands, Jake the dog
and Finn the human, the fun will never end, it's adventure time.
Hello, this is the Bayesian Conspiracy, I'm Inyash Brotsky, I'm Katrina Stanton, I'm
Steven Zuber and with us as our guest, Patrick Chapin.
And Patrick, the reason you're here is because you are the person I know who is most into
go and can explain the AlphaGo stuff to us and you also know a lot about AlphaGo itself,
but introduce yourself as what you do, what your effects piece is, that sort of thing.
Okay, my name is Patrick Chapin, I'm a game designer, I'm a professional card player,
game player I guess.
Which card game?
I'm a card game?
I'm a card game?
I studied into a number of other ones including poker, but largely Magic the Gathering.
Oh man, Nerd Alert.
Hey, no.
Absolutely.
The first time I met him, he said I'm a card player and right away this was when the poker
player thing was really big, all the poker players were winning and giving all their
money to EA, well not all of them obviously, but there were a number of big EA donations
from poker players and I was like oh, oh, cause you know at a less wrong meetup, he's
probably one of those guys that plays poker professionally, donates you know three-fourths
of the money to give well.
Yeah, you would assume that.
I would because he said card game and like people who play Magic say I play Magic as
far as I've noticed.
Sometimes it takes a little while to explain context.
You were in a group of nerds dude, everyone there knows Magic.
Dude, I don't know.
Okay.
Like it was a power.
If you say Magic and they're like oh, are you going to pull a rabbit out of your head?
It was like an hour and a half later and you were like oh wait, you played Magic professionally
and you won?
OMG.
So it was kind of cool.
Also, I'm sorry I interrupted you, but not card player, Magic player.
Much more prestigious in this particular circle of listeners.
Magic the Gathering Hall of Famer actually and two-time loser in the finals of the World
Championships.
Oh.
Wow.
You made it further than most people do.
Round of applause.
Oh wow.
Sorry.
I can find the sound effects too.
We can do like a cheer.
Thank you.
I also do video game stuff here in Denver including a lot of work on AI for some of the video
games we make.
Fantastic.
Any video games anyone might have heard of?
Yeah.
Unfortunately, the Art Shadow Dev works.
Gotcha.
Okay.
So we'll have to talk.
But we got some coming soon that we can talk about.
Eternal.
An online card game that I'm really excited about.
So sort of all over the place, but very passionate about AI and games.
Cool.
Okay.
Awesome.
So Patrick, we're just going to throw you right into the fire.
Do you want to explain the recent AlphaGo tournament and kind of give us a little bit
of background there?
Before we begin, the thing that when you explained Go To Me and why it was so difficult for computers
to do, I was really, like I have almost no idea what Go is.
I mean, I know like a few of the basic rules, but I've never played it.
So tell me this whole thing about Go being probability distributions and not knowing
how it's going to end until right near the end.
Yeah.
So Go has long been sort of the final frontier of at the very least perfect information games.
Go is relatively simple.
You just like, for instance, the way they play, there's a 19 by 19 board and then there's
just each side only has one kind of piece.
And there's very few rules to it.
It's actually very, very simple, but the complexity of the game is so just off the charts that
for the majority of time that people have had computers, it's always just sort of been
thought that it might be impossible to have computers be good at the game.
The combination, like the number of different possible board states and games of Go that
you can play on just a 19 by 19 grid is many hundreds of digits more than the number of
different chess games you can meaningfully or 40 move chess games you can meaningfully
play.
So it's it's far, far, far too complex to brute force.
And the strategy, so to speak, the way of thinking about Go is something that is very,
very difficult to put into words for or at least commands for a computer.
Go is often described as a very intuitive game, and you have to sort of just have a
sense of what moves to make and how it might turn out.
And you don't know the consequences necessarily of your actions, you're kind of giving yourself
better chances like you're creating regions, areas where you have some chance of hopefully
getting an advantage of some sort and you have to be able to change what kind of an
advantage you're going for.
But it's it's it's very hard for computers.
How long was this a goal for AI scientists?
Well, 20 years ago, the popular wisdom was that it was impossible to make a program that
could ever be a master level, like a high level human.
Ten years ago, people thought that hopefully by 2050, we would have technology that good.
A year ago, I think people would have said, you know, hopefully within 10 years.
But earlier this year was the first time that a Go program was able to actually defeat a
high level player.
And from there, they set up a match against the 18 time titleist and world champion,
Lisa Dahl, who has been kind of the most iconic Go player of the last many decades.
And that's the the Alpha Go match that recently got a whole lot of buzz.
So I guess this has been the Holy Grail since longer than people thought it was necessarily
possible.
But it really caught the world by surprise to suddenly be here, which we've jumped this
far ahead.
And you mentioned that there's various clouds of probability across the board as to what's
going to happen.
And you have to juggle those that it's really hard for even a grandmaster to tell what the
score is going to be, even as much as a few moves away from the end.
Yeah.
So the, yeah, I mean, it spirals into so many complex trees of possibility.
And it's so hard to know exactly how a region is going to turn out that like even the commentary
for Alpha Go, one of the two commentators in the English dream is the best Western, the
best Western Go player in the world.
And him trying to evaluate the position and figure out is this evenish, is this slightly
better for Alpha Go, is this slightly better for Lisa Dahl, and by, you know, how much
or how likely is it that this battle will work out for Lisa Dahl or Alpha Go?
It's there's a lot of judgment.
It's got a lot more people have sort of a feeling.
Who are the creators of Alpha Go?
So Alpha Go is part of Alphabet, which is functionally Google.
It's actually made by DeepMind, which is a company that Google acquired, like a company
that's been doing AI research for a number of years that Google acquired a couple years
ago.
And one of the people in DeepMind pitched the idea of combining a few different types
of AI systems, a few different types to, to play Go.
And this is, this is actually the 49th, I believe, game that this system, I mean, you
could say Alpha Go has played, but they only call the one that plays Go Alpha Go.
But this system that they've been developing, this is the 49th game that it has learned.
Wow.
It's a general learning algorithm.
It's not actually, they didn't teach them to go strategy.
That's part of the crazy thing is that, yeah, they actually didn't teach it.
That's the thing that, well, that's part of the thing that's so crazy about it is that
it has developed its own strategy and did not get strategic advice from the humans.
I read that there are kind of three different processes and you may be able to eliminate
us on these that they took towards teaching this computer, this program, Go.
They gave it different tools, the strategy, so it's interesting, they trained it a number
of ways.
They, they gave it a bunch of information for improving its, for improving its understanding.
The actual strategy that it chooses isn't actually, I guess I'll walk through it a little.
The first thing that it did is they trained it to identify what humans would do in a given
position and they let it look at millions of games, basically just an enormous number
of human games, tens of thousands of top level games, and I'm not even sure how many other
games, but they let it over and over and over again study sort of using the sort of technology
that allows facial recognition software to learn when a picture has a cat in it or not.
I heard that called a neural network.
Yes, yeah, it has a neural network that is, a neural network is like a series of computing
systems that are hooked up sort of like brain cells and the neural network can be trained
and can learn how to do things better and think about things better without you just
directing it, without you just saying, this is how you should think about it.
You know, like a traditional computing system, you just write out what the logic is.
A neural network can be used to learn about something in a way that is, like you give
it guidelines, but you don't know the way that it's going to end up thinking about it.
And in this case, it learns to predict what a human would do by trying various strategies.
Like there's lots of variations of AlphaGo that are slight mutants of each other that
some of them would predict slightly more this way, some would predict slightly more that way
and have lots and lots of different combinations of ways they would interpret something.
And then they let over time, it's like a general, it's like a learning algorithm that gets better
and better and better and they reproduce, like the ones that successfully guess,
like the part, the brain cells that successfully guess what a human would do are rewarded and get
to reproduce more. And over time, AlphaGo became substantially better at guessing what humans
would do than any other program has ever been just from the amount that they have given it to
work with. Now you mentioned something about there's two neural networks that work together.
So there's actually, there's two main AI systems that it uses. One is the neural network that
evaluates a board where it'll look at the game, the state of affairs right now, and predict what
the human would do. But there's a different part of it that predicts like the probability of winning
based on like what it's learned about games. But that whole system that just evaluates board
positions is only one of the two AIs and they're actually different AI systems that continually
message back and forth with each other over and over and over again. The other one is the policy,
the, it sets the policy, like what moves the computer will make, and it will suggest a variety of
moves. You know, one of the, the baseline is sort of whatever the human would do is sort of believed
to be the starting point. Like if a human would do this half the time, then you should, and that's
the highest percentage move, then that's your starting point of you is to assume that this is
what you should do. Because in general, the computer just assumes the humans are right.
But then it tries a variety of experimentation. It'll try, like it'll pick a random move that
it thinks a human would make, and then it will pick a move that it thinks a human would make from
the other position and just go back and forth giving a, doing a probability distribution of
how likely of a move it thinks it is. And then it plays the game all the way to the end, which takes
like, you know, a few milliseconds and then it rates that and tries over and over and over again
and continues to have the games that it plays in its head inform the, its evaluation of the board.
And each time it's evaluation of the board changes that helps inform which new games to
play in the future. And it does this back and forth tens of thousands of times in a matter of
seconds and decides what move to make. Sounds like some pretty beastly calculations. So how much
processing power is behind that? AlphaGo actually uses thousands of CPUs. I'm not sure how much
the processing power adds up to total, but it's a little over 2000. Like it's, it's very, very,
a lot of power. And actually part of AlphaGo, their team's theory behind it, they eventually
found that they could improve AlphaGo's ELO as a function of how much more processing power they
give AlphaGo. And it starts scaling very dramatically. Like you have to increase the amount of
processing power it takes to get the next 100 ELO points is much larger than the previous 100.
So it actually scales up in kind of a funny way. What does that mean for people who don't really
know what CPUs are? What does that mean in terms of like space? And just to clarify,
ELO was a ranking of how well do you play? Yeah, the, so the processing power for this,
think of it as like thousands of really strong computers and a couple big rooms full of just
computers. So it's definitely the type of thing that it isn't super practical to just download
to your laptop. So you mean like one of those 1960s calculators that was, you know, they would take
up a giant room, right? Yes, it's almost as powerful as your smartphone. As far as the ELO
rating and the number of CPUs, there's a nice little graph on the Wikipedia page for AlphaGo,
which will be linked to in the episode description on the website. We love Wikipedia.
It's great. Oh yeah, we defended Wikipedia a few weeks ago. Wikipedia is great. You know,
largely dominated by three different AI systems that are continually scouring themselves looking for
I had no clue. It's a self-learning algorithm, because what happens is they, when it tries to
predict, oh, somebody just vandalized something. And then you, the human says, you were right,
or you were wrong. And the algorithm warns over time how to predict what is vandalism or not.
And they've gotten extremely good. That's awesome. Just take that Wikipedia critics.
So is there anything else you want to cover for Groundwork or should we dive into the games?
I guess the biggest thing that's important to know for this new match is that the match
a couple months ago where AlphaGo, an earlier version of AlphaGo played against a former European
champion, that player is, so he's very good, but he's- Hold on. Let's give the thing a few
seconds. The match a couple months ago against the former European champion, he's a great player,
but he's several ranks lower. He's a substantially less strong player than Lisa at all. He's ELO's
lower? Yeah, like let's put it this way. If the two of them were to play, he would get several
stones. He would get a handicap of several stones. He would get several moves in a row as a,
just an advantage to try to make it more fair because Go is an extremely skill testing game.
So somebody who's a stronger player can require, they can overcome a great disadvantage in the game.
Someone told me that time is important in Go games, that the different moves are timed,
impacts your score. How does that work? So the rules they were playing under, the Chinese rules,
I guess you, each person has two hours of clock time and whenever it's their turn, their clock is
going down. And when they run out of time, they now have one move, I'm sorry, one minute per move,
and they also have three overtime periods. So they can basically, they can go over three times,
they get three bonus minutes, but then you, if you ever run out of time, you just lose.
Does that automatically benefit a computer player?
One might think, I mean, the computer never misses its move and always takes enough, like takes
however much time it has available, or that it thinks is a good idea to do, and that always
makes the perfect amount based on what it thinks. But as we may find out, humans have a way of,
there's a little bit of human ingenuity that not every computer is ready for.
So yeah, the match a couple months ago was 5-0 in favor of AlphaGo, and this was right on the
heels of AlphaGo defeating the best Go computers in the world, 499-1. And I actually ran it back,
it was 500-0 the next time, but played against the European champion defeated in 5-0, and people
analyzing the game afterwards were impressed that a Go program could be this good, but they were not
impressed compared to world-class players, and going into the match against Lee Sedol,
many of the top Go players predicted that it would be a landslide, that it would be,
there was at least a 95% chance that Lee Sedol would win, and when they asked Lee Sedol actually
before the match his prediction, he said it would be a landslide, he would win 5-0, 4-1 at most.
Sounds like they need some more practice assigning probabilities to their beliefs.
How did Lee Sedol's predictions update after being crushed?
AlphaGo got a lot better in the meantime.
Google tried telling them that it got a lot better, but I think he had a sort of anchoring
bias of how much could a human possibly improve in that amount of time, and he did not take
into consideration that it's not learning at the same rate as humans. Is that interestingly,
from what I saw, the less wrong community was substantially more accurate with regards to
their predictions of how things would go now, they did not necessarily nail it exactly, but I
think that their market was much better. As far as I know, what I saw, it was much closer to
a 55-45 or 50-50, where there's a lot more benefit to the doubt given that Google would not be
setting this up if AlphaGo wasn't ready. Let's go into game one, when everyone was still thinking
that this was going to be an even fight. Well, not everyone, the less wrong community, the humans,
many of the humans thought it was going to be a landslide for the humans. Game one, Lee Sedol
used a slightly exploitive strategy. There's ways that you can play against computer programs in
many games to get an advantage because of things that the computers are notoriously bad at understanding,
and he took a strat. He made a couple moves, one move in particular, early on, that it was fine.
It wasn't like it was a bad move. It was just not a move that you would normally make against another
world-class player, but he wanted to just punish the AI because no computer programs could ever
normally be good at this sort of thing. The end result was that they played a long drone out game
in which AlphaGo played extremely impressively throughout, and eventually that minor weakness
early on ended up causing him to lose. Lee Sedol actually was defeated by AlphaGo, but the match
itself was very deceptive with regards to AlphaGo's true capabilities, because even though it won,
it won by a tiny margin, and it made a number of plays near the end of the game that a number
of people watching thought were evidence that AlphaGo isn't actually very good. It was making
these weak plays, but in reality AlphaGo doesn't care about the margin that it wins, and once it's
actually seen a path, it will be contempt with whatever. It just wants to maximize its chances
of winning, not its margin of victory. So instead of going for one more point to increase the margin,
it will get it down to as little as possible if that will give it just 0.01% greater chance of
winning in the end. That reminds me of that Star Trek episode that I saw a clip of the old school
Star Trek where Spock and Kirk are playing chess, and Spock wins, or excuse me, no, Kirk wins,
and Spock's like, oh, I forgot to take into account that you would be irrational and stuff during
the game, and it's like, it's not irrational if I won, and that's where it is with AlphaGo. You're
right. It doesn't care. It's like, I won. It doesn't matter how big of the margin it was.
We'll talk about Spock at length later, but I didn't watch any of the games I'm planning on it.
Maybe once I get some more background on, I've only played like two or three games of Go in my
life, but I did read a lot of the commentary post per game, and everybody was saying, yeah, after
game one, like, oh, yeah, Lee's got this next time, no problem. Absolutely. The popular wisdom,
I guess, was there's no question this program has improved a lot. It's going to, you can
underestimate it. Lee Sedol's problem was the underestimated it. He also didn't play his
best game ever. He was just coming off of another match, and he didn't have the full amount of rest,
and there was every bit of making excuses, and every bit of, well, Lee's just going to tighten
up his game. He's going to play a much more, you know, he's going to take the things he's learned,
and they actually asked Lee Sedol after the game, what is your prediction of the match now?
And he said that AlphaGo has, it's incredible. He's very, very impressed. AlphaGo has improved a
great deal. However, he will beat AlphaGo most likely. His hope is that he thinks it'll be about,
you know, he said about 50-50 at this point, now that he's down a game, and they're putting
best three out of five for a million dollars, and Lee Sedol is getting 150k just for playing,
and then 20k for each win. And in game two, game two rolls around. This time,
the rolls are switched. And to give an idea of, and go, whoever goes first, the way that they're
playing actually has to make up a seven and a half point handicap, because of just how strong
an advantage going first is. However, because of the seven and a half point handicap, going first
is actually slightly worse. Seven points would be too much, seven and a half is too much the other
way, so it ends up just being like a tiny edge for whoever is going second.
And the second game AlphaGo went first. In the second game, AlphaGo is playing the slightly weaker
position, and they play out their game, and in move 37, basically. Just to be clear, the slightly
weaker position being AlphaGo went first. Yes. Okay. Yes. And the burden is on whoever goes first
to generate action. Whoever goes first has to make up this disadvantage, and so there's a
little bit of pressure on them to make things happen. Move 37, like this is, that is the moment that,
I mean, I feel like the number 37 with regards to AI is going to be significant for years to come
because of move 37 here, but move 37 was the least human move, like a move that was so inhuman that
it shocked the Go community watching. It was the sort of move that 14-year-old Go players know not
to make. I heard that Lee got up and walked out of the room since then, like 15 minutes
starting to figure out what the hell just happened. Well, I mean, at first he was just in shock, and
then he ended up running to the restroom, and I don't know, they said he was just washing his face
and trying to get his composure, you know, like, like it was just sort of a, I don't know what's
going on, how nobody would ever make this move. And in fact, AlphaGo said that there is a less than
one 10,000 chance that a human would make the move, which it takes an enormous amount of evidence
to make AlphaGo want to overcome that bias. However, it was very confident that it had a better
move and that humans would not make it normally, but it thought it was better. And part of the way
that AlphaGo evaluates these things is that it plays, it does Markov simulations over and over
and over again, where it'll play out the game with many, many different possibilities and then
average them together over time to try to get an understanding of the position. It, you know, in this
case, found a path that it was confident was better than what humans would do, and it knew that the
human, you know, they knew just looking at the readout, oh, they're going to be, and you can
actually see the guy, the, one of the, the Google people who was there making the moves,
who's a very high, very strong go player in his own right, the, the rise smile on his face when
he made the move. And actually the commentators watching were just like, sure that this was a
mistake, like in some way where they were just like, what? I remember since none of us had this
information at the time, I was like on a 15, 20 minute delay, but when I heard about it, that's
when I first texted you. And I was like, so is this a glitch in AlphaGo? Did they just bug out
on us? Like no one knew, they couldn't say at the time. And over the course of the next, you know,
for the next hour or so, people were convinced, well, this is evidence that AlphaGo is not actually
that good. See, see Lee Sedol is going to win for sure. And then over time, it was like, well,
based on our sort of heuristics, our sort of evaluations of the board, it seems like Lee
Sedol is winning, but it's really confusing. And then it started seeming like he was winning by
smaller amounts. And eventually they're just, you know, they're asking the guy to, to, you know,
kind of score, like, what does it look like the score is? And he's like, I, I don't know. I mean,
I mean, I guess Lee Sedol is up by conventionally, you would say Lee Sedol is up by like 20, but like,
there's this, this, this, this, this, there's a bunch of different areas that are very confusing
and unresolved. And so depending on how they turn out, and it's just very strange. You know,
I think at one point he actually mentioned, if AlphaGo actually wins this, it is over for, I
mean, it's like, there's no way that this can be like, if this is Go strategy, then I don't even
know Go strategy. I remember a Lesseron commenter saying that this may be evidence after, after the
win happened, this may be evidence that humans just can't play against machines. It's not a, a,
the same level of thinking. Yeah, this is the one where it's like, if this is actually a winning
line, then I don't know that there are any good human Go players. So long time later, eventually
it becomes clear AlphaGo is going to grind out a small advantage. And he even made some slack moves,
some moves that seemed weak, like in that earlier game, that were actually just evidence, you know,
at one point it was like, see, maybe this means that AlphaGo is just off its rocker. And the other
guy's like, no, actually this, that's evidence that AlphaGo has already won. And the, the end
result was Lee being defeated by, by this very strange position. And with a move that no human
has ever made, I mean, nobody could see that. I mean, it was just so bizarre. It's actually very,
very strategically sound. It's just so like the humans making new moves will often build on variations
of patterns in previous games and try minor experimentation. This was so different of a way
than humans would normally play that humans is very unlikely that human would just try going
down this path because it's so unusual, so unlike any previous patterns. And after game two, they
asked Lisa at all, what is your prediction of the match now? And he said, it is my greatest hope to
win a game. Can we fast forward to game four? Absolutely. Well, does game three have anything,
does game three have anything? I was, I was really eager to hear about game four too,
but I want to, let's game three have a lot of highlights. Let's do game three too. Yeah, no.
So game three, and I'll make it more brief, I apologize for implying on, AlphaGo is a hero of
mine. I am intensely interested. Game three, Lisa actually just played a very, very,
like two strengths and played a very, I'm not going to try anything crazy. I'm going to play a
good disciplined game. I'm going to, and he tried steering AlphaGo in a direction that was the sort
of going towards the very, very nebulous situations where it's just too hard to calculate what the
end result is. How does AlphaGo do in a situation where AlphaGo has no chance of being of the brute
force? And AlphaGo just played a remarkable game that was, it just seemed like despite the fact that
Lee Sedol played a nearly flawless game, like just an absolutely incredible level game, he still
lost. And it was like afterwards they asked, they were like, so what, what did he do different?
What should he have done different? Where was his wonder? And they're like, I don't even know that
there's anything I'd point to that it's like I would do different in the future, but it just ended
up in a bad spot. Like how can you, how can you know? And it was, it was just, you know, and Lee was,
Lee Sedol was devastated, but it was clear that the computer could in fact play from situations
that require sort of an intuitive estimate of what it would look like. And then we go to game four.
So at that point, after game three, the computer had won, AlphaGo had won, because it was the best
three out of five situation. And that meant that how much prize money went to Google?
Actually, Google donated the money either way, but they donated the million dollars to charity.
Just a million dollars.
Yeah, actually, one million and 40,000 or something. It's also 20k per game.
And then the last two games are just for 20k, but Lee Sedol is playing for pride. I mean, this is
like, this is just blowing his mind. We actually counseled with his teammates and friends who are
some of the best go players in the world to try to devise a strategy. They had a little bit of rest
and that, you know, they were trying to come up with how can they possibly do this? Like how can
they possibly try to take a game off of AlphaGo? And he, he actually game four was truly brilliant.
He radically changed his strategy. He, he actually did something that a world-class
go player often would not want to do, which is make the game very all or nothing.
One of the things they realized is that you can't just grind out little advantages
against AlphaGo. You eventually lose and they try to make it instead. Let's just
make it all or nothing where there's going to be one big battle and whoever wins that
will just win decisively the match instead of having it be up in the air, which is a risky
approach, but one that they thought might be given better chances. The, uh, they also realized
that AlphaGo takes the same amount of time, no matter how much time Lisa doll uses.
Actually, after he had already lost game three, he stayed playing for hours just with no clock,
but just studying or like trying to learn more about AlphaGo. And they realized they theorized,
I bet if you just use all your time early on, AlphaGo will not speed up. It'll still take
two and a half minutes per move or whatever. And you can think during AlphaGo's turn.
And so Lisa doll actually used all of his clock as to how he used it all up,
including 35 minutes, his last 35 minutes in the three moves prior to his master stroke in game four,
where he wanted to be sure that he had thought about as much as possible on his clock. And then
he knew that after he pulled off his, this big trap that he had been setting,
AlphaGo was going to have to reevaluate its whole understanding of the game. And he didn't want it
to be doing that. Yeah. He wanted to use as much time to think beforehand and then think on AlphaGo's
time. He set a trap that involves several pieces of the trap being many moves apart so that AlphaGo
would not anticipate them near each other. And he exploited, like he studied some actually about
how Markov simulations work. And they studied a little bit on how to try to find weaknesses
in the thought process of a computer that uses Markov simulations. And one of the things they
realized is that if a move is near lots of really, really bad moves, if it's a long,
convoluted chain that is surrounded by lots of really bad things, it's possible that early on,
you know, if the first thousand times it starts looking down this branch, it looks really bad,
it stops looking down the branch very hard. And so if you make a long, convoluted chain that
involves lots of really bad decisions in the way, it can sometimes miss it. And he made a move that
one of the best go players in the world described as the hand of God, making a move, move 78. And it
was, it was, I mean, it's obviously somewhat hyperbolic, but it was actually from AlphaGo's
estimate, less than one in 10,000 chance that a human would make that move as well. It was actually
it was the most surprising thing that Lisa Dahl did in the match. And it was a move that was,
it was off its scale. It was the, you know, where it could not imagine that a human would do this.
And as soon as he did it, the people watching were just like, that's brilliant. They didn't
know how it was going to end or where it was going to go. But they could tell you would not do this
unless you were setting up this, this, this big plan. And it, it looked like it might not,
it might, it looked, it's actually the next 10 moves after it looked like AlphaGo is making
blunders. What's going on? Why is AlphaGo making these slack moves? And they realized AlphaGo thinks
it's winning by a lot. It's playing so conservative. It thinks it's winning. And then 10 moves later,
suddenly AlphaGo's percentage chance of winning the game, according to itself, plummeted rapidly.
What happened? Why, how could it possibly have dropped that much that fast? Like,
it went from like 70 something percent to like 28 percent or whatever. What happened?
And AlphaGo said 10 moves ago that it blundered. It just didn't realize it. It didn't understand
the position 10 moves ago. And it was too late now. And then it started playing desperate where
it started doing crazy things to try to like the type of move that if it works out, it would be a
huge win, but it's never really going to work out. It's just too easy to see partly because it,
it thought it was losing by so much that it started getting very, very desperate for
what moves to make. But it also didn't take into consideration, Lee Sedol has no time on this clock.
If the computer would have just been playing quickly, it would have, it very likely would have
beat him actually very easily because it was a very complicated game that went on for over two
hours with Lee Sedol having no clock where he just was just having to just play. And it was like
an incredible test of his constitution, but also the computer just not understanding that it shouldn't
take the full amount of time every turn to think about its moves. Okay. So can the developers
fix that and what caused the blunder in the first place? Yeah, absolutely. I mean, they,
they definitely could never even occurred to them to have Alpha Go change how much time that it takes
based on the other person's clock. You know, Alpha Go, one of its flaws is that it always
thinks that humans are making decisions at peak human level. Like it doesn't evaluate, oh, you've
been making some mistakes recently, or you're in an emotional state where you might be compromised,
or you don't have any time on your clock. It actually just assumes that humans behave
sort of like a machine. So on the chain of bad moves and the kind of traps that it laid,
that that Lee Sedol made laid for Alpha Go, is that the kind of trick that a human Go player
of Lee Sedol's level would have felt fallen for? Well, what Lee Sedol did was a play that was brilliant
at any level. I mean, it would be very, very difficult for a world, like world class player
if once they're in that position to, to defend against. But the problem was that Alpha Go
actually started making crazy, risky moves against Lee Sedol, and no world class player
would fall for the things that Alpha Go was trying. Alpha Go just thought it's so desperate.
Let's get some humans sometimes fall for this. And so it's like, even though this is a small
percentage chance, it has seen that some humans would fall for this, but not Lee Sedol. Gotcha.
Wow. Part of that was just that it thought at first, it thought that it's winning by so much
that it can be just giving away edge, because it's just playing conservatively, where it just wants to
if you willing to give up 10 points, because it's like, oh, this gives me a 2% chance better
of winning, but it just didn't understand the board position. And then later it thought,
I'm losing by so much. I need to do crazy things to try to win.
So Alpha Go is very much less good when it thinks it's winning by a lot or by a little and is wrong.
That's real. I just want to quickly say, listening to you tell this story, it's hard not to,
to throw my hopes behind Lee Sedol. And you know, this is such a John Henry driving railroad
ties story. Are they called railroad ties? Yeah. Yeah. Yeah. Where you just really want him to
win against that steam powered tie driver. But so Lee won that fourth game. Can I go back to your
question? When she asked that, you know, the human programmers could program around that.
They're not going to change it in the middle of the match either. No, no, no. Just not in the middle
of the match. But wouldn't, isn't that kind of, doesn't that feel like cheating? Like,
isn't it something that you would want to make an AI that can recognize that and fix it on its own
rather than humans having to come in and patch it? Long term. Okay. But, you know, this is,
this is just one more step along the way towards the, like my real question isn't,
can we patch the problem? The real question is, can we make an AI that can self modify so that
it sees this problem and fixes it? Yeah, it's not so much that we would patch the problem,
because that's not how it operates. It's more of a, we would give it another access that,
another access that it can explore for determining how to fix itself. Because right now it doesn't
even have the ability to change that part of itself. But absolutely, it needs to be able to do it.
It's so unlikely that a human could just predict the right amount, the right patch. So to speak,
it's just much better to let the neural network build its own patch. The fourth game, though,
was crazy because for like an hour, an hour and a half, it's like, okay, Lee Sedol is definitely
winning. He has this game locked up and the computer is doing crazy, ridiculous, awful things.
Lee Sedol has to have the game won. And the other guy's like, you saw what happened before. We
thought it was crazy. This has to just be some genius master show. He's like, there's no way.
There's no way. There's no way. He would never, this is just horrible. And then at the very end,
eventually, AlphaGo resigns. If AlphaGo's probability of winning drops to a low level,
it resigns because it just starts playing more and more embarrassingly. And they're just like,
let's just get it over with. It's below 20% I think? Yes. But it's also kind of a weird 20%
because it's 20% among humans. And it starts to spiral out of control. Because as we've seen,
when AlphaGo thinks it's losing, it starts becoming more and more desperate because its only goal
was to maximize winning, not avoiding looking foolish. And so after game four, the fact that he
actually won, he's already a national hero in South Korea anyway, but this is like this great
redeeming moment. And actually he had in game five, he could have had the privilege of the slightly
better position, but he actually requested the to play black because he had already won. The one
he won, he won with white. And he wanted to, he actually asked Google if it would be all right,
if he played from the black side, because basically just as a sort of pride, like I want to try a
chance, partly AlphaGo is better from the white side. They realized is that a start side? Which
side is the start side? So black goes first. Got it. So it's at a slight disadvantage. Yeah,
it has to make up a seven and a half point handicap. And it is at a slight disadvantage,
but also AlphaGo exacerbates this. Like AlphaGo is very good at defending. And so they play the
fifth game. And the most remarkable thing about the fifth game, because there was a question
watching it is, is Least at all just going to demonstrate the loop? Like is he just unbeatable
now? Is he just going to beat the computer? Or was he lucky? And Least at all immediately adopted
a very, very aggressive attacking position and took lots of territory all over. Because after
analyzing the fourth game, he realized AlphaGo has a weakness in understanding when, like when you
take lots and lots of territory all over the board, it loses track of, like it doesn't have a
great understanding. It doesn't have as good of an understanding in, and this, this is not the
kind of game that Least at all would want to play normally, but he can play any style. And
in the early game, it seemed as though he was just routing AlphaGo. They were actually wondering
if AlphaGo is going to resign because he looked like Least at all was winning by so much. Now,
AlphaGo actually did not believe that it was losing by as much as the humans watching believed
that it was, but it thought that it was losing pretty clearly. And it would, but it's one of
those Least at all's plan worked marvelously. He gave himself a 60-40 edge. And in the end, AlphaGo
got its 40% and got there. Because even if you pull off a 60-40 edge, it doesn't mean you've won.
It means you are 60% to win. So in the end, 4-1 in favor of AlphaGo. And that one may be the only
one ever that AlphaGo will loses because I got a feeling that when it plays against Guli in the,
the best Chinese player in the future, which I assume will be the next match they set up,
I do not think AlphaGo is taking any losses.
So do, there's a lot to process. I'm trying to get to the, I guess, the next
way to look at this. So I think, is Google going to go and do any revamping whatsoever?
Or any, any, any, so when is the, anytime scheduled or announced yet?
Okay. So they've got some time to, to make sure that's not vulnerable to the same maneuvers that
Least at all was utilizing. It's not so much that they're making sure that it's not vulnerable.
It's more of a, they've warned lots of new places to let the neural network be able to explore and
try new things. And additionally, it's going to be thinking about the game more. AlphaGo gets better
all of the time. Like it, it thinks about the game a lot. And part of that involves playing
millions and millions. I mean, it's only played about one human lifetime worth of games against
opponents, but it plays against itself in its head a lot. It has literally nothing else to do.
Very, very little else. Once in a while to do an exhibition against the world champ, but
it mostly just thinks about go.
Why are we having AI systems play games? Why is that really important to be happening now?
So AlphaGo is a general learning algorithm. It wasn't designed to play go. It was just designed
to learn and think. And that's part of what's so fascinating about it is that like, for instance,
when some of the previous games that AlphaGo learned, AlphaGo learned how to play, the deep
mind learned how to play Pong. And it's just the best Pong player in the world, which is kind of
bizarre to watch it play because it doesn't play like a human would. They had it look at every
YouTube video on the internet and identify which ones have cats, which ones have humans, and what
are all the cats and what are all the humans. And it's kind of crazy the amount that this same
system is learning how to think about things, like it's figuring out how to make relationships
between things. It's a very intuitive program. I mean, it's properly described as intuitive,
and some have described it as empathetic. It's continually trying to understand the world view
of the other actor, like what are their values? What are they going to do? What are they? And
the hope is that this can be used in any number, and basically every other area, everything from
optimizing street lights to where to investigate for medical advances to where to drill for oil to
how to efficiently move resources around to where to build a new power plant. Basically,
just things that people think about. The hope is that systems like this will be able to think
about them better than a human would be able to and accomplish these tasks better. It would be
great if AlphaGo is better at Go than any human in the world. That's great for winning a Go,
but imagine if we had something that was that much better than any human in the world at
figuring out what combinations of chemicals to put together in order to cure some disease,
or how to lay out a city so that there would be less traffic or anything like that.
Or how to do accounting really well. Absolutely. So why is it playing?
That's going to be my next question. How much longer till we're all unemployed?
Unemployed is kind of an interesting thing. I mean, sometimes when you don't have to carry
the heavy load yourself, you're a little bit more free. Is it that you're unemployed if they don't
need you to drag the plow across the field, that the robot does it for you, and then you get to
spend your time recording podcasts? Why are we so concerned about this computer playing Go
when we could be more concerned about it taking Inuyash's job and doing accounting,
which might be, which would be significantly more useful. Sorry, Inuyash.
That's true. I don't know, it depends. I think different people are concerned about
different things. I would guess he's more concerned about his job being taken. But
I think that the big thing is this AI is a big step forward for, I think super AI is the real
thing on the horizon. But there's also just some of the AI systems like recently,
it was revealed that there's an AI system called Skynet that evaluates people and determines things
like who the drones should go after. So it's really important to know the state of affairs of
where AI is and what we can do to improve it, because one way or the other,
AIs are continuing to get built and will be stronger and stronger in the future.
Would it be fair to say that we're not at a place quite yet, we're at a place where
human go players can be beaten by AI, but human accountants cannot be replaced by AI yet?
That we don't have that technology, that Go is kind of at a different level?
We did, I wouldn't have a job right now.
Not necessarily.
A big important thing here is the difference between
perfect information games and hidden information games. Accounting, funny enough,
is sort of a hidden information game. And Go is sort of the final frontier of perfect information
where it's just like, okay, this is clearly computers are just better than humans at games
where all the information is known, where there are concrete sets of moves you can tell when the
game is over. When things involve hidden information and undefined victory conditions
and endpoints, that's, there's a lot of rich area to explore in that area. And well,
AIs will continue to replace humans in different areas. It's hard to predict which ones are going
to get replaced first. And I don't think that there's any reason to believe that accounting
is next on the chopping block. So the trillion dollar question is how long until deep mind or
a similar AI program is better than any human at designing AIs?
Well, in some ways, there already is.
Well, so it deep mind, I mean, the neural network is already
incomprehensible to humans. Like, why is AlphaGo making the decisions? Like, how is AlphaGo
doing the things that it's doing? It's not clear. It's tried lots and lots of combinations. And
there's like this sort of, it's almost like a tower function that maybe maybe humans are higher
up on the chain where the humans are still the ones who can unplug it and the humans are still
the ones directing it. And maybe it's not too far in the future for when AIs will be better at directing
it than humans. Because already we have AIs are much better at than humans at making a specific
type of AI. Like if the AI, you need to come up with some artificial intelligence for a
particular thing. And AI is much better at figuring out how to do that thing in general,
for at least for perfect information systems. Now when it comes to just making AIs in general,
I think that when a computer is better and has the direction, at that point, there isn't really,
there isn't that much game after that. Like, all of the games before that, because at that point,
the die has kind of been cast. You mentioned when we spoke about this a couple weeks ago,
that your estimate for the timeline of this draped radically in the last couple of weeks.
This is the first time that my, my, I guess if you could call it a, I think that the most likely
scenario is that the AI will be in charge. The AI will be, there will be a super AI and it will
functionally be God and sovereign. Yes. Do a little callback. We did a, some homework to look up some
of the technical definitions that LAIs are brought up in our last, or I guess in episode three.
And before you thought this wouldn't ever be a thing that could happen? No, no, no, I thought it
would be, but it's more of a, instead of, instead of, you know, by the end of 2040, the 2040s,
maybe a little closer to the beginning or possibly even the late 2030s. I don't know. This is such a
unexpected jump forward and the system that they've described is so widely applicable. Like, so
the, the, the, the two different styles of combining the neural network, you know, the,
combining deep learning with the, the look ahead ability that it has. I think that this is
really decreases my confidence that I can even meaningfully be able to predict when
certain things are going to be unfolding because it's going to happen very, very radically and
likely by surprise. I have a question for all of you. Okay. Are you frightened?
I've always been frightened. This moves my frightness up a little bit, but I guess that
means I'm a little more frightened since then we have less time to, to figure out the friendly AI
problem. You, I remember you were very pessimistic when we talked a few weeks ago. You were kind of
pessimistic. I don't know if pessimistic is what I would say. I would say you thought that we did
not have enough time left now to do it. No, no, no, that's not what I'm saying at all. No, no, no,
I'm saying that there isn't a, I would say if you're describing pessimistic as a funny word,
because I wouldn't even describe myself as frightened. I don't think there's a ton of utility
in the fear. I would say perhaps motivated. I think that it is unlikely that one could
stop there from being AI, like from being a super AI eventually. I think that most likely the best
way, if that's your only goal, the best way would be annihilation. But I think realistically there's
going to be, that there's going to be super AI. I, I don't know, I got to take the position
though, just play to your outs. Like, we have an opportunity right now to help shape things as
best as possible. I also think that some of the factors that go into whatever that world is
are so incomprehensible to us right now. I don't overly worry about it, because it's very easy to
imagine how scary the world one might be. But I can't possibly evaluate the decisions that are
being made in that world. It's, it's like looking in, you know, like an ant can't evaluate the,
the position, like the decisions being made by humans in a meaningful way. And instead, I think
that you kind of just, when you're the ant, you evaluate, are you going to take this little
kernel of corn back to the, you know, to the colony? As people though, we have the opportunity to
affect policy. We have the opportunity to sway the decisions that corporations make. And if more
people maybe have their nervousness meter up a little bit more, that could pave the way for
more research into friendly AI, pre super AI. I wanted to say from your initial question that my,
my anxiety level and my eagerness level both go up. Because, well, because there's a lot of ways
that could go wrong. You know, even, even if exactly they're, they're way more ways that go
bad than it can go right. So my, my thinking is that I guess I personally don't like being left
out of the loop, even though that's that kind of has to happen of, of even a good future, like
it's not going to consult me first. And see, actually, that's one of the areas that I think that it is,
we don't know enough to be so sure. I mean, how do you know that you're not going to be able to
have an increased level of awareness and understanding, like for instance, being augmented
by AI or having all of the, like think about how much knowledge you have access to now that you
could not possibly have had access to 15, 20 years ago, not only, you know, growing up and having
all the experiences you've had, but also the technology, the fact that you have a smartphone
and can just get 99% of the world's knowledge at will. It's, it is fun to reflect on that
anywhere that I have cell phone reception, I have access to more information than the
president of the United States did 20 years ago. Absolutely, more than existed. Yeah. I mean, like,
so I think that if you just look at just how much power, how much knowledge, how much
awareness and understanding you have at your disposal now, there is no reason to think that
that there can't be futures in which you continue to increase your awareness and understanding.
And even if you don't, at a certain point, there's also the
your children carry on like this, this is the philosophical thing that I wanted to ask you
about because I saw Hugh Howley, who wrote the world series was recently posting about
how right now we are watching our baby AI grow. Yes. And every time she does something like beat
her chest, chest, grandmaster or go grandmaster, it's like, look, she took another first step.
This is so amazing. Why is it a baby girl? I saw the headline. I saw the headline.
The buzz, buzzwords headline and assuming because he thinks baby girls are cute. Sorry, women,
it's a baby boy from now on just to be clear. How about just a baby? Why does AI have to be
gendered? Because ships are ships are girls. Whoa, we can talk about that another time.
Isn't that the thing our vessels named after traditionally girls? Oh, good. Having, having
never or female, I guess, right? Having, having never been on a proper example of something sexist.
See, I actually think that part of the reason why they default that way is the wanting to help
bias a way for like, there's been a historical bias where a lot of males will design systems or
tech and just talk about it in very masculine language that is not as inclusive as with the
use or like ships. A lot of times they talk about it with with female pronouns and as female entities.
Ah, yes. Like the race, like, for instance, describing their baby. They're, you know,
this is the thing I made and speaking almost diminutively, right? I mean.
Right. I think he was trying to make it really cute. And so, you know, you, you call it a baby,
humans are used to babies being gendered. So he was going to have to pick a pronoun.
I don't want to derail this. I really don't, I don't want to derail this conversation to be about,
about gendering things and things being biased in favor of gendering versus not. So
let's move on. That aside. So I, yeah, I wanted to ask about the whole children thing because
there's a lot of, a lot of thought along the lines of, well, this is the next thing we make.
This is the next step in life on earth. And so these are like our children. But I personally
do not really want to die ever. And so I don't want to become irrelevant either.
So are you okay with changing?
Yeah, I'm totally okay with changing. But isn't there a theoretical limit to how much a human can
change before they're not human anymore? I mean, are humans today humans the same way that cave
people were? No, not, not even a little bit. Is it okay? Like even though the cave people might
be really attached to the way that they were, it's pretty easy to imagine that it's better,
humans now are better than the way that cave people were 100,000 years ago.
It is. It's really weird to just recently read an article about this tribe that doesn't have
counting, like one of the few in the world that still doesn't have counting. And you read about
their way of life. And it's so bizarre. I literally felt like I was reading about a different species.
I was like, I, I cannot empathize with these people as humans anymore. I empathize more with
my pet dog because it was, it was the most bizarre thing. And yeah, no, we, we are significantly
different. And are we losing something? Can we even become a different thing and keep ourselves?
Yes. Well, I just wanted to, to give a quick two cents answer on that, that I'm thinking
if I were a cave man from 140,000 years ago, and I could have it sufficiently explained,
I probably couldn't have it sufficiently explained to me what it would be like to be
a person in the 21st century. But now if I could, if I, as that person I could grow up and to be
that person for the 21st century, I would look back and say, I'm so glad. And now it's easy
having taken that perspective, I can imagine in some, some future looking back and saying,
I'm sure glad I changed. Yeah, but I think that may just be accident of birth. Like the cave man
would be like, these people never go chase after game and throw spear. Instead, they sit in front
of glow box and manipulate pixels on a screen. Where is the glory in manipulating pixels on a
screen when you can murder a mammoth? You know, but now I can shoot fireballs from my hands in
Skyrim. So, you know, in the future, the future, it's like, you were so content to throw pixels on
a screen. In the future, you can literally just rearrange atoms in a solar system to build things
and do things like work on problems that actually matter, like heat death of the universe.
Yeah, but can we? Or is that something so beyond humans that only a something so alien that you
don't even recognize it as human anymore can do? That's, I guess it ends up just being, what is the
important thing? Who cares? Who cares? I don't, we're all different one day to the next. I don't
think it's very important that that humans continues. And I think that, well, that's,
that might be overstating it. I mean, in the very long term, I think that's natural. If you want,
if your argument is, oh, we're going to be losing something, we always, we always lose something
through the history of the universe. There's always been change, right? I agree with you
entirely. If I could change, if I could change into that, I'm totally okay with changing into
that. I'm just worried there may be some level where my neurology cannot change anymore.
So who do you think is in a better position to evaluate this?
Well, assuming future AI's than me, yeah, but there is, it's theoretically possible that I
cannot be modified that much, right? Yeah, it's theoretically possible that you spontaneously
combust. Who cares? Fine. Okay. Right? Like, I mean, I mean, we care because we know you and like
you. Yeah. But no, I'm not like, who cares is in who cares about that tiny probability? Oh, yeah.
Like I'm talking like, do you really think it's a tiny probability though? I'm saying that it's
not like if you spontaneously combust, it's not like if you spontaneously combust, it's not like
you're like, well, I shouldn't have let some of these atoms move in this place. No, you could have
done about it. There's no way a human could think up move 37. That's not true in game four. He thought
of move 78. move 78 was as inhuman and as improbable as move 37. Way to bring it back. So brilliant.
That's what makes us so brilliant. So I for one, as a human, I'm very proud and excited about AI
babies. And I and I'm happy to hopefully they're going to grow up to be good and kind and wonderful.
And I'm happy that they'll be taking over the world. But as long as we program them to be good
and kind and wonderful, I don't come around in this whole don't be evil thing. That's a great
like when you first you got to define evil for the machine, though. I define itself. I'm worried
about that. And as you should be, I totally agree with you. So to bring things kind of back,
back, back, back to rationality. So we are all members of Denver area less wrong. And we are
broadcasting from Denver, Colorado, just some back information. And we're very concerned with
rationality. What is the connection between rationality and what Eneash and Steven, you
and I have been talking about on this podcast for the past few episodes and AI. AI's are,
if I can give a quick sound bite, the ultimate, the ultimate rational agents. So we're where
humans can be aspiring rational agents, we are still built with all of this weird ape programming
that worked great for hunter gatherers on the savannah, but doesn't work great for
optimizing decision making. And AI is, you can build from the ground up and not include all that
baggage. I think the the main connection is that if we're going to eventually make some sort of
superhuman thinking device, which will more or less become a God, we want to design it so that
it thinks well, and so that it values the same things we do. And that rationality were the first
tools to try to explore, well, not the first tools, but they were a major step forward in trying to
explore what makes thinking, how to codify thinking in a way that it can be programmed into a machine.
Not just, you know, intuitive thinking, but actually breaking thinking down into math,
and probability distributions, and how to make a machine learn it. That's a good explanation.
Yeah, I mean, that's that's actually part of the brilliance of AlphaGo's system of thinking. It
isn't just that it has a neural network that that can evaluate these positions. It thinks about like
it has an idea, you could say, based on, you know, just let's let's roll some dice based on what we
think humans would do, play it all the way to the end and see how that idea turned out and continue
to change its values. As it thinks about the problem, change its values and steer what sort of
ideas it has, based on what sort of positions is it getting to, and are we getting warmer or cooler?
And in many ways, it's like when a human thinks about a problem, and can feel like they're making
progress, like moving closer to the answer, even when they don't have, it's not like a partial
credit thing. It's like, this is the right place to be thinking about this is the right access to
be thinking about, you know, what surprised me is all of this neural network and kind of
natural not natural, but it's artificial selection way of developing the artificial intelligence
AlphaGo. I was kind of surprised that we used things that are already in place, evolution,
brains, human brains. One of my favorite, I guess, adventures with computing involving evolution
is a microchip that is self like, like a genetic algorithm, like it improves itself over time
for solving particular types of problems. And the people who were working on the chip as an
experiment, they were trying a variety of ways to effectively stress test the microchip and they
gave it a problem that the human quote unquote solution involves thousands of logic gates.
And they only gave it 100 just to see what would happen if it were under these extreme
conditions. And it was effectively when a sound is played, if the computer powers up,
that's a success. And if a different sound of a different frequencies played and the computer
powers down, that's a success. And then they just seeded it with 50 random combinations to see,
and then let it evolve over time where it would take different, some of the combination, some of
the seeds and reproduce depending on the success and, and continue to evolve and the microchip
would just evolve itself. And over time, and it took a while before the microchip was even like
responding to stimulus, like, real reliably, there was a lot of noise for a while. But eventually
they get to a spot where the microchip is actually producing the correct answer more than
50% of the time up and more than 50% down. And they let it play more, you know, hundreds of
generations, hundreds and hundreds of generations go by. And eventually, it's actually pretty consistently
getting the right answer. And eventually, after some number of thousands, it stopped
meaningfully evolving at all. And they were just like, Okay, let's take a look. It's done cooking.
I can't believe it actually solved the problem. And they look under the hood. And, and then to that
point, the guy leading the experiment was about ready to punch the other person or
experiment for for trolling him for like, why would you pull a prank like this on me? This is
just this is not funny. And he assured him there was no prank. And the security cameras in fact
affirmed there was nobody tampered with it. He he's like, this is this doesn't make sense. He
looked down. First of all, I was only using 37 of the 100 logic gates. This doesn't even make sense.
Second of all, 32 of them are are in one corner, where the power is, the other five are just in
a circle in the opposite corner, not even plugged in. What in the world is going on in the 32 over
here involves two little loops in opposite directions of current and a few logic systems.
But it's like, this is baffling. Nobody would ever make a microchip like this. This doesn't even
this is, this is nonsensical. And they looked and sure enough, it continued to work. And he tries
this as an experiment. He's like, Okay, first of all, let's take these five out, because he must
have just grown some vestige that wasn't necessary as an instantly it didn't work. And they're like,
how in the world is not even plugged in. So he puts it back and he tries sliding it over a little
doesn't work. So huh, what in the world? And so they're like, Okay, well, clearly, we're going to
have to like, we're gonna have to revise what it is we're looking for here. It can't be anything
that's possible. So we're gonna need to start coming up with ideas that are impossible to try to
describe this because and apparently the theory they eventually came up, the came to is that
when like, so that when you have the power on, there is a magnetic current, there's an
the electricity going opposite ways creates a magnetic field. And when the sound plays,
it vibrates the circle in the opposite corner, almost like a tuning fork, or I don't know,
a human ear. And it, it actually distorts the magnetic field in a way that causes what few
logic gates are actually plugged in and doing anything logic gated to power up or power down
the computer in response to that change in the magnetic field. And this is a radical departure
from how microchips are made by humans. I remember reading that same thing. And that was the reason
extremely effective. Yeah, you couldn't move it. All right. Yeah, it was like literally optimized
to that exact position relative to the computer on planet Earth. It was fantastic. That's awesome.
The one of the things I wanted to ask, which I think you sufficiently answered,
we can expound on it for much, much longer to anybody who who listened to the whole episode,
I'm glad I didn't ask this at the beginning. So I think an uninformed outsider could be like,
so what deep blue beat Bobby Fisher at chess 20 years ago, who cares? So why is this a bigger deal?
But I think that's been sufficiently covered. But if you if someone asked you that question,
and you weren't an elevator, what would you say in 30 seconds? Yeah, you have 30 seconds to tell
them why go is different than chess. 20 years ago, the best chess player of all time, or at least
one of the best Gary Kasparov was defeated by deep blue, but it was a system in which humans
taught the computer the strategy. The humans came up with the AI, they gave it the formula,
and it just relied on brute force and an enormous series of tools given to it by humans.
AlphaGo made its own strategy. AlphaGo, while it has an enormous amount of processing power,
is playing a game that cannot be solved by brute force. And there's not enough big enough universe.
And it is it made its own strategy and taught itself. And that's part of what makes it so
revolutionary. Awesome. I wanted to actually before we go any further, is there anything you
want to plug or promote at this point? Oh, God, that you can already promote at the beginning.
If not, we'll put it on the if you can't think of anything we'll put on the website.
I mean, down for whatever it depends. I guess you said a lot of people play magic,
author of next level magic, and a series of books on magic, write articles for spare city games.
And I got a music album coming out later this year, kind of excited about that other another
music album. But what's it called? I can't I hasn't come out yet. But the Gathering is on
YouTube, some of the stuff I did with Bill Bowden. But anyway, so it's a pleasure to remind Patrick
remind everybody of your full name so that they can look you up. And of course, we'll have this
on the website as well. Patrick Chapin. All right. And again, we'll have this on the website,
the Bayesian conspiracy.com. So visit us there. Or you can go to the subreddit slash R the Bayesian
conspiracy. Those are those are the two resources that and the the description and the iTunes download
and you can click those links. And we do have a correction from the subreddit. Last episode,
or was it two episodes ago now? We mentioned that Robert Almond was the Nobel Prize winner
deeply religious. Yeah. And there was a brief conjecture that he might be Jehovah's Witness.
And no, he's actually Orthodox. And I and I knew that after we we we stopped recording. I
remembered that it wasn't I knew I knew I think we knew it was never Jehovah's Witness. We just
knew he was fundamentalist somehow. But Robert Almond was a an Orthodox Jew and an old school.
Well, no, it must be a modern Orthodox. I'm assuming we do not want to speculate on that
right now. If you have any corrections or any comments, please talk about it on the subreddit
or on the website. Thank you for joining our conversation. Bye.
