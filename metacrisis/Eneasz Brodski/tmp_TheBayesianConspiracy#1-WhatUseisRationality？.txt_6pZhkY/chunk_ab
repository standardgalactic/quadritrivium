So what are they trying to find out?
What is the difference between the two groups?
I think they want to check, like, measured life success, probably self-assessed life
success.
They wanted to compare it to self-reported happiness levels and that sort of thing.
So the idea is to see what the difference would be between the success and happiness
of somebody who was interested in applying rationality in their lives and people who
were not?
Well, it's everyone who's interested in applying it, but people who were formally trained.
I see.
Then again, that's also assuming that the people who didn't get to CFAR classes aren't
going to go on and self-teach themselves.
So they're going to have to qualify them at some point.
How many freaking variables is that?
A lot.
But Julie is, I think, just a master's in statistics.
Oh, well, then I should trust you.
And they have a team.
Yeah.
Well, we can trust that they at least are aware of the problem.
So, yeah.
But as far as answering that question, I was hoping that the research had been completed
but it's not.
So anyone have any anecdotes or other research?
Well, my immediate knee-jerk reaction to that question was that's what we're going to be
talking about is how we can apply the methods of rationality to things in our daily lives
and to get a better outcome than otherwise.
Now, I think now is maybe a time when we should talk about the different types of rationality.
So what kind of rationality we are actually referring to?
Before we do that.
Okay.
Well, because I wanted to jump in with my thing on it too.
That's fine.
So the question, whether the rational approach decision-making is better or not, I'm not sure
there is really a rational approach to decision-making.
I've always thought of rationality as more of a toolkit of ways to think and ways to
make sure that you aren't doing a terrible job fooling yourself or, I guess, a really
good job fooling yourself.
Is that not a rational approach?
Well, that is, I don't know if that's a rational approach.
Like when I think of rationality, it's like the tools to make sure that you have good
info, that you aren't pulling from biased sources, that your own mental proclivities
aren't pushing you in a way that you don't want to go.
And so that seems to me like you're laying the groundwork for decision-making with the
rational tools.
A little column A, a little column B.
Okay.
Because I think that that sounds exactly like an approach.
Like so it's, I made the analogy to skepticism earlier.
I'm not sure what someone would say the skeptical approach is, but it's basically, yeah, it
can be summarized in one sentence of having a reasonable standard for evidence.
So then they'll take that and then work from that framework.
All right, what's reasonable here?
What can I safely assume or whatever?
How outlandish is this claim or something?
Like when he asks, what's the rational approach to decision-making, I want to contrast it right
away to what's an irrational approach.
Yeah, exactly.
I don't think that's fair to him though, because then he says down that his alternate is, what is that?
Smart ad hoc, smart and farmed ad hoc decision-making.
And I think the smart part is gaining data and informed ad hoc is making sure that your ad hoc
rules, because no one can analyze every single thing all the time.
You develop good heuristics and you test those every now and then.
Heuristics is like rules of thumb, I guess, should we point that out?
Yeah, heuristics I think are, yeah, they're mental shortcuts or cognitive shortcuts.
You develop good heuristics and you test them every now and then to make sure that they're
still working and that they're good and you alter them as you need to.
But that's all a rationality method.
Well, we're going to get to rational and yet potentially irrational heuristics in just a moment.
But yeah, this is a good question, I think, that we're probably going to be spending every time we
podcast answering it.
At least a little bit.
It can tie back to everything.
So yeah, I like the idea of what's an irrational approach because I mean, sort of like literally
like tossing a die for every problem that you can think of.
But then again, too, on what you assign for each of the six decisions if you're throwing
a six-sided die, some of those can be way more outlandish than others, right?
So if it's rational or not, right, it depends on what your goal is, right?
And it depends on if it makes sense to use intuition, to use effect heuristics, to use those
shortcuts to make that decision, right?
Yeah.
Given the situation, sometimes it absolutely does.
In fact, probably maybe even most times it makes sense to use some of our mental evolutionary
shortcuts to make decisions.
But using rationality is kind of getting over those mental shortcuts for times that they
don't work.
Well, so that's, I mean, that's why we have the heuristics that we do.
Yes.
Because they're beneficial most of the time, right?
But it's when they reliably fail.
So part of rationality is knowing the circumstances under which your heuristics are likely to be
poorly calibrated.
But what you described, Katrina, was partly, I think, adjacent to two kinds of rationality,
and that's a good segue is there's instrumental rationality, which is, I guess what, achieving
object-level goals and epistemic rationality, which is finding the truth, or I guess a reliable
way of finding the truth, or trying reliably to find the truth.
Yeah.
And I actually have a couple kind of full definitions here.
Right on.
And you're absolutely right, of course, Steven.
But I kind of like the way that they explain it, or the way whoever wrote this explained
it, unless wrong, which you can find unless wrong, wiki.
So instrumental rationality is concerned with achieving goals.
More specifically, instrumental rationality is the art of choosing and implementing actions
that steer the future towards outcomes ranked higher in one's preferences.
Your preferences can be all sorts of things, anything you care about, whereas epistemic
rationality is when your goals are truth and knowledge, specifically.
So those are goals in of themselves, right?
Whereas if you're talking about instrumental rationality, you could have falsehoods and
that could be part of achieving a different goal.
Think politics.
Or something less inflammatory, like physical health.
Like, I want to be healthier.
That's your object-level goal.
So how are you going to go about doing that?
So you can watch Oprah and get your health advice from Dr. Oz, or you can buy any book
off the shelves that's probably less reliable, or probably more reliable than Dr. Oz, but
less reliable than other sources.
But, you know, the thing is that you can look at, this is my goal, and with careful definition
you can say, all right, how much closer am I getting there?
You know, I guess what exactly are my health goals?
Rationality, I mean, we disparage Dr. Oz, of course, for good reason.
But rationality also helps you realize why you should disparage Dr. Oz and go with someone less crazy.
Who will actually probably help you.
Because this is a place in which truth and having good, accurate knowledge will probably help you get healthier.
I agree. Maybe that was a bad example.
But we also want to explain how rationality leads you to the concept that Dr. Oz is not a reliable source.
I feel like that's even easier than hard rationality, and that's just skepticism.
Well, I guess it's rationality too, in that you can look at, maybe it's the use of the strong, the distinction all the time.
How reliable is this person in general?
And then, okay, now what's this new claim they're making?
Well, how much should I trust that given, A, what I already know about the human body and or his reliability about something?
I think I have an example of one of the different kind of rationality.
So, instrumental rationality, let's say that your goal is to be best friends with Dr. Oz.
At that point, again, disparaging poor Dr. Oz, the truth isn't necessarily going to be helpful in getting to that goal of being best friends with Dr. Oz.
It might be more useful for you to hold similar opinions to that person.
That's true. I guess you're working, but within a framework of you want to believe true things about what it takes to be best friends with Dr. Oz.
Again.
But, you know what I mean?
So, like, you might believe, okay, well, this fruit tonic that he's selling probably won't cure my cancer, but I'll pretend to believe that because I'll only be his friend, right?
I think, actually, this is getting to the root of the guy's question, that sometimes if your goal is to become friends with Dr. Oz, it's better not to be rational.
This Dr. Oz scenario is kind of ridiculous, but the one I enjoyed much more from Les Wrong is the Bayseans versus Barbarians conundrum.
If you're in an all out war for survival, you kind of want to place your bet on the Barbarians because they don't care if they live or die.
They're going to go out there and fuck shit up.
And if, you know, however many of them die along the way, that's fine.
Whereas the Bayseans are more likely to be kind of like, oh, I don't want to necessarily risk my life for this.
And at least that's, you know, the common perception.
And I also do not want to join the military for similar reasons.
But the answer that Eleazar proposes is that if the Bayseans do that, they will lose the war.
And they will see that and losing the war is a very suboptimum outcome.
So the rational choice is not to lose the war.
That would be irrational and, can I say stupid in this case?
Because losing a war is kind of stupid, right?
If your goal is to win, sure, yes.
Only if your goal is to win.
And so the more correct answer is for everyone to see that the way to beat the Barbarians was to take some percentage of them and self-modify to becoming irrational and barbarian-esque while they're out fighting this war until it's over.
So from personal life, this is a good time.
I feel like there are plenty of times when it doesn't make sense to be super rational because it might cause discomfort with the position that you have to be in.
So if you're in a job in which you disagree with actions taken by your boss, then it might be time to put those niggling little doubts away for the goal of being happy in your position.
Isn't it rational to do that, though, if your goal is happiness over being right?
Well, yeah, that's the example.
That's the point.
It's meta-rationality.
Right, so it's instrumental rationality.
And I mentioned politics before.
I've been in plenty of environmental lobbying situations.
And as a rationalist, sometimes people use approaches that are uncomfortable to me, make statements that I don't feel are adequately backed up.
But when the goal is changing people's minds or getting people to come out to vote or something like that, sometimes being false actually helps with that outcome.
Yeah, I think it's going dangerously into dark arts territory, though.
I didn't say I liked it.
Well, once the people find out that you've been lying to them, it's going to backfire on you.
Yeah, I think people are used to that.
I think they should be.
Because, yeah, on the one hand, it might be worth even possibly that fallout if, like, say the bill's already been passed or something, if they find out that you were not being genuine in your campaign.
Well, you know, whatever my side already won.
So you guys can try and revoke that law, but it's already there.
That'll be harder.
So I think the loss of goodwill and the animosity you build is not going to be worth that.
Yeah, you know, what I'm talking about is falsehoods are a lot more subtle.
It's not that it's more of differences of opinion and maybe things that I might personally think are poorly supported opinions that are pushed to the forefront of messaging.
Fair enough.
So it's a little bit more subtle than all that.
And that sounds like about as honest political campaigning as can get, right?
I don't mean that like as a jab at all, but like that, I mean, focusing on the things that we all agree on is kind of like the only way to get people to like stay on board, right?
So, and not pointing out like the small areas or the disagreements and whatever.
Am I, am I missing something?
No, I mean, you don't have to focus on the disagreements, especially when you're trying to build a consensus.
On the other hand, I also, I think once you stop caring about the truth in order to pursue your agenda, then the truth is forever your enemy.
And you don't want to be on the wrong side of, you know, reality, because eventually it's going to win.
So here's what people actually do.
They rationalize, right?
They convince themselves that it's the truth.
And maybe they maybe they already thought it was, but they don't want to hear any evidence otherwise, because it's better for them to continue believing something that works towards their goal.
But the whole point of the methods of rationality is to get over that sort of thing and not believe things just because you want to believe them.
Right.
So which brings us to what we're going to try to focus on, which is epistemic rationality in pursuit of the truth rather than in pursuit of goals.
Although I think that some of the topics that we're going to get into, they are going to involve other goals in addition to truth.
I like them both because I probably use rationality more as a as instrumentally than I do empirically.
I mean, for the most part, I can get a reliable picture or something from Wikipedia.
And that's, you know, if I'm just trying to get a base idea of whatever's going on or what the truth, what the general truth consensus is on something.
