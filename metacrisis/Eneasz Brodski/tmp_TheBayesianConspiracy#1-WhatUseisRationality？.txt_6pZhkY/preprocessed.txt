Hi, this is Enesh Brodsky.
Hi, I'm Katrina Stanton.
Hi, I'm Steven Zuber.
And this is The Bayesian Conspiracy!
And we're gonna have music at some point here.
You're doing the music?
Excellent.
So this is, as I've been telling people, a conversational podcast
for people who are rationality adjacent in the rationality sphere anyway,
but are not smart enough to be really the hardcore leaders.
That's not fair.
Is that not fair?
No.
Okay, so let's cut this out and start over.
How about more casual?
Okay, how would you introduce us?
This is staying in, by the way.
No, this is not staying in.
This is totally staying in.
We're working out what the show's about.
Everyone's gonna be bored hearing what the show's about.
Oh, everyone's gonna be bored anyway.
Oh, damn it.
I'm gonna listen again.
No, this will be fun.
No, we gotta cut out this boring part.
Now that it's gone on this long, we may have to.
Great.
Congratulations.
Hey, you gotta win somehow, right?
Sorry.
I think the introduction was fine rather than the stupid part.
I should not say the stupid part.
I don't think that we need to call people stupid.
I think that is antithetical to what we're trying to do here.
Yeah, you're right.
Okay.
Yeah, this is for people who are interested in rationality.
There's tons of podcasts out there for skeptics, which...
And I'm willing to defend this, I think, is like rationality 1.0,
or rationality is like skepticism 2.0.
Okay, I could see how you say that.
I don't see that there's much out there for rationality, audio consumption.
So, yeah.
We'll see if this sticks.
We'll find a way to put that all together.
No, I thought that was pretty good what you said.
Cool.
I thought it was good, too.
Yeah, we'll use that.
Everyone will probably hate it.
It's possible, but in the meantime, we can have fun
and we can chat about rationality topics, how they apply to our lives.
That is what I'm hoping for.
We'll take the risk of public chagrin in stride.
Yeah.
But the thing with Julie Gale's podcast, which is great,
check it out rationally speaking, but it's not about rationality in general.
It's sort of they'll pick any random topic and talk about it at a level
that a rationalist can enjoy, but it's not about rationality in general.
And this isn't going to be about that, I think, every week forever.
So, like, the second most recent rationally speaking episode was on false memories.
Cool.
Yeah, it was awesome.
It was with Elizabeth Loftus, who I actually saw give a talk at the amazing meeting.
And the reason I brought that up is because Julia in the podcast was like,
I think I saw you talk at something once.
I was like, yeah, Julia, we are at the same thing.
But there's, I think, the first episode, they try and tackle what is rationality.
But that was years ago and I don't even know what they said.
So that's not really the focus of their show.
So we get to go from scratch here.
And we're staying on scratch for a while anyway.
I do assume that most of the people listening to this will have some exposure
to the less wrong rationality type tools.
These won't be a complete new thing to them.
Not some of my friends.
No.
I can't imagine how they'll stumble across it otherwise unless someone shows it to them.
But hopefully that's their only exposure.
Yeah.
I mean, hopefully even with minimal exposure over time, listening to this podcast,
you'll be able to pick up some things.
Right.
It's kind of the hope, right?
Yeah.
Okay.
And since we haven't mentioned straight up, there's a long series of blog posts
on a website called Less Wrong.
And this is the, we're aiming to be less wrong inspired,
but less wrong podcast.
So it's not necessary to have the familiarity with the website or anything,
but we want to be, I don't know, I'm picturing sort of like the level of discourse in the comments.
Yeah.
So we'll see.
We'll see.
We'll be drawing heavily from less wrong source material along with probably other blogs,
which we'll hopefully cite correctly.
Try.
Yeah.
Oh yeah.
I grabbed some definitions from them.
So we'll be using this as we go on.
Do we want to jump right into that question that we got?
Yeah.
The only thing I think the only other thing I wanted to mention was about the,
about citing other sites, sources and stuff.
There will be on the website in the episode descriptions,
we'll have links to more or less everything we talk about in every episode,
especially the website.
Oh yeah.
Oh, the website is the Bayesian Bayesian.
Okay.
The BayesianConspiracy.com.
And I guess every time we put it up an episode,
we'll have links to relevant things.
You bet.
Thanks for setting that up.
Yeah.
No problem.
Thanks to you guys for maintaining it.
And you all should 80, 90% of the legwork we're setting the website up in exchange
for us doing the maintenance work later.
So.
All right.
So do you want to introduce this question?
Yeah, sure.
This was fantastic because we were talking about setting up this podcast.
We were trying to figure out what our first topic would be.
We're like, let's introduce, you know, our favorite rationality tool.
Then like a day later, I got this email that was a very good intro question.
And I was like, Hey guys, I don't really know the answer to this.
I kind of sort of got an idea, but it's actually a fairly hard question for a
fairly basic thing.
Let's, let's start with this because this would be a really good way to start off
the podcast.
And the two of them said, yeah, we think that's a great idea.
So here's the question.
It's a multi-part question and it starts, is there evidence that a rational
approach to decision making either on the personal or institutional level will be
more likely to achieve desired outcomes?
For example, Harry Potter and the methods of rationality implies that a perfectly
rational decision maker will do a better job than a very smart and informed ad hoc
decision maker.
But I don't understand why this should be the case.
After all, the Bayesian priors for any real life problem aren't available.
And if you're estimating, how are you doing better than somebody using their
knowledge and intuition?
I don't include empirical decision making as inherently rational here.
So for example, if give directly were the best charity I see, that is more of a data
driven outcome than a rational one.
Obviously, the two are not mutually exclusive.
So I could be missing something.
That's really long.
We'll tackle it piece by piece.
Yeah, just do the first line.
But I wanted to say really quick, hey, do you have this person's name?
You didn't put it in our forward.
I did not put it in the forward.
No, it came to me on Reddit, so it's their Reddit name.
And I will credit them later.
We'll give you a shout out on the website because I think that's awesome.
That sounds like a non...
A complex question.
Yeah, that sounds like I would assign a non-zero probability to that being
LEI zero in disguise to see if you knew what you were talking about.
Really?
This is the test.
No, that covers everything.
You don't give the right answer.
That's the intro question of a 101 class that would just really piss off the instructor
because it's like, well, I was going to teach, but we'll just cover this one
question for 90 minutes because it's that in depth.
But I think it's great.
Is Eleazar often going undercover and...
Oh, he is secretly everyone in the Bayesian conspiracy.
What?
Yeah, just with different voice modulators.
But if he was, we might not know.
If he was, we wouldn't know, right?
Right.
And he can neither confirm nor deny that he's actually all of us.
Fair enough.
Great.
Until we get him on the show.
But then that could be him playing the, you know, deeper game.
Okay, okay, do you want to start with the first sentence?
Let's do it.
Is there any evidence that a rational approach to decision making either on a personal or
institutional level will be more likely to achieve desired outcomes?
Did you say you had some research on this?
I said that I was aware of some research on it and I did check into it again recently.
Drilla Galev runs the Center for Applied Rationality and one of their, their object
level goals is to acquire more research on exactly that question.
So they did, they're doing a longitudinal, a blind study of a statistically identical
group of basically the way it works is people who wanted to be, they took a bunch of applicants
who wanted to come to CFAR classes and a CFAR sort for Center for Applied Rationality and
then took half of them.
And so then they compared the base demographics and stuff made those all the same.
And last time I checked, they are still doing the research.
So what are they trying to find out?
What is the difference between the two groups?
I think they want to check, like, measured life success, probably self-assessed life
success.
They wanted to compare it to self-reported happiness levels and that sort of thing.
So the idea is to see what the difference would be between the success and happiness
of somebody who was interested in applying rationality in their lives and people who
were not?
Well, it's everyone who's interested in applying it, but people who were formally trained.
I see.
Then again, that's also assuming that the people who didn't get to CFAR classes aren't
going to go on and self-teach themselves.
So they're going to have to qualify them at some point.
How many freaking variables is that?
A lot.
But Julie is, I think, just a master's in statistics.
Oh, well, then I should trust you.
And they have a team.
Yeah.
Well, we can trust that they at least are aware of the problem.
So, yeah.
But as far as answering that question, I was hoping that the research had been completed
but it's not.
So anyone have any anecdotes or other research?
Well, my immediate knee-jerk reaction to that question was that's what we're going to be
talking about is how we can apply the methods of rationality to things in our daily lives
and to get a better outcome than otherwise.
Now, I think now is maybe a time when we should talk about the different types of rationality.
So what kind of rationality we are actually referring to?
Before we do that.
Okay.
Well, because I wanted to jump in with my thing on it too.
That's fine.
So the question, whether the rational approach decision-making is better or not, I'm not sure
there is really a rational approach to decision-making.
I've always thought of rationality as more of a toolkit of ways to think and ways to
make sure that you aren't doing a terrible job fooling yourself or, I guess, a really
good job fooling yourself.
Is that not a rational approach?
Well, that is, I don't know if that's a rational approach.
Like when I think of rationality, it's like the tools to make sure that you have good
info, that you aren't pulling from biased sources, that your own mental proclivities
aren't pushing you in a way that you don't want to go.
And so that seems to me like you're laying the groundwork for decision-making with the
rational tools.
A little column A, a little column B.
Okay.
Because I think that that sounds exactly like an approach.
Like so it's, I made the analogy to skepticism earlier.
I'm not sure what someone would say the skeptical approach is, but it's basically, yeah, it
can be summarized in one sentence of having a reasonable standard for evidence.
So then they'll take that and then work from that framework.
All right, what's reasonable here?
What can I safely assume or whatever?
How outlandish is this claim or something?
Like when he asks, what's the rational approach to decision-making, I want to contrast it right
away to what's an irrational approach.
Yeah, exactly.
I don't think that's fair to him though, because then he says down that his alternate is, what is that?
Smart ad hoc, smart and farmed ad hoc decision-making.
And I think the smart part is gaining data and informed ad hoc is making sure that your ad hoc
rules, because no one can analyze every single thing all the time.
You develop good heuristics and you test those every now and then.
Heuristics is like rules of thumb, I guess, should we point that out?
Yeah, heuristics I think are, yeah, they're mental shortcuts or cognitive shortcuts.
You develop good heuristics and you test them every now and then to make sure that they're
still working and that they're good and you alter them as you need to.
But that's all a rationality method.
Well, we're going to get to rational and yet potentially irrational heuristics in just a moment.
But yeah, this is a good question, I think, that we're probably going to be spending every time we
podcast answering it.
At least a little bit.
It can tie back to everything.
So yeah, I like the idea of what's an irrational approach because I mean, sort of like literally
like tossing a die for every problem that you can think of.
But then again, too, on what you assign for each of the six decisions if you're throwing
a six-sided die, some of those can be way more outlandish than others, right?
So if it's rational or not, right, it depends on what your goal is, right?
And it depends on if it makes sense to use intuition, to use effect heuristics, to use those
shortcuts to make that decision, right?
Yeah.
Given the situation, sometimes it absolutely does.
In fact, probably maybe even most times it makes sense to use some of our mental evolutionary
shortcuts to make decisions.
But using rationality is kind of getting over those mental shortcuts for times that they
don't work.
Well, so that's, I mean, that's why we have the heuristics that we do.
Yes.
Because they're beneficial most of the time, right?
But it's when they reliably fail.
So part of rationality is knowing the circumstances under which your heuristics are likely to be
poorly calibrated.
But what you described, Katrina, was partly, I think, adjacent to two kinds of rationality,
and that's a good segue is there's instrumental rationality, which is, I guess what, achieving
object-level goals and epistemic rationality, which is finding the truth, or I guess a reliable
way of finding the truth, or trying reliably to find the truth.
Yeah.
And I actually have a couple kind of full definitions here.
Right on.
And you're absolutely right, of course, Steven.
But I kind of like the way that they explain it, or the way whoever wrote this explained
it, unless wrong, which you can find unless wrong, wiki.
So instrumental rationality is concerned with achieving goals.
More specifically, instrumental rationality is the art of choosing and implementing actions
that steer the future towards outcomes ranked higher in one's preferences.
Your preferences can be all sorts of things, anything you care about, whereas epistemic
rationality is when your goals are truth and knowledge, specifically.
So those are goals in of themselves, right?
Whereas if you're talking about instrumental rationality, you could have falsehoods and
that could be part of achieving a different goal.
Think politics.
Or something less inflammatory, like physical health.
Like, I want to be healthier.
That's your object-level goal.
So how are you going to go about doing that?
So you can watch Oprah and get your health advice from Dr. Oz, or you can buy any book
off the shelves that's probably less reliable, or probably more reliable than Dr. Oz, but
less reliable than other sources.
But, you know, the thing is that you can look at, this is my goal, and with careful definition
you can say, all right, how much closer am I getting there?
You know, I guess what exactly are my health goals?
Rationality, I mean, we disparage Dr. Oz, of course, for good reason.
But rationality also helps you realize why you should disparage Dr. Oz and go with someone less crazy.
Who will actually probably help you.
Because this is a place in which truth and having good, accurate knowledge will probably help you get healthier.
I agree. Maybe that was a bad example.
But we also want to explain how rationality leads you to the concept that Dr. Oz is not a reliable source.
I feel like that's even easier than hard rationality, and that's just skepticism.
Well, I guess it's rationality too, in that you can look at, maybe it's the use of the strong, the distinction all the time.
How reliable is this person in general?
And then, okay, now what's this new claim they're making?
Well, how much should I trust that given, A, what I already know about the human body and or his reliability about something?
I think I have an example of one of the different kind of rationality.
So, instrumental rationality, let's say that your goal is to be best friends with Dr. Oz.
At that point, again, disparaging poor Dr. Oz, the truth isn't necessarily going to be helpful in getting to that goal of being best friends with Dr. Oz.
It might be more useful for you to hold similar opinions to that person.
That's true. I guess you're working, but within a framework of you want to believe true things about what it takes to be best friends with Dr. Oz.
Again.
But, you know what I mean?
So, like, you might believe, okay, well, this fruit tonic that he's selling probably won't cure my cancer, but I'll pretend to believe that because I'll only be his friend, right?
I think, actually, this is getting to the root of the guy's question, that sometimes if your goal is to become friends with Dr. Oz, it's better not to be rational.
This Dr. Oz scenario is kind of ridiculous, but the one I enjoyed much more from Les Wrong is the Bayseans versus Barbarians conundrum.
If you're in an all out war for survival, you kind of want to place your bet on the Barbarians because they don't care if they live or die.
They're going to go out there and fuck shit up.
And if, you know, however many of them die along the way, that's fine.
Whereas the Bayseans are more likely to be kind of like, oh, I don't want to necessarily risk my life for this.
And at least that's, you know, the common perception.
And I also do not want to join the military for similar reasons.
But the answer that Eleazar proposes is that if the Bayseans do that, they will lose the war.
And they will see that and losing the war is a very suboptimum outcome.
So the rational choice is not to lose the war.
That would be irrational and, can I say stupid in this case?
Because losing a war is kind of stupid, right?
If your goal is to win, sure, yes.
Only if your goal is to win.
And so the more correct answer is for everyone to see that the way to beat the Barbarians was to take some percentage of them and self-modify to becoming irrational and barbarian-esque while they're out fighting this war until it's over.
So from personal life, this is a good time.
I feel like there are plenty of times when it doesn't make sense to be super rational because it might cause discomfort with the position that you have to be in.
So if you're in a job in which you disagree with actions taken by your boss, then it might be time to put those niggling little doubts away for the goal of being happy in your position.
Isn't it rational to do that, though, if your goal is happiness over being right?
Well, yeah, that's the example.
That's the point.
It's meta-rationality.
Right, so it's instrumental rationality.
And I mentioned politics before.
I've been in plenty of environmental lobbying situations.
And as a rationalist, sometimes people use approaches that are uncomfortable to me, make statements that I don't feel are adequately backed up.
But when the goal is changing people's minds or getting people to come out to vote or something like that, sometimes being false actually helps with that outcome.
Yeah, I think it's going dangerously into dark arts territory, though.
I didn't say I liked it.
Well, once the people find out that you've been lying to them, it's going to backfire on you.
Yeah, I think people are used to that.
I think they should be.
Because, yeah, on the one hand, it might be worth even possibly that fallout if, like, say the bill's already been passed or something, if they find out that you were not being genuine in your campaign.
Well, you know, whatever my side already won.
So you guys can try and revoke that law, but it's already there.
That'll be harder.
So I think the loss of goodwill and the animosity you build is not going to be worth that.
Yeah, you know, what I'm talking about is falsehoods are a lot more subtle.
It's not that it's more of differences of opinion and maybe things that I might personally think are poorly supported opinions that are pushed to the forefront of messaging.
Fair enough.
So it's a little bit more subtle than all that.
And that sounds like about as honest political campaigning as can get, right?
I don't mean that like as a jab at all, but like that, I mean, focusing on the things that we all agree on is kind of like the only way to get people to like stay on board, right?
So, and not pointing out like the small areas or the disagreements and whatever.
Am I, am I missing something?
No, I mean, you don't have to focus on the disagreements, especially when you're trying to build a consensus.
On the other hand, I also, I think once you stop caring about the truth in order to pursue your agenda, then the truth is forever your enemy.
And you don't want to be on the wrong side of, you know, reality, because eventually it's going to win.
So here's what people actually do.
They rationalize, right?
They convince themselves that it's the truth.
And maybe they maybe they already thought it was, but they don't want to hear any evidence otherwise, because it's better for them to continue believing something that works towards their goal.
But the whole point of the methods of rationality is to get over that sort of thing and not believe things just because you want to believe them.
Right.
So which brings us to what we're going to try to focus on, which is epistemic rationality in pursuit of the truth rather than in pursuit of goals.
Although I think that some of the topics that we're going to get into, they are going to involve other goals in addition to truth.
I like them both because I probably use rationality more as a as instrumentally than I do empirically.
I mean, for the most part, I can get a reliable picture or something from Wikipedia.
And that's, you know, if I'm just trying to get a base idea of whatever's going on or what the truth, what the general truth consensus is on something.
But it depends on the article.
It does.
But and I'll find the study and link to it in the in the episode description.
But the there was a study done and Wikipedia as a whole some years ago and I can't remember any of the details, but it'll be there had less had fewer scientific errors.
I think in the mid 2000s, then encyclopedia Britannica.
I remember that.
Yeah.
Does encyclopedia Britannica still update?
I don't know.
But but the idea was that to anybody who says like, oh, it's it's it's all, you know, up for anyone who goes up and edits it.
But I think they have like a board of thousands of volunteers and some paid people to go through and monitor edits and stuff.
Yeah.
So it's not just anything.
Although Wikipedia is remarkably accurate.
But as long as you stay away from like hot topic political.
The more controversial and the more obscure the page, then yeah, then those will be way harder to get truth on.
I know somebody who added an entry for a Lovecraftian beast, right?
Lovecraftian elder God or old one or whatever and just watched as metal bands started naming their bands after it.
Nice.
And it was just added a completely false one.
That's about her.
And then of course, Stephen Colbert was all happy that he saved the elephants.
He increased the elephant population by having everyone who watches the show say that they were like their populations were skyrocketing.
It was a thing that he did to have all his fans going to Wikipedia and edit it up.
Elephant specifically.
Elephant specifically.
Yeah.
That due to Stephen Colbert's influence, elephant populations were rising on Wikipedia.
Yeah.
Well, I mean, obviously they reversed it.
Yeah, of course.
But for a little while.
And just giving us just letting us know that all of our information is unreliable.
And maybe it's not unreliable.
It's just it can be manipulated, which is true of any information.
It is.
It's a varying degrees of certainty.
That's why I said all not just Wikipedia.
Okay.
Sorry.
Yes.
But and I'm sure this is somewhere in the comments on that wiki page, but I haven't gone through them.
But I think that in a lot of ways, epistemic rationality is a necessary condition for instrumental rationality, at least in a lot of cases.
You can't manipulate reality if you don't know what is real.
Exactly.
I see.
So like, you know, again, using just a vague example of like, I want to lose 20 pounds or something.
You know, if your if your idea of how that works is like, well, if I only eat in the morning, then I should be fine.
Right.
But you want to know the truth behind that claim before you just go for it to help you better achieve your goals.
It's also why I don't think that you should, you know, tell people not the truth to manipulate them, because it's inherently a hostile act.
You're sabotaging their map of reality for your, you know, purposes.
And once they find that out, they are not going to be terribly happy with you.
I've taken away their power to alter reality by, you know, giving them a wrong idea of what it is.
I have everybody believes you.
No, because then reality still hasn't altered unless it's a social construct.
Yes.
A lot of things are social constructs.
Yeah.
We're going to run into a lot of that.
If everyone took Wikipedia's word for it, an elephant population should be a stop to worrying about it.
Well, the elephant population exists at its number no matter what Wikipedia says or what people believe.
Right.
I know.
Something else in the population is not a social construct.
Exactly.
But speaking of painting.
For me, lying is saying to yourself, I have a better grasp on what this person's view of reality should be than they do.
Yes.
And there are some cases I think where that could be true.
If somebody's not sound of mind or otherwise in some distress, you know, that that could help temporarily.
But I have in mind already an idea for somebody to bring on as a guest who will defend the position of lying.
Okay.
And I, I, we didn't get that far when we were talking about it, but I couldn't bring her around to the idea that like, no, I think that that would still be a mean thing to do.
And she.
Who is this person?
I don't know if she, well, Shelly.
Oh, yeah.
Yeah.
Is she pro lying?
Yes.
Which is weird because.
She has a lot of very interesting positions, but I don't know if she would.
She might come on a defend lying for us.
We'll see.
Okay.
That would be very nice of her.
So there was another line.
I'm not sure if we're ready to move on.
And I guess we can take it in order.
I think the one I was thinking of was further to the end.
So next line of the question.
Okay.
So the next line is, for example, Harry Potter and the methods of rationality implies that a perfectly rational decision maker will do a better job than a very smart and informed ad hoc decision maker.
But I don't understand why this should be the case.
Do you want it?
I wanted to actually just jump in and summarize, I think the last section neatly, which is there's lots of anecdotal evidence supporting the idea that people who make the effort to train themselves rationally, more efficient at instrument, I guess that achieving their goals.
We didn't actually give any anecdotal examples though.
No.
It intuitively makes sense to me that if you know what reality is and how to manipulate it that you'll be better at doing that.
But we didn't actually demonstrate that.
And in summary, we don't have any research to, we don't have any large scale research to back that up.
In summary, we don't have any evidence to back up our position on this, but that we want some.
It seems almost crazy to think that the opposite would be the case though, that you can just do whatever you want without regards to the rules of the world.
Oh, that's not what they're saying.
They don't say whatever you want without regards to the...
They're saying, very smart and informed ad hoc decision maker.
A very smart, informed ad hoc decision maker, why would a rational, a perfectly rational in fact decision maker do a better job than that person?
I think a very smart and informed ad hoc decision maker is basically a rationalist.
Except when he throws in perfect?
Because then I'm thinking, well then that person wouldn't need to ad hoc justify anything because they'd get it right the first time, every time.
And then that'd be wonderful.
It's like the Bayseans versus Barbarians thing.
A perfect Bayesian would be able to, you know, self-modify to want to be crazy while on the battlefield, right?
But we can't because humans are very difficult to modify.
So we allow for a little bit of irrationality and violence in the population in general so that if we need to call up on that, we have it there.
You allow for some irrationality to exist so that you are more flexible overall.
Whoa.
That sounded like a bigger claim than the Barbarians versus the Bayes.
Do you not think that we accept more violence in society than we should necessarily?
I don't want to go into it.
Yeah, I think that's too big.
I think that's too big of a topic for right now.
I'm not sure what accept means.
Except that it exists and that there's nothing I can do about it because I'm just one person.
I think he's talking about who we send to war to fight our battles for us.
I live next to a fire department.
I see.
So if you guys can hear the siren, it makes me feel like actually rationally speaking records in downtown New York City.
So every single time they're recording, you can hear police sirens, which is great when you're driving.
Every time you hear that, you know that you are safe from fires.
That's right.
And there's EMTs literally right on the side of my balcony.
Oh, thank goodness.
Yeah, just in case.
Okay, because my arm just went numb.
Which one?
Because I think heart attacks present differently in females than in males.
Seriously?
Yeah, it's a little known.
I was going to say left because I thought it was left.
And I think in women, it's like, actually, I should know this because I was shocked to hear this when I first learned it.
But yeah, it turns out that the symptoms of a heart attack are taught male-centrically that it presents differently in females.
Now that I don't know how it presents differently, that knowledge is basically useless to me.
That's shocking.
I can't believe that the medicine is so sexist.
What I was thinking with regards to the ad hoc explainer versus the perfect, I guess.
The rationalist.
Is that the ad hoc explainer will have a better toolkit to understand what went wrong and hopefully be less likely to make those mistakes in the future.
So if you realize that you trusted this source because you liked them a lot or something, and for whatever reason then you found, oh no, they were radically wrong in all these things.
If you're realizing that ad hoc, you can try and update how much you'll trust somebody just because you like them in the future.
So you'll be forced to make less ad hoc justifications later on.
So are we assuming that the perfect rationalist actually has no previous knowledge or has a different amount of previous knowledge of what went into making a problem or an issue or an obstacle that needs to be overcome?
I think we could say the perfect rationalist discussion for our episode on Mary, the Machine Intelligence Research Institute, because I think the perfect rationalist would be a super AI.
Yeah, I think the perfect rationalist, first of all, would have a lot more prior knowledge than we have.
And processing power.
Empty, what?
And processing power.
And processing power, yeah.
But I think, you know, having a ton of foreign knowledge is part of what helps make you rational.
But I also think that this podcast is more for people who are not perfect rationalists, aspiring rationalists at best.
Yeah.
And so I don't think we need to optimize to that level on the podcast, but it's still, it's an interesting question.
Like, a lot of the ad hoc heuristics we come up with are pretty decent, and would devoting all those resources into making a perfectly rational decision maker really make that big of a difference?
Or would it just waste resources?
So maybe now is a good time to talk about a couple biases that there are rationality tools to kind of get us by.
Yeah, I want to do that too.
But the only thing I wanted to add to the last thing you said, which I've already slipped on the exact wording, but I'll find a way to put it together nicely.
That part of the aspiring to be, I guess, you know, perfect rationalist, whatever, but being more rational in general is I want to look back day to day, year to year, and think the thought,
Man, that was stupid of me less than I used to.
And to me, that marks improvement is if I'm not looking back and thinking, man, I was a fool.
For X period of time in the past.
Or like, you know, just less frequently rather, right?
So I mean, there'll be times, you know, if I'm encountering something new or I should have done this better, but I don't want to be constantly thinking that.
I see.
Okay.
I don't want you to have to edit out even more stuff by getting flat tracked.
Okay, so there's a couple, a couple of things that we wanted to go over.
And we mentioned this a little bit earlier, but the effect heuristic.
So this is kind of the bias that there are rationality tools in order to help us get by.
So effect heuristic is anytime that a subjective emotion about something acts as a mental shortcut.
We have a couple examples.
And one of those is the halo or horns effect.
Thinks less wrong.
And so in this case, the halo effect would be if you think that somebody is a great person, you really like them, then they can kind of do no wrong, right?
Like if you think someone is smart, you'll think they're more attractive.
If you find someone attractive, you'll often think they're smarter, they're better morally, just all good things tend to shine and make the other good things look even more good.
And all they say is true.
So you're more likely to believe them.
And a lot of those make sense, right?
Yeah, for sure.
And that's kind of the bummer, right?
But there's no reason that someone two inches taller than me should be more credible in any single area.
Or in general, just average sampling in every single area.
So attractive celebrities, people take their words on everything.
And even to use less obvious examples, like staying clear of Jenny McCarthy and anti-vaxxing, you can get the Nobel Prize in chemists.
People apparently ask these people, what's your thoughts on the Middle East conflict?
Yeah.
Why would I know anything about that?
How do you feel about the blacks?
Tony answer that one, it's a trap.
But as far as being able to just dissociate yourself completely from, I guess, emotional effect to any problem at all, it's going to be really, really hard.
If doable at all.
But I think the goal is to be aware of the bias and then try to prime yourself against it.
Yeah.
We're going to get into a couple of tools.
I just wanted to...
I ran across the Horn's example really recently when I was over at dinner with my parents.
Someone brought up George W. Bush and they were a fairly liberal family.
And I was trying to say, look, not everything he did was awful.
I mean, the guy sucks.
Listen, Hitler had some good ideas.
Thank you.
But like, he apparently had a really good AIDS program that helped out in Africa quite a bit.
And the person, unnamed person that was sitting next to me was like, yeah, well, fuck him and fuck that stupid AIDS program and didn't, you know, it's like, no.
It must have been bad because that's a bad person and we don't like him.
And I'm like, you know, he is a terrible person, but come on.
He did a good thing, right?
He might even be an okay person.
I've never met him or even known.
Even if I'd met him at a conference or something, it's not like I know him as an individual.
Everyone's nice personally.
I've heard a lot of people say that he's a nice person personally.
Sure.
But yeah, so it's who the unnamed person sitting next to you didn't make an effort to calibrate against the fact that they hate George Bush.
versus assessing the effectiveness of the AIDS program.
Yeah, well, and I will say I was greatly exaggerating for story effect just now.
It's not that they said fuck his AIDS program.
They started coming up with things like I'm sure it wasn't that effective.
And it could have been a lot better than it actually was merely because he ran it and the person didn't actually have any data to back any of this up.
That's actually a stronger, I think that's more interesting case than the hyperbole that you gave because that's saying, that's immediately not saying, well screw him and screw everything he did.
It's saying, I bet that sucked because he did it.
Well, yeah, that's the best example of the horns effect there is.
Like if we had had a different president, he could have done the AIDS program better.
Right.
Like maybe, but he did good.
I feel like this happens to all my exes.
Explain that they think bad things about you.
Yeah, but I'm going to turn that around to have me thinking it instead.
Okay.
Because I think it's rude the other way.
Anyway, so when you break up with somebody, I think sometimes for some people, it might take a while to remember that there were good things about them and that, you know, that it's not all terrible and they're not a terrible person and they're normal human being.
That can be a bit of a survival strategy too.
Yes.
I broke up with my wife, my first wife way back when we stayed friends and then eventually we kind of got back together again and that was just a clusterfuck because the reason we broke up is we were not good together.
And so we lost another two years with the whole, you know, getting back together and then I was like, no, I just had the second time we broke up.
I was just like, I'm done with you.
Everything about you is horrible and I can't be near you.
And then I was actually able to go on with my life.
Okay.
Yeah.
So sometimes, so these heuristics.
Shit.
Is that an example of where it's rational to be irrational?
That's what that's what we're saying.
Maybe it is or rather not necessarily rational but helpful because it's not rational to be irrational.
Right.
It is a helpful survival technique to be irrational.
I know she is actually a pretty good person all over.
We just we can't be together because we might fall back together.
That's like meta rationality because I made the decision to be, you know, about her.
Everything doesn't have to be rash.
That's what I was going to say that was that your goal wasn't to properly in that moment wasn't to properly assess is this person, you know, a good person or not.
It's I don't want to be miserable with this person.
So what do I need to do to make that happen?
Well, if I if I express to myself and to her that I think that, you know, she's terrible and I want to be around her.
That'll get that'll get the job done.
I don't know about you, but one of my goals is to be a kinder, better person.
So I think that's where rationality can actually or rather these rationality tools can really help.
One of the ones is the principle of charity.
It's a good one.
This is my favorite technique in which you evaluate your opponents position as if it made the most amount of sense possible, given the wording of the argument.
And again, this reminds me of arguments, maybe with a significant other or a family member where you get into a place where you want to misinterpret or you're angry and you're hurt.
So it's not that you want to, but you misinterpret everything they say to be the worst possible meaning.
And you see that a lot on the Internet, too.
It's so sad.
It seems to be like the main reason you should never read the comments.
Someone says something.
That's not what they meant.
So there was a quote that I really loved about this from a lesser on post that I came across years ago, and I don't think I found these regularly, but it was linked to somewhere.
And it was, if you're interested in being on the right side of disputes, you will refute your opponent's arguments.
But if you're interested in producing truth, you'll fix your opponent's arguments for them.
To win, you must fight not only the creature you encounter, you must fight the most horrible thing that could be constructed from its corpse.
And that person's username was black belt Bayesian.
So that's a good example of steelmaning, which to me I think is like principle of charity 2.0, right?
Yes, exactly.
So the definition about that that I pulled from lesser on, I believe, is a straw man is a misrepresentation of someone's position or argument that's easy to defeat.
While steelman is an improvement on somebody's position or argument that's harder to defeat than their originally state position or argument.
I personally, when I've used this before, I don't think it has to be better than their originally stated argument, but I think that it makes sense to pull their strongest arguments and go up against those rather than their weakest ones.
It's game time.
Can we steelman the question that we got?
From that person?
Yeah.
I think it's too complex.
I feel like it's pretty solid on its own, plus it's complex, but we can give it a shot.
I don't know, it didn't seem ill-formed or anything like that, right?
So to me, steelmaning would be, I don't know, I'm trying to think of a really non-inflammatory example, but it's hard to...
I think we have been.
I think we've been trying to characterize, we've been using the principle of charity, we've been trying to characterize the question
in ways that we can address, right?
In ways that, well, do they mean this?
Well, that would make sense.
So they mean this, that would make a lot of sense.
And we've also, at least, I certainly haven't, and you guys have too, we've been disagreeing with, we've been making arguments and then, well, what situation would it be the best thing to not be a-rationally, right?
Yeah.
We've already been playing this game.
Yeah, I think that that might be less of a game time thing and more of a welcome to the show kind of thing.
We would all the time.
I mean, it's, especially when you're coming across a new argument for something, right?
So one of my intro to philosophy classes, one of the best learning, I don't know what you call them, not examples, but...
Where they try and give people to do things, whatever.
Exercises?
Exercises, thank you.
Was we had covered the, each side of the, I guess, teaching creationism in science classrooms debate, which basically everybody, most people, I assume, walked into that room already, like, having a belief about it before it was, you know, expressed and articulated.
But nevertheless, the arguments were given and then the teacher had us raise our hands.
If you, you know, all right, creationism, not in the science classroom.
Everyone raise your hands to beliefs and then vice versa.
All right, cool.
So you, everyone who believed that creationism should be taught in the classroom, moved to this side and now argue that creationism shouldn't be taught in this classroom or in the science classroom.
And I thought that was fun because I then was forced to think of the best because, you know, I can't remember if there was a prize or if it was just winning, but I'm like, all right, fine.
We haven't, all the arguments we covered sucked because I already thought of them and dismissed them.
But what's the best thing I can think of?
Why on earth should we try and teach it?
I think I've heard that called the ideological turing test.
Yes.
When your opponent cannot tell if you are natively for or against the position by your argument.
Oh man.
Oh, that's a fun one.
Yeah, like, oh, is he someone who hates this argument, but is pretending to like it?
Or does he actually like it?
Right.
I have something for you to steal, man.
And it's the next part of the question.
Oh, before we get to that.
Oh, okay.
No, it's okay.
I was just going to say, since we got on the creationism thing, I think one of my favorite arguments for teaching creationism in the classroom is that it should always be taught by someone who is scientifically literate.
So we can demonstrate this is how science works and this is why it's not science.
Creationism is a perfect example of the traps you can fall into if you have a cursory knowledge of science, but you don't actually do it using the tools of the scientific method.
I wish I had thought of that when I had this example.
Well, but then someone could tell obviously you are not for creationism.
But that wasn't the goal of the exercise.
The goal of the exercise was just to argue for it, to argue for position you disagreed with.
And I think I settled on something.
This was like eight years ago.
I'm trying to remember.
I think I settled on something along the lines of pluralistic education or something, you know, teach.
I think I tried to think of the best form of teach both sides and, you know, whatever I only had a few minutes to think, but that was, I think that's the best way to put it is, yes, we should teach it.
Followed by an example of this is exactly how science isn't done.
That would have been so awesome.
So moving on.
So the next part is the Bayesian priors for any real life problem aren't available.
So if you're estimating, how are you doing better than someone using their knowledge and intuition?
So the way that I would respond to this, which is not at all steel manning is that that the writer maybe is a little bit confused because we always go into things with Bayesian priors.
It's a definition of what they are.
Yes, we always have, we are constantly giving outcomes probabilities, right?
So, and we update those as we become more experienced and have knowledge and have, and the reason that we have intuition to draw on is intuition draws from our stores of knowledge and experience, which is our priors.
So we can make it even easier than that and just say, the commenter said that real life problems never have what, given Bayesian priors or something.
And that's just false.
He assumes Bayesian priors are a tag?
You are doing a bad job at steel manning.
I'm not going to straw man, but I'm going to look at it and then we can steel man it.
But just looking at it as given, so I mean, it depends on the problem, right?
So like, if I want to know the accuracy of my pregnancy test or someone else's pregnancy test rather, I probably wouldn't come up positive.
But in general, although I have heard anecdotal stories of some types of testicular cancer giving positive results on pregnancy tests.
So something to keep in mind.
Yes, we should all keep that in mind.
Don't quote me on that, we're not citing that.
But the odds are given on the package and in the included documentation.
So like, given a positive pregnancy test, you know with like what 98% probability that you're pregnant.
If it's within X number of days of your missed period.
Exactly.
And so that's exactly the appropriate way to look at it and then you get your prior from that.
But then I'm assuming that in any real world problem, maybe they meant something like that you just encounter, you know, without prep time.
But I'm trying to think of an example where you likely wouldn't have any priors at all like so I got to start my car tomorrow and it doesn't start.
Well, there are a lot of things, you know, but I could I could look at what other issues I've had with it or something like that to give me.
Depends on how many times you'd run into this problem before.
What are the short of symptoms like is it just not starting period is it doing this and not that even even if you are thrown into a position for example for me it would be which sports star do you think is going to win the game of you know some random game and I don't care about sports or
any people even then you you have some priors because you are I would be able to say I have no idea.
So I'd be able to assign what would be like their two people 5050 would be my I have no idea priors.
Yeah, and then you could ask your your sports savvy friend.
Hey, who do you think this is and then you can say okay well how much are they savvy about sports, etc.
And then and exactly but then but then you can update accordingly and say well they.
They're a big fan of this team so they're always going to say they're going to win so I can't use his knowledge or they've or they've won their fantasy league every year for the last eight years.
They I'll bet 99% or but I'll bet 90% now that this person's going to win because that person knows what's up and they're they're not motivated to lie to me about this.
You should go with 90% person's going to win.
You should go with whatever your friend said.
If your friend said there's only 70% chance they're going to win.
Oh yeah.
Yeah.
Okay, perfect.
So update your beliefs accordingly.
There's actually a really good in the site where something worth diving into later with problems with Bayesian updating.
But you can get you can run into examples where like that sort of reasoning what does my friend think and they're they tend to be reliable can run into like sort of wider problems.
Like I'm not sure what the percentage is 80 70% of the of the world population is religious.
Well, they can't all be idiots.
So should I update in favor of becoming religious, you know, given that it's such a popular belief and there's a confident in it.
But then that that sort of runs into the problem because they're not arriving at that conclusion with Bayesian here with Bayesian guidance.
So I don't know.
But I thought that was a fun.
That's a fun counter example that was worth exploring at some point.
I think yes, totally.
Okay, so let's go back to the after all the what was it.
So after all the Bayesian priors for any real life problem aren't available.
And if you're estimating, how are you doing better than someone using their knowledge and intuition.
So we so our non steel manning was that the the author of the question is maybe a little bit confused about what we mean by Bayesian priors.
So steel manning.
What what could they mean?
What do you think they do mean by that?
I for some reason I thought this was the easiest part of their question to answer.
And that's just and that's because like, so a well informed expert will get things right more often than others because they have a better background to work with.
That's just what it means.
So like, I'm trying to think, you know, like chess grandmasters, they, they can, they can speed through.
And I, I'm not going to edit this out of camera before I was going with this.
But I think this is actually the chess analogy good for steel manning would a perfectly rational Bayesian who is not a chess grandmaster just kind of familiar with the rules.
Would he be better at chess than a non Bayesian chess grandmaster?
So would deep blue be the chess grandmaster?
Because deep blue, but only knows the rules.
Yeah. And the deep blue doesn't know anything else.
Right. So I think it can also go quickly.
Right.
I think deep blue went pretty quick.
I'm not sure about the speed, but I bet and that was what 20 years ago.
I'm sure the best chess playing computers now are are crazy good.
We need to talk about that computer one at go next sometime.
What if, what if you're a human go is a much bigger deal because as far as I recall deep blue just basically brute force the problem, right?
Right.
Yeah.
Because chess is like computable.
Yeah.
In a way that go just isn't right.
So you were asking.
So let's say that our perfect rationalist is a human being.
Yes.
And they're nearly perfect rationalist, but I think that you know, compared to somebody with deep experience.
The truth is that it only works if they have the evidence that they need in order to process a solution.
Right.
Yeah, I think that's fair.
I have to have the input in order to make the output.
I think the perfect rationalist would expect it to lose to the chess grandmaster.
Yes.
Because the chess grandmaster has a lot more knowledge and has those techniques, you know, almost internalized.
I would assume I haven't met any almost internalized to the point of intuition.
Whereas the Bayesian doesn't have all of that experience on his side.
I think you're right.
I think they probably bet on themselves losing.
And therefore win.
Right.
Lose the game, win the bet.
But the question of encountering, I guess, Bayesian priors.
Read the sentence again.
The Bayesian priors for any real life problem aren't available.
If you're estimating, how are you doing better than someone using their knowledge and intuition?
Well, you aren't.
Everyone just uses their knowledge and intuition.
And I think, I think that it's not even, it's not, it's not being uncharitable to say that the first part of that question is, is factually incorrect.
Because the priors are given a lot of cases.
I think the point with rationality is, is that if you are dedicated to epistemic rationality, you will have better knowledge because you will have, you know, weeded out the bad knowledge that you have and kept only the good stuff.
And you will also tend to get better intuition because you'll be able to figure out that all these intuitions usually don't work and I shouldn't rely on them.
And these other intuitions tend to work better and I'll adopt them instead.
Like either way, you're working on knowledge and intuition, but if you apply the methods of rationality, you have generally the hope is better knowledge and better intuition through the, you know, rationality process.
I think that's a good answer.
I've got nothing to add.
Okay.
So we move on in the question.
Sure.
Is that too arrogant sounding?
I always feel like I sound like an arrogant prick when I say that rationality is the best way to get these things.
I mean, you're not saying I'm the rationality expert and, you know, that's why I have everything I want.
I'm just trying to get a little bit better.
These are the tools I used to get a little bit better when I can.
And I think that's a fair way of putting it, right?
So like, and I'm not sure if it's unfair to put it this way or not, but like, what's the alternative?
It's like, am I going to get there by other means?
Like just trusting my friends over and over.
Trust your gut.
Yeah.
But the thing is like, what if your gut sucks?
Your gut does suck.
That's the whole point of this.
Exactly.
And just using the example of chess again, they can intuit through a game.
That's why you can play speed chess and kick ass if you're really good at chess.
Because they're not even thinking about it.
They just, they can look at a board.
They've seen it a hundred thousand times.
They can just go right through it and play it, you know, as fast as it takes to move the pieces.
But the difference is that, I guess, what other things would you use to guide your decision making?
I don't know.
The person tries to kind of clarify that previous sentence here by saying, I don't include empirical decision making as inherently rational here.
So for example, if give directly, we're the best charity, I see that as more of a data driven outcome than a rational one.
So I guess what they're saying is, they don't see data, evidence-based decision making as inherently rational?
I challenge politely.
You challenge what I'm saying?
No, what the assertion is.
One of the most important tools of rationality is data and empiricism.
And yeah, so like, if you don't count empirical decision making as rationality, I think maybe it would be unfair to equate them.
We may have just differing definitions because rationality doesn't have a lock on data and empiricism.
I mean, that's been with humans for a long time.
So maybe he's discounting it for that, but we use data and empiricism.
Or he's asking for examples outside of empirical data that count as rational decision making.
Because that sounds like a harder question because we've had empiricists around, you know, formal empiricists for at least, I think empiricism was a school of philosophy and starting in what, 15, 1600?
So like, I mean, there's been people dedicated to that for centuries, but they haven't been Bayesian rationalists.
So I guess where does Bayesian rationality come in outside of empiricism?
I think Bayesian rationality incorporates empiricism but isn't equated with it.
Well, Bayesian rationality is a little bit different.
So the way I was trying to explain it to my mom the other day was Bayesian rationality is like the difference between Bayesian statistics and the standard statistical model.
Frequentist.
Frequentist.
I believe, yeah.
Okay.
This is the term that I've seen anyway.
So, and we've seen it work better, get closer to the truth in statistical models, which is why there's a huge movement towards using it in physics and genetics, right?
And it handles new problems better.
Yeah.
And it's updating, it is updating your priors, it's updating your probabilities.
I don't want to.
Right, based on additional evidence.
Yes, it is.
I don't want to paint Bayesianism as like the ultimate end all be all of statistics.
But I think it's another tool that if you don't use it, you're handicapping yourself.
But I've seen arguments for why Bayesianism isn't always the best way to go, that there's other ways of analyzing data as well.
Well, it depends on the problem too, right?
Yeah.
But I mean, the fact that Bayesianism is getting a lot more, the reason I like Bayesianism a lot is because it is the formalized version of how I prefer everyone to think intuitively, that rather thinking in absolutes, you always think in probability distributions.
And when you get into the nitty gritty of which statistic is best, I'm not a scientist or statistician for that point, for that matter.
So I don't want to lay down my flag and say, no, Bayesianism is better in all cases and this is why.
But I think that Bayesianism has a lot going for it that if people applied it more to their day-to-day lives would help people update their beliefs, I guess.
But the base that's what it is is statistical method, right?
So it is at its base empirical, right?
Right, but we don't use it that way in our day-to-day life.
I would on an 80% chance that I'm going to get to work without, you know, within my 15-minute window.
And why not? You could never be late.
But the thing is, I do that every day and that's why I leave 10 minutes early, right?
Right.
Because I have a heuristic now for about how much time I need to get to work to be on time 90% of the time.
Right.
But I guess that was calibrated through that sort of updating.
Your first step is to look at your phone. Well, it says it's a 36-minute trip and then 55 minutes later you roll into the parking lot.
I started working this job before we had those smart phones that could tell you the minutes, so yeah, for me it was just a bunch of trial and error.
And I update it a little bit every time.
But you should be, one, you do assign probabilities to things automatically.
Yeah, but not consciously.
Not consciously.
And two, it might help to make it more conscious.
It probably would, yes.
That's why I'm still aspiring.
For me, the problem of always applying, trying to put numbers on it, is I always feel like I'm just, like, humoring myself.
Because I'm not sure if this is 80% or 90% sure that I'll make it to work on time, but I'm pretty sure and that's why I'm leaving now.
Keep doing it and you'll get better.
I guess. I need to, maybe I should start writing it down or something.
Calibrate those.
Nope.
Oh, okay.
Do you do it with anything?
Probably.
Like, I mean, like, keep track of it.
I can't think of something offhand, but I've definitely had conversations with friends in which I ask them to put down a probability on what they think is true.
I'm going to take an example from my life of someone who I consider not entirely trustworthy.
And this is a coworker that whenever, no matter where she's working or who she's working with, it's always a terrible place and everyone's out to get her.
And so I have a prior now that if she says something bad about her coworkers, it's probably a problem with her and I'm not changing how I view about the people she's talking about at all.
The horns effect.
Well, just from falling in the hole.
No, just from my experience with her, her, her narrative of other people is not reliable.
So I can't use it to update one way or the other.
I think the horns effect would be assuming the opposite of what she said, necessarily, or I guess maybe a little bit.
I'm assuming that since she is an unreliable person in general and because you have this previous experience with her as an unreliable person, you're using your experience, but also it might be an emotional like, oh, she's complaining again.
I've had that with a coworker too.
And you know what?
It resulted in me not listening to some really good ideas.
And yes, it would be if it was something accounting related that I was dismissing because I was, you know, used to dismissing her opinions on people, that would definitely be the horns effect.
I trust her to do her job entirely well.
It's just when she talks about other people, I don't pay attention to that.
But I think that is where the probability comes in.
I have assigned very little probability to what she's saying being accurate and I still listen to it and kind of file it away, I guess.
You know, I've never actually put like, I put 99% on her not being reliable when describing other people.
I just think, yeah, I'm not going to pay attention to that.
And I don't think people generally do think of numbers on those sorts of personal day-to-day issues.
I do think that the difference though between that and the horns effect is that you're not looking at, how do I feel about her?
So that's how I'm going to assess what she's saying.
You're saying how reliable is this person been in the past?
And that, I think, is a much more sound way of looking at things.
So if you have a really highly...
It's a piece of evidence. The question is, how good is it?
Also, if you have a really...
I've been working with her for five years.
If you have a really reliable doctor with great reviews online or something, you might trust their diagnosis more than your friend in med school who is getting straight Ds.
Right.
So you're intuitively putting percentages on things, but you don't have actual numbers in your head.
And you might even like your friend more.
Right.
But it's just, well, I'm in this area and on this topic, this person probably knows more.
So, I don't know. But, yeah.
Was there anything else in the question that we have to address?
Let me just double check. I don't think so.
So then they ended by thanking you again for putting together the Harry Potter and the Methods of Rationality podcast for me.
The podcast version and they're very nice and I thank them and hopefully we'll get their Reddit name.
So we can thank them properly for submitting a question that we then pulled apart and probably terribly misinterpreted.
I would like to, yes. I would first of all like to invite this person to, once they hear this, to write back and tell me if they think we addressed the question, if they think we were fair.
If they have any follow-up questions.
And really, anyone who's listening to this, if you have anything to say about this question, like, what did we miss?
What would you have liked to have said if you were on this podcast?
Or what should we address? Leave us a comment or something. We will read these or send us an email.
We'll totally read them on, not online, but, you know, on the podcast and respond to them.
Okay. How can they leave a comment and how can they send us an email?
Go to theBaseonConspiracy.com and click on the comments for the episode that you want to comment on.
Or they can send us an email at BayesianConspiracyPodcast at gmail.com.
Unfortunately, the Bayesian Conspiracy was taken.
Yes, it was.
Somehow.
Yes.
I think that was it. Did we have anything else we were going to?
No, if anyone else has any other questions, we've got like a list of topics that's going to keep us going for a year at our schedule.
But if anyone wants to send any questions or anything, we will be glad to take them.
Thoughts, suggestions, comments, concerns.
Tell us how much we suck.
How we can make this better.
The first step to getting better is knowing how much you suck.
But constructively.
No, that's true. Don't be jerks.
All right.
Because I can't take any more trivia, dude.
Hey!
I have nothing to contribute. Have we stopped?
Wait, wait, wait, wait. Before we go, we have to do our sign-off.
Thanks for listening. I'm Inyash.
I'm Katrina.
And I'm Stephen. We'll see you next time on the Bayesian Conspiracy Podcast.
Oh, tell them that we'll see them in two weeks.
Oh, yeah. We'll see you in two weeks on the next episode of the Bayesian Conspiracy Podcast.
