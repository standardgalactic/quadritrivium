Tony answer that one, it's a trap.
But as far as being able to just dissociate yourself completely from, I guess, emotional effect to any problem at all, it's going to be really, really hard.
If doable at all.
But I think the goal is to be aware of the bias and then try to prime yourself against it.
Yeah.
We're going to get into a couple of tools.
I just wanted to...
I ran across the Horn's example really recently when I was over at dinner with my parents.
Someone brought up George W. Bush and they were a fairly liberal family.
And I was trying to say, look, not everything he did was awful.
I mean, the guy sucks.
Listen, Hitler had some good ideas.
Thank you.
But like, he apparently had a really good AIDS program that helped out in Africa quite a bit.
And the person, unnamed person that was sitting next to me was like, yeah, well, fuck him and fuck that stupid AIDS program and didn't, you know, it's like, no.
It must have been bad because that's a bad person and we don't like him.
And I'm like, you know, he is a terrible person, but come on.
He did a good thing, right?
He might even be an okay person.
I've never met him or even known.
Even if I'd met him at a conference or something, it's not like I know him as an individual.
Everyone's nice personally.
I've heard a lot of people say that he's a nice person personally.
Sure.
But yeah, so it's who the unnamed person sitting next to you didn't make an effort to calibrate against the fact that they hate George Bush.
versus assessing the effectiveness of the AIDS program.
Yeah, well, and I will say I was greatly exaggerating for story effect just now.
It's not that they said fuck his AIDS program.
They started coming up with things like I'm sure it wasn't that effective.
And it could have been a lot better than it actually was merely because he ran it and the person didn't actually have any data to back any of this up.
That's actually a stronger, I think that's more interesting case than the hyperbole that you gave because that's saying, that's immediately not saying, well screw him and screw everything he did.
It's saying, I bet that sucked because he did it.
Well, yeah, that's the best example of the horns effect there is.
Like if we had had a different president, he could have done the AIDS program better.
Right.
Like maybe, but he did good.
I feel like this happens to all my exes.
Explain that they think bad things about you.
Yeah, but I'm going to turn that around to have me thinking it instead.
Okay.
Because I think it's rude the other way.
Anyway, so when you break up with somebody, I think sometimes for some people, it might take a while to remember that there were good things about them and that, you know, that it's not all terrible and they're not a terrible person and they're normal human being.
That can be a bit of a survival strategy too.
Yes.
I broke up with my wife, my first wife way back when we stayed friends and then eventually we kind of got back together again and that was just a clusterfuck because the reason we broke up is we were not good together.
And so we lost another two years with the whole, you know, getting back together and then I was like, no, I just had the second time we broke up.
I was just like, I'm done with you.
Everything about you is horrible and I can't be near you.
And then I was actually able to go on with my life.
Okay.
Yeah.
So sometimes, so these heuristics.
Shit.
Is that an example of where it's rational to be irrational?
That's what that's what we're saying.
Maybe it is or rather not necessarily rational but helpful because it's not rational to be irrational.
Right.
It is a helpful survival technique to be irrational.
I know she is actually a pretty good person all over.
We just we can't be together because we might fall back together.
That's like meta rationality because I made the decision to be, you know, about her.
Everything doesn't have to be rash.
That's what I was going to say that was that your goal wasn't to properly in that moment wasn't to properly assess is this person, you know, a good person or not.
It's I don't want to be miserable with this person.
So what do I need to do to make that happen?
Well, if I if I express to myself and to her that I think that, you know, she's terrible and I want to be around her.
That'll get that'll get the job done.
I don't know about you, but one of my goals is to be a kinder, better person.
So I think that's where rationality can actually or rather these rationality tools can really help.
One of the ones is the principle of charity.
It's a good one.
This is my favorite technique in which you evaluate your opponents position as if it made the most amount of sense possible, given the wording of the argument.
And again, this reminds me of arguments, maybe with a significant other or a family member where you get into a place where you want to misinterpret or you're angry and you're hurt.
So it's not that you want to, but you misinterpret everything they say to be the worst possible meaning.
And you see that a lot on the Internet, too.
It's so sad.
It seems to be like the main reason you should never read the comments.
Someone says something.
That's not what they meant.
So there was a quote that I really loved about this from a lesser on post that I came across years ago, and I don't think I found these regularly, but it was linked to somewhere.
And it was, if you're interested in being on the right side of disputes, you will refute your opponent's arguments.
But if you're interested in producing truth, you'll fix your opponent's arguments for them.
To win, you must fight not only the creature you encounter, you must fight the most horrible thing that could be constructed from its corpse.
And that person's username was black belt Bayesian.
So that's a good example of steelmaning, which to me I think is like principle of charity 2.0, right?
Yes, exactly.
So the definition about that that I pulled from lesser on, I believe, is a straw man is a misrepresentation of someone's position or argument that's easy to defeat.
While steelman is an improvement on somebody's position or argument that's harder to defeat than their originally state position or argument.
I personally, when I've used this before, I don't think it has to be better than their originally stated argument, but I think that it makes sense to pull their strongest arguments and go up against those rather than their weakest ones.
It's game time.
Can we steelman the question that we got?
From that person?
Yeah.
I think it's too complex.
I feel like it's pretty solid on its own, plus it's complex, but we can give it a shot.
I don't know, it didn't seem ill-formed or anything like that, right?
So to me, steelmaning would be, I don't know, I'm trying to think of a really non-inflammatory example, but it's hard to...
I think we have been.
I think we've been trying to characterize, we've been using the principle of charity, we've been trying to characterize the question
in ways that we can address, right?
In ways that, well, do they mean this?
Well, that would make sense.
So they mean this, that would make a lot of sense.
And we've also, at least, I certainly haven't, and you guys have too, we've been disagreeing with, we've been making arguments and then, well, what situation would it be the best thing to not be a-rationally, right?
Yeah.
We've already been playing this game.
Yeah, I think that that might be less of a game time thing and more of a welcome to the show kind of thing.
We would all the time.
I mean, it's, especially when you're coming across a new argument for something, right?
So one of my intro to philosophy classes, one of the best learning, I don't know what you call them, not examples, but...
Where they try and give people to do things, whatever.
Exercises?
Exercises, thank you.
Was we had covered the, each side of the, I guess, teaching creationism in science classrooms debate, which basically everybody, most people, I assume, walked into that room already, like, having a belief about it before it was, you know, expressed and articulated.
But nevertheless, the arguments were given and then the teacher had us raise our hands.
If you, you know, all right, creationism, not in the science classroom.
Everyone raise your hands to beliefs and then vice versa.
All right, cool.
So you, everyone who believed that creationism should be taught in the classroom, moved to this side and now argue that creationism shouldn't be taught in this classroom or in the science classroom.
And I thought that was fun because I then was forced to think of the best because, you know, I can't remember if there was a prize or if it was just winning, but I'm like, all right, fine.
We haven't, all the arguments we covered sucked because I already thought of them and dismissed them.
But what's the best thing I can think of?
Why on earth should we try and teach it?
I think I've heard that called the ideological turing test.
Yes.
When your opponent cannot tell if you are natively for or against the position by your argument.
Oh man.
Oh, that's a fun one.
Yeah, like, oh, is he someone who hates this argument, but is pretending to like it?
Or does he actually like it?
Right.
I have something for you to steal, man.
And it's the next part of the question.
Oh, before we get to that.
Oh, okay.
No, it's okay.
I was just going to say, since we got on the creationism thing, I think one of my favorite arguments for teaching creationism in the classroom is that it should always be taught by someone who is scientifically literate.
So we can demonstrate this is how science works and this is why it's not science.
Creationism is a perfect example of the traps you can fall into if you have a cursory knowledge of science, but you don't actually do it using the tools of the scientific method.
I wish I had thought of that when I had this example.
Well, but then someone could tell obviously you are not for creationism.
But that wasn't the goal of the exercise.
The goal of the exercise was just to argue for it, to argue for position you disagreed with.
And I think I settled on something.
This was like eight years ago.
I'm trying to remember.
I think I settled on something along the lines of pluralistic education or something, you know, teach.
I think I tried to think of the best form of teach both sides and, you know, whatever I only had a few minutes to think, but that was, I think that's the best way to put it is, yes, we should teach it.
Followed by an example of this is exactly how science isn't done.
That would have been so awesome.
So moving on.
So the next part is the Bayesian priors for any real life problem aren't available.
So if you're estimating, how are you doing better than someone using their knowledge and intuition?
So the way that I would respond to this, which is not at all steel manning is that that the writer maybe is a little bit confused because we always go into things with Bayesian priors.
It's a definition of what they are.
Yes, we always have, we are constantly giving outcomes probabilities, right?
So, and we update those as we become more experienced and have knowledge and have, and the reason that we have intuition to draw on is intuition draws from our stores of knowledge and experience, which is our priors.
So we can make it even easier than that and just say, the commenter said that real life problems never have what, given Bayesian priors or something.
And that's just false.
He assumes Bayesian priors are a tag?
You are doing a bad job at steel manning.
I'm not going to straw man, but I'm going to look at it and then we can steel man it.
But just looking at it as given, so I mean, it depends on the problem, right?
So like, if I want to know the accuracy of my pregnancy test or someone else's pregnancy test rather, I probably wouldn't come up positive.
But in general, although I have heard anecdotal stories of some types of testicular cancer giving positive results on pregnancy tests.
So something to keep in mind.
Yes, we should all keep that in mind.
Don't quote me on that, we're not citing that.
But the odds are given on the package and in the included documentation.
So like, given a positive pregnancy test, you know with like what 98% probability that you're pregnant.
If it's within X number of days of your missed period.
Exactly.
And so that's exactly the appropriate way to look at it and then you get your prior from that.
But then I'm assuming that in any real world problem, maybe they meant something like that you just encounter, you know, without prep time.
But I'm trying to think of an example where you likely wouldn't have any priors at all like so I got to start my car tomorrow and it doesn't start.
Well, there are a lot of things, you know, but I could I could look at what other issues I've had with it or something like that to give me.
Depends on how many times you'd run into this problem before.
What are the short of symptoms like is it just not starting period is it doing this and not that even even if you are thrown into a position for example for me it would be which sports star do you think is going to win the game of you know some random game and I don't care about sports or
any people even then you you have some priors because you are I would be able to say I have no idea.
So I'd be able to assign what would be like their two people 5050 would be my I have no idea priors.
Yeah, and then you could ask your your sports savvy friend.
Hey, who do you think this is and then you can say okay well how much are they savvy about sports, etc.
And then and exactly but then but then you can update accordingly and say well they.
They're a big fan of this team so they're always going to say they're going to win so I can't use his knowledge or they've or they've won their fantasy league every year for the last eight years.
They I'll bet 99% or but I'll bet 90% now that this person's going to win because that person knows what's up and they're they're not motivated to lie to me about this.
You should go with 90% person's going to win.
You should go with whatever your friend said.
If your friend said there's only 70% chance they're going to win.
Oh yeah.
Yeah.
Okay, perfect.
So update your beliefs accordingly.
There's actually a really good in the site where something worth diving into later with problems with Bayesian updating.
But you can get you can run into examples where like that sort of reasoning what does my friend think and they're they tend to be reliable can run into like sort of wider problems.
Like I'm not sure what the percentage is 80 70% of the of the world population is religious.
Well, they can't all be idiots.
So should I update in favor of becoming religious, you know, given that it's such a popular belief and there's a confident in it.
But then that that sort of runs into the problem because they're not arriving at that conclusion with Bayesian here with Bayesian guidance.
So I don't know.
But I thought that was a fun.
