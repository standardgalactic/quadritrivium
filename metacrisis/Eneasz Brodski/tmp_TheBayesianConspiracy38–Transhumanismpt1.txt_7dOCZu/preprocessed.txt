Hi, welcome to the Basin Conspiracy, I'm Eniash Brodsky.
I'm Steven Zeeber.
I'm Shelley.
Hi, Shelley.
Hey!
And today we are going to talk about transhumanism.
Ooh.
It has a spooky connotation, but I challenge this boogie.
I would say that all three of us would consider ourselves transhumanists.
That's correct?
I definitely would.
Yes.
Cool.
So what do we consider ourselves?
Not using the word transhumanist.
Oh, son of a bitch.
I'd be like, a consistent humanist?
Yes.
I think I just re-read the simplified humanism post before this.
Yeah.
But I don't want to hijack this because Shelley eventually brought it up, so we want to...
Oh, well, the simplified humanism post by Eleazar Yudkowski is really good introduction
to transhumanism, and the idea is just that if you want good things for people, you may
even continue to want good things beyond what's considered normal or baseline.
I believe the two things that he spoke about in his simplified humanism post was old age
and health.
That if you see a four-year-old on a train track, it's the ethical thing to save them.
If there is a 45-year-old who is sick, it is the good thing to make them healthy again.
And likewise, if you see a 100-year-old on a train track, it's good to save them.
And if there's a 140-year-old who feels like he has a cough or something, it's good to
cure him of that, and there's no limit to that.
There's always more life, more health is better, period, no matter how old you are.
And it's really weird that some people think that there should be a point where suddenly
it's better to be dead.
Right.
So it's weird from where we're sitting, but I don't know if that's fair to people who've
never heard of the idea.
The idea of radical life extension has this knee-jerk reluctance to it that while I don't
agree with, I think I could sympathize with just because it does sound weird in the future
and all that stuff, but I don't want to say it's just weird because it is, but I want
to give some, if anyone was repelled by that way of raising it, rest assured we're sympathetic
sometimes.
Yeah, deathism is widely held by a lot of people if they bother to think about it.
I don't want to turn this into a let's talk about things that we hate episode, but I saw
an article very recently at the Daily Telegraph, I think it was, where someone wrote about
who was it?
Mark Zuckerberg recently invested $3 billion into curing all disease, which is wildly optimistic,
but at least he's trying, right?
And they wrote, oh, by the way, Mark Zuckerberg, this is the worst idea ever.
What about when people stop dying and we overpopulate the planet and then everyone dies from being
scrunched up too close together or some shit like that?
I was like, fuck, like literally just knee-jerk.
Oh, curing disease is bad because people would live longer.
That is what the argument was.
I'm like, get the fuck out of here.
You don't deserve to have a voice.
The argument from overpopulation, I think, is a pretty quick go-to for a lot of people,
which I mean, is willing, like I'm willing to engage that, but I think it's pretty quick
to knock down.
I think it's just what people come up with immediately to complete the pattern.
But I mean, A, there's a lot more space on Earth than we're currently utilizing, and
B, the list of possible habitable spaces in the universe does not end with Earth's surface.
So there's all kinds of room out there for us to stretch out into.
And I've always been of the opinion that, yes, this would be a problem, but there are
many ways to solve this problem, and possibly the worst possible way to solve this problem
is literally kill everyone that is currently alive.
Actually, I think I remember that being a quote from you that I've used other places
that of all the problems that death is supposed to solve, it doesn't solve any of those problems
well, which I remember hearing from you in like a little talking heads thing that you
did with PZ Myers, Eli Diodkowski, and David Brennan.
That's right.
So we'll have to link to that because that was fun.
OK.
So I guess we've started on one aspect of transhumanity already, which is life extension.
And maybe you can go over some of the objections, like the one where overpopulation
is one of the most common objections.
Another one is that people think that you'll get bored or you won't value your
life as much.
I mean, I guess that's possible, but no one's like stopping you from killing yourself
if you get too bored.
That's sort of always been my thing is like the idea of compulsory living forever
sounds sounds like it could go really bad.
Yeah. So I'm all about like just being able to check out whenever I want.
And if that's in 50 years or 50 million years, you know, I'd like that window.
And I think a lot of the especially religious objectors to life extension
would really come down hard on the side of saying,
death should creep up on you gradually without your consent.
But why?
But why is a very good question?
And more specifically, why religious people?
Oh, because, you know, like suicide being not good.
And just taking life and death into your own hands is what a lot of religious people don't like.
Yeah. But I mean, religions have evolved so much as as technology has come.
I'm sure one of the next big steps once immortality was achieved would be like,
oh, hey, actually, it's all right to kill yourself in certain situations.
Like you got to go get the priest's blessing or something.
And then he's like, yes, it is OK with the big GZ.
Do your thing.
You'll probably put it just like that, too.
He probably wouldn't use those terms.
And I sort of apologize to anyone who's offended.
Um, if you're going to talk about Catholicism as a particular example,
Catholicism has still not condoned birth control,
even though individual Catholics may have movements.
The official Catholicism has not, I predict, never will.
Hasn't the Pope officially condoned it?
No. Oh, he's he has said things like it is for your own conscience or something.
He he's basically said things like
a lot of things that are considered sinful
still won't may not prevent you from going to heaven
if she's tried and give live a good life up in other ways, right?
Like this is the pope that said even atheists can get to heaven.
Right. Yeah, I like this pope a lot.
He's he's good for a pope.
Yeah, absolutely.
I think his perspective, though, is like maybe you'll have a deathbed
conversion or repentance.
And so, like, even if you're doing all these things, you can still go to heaven.
And I don't know, maybe if with enough time in a purgatory,
you can get there even if you didn't convert on your deathbed.
I'm not sure what the official Catholic doctrine on this is.
It could be something like that.
It's probably something like, hey, man, we don't try to speak for God in all things.
So who knows what's possible.
I didn't know that posthumous conversion was possible.
And if it is, I would think I'd probably convert fairly quickly.
And I'm not sure it's exactly about conversion, but like you still have sins
on your soul to work off in purgatory.
But like no one goes straight to heaven in the Catholic mythology.
Oh, well, if that works out, then Jesus got straight to heaven.
He had to spend three days in hell.
Yeah, but I mean, I'm sure it was cake for him.
No, I'm sure that was probably the nice part of hell.
Just the part where like you're trying to fit a sheet over over mattress
and it just the last corner keeps popping up the whole time.
That's that's Jesus as hell.
Yeah. Sorry, that was probably way out there.
But this whole tangent was just my way of saying like, I think there are
definitely some religions that you can't expect to embrace immortality
or or suicide in the future.
And yes, some religions will definitely always cling to certain things
that will never change.
I think the indictment against suicide and religions was a fairly smart move
because if you believed everything that the religion teaches minus the suicide
getting to heaven as quickly as possible sounds like the best way to live your life.
Right. Yeah.
So it's like the first second that you accept it and have the means
you just kill yourself and get straight there.
Yeah. So saying saying you can't do that is great
if you're a memetic fitness for your religion.
The most the most altruistic thing a priest could do is murder all the babies
as soon as they're born because like, boom, straight ticket to heaven.
Well, I mean, not straight because no one goes straight to heaven,
but at least you don't spend a lot of time in purgatory
because you haven't sinned too much yet.
All right, so this might depending on your religion,
some babies just go straight to heaven with no time in purgatory.
So yes, you're damning your own soul to hell,
but think of how many people you're saving.
It's totally worth it because you only can go to hell once,
but you can get a lot of babies into heaven in a lifetime.
So this might be a digression, but you're you might know the answer.
I don't really get what is it that babies have sinned for?
I thought that wasn't the whole thing that Jesus died for everybody sins.
Well, yes, which is why it's possible to go to.
OK, see, this depends a lot on which actual religion you're talking about
because there are many flavors of Christianity.
Right.
But one of the big things is that everyone is born with original sin.
But I didn't.
So Jesus died for all of our sins, except original sin, including original sin.
But before Jesus died, you no one could get into heaven period.
Oh, so he got there. Now it's possible to get there.
You could have done better than that.
No, so.
Are you saying you could be a better Jesus than Jesus?
I would have said, you know what?
That whole hell of business is kind of fucked up, right?
Most religions do consider the crucifixion of Jesus to be retroactive,
but you do have to be baptized in many religions
in order for that to apply to you.
Weird. All right.
The whole I mean, and there's a lot of them.
We agreed way back in the day.
This wouldn't be like an anti-religion podcast.
Yeah, I'm going to just point out that that makes no sense to me.
Well, I mean, it's not supposed to necessarily make sense.
It's supposed to form a community, which is very different from making sense.
Well, but it's also supposed to make sense.
No, really, they're going to say it's supposed to make sense.
It really is supposed to make sense.
You think? Yeah, I think if it was supposed to make sense,
they could have done a much better job.
Well, I think that they're, I mean, you're to be fair, you're the expert.
No, I'm not.
Well, you're the only one who came from a fundamentalist,
like Christian background.
So I mean, a lot of theologians throughout history
have done a really good job given what, you know, they had to work with.
That's true. Yeah, given their limitations.
Yeah. Yeah, I guess I guess my argument is
if you were really dedicated to making sense,
you would reexamine some of the basic premises
because you can't make complete sense out of what you got.
But you're right, given what you got, some of them do a decent job.
And if their goal wasn't to make sense, they would spend more time.
They would they would just wave their hands.
But if someone points out, you know, a logical contradiction or some confusion,
they'd be like, you know, it's not supposed to make sense.
Yeah, they would just say that they wouldn't write books
or they would say you have to have faith.
Well, they often, I think, do say that,
but it comes at the end of, you know, book link through bottles, right?
Right. Sometimes. Yeah. Yeah.
All right. Where were we? Back to back to our objections to immortality
or life extension, the religious objections,
the ones where the only way you could die is if you killed yourself
and that's bad and the ones where you wouldn't appreciate life.
Some of them are what one in particular is that if you get old,
then you won't be able to change your mind.
And then you'll just have a whole bunch of like
calcified opinions hanging around forever.
I think that is, again, one of those problems
that is worst solved by death by killing everyone who doesn't change their mind anymore.
I mean, maybe there will be a period of centuries or millennia
where a lot of people don't change their mind, but I figure at some point
we will figure out a technology to help people change their minds again
to increase brain plasticity or openness to new experiences or something.
And I think there is actually some pretty good evidence that age by itself
doesn't as much as you might think,
set your opinions or keep you from learning.
A lot of older scientists are still doing great innovations in their old age.
They're questioning past scientific results.
There they are learning new things.
I think it is a lot of maybe the limitations of some older people
or just due to like the beginnings of dementia, perhaps are.
I hear a lot of older people are actually a lot more flexible than younger people are.
Yeah, they they have more experience.
They don't they aren't as filled with the fervor of true belief
that you have when you are very young or that some people have.
I was victim to that.
I think there's a couple of things there that one,
many old people have had the experience of changing their mind a bunch of times
and it was less painful for them to do it one more time.
The other thing that today's old people don't really represent
what old people will be like in 70 years from now,
because they grew up, it was very easy to be an old person up until like 15 years ago
and never have internet.
Right. And so I mean, not the internet is the gateway to opening your mind,
but it often can be.
And so like growing up in a worldwide community rather than like the community of your town,
I think make opens is probably one avenue to like making people more plastic
in their belief, rigidity.
I like where you're going with this.
I do think that change, cultural change has happened very quickly
in the last hundred years, and that may have presented
an extra challenge for people to remain flexible in their old age.
I know I've said this to people before.
I'm not sure if I've said it on the podcast before,
but one of the reasons I do consider the rationality movement important
and I want to spread it is because I think a lot of the rationality movement
is about changing your mind and how to change your mind correctly,
how to be open to new evidence.
And since I want to live for as long as possible in the trillions of years
and I want that for as many people as possible,
I feel like this is a very important skill slash social technology
that we will have to be able to implement on a wide scale
because having everyone stuck in their current mentality forever
is probably a bad idea as we discover new things.
We probably will want to change your mind on things.
And so since I think rationality is such a good way to
pass that skill along of being able to change your mind and adapt to new information,
I think it's important to pass along to people.
Like I see I very rarely see people who are rabid in their beliefs
on in the rationalist community and as much as I do with like just
my general friends out in the public, they they they're like,
yes, I believe very strongly in this one thing and I will never change.
And sometimes I'm like, I mean, I agree with you on that thing
and I am happy you hold that belief.
But on the other hand, the and I will never change thing kind of worries me.
And also, why you got to demonize everyone that disagrees with you?
They can have honest disagreements.
This is why we talk about things.
I've run into that a lot in the last year.
Maybe it kind of depends on what you're disagreeing about, though,
if you're disagreeing about a claim of fact.
Oh, it's kind of weird to strongly believe in.
Yeah, it's more about things like social issues and then moral issues
and how to approach the world, not actual claims of fact.
Those are much easier to do.
Yeah, I know I think of examples of people that would say, I, you know,
I resent any investigation into this fact because I want this to be true
or not true or something, right?
Not typically like rationalist people, but that's definitely out there.
Well, some of them are like incorporating claims of fact, things like I
maybe that the death ism could be a good example.
Like I strongly believe that being immortal is bad for humans.
And that incorporates some some empirical claims as well as moral claims.
And someone said like they'd never change their mind on that.
I just whenever someone says something like that, I'm like, hey,
how about instead of killing people, we teach the rationality?
It's a much better solution.
But killing is so much easier.
I mean, yes, there's that.
Yeah, I mean, sort of killing one person is so much easier.
Once you got to start organizing a genocide, it gets harder.
Well, letting people die is very easy.
That's true.
All you got to do is not do anything.
Yeah.
And part of I think transhumanist mindset, maybe just like maybe
rationality mindset, the kind of overlap is like, just because it's hard,
doesn't mean like that we should do it or that we shouldn't do it, rather.
You know, if it takes more effort, then it takes more effort.
You know, buckle down, get it done.
The if you're in the 16th century and you come across smallpox, you should cure
smallpox and the fact that you can't sucks, but you still should if you could.
And it was, I guess I shouldn't have been so surprised when I found out that
people objected to curing smallpox back in the day.
But I was very shocked.
Not heard of that.
I mean, I don't know how many people, maybe just a few.
I don't know, but the fact that anyone, yeah, some people were writing things
about like trying to avoid the wrath of God and this is a sin.
I was going to make you pause and say guess who was opposing it, but you
probably you probably could have guessed.
I actually, that would not have been my first guess.
That probably would have been my second guess.
Who would be your first guess?
My first guess would be people who say that it makes the human race weak to
allow the weakest immune system people to survive instead of being
weeded out by smallpox.
That's a good first guess.
I like that.
Probably both, I don't know.
But yeah, I only heard it in the context of religious people.
Yeah.
Like, and.
I mean, they were the people who were most against organ transplants too,
right?
Most against anything that like saves lives and makes the world a better place.
Sorry.
I hear they were the most against throwing buckets of water on fires.
The fire is God's will.
Exactly.
He wouldn't have let your house on fire if he didn't want you to burn to death.
You should, you should also not go outside the house.
Okay.
So your, your blog is called that is bad.
Yes, it is.
Do you want to need to talk about how that, that came to be important enough to
you that you would name your whole blog that it was mainly from talking to people
who, who basically had your opinion, not, not necessarily even that it was good
for consequentialist reasons, like we got to avoid overpopulation or whatever,
but who just simply embraced death as a moral good into itself, that it makes
life meaningful and that it is an important phase shift that every human
has to go through.
And what would you do with your life if, if you didn't like have the
experience of watching a loved one die?
It wouldn't be as, as meaningful of a life or something.
And, and I was like, you know what, fuck you, I'm just going to say straight
up as, as an axiom that death is bad period.
And I will not, I will not entertain any arguments to the contrary.
Sounds like you're dangerously close to saying, I'll never change my mind about
that.
I mean, if someone was torturing me and I knew that the rest of my life would
be tortured, then I would prefer death.
Sure.
And I was worse than death, but just in general death is a moral bad thing.
I totally agree.
I was just being tongue-in-cheek.
I'm kind of exactly in your camp.
I, I don't think that, uh, I mean, so like you said, what would you do?
Like the idea that, you know, your life is finite, loved one's life is our
finite makes you appreciate the more, A, I don't know if that's true.
Cause we don't have any counterfactual worlds to compare that to.
We might, we might be more open to appreciating the more if we weren't
constantly thinking, well, they'll be dead, you know, either tomorrow or in 20
years, um, or however many years.
Hey, I don't know if that's true or not, but B, I'm not convinced that
makes it worth it.
So even if, even if I cared about people, I love a little less, but I got to
care about them for a billion years.
Yeah.
Um, it still sounds like it comes out positively that way.
So I, I think really the main problem with all of these is that it just
sounds like it's, it's arguments that people have heard and just rehash
without thinking about it, which is the problem with a lot of, you know, new
ideas or ideas in general.
And there's just, for me personally, there's so many things I want to do.
And it's depressing as hell that in 80 years, you only have time to do a few of
them.
It takes a long time to master a new instrument or a language or an art
form or something.
And, and there's just so many things I want to try and become good at.
And, and you don't have enough time to do all of them.
Cause you know, life currently is finite.
So every time you go down one pathway, that is all those years you will not
have to dedicate to another pathway.
And it kind of sucks that you have to just choose a few things.
I completely agree.
And I'm looking forward to a time when I can do everything I want to do.
I'm like, yes, in three centuries from now, I'll get around to the cello.
That's, that's, I mean, part of my way of looking at it too.
I think, and Shelley did this earlier that like you'll get bored with eternity.
And I did love and method rationality was the most concise version of challenging
that that I'd ever heard, which was, I don't think it was Harry arguing with
Dumbledore and he would, he was explaining to Coral later.
He's like, I don't think Dumbledore had in his mind a mental model of himself,
you know, continuing in the afterlife when he was telling me how bored I'd get
being alive forever.
Right.
And that seems to be, you know, the people who most, I mean, a lot of people are
religious, a lot of people make the argument that you'll get bored.
And it's like, so you're not going to get bored sitting on a, you know,
cloud learning the heart for a trillion years or whatever it is you do.
I guess the idea though is like regular life will get boring, but transcendent
afterlife is a fundamentally different thing.
And then we make, then we make earth transcendent and awesome.
Yeah.
But I also maintain that there's a lot more to get, like a lot more to do, even
in regular boring earth right now, then you can fill in the lifetime.
So once that gets boring, then we'll have plenty of other cool stuff to do.
The Chinese afterlife is really interesting.
Oh, okay.
Actually there's a little wide bunch of diversity of cultures in China.
So I don't want to say the Chinese afterlife, but, uh, at least one of the,
one of the major ones that I've read about, it's basically just real life,
except with ghosts, like they have bureaucracies and, and, uh, people that
they have to file paperwork up to and they have cash problems.
And it is, it is like, you just get life again, except now you have a ghost
body instead and it goes on for a lot longer.
And I, I, it, it almost seems like, oh my God, I thought I was done with this shit.
And now here I go again.
When will this unending hell ever ever?
You just move next door, basically.
Shift one dimension over.
What are you looking forward to in the immortality?
A lot, like the whole, the whole fun theory sequence is perhaps my favorite of
the whole of all the sequences.
Should we, should we get into this now?
Or should we talk a little bit more about the different elements of transhumanism?
Uh, that's a good question.
Did we want to do that?
Cause I remember the second one was IQ, which we didn't even get to in the,
in the simple, simplified humanism.
Oh yeah.
We don't have to go over everything in order.
I'm totally fine to do whatever we want, but I am curious, I kind of
want you to steer this.
So if you want to do more, more exploration, like ground level stuff,
before we get deep into one thing, if you want to get deep on one thing,
then drag ourselves back out and go down to other avenues.
Okay.
I guess I would want to at least briefly mention some of the different paths we
can go down before going down, what any one of them, which is, um, IQ,
intelligence, um, enhancement is really important.
The, well, because first of all, as far as I can tell, intelligence
really impacts your enjoyment of life.
I don't know if this is a controversial opinion, but I, I, I go back to the old
better to be a sad soccer tease than a happy pig quote, just because there
seems to be so much more to enjoy and more ability to enjoy it in different
ways when you have more mental capacity.
And the, the example that Elias are used in the simplified humanism one was
if someone has an IQ of 130 and a lead heavy environment is slowly degrading
that down to 120, most people would say it is a good idea to prevent that
from happening to let him stay at 130.
Uh, but if his sister has an IQ of 120, a lot of people wouldn't say,
let's embark on a program to gradually increase her IQ to 130.
They would be like, now let's just leave her where it is, where she is.
And, and transhumanism doesn't make that claim.
Transhuman says if a higher IQ is better than it would be just as good to
raise her to 130 as it would be to prevent him from dropping down to 120.
And, and I think that is true too.
Why not just keep.
I think there are two elements to consider that are not quite as straightforward
as just like smart as good.
Okay.
Um, one is that it should be their choice.
Like if he wants to let the, his IQ degrade, yeah, there's not going to force
any treatment on him.
I, I don't think we ever mentioned this explicitly, but I do think bodily autonomy
is a core component of transhumanism as well.
Yeah, I agree.
Yeah.
Which is a thing like no one can tell you that you have to do things or that you
can't do things to yourself.
Like if you want to kill yourself, you can.
So we have morphological freedom is one of the key components of my transhumanism.
But the second thing also is that there is an argument for not changing in so
far as it's part, you may be changing things that are part of your identity.
And there are, I can imagine enhancements, the things that could be
considered enhancements that I wouldn't want to, to go through myself in the
interest of still being me and not being like me being dead and replaced by
someone who has all my memories.
Yeah.
Like I'm sure a 200 IQ me would be a drastically different person than the
me I am now at only 198.
From my perspective on changing a lot of these things is that if you change gradually,
then you can still maintain some coherence of identity so that you can get there
eventually in a lot of ways.
Still, there are some enhancements where I wouldn't be able to get there even
gradually, I don't think, but as far as IQ, I feel like you could continue being
yourself as long as you had the ability to do maintenance to continue being coherent.
Can I, I want to touch on those in reverse order.
I like the point of gradual increase because, I mean, in some important way,
I continued from childhood to adulthood in a way that wasn't radically disrupting
overnight, right?
So if I winked out for a second and all of a sudden I have the mind of an adult,
that would have been very disrupting and I might not have appreciated the change,
but going slowly, I liked it, I think more than I would have otherwise.
Although, to be fair, I didn't try the otherwise.
But the other thing was, as far as autonomy, I agree that if someone's in capacity to make
a decision that it's good, but then I'm wondering if that holds true for the reverse case.
So in the example of the sister with the 120 going to 130,
say it was 60 and she could go to 130.
And now at 60, she might not have the capacity to make a choice.
But so do you give her the shot that fixes her brain?
I am tickled pink that you are holding that copy of analog right now because that is what
my story in there is about.
Oh, perfect.
It's about, is it moral to force a change on someone when they are not competent to make
that decision for themselves?
Well, if your answer is no, then I disagree with you.
I mean, at least I softly disagree with you.
I am currently persuaded that lacking the capacity to make a choice,
it's okay to let people who can make good choices make that choice for you.
Just like my go-to example of this is if you offer a three-year-old the chance to get a vaccine,
they're going to say, no, it hurts.
And that you can't explain to them, no, no, seriously,
you don't want to die of mumps, right?
But so you deferred the person with the capacity to make the choice for them
and override that kid's wishes.
Yeah.
I don't see a problem with that.
But I view that three-year-old as actually not having full autonomy
by virtue of the fact that they don't understand.
But.
So you're not actually violating their autonomy.
It is their childlike mind that is violating their autonomy.
Yeah.
But what if the trans are transhuman descendants with IQs and the unmeasurable thousands
look at us and say, we are like children.
We do not have enough autonomy to make that decision.
And so we will make it for you.
It's possible.
And I'm not going to rule it out.
But I feel like the ability to make decisions to the degree where you can have some kind of
sensible autonomy happens pretty low in the IQ scale.
Yeah.
No.
I mean, in the current world, it's pretty...
Okay.
It can be a little bit iffy around the edges when we're talking about people with developmental
disabilities.
But we have just the right now, the straight outline at 18 years of old.
Unless you get emancipated early.
It's where you can actually make your own decisions.
But I think that's because everyone becomes an adult eventually
and everyone wants to have that bodily autonomy.
And in a future world where there is an entire race of humans like us who will never
reach transhuman godhood, the superior race would just have to make that decision
for us by themselves or respect our autonomy if we want not to, right?
I mean, where there is no sharp line to draw anymore.
We will never reach that without their intervention.
I think...
I see where you're going and I like that.
I really like that point because the analogy I think is pretty solid.
But I would like to think that I have more capacity to understand arguments and be persuaded
than a toddler.
Yes, but probably you to a toddler is less of a jump than you to the transhuman gods.
Yeah, but hopefully they could, you know...
So should they respect your autonomy or should they make the right choice and give you the vaccine?
I was going to go with the third option and say that they could probably talk me into it
for reasons that I would agree with.
Now granted, if they have super persuasion powers, that's almost like taking away my autonomy.
But at least, I mean, I don't know if it's...
I think I'm kind of on board with Steven, which is that ability to reason
doesn't require you to be even as smart as Enias.
Like you could be dumber and still have enough...
And still have ability to reason such that the most intelligent creature could still come and
reason with you and respect your autonomy.
This relates to my major beef that whenever...
If you make the wrong choice.
Well, that may...
Reverb to a previous save?
No.
Then that's not respecting your autonomy if they just keep trying until you make the right choice.
No, I personally really strongly believe in the right to harm yourself.
And I agree and that is why I did finally come down on the side of
you should let people make the wrong decision.
Even if it's something to the effect of I refuse the mum's vaccine.
I think we all sort of agree because like Shelley points it out, kids aren't really making the
decision. They're just... They hear it's going to hurt and they say no.
And that's not not a decision or that's not an expression of preference,
but they're not really thinking about it because they can't.
And so their lack of rational capacity does, I think, mean that they don't really have autonomy.
But the way those people would... Those human people would look at us would be
as if we right now went downstairs and saw a group of three toddlers around microphones
discussing whether or not they should be forced to get vaccines.
I don't know if they would think that and they might totally think they would.
We literally do not have the mental capacity to make that decision for ourselves,
much like those toddlers don't.
I mean, maybe.
We may physically not have the capacity because we cannot grasp the things that they're trying to
say, much like a toddler just wouldn't be able to grasp why this is beneficial for them.
Sure. I mean, this sort of relates to my one beef with one of the things Neil deGrasse Tyson
always says when he talks about aliens finding intelligent life on earth.
And he's like, oh yeah, look at Stephen Hawking. He can do complex
higher order mathematics in his head, just like a little four-year-old Timmy over here.
And he jokes that we'll stand in relation to them like we do to ants.
And I don't know... And so they'll... I think I might be butchering the rest of his quote or his
point, but he talks about how they won't feel like we're intelligent.
I strongly disregard that because there's definitely a difference between what we're doing
and what ants are doing and what ants are doing and what rocks are doing.
At the very least, they'll find something interesting here, even if it's super boring to
their standards. Even if something that their lower order pets are operating at the same level
we are, they're still going to say that's super rare and awesome.
I just hope they have an ethical theory close enough to mine to say,
let's let those humans make their own choices. Even though we don't think that they are qualified
to do that. I think the main reason we do it for children is because we know that
the children will grow up and we want them to not die. So we impose our will on them anyway.
But I mean, you could make that same case for the aliens. They're like these stupid humans are
going to die if we don't upgrade them. I don't know, it's just different with children because
I'd go to jail if I didn't try to keep a child alive.
That's a very different kind of argument.
Yes, it is.
I just don't think that differences between IQ are the same at every level.
A child versus you is not the exact same thing as you versus someone with 100 or 200 extra IQ
points. I agree, but were you raising any religious context?
So how would you compare yourself to God in terms of being able to understand anything?
The way you understood God earlier?
Well, as a religious person, you are supposed to say that you can't understand anything about
God hardly at all. It's not inconceivable that a highly advanced race would be as unto God.
But now that I'm not religious, I don't take those mystical,
mysterious ideas seriously anymore.
Okay, so you don't think there's a level past what you could comprehend?
There's definitely things that I can't comprehend, but I don't need to be able to
understand really complicated things in order to have respectable autonomy, whereas a three-year-old
is missing something that is not just inability to understand complicated things.
They're missing something much more related to basic ability to reason.
I think this is one of the reasons I shouldn't have children,
because that would be like, okay, little Timmy, I'm not going to violate your bodily autonomy.
You may run in that street if you want to. I'll just tell you, I think it's a bad idea,
but I also get the impression that if I, based on the other adults I've seen,
that if I were to have a child, all of a sudden those values would change.
And I would be like, get your ass over here, Timmy, or I'll beat you,
because I don't want you to die.
So I maintain that, like with my alien example, I think if God wanted to,
God could convince us of whatever reasons that he had for whatever, right?
So the idea that it might be inscrutable to us, I think, is just hand-waving,
we don't know, kind of thing, what we talked about. It's not supposed to make sense.
You would think that, hey, if you're really as smart as you're, tell yourself you are,
tell us that you are, you could dumb this down for me, right?
Yeah.
What if it forced you to have a phase shift into some sort of spiritual
entity, and you like your physical body?
I mean, well, I'll get back to that in a second, but I wanted to say that,
I think what Shelley was getting at is I think the next important part,
that you can have kids with really high IQs.
That test, they can do things that their peers can't, and that's how they measure IQs.
And I think that running with just that number is not really the most useful metric,
because a lot of other stuff comes along with, like, I think this was on the air.
Taryn, in a previous episode, you and I and him were talking about coming online as a sentient
adult. Kids don't have that, right? So part of that is, or that's a big part of what it means
to be a rational agent. I don't know if that's the main ingredient, or even the biggest,
but it's definitely part of it. The idea to reflect on the fact that not just that things
are happening to you and responding to them, but to be aware of like, oh, I'm currently thinking
about what's happening around me, and I'm thinking about how I'll respond to it,
that's a different level of existence.
When you were a child, you couldn't imagine that being beyond that level of existence, right?
Yes.
And so there is, hypothetically, maybe a level of existence beyond what we're at now,
which we can't imagine. Hypothetically, but I'm not sure that it makes good
sense to assume that there is. No, I don't, I agree with you, but I'm just talking about
hypothetics right now, that there could be a situation where we want someone to
respect our bodily autonomy in a way that we don't for lesser beings.
I think now I'm capable of understanding the argument, look, I really just can't explain
this to you because your mind is literally incapable of handling it. Whereas a kid,
I couldn't understand that argument, right? So if God came to me, or if a transhuman
person with a five-digit IQ would have told me, hey, look, I really just can't explain this to
you, but trust me. And I'm capable now of at least acknowledging that kind of sentence
that's sort of thought. Whereas before, I'm just like, nope, needles hurt or whatever, right?
If you ask me if I wanted to get vaccinated or something. I don't know if there's that much
more to it. I mean, on the plus side, I don't strongly anticipate us running into that situation.
I do see us getting there along the gradual path, probably, or at the very least something
that we do to ourselves, not that is imposed on us from the outside.
And I personally just think that the ingredients necessary for having autonomy that should be
respected is reason and self-aware sapience. And you really don't need to have powerful,
cognitive abilities beyond just what's necessary for those things.
But maybe there's a secret third thing that none of us know about because none of us have it.
Well, in that case, screw that thing.
Cool. That is also the conclusion I finally went with, mainly because I like who I am right now.
Yeah. And I don't know if it was the right answer, but it's the answer I like.
Whoever these super intelligences are that would consider that necessary,
they don't just don't have the same values as I do.
Well, I mean, the whole, we were talking a lot in the terms of we, but it could very well be
a different section of we. It could be like the we that grew up on Mars discovered this technology
first, and now they're still human and they're trying to impose this on the earthlings and
there are things that don't want it. I mean, so I get very similar values since they both,
they grew up human as well. I get the challenge you're trying to make,
but I feel like in that situation, just to run with it as presented,
that you'd get some number of volunteers from earth who would say, no, no, trust me,
you know, I'm going to go try this. Just let me see it. Let me tell you, it turns out,
I go, I'm gone for an hour and I come back. I'm like, you know what, it's actually pretty awesome.
And I'm still the same me. I can answer the same questions. And now I have this mission,
this really cool sense that I can't explain to you. And I'm sure most people will be convinced
by that, but there's going to be some tiny fraction hold out of Amish people who are like,
I lack my lack of zippers and I'm going to stick with it. So that I'm actually, I'm torn with,
like I used to, when I grew up, I thought like the Amish community ideal was kind of like quaint
and cute. And then I realized like, no, kids are like dying of splinters. And so I'm sort of not
in favor of that. So what we're, again, we're, we're hammering on the same difference. I'm not
sure if we disagree on anything, but we might, but like the difference is that like, if an adult
decides, you know what, fuck this, they throw their phone in the river and they're going to just go
live in the woods, cool, go for it. If they drag their kids with them, then we have a problem,
right? Because the kids aren't capable of consenting, but they're, they don't have the autonomy to
disagree. There's, I think a huge barrier that we're regarding a difference like in, in kind of
like minds, right? Now I get like, there could be some third stage here, but I don't...
So not to get all...
I might have just turned my position around.
Not to get all incendiary on you, but in that case, would it be child abuse? If you had a child
that was deaf and you could correct it with a cochlear implant, but you wouldn't because you
grew up deaf and you want them to have the same culture?
I mean, so you're choosing an example that happens to have a real life component to it,
which makes my answer inflammatory, but no, no, but I'll go for it and I'll say yes. I mean,
consider the alternative where you were born deaf and you, the reason that you like it and value
is because you developed this really cool community with people that have the same struggle that you
do and it was valuable to you growing up, but your kid's born as a hearing person and you're like,
I want the new raised deaf and you, you know, grab a chopstick and pop their ear drums or
something, right? So that's child abuse. I don't see how, so like that's the same example as raising
the kids IQ versus not, or, you know, whatever, right? The one from the simplified transhumanism.
So I definitely would see them not necessarily as equivalent because one, you're, you know,
harming and it takes a different kind of person to harm somebody, et cetera, et cetera, but I see
the outcomes as basically the same. You're subtract, like imagine the alternative, like,
I mean, if you want to raise them with adversity and have them have that cool community,
why do you just stop at deafness? Like, why don't you take their eyes out too, right? So like,
that's all kinds of adversity. If adversity makes you a stronger, better person, imagine having no
hands, no eyes and no ears, right? So I mean, how far do you want to turn this up until like, no,
okay, that's, that's got enough nightmare fuel for now. Thank you. I'd slightly modify that by
saying like, in the case where you have to actually make an effort to get them to a plant,
then there's like money and then there's the time and there's the risk. There's always risk with
surgery. But still, I would say in general, like, choosing not to do it is the bad, the wrong choice.
Yeah. And I don't feel like in situations, especially like with cochlear implant, that
you're, that's your true objection that I think surgery is unsafe, right? That's not, I don't
think that's even the objection they would give. They would just say, I don't want my kid to have
them. That's it. I'm the parent. I think choosing not to do it is the wrong choice as well. But I
think that the parents autonomy and wishes should be respected.
What about the Amish kids? Dying with splinters?
Yeah. I mean, it sucks to be that Amish kid, but they signed up for it by being born into an Amish
family. As long as you're willing to acknowledge how unfair that is, and run with that to the
bottom, then that's fine. Okay. And I would also mention that, although I think that being, that
being able to hear is better than being deaf, it's not like a huge negative to be deaf.
That's true. It's a small. It's a soft example. That's that. I mean, you are losing,
some major components of happiness. Some people like music, and we won't have that.
It's also weird because while I'm like, it's okay for the Amish kids to die of diseases,
even though that sucks, if there was a religion that tortured children or sexually abused them
or something, I'd be like, nope, your religion gets to die now and all your children are being
taken away. But if nature abuses them because of their religion, then it's thereby magically okay?
Yeah, I don't know, man. All right. Well, it's because of the virtue ethics side of things,
the intuition that most of us have, even if we are not virtue ethicists,
that it matters what kind of person you have to be psychologically in order to do those things.
Yeah. I mean, I can easily imagine a couple who love their daughter walking up a volcano to throw
her in, right? I mean, you can be, I think, a psychologically healthy person just with a bunch
of stupid ideas and do bad things to your children. I mean, I don't know where Aristotle
would draw the line there. I really don't think that cultures that brainwash people into child
sacrifice are simultaneously preserving their psychological health. Maybe not across the board,
but as far as how much they love their kids, I think that if you really thought, I mean,
it's going back to the priest killing babies, right? Well, yeah. So if the priest really loved
babies and he really believes what he is preaching, that seems like a great thing to do, right?
And it would, if anything, mean that he just loved them enough to do it.
Yeah, that he was willing to sacrifice his own eternal afterlife to save so many babies.
And also he'd be really screwed up psychologically.
Yeah. I mean, to take it to a real example of that happening, I was raised Jehovah's Witness and I
don't, I think they may be getting a little easier on the blood thing, but while I was
raised Jehovah's Witness, you couldn't get blood transfusions because that was a sin.
And there were a bunch of kids that died. There were court cases that went to the Supreme Court.
Can we, can we force the kid to get a blood transfusion over their parents' objections
and even over the kids' objections? Because the kid's like, I don't want to sin against God.
Don't give me no blood. So yeah, I mean, that actually does happen. It's the modern equivalent
of throwing someone in a volcano. You're killing a kid for no good reason aside from this like,
spiritual bullshit. Yeah. And I, I think they're psychologically unhealthy for doing that,
but I don't necessarily know if that's a popular opinion. Like for the most part,
people will be like, no, they're just having their religion let them do their thing.
I don't want to turn this into this whole big anti-religion circle.
Most of the court cases where the kid is not old enough to make that decision on their own and
the parents do not get to kill their kids. So we're going to force the blood transfusion on them.
Solid. That was really helping. That was the case, but I didn't want to ask in cases the
other way and I don't get bummed out. So yeah, I mean, that to me makes the most sense, right?
Like we talked about like with the kid getting a vaccine. The kid doesn't know and like the kid,
you know, a five year old is not old enough to like be a Jehovah's Witness, right?
Don't have to be told they're Jehovah's Witness, but not old enough to be one.
I, there was a story when I went to church once about a kid who was, went on, you know,
on trial to project the being forced to get a blood transfusion for religious reasons.
And the judge turned the kid and said, name the first five books of the Bible.
And the kid couldn't do it. And he was like, no, don't believe you. You got to get a blood transfusion.
And since then, like all of us memorized the first five books of the Bible were like,
Genesis, Exodus, Leviticus, yeah, you know, because you never know when a judge might ask you
so they can force a blood transfusion on you. That's hilarious. Yeah.
And I don't even think that's a particularly good test, but no, no, it's not.
But that was what we heard in the, in church.
Good way to make you memorize the first five chapters anyway, or the five first five books.
Yeah, footbooks. Yeah.
I had another thing actually looking forward to transhuman wise that, well,
I don't know if we want to object or if we want to discuss more of like the philosophy of like
why it's desirable. Is there any other objections that like just I'm not a transhumanist because
that are worth entertaining? Like it's unnatural is a popular one. But to me,
I think I've said this before, but my tongue in cheek response to that is like, I wear glasses.
Yeah. I don't even think it's tongue in cheek. That's a solid response.
Yeah. But it, but it, I guess you're right. I like it, but it does seem to violate like,
well, no, that's fine. Cause that's just, you know, that's outside the body or something.
And it changes when you get it inside or it's like they can draw new lines for natural. But
yeah, like if I was going by what's natural, I couldn't have driven here today. So, well,
then again, none of us could have driven here today because we'd all be walking.
So I mean, cars, cars are transhuman in the sense that you're moving beyond what nature gave you.
Yeah. I mean, I've had my eyes lasered. I've had chunks of the inside of my throat scooped out.
My ex-wife had her appendix removed. There's, there's a lot of people that would have,
would be dead if it wasn't for, you know, medical advances like that.
And there's no reason to stop.
I agree. I just, I'm wondering if there's anything else that they're worth entertaining.
I don't want someone to get all the way through this and be like, you know what,
they didn't, they didn't address this obvious comeback, but I can't think of what that might be.
I would think maybe one of the things people have somewhat of an objection to is the eugenics side
of things. Yes. Since that whole comes with the Nazi baggage.
Yeah. I mean, is there a strong eugenics component other than like the things that we've kind of
talked about, like forcing people to upgrade or not? I mean,
yes, because you can't consent to having your genes tinkered with before you're born, right?
But it is not consenting to being born anyway. I'd rather be born with all the,
you know, all my parts working as good as possible.
I know there's a lot of people objecting to, it is kind of a transhuman ideal that you aren't,
no one has to do this obviously, but that you would want to have the best genetics possible
either by, you know, reverse engineering yourself with CRISPR or by choosing them
for your kids, the better ones to make them taller, smarter, more attractive, whatever.
And some people have problems with that. One of the objections is that it may increase
inequality if certain modifications become available, accessible to only certain people,
only the rich people, I suppose. I knew I was forgetting a good one. Thank you.
I might come back more, my reply to that has always been, that's always been the case for
everything in history. Rich people had cars first, rich people had the ability to fly first,
rich people had cell phones first, rich people always have huge advantages over non-rich people.
That's the whole point of being rich. So that'll, that'll always be true. And
eventually it gets cheaper and more affordable and more people are able to partake in it.
Cell phones are a good example of that. They started out as like a toy for rich people and now
what, everybody but the bottom billion people on earth, maybe bottom two billion have access to
cell phones. I remember seeing something that like, even like Somali pirates coordinating,
you know, in the middle of the third world are coordinating by cell phone, right? So I mean,
it's, there's a lot of people who never had landlines because they couldn't afford the
infrastructure, but they have cell phones. And that's what 25 ish years before it went from like,
you know, handful of people have them in New York before like everybody had one.
Yeah. I still remember when it was still a sign of being a douchebag if you own a cell phone,
because it meant you were rich and you were like, look at all this money I have on this fancy phone
I can use in my car, like wait till you get to a payphone motherfucker.
It is theoretically possible, although I think unlikely, that massive advances in technology
would grant so much power to an elite that they would actually be able to just completely wipe
out or enslave a lower class to the point where that like, none of the good stuff would filter
down to that. True. Did we ever talk about the, we did talk about the drone thing, right?
I don't know, but I would just want to refine that point with what I was thinking that
this is a different class of upgrade, you know, being able to talk to people from not your house
is only kind of cool and useful, but being able to be twice as smart as your neighbors,
especially your poor neighbors, if you want to keep them that way and keep yourself in the top,
that could be dangerous and scary. I mean, the comeback to that might be there's no stopping it.
Right. I mean, but that doesn't make it desirable.
Even if we stop it here, they're going to do it in China.
That's, I mean, that's the other thing too. And I think that was the thing,
even with like a genetic modification of embryos, right? That they're like, fuck, it will do it.
Yeah. All the, all the bioethicists in the West said, nope, can't do it.
A lot of our government started passing laws and 10 years later, China was like,
Hey, here's our first embryonically or genetically modified embryo.
So in 30 years, they're going to have this race of super humans.
And we're all, I mean, depending on how it turns out.
Yeah. But if it goes only to be 60 years, if there's still the only ones doing it,
they're going to have some time to tinker out some of the bugs and they're going to be
just crushing the Olympics and, and every standardized, you know, cognitive test.
Dude, who was the guy, um, the Chinese basketball player, the really tall dude?
I just saw him on TV like two weeks ago and I can't remember his name.
Oh man. This is what comes from not following sports.
Apparently he was a, uh, Chinese selective breeding experiment.
Really?
Yes. They got all the tallest people in the country and got them together
and paid them to like have children and did that for a while.
And he was the result of that.
That's kind of interesting.
Yeah. Well, one of them, I mean, he's not the only person who was born.
He's the apex of that experiment.
Right. Sorry. Well, so I interrupted earlier.
Someone was saying something.
So what, one of my actually legitimate worries about
many different possible modifications to humans is that it'll just be screwed up.
Like you want to do X, but you'll end up doing Y and it'll be terrible.
And, and it maybe it'll be too late to really pull back from that, that change that you introduced.
I kind of slightly fear.
Did you ever hear our age of M episode with Robin Hansen?
I kind of slightly fear that future where we do figure out how to emulate people in software.
And then, uh, God, there was this great book called, uh, the quantum thief, I believe,
where, uh, there's various factions throughout the solar system that have taken control,
but one of them are just these eight people, which is, uh, have taken over.
I don't remember if it was a gas chance or the inner planets,
but they're basically a descendants of earth and they're eight guys because,
or eight people anyway.
They weren't all men who just repeated themselves at infinitum.
There's like entire multi-billion clusters of them all working together.
And all the other humans have been out competed and died off because
they were not the most efficient people.
And, and I'm worried about that sort of Hansonian thing happening where
everyone who is not the most efficient people just sort of goes away.
I'm not, I'm not like worried about them wiping us out in a genocidal war or anything,
but just stop breeding cause you don't have enough money for kids and such.
And only those who are super productive in their awesome conglomerations remain.
On the plus side, Robert Hanson's age of Emily lasted a few years.
Yes.
So it's not like we're going to die out through the generations.
Right.
But I just see that the general point still, still lands.
Um, that is a reasonable concern, but I, I wonder if that's the, like the,
the rebuttal to, you know, AI research, like, oh, what if it went badly?
Yeah, but people are going to do it anyway.
So we should try and do it as best we can.
So this could be like that, but I mean, how do you, I mean, I guess testing this,
you'd have to do slowly and carefully.
But I, I mean, then the, then there's obviously a safety penalty.
People are doing it slowly and carefully are going to do it slower.
And then the people who are like, fuck safety,
we're going to just pedal the metal this and see how it turns out.
I think that kind of brings, brings the whole thing back to the idea of a singleton.
A, uh, familiar with the concept of singleton.
Okay.
Where there is just one entity that has complete control over what can and cannot be
done, basically like a sort of tyrannical government that it decides no, no one is allowed to
enslave children or to have more than two copies of themselves or whatever,
because of eventually it'll result in this horrible thing.
Like the only way you can stop, as we were saying, this race to the bottom of let's get AI done
first, because it'll make us win, no matter how unsafe it is,
is to have someone controlling all the governments and all the people in the world
and saying, if we see you doing this, we put a stop to it.
A, a, a unchallengeable sovereign.
Yeah. I mean, I'm not really sure what to say about that.
I mean, is, is there any other way to prevent these sort of scenarios other than having that?
Because as long as you can have one person somewhere that can break the law
and make a super smart AI that accidentally destroys the world, you got a problem.
Right. Well, AI is a special kind of problem because that does propagate quickly and can
do things badly. If all that happens, say if we want to do fun gene modifications on embryos,
and it goes badly and you raise kids with IQs of 250, but they're like super miserable or they
have other crazy side effects, well, then it sucks to be them. And then that's all that happens.
Yeah. But if you make really great kids with IQ of 250,
your culture takes over the world in a few generations.
Good. Well, I mean, maybe. I mean, if they're legitimately happy, then
it's true. Yeah. Yeah, everything goes great. That's kind of.
But it's sort of the argument you cannot have any North Korea's on the planet.
Everything has to be under the control of one governing body that decides whether or not you're
allowed to do these things. Well, I mean, in like the AI example, I think
an approach to that would just be to try and get as many really good people on your team as
possible and work really fast and win the race. You don't necessarily have to go around
shutting down the other people. Well, once you win the race, then you have to prevent other people
from. After you win the race of creating that AI that's going to take off, then that will to
be taken care of. Then we will have our singleton. Yes. Hope we programmed it well.
Another thing I'm really looking forward to with like, or I mean, one thing that I look forward
to positively with regards to transhumanism is like upgraded meat suit. Yes. I don't like
physical bodies kind of suck. Yeah. I mean, well, I guess let me take that back. Having a physical
body can be great, but having the one you're born with often sucks. Yes. Even if it even
there's no real problems like you still like, you know, have to use the bathroom and eat and
sleep and all this boring, you get tired, you get irritable pain in the ass. I have gotten
to the point where my body is starting to break down in certain ways. Did you ever see the incurable
shitty ankle bit by the Louis CK? Yeah. Yeah. So after the last time we moved from like hauling
all the heavy shit up and down the stairs, my need was just hurting for like a week, right?
So I went to the doctor and he's like, here, here's some medicine. I'm like,
I don't want medicine. Can't you just surgicalize it and make it good forever? And he's like,
nope, that is an overreaction. Here is some medicine. So I took the medicine, I went home,
and I Googled what it was. It's the same medicine that give people for arthritis.
I do, I am not old enough to be taking arthritis medicine. This is bullshit.
On the plus side, I want my new meat suit that isn't all crappy and worn out.
In your particular case, since you are young enough to not have, you know, chronic arthritis,
what it could do is reduce the inflammation in your knee until it has time to get itself
better. And then that's how it treats itself. I was given arthritis medicine and not okay with this.
You're almost dead now. Right. I might as well just pack it in.
So Steven, maybe this is a good time to get into the upload versus not upload thing because
you're just talking about how, how crappy it is to have a body, but I thought you were against
uploading. Well, I meant how bad it is to have this body. I would like an upgraded meat suit.
It looks like a term. No, sorry. I agree.
It's not great for us either, Steven.
Yeah. Shelly and maybe Enyaush and I disagree on, or at least I softly disagree. Do you want to
lay out the scenario? Don't me too. No, you go ahead. And correct me if I'm wrong. So there's
one of the, one of the other possible ambitions of transhumanism, which like to say transhumanism
has, you know, X, Y, and Z ambitions is sort of like saying Catholicism has X, Y, or like,
Christianity has whatever X, Y, and Z things. It's not, it's not that faction and like the
factions that are factions don't hate each other with as much vitriol as like the Protestants and
the Catholics, but there are disagreements. Like not everyone wants AGI, not everyone wants IQ
increases or whatever. But one thing that I don't, I'm not necessarily on board with yet is,
well, you're not in favor of a forced emigration either, but I have reluctance to the idea of
like simulated worlds that people move to or otherwise abandon this world to get into.
The way that I think about it is I have like this weird sentimental attachment to reality
and that substituting that for something that's cooler but fake. I don't know if I can articulate
exactly what it is about that, but I don't like that. You'd be okay with vacations to fake world,
right? Just like you currently watch TV and play video games. Yeah. In, in as far as that,
I didn't anticipate them or at least didn't have good reason to think that they would like
modify my preferences to where like real world sucked afterwards, right? Dude, I got that even
with video games sometimes. Yeah, same here. But at the plus side, I know that I can't move there,
so it's not a problem. But I mean, what, what about it? Do you like about the real world? Like,
is it the people? Cause what if all the people left and went to, to virtual world? It's literally
the realness. I don't know if that's a good answer or not. But would you want to just be
wandering around an empty planet by yourself? No, I'll be boring by myself, but that's not,
that's, I mean, that would be a different world than one we're in by, by a bit. But I,
just the idea that like, you know, like discovering the laws of physics here on earth
in, in our, in the universe is rewarding in a sense that we're figuring out how the actual
universe actually works. But if, if we got emigrated to some, you know, digital world,
all I'm figuring out is what the programmer dreamed up, right?
Like Shelly, you would take the blue pill. Well, I would upload, but also just because you upload,
there's, it doesn't mean you cannot interact with the physical world. You can use like
robot hands to, to pick up rocks and cameras. If I could interact with the physical world,
I'd be much more okay with this idea. And I bet you could interact with it even better,
like in more fidelity and, you know, I see, I, I assumed that your position was that a virtual
world completely cut off from the real world would still be preferable to the real world.
It would still be preferable, but I don't, I see no reason to rule out having both.
No, I mean, if we could have both, that'd be great. I would, I would not mind like having my
own little virtual world and still being able to like download into a robot body or something
and run around here. But if, if, if I was,
Imagine you could just run out, run up to a real volcano, like a reality just in a different
interface. Yeah. No, I think that that's, that's a very different scenario than I was imagining.
I was imagining the one from a story willing to called friendship is optimal.
Which is, I think we might have mentioned before, somebody writes this, like my little pony AI that
it's like, what was the Celestia was the king, was the queen pony. I didn't see my little pony.
I don't know. Celestia is the queen pony. So basically she's going to maximize
human happiness through friendship and ponies and, or optimize humans.
Yeah. So your whole life is then afterwards lived as a pony in pony land.
Right. And that's, that's something I'd object to. Now, if I could,
if I could go live in Skyrim and still, you know, cool, I'm going to go jump in my,
my simulator and I see me or not my simulator, but my, whatever, my robot pilot suit
and jump into a robot that lives here on earth. That'd be cool. And I think that'd be fine.
I mean, I feel like I'm losing a lot less that way.
When you want to interact with reality, you can do so. And I imagine that I would spend
like 70% of my time in virtual world.
Yeah. In the, in the, I, when we talked about this earlier, I thought you,
I thought you were, uh, saying that it would be better to be in a virtual world,
completely cut off from the real world than in the real world.
Well, I mean, by definition, it can't be completely cut off from the real world because
it, it exists like the computer. Yeah.
But your existence could be cut off from it in every meaningful way,
like other than like the electricity you're running on.
It could. And I imagine that there probably be some people who would prefer it that way.
And I think that's totally fine.
Yeah. So I want to clarify that in the story that I was modeling our previous discussion on this
on where we discreet more strongly in the My Little Pony fanfic, you can't, uh,
and that's like using the word emigrate because that's what they called it,
where you basically, through, uh, destructive means,
chopped your brain up and put, and then, you know, put your conscious,
your whatever yourself into this, this fake world and there's no going back.
So that, that would be different than.
I don't remember in that story, do people do the ponies who have, you know,
ended up in Ponyland get to, to send messages back to the humans or?
I think so. Yeah.
I think that you can talk to people who went there because like the other,
I'd spend a few years, but the other thing that they had was like video game pads that
like kind of like a big Nintendo switch where you're playing this game and it was like an
MMORPG, but the people there were some of the more real.
I think what you were saying that, that some people would choose to do that,
I would purely out of respect for bodily autonomy, think that they could do that.
It should be possible, but, uh, I would feel like it is tragic.
It would be kind of like suicide.
I don't think it's particularly tragic.
It's not that much different from someone who lives in Manhattan and never leaves Manhattan.
I would disagree.
I mean, part of it would just be knowing that nothing really is real.
Ha, Enya likes reality too.
I am a fan of reality.
Well, one of the things you see in all sorts of movies that are like about living in a virtual
world, like whether it be the matrix or the 13th floor or whatever, is that as soon as people
find out that they're in a virtual world, they want to get out.
They're like, I would like to be in the top level of reality, please.
Thank you.
Well, it's because there's so much that the information they don't have access to,
they don't know like, the whole thing is a lie, right?
They didn't know that they are in a simulation.
Who knows who's controlling it?
Who built it for what purpose?
You know, that's actually a really good point in the matrix if, because like how much the
real world sucked, if they had the capacity to build a matrix themselves and say, you know
what, let's build the simulation of Earth 50 years ago where it didn't suck and we can all
live there, they probably would have done that.
That's literally what they did, dude.
Remember, they first tried to make a paradise, but no one would buy it.
No, that's what the robots did to the humans.
What I'm saying is that the humans' robots aside could have decided this was good for
themselves and did it to themselves on purpose.
And they wouldn't have had to be lying to their whole society.
And they could have just like, you know what, we all know that our bodies are rotting back
in the shitty world, but we're going to be here where there's like food and, you know,
shelter and stuff and no killer robots.
So I think that that would be one thing.
If you're in there, I mean, yeah, if you're in there and you know it's not the real world,
part of that might ruin your immersion, but I think taking that away would be a different
level of, I don't know how to tackle that problem, right?
So like raising kids in the matrix, not telling them, by the way, this world's fake.
That'd be interesting.
Oh, that would be interesting.
And maybe like on your 18th birthday, parents like, by the way.
That's just mean.
Well, I mean, you couldn't tell them when they were a baby.
Why not?
Because they wouldn't understand English yet.
I mean, but okay, but you could grow up with this, like learning as much as you're capable
of learning.
Okay, I see.
Kind of like as a small child, I probably understood that like,
my hometown wasn't the entirety of the world.
And like I knew other places existed.
And I'm sure like, you could explain to a five year old in the matrix in this fake one that,
or they're all fake, whatever, in this hypothetical one that wasn't based on, that's not the movie.
That, you know, yes, this is where we all live.
The real world is a little different, but it's not as cool.
That's why we all live here.
And then as they get older, you teach them more about it.
So I guess we're all agreed that it would be cool to have virtual worlds that we
can go to as long as we weren't confined to them.
And we could pop back into the real reality.
Especially if I could plug into it, like, I mean, yeah, absolutely.
I think that'd be fun.
Like, again, playing Skyrim, except not playing and shooting fireballs with my own hands.
That sounds fucking awesome.
But I also think that there may not be a whole lot important going on in the physical world
that, you know, like, mainly, hopefully, this whole thing is being run by Benevolent AI.
And it's taken care of, like, making sure comments don't hit the mainframe.
So that would make me really sad.
I don't feel like I'm in a Disneyland.
Also, is there anything valuable happening in the simulation that's not happening in the real world?
You said, like, you didn't feel like there was anything, what, important or valuable?
Like, if all the people left and went to the simulation, like...
The reason that would bother me is because...
Can you please stop saying it?
Okay, Sarah, go ahead.
No, I just feel like any learning, for instance, you want to learn about zoology.
You can download the information into your brain.
And you don't have to, like, make sure there's a real bunny for you to go look at.
But real bunnies are so much cuter than just, like, clicking a button and learning all about bunnies.
I say that having observed a bunny, like, two hours ago.
And they're really cute.
And you can enjoy a simulated bunny just as much, if not more,
as a bunny in the physical world that has a bit of a weak leg and maybe...
No, I mean, that's slightly hungry that day.
No, you're making a good point that, like, the real world sucks.
And that there wouldn't be sad bunnies in the simulation where there are sad bunnies now,
somewhere a bunny is getting ripped apart by a hawk.
And so I see what you're saying.
What is it that you want to see in the trees?
Do you care about the trees?
I think that's more of an argument for why nature should be bulldozed,
which we should maybe get into at some other point.
And I disagree, by the way.
But the thing that would really make me sad about all humanity retreating into servers
like that and having the AIs, like, watch over them and make sure that
comments weren't hitting it or anything, would be that everything...
We would have checked out of reality.
Everything of importance is now belongs to the AIs.
They interact with matter and the base level universe, and we don't anymore.
So they are the only agents left.
To, I think, counter that a little bit.
I see where I sympathize with where you're going,
but I feel like right now, or in that world, it would be undesirable
because everything belongs to the AIs.
Right now it belongs to no one.
Well, right now we are the most agenty things in the universe that we know about.
Well, that sounds like just an argument against building smarter agents.
It can be, yeah.
But you're in favor of building smarter agents.
I am, more or less, but I don't know if I am if they...
If they usurp us completely.
Right, so Matt is a big fan of the culture series by Ian Banks,
and some of the books in it are really good.
And it's been described by a lot of people as a utopia.
People cannot die unless they want to, except for some very fringe cases.
And the AIs take care of the humans and make sure that we have everything we need.
We can live as long as we want.
We can do basically anything we want with our lives.
And I saw someone on Reddit describe it as a hellish dystopia.
And I was like, please explain.
And actually, it wasn't me that had to type that
because someone had beat me to it.
And the person replied,
because humans don't do anything meaningful in the universe anymore.
The AIs are the only decision makers that matter in the universe,
and the humans are just kind of fapping about,
and what they do with their lives doesn't matter.
It doesn't matter now.
I think that the things that matter the most are interpersonal interactions.
And there's no reason for the AIs to come in and do those.
Those, the things that really matter will still be done by us, which is friendship.
That raises two things.
One, that I do agree that an earth without humans and just me would be boring.
I mean, it wouldn't be, it'd be better than nothing,
but it wouldn't be desirable.
The other thing is that what if the AI was making some NPCs
that were like more engaging in every way to your psychologically than other humans?
Oh, God, that'd be a nightmare.
Right. That sounds like it would suck.
I think that the same general principle that tells the AI don't wirehead me
would also tell them, don't create fake people to interact with me.
Because that is basically a form of wireheading.
You know, in that the interaction doesn't mean, doesn't mean anything.
But you think it does.
No, I'm kidding.
But yeah.
Going off the argument that you cannot create a P-Zombie,
that anything that looks like a P-Zombie is actually conscious,
that would be impossible to do, right?
The actual argument though, the actual definition of a P-Zombie
is that it is molecularly or on an atomic level, identical to me.
But the counter argument is that is literally impossible
unless you believe in souls.
That anything molecularly identical to you will have self-awareness.
That is in fact impossible.
But creating an NPC doesn't have to be on the atomic level identical to me.
And maybe that is possible, even though the P-Zombies are impossible.
A non-sentient, believable zombie, perhaps, maybe.
I don't know.
If it was really non-sentient that at some level it would fail
and you would be able to tell that this isn't really an interesting person.
That anything interesting enough to be indistinguishable to a human
would have a level of consciousness and qualia that approximates a human.
And they would be a person.
I think even if that was true, we could just say,
all right, cool, so the NPCs are conscious.
But then it would still be a drag to have the quote fake people,
the generated ones, you're not fellow quote humans or post humans or whatever,
to have one of these generated people be more engaging than your loved ones, right?
Not when you can modify yourself to gradually over time to be also very engaging.
I already have a lot of history with my loved ones.
Even if you're more boring than this person, I still love you a lot.
Let's get more interesting together.
I haven't had this conversation before, so I'm just thinking of this on the fly.
So this is what it's like.
Well, I mean, me too.
I haven't rehearsed my position.
But it's fun, right?
Yeah, but I guess if it seems like I'm scrambling because that's sort of what I'm doing.
No, I appreciate all the things you're coming up with.
Well, I don't know if it was really that more compelling to hang out with these NPCs
than it was with your friends, then I feel like something might be lost there in the sense that like...
But they're still conscious persons.
It's not that much different from genetically engineering a baby that's going to be super
charismatic.
It seems like you have a problem with someone who wasn't born into a meat suit.
Like if they were born in silicon fully formed, then they're not real people.
Not necessarily.
I guess I'm thinking of ones that were developed specifically by the AI to be super compelling
to you personally or something.
Oh, like it makes you your own perfect mate kind of thing?
Right.
Now, I am also on the fence about this one, but leaning negative towards like tailoring
new mines for the sake of existing mines.
Yeah, it seems kind of wireheadish as well.
Like a large part of the fun of knowing other people is that there are things that are different
and you can explore new things and be challenged.
I guess I just don't really like people's purpose in life to be so focused on a particular
other person.
I think this can happen actually in reality already, but I don't like it.
Agreed.
I think that it would just be easier to do in a simulation, which isn't a general argument
against simulations.
It's just like one of the things that I'm wary about.
My main thing though, and I still don't know if I feel better about is like losing touch
with like physical reality.
And granted, I'm not experiencing physical reality directly right now.
I'm experiencing it as directly basically, well, so far as possible.
There might be cooler ways to get in touch with it.
Speaking of cooler ways to get in touch with it, brief digression.
Our last episode is about recreational drug use and I was so compelled by some of the
points made that I took LSD the next day and it was a different way of interacting with reality.
I don't know if it was actually interacting with reality per se because the ground doesn't
usually move like that, but that was interesting.
Your mileage may vary, use caution, etc.
But would you recommend the experience?
If they had the experience that I had.
I can't guarantee that you'd have that experience.
I actually still haven't even tried the medafinal that I got because I'm like,
I don't know, I'm feeling okay today.
And maybe someday I'll get around to it.
I mean, it'll make you feel better than okay.
I would try it sometime just on a, you know, whatever day.
I should try it at some point, I guess.
Especially if you have anything like due to do that day.
If you're like, okay, I got to crack this out today.
Like, all right, fuck it.
Give it a shot.
I mean, it is not a reality altering drug.
The thing is, I don't have a lot of hard, I don't have difficulty concentrating on
things I want to do anyway.
On Monday, I spent, I think, 13 hours just working on this little project that I'm really into right now.
Got to envy you.
I can't even do things I like doing for that long.
And I was like, oh God, I got to go get food.
All right, fine.
And no, if I get really into something, I don't, it just, it takes my attention.
Me as, like my goals as a, as like a transhuman, like alteration is closer to you as a person.
I can't even find the motivation to do stuff I like, I like doing.
Well, the motivation to start it is hard.
I didn't even get started until like noon because I was just like,
I could be doing other things.
I could be on Facebook.
I don't want to do this.
But like once I dove in, it was 1 a.m.
And I was like, man, I should get to bed.
I need a stick or a carrot, but usually the carrot is like, you know, lack of a stick.
So I need some motivation to do stuff, but like some external motivation.
I can't just like drive myself to do things.
But maybe I haven't found something that I like doing that much.
I don't know.
I mean, I'm sure there's some things you like doing.
I'm sure.
No one's ever had to force you to drink alcohol or have sex, right?
Fair point.
I consider responding with a joke, but none of those would have been tasteful.
So yeah, I mean, but I guess what I'm getting at is like,
I wanted to sort of just dial this back from like, you know, wild discussions
of like the like idiosyncrasies of like simulated worlds back to like
the basic transhuman thesis is that like, it's less about like, you know,
how you'll interact with simulated people and simulated worlds and more about like,
how can your life be a little better in ways that you can even currently imagine,
right?
Like my eyesight could be better, even though our glasses, my sense of touch could
be more acute and I could be, you know, have finer motor control and be
better at whatever mechanical stuff.
Like all these things and, you know, not to even extent that is superhuman,
although there's no reason to stop at regular human.
But I mean, you can look at other people who can do things that you're like,
you know what, that'd be kind of cool if I was, you know, as physically fit as
that person or something or whatever it is.
So I think that that's really the takeaway, like that it's more just about
just being okay with like a general growth mindset of like, you can be a better person
and there's no reason to stop where you're at.
And then of course, like the next step, I think that might, that might just be humanism.
And then the transhumanism part is like, there's no reason to stop it.
Like, well, let's not stop at peak current human, right?
You can do better than that.
But I, getting back to your culture example, well, I'm not super familiar.
Haven't read that many of the books.
But the, your argument about how you don't want the AI to take over a lot of the like
survival tasks for humans or the advancement in power over the world.
It's not just the survival part.
It's the fact that anything of importance is done by the AI now,
aside from friendship with other humans, I guess.
I mean, to me, that's the number one thing that makes anything important.
I'm sort of curious what the other important things are.
Like figuring things out.
Like to me, like scientific progress is important.
And I hate to have that all taken away because it's fun for us.
Like that, I mean, so as long as it's not taking away fun and like interpersonal friendships.
It includes scientific progress, but it includes anything humans can do.
AIs can do it better.
You want to write a song?
I can write a better song.
I mean, sure, humor yourself as a human do writing your human songs.
But there's, there's, there are other races in the galaxy or galactic cluster.
Maybe I don't know that, that are interacted with and generally the AIs handle that every now and
then humans like get special assignments and stuff, but it's always like the AI say,
okay, here's an assignment that a human can handle.
We will humor them and let them do their thing.
But when the shit is really serious, AI step in and like, nope, we're fixing this now.
Go about your way, human, because this is not playtime anymore.
Part of me, all right, having not read this either caveat.
I've actually not read very many in it, but the few that I've read, that is the distinct
impression.
So I think as long as, all right, so your first thing about, you know, the AIs will,
they can do science better than we can.
And they'll just, there's like letting us do it because it's fun for us.
That's sort of like completely fine with me.
Like that's exactly what you're doing with a kid, you know, who wants to see, you know,
they haven't heard of Galileo, but they want to see which one falls faster.
I mean, you could just grab their arm and stop them from doing the experiment and just tell
them the result and show them the math and say, whatever, or you can let them do it and have
fun, even if they're not like doing it in a refined way, they're still having fun.
They're still learning things.
I find that completely fine to, you know, just because, like, so I guess I'm going to
guess that's our current state now with adults and children and like me and real scientists.
But I wouldn't want to be trapped into childhoodness forever.
No, me either.
But I mean, well, if the cost of, or I guess the benefit of that was, like you said,
when things got serious, the AI was like, no, I'm doing this, I'm kind of super fine with the AI
having like a safety net over the universe or our universe than, you know, me doing it or,
you know, my peers doing it, because we don't know what we're doing.
Right.
Well, I mean, but that's, that's literally the infantilization of the human race.
We can have fun and do our things, but when anything important happens, the parents step in.
We're sort of infantile, though.
Well, I think that you may be able to make a choice in the scenario to say, well, I really
want to be part of the super intelligent jobs that are out there to do.
And then you basically have to become part of the AI, right?
Right.
You have to modify yourself to such a degree, right?
And, and there's, that's inherent in the task.
Like if you want to do it the best, you have to be the best so you can't be what you are.
Yeah.
That that's not something you can get away from.
That is one of my kind of minor hangups about creating AI that we might make ourselves irrelevant.
Well, but I mean, it's inherent in any, any power to be that great, right?
Right.
I mean, it's, it's, it's kind of the reason behind if there was a God, we would have to kill him,
right?
Because you don't want that sort of adult making all your decisions for you.
I was actually just going to bring up that point, but take the other side with it.
A, I wouldn't say all the decisions.
That's a false dichotomy.
It could just be the really important ones that we, that it believes we would make the wrong
decisions on, right?
Right.
All the decisions that matter.
Well, not even.
I mean, like how I treat the people I love matters.
All the ones that can end in disaster.
Yeah, exactly.
All the ones that like literally will end in the world.
So like, it'd be really cool if, you know, all right, there's an asteroid coming and we can,
as a, as humanity scramble and see if we can take care of it and solve it.
But if we can't, we're all dead or, you know, God could just step in and move it.
And so it doesn't clutter there.
But then it doesn't matter.
Why not just not even bother trying to stop it?
Because, you know, God will take care of it.
Well, all your efforts are just an exorcism.
It's your size in play.
It literally doesn't matter if you do anything or not.
All your efforts to save the world.
Like I would rather, I would rather have that choice taken out of our hands
than have the overwhelming likelihood be that we would fail to do it.
That's right.
I would.
I mean, I guess because I think for me, like my terminal bad value is the end of the human
species or the end of the end of sentient life in the universe, right?
I think it's actually quite immoral to say I want there to be the chance of horrible
things happening because I need to be a hero.
Yeah, dude.
What the hell?
I got my white savior thing going on again.
My bad.
Actually, in the, I think it was in the fun theory sequence that
Eleazar Yakevsky wrote this part of an article, this sentence.
Like he talked about how he is a hero.
He really wants to be important.
But he also says, you know, should we then have put a baby at risk and the mother at
risk of the baby dying or whatever it was?
I forgot the scenario.
Involved a mother, I think.
Should we have people put at risk so that we can save them?
No, no, we should not.
And I feel that, yes, you probably are really on the wrong side here.
Really?
You're going to get a lot of angry listening feedback this episode.
You're really attracted to the hero scenario, to the point where you're tempted to put
people in danger so you could save them.
I am not going to admit to that on the microphone.
I didn't necessarily take your position as like, I want to be the hero, not even you
personally, but just like the idea that.
Humans should be saving themselves.
Yeah, or that there's no safety that makes us the ultimate arbiters of our well-being.
That is, I can see the appeal on some level.
I think there's a loss of dignity.
Yeah.
I mean, there's also a loss of dignity in everybody dying, right?
Right.
I mean, yeah, I guess then it comes down to what's more important to loss of dignity
or everyone dying.
That's sort of.
I think maybe this is my version of your bodily autonomy thing.
That some things are more important than good consequences.
We were talking earlier about that logical.
Yeah, not non-consequentialism.
Some categorical imperatives that are important for their own sake,
even if they have bad consequences, shit.
I understand you now.
This is awesome and awful.
I hate understanding other humans.
So I'm going to take this even to a more controversial level
and hopefully we'll get lots of hate mail and then you'll be like,
sorry, Shelly's never come back on the podcast.
Now I'm really excited.
Not because you'll never be back,
because I'm really excited to see what you get to possibly be that inflammatory.
Okay, so it's along the lines of don't make people need you so that you can be the hero
and be needed by them and so you can swoop in, right?
Yeah, it seems like a bad thing.
So don't make children because the definition of a child is someone who needs to be
parented.
I was going to say the whole teach a man to fish thing.
And then it makes me think of that Ron Swanson quote,
give a man a fish and defeat him for a day.
Teach a man to fish and he'll feed himself.
Fishing is not that hard.
He's an adult.
So yeah, okay, so you're bringing this to-
So imagine that you can create new people in the matrix, the uploaded world.
Are you going to create them with adult minds?
Or are you going to create them as people who need to go through a childhood and become adult?
Not all of childhood was terrible for me.
I mean, but the parts that make it, definitely a childy, right?
The parts that make it a childhood and not just existence?
There was a fantastic novel called The Golden Age, I think, by John C Wright,
who later went crazy.
But at the time, he wasn't, he was crazy at the time too,
but he was crazy in a different way.
Anyways, in this novel, they live basically in a sort of uploaded state
and they can interact with reality too,
but they can create their own worlds.
And there is a character who creates her own world where everyone is a child who is
psychologically dependent on her and will never grow up.
And it's kind of horrific.
It's like the children that locked their,
or the belts that locked their children in the basement, right?
Psychologically retarding millions of people
because she likes being this mother character.
And it's considered a huge crime.
They kick her out of society and everything,
but that reminds me of what you're saying.
I'm not okay with that forever.
I'm not okay with that for 18 years.
I'm probably not okay with that for a month.
Not okay with that for a month.
Yeah, I would say if there are certain processes that have to be gone through
in real time in order to actually form a mind,
then go ahead and have those lived through.
But otherwise, let's pop you up to adulthood.
That's sort of what I was going to say,
that as long as it's currently necessary,
that I'm totally on board with it,
kind of like if I needed surgery to save my life,
but it was going to make me bedridden for a month,
I would be totally fine being dependent on somebody.
Obviously, I wouldn't want them to hurt me,
so I had to stay in bed for a month so they could take care of me.
That sounds like a different thing.
I do see what you're saying.
There's no particular reason it has to take 18 years to get to being an adult.
So why draw that out for 18 years?
And at that point, how long is it immoral to force people to be children?
Let's maybe apply this to the current reality,
where of course you don't have the choice to create people as adults in this reality.
But there could also be some argument that maybe you in particular,
I don't know if you're a good example in the ash,
because you're not sure if you want to have kids,
but just pick a certain person.
Do you in particular really need to create a new human,
in addition to all the other ones that are being created right now?
Yeah, but I mean,
you can make that argument for literally every person on the planet.
Right.
I get the feeling she's going to.
So where I'm going with this though,
is like what the person really wants in most cases,
I suspect, is not that they want to add another person to the population.
They want specifically a child.
Am I wrong?
I want to do one thing.
First, I wanted to say that as far as in the simulated world,
I do see no point in raising people through painful infancy and all that nonsense.
I mean, if you could spring somebody,
then again, all right, I just want to touch on that.
I don't want to belabor that point,
but it just occurred to me.
I was going to say if you could spring them in fully sentient adults,
but then where would all their background come from?
Like their sentience, all their memory, their history,
a lot of stuff that makes you a sentient person is like,
having familiarity with having had thoughts before.
So like.
That's a book called Diaspora by Greg Egon.
You've read like absolutely everything, right?
You've got a book for every point.
That's awesome.
No, that's great though.
But how much of that do you need in order to?
Yes, it was a pretty short process.
Yes, maybe the minimum.
And you could load them with like a basic template or something that,
you know, whatever.
And maybe you could have some guidelines as far as how to tweak it.
If you wanted it to be like your new friend or your new,
your offspring in some way or something.
All right.
So that said, in the current world of people wanting to have kids
for the sake of having kids,
like because they want to be parents, they want to have children.
Yeah, I feel like they don't necessarily want to just have
another person in the world.
And maybe they want like after the kid grows up,
the kid will take care of me or something.
I don't like that idea either.
I totally agree that they aren't doing it for altruistic reasons
to perpetuate the human race.
They're doing it for selfish reasons.
But I think one of the key reasons.
Someone's got to have kids.
And I mean, that's as good a reason as any other.
Well, but if, if you were going for the best reasons,
then the pattern of who has kids and who raises them and all that
would be very different.
But just, just from like maybe a psychological perspective,
I do suspect that I want to have a close intimate loving relationship
with someone who is weaker than me is a big draw.
Perhaps the primary draw for a lot of people.
I don't know.
I don't know if that's the case.
I think you're also saying that would also apply to pet owners though.
Yeah, that is incredibly, yeah.
Well, all right.
One, defending pet ownership.
I'll say that another reason to be a pet owner is to,
like I wouldn't ask someone to go make me a new animal
so that I can take care of it.
I'll say, well, this one's alive.
And it's, it's, it's currently living in a cage.
It can live in a bigger cage.
Although a lot of people do breed pets for,
for the sake of being pets.
Yeah.
And that's interesting.
I don't know.
I'll table that for a second and touch on the parenting thing.
So I don't know, I don't know if that's an,
I doubt that's an expressed preference of many parents,
but I don't even know if that's a background preference.
I think a lot of it, part of it is like,
we as humans, many of us either are like,
have many of us want to have kids because we're programmed to.
And to the extent that many of us,
we're all, many of us, almost all of us are programmed
to want to have kids, but many of us choose not to
because we've made the decision to overcome that programming.
I think that.
I guess, okay.
If a lot of people just don't think about it and just do it.
So it would be stretching it to say that that's their reason.
They just had kids because that's what you do.
And that's probably most people.
I think that's how I came to exist.
That my parents weren't like, you know, well, let's see,
we've got a plan for retirement.
Well, if we have kids, they can,
they can subsidize our, our, our life in our last,
you know, decades.
I don't think that there was ever a conversation like that.
I don't think there's ever a conversation of like,
it'd be really cool to have this infant human dependent on me.
I don't think there's ever,
I think it was just like, this is what adults do,
but that's not a good reason.
Do you think about it?
Yeah.
Uh, I think a large portion of people have a strong,
perhaps hormonal desire attraction.
You could call it baby hunger,
or maybe for some people it's toddler hunger, right?
I love that you use the term hunger.
Yeah.
Makes it seem particularly gross.
Yeah.
And, and yeah, it is a particular like desire for closeness
with this particular category of human,
which I think a lot of people have.
And I think it's because like the essence of the,
the child child likeness, which attracts them,
are the things that make children, you know,
in need of being rescued by a hero,
which we would call, you know, a parent.
Part of that might come from our,
like we might find the idea of rearing that appealing,
because it's part of being a parent,
and we're biologically programmed to want to be parents.
But, um, A, I can't remember,
Katrina was on the fence about wanting to have kids,
and she was leaning towards definitely wanting to have one.
And I can't remember if she expressed an opinion on whether
or not she loved the idea of rearing a child.
Although I get the feeling that she definitely said that
I think she thought she'd enjoy being a parent,
but I would love to get her take on that particular point.
Well, I did try and, uh, question her about this.
We didn't really get too, get too much into it.
We had, I had to leave.
But what she said, and what I've heard a lot of other people say,
is that they have an intuition about the quality of the emotions
and the love that would be there with specifically a child.
Right?
And I only have heard that sort of thing.
And I only named names because she's the only,
like, rational person that I know who wants to have kids.
I was trying to think of, what would,
what would a rational Steven who wanted to have kids think?
And I'm like, I don't know.
I'll have to ask Katrina.
So, yeah, part of it is like,
I think you'd find it extremely rewarding
because you're, you'd be rewired and you'd love,
you know, having a baby or something.
But that's actually interesting.
I don't know.
I, I have heard it is a, like, like you said,
a very unique, just based biologically rewiring sort of love thing.
I guess I, I try to get people to imagine scenario
where their child was enhanced to the point
where it really didn't qualify as a child anymore.
It didn't need parenting.
And like, would they still have that emotional tug
towards that situation with those enhancements?
If, if like, if they went up to adulthood
in the course of one month, I really doubt it.
I think a large part of it is the sacrifice
of putting so much of your life into this other child.
It's kind of like people bond going through horrific things
in bootcamp.
A lot of it is about just putting people through horrible shit
so they bond with other people.
A lot of it is just investing the time and the energy
and all this sacrifice.
And that's what creates the emotional bond.
And if someone just grew up in the course of one month,
you don't sacrifice all that much.
You don't have all this time to build the emotional bond.
I think that that's probably true.
I don't know,
Shelley, just to make sure I understand,
I don't want to misphrase you.
You're, you're claiming, you're, you're putting forward the idea
that you think that might be subconsciously
a majority motivating factor for parents.
For those who think about it,
and yeah, have emotional reason for doing it
as opposed to like, I'll do it because I'm told to
or I'll do it because I never thought about it.
I, that might be true.
I'd like to get some, I'll have to think about that
and we'll have to ask some smart parents.
And you're saying that's a very bad reason to have kids.
I think that's a weird reason to have kids, for sure.
I think it's a cringey reason.
I wanted to put forward another reason
I think a lot of people have kids after self-reflection
is that I can make a better version of me.
And that's not so much that like,
I want the idea of like having it be dependent on me,
but like, if I could redo my own childhood
and give myself all these awesome benefits,
well, since I can't do that, I can do the next best thing
and raise a kid and give myself,
give my offspring all the benefits I would have given myself.
If I were to have kids,
one of the kids I would want to have would be a
straight up clone of me, just to be like,
see if I can do a better job than my parents.
Wow.
What?
I think it'd be an interesting challenge.
I think that's part of the point though,
because you want to make something better than yourself
and the only thing you have to go on is like,
well, I know I had this,
and so I can do the parts that I liked
and not the parts that I didn't like
and add some stuff that I think I would have liked.
Yeah.
Right?
Maybe, I don't know.
That was cringing all over the place right now.
But I'm not sure why.
I'm kind of cringing at this.
Well, again, it's the mind being tailored
for the purposes of another mind, right?
Well, in the event that we were just to generate them
fully sentient and then emerged,
we'd be, unless you're saying we'd have to do it
only with a shuffle button,
like we'd be doing it anyway.
Well, you could like tailor them to fit into the society
rather than to be like the ideal friend or partner
or son or daughter of a particular person.
So a constrained shuffle button
would be the only ethical way to do that.
I'm not, I'm not meaning that to be funny.
No, I'm just saying like,
there wouldn't have to be any shuffle button,
but it could just be less narrow in the targeting.
I mean, what, so I guess that,
how narrowly do you want to define like society though?
Like what if you were wanted to be a great scientist
or something or to be a great musician or whatever?
Like, is that too narrow?
Oh, I guess I was just imagining like an AI
is taking care of one trillion people
and it was like, okay, we're going to create one more person
to add to this one trillion.
And what would be that optimal addition
based on everything in the whole entire society?
So the AI would be building another optimal person?
That'd be kind of cool.
Yeah.
Right? Or would you have a problem with that?
I mean, that's maybe my least bad scenario
of creating a person.
Yeah.
Is there any good scenario
for creating a person in your opinion?
That's like the scenario
that I would consider as being maybe okay.
Is there any good scenario?
Like, because you said it's your least bad
and to me that sounds like the optimum way
you could possibly create someone.
Yeah.
So you're just, you're just in general
against creating people.
I tend to be, I lean towards that side.
I'm not going to be like hardcore about it, but.
Okay.
Yeah.
I want to keep talking about this.
Can we do this some other time though?
Because it is, we're almost at two hours
and I was supposed to meet someone 15 minutes ago.
Yeah.
That went by fast.
Yeah, it did.
Okay, cool.
All right.
Well, we got to do the signing off and all that stuff.
Oh, oh, yes.
I don't know.
Okay.
Anyways, thanks for joining us.
I'm Emi Ashbrotsky.
I'm Steven Zuber.
I'm Shelley.
Bye.
And really quick, thank you to our Patreon supporters.
Thank you to anyone who writes reviews on
iTunes and to our sound engineer, Kyle.
Yeah, that's fantastic.
Cool.
That's it, beginning two weeks.
Bye.
