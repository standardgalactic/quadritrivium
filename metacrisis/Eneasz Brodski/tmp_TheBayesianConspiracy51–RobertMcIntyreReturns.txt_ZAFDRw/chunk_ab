because you can bear unbearable things. And yeah, I,
oh right, did I say it wrong? Yes, you can, you can bear bearable things, but you can't bear unbearable
things. And so yeah, that's true. But I think the purpose of the thought experiment was more to
not so much like, weigh one ruined life over trillion, trillion, very minutely,
or disturbed lives. It was to get your, get you thinking about, you know, if that is an actual,
if say the dust spec was an actual negative utalon experience a large, among that large
of a group of beings, then how could you possibly weigh that against the maximum
number of negative utalons from one being? I think that they sort of missed the point.
Does that make any sense to you? No, that makes total sense to me. And that
basically is what the argument boils down to. The, the one unbearable thing is unbearable
versus the numbers are higher. And I don't, that's just, there's no, I don't know if there's a way
to resolve that. It's just a philosophical difference that some people do not care about the numbers
and other people do. Yeah, I agree. And it's, I mean, I'm still sort of pretty undecided on it.
Luckily, I'm not standing over a switch to make this decision. So,
well, I mean, it comes more of a question of how do you program your God? And if your
God thinks in numbers, that's an issue, right? Luckily, I'm not programming God either. So,
right. Metastable, who says that I said in one of our earlier episodes that Elon was an early
founder of Mary. But I think you got him confused with Peter Thiel. And yes, that is the case.
I meant Peter Thiel. And I did at that point confuse him with Elon because, you know,
my brain's stupid. And they're both cool Silicon Valley guys that I look up to. So it made a little
switch. I'm sure it's good. It's worth pointing out that there's conversation happening over at
subreddit r slash the Beijing conspiracy. If you like stuff to talk about between episodes here,
looks like on the drugs episode Jenkins expanded his, or I guess added some notes to the
conversation on the episode 49, the spirituality via drugs episode. So if you're interested in
seeing more of that from him, you're welcome to check that out there. Yeah, to provide, you know,
a balancing view to what I had just said earlier, it is good for Jenkins to be able to say his piece
as well. And that is on the subreddit, the spirituality via drugs episode, the first comment.
Oh, nice. I mentioned the baptizing the dead thing that the Mormons do. And
they had somebody, let's see, hanky USA, or honky you so I'm not sure how they say it. But
on that same episode, they, they have a lot to go on here, but they talked about some of the
dead people that are baptized and apparently Carl Siggins on the list. That's kind of funny.
Yeah, we had a number of people right in a number of ex Mormons right in to tell us
about Mormon afterlife. And for the most part, it sounds like basically everyone gets a heaven
or at least a sort of spirit prison where they're not actively tortured or anything. They just sort
of hang out until they get, they get some sort of saving things. The even, even like the worst
people on earth, like criminals and horrible people get a type of afterlife that's not really a
hell, they're just trapped in the company of other horrible people, whereas as opposed to being,
you know, actively tormented by God or anyone else, they just have to hang out with each other,
which kind of suck. And like the normal people just get sort of, you know, a nice place, but it
isn't like heaven, heaven, you know, and then like the really good people get ultimate awesome
bliss heavens and get to become gods of their own universes if they were like really cool.
They, the only real hell they have is this thing called the outer darkness in capital letters.
And that is where rebels against God go. So literally Satan, the other one-third of the
angels that rebelled with Satan and any people who were Mormons, but then left Mormonism
because they are in God's presence, but then they left. So they are, you know, basically
rebels against God and they get to go to the actual hell hell that's awful called the outer
darkness. And do they get to get saved? Because I'm pretty sure Carl Sagan was like an, you know,
an active campaigner against religion. And that sounds like an enemy of God.
I don't think he was a Mormon in life and then the church.
Yeah, no, but you said that it was for rebels of God or ex-Mormons.
Well, rebels against God are literally people who were Satan or those angels or people who were
Mormons and left. Okay, so that's the only three. It's not like, say, if you and I made our life
school to de-convert as many Mormons as possible, only the Mormons who got de-converted would go
to hell, not us. Right, exactly. Because we're actually in the presence of God and then rebelled,
whereas we were never blessed enough to be in the presence of God, so we couldn't rebel against him.
Cool. Which, I mean, I guess it sort of makes sense because it's not too
torturous for everyone else, but it has very strong penalties for being an apostate, which
most religions do. They generally hate people who were that religion and then left far more than
anyone else. So, okay, there we go. Perfect. I'm not seeing a whole lot of other feedback,
but I thought we were behind on some stuff. Anything jumping out at you?
No, that is about it. And since we took some time with host feedback, I think this is a
decent time to style. Works for me.
Hey, folks. This is Kyle, audio editor for the Bayesian conspiracy podcast. Due to the aforementioned
chopped-up nature of today's episode, we neglected to record an intro for our guest.
So we now present to you our interview with Robert McIntyre. Please enjoy.
All right. Well, in that case, welcome to the Bayesian Conspiracy. I'm Stephen Zuber.
I'm Minyash Brotsky. And I'm Robert McIntyre. Robert, thanks for coming back on.
It's a real pleasure. I don't know if Minyash mentioned last time something got garbled in
the audio when we were, I guess, between recording and saving the file. And it came through kind of
like, if you're shouting through a woodchipper the whole time. So at the very least, I think it would
be useful to cover some of the same stuff as last time. But before that, I wanted to hear about the
talk that you gave at the Future of Humanity Institute. That's the one. That was their vision
conference, I believe, a couple of weekends ago. And they invited a bunch of people over. The talk
that I was part of was, you know, kind of what the future of mind uploading was going to look like.
And we're going to discuss some interesting stuff. What was your take on what the future is going to
look like? They were trying to compare it to Hanson's book, Age of E.M. And I think one of the
main things there is he thinks that AI is not going to really happen before we get the uploading.
I don't see how you can't get some type of AI, especially if you, you know, have gotten to the
point where you're beginning to really understand how the brain works. It seems to me like if you
can upload a mouse, that gets you certainly some amazing tricks to do with our current neural networks,
which may lead to something that's got some fairly robust intelligence. So that's the main
conceit that he makes in that book. Probably because he's wanted to explore, you know, what would
happen in the absence of AI, like many science fiction books do. And AI makes it very hard to
predict because then you're trying to look past kind of the singularity, right?
And, but I do think that's somewhat unrealistic. So that was, that was the main thing I had.
I agree. I, we talked with Robin about his book Age of E.M. actually. What was, I think just in
general, what we talked about Age of E.M. And that seems to be one of the, I guess,
priors to the book, that if AI happens, it's too slow a takeoff for it to
subvert the Age of E.M. from happening for a couple of years. I think he's been a proponent
of slow takeoff for as far back as I've been following his stuff. Oh, speaking of AI takeoff,
have you guys seen the new, what was it? Not AlphaGo, but AlphaZero? AlphaGo Zero. Yeah,
I read about that on a post by Yudkowsky responding to some letter of criticism.
Okay. Yeah. It took it like, what, four hours to go from doesn't know the rules of the game to
superhuman playing? Well, that's superhuman better than Stockfish, which was our best,
was our best AI for playing chess. So not only did it go past all of the many hundreds of years
of human experience, you know, with masters teaching their students on how to play chess,
but then it also ended up being the best AI we were able to build to play chess as well.
I thought it, I thought it played go. That was for AlphaGo, but for AlphaZero, they used chess
for it. And there was like no outside influences at all on it. It was just given the rules of the
game. And basically that's it. And with just that, it played against itself until it figured out chess
and was able to break the game so that it would never lose. It's pretty good. It only took four
hours. It's insane. And first of all, not only how many hundreds of years of humans working to
perfect the game, but after that, decades of programmers making computer playing chess programs,
which were basically programmed with special knowledge, things like opening moves and gambits,
and what to do when this situation arises, looking forward, X number of moves in advance.
And then all of those decades of work and programming was just blown away by a learning
algorithm in four hours. It is, I think, as Eliezer said, an indication that his idea of
fast AI takeoff is more likely because once they figured out how to do the learning in a
good learning algorithm, it just demolished all previous work. So I'm not an AI programmer,
but isn't it a different sort of problem? Because you can program in the rules of chess,
now go forth and figure out how to kick ass at it. But can you program in the rules of the universe
and say go and figure out how to kick ass at that as easily? Not obviously as easily, but is that
even the same kind of problem? I don't know, but it's pretty impressive that you can blow past,
like we care about chess a lot and we spent hundreds of years trying to get good at it.
And this thing just blew past it from nothing to better than everything in four hours. It's
very impressive, I think. Oh, I totally agree. The main point being that it managed to self-improve
over just four hours, starting from scratch, rather than incrementing further from previous
efforts. The whole slow AI takeoff argument depends on an AI slowly incrementally learning at a rate
where we can see what's going on and maybe react. Whereas this kind of shows that once an AI learns
how to learn in the proper way, it can take off at an exponential rate far faster than we're able
to understand what's happening. And in the process, just cruises right past everything that we had
thought that we'd figured out over centuries and or decades. So I get like these mixed feelings
whenever I think about super AI, like impending. And, you know, I think the same feelings everyone
else gets, but talk to me to be stoked, but it's also super exciting, right? Right, it's terrifying.
Yeah, although chess is still a pretty circumscribed type of game, right? So,
just because it can blow it away that quickly, may or may not have a lot of bearing on what it can
do in general. But still, you know, I think there are a lot of other games that humans play that
are very important economically that, you know, you may very well see an AI go from totally not
competitive in those fields to better than not only anything we could do, but anything we could
work with a computer to do very rapidly. And that itself could be a big deal.
Yeah, well, Charles Strauss has explored this concept before in his books as well. But
a lot of our financial instruments, financial markets can be gamified and see viewed as a game.
And AI bent on learning those could, in theory, come up with the financial instruments that we
cannot even really understand what it's doing and throw the entire economy into complete chaos.
There you go. Yeah, it could be nuts.
And yeah, chess is really a limited arena. But I think it's a proof of concept that if you get
the learning algorithms right, they can really advance a lot faster than a lot of people have
been thought possible. I think you're right. It's, it's both exciting and daunting. We'll
see. We'll see how it goes. Fingers crossed. Hooray. So that's the thing. They're going to be,
when they go to like hit enter on these machines and start things going, they're going to be like
popping champagne and be like, all right, guys, we're all excited, or I think they're going to be
like just getting wasted and thinking hopefully this doesn't kill everybody. Well, I hope if
that's their actual fear, then they wouldn't press the button in the first place. Yeah, you're
probably right. I guess. Yeah, you gotta make that money, right? Yeah, I think the button
will be pressed by other people who are afraid that if they don't press it, other people will
beat them to it. And it can't be that bad, right? Well, that's, that's almost worse than the two
options I laid out. Because your two options are like either we get it right, or, you know,
or we have no idea, not like, not like we better hurry, right? Because, you know, what if the
Chinese get it before we do? And then the Chinese would have this huge advantage and maybe take
over the world. And we don't like that. We want hegemony. Back in the day, we had a little bit
more primitive morality, I think, where, you know, the king is good because he's so powerful, right?
And this concept of that you can be good and not powerful. And like, just because you're
powerful and can kill everyone doesn't necessarily mean that you're right or good or moral. I think
that's a thing we had to figure out over time that wasn't as obvious at the beginning. And I
think a lot of the people, so I've talked to, you know, that are actually working on AI still
have this notion, you know, where they think if the AI is is better at playing some of these games,
and it deserves to be able to take over everything. And there's this kind of fatalism that's part of
that. So anyways, yeah, not a fan of that, that thesis, I guess. Yeah, something that's just straight
up if might, you know, might makes right. If they can kick my ass, it deserves to be the boss of me,
right? Fuck that. I think there's a bit of an is on distinction here, because something that can
kick my ass will be the boss of me. But I don't think that's necessarily a good thing. So maybe
if I can stop it from coming into existence, I would like to do that. Agreed. Okay, well,
before I forget, I wanted to do a kind of quick recap of our last conversation. So talk about,
I think the small animal brain freezing award, I'm sure there's a better name for it,
preservation award. Sorry, I'm not running on all cylinders today, I should have mentioned that
at the beginning of the call. So, but I'm definitely good enough to talk, just not lead.
The small mammal brain preservation prize, and how, you know, this aldehyde stabilized
crowd preservation technique that I made, managed to win that prize. The large mammal prize is still
outstanding. But I've seen some of the early results for them, they're looking fairly good. So
I do hope that that will be one. And then we went into more talking about, you know, brain
preservation, mind uploading, you know, what that future sort of looks like, what you would probably
want in a brain preservation technology. Now we touched on some of our course stuff. And that
was most of the conversation, you know, some of the things I'm trying to do for next home, I guess.
So maybe this is a bit of a personal question, but I'm assuming that you would like have your
brain preserved using the process where it gets fixated first, the what was it called?
The aldehyde stabilized crowd preservation. Yeah, I'm assuming you'd prefer to have your
brain preserved using that method, right? Well, it's an interesting thing. I think that what we need
is a technique that's been validated to preserve the connectome. And in lieu of that validation,
it's really sort of a crapshoot as to what you're going to use to preserve yourself.
The aldehyde stabilized crowd preservation technique, I think would have very serious
problems being applied to someone postmortem, more than a couple of hours. And, you know,
that is the one thing that the current crowd preservation techniques probably have over it,
is that you can still kind of use them even like five hours afterwards. On the other hand,
those have not been demonstrated to preserve the connectome. So, you know, what are the
metrics of success that you want to use to judge your brain preservation protocol,
or to say that, you know, this preservation protocol is better than this other preservation
protocol? What Hayworth did for the brain preservation prize is he said, you know, the first
metric of success ought to be the ability to preserve the connectome. So, for every synapse in
the brain, can you trace that synapse back to an originating neuron, or can you not trace that?
Because what neuroscience tells us is that memories are stored in the connectome. And so,
if you're disrupting the connectome to the point where it's not recognizable or it's not traceable,
then it's likely you're not preserving memory. Now, maybe there's some other redundant mechanism
that's encoding memories, but it does seem like the connectome is a good place to start.
And so, under that metric, there is no human brain preservation technique currently in existence
that has been proven to preserve a connectome. So, why, first of all, doesn't it work
more than a couple hours after death? Well, because fixatives are being delivered through
perfusion. And if you are trying to do this couple hours after death, you're going to have a lot
of problem with blood coagulation. And the process of fixation makes that much worse. And so,
you're going to have an emboli, you know, blockages in the bloodstream that are going to cause a lot
of trouble with perfusion. With the cryo-optectin perfusion, although that's still, you know,
