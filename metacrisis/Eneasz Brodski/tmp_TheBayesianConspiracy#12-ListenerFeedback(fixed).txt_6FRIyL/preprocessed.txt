Welcome to the Bayesian Conspiracy, this is Katrina Stanton, this is Inyash Brotsky, and
this is Stephen Zuber.
This week we are going to be addressing some feedback that we've gotten from people who
listened to our past episodes, all of our 10 past episodes.
So this might take a little while, hold on to your horses, and away we go!
This is going to be our listener feedback episode.
After this we're planning to do a little bit of feedback at the beginning or at the end
of every episode, so we don't have to have big listener feedback episodes every few
months.
Yeah, okay, so let's start with one of the most recent ones and move back from there.
We did two episodes on polyamory, and I'd like to address those quickly.
Eustis on Reddit wrote, Katrina seems to be searching around for a word that encapsulates
how polyamory allows you to explore relationships you may not otherwise have tried if you had
to be fully invested in them.
I might take a little bit of issue with that wording, but that's okay.
The term I use from economics is opportunity cost.
Bringing a new relationship while poly has a lower opportunity cost because your options
aren't near as limited.
And good point.
Thank you so much for that.
Wonderful term.
I take it you take exception to the fully invested part of that?
Yeah, but you know, nitpicking, that's for other times.
Well, we have a second, I think, I mean, just to steel man, I think what they're trying
to say, I think in that you can't give two people 100%, right?
Yeah, but you never give one person 100%.
Well, I guess you could theoretically, but you know, you have a life, right?
Or poop sometimes.
So however much you give to one person is then divided between two, three or four.
Maybe that's what they meant by fully.
So however much fully is divided by number of partners.
Yes, but other people, some people like different things in different ways.
Like I know me and Melissa have very different taste in music.
So sometimes I will go to concerts that is a type of music which I love with someone
else rather than with her because she would get a lower value out of that time and someone
else would get a higher value.
For sure.
No, I think that that's a fair and accurate and correct point that to say, I was just
trying to give the best benefit of the doubt to comment or use this.
Okay.
Yeah.
Okay.
And our other commenter is going to be very difficult for me to say the Reddit name.
The Svevolat?
Your guess is as good as mine.
Oh, we should mention real quickly, these questions were taken from our webpage where
we have comments enabled on every single episode or on the, what is it, slash the Bayesian
conspiracy subreddit where we post a thread for every episode or just through email.
You can email us at the Bayesian, you know, at BayesianConspiracyPodcast at gmail.com.
Yes.
That latter one.
All right.
Anyway, Svevolat, still talking about the polyamory episodes that it would be very interesting
to hear you touch on the question of children in such relationships.
Now we didn't talk about that at all in the polyamory episodes.
I think two of us don't even ever want to have children.
One of us, I think, is leaning towards having a family at some point, which would be me.
So I wanted to address this very quickly without going into great detail because of course
it's a huge topic.
So here are my points.
People divorce and remarry and raise children during this process.
So we're already in a situation where lots of kids have mom, dad, mom's boyfriend, dad's
wife, you know.
So it actually, as far as stigma is concerned, that's a great out for kids kind of in their
teens.
They can just pretend like they have a broken, I'm sorry, that was a joke.
I don't actually think that those families are broken in any way.
I also would like to, oh wow, I also would like to point out that the modern nuclear
family of just two parents in their own home is a fairly recent invention.
People generally used to live with their older relatives, both to take care of the older
relatives and so the older relatives could watch children.
Exactly.
So parents are responding to the lack of cohabitating with extended families in various ways and
that could be co-housing, intentional communities where there's more adults involved in raising
kids and through poly households.
Now there hasn't been a ton of research on kids specifically in poly households, although
there has been research in communal raising and other forms.
So Elizabeth Sheff is actually the only, she's done the only research that I really know of.
That's Elizabeth and then Sheff is S-H-E-F-F.
I put up a link to her stuff on the episode 10 blog on our website.
Anyway, what she found is that kids in poly families are more articulate and thoughtful.
But caveats, like major caveats, the mainstream polyamorous communities in the US, Australia
and Europe are mostly white middle class, highly educated people.
Those kids already have huge advantages.
Also people who voluntarily enroll in studies about polyamorous families tend to at least
think that they got their shit together.
I can go at this at a more ancient, anthropological point of view.
There are a number of cultures in the Polynesian and Pacific Rim areas, as well also in Southern
America that would traditionally have multi-family or multi-adult families.
Often children have two fathers or possibly two mothers, but the two fathers was more
common.
They had a thing that was called paratol, no, partial paternity, God, I don't remember
the exact word now, or partial paternity.
Anyways, the idea was that a child could have literally more than one biological father.
Other men who had sex with the woman while she was pregnant, their genetic contribution
would be included as well.
And obviously they didn't use the word genetic contribution because they didn't have a concept
of genes.
But there were families that had multiple fathers and there was a time period where
these were studied because they were still around while anthropology was going on.
And it was found that the children who had multi-parent families did no worse than any
other children in their communities.
The only thing I would caveat there is that in these communities, in these antiquated
communities, they didn't actually have more than one biological father, they believed
they did.
Yes.
They were not like, hey, we thought it was the case so it was.
It also often had advantages of the children would have still multiple parents if one parent
happened to die.
And it was found that these children happened to die less often of violent means while they
were young.
Because apparently in these societies, you still sometimes died violently when you were
young.
Yeah, I mean, so the nice thing about having a bunch of adults is that there's more adults
to serve as role models, more attention, more adults providing rides to kids who can't
drive yet, education, all of that good stuff.
I've talked to a number of parents who think that two people really is not a lot of people
for watching children, especially if you have more than one.
And having that burden shared by more people would probably be a great boon to them.
Yeah, so people go about it in different ways.
There's so much more to get into there, like legal stuff, how kids feel about it.
They tend to not really care, but they like stability, of course.
So there's all sorts of other nuances to that question that is just way too much.
For parenting in general, for what it's worth, I think it was Gail's psychologist Paul Bloom
was doing some research, or quoted some research that I heard that it was, essentially kids
are going to turn out, how they're going to turn out, mostly irrespective of parenting
impact.
As long as you're not really screwing up, they're going to be fine.
That turned out to be not just his opinion, but what was found through some meta-study
data.
Tyler Cowan wrote a book about that.
Maybe that's what he was quoting.
This was a couple years ago that I heard something about this, but it was interesting.
One more comment on the polyamory episode.
I also want to point out that I think, if I am recalling correctly, that it has been
shown that generally children from single-parent families don't do as well as children from
two-parent families if they're being raised just by one person because one of the parents
is dead or ran off or something, they tend to have slightly worse outcomes.
But there has been a few studies shown that parents or children in families with more
than two parents don't do any worse than children in families with just two parents.
So if we're okay with children being raised in one-parent families, we should, I imagine,
be okay with children being raised in more than two-parent families since they do better
off than children in single-parent families, and yet we don't disallow those.
Did that make sense?
Yeah.
It makes sense to me.
If we're going to be consistent, and we're not going to draw arbitrary lines, that's right.
If we're doing it for the children, which I've always thought is a bullshit reason to
do things.
What if children ever done it for me?
Well, I just mean that for the children is used as a bludgeon all the time to stop adults
from doing anything adulty.
Hiding guy flags, the Bible, and children.
Yes.
Yeah.
Are you guys ready to...
Let's go on.
Keep on rolling.
Okay, so last comment about polyamory, the polyamory episode was from my husband.
He pointed out, and now I'm starting to quote, overcoming bias at all jumped on the polyband
wagon after I did.
The first result for polyamory I can find on less wrong or overcoming bias is a comment
on a November 2009 post, and we started dating in 2008.
I was even sort of okay with this idea when I was a conservative Christian, and he tells
everybody, you can update a tiny amount in favor of convergent ideas among a population
subset as being a potential reason for a higher percentage of polyidentified people in less
wrong, and a little bit away from founder effects.
Right on.
There were actually a couple more on polyamory.
Oops, my bad.
I forgot that we weren't skipping this one, but go ahead.
Well, Mark Plus, in a thread that went on for a while, made a point in the latter half
of it that in Silicon Valley there is a larger than average percentage of polyamorous people,
and it's also really freaking hard being a single guy trying to get a date in Silicon
Valley, and he thought the two were possibly related, and linked an article about that,
and the article itself said things along the lines of guys here are insanely focused
on their careers, and the male-female gender ratio is really imbalanced in Silicon Valley.
I don't want to get too far into it, but yes, Silicon Valley is in a uniquely bad position
if you're a single male.
You outnumber the women by a fair bit, not quite as bad as China, but getting there,
and everyone there mentions that the culture is very much about work, about working on
your startup, people are really passionate about their jobs there.
It's what they want to do with their lives, and so that's where they invest their time
and their money, and as a result, relationships tend to not get the investment that a lot
of people want from their partners.
So I think it is the case of Silicon Valley is bad for dating, and not that polyamory
is bad for Silicon Valley.
Yeah, you mentioned it might even be a response.
I did, because that, well, I don't want to get too much into that because it sort of
treats women as a commodity, but if the amount of relationships that women are in is greater
than the amount of relationships men are in, then it's actually helping the problem to
have polyamory rather than making it worse, and it does seem to be the case that women
reach polysaturation less quickly than men do.
I didn't know that one.
Yeah.
So polysaturation means that...
You have too many partners, and you can't even deal with it anymore.
That's mostly what I was going to say.
I don't know that to be the case at all.
Someone told me, anecdotal evidence at one of the last round meetups that that was something
that they believed to actually be the case.
In fact, she sent me stuff on it.
She did?
Great.
Well, then you can pull that up, and we'll put it on the episode notes.
I have no chance to review it for legitimacy, I'm just saying someone else at least put
that idea forward.
I've noticed that as well, but I don't have any hard data.
Fair enough.
I'll see if the data I have is hard.
Okay.
Anish also looked into the case of the family that was going to be evicted from their home
by the HOA.
Yeah.
I contributed to their legal defense fund a ways back when they ran that, so I had the
email address of the guy who coordinated it, and I asked him how that was going.
He said they are still in legal proceedings that they had to counter file a federal claim
in order to fight this, and it's kind of stressful for them all, but they are still in that house.
They haven't been evicted yet, and so they'll see what happens pending the court case.
I would like to write their prediction that, you know, bad for kids isn't a Polly family,
but it's having people in the yard screaming, get out of our neighborhood.
Actually, there aren't any people in their yard.
Oh, that's good.
They're nice.
I think that they...
Why are they pushing them out?
It's precedent.
Oh.
Yeah, I know.
You know, it's bad for kids, stupid precedence.
I'm going to say that.
Hi, Fib.
Hi.
I imagine that someone's going to give up at some point.
Okay, so moving on back, moving on back, are you guys ready to move on back to the episode
8, where we interviewed Brian Dunning.
Dunning.
With regards to conspiracy theories, commenter Adam spelled 4-T-0-M.
Adam.
He or she?
They.
Thank you.
Paraphrasing what Adam said here, they remarked that in Harry Potter and the Methods of Rationality,
Harry reflects on the nature of a perfect crime, and once his mind stops completing the pattern
of there's no such thing as a perfect crime, he realized perfect crimes probably happen
all the time, but you never hear about them because they were executed perfectly.
And to that effect, conspiracies may well plausibly happen all the time without us necessarily
being aware of them, if it was a good conspiracy.
But if someone commits a perfect crime and doesn't get caught, that's a great thing to talk about
on their deathbed, right?
And that's how you know about perfect crimes.
Perfect crimes, even.
I mean, maybe.
Maybe not.
They might die unexpectedly.
They might.
I'm just saying that the only perfect crimes they're the ones that never ever get discovered
is faulty.
Sure.
I mean, I would have all kinds of reasons to hide a perfect crime that I did, not wanting
to tarnish my reputation, or say if I stole two million dollars, I wouldn't want that taken
and seized from my family afterwards or something, but the only thing with regards to conspiracies
is-
Same with conspiracies.
Well, so the thing with my thought when I think about conspiracy theories, so the moon landing
was a hoax, that would require tens of thousands of people to be in on it.
That's not gonna happen.
If that really good perfect crimes have a small group, if it's not just maybe one person,
exactly, right?
One person can keep a secret really well.
Steven, I'm finding that in this recording, and that's gonna be your ringtone.
The moon landing was a hoax.
Oh, shit.
Every time.
There it is.
Yeah.
So this is probably the whole mistaken attribution, because all quotes end up being quoted as either
Benjamin Franklin or Abraham Lincoln.
But there is a famous quote that three people can keep a secret as long as two of them are
dead.
Ooh, bum bum bum.
That reminds me of the Muppets.
What is it, the pirates?
Treasure Island?
The Muppets Treasure Island?
I never saw it.
I didn't grow up on Sesame Street either.
Or are the Muppets different things?
The Muppets are different.
Seriously?
Oh, yeah.
Slightly different.
I mean, they both had Kermit.
It's a different friends.
There's a crossover from both of them?
Yes.
Oh, wow.
Yeah, the Muppets are much more of an adult-aimed show.
We are going terribly off track here, but for lovers of Muppet Treasure Islands, you
probably remember when the pirate buries gold and then kills everybody else who was involved
in burying gold, because that's what pirates do.
That's how you keep it secret.
I agree that a good conspiracy would be one that no one ever found out about, and so that
could be some mild evidence in favor, or some reason to update in favor of there might be
conspiracies that we just don't know about.
That's all it's going to do for us.
It's not evidence in favor of any particular conspiracy theory claim.
Right.
The fact that we can all tacitly agree that conspiracies are probably real doesn't lend at all credence
to the Bushed 9-11 conspiracy.
I love how Adam ends this.
He says, if you accept this reasoning, yes?
They.
They say that if you accept this reasoning, then the thing that should be your criteria for
conspiracy is not how believable or likely the conspiracy is, but how accurately it predicts
the future.
And I think that is a great line, because as Eliezer has said before, all beliefs should
pay rent.
If your belief has no applicable use in the real world, evict it, because it is just wasting
your brain space.
So if you approach the world with this conspiracy in mind and make predictions and those are
more accurate than predictions you would make if the conspiracy did not exist, then that
is a useful belief to have.
And I would like to see someone make these sort of predictions beforehand and then test
them out afterwards, because I have yet to see a conspiracy belief that has no evidence
behind it make more accurate predictions than one that does not include conspiracy in it.
And then a conspiracy theory come out to be confirmed true.
Right.
It is like, was it Laplace?
It was Napoleon's mathematician.
I forget exactly who it was.
I think it was Laplace.
I think it was Laplace too.
Yeah.
When he presented his mathematical model of the solar system to Napoleon, Napoleon said,
well, this is great, but where in your theory is God?
And Laplace replied, I have no need for that hypothesis, which a brilliant singer, but
it can also a great way to treat conspiracy theories.
It all works great without it.
Yeah.
I think one conspiracy theory that allegedly I haven't looked into this that much was in
popular discourse before it was confirmed true was MKUltra, you know, Dragging Soldiers
with LSD and stuff.
Yeah.
That said, I don't have a good source for that.
Actually, we have three relations to this episode with Brian Dunning.
He was on an episode of the Joe Rogan Experience podcast, which is, I think it's just past
its 800th episode and they do a few a week.
It's a big thing.
Joe Rogan made Brian Dunning's list of the top 10 most harmful celebrities.
Wow.
Because Joe Rogan puts out whatever conspiracy theory he wants all the time and he's got
a huge audience.
And Dunning argues that if you have a big audience, you have a responsibility to do your homework
before putting stuff out there.
And that's why Oprah made the top of the list.
So it was the 10 celebrities promoting the most harmful pseudoscience or something.
Anyway, so they talked about that quite a bit and MKUltra was the only one that Rogan
was around before it was actually out there.
Anyway, fun episode.
They go in circles for about 90 minutes.
It was actually kind of annoying.
But if you want to see a clash of worlds, it was good.
Adam goes on to make a point that they feel that people who are on the fringes and are
willing to believe conspiracy theories and woo science are actually those who are best
to convert to rationality because they spend more time thinking about their crazy beliefs
than the average person does.
So the first step to getting them to look at rationality is to first be the group of
people who will take every idea seriously and really run it through our rationalist
filter and not patronize or dismiss them out of hand those things that we think are ridiculous.
I have mixed feelings about that's the end of quote.
I have mixed feelings about that sentiment.
I feel like I propensity to believe crazy things doesn't necessarily mean that you're predisposed
towards great rational thinking.
It does mean that you spend a lot of time thinking about stuff.
So that part's true.
I think the fact, I kind of agree with him.
I think the fact that these people are willing to entertain ideas that are socially unpopular
and really think through their consequences makes them an ideal group because a lot of
people will hear some things like, you know, we think you should freeze your body after
you die and just be like, oh, these people are crazy and they don't believe in God.
They're obviously heathens and we don't want to have anything to do with them.
And the people who are more willing, who are used to being mocked by the mainstream and
are willing to state their opinions anyway are much more likely to be the kind who are
willing to listen to rationalist ideas.
And I think we could have the most impact on them because they would be the most likely
to both listen and possibly come around if they see these tools being useful and implementing
them in their own lives.
That's possible.
I think that there's a propensity to disregard country evidence that's way higher in the
conspiracy theory community than a healthy rationalist would endorse.
That said, it's food for thought.
We appreciate the comment.
Don't stop commenting.
Adam, we appreciate it.
And I do like that quote pointed out that one of our strengths as rationalists is that
we do take all ideas seriously no matter how silly they seem at first.
Speaking of taking all sorts of different ideas seriously, do you want to move on to the
next episode, Kill All Humans?
That was a brilliant transition.
That deserves a high five.
Now, actually, I was wondering if you, I wasn't on that episode.
I'm dating Jason, who I care for deeply, but it also means, well, not necessarily, but
in this case, it means that I have pretty emotional reactions in the negative sense to
some of his ideas.
And it takes me a while to kind of get used to them and not have a seriously negative
emotional reaction to it.
To be fair, I had a pretty emotionally negative reaction to the proposition as well.
But I came up with a few points and I wondered if you would address them.
Sure.
I guess I'm the person who's getting feedback and commenting in this case.
Let's do it.
So first, I said this is a great example of the danger of general AI, right?
That it may not have the values that we want it to have.
Right.
If it's a pain minimizer, if it's pain and suffering minimizer, like Jason, or if Jason
is programming that AI, which he's not, by the way, don't go after him, people, then
that would be a completely horrible thing because it could potentially end in, in the
end.
I agree completely.
And also for the record, don't go after anyone.
No, please don't.
Ever, regardless of what they're working on.
That said though, Jason, I think would argue that that would be a good thing if the AI
misfired and killed everybody.
Well, yeah, he would.
Yeah.
That's why he isn't programming AI and he's not allowed to.
So it raises, if we want to say that the proposition is bad, then it's a good argument
against, or it's a good argument about friendly AI caution.
But if we're...
Oh, right.
In Jason's case, this is a good reason to put the pedal to the metal on AI research.
That's what I was trying to say.
Thank you.
Yes, exactly.
You're right.
The other thing is, I had a problem with his statement that pain and suffering of being
tortured approaches negative infinity.
And because that was his reason for saying that pain and suffering is literally the only
thing that should be considered and that matters.
And isn't it clear to everybody that pain and suffering is not negative infinity?
That in terms of people's feelings.
Nothing approaches infinity?
I don't think it is clear to everybody.
I think it's clear to me that's not how I do my thinking, but I've had too many conversations
with too many people to say that it's clear to everybody.
I mean, it's clearly not clear to everybody.
It's not clear to Jason because he used that argument.
But I don't know, back me up here.
People's emotions and feelings don't approach infinity.
Yeah.
In the case of a sacred value, when you say that torture is something that we can never
do and you have this instinctive emotional reaction to it, which is what we want to
encourage, because that is how deontology feels from the inside is this thing is a
sacred value and we can never transgress against it, then that is the way that is
expressed in utilitarian terms, that it is negative infinity.
If you want to have a deontological ethics system, which I believe we do want to have
something similar to that for humans anyway, since utilitarianism isn't safe for humans.
Have heart and drug.
Okay.
We should maybe go into some day whether utilitarian ethics is safe for humans.
But assuming that it's not and we are best off sticking to something like deontology,
then that is how that would be expressed, that torture approaches negative infinity,
even if that is not literally true.
Yeah.
All right.
And actually, I think my most important point is that organisms, generally the ones that
are alive, have a strong preference to remain alive, like a really strong preference in
most utilitarian systems, or I'm not sure about this in most, but in a lot of ethics
systems, you take the preference of entities into account.
Can I argue Jason's side?
Please do.
And not that I agree with this, but I understand where he's coming from, I think, which is
why I'm going to argue his side.
Okay.
And if I'm wrong, I hope Jason writes him to correct me.
I can try to correct you since he's argued this with me before.
Okay.
So one of the things is that people's preferences can possibly be wrong.
Someone may have a preference to keep living because it has been instilled through evolution,
even though their life on whole is a net negative and they are suffering a lot more than they
are having pleasure.
But why do somebody, why does somebody else get to make that choice for them?
Well, that's the same question as why do parents get to decide whether their children have
chemotherapy or not?
A child with leukemia may not want to get chemotherapy because it makes them sick and makes their
lives miserable, but the parents know better that this is a thing you go through so that
you're better off at the end.
Like vaccines.
Like vaccines.
Same thing.
Those hurt when you get them.
But analogously, parents will also control a child's diet because the child wants to
eat as much sugar as possible because in an intestinal environment, sugar will scarce
and connect it to good food.
Right.
Now it's in nutritional cereal.
But sugar makes people happy.
Completely disregarding the kid's opinion on chemotherapy or eating sweet things is also
not the way to go.
Right?
How much to update in paper what the kid wants?
I see what you're saying if you would try to explain it to the kid.
Yeah.
But if the kid doesn't understand?
Obviously you got to do what you got to do.
Okay.
That's the argument, right?
Yes.
But we have strong preferences for staying alive and living and experiencing things,
right?
I sure do, yeah.
I have strong preferences for eating things with sugar in them, which I am happy to fulfill.
It makes me happier.
I'm an adult and having somebody else who is equal to me, like I don't have a mom.
I do have a mom.
I'm sorry, mom.
I'm not a child anymore.
I don't have somebody who is, like, superior to me telling me what to do.
Well, you guys know what I mean.
Yes, we do.
So if, you know, somebody else, like, adjacent decides that he gets to decide for all other
humans that are just like him, that they get to die now, or all other organisms, then
that's not good.
I would fight against him, yeah.
This is one of these places where you just can't reason to each other about it.
It comes down to a conflict of terminal values and ultimately the only arbiter is Force.
If it comes down to it, it's maybe not worth driving into, but it was an interesting conversation
I had for about an hour today that the three of us have a strong incentive to live.
But as far as other organisms on the planet, maybe excluding dolphins, great apes, and
elephants, I was talking to somebody today who argued that they really don't care whether
they live or not.
Excuse me.
Let me clarify their position better, because that was my initial reaction, that they do
care.
They have an instinctive, so I said, well, they run from predators.
They go for food.
They clearly want to keep living.
And she said, yeah, but not in the reflective way that you and I think about, we go to bed
at night thinking, I sure hope I wake up tomorrow because I'm excited to keep living.
Your dog doesn't feel that way.
Yeah, but it's not just instinctive to try to keep living.
Your dog really does want to keep living, even if it's not talking to itself in bed and
thinking, boy, howdy.
I just have so many plans.
I hope that I don't snuff it tonight.
I think that actually is a qualitative difference.
I agree with the person Steven was talking to.
And I'm inclined to agree with Katrina, because I feel like their preference is that they
have something sort of like that, even if it's not like what we have.
Yeah, it might not be expressed in the same way, but it's awfully similar.
How many humans think, I sure do like being alive and hope that I continue to be alive?
I think existential dread is one of the things that it makes being a human being a human.
How do you know that non-human animals don't feel that?
I think that your understanding of the intelligence of non-humans is greatly lacking.
I would also just say that most humans don't go to bed thinking that, but if asked, they
would say, if asked, the dog would just look at you.
Again, I'm in your camp.
I think their existence matters.
I think that their dog-like preference to want to wake up again tomorrow and keep being a dog matters.
Yeah, I don't think their preferences matter zero, not at all, but I do think it's far less important
than what a human's preference would be, because I know you're about to disagree with me,
but the fact that they don't have language is one of the things that makes me think that they don't
have the same sort of reflective ability to think about themselves.
Lots more animals have language than you realize, too.
I have so much content for this that this needs to be another episode and not a feedback to what people are.
Okay, then let's leave this.
I did get us on that path, but I did want to say that there are smart rational people who I think
that do have varying opinions on this, and this is something worth exploring.
Okay.
Next time, later.
Yes.
Another time.
Another time.
Another day.
Another day.
Another place.
Probably here.
So, I mean, I talked about preference being really important.
That's honestly the same.
It's a very similar argument to the one that I have against what Anayash went into in that episode,
which was to talk about how there's no moral value to an organism that experiences great heights of pleasure all the time.
Experiences?
Oh, yeah.
Nothing but the maximum pleasure.
Right, and that wasn't really clarified on the episode that it was they only experienced maximum
pleasure, not thought or desire or anything else like that.
And I think that that's kind of, I don't know.
I don't even know how to address that because it's a very specific thing.
Right.
We can touch on somebody commented about orgasm, didn't they?
Oh, a lot of people commented on orgasm.
Why don't you guys talk about it?
Okay.
Sure.
Do you want me to play the part of jewels?
Please do.
Comment your jewels.
So, Anayash, if you don't think that orgasmium is to be considered plus infinity,
replace orgasmium with something that actually is plus infinity,
and then, I guess, what is that and would you do it then?
Yeah.
So, the question is no, because what I would...
The answer is no.
Oh, sorry.
Yes.
So, the answer is no, because one of the...
This is not the only thing that I consider important,
but one of the most important things for me is the esteem of my peers,
having social interaction with other people and being liked by them.
So, you could make a drug that makes me think I'm liked by other people,
and then, I guess, I would be really happy if I was taking that drug,
but I wouldn't actually be liked by really people.
I wouldn't actually be liked by my peers.
The thing that's important about happiness is you get that feeling when the state of the world
is the way you want it to be.
And when you take a drug that just gives you the feeling,
that feeling is now divorced.
Yes, the feeling is now divorced from the actual state of the world.
It is now useless as a signal as to whether things are the way you want them to be or not.
You may as well just be inside your own head all the time and not interacting with the world.
Right.
So, that's that other person that we were talking to before.
His point is happiness is actual good things happening instead of just pleasure.
Yes.
Yeah, and the same person I was talking to today, actually,
I think would argue that, or she did argue,
that she would be fine living the rest of her existence in a simulation.
Well, yeah.
But I have this weird sentimental attachment to reality.
Okay.
Right?
To the physical world.
I might be okay in a simulation if, you know,
there were other people in the simulation with me that I could interact with.
They weren't just NPCs.
Exactly, because then I could still gain their esteem.
I could live with that too.
Yeah.
Well, are they good NPCs?
Then they might as well just be you.
They might be just, might as well be just like you.
But simulation theory is another discussion that we'll get into, huh?
But the merits of living in a simulation is what we were talking about.
But the orgasmium, what it does is it changes your brain to be in a happy state.
And I don't place any value at all on the mix of chemicals in my brain that makes me happy
if that is disconnected from the greater reality.
I do.
You do.
I would if you experienced the opposite.
You mean if I was depressed?
Yeah, no.
If you have a mix of chemicals that forces you to not be happy no matter what's going on.
Yes, that is extremely shitty.
Yeah.
This is ties into another.
So it's better if you have a mix that makes you pretty happy no matter what's going on.
But, yeah.
I want the mix to be entangled with what's happening outside of me.
Okay.
At least somewhat entangled.
Yes.
He can be pretty happy, but there's a limit.
Yeah.
Please don't put me in a matrix where no one else exists.
Or into what Mr. Olive, what commenter Mr. Olive Law described.
They had said, what if your orgasmium gave you a hallucination that you were living in the most fulfilling life possible?
Would you, Inyash and Steven, object to it then?
Yes.
Yeah, I would too.
I think so that's kind of what we're talking about with the simulation thing.
And I have an objection to being tricked that I'm living the best life possible.
I might as well be living in some sort of Truman Show situation.
Because you love truth.
Yeah.
And this is actually, this goes back to, there's a name for this.
I'm not sure if Mr. Olive Law was aware of it or not.
Robert Nozick's experience machine is the thought experiment that ties into this.
It's got a great Wikipedia page.
Before that, wasn't there a demon that would trick you?
There was Descartes' demon about the head in the jar.
Well, he didn't use the head in the jar phrase, but Descartes' demon was...
I mean, it all basically boils down to what we nowadays refer to as the Matrix from the movie.
Well, except the Matrix wasn't the perfect, you know, fake Udym on life, right?
Right.
So, yeah.
But as far as I can tell, anyone who's seen the movie agrees that even if the Matrix was perfect, they still wouldn't want to be in it.
Hey, y'all.
Or most people anyway, yes?
You want to know how I feel about that?
Yeah.
I feel like it depends on my options.
Ah.
If my options are, don't exist at all, or exist in a state where I'm tricked into thinking that I'm interacting with people, but they're not real people,
but I still think that they're fine, that everything's going pretty good,
then I'd rather be in that situation than non-existent.
I'd rather be non-existent.
Okay.
Than to be the only person in my universe.
Uh, undecided.
Yeah.
I think I'd prefer to live, even if all I did was, you know, live in a video game where no one else was a player character.
If you don't know, then it's not going to hurt, yeah?
And not everyone has the same desire that I have.
Some people could give less of a shit about what other people, they interact with other people or not,
but instead they want to, like, plumb the depths of the mysteries of physics or whatever.
Mm-hmm.
There's other things that other people want out of life.
Mm-hmm.
Fair enough.
Just to me personally, I would not be happy with that.
Did we want to move on?
We did.
We jumped around a bit with a Donald treadmill.
I just figured since we already basically articulated the experience machine, I wanted to get on to that.
Uh, Googleplexbyte asks,
is there a utilitarianism focused on maximizing fun?
I think that'd be my moral philosophy.
Happiness and suffering utility would be measured against how they affect fun.
Um, I don't know if there's one specifically for fun, but utilitarianism is extremely broad.
Uh, you could read Eleizer's fun theory sequence where he talks a lot about the fact that human value is a very large, multifaceted, many-tiered thing
and does not boil down to just fun or not fun or happy or not happy.
And I think there's a wonderful post called, uh, Morality is Awesome, which starts out with the question,
this is a less wrong post, not by Eleizer, a different contributor, starts out with the question,
a wizard has turned you into a whale.
Is this awesome, yes or no?
And it extrapolates further from there, but at some point it gets to the question,
what does any of this have to do with morality?
And his answer is, morality often gets caught up in these things, these grand words like good and bad,
and really what we want to know is whether something is awesome or not.
So we should just use the term awesome so we don't get sidetracked by all these ethical terms.
I use the word well-being when I'm talking morality with people, because they get hung up on,
well, good just means this or whatever.
And happiness is terribly hard to measure.
Exactly.
But well-being isn't as hard to measure.
Well, it more or less I think means exactly what our intuitions think it means, right?
And I don't think your intuition of well-being is going to radically differ from mine.
Do you get to do some of the things you want to do, like have water and food and live and shelter
and not be horribly diseased all the time?
I think those all fall under, you know, like, yeah, exactly.
And there are things, I agree, and there are things that fall obviously outside of well-being,
like being in constant agony, you know, feeling terrible emotionally.
Those things would not fall under the umbrella of well-being.
There's a hard to translate Greek word called eudaimonia, which applies to more than just well-being,
but it's also somewhat applicable.
It's just about living the best kind of life, the kind of life that if you could pick,
you would pick for yourself and for those you love.
So I'm sure the answer is yes, probably.
There's lots of utilitarianisms focused on other things.
And if not, make one.
Or at least consequentialist idealists.
Yeah.
Shall we go on?
Yeah.
To the cryo episode?
I guess that's all of it.
Yes, let's go on to the cryo episode.
Well, Adam had this thing about some of the point about Hedonophenyls.
Oh, he did, right, yeah.
They did, yeah.
They did, sorry.
Yes.
We don't, we do not know the gender of any of these commenters.
Yeah.
With the possible exception of Mr. Olavla.
Probably a Mr.
I, my money's on Mr.
Okay.
We'll just go with that.
Also, my husband is, he prefers he pronouns.
Adam says, since increasing wages in particular and changes in circumstance in general
do a poor job of actually increasing happiness, due to the adonic treadmill we were talking about,
they think the world is more safely improved by reducing suffering than by increasing happiness.
Because it seems like policies intended to increase happiness will cost a lot and tend
to have decreasing success, especially since other people do not necessarily agree on what
increases happiness.
Whereas reducing suffering doesn't involve the same sort of arms race against human expectations.
I think that's a fairly sound point.
I think that is too.
I think it'd be easier to make things a little better for the worst off than it would be to
make things better for the, for the well off.
Generally everyone can, can agree that being in constant pain is bad.
And if you can reduce that, then you're doing a good thing.
But there are things like inventing the internet, improving communications, that makes things
better for everyone and it decreases suffering, increases pleasure and decreases suffering.
Yes.
I don't know if it's so easy to measure the cost of increasing happiness and, and pleasure
and all that, whatever positive stuff versus decreasing the negative stuff.
I'm, I'm trying to think of an example.
It could be that like raising the minimum wage in the United States to $15 an hour might
do less to impact the, you know, average hedonic index of the world than say curing malaria.
Yeah.
So, so, so that, that, that I think it could be, you know, I guess even if it was affecting
the same number of people, if we wanted to find something that balanced up between those
two, that the people who are making a little more an hour are only marginally better off.
But, but it's easier.
Specifically with malaria.
Well, I mean, so like whatever it is, all the things that we take so for granted, you
know, even, even the people who make $6 an hour take for granted.
Having a place to, you know, more or less to live, having access to drinking water, having,
you know, not constantly starving.
Those are things that some people on earth actually deal with.
Okay.
So maybe for right now, maybe for right now we can make the argument that it costs less
to be suffering in the world than it costs to increase good happiness.
I don't know, whatever.
Although I really think that they're kind of the same thing.
I think you also decreasing suffering, you're increasing happiness, but I think you also
made a good point when you brought up the internet, because in addition to increasing
happiness, the internet has been a huge engine for increasing wealth.
Yes.
So it's, it didn't cost anything to make the internet.
Maybe it provided a lot more money than it ever costs.
So increasing happiness can also be a net positive rather than a cost.
Right.
And there also might be a point at which decreasing suffering is much more difficult and more
costly.
Yeah.
And those two things are reversed.
But again, I don't think that we can really make a judgment.
Yeah.
The three of us can make a judgment on which is more costly or less costly.
Aside from specific things.
Well, also we're supposed to work rather than looking at a specific thing.
We have to look for a bunch of specific things and then we have to write a paper about it,
Steven.
Well, so Adam's point was.
We have to have the episode on effective altruism, Steven.
We will.
But I will just say that Adam's point was as long as we're trying to keep things as
still manned as possible, it seems like policies intended to create or to increase happiness
cost a lot and have decreasing success.
Whereas reducing suffering doesn't involve the same sort of arms race against human expectation.
I think that that point remains sound.
Yeah.
I do think that decreasing suffering is increasing happiness.
I think that those are basically the same thing.
But policies directed towards increasing happiness are going to be less or are probably less
successful than policies directed towards alleviating suffering.
I think that that might have some merit to it.
I think that's a fair point.
Want to talk about cryonics?
Let's do it.
Okay.
So episode six, that's when we talked about cryonics.
And I am still cryocrocinating if you want an update on that.
But I also discovered something really sad about Europe.
Oh, yeah.
That Aniyash just told me.
So now he's going to tell all of you.
Some European.
Okay.
I don't remember who exactly it was.
Harsises.
Harsises.
Yeah.
That's right.
Harsises asked us if we know any good places to read about different life insurance options.
They live in Europe, at least for now.
And they understand that Rudy works only with US customers.
Question mark?
Yes.
Rudy only works with US customers.
People in Europe have much more greater difficulty accessing cryonics.
In some countries, it's actually illegal.
In most countries in Europe, it's illegal to fund cryonics through life insurance.
Because then it's viewed as some sort of scam.
I know in Russia, you can get cryonics.
As opposed when it comes to individual countries looking to your local laws,
but you may be out of luck, at least for now, until you get some laws passed.
Because I know there are a few countries where it's actually illegal.
And I would be willing to bet that Rudy Hoffman would be a better resource than any of us about any legal questions,
even outside the US who might have resources to direct you towards.
I noticed that Rudy Hoffman was no longer linked, and somebody else had been linked.
Which of you guys changed that?
Not me.
It was definitely on there when I put it on there.
I mean, I may have changed it on accident.
Oh.
Is he not linked anymore?
No.
Oh, well, okay.
I will go back and re-link him then.
Well, make sure that you're linked to somebody who works in the US.
Or it's Google Rudy Hoffman.
Or it's Google Rudy Hoffman.
It smelled like it sounds.
Okay.
Let's see.
Probably doesn't help anyone who's not a native English speaker.
Right.
It smelled like it sounds.
Good luck.
Go to the website.
Oh my gosh.
So this is a question about cryonics that has actually crossed my mind,
because I'm still on my driver's license, an organ donor, commenter JJ.
My one uninformed qualm is that if people get really into cryonics,
the number of organ donors in the world would decrease.
I'm not sure about this actually.
I should probably ask someone at the cryonics institute or somewhere if they would even
have time to take an organ from me before they put me under,
before they put me full of crowd protecting.
I sort of doubt it.
Unless,
In C.I.
For the cryonics institute, you cannot donate organs because they only do full body preservation.
Alcor, possibly, because they allowed just head preservation.
If you go with Alcor, I don't know what their policy is.
I think in absolute terms, the answer to this question is yes,
they probably would reduce the number of organ donors slightly.
If everyone was signed up, it wouldn't really have much of an impact though,
because generally if you die in an expected way in a hospital,
your organs aren't really usable anymore.
Maybe a few of them, but usually people who go into cryo are people at the end of their life
who die of old age and their organs aren't worth shit.
Now.
Currently.
Yes, currently.
The people whose organs are used are often traumatic accidents at the prime of their life.
Motorcycle accidents are a really big one.
If you die in a way that your organs can be harvested and given to other people,
you generally are not a good candidate for cryonics.
You've died in some way that was usually messy,
and there's probably won't be much of you left.
What's the medical term for head smash?
Yeah.
So I mean, a good crowd patient doesn't suffer from terminal head smash,
and those are people who are great at donating organs.
Exactly.
And most people who are dying of cancer generally you cannot donate their organs
because cancer has spread at that point if they're dying of it to a lot of different places in the body.
Related to that question, somebody named Patrick wrote,
you didn't really discuss the ethical consequences of spending a substantial amount of money on cryonics.
As most rationalists, I consider myself a consequentialist and I try to live as an effective altruist.
So for the question, if I should sign up for cryonics,
the main issue isn't whether I can afford cryonics,
but rather if I can justify spending 100k on a single digit probability that I might survive.
I could donate this money to AMF, against malaria, foundation by the way,
and save 50 children from dying from malaria.
Investing in the help of extremely poor people today has a very high return of investment.
I find it unlikely that I would be able to achieve more good by signing up for cryonics
than donating this money to the most effective charities.
What say you?
I have a very strong opinion on this.
I want to just take a quick pick with the way that Patrick phrased this,
save 50 kids the 100k, that charity sounds like shit.
And that is not the against malaria foundation,
they can save way more than 50 kids with $100,000.
No, I think the most recent estimate was $4,000 or so per child saved.
Per malaria?
No, not for malaria, per child saved.
It's not just the cost of malaria net,
there's the cost of getting them over, of running the organization and so forth,
and comes out to about $4,000 per child.
Sorry Patrick, my bad, I might cut that.
No, you need to stay on here apologizing.
That works.
Okay, cool.
We just updated.
We just updated, it's a good thing.
That's right, if someone else had the same qualm that I did, now they know better.
I can't believe it's $4,000 a life.
Yeah.
How, Peter Singer wrote an essay in the 70s called,
Maybe only 2,000 a life, I don't remember exactly,
but it's definitely in the more than 1,000 category.
Maybe that's how long it takes them to guide them all the way from babyhood through like,
tons of nets.
You don't just throw one net at someone and save their life.
What do you do from mosquitoes, from malaria?
One that will not save their life for their entire life.
They need more than a single net to keep themselves going.
And like, one net wouldn't even last for a year.
We need a malaria vaccine like now, like yesterday.
Oh yeah.
Jesus.
And you know, nets tear, nets are sometimes used for fishing or other things.
That's true.
Yeah, that's a big problem.
I only brought up Peter Singer's thing because he mentions that a,
like a pack of oral dehydration salts in the 70s cost like a nickel,
and that could cure a lethal case of diarrhea.
In parts of the world where you could still die from dehydration from diarrhea.
So in that sense, a nickel literally saves a life.
Yes, but you cannot literally just throw a nickel and have someone get a dehydration salt in their mouth.
That's true.
There's costs of transportation and yeah.
All right.
And distribution.
So I have strong opinions on this.
In the most basic utilitarian calculation, yes,
your one life is not worth more than 50 children's lives.
But I am of the opinion that people are allowed to prioritize their own existence over that of others.
And the existence of close loved ones over that of far off strangers.
I wrote this out in terms of straight utilitarian calculation.
That doesn't have much support.
There is some, but I'm not going to get into that.
However, in terms of practical utilitarianism, the case is far stronger.
Consequentialism.
Yes.
Because as was pointed out in the post, morality should be moral.
Spreading one's moral system may be immoral if one's moral system brings misery to those who adopt it.
If, for example, if a moral system demands that its adherents kill themselves in the prime of their lives in order to donate their organs to save 20 other people,
that moral system may in fact be immoral regardless of utilitarian quality of life calculations.
This is the personal morality reason to sign up for cryo, even if you're an EA.
And I do, yes, personally consider it suicide to not sign up for cryonics.
So is it suicide to, are you committing suicide to save other people?
And if so, can you morally spread that philosophy telling other people they should also commit suicide to save other people?
Furthermore, from an even more practical perspective, a moral system that kills its own members will fail to be passed on in the human population regardless of how good it may be.
If effective altruism demands that I kill myself, which is literally how I interpreted your argument, I will simply not be an effective altruist.
Given the choice between suicide and signing up for being a Nazi, a lot of people are going to get busy on justifying why Nazism isn't really all that bad and maybe trying to swing it away from its worst aspects.
So, that is why I consider it important to never force that moral choice on someone.
It will rapidly make the world a worse place, because you will not be driving them to effective altruism, you will be driving them away from it if they have to choose effective altruism and suicide.
That is the strategy reason to not oppose cryo if you're an effective altruist.
And finally, as an effective altruist, it's practically guaranteed that you are still doing vastly more good in your life than almost everyone else in the population, even after reducing your EA giving budget for the amount needed to fund the cryo.
So, you get a pass here.
No one can be perfect, no one is expected to be perfect, don't kill yourself trying to be perfect.
You seem to put no value at all on your existence, and I think that is a mistake, because the future would be better with people like you in it. Please join us there.
That was awesome. I think that's a great way to refute that. The only thing I was going to quickly comment is that the conversation I had this afternoon lasted from about 11.30 to 3.30, and we also talked about exactly this.
Jesus!
And I'm not sure if maybe I skimmed this earlier, but this was largely the same point that I made, was that it's okay to be a little selfish. There is a utilitarian-esque moral system called Care Ethics that basically is exactly what you said.
You care about your immediate circle a lot, their immediate circle a little less, their immediate circle a little less, and basically to where you don't care about strangers in third world or something.
And while you should still do something, it should. And the example that I raised when I was talking about it today was that you could probably do, you know, so if, so comment to Patrick said, I could spend my 100k life insurance policy and donate it to the McKinsey-Millaria Foundation.
You kind of turned it up to 11 by saying, yeah, but you could also just kill yourself to donate your organs, and you could also start up that life insurance policy now, get a way more expensive than one that you can sustainably afford, so that it pays out $3 million, then have someone, you know, spend $10,000 with that to have someone murder you, and then your organs are harvestable, and your life insurance gets paid out, and boom, in a week or two, however long it takes to get the paperwork finalized, you saved tons of people.
But you're right, even just mimetically, that moral system won't survive if it requires suicide, so I think that was a great way to put it. That said, Patrick, we appreciate your comment.
We do.
Please keep writing it.
And thank you because it allowed me to get that rant out. I always enjoy being able to do those sorts of things.
Who doesn't like a good rant?
Exactly.
We have a question from Not Without Incident that I think is a tough question.
It's what about older people with families who aren't rich?
The bar for not wanting all of your life insurance payout to go to your family, and specifically your children, is pretty high for me, and I imagine for many others.
Would I sign cryo a success chance of under 10%, which still seems high to me?
That feels hard to justify over the money going to my children.
Would it be fair to say that cryo preservation is currently the domain of the rich, the young, and the childless?
Let your kids make it themselves.
Make what themselves?
It's a gift. Make their way in life.
I'm just, that's a joke.
The question is all your money.
I mean, a lot of people get a life insurance policy that does involve a payout to surviving family and also freezes them.
Sure, but then that does sort of make it the domain of people looking forward to a nice life insurance policy.
And that might be the case.
We did touch on in the episode that it's unfortunate that this isn't super cheap.
It's not as expensive as people think, but it doesn't cost next to nothing.
So this is something that you need money to have to sign up for.
You need the luxury of, I guess, being okay with your children's financial future being a little less certain, you know, in the light of having to spend 100K-ish on this.
Yeah, it depends because, I don't know, a 200K policy where 100K went to your children and 100K went to cryo might be doable,
but on the other hand, depending on how much she makes and how old he is, that could be really expensive.
It might not be something that he or she or they might not be something they can afford.
And I just don't know what to say about that.
It feels sucky to say that if you're too old, you're screwed if you don't have enough money.
But I don't know what else to say.
I mean, I think you're in sort of the same boat as everyone was a century ago.
You know, you're just not capable, which I mean, it's the worst.
That's like the worst fucking thing to say ever, right?
Yeah.
Because you can't save everybody, people who have the resources, try and get on board.
As far as you made the joke about letting kids make their own way, and I misinterpreted it to say,
let them make their own decision to sign up.
And I would argue that if you're signed up for cryonics and you don't sign your kids up, you're a bad parent.
I think that you're deciding, well, my life's worth me, gambling's trying to save, not theirs.
That said, I think I could see the argument going both ways, but I think that there is a good argument in favor of saying,
if you think it's worth signing up yourself, I think it's worth signing your kids up.
And if you decide against it, then you might as well decide against the possible benefits of other possible life-saving interventions for them, right?
You know, you should maybe decide against having kids if you aren't willing to pay that amount of money.
That works.
Especially since life insurance for children should be really cheap.
Because they're more or less guaranteed from an insurance company perspective to live long enough to make it worth their money.
So there was an essay or an argument, something back and forth where somebody made mostly the point that I just made,
that if you're in favor of cryonics personally and you don't sign your kids up, you're doing parenting wrong.
I'll find out where that was and link to it.
We did it. We answered a question that no one asked.
About kids?
Yeah.
I think that it's unfortunate that cryo is not more widespread because the cryo itself isn't that expensive.
It's less than 20,000 or around 20,000 to get your entire body preserved.
The major expense is in flying out a team of specialists from Florida or wherever they're coming from and to do nothing but sit by your bed for an entire day while they wait for you to die and then rush you through everything.
If every hospital had a doctor on staff who could be doing other things but knew how to do the cryo and would just come over to your bed when you're about to die and go through the process,
it would be vastly cheaper.
And to the point where I hear a lot, I don't have exact numbers of this, but I hear a lot of our medical spending nowadays goes into the last few months of life.
And those last few months, in addition to not giving you that much more time with all the amount of money you're spending, it's also generally the worst, crappiest months of your life anyway.
You're in a lot of pain, you're going under constant medical procedures, and if we could divert that money to instead go into cryo, I think that would be a much better use of that money than trying to eke out another few months of bad quality of life.
But that is not an option right now, so right now I'm just, you know, talking about wishes and dreams.
Good response.
User John asked us to talk about the difference between Alcor and the CryoNix Institute and about neuropreservation versus whole body preservation.
Honestly, we could talk about it a bit more, but the best resource for that, that'll avoid any of our, I guess, memory errors, would be just checking out.
CryoNix Institute has a great frequently asked questions page, and I'm sure Alcor does too. They're your best resources.
I know that Alcor is more expensive, and CryoNix Institute is a little cheaper. That's the main difference that I cared about when signing up, so check out their websites.
Yeah.
Are you ready to move on to the next episode?
Let's do it.
Rolling back, what is rationality? We explained it to our grandmothers.
Episode 5.
Mr. Olive Law asked us, if you talk about not wanting to repeat what Eleizer wrote unless wrong, but personally I find the podcast more engaging and much more suitable to my media habits than the blog.
Why not have some episodes talking about what Eleizer writes on his blog?
My thoughts on that are basically, we sort of do, we just don't read the essays. A lot of the same central ideas, a lot of the lexicon carries over to here.
A lot of what we present is based directly off his writing or off writings of people that have been offshoots of the last wrong community.
So, even if we are not specifically reading his essays word for word, we do get into them a lot.
Every single episode so far has had some kind of mention of something that either appeared in less wrong or slayed star codex, I think.
Including this one. We already mentioned that the essay morality is awesome.
Yeah.
So, yeah.
Googleplexbite has another question for us. Why do you believe Eleizer is the person who should answer the question, what is rationality?
And they went ahead to give a list of other sources, including I think Scott Alexander, among others.
Yeah, not only that, but a lot of really academic sources of people who do a lot of formal Bayesian papers that they've had published.
The reason that I think Eleizer is the best source is because Eleizer, first of all, is really entertaining.
And he understands that he's talking to a lay audience, and so he goes all the way back to the point of common ground and takes us through every single step to get to where he needs us to be so that we can have a conversation on the topic.
If someone were to give me the stack of links that Googleplexbite listed and tell me to read through all those, I would say hell no.
This is a dry, this is boring, this is above my head for that matter. I can't do all this.
And I think that if you were to read those and really study them, like Googleplexbite seems to be promoting, then yes, you would be a better Bayesian, you would be a better rationalist.
But I think, again, this is a case of the perfect being the enemy of the good.
Because you don't need to be a perfect Bayesian as long as you are familiar with some foundational concepts that already makes your life better and has knock-on effects for the rest of the world around you if you are a more rational person as well.
My answer to that question, why do you believe Eleizer is the person who should answer what is rationality, is I don't. Moving on.
I just think it's better for society if everyone has access to some easy to use but non-optimized tech.
I think people absolutely should have access to easy to use and optimized information and that's great.
We know that Eleizer Yikowski is working towards that along with some other people, the more the merrier and if there are super easy to understand resources, throw them this way. We love that shit.
I have mixed feelings about identifying under umbrella term labels like rationalist. I feel like it gives people license to say, oh you're a rationalist, so you believe this, this, this and this.
Or you're an atheist, you believe this, this and this. I don't love labels for that reason but I see their utility as giving you a huge shortcut as to whether or not you agree with somebody.
Whether or not I could make expected to agree with somebody on something or where inferential distances lie.
I also find Yikowski's writing accessible. I did go back and read some Overcoming Bias and Robin Hansen writes really well. Let me put it this way.
He's very bright and knows what he's talking about. That doesn't come through very clearly in his writing.
Let me clarify. People can read Hansen on their own and make their own conclusions. I found it really opaque and confusing and not at all. It could have been way longer, right? But that's one man's opinion.
Well that's a good opinion. If you end up not reading it, then that is bad. It's better to read something that is imperfect and incomplete than to read nothing at all.
Fair enough. Yeah, that's a strong point.
Google punksbite made another comment and this one was, should rationalism discourage people from libertarianism?
And they go on to say, libertarianism works much better when all humans are rationalists by default. But rationalists are a tiny subset of population.
Something that rationalists are acutely aware of. This severely undermines how well libertarianism works.
And well, there's a lot to unpack there. You have to know a lot about libertarianism and it's a political system that's as close to, or an economic system that's as close to 100% free market as possible.
I think it's a bullshit definition.
Okay. I didn't put that definition in. Sorry.
Yeah, that's his.
Okay.
Oh, so yeah, I guess they did. I mistook the change in text as we were adding stuff.
So they said the efficiency of this free market assumes rational economic actors and rationalists should be the least likely to see each other as rational by default.
I answered that one with articles about how the free market works with irrational actors and that people being rational is not necessarily for the free market to work.
But I want to note right now that libertarians tend to want to minimize, want to believe that externalities are not real more than people in other political systems.
And that's a problem. But I don't think that's a problem with being rational or not, or that it's a system that works for rational people but not other people.
Or that people in the rationalist movement should be encouraging people to follow any political system or not follow any political system.
I personally am not politically aligned.
Fair enough. I don't have much to add. I think that those are all really good points, so.
Keep commenting.
Moving on back to the AlphaGo episode, we had a commenter ask, there's no attribution for this one. Does anyone have a link to an article or paper on the computer-generated electromagnetic circuit that was discussed towards the end of the podcast?
And the answer is yes we do.
It took a long time to dig up.
Yeah. And we are putting that link on this episode's comments.
On the webpage.
Yeah. We'll bring everyone on the webpage.
And on the AlphaGo webpage. We'll go and retroactively fit it there too.
Yeah. Thanks Patrick Chapin for finding that link for us.
Mud asks, are they Google? Do they only have one AI that's thinking hard about Go or is there a hundred of them minding their own business?
Maybe learning what others learn and thinking about learning in general.
This was sort of this hard to parse question, but if I'm understanding the question right, does Google have any AIs other than just the one that thinks about Go?
Yes.
And yeah, of course, they got lots of AIs doing lots of things.
And they just recently I learned from Patrick.
I went to talk to him on Tuesday that Google recently put in for a patent on a chip architecture, I think.
I may have misunderstood Patrick, that is optimized for learning systems and runs a lot faster.
That's right. That is what he said.
Yeah.
Yeah. So that'll be more widely available soon.
So more people will be able to use that dual learning system.
These conversations always raise my excitement and anxiety levels about the same.
But I mean, we know Google has AIs that are working on self-driving, you know?
They have AIs working on lots of things all the time.
Yeah, but are there any that are non-focused other than that chip that can be used for anything?
I don't think that...
You'd have to ask Google.
You'd have to ask Google what they've got different programs working on.
You might be able just to Google that question.
Sure, why not?
It's worth trying.
Episode three, that was our interview with Elias Rikowski and nobody had anything really to say about it except that it was great.
Yeah.
We're awesome. Thanks.
Woo!
Episode number two was called Raising the Sanity Waterline.
Google FlexBytes says, on making purchasing decisions, you didn't really get to the foundation of how you make these decisions.
From my perspective, when buying, I don't even consider the ethical impact.
Individual consumers are incapable of assessing the negative externalities of a product without expending a greater cost themselves.
Yeah.
You can keep going. That's part of...
I know, but they kept on going.
I don't even know if we need to go on with that. They're right.
Yeah.
Yeah.
I basically more or less agree that any sort of activism should be done, not when you're at the register.
It should be something that is evaluated outside of that.
It can be, though.
And regulation passed, you know, in the courthouse.
If you're trying to make decisions when you're buying things off the shelf, that's generally the wrong place for it.
Oh, that was the episode where we talked about, like, voting with your spending money, right?
Yeah, voting with your spending.
Actually, I think you totally... I mean, if you decide to get a mob together and do something, then why can't the mob figure that out?
I mean, on an individual level, of course, you can affect no change.
Do it makes you feel good, because that's the important part.
If you don't like how iPhones are manufactured, don't buy an iPhone.
Yeah.
But you're not going to shut down Apple by making that decision.
Yeah.
So I think it's kind of like, you know...
It's personal.
Yeah.
And you get some marginal benefit knowing that you're not helping contribute to a system that you don't like.
Yeah.
And I actually have an analogy for this that I used for vegetarian veganism that actually my husband came up with.
I don't know if he's the original person.
Anyway, when defending our diets, he said, imagine that you're in a firing squad.
In front of you is somebody who you know to be innocent.
But there are 20 other people in the squad.
Do you shoot the innocent person knowing that everybody else is also, you know, the person's going to die anyway?
Or do you not?
Oh.
Yeah.
If it's no cost to you to not shoot somebody who's innocent, don't do it.
Don't buy those sneakers.
Buy different sneakers.
Buy a different phone.
It's okay to stand up for what you believe in.
It is.
I also think it may be one of the cases of where you think you're doing something but actually not doing anything at all.
And so your energy is being wasted.
An example of this is the divestment movement, which is where a number of people convinced institutions, certain investing institutions,
not to buy stock in any company that has certain unethical practices and to sell any shares they had in that company.
And after a number of years, analyzing the effect that it had, it turned out that it had no effect at all
because people who were willing to buy shares in that company would just snap them up.
It was an efficient marketplace and so what one actor did didn't really matter.
Interesting.
Yeah.
So if you want to, if you want to make your products more ethical, do it on the government level.
But do what you want.
That too.
Do what you want to do.
Nothing matters.
We're all going to die.
Let's go watch some TV here.
If it's going to make you feel bad, then don't buy it, but...
Signaling is a real thing.
Right.
It's your life.
He goes on to say, or not he, they go on to say the most effective way to reduce negative externalities associated with consumption
would be to consume less and donate the difference in money.
He says they say all your money, but I'm going to go with the difference in money,
to charities that rank highly on GiveWell or other charity assessment organizations.
I think this is a wonderful point.
And I also would like to point out that there has been a suggestion made by Scott Alexander
that if you have trouble not eating meat, but still think that factory farming is unethical,
you should simply spend more of your money on offsets to those things,
either fighting against the...
Mostly vegan advocacy groups.
Those tend to be rated as the most effective charities.
Yes.
Either vegan advocacy groups or in other ways making up for that harm you're doing to the animal.
Basically...
By making more people vegan who are not you.
Moral indulgences, really.
Yes.
It's a weird way to look at it, but if you can buy carbon offsets, why not meat offsets?
Why not any offsets?
Right.
Murder offsets.
It does get into a weird murder offset place.
If you can save three lives, are you okay killing one life?
Because on net, you've saved more lives, right?
Good question.
Yeah, we'll find a link to that essay.
I think that's one of the more...
They're all fun, but that's one of the ones that I think you don't even have to come from any particular place to enjoy.
You don't have to be on either side of that question or whatever.
It's a fun exploration of that sort of trade off.
So, moving on.
JJ commented on the same episode.
I think the main issue is that people are so resistant to change, and once they get into a pattern of gathering data from one place, they stop looking for others.
This commenter came from a Christian background where, really, that was the best belief set for them given what they were exposed to.
And I think that that's a fair position.
I mean, you can't really be blamed, I think, for not having access to good information.
Especially if you're in the utmost position where you're homeschooled, controlled internet access, that sort of thing.
I think that part of practical rationality is the skill of knowing how to find and assess various pieces of information and sources.
And one typically needs motivation to want to change their beliefs.
I think this is also a great argument for having a large number of, not necessarily friends, but at least contacts and acquaintances across the political spectrum.
Because then you will see some of their arguments, and since they believe in them, they are motivated to present good ones, usually.
And that helps keep some of that in check.
I agree. I think that having a diverse pool of people to interact with on various topics is great mental exercise, and it's great to be exposed to various viewpoints.
And I would caution anybody who only has liberal friends or only has conservative friends or only has Christian friends to try and find some more friends.
But I think anyone who's that shut off probably won't be listening to this.
So it might be hard to reach unless they're already in their group, right?
Comment to JJ probably wouldn't have heard this podcast three years ago.
They wouldn't have heard this podcast three years ago because it wasn't made three years ago.
But they wouldn't have found something like this.
So I think it's sort of hard just to think of how to reach people who aren't authorized or reached.
I mean, that's a problem for another day. That's community and outreach and stuff.
It's also a good argument for being nice and being fun to be around, because then other people want to be around you and you can expose them to your views.
Like, don't be mean to conservatives and Christians and be like, oh, you stupid idiot Christian. Be nice to people.
Then they'll want to hang around with you and they'll learn some of the things you think.
I actually had a friend ask me. They are trying to protest some religious extremist group in their neighborhood who wants to talk about evil, trans, and gay people are and stuff.
And they were like, I don't really know how best to go about reaching people.
I want to try and set up something to where they can have an alternate information source.
And I said, well, really, I'm not the best at how to deal with getting kids to believe things other than not trying to argue them out of their religion,
but try, I guess, really just to A, be super nice and approachable. Don't go shout back at them.
And B, I think try and just slip in there that it's easy enough to find some common ground where they don't have to give up their religion
to agree that the the tenants that homosexuals and trans people are subhuman are not to be morally concerned or whatever it is.
This hate group church was advocating.
So there is a great episode.
Oh, I think it's 99% invisible, but I'm not sure I'll check and I'll update the webpage.
But there's this great episode about a small conservative conservative town in a rural area, which is one of the most protrans towns in the nation.
And it's because there's this one old guy who owns a movie theater that everyone loves and he's the best guy.
He owns the only movie theater in town and every now and then he would constantly dress up when a new movie came in town as one of the characters.
And as a lark, he would start out for every now and then like, oh, I'm playing the woman, you know, but he was trans and slowly he did that more and more.
And the thing is, everyone loved him.
So when he finally actually came out and he's like, yeah, this is just 2am all the time.
People are like, yeah, it's okay. That's cool.
And just the fact that he was their friend, he made the town one of the most protrans towns.
She even.
She, thank you.
She, yes.
The show started out with him.
He and then transitioned to she over time.
Well, they don't want to give away.
They don't want to give away the twist.
That spoiled the story and confused it a lot.
Um, David Youssef made a great point that in the book, thinking fast and thinking slow by Daniel Kenneman, the part of us that does rationality is really, really energy intensive.
Whereas our illogical and fast brain is really good at conserving energy on a daily basis.
Just this information taken on its own means that you're not trying to make people rational in the sense of them doing them with knowledge, but by building a set of habits and techniques and really simple rules of thumb.
When looking at someone who has done this from the outside, you'll notice that this doesn't make you on the surface smarter.
So they're talking about how it helps to develop heuristics to make rational thought easier instead of slow and clunky like everyone thinks it is.
And I agree completely. I think that's what I'm trying to promote.
The part that's confusing me of what they say though is that when looking at someone who has done this from the outside, what you notice is that this doesn't actually make you smarter on the surface.
I feel like it does.
When people think of smarter, they often think of book smarts like knowing, you know, the square, knowing what pi is out to 17 digits.
Okay, yeah, so maybe they put smarter in quotes.
Or they did, sorry.
Never mind, I read that backwards.
Sorry, David, you were perfectly clear.
Clack in whatever video I heard.
Oh yeah, this was another point that was brought up in that Raising the Sanity waterline.
It turns out my initial impression of Neil deGrasse Tyson's position on the 7% of religious people in the National Academy of Sciences was that Tyson was arguing that atheism isn't a solution.
That atheism isn't as obvious as the theory of gravity.
And that's why it's not as ubiquitous in the National Academy of Sciences.
I liked Inyash's interpretation more.
Yeah, it turns out Neil deGrasse Tyson was saying since there's still 7% theists in the National Academy of Sciences, that means you don't have an ironclad case.
And that made me very disappointed.
But I am a firm believer in death of the author and I will take that on a slightly more literal level at this point and say that I can interpret his words to be the words that I like.
The version of Neil deGrasse Tyson you heard in your head is right.
Exactly.
Thanks Clack for backing us up there.
On to Episode 1.
The last one.
We called that one What Uses Rationality.
Googleplexbyte says, default human decision making is founded in intuitive pattern recognition.
It's tempting to rely on this intuition because the human brain is machine-meltingly amazing at pattern recognition.
And when you're born with a hammer that makes artificial version look like a foam club, then everything is a nail.
Which I think is a very astute observation.
We are really good at that sort of intuitive pattern recognition.
He goes on to say, for this to be effective requires two things.
One, human-scale domain which we involved in.
Like where a baseball is going to land or whether a piece of art is forgery.
Or two.
Someone read that Malcolm Gladwell book.
Or two.
Someone has no idea what the two of you are talking about, but okay.
Sorry I said or two.
And two.
Number two.
Frequent exposure to similar events in order to develop an intuition.
Example, a baseball player or a craftsman running into these situations over and over.
In the rare cases where either of these requirements aren't met, it's better to use rationalist decision making.
As pattern recognition simply is not capable of handling variable outcomes or situations that they have been insufficiently exposed to.
Now I believe that the book was called Blink and he's making the same argument that Malcolm Gladwell did,
which is your intuition is great.
All of those pattern recognition mental shortcuts based from your experience are awesome,
but they break down in certain conditions.
That's it.
That's the whole book.
I wasn't really keen on it.
I enjoyed the hell out of the book.
I think Malcolm Gladwell is a very entertaining writer though, which is...
It was called Blink?
It was called Blink, yeah.
That sounds really self-evident and obvious.
That's the whole point.
Yeah, that's why I don't like Malcolm Gladwell.
Because he's writing obvious stuff, but he's spelling out obvious stuff.
He's entertaining about it too.
I will give it a skim and see who I agree with.
So yes, we agree and one of the reasons that rationalism became a thing is to handle the cases where we don't have a lot of cases that we can train our intuition on,
such as what happens if we create a superhuman AI?
Or biases like scope neglect.
We're not great at emotional multiplication.
Yeah.
So yeah, I think that's really interesting.
That's why we have rationality and thus that is a use of rationality.
For sure.
Russell says,
I consider it important to avoid idolizing Eleazar Yackowski since it's a common focal point of criticism of rationality.
Agreed.
I...
It's not that I disagree.
I agree with what he says in a literal sense, but I disagree with the implications behind it.
Oh, I don't know.
It's not like we want to throw him under the bus.
We think that he's a hugely influential person.
Yeah.
But the community as a whole often is asking themselves,
do we give too much credit to Eleazar?
This is one of the most self-critical communities I've ever seen.
And so it's actually started to annoy me recently when people are like,
well, don't give too much credit to Eleazar.
I'm like, yeah, no, okay, obviously he's not God.
And he has some personality flaws as we've witnessed a few times.
But the man is legitimately a genius at explaining things to people.
And he's very entertaining.
And stop bagging on him.
He's contributed a lot.
He brought us all together in this community.
And I think that we aren't idolizing him.
It's not like we're treating him like David Karash, you know?
We think he's a smart guy who wrote some...
Who's David Kresh?
I don't know who David Kresh is.
Oh, wow.
Did I just date myself?
Maybe.
He was the leader of the Branch Davidian cult,
who are famous because they locked themselves up in a compound when the FBI came
and then burned them all down and killed a lot of them.
Sorry, our issues that we don't know history.
Actually, I should know this because less than two months ago,
there was a Skeptoid episode on conspiracy theories surrounding that incident.
Oh, okay.
Back to Brian Dunning.
So, yeah, no.
No one idolizes him to the point that a lot of people are saying,
and it annoys me when people try to make us out to be people who idolize him.
We don't idolize him.
We admire him because of the contributions he's made.
And some people are more given to being fans
and being kind of in a fan space of all sorts of different people.
So give them some slack.
And I think that the difference that you know what you're trying to get at is
that there's an important difference in kind between merit-based admiration
and blind idolatry.
And I think that the lesser-run community probably falls more into the former camp.
The less-run community gives EY a lot more flack than the I Love Science community
gives Neil deGrasse Tyson or anyone else.
There was a great comment on Reddit where somebody said Neil deGrasse Tyson came to their high school
and that turns out he was kind of a dick like the staff.
And someone in all caps, who's your god now, Reddit?
I know, that's the thing.
No one is going around saying, we shouldn't idolize Neil deGrasse Tyson too much.
We might turn into a Neil deGrasse Tyson cult.
And I don't know if he's actually a dick.
That was one person on Reddit, so who knows.
But of course he had to have been nice because he's Neil deGrasse Tyson.
All right.
Russell, we know you.
Thanks for your feedback.
Critical feedback as always.
We assume that you're the same Russell that we know actually.
It actually is.
That's the email that we received from Russell.
Yeah, okay.
Yeah.
All right.
So Ian Rodin, if a method of decision making produces consistently better results than
you are getting with your rational approach, then you are no longer being rational by following
your approach.
Answer, Ian, yes.
Yep.
Yep.
Yeah, that's the same thing.
That's the same thing that people say in science.
Like, oh, you're, if science is giving you the wrong answer, then you're not doing science,
right?
And whatever.
If your predictive model, yeah, if your predictive model doesn't predict things very well and
you get a better one, then you've got to update your model.
Yeah.
That's what Bayesian rationality is all about.
That's true.
Update your model.
And we had one more question that we're going to save for our episode on ethics.
Random Anon said, how is virtue ethics and or stoicism regarded in the rationalist world?
We'll do our best to cover that in an episode that we're going to do ethics at some point,
maybe more than once.
So we'll see you then for another day.
We are so done.
Yes.
Yeah, that was fun.
But keep sending it stuff.
This is a lot of fun.
This is one of the more fun episodes.
It was, they're all fun.
Yeah.
Whatever.
I had fun doing this.
I did too.
And we will start doing this at the beginning or end of various episodes so that we do
not get this big backlog that we have to power through in just one sitting.
Cool.
So if you want to be featured with your screen name and I guess go ahead and send us your
preferred pronouns so people don't slip up constantly or use they.
And if you want us to use your name or your username or whatever you prefer, let us know
and we can adjust that.
Write us on, yeah, write us on our subreddit, the Bayesian Conspiracy.
Write us on episode specific posts on our website.
The BayesianConspiracy.com.
Or email us at BayesianConspiracyPodcast at gmail.com.
And we look forward to hearing from you.
And if you haven't done so yet, please consider leaving a review at iTunes or rating something
to help keep the podcast relevant in the iTunes marketplace.
All right.
Thanks for listening.
Bye.
See ya.
