However, in terms of practical utilitarianism, the case is far stronger.
Consequentialism.
Yes.
Because as was pointed out in the post, morality should be moral.
Spreading one's moral system may be immoral if one's moral system brings misery to those who adopt it.
If, for example, if a moral system demands that its adherents kill themselves in the prime of their lives in order to donate their organs to save 20 other people,
that moral system may in fact be immoral regardless of utilitarian quality of life calculations.
This is the personal morality reason to sign up for cryo, even if you're an EA.
And I do, yes, personally consider it suicide to not sign up for cryonics.
So is it suicide to, are you committing suicide to save other people?
And if so, can you morally spread that philosophy telling other people they should also commit suicide to save other people?
Furthermore, from an even more practical perspective, a moral system that kills its own members will fail to be passed on in the human population regardless of how good it may be.
If effective altruism demands that I kill myself, which is literally how I interpreted your argument, I will simply not be an effective altruist.
Given the choice between suicide and signing up for being a Nazi, a lot of people are going to get busy on justifying why Nazism isn't really all that bad and maybe trying to swing it away from its worst aspects.
So, that is why I consider it important to never force that moral choice on someone.
It will rapidly make the world a worse place, because you will not be driving them to effective altruism, you will be driving them away from it if they have to choose effective altruism and suicide.
That is the strategy reason to not oppose cryo if you're an effective altruist.
And finally, as an effective altruist, it's practically guaranteed that you are still doing vastly more good in your life than almost everyone else in the population, even after reducing your EA giving budget for the amount needed to fund the cryo.
So, you get a pass here.
No one can be perfect, no one is expected to be perfect, don't kill yourself trying to be perfect.
You seem to put no value at all on your existence, and I think that is a mistake, because the future would be better with people like you in it. Please join us there.
That was awesome. I think that's a great way to refute that. The only thing I was going to quickly comment is that the conversation I had this afternoon lasted from about 11.30 to 3.30, and we also talked about exactly this.
Jesus!
And I'm not sure if maybe I skimmed this earlier, but this was largely the same point that I made, was that it's okay to be a little selfish. There is a utilitarian-esque moral system called Care Ethics that basically is exactly what you said.
You care about your immediate circle a lot, their immediate circle a little less, their immediate circle a little less, and basically to where you don't care about strangers in third world or something.
And while you should still do something, it should. And the example that I raised when I was talking about it today was that you could probably do, you know, so if, so comment to Patrick said, I could spend my 100k life insurance policy and donate it to the McKinsey-Millaria Foundation.
You kind of turned it up to 11 by saying, yeah, but you could also just kill yourself to donate your organs, and you could also start up that life insurance policy now, get a way more expensive than one that you can sustainably afford, so that it pays out $3 million, then have someone, you know, spend $10,000 with that to have someone murder you, and then your organs are harvestable, and your life insurance gets paid out, and boom, in a week or two, however long it takes to get the paperwork finalized, you saved tons of people.
But you're right, even just mimetically, that moral system won't survive if it requires suicide, so I think that was a great way to put it. That said, Patrick, we appreciate your comment.
We do.
Please keep writing it.
And thank you because it allowed me to get that rant out. I always enjoy being able to do those sorts of things.
Who doesn't like a good rant?
Exactly.
We have a question from Not Without Incident that I think is a tough question.
It's what about older people with families who aren't rich?
The bar for not wanting all of your life insurance payout to go to your family, and specifically your children, is pretty high for me, and I imagine for many others.
Would I sign cryo a success chance of under 10%, which still seems high to me?
That feels hard to justify over the money going to my children.
Would it be fair to say that cryo preservation is currently the domain of the rich, the young, and the childless?
Let your kids make it themselves.
Make what themselves?
It's a gift. Make their way in life.
I'm just, that's a joke.
The question is all your money.
I mean, a lot of people get a life insurance policy that does involve a payout to surviving family and also freezes them.
Sure, but then that does sort of make it the domain of people looking forward to a nice life insurance policy.
And that might be the case.
We did touch on in the episode that it's unfortunate that this isn't super cheap.
It's not as expensive as people think, but it doesn't cost next to nothing.
So this is something that you need money to have to sign up for.
You need the luxury of, I guess, being okay with your children's financial future being a little less certain, you know, in the light of having to spend 100K-ish on this.
Yeah, it depends because, I don't know, a 200K policy where 100K went to your children and 100K went to cryo might be doable,
but on the other hand, depending on how much she makes and how old he is, that could be really expensive.
It might not be something that he or she or they might not be something they can afford.
And I just don't know what to say about that.
It feels sucky to say that if you're too old, you're screwed if you don't have enough money.
But I don't know what else to say.
I mean, I think you're in sort of the same boat as everyone was a century ago.
You know, you're just not capable, which I mean, it's the worst.
That's like the worst fucking thing to say ever, right?
Yeah.
Because you can't save everybody, people who have the resources, try and get on board.
As far as you made the joke about letting kids make their own way, and I misinterpreted it to say,
let them make their own decision to sign up.
And I would argue that if you're signed up for cryonics and you don't sign your kids up, you're a bad parent.
I think that you're deciding, well, my life's worth me, gambling's trying to save, not theirs.
That said, I think I could see the argument going both ways, but I think that there is a good argument in favor of saying,
if you think it's worth signing up yourself, I think it's worth signing your kids up.
And if you decide against it, then you might as well decide against the possible benefits of other possible life-saving interventions for them, right?
You know, you should maybe decide against having kids if you aren't willing to pay that amount of money.
That works.
Especially since life insurance for children should be really cheap.
Because they're more or less guaranteed from an insurance company perspective to live long enough to make it worth their money.
So there was an essay or an argument, something back and forth where somebody made mostly the point that I just made,
that if you're in favor of cryonics personally and you don't sign your kids up, you're doing parenting wrong.
I'll find out where that was and link to it.
We did it. We answered a question that no one asked.
About kids?
Yeah.
I think that it's unfortunate that cryo is not more widespread because the cryo itself isn't that expensive.
It's less than 20,000 or around 20,000 to get your entire body preserved.
The major expense is in flying out a team of specialists from Florida or wherever they're coming from and to do nothing but sit by your bed for an entire day while they wait for you to die and then rush you through everything.
If every hospital had a doctor on staff who could be doing other things but knew how to do the cryo and would just come over to your bed when you're about to die and go through the process,
it would be vastly cheaper.
And to the point where I hear a lot, I don't have exact numbers of this, but I hear a lot of our medical spending nowadays goes into the last few months of life.
And those last few months, in addition to not giving you that much more time with all the amount of money you're spending, it's also generally the worst, crappiest months of your life anyway.
You're in a lot of pain, you're going under constant medical procedures, and if we could divert that money to instead go into cryo, I think that would be a much better use of that money than trying to eke out another few months of bad quality of life.
But that is not an option right now, so right now I'm just, you know, talking about wishes and dreams.
Good response.
User John asked us to talk about the difference between Alcor and the CryoNix Institute and about neuropreservation versus whole body preservation.
Honestly, we could talk about it a bit more, but the best resource for that, that'll avoid any of our, I guess, memory errors, would be just checking out.
CryoNix Institute has a great frequently asked questions page, and I'm sure Alcor does too. They're your best resources.
I know that Alcor is more expensive, and CryoNix Institute is a little cheaper. That's the main difference that I cared about when signing up, so check out their websites.
Yeah.
Are you ready to move on to the next episode?
Let's do it.
Rolling back, what is rationality? We explained it to our grandmothers.
Episode 5.
Mr. Olive Law asked us, if you talk about not wanting to repeat what Eleizer wrote unless wrong, but personally I find the podcast more engaging and much more suitable to my media habits than the blog.
Why not have some episodes talking about what Eleizer writes on his blog?
My thoughts on that are basically, we sort of do, we just don't read the essays. A lot of the same central ideas, a lot of the lexicon carries over to here.
A lot of what we present is based directly off his writing or off writings of people that have been offshoots of the last wrong community.
So, even if we are not specifically reading his essays word for word, we do get into them a lot.
Every single episode so far has had some kind of mention of something that either appeared in less wrong or slayed star codex, I think.
Including this one. We already mentioned that the essay morality is awesome.
Yeah.
So, yeah.
Googleplexbite has another question for us. Why do you believe Eleizer is the person who should answer the question, what is rationality?
And they went ahead to give a list of other sources, including I think Scott Alexander, among others.
Yeah, not only that, but a lot of really academic sources of people who do a lot of formal Bayesian papers that they've had published.
The reason that I think Eleizer is the best source is because Eleizer, first of all, is really entertaining.
And he understands that he's talking to a lay audience, and so he goes all the way back to the point of common ground and takes us through every single step to get to where he needs us to be so that we can have a conversation on the topic.
If someone were to give me the stack of links that Googleplexbite listed and tell me to read through all those, I would say hell no.
This is a dry, this is boring, this is above my head for that matter. I can't do all this.
And I think that if you were to read those and really study them, like Googleplexbite seems to be promoting, then yes, you would be a better Bayesian, you would be a better rationalist.
But I think, again, this is a case of the perfect being the enemy of the good.
Because you don't need to be a perfect Bayesian as long as you are familiar with some foundational concepts that already makes your life better and has knock-on effects for the rest of the world around you if you are a more rational person as well.
My answer to that question, why do you believe Eleizer is the person who should answer what is rationality, is I don't. Moving on.
I just think it's better for society if everyone has access to some easy to use but non-optimized tech.
I think people absolutely should have access to easy to use and optimized information and that's great.
We know that Eleizer Yikowski is working towards that along with some other people, the more the merrier and if there are super easy to understand resources, throw them this way. We love that shit.
I have mixed feelings about identifying under umbrella term labels like rationalist. I feel like it gives people license to say, oh you're a rationalist, so you believe this, this, this and this.
Or you're an atheist, you believe this, this and this. I don't love labels for that reason but I see their utility as giving you a huge shortcut as to whether or not you agree with somebody.
Whether or not I could make expected to agree with somebody on something or where inferential distances lie.
I also find Yikowski's writing accessible. I did go back and read some Overcoming Bias and Robin Hansen writes really well. Let me put it this way.
He's very bright and knows what he's talking about. That doesn't come through very clearly in his writing.
Let me clarify. People can read Hansen on their own and make their own conclusions. I found it really opaque and confusing and not at all. It could have been way longer, right? But that's one man's opinion.
Well that's a good opinion. If you end up not reading it, then that is bad. It's better to read something that is imperfect and incomplete than to read nothing at all.
Fair enough. Yeah, that's a strong point.
Google punksbite made another comment and this one was, should rationalism discourage people from libertarianism?
And they go on to say, libertarianism works much better when all humans are rationalists by default. But rationalists are a tiny subset of population.
Something that rationalists are acutely aware of. This severely undermines how well libertarianism works.
And well, there's a lot to unpack there. You have to know a lot about libertarianism and it's a political system that's as close to, or an economic system that's as close to 100% free market as possible.
I think it's a bullshit definition.
Okay. I didn't put that definition in. Sorry.
Yeah, that's his.
Okay.
Oh, so yeah, I guess they did. I mistook the change in text as we were adding stuff.
So they said the efficiency of this free market assumes rational economic actors and rationalists should be the least likely to see each other as rational by default.
I answered that one with articles about how the free market works with irrational actors and that people being rational is not necessarily for the free market to work.
But I want to note right now that libertarians tend to want to minimize, want to believe that externalities are not real more than people in other political systems.
And that's a problem. But I don't think that's a problem with being rational or not, or that it's a system that works for rational people but not other people.
Or that people in the rationalist movement should be encouraging people to follow any political system or not follow any political system.
I personally am not politically aligned.
Fair enough. I don't have much to add. I think that those are all really good points, so.
Keep commenting.
Moving on back to the AlphaGo episode, we had a commenter ask, there's no attribution for this one. Does anyone have a link to an article or paper on the computer-generated electromagnetic circuit that was discussed towards the end of the podcast?
And the answer is yes we do.
It took a long time to dig up.
Yeah. And we are putting that link on this episode's comments.
On the webpage.
Yeah. We'll bring everyone on the webpage.
And on the AlphaGo webpage. We'll go and retroactively fit it there too.
Yeah. Thanks Patrick Chapin for finding that link for us.
Mud asks, are they Google? Do they only have one AI that's thinking hard about Go or is there a hundred of them minding their own business?
Maybe learning what others learn and thinking about learning in general.
This was sort of this hard to parse question, but if I'm understanding the question right, does Google have any AIs other than just the one that thinks about Go?
Yes.
And yeah, of course, they got lots of AIs doing lots of things.
And they just recently I learned from Patrick.
I went to talk to him on Tuesday that Google recently put in for a patent on a chip architecture, I think.
I may have misunderstood Patrick, that is optimized for learning systems and runs a lot faster.
That's right. That is what he said.
Yeah.
Yeah. So that'll be more widely available soon.
So more people will be able to use that dual learning system.
These conversations always raise my excitement and anxiety levels about the same.
But I mean, we know Google has AIs that are working on self-driving, you know?
They have AIs working on lots of things all the time.
Yeah, but are there any that are non-focused other than that chip that can be used for anything?
I don't think that...
You'd have to ask Google.
You'd have to ask Google what they've got different programs working on.
You might be able just to Google that question.
Sure, why not?
It's worth trying.
Episode three, that was our interview with Elias Rikowski and nobody had anything really to say about it except that it was great.
Yeah.
We're awesome. Thanks.
Woo!
Episode number two was called Raising the Sanity Waterline.
Google FlexBytes says, on making purchasing decisions, you didn't really get to the foundation of how you make these decisions.
From my perspective, when buying, I don't even consider the ethical impact.
Individual consumers are incapable of assessing the negative externalities of a product without expending a greater cost themselves.
Yeah.
You can keep going. That's part of...
I know, but they kept on going.
I don't even know if we need to go on with that. They're right.
Yeah.
Yeah.
I basically more or less agree that any sort of activism should be done, not when you're at the register.
It should be something that is evaluated outside of that.
It can be, though.
And regulation passed, you know, in the courthouse.
If you're trying to make decisions when you're buying things off the shelf, that's generally the wrong place for it.
Oh, that was the episode where we talked about, like, voting with your spending money, right?
Yeah, voting with your spending.
Actually, I think you totally... I mean, if you decide to get a mob together and do something, then why can't the mob figure that out?
I mean, on an individual level, of course, you can affect no change.
Do it makes you feel good, because that's the important part.
