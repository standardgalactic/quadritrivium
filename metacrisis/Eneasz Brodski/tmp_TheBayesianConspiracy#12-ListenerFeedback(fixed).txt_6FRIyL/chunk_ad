replace orgasmium with something that actually is plus infinity,
and then, I guess, what is that and would you do it then?
Yeah.
So, the question is no, because what I would...
The answer is no.
Oh, sorry.
Yes.
So, the answer is no, because one of the...
This is not the only thing that I consider important,
but one of the most important things for me is the esteem of my peers,
having social interaction with other people and being liked by them.
So, you could make a drug that makes me think I'm liked by other people,
and then, I guess, I would be really happy if I was taking that drug,
but I wouldn't actually be liked by really people.
I wouldn't actually be liked by my peers.
The thing that's important about happiness is you get that feeling when the state of the world
is the way you want it to be.
And when you take a drug that just gives you the feeling,
that feeling is now divorced.
Yes, the feeling is now divorced from the actual state of the world.
It is now useless as a signal as to whether things are the way you want them to be or not.
You may as well just be inside your own head all the time and not interacting with the world.
Right.
So, that's that other person that we were talking to before.
His point is happiness is actual good things happening instead of just pleasure.
Yes.
Yeah, and the same person I was talking to today, actually,
I think would argue that, or she did argue,
that she would be fine living the rest of her existence in a simulation.
Well, yeah.
But I have this weird sentimental attachment to reality.
Okay.
Right?
To the physical world.
I might be okay in a simulation if, you know,
there were other people in the simulation with me that I could interact with.
They weren't just NPCs.
Exactly, because then I could still gain their esteem.
I could live with that too.
Yeah.
Well, are they good NPCs?
Then they might as well just be you.
They might be just, might as well be just like you.
But simulation theory is another discussion that we'll get into, huh?
But the merits of living in a simulation is what we were talking about.
But the orgasmium, what it does is it changes your brain to be in a happy state.
And I don't place any value at all on the mix of chemicals in my brain that makes me happy
if that is disconnected from the greater reality.
I do.
You do.
I would if you experienced the opposite.
You mean if I was depressed?
Yeah, no.
If you have a mix of chemicals that forces you to not be happy no matter what's going on.
Yes, that is extremely shitty.
Yeah.
This is ties into another.
So it's better if you have a mix that makes you pretty happy no matter what's going on.
But, yeah.
I want the mix to be entangled with what's happening outside of me.
Okay.
At least somewhat entangled.
Yes.
He can be pretty happy, but there's a limit.
Yeah.
Please don't put me in a matrix where no one else exists.
Or into what Mr. Olive, what commenter Mr. Olive Law described.
They had said, what if your orgasmium gave you a hallucination that you were living in the most fulfilling life possible?
Would you, Inyash and Steven, object to it then?
Yes.
Yeah, I would too.
I think so that's kind of what we're talking about with the simulation thing.
And I have an objection to being tricked that I'm living the best life possible.
I might as well be living in some sort of Truman Show situation.
Because you love truth.
Yeah.
And this is actually, this goes back to, there's a name for this.
I'm not sure if Mr. Olive Law was aware of it or not.
Robert Nozick's experience machine is the thought experiment that ties into this.
It's got a great Wikipedia page.
Before that, wasn't there a demon that would trick you?
There was Descartes' demon about the head in the jar.
Well, he didn't use the head in the jar phrase, but Descartes' demon was...
I mean, it all basically boils down to what we nowadays refer to as the Matrix from the movie.
Well, except the Matrix wasn't the perfect, you know, fake Udym on life, right?
Right.
So, yeah.
But as far as I can tell, anyone who's seen the movie agrees that even if the Matrix was perfect, they still wouldn't want to be in it.
Hey, y'all.
Or most people anyway, yes?
You want to know how I feel about that?
Yeah.
I feel like it depends on my options.
Ah.
If my options are, don't exist at all, or exist in a state where I'm tricked into thinking that I'm interacting with people, but they're not real people,
but I still think that they're fine, that everything's going pretty good,
then I'd rather be in that situation than non-existent.
I'd rather be non-existent.
Okay.
Than to be the only person in my universe.
Uh, undecided.
Yeah.
I think I'd prefer to live, even if all I did was, you know, live in a video game where no one else was a player character.
If you don't know, then it's not going to hurt, yeah?
And not everyone has the same desire that I have.
Some people could give less of a shit about what other people, they interact with other people or not,
but instead they want to, like, plumb the depths of the mysteries of physics or whatever.
Mm-hmm.
There's other things that other people want out of life.
Mm-hmm.
Fair enough.
Just to me personally, I would not be happy with that.
Did we want to move on?
We did.
We jumped around a bit with a Donald treadmill.
I just figured since we already basically articulated the experience machine, I wanted to get on to that.
Uh, Googleplexbyte asks,
is there a utilitarianism focused on maximizing fun?
I think that'd be my moral philosophy.
Happiness and suffering utility would be measured against how they affect fun.
Um, I don't know if there's one specifically for fun, but utilitarianism is extremely broad.
Uh, you could read Eleizer's fun theory sequence where he talks a lot about the fact that human value is a very large, multifaceted, many-tiered thing
and does not boil down to just fun or not fun or happy or not happy.
And I think there's a wonderful post called, uh, Morality is Awesome, which starts out with the question,
this is a less wrong post, not by Eleizer, a different contributor, starts out with the question,
a wizard has turned you into a whale.
Is this awesome, yes or no?
And it extrapolates further from there, but at some point it gets to the question,
what does any of this have to do with morality?
And his answer is, morality often gets caught up in these things, these grand words like good and bad,
and really what we want to know is whether something is awesome or not.
So we should just use the term awesome so we don't get sidetracked by all these ethical terms.
I use the word well-being when I'm talking morality with people, because they get hung up on,
well, good just means this or whatever.
And happiness is terribly hard to measure.
Exactly.
But well-being isn't as hard to measure.
Well, it more or less I think means exactly what our intuitions think it means, right?
And I don't think your intuition of well-being is going to radically differ from mine.
Do you get to do some of the things you want to do, like have water and food and live and shelter
and not be horribly diseased all the time?
I think those all fall under, you know, like, yeah, exactly.
And there are things, I agree, and there are things that fall obviously outside of well-being,
like being in constant agony, you know, feeling terrible emotionally.
Those things would not fall under the umbrella of well-being.
There's a hard to translate Greek word called eudaimonia, which applies to more than just well-being,
but it's also somewhat applicable.
It's just about living the best kind of life, the kind of life that if you could pick,
you would pick for yourself and for those you love.
So I'm sure the answer is yes, probably.
There's lots of utilitarianisms focused on other things.
And if not, make one.
Or at least consequentialist idealists.
Yeah.
Shall we go on?
Yeah.
To the cryo episode?
I guess that's all of it.
Yes, let's go on to the cryo episode.
Well, Adam had this thing about some of the point about Hedonophenyls.
Oh, he did, right, yeah.
They did, yeah.
They did, sorry.
Yes.
We don't, we do not know the gender of any of these commenters.
Yeah.
With the possible exception of Mr. Olavla.
Probably a Mr.
I, my money's on Mr.
Okay.
We'll just go with that.
Also, my husband is, he prefers he pronouns.
Adam says, since increasing wages in particular and changes in circumstance in general
do a poor job of actually increasing happiness, due to the adonic treadmill we were talking about,
they think the world is more safely improved by reducing suffering than by increasing happiness.
Because it seems like policies intended to increase happiness will cost a lot and tend
to have decreasing success, especially since other people do not necessarily agree on what
increases happiness.
Whereas reducing suffering doesn't involve the same sort of arms race against human expectations.
I think that's a fairly sound point.
I think that is too.
I think it'd be easier to make things a little better for the worst off than it would be to
make things better for the, for the well off.
Generally everyone can, can agree that being in constant pain is bad.
And if you can reduce that, then you're doing a good thing.
But there are things like inventing the internet, improving communications, that makes things
better for everyone and it decreases suffering, increases pleasure and decreases suffering.
Yes.
I don't know if it's so easy to measure the cost of increasing happiness and, and pleasure
and all that, whatever positive stuff versus decreasing the negative stuff.
I'm, I'm trying to think of an example.
It could be that like raising the minimum wage in the United States to $15 an hour might
do less to impact the, you know, average hedonic index of the world than say curing malaria.
Yeah.
So, so, so that, that, that I think it could be, you know, I guess even if it was affecting
the same number of people, if we wanted to find something that balanced up between those
two, that the people who are making a little more an hour are only marginally better off.
But, but it's easier.
Specifically with malaria.
Well, I mean, so like whatever it is, all the things that we take so for granted, you
