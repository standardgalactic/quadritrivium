As a content warning, this episode contains talk of suicide and the destruction of the human race.
Use your best judgment as to whether you should listen to this episode or not. Thank you.
All right, Steve, bring us in.
Hi, welcome to the base of conspiracy. I am Steven Zuber.
I'm Inyash Brotsky.
And I'm Jason, her writer.
And Jason is sitting in for Katrina today because Katrina is out on vacation.
Traveling abroad.
In Australia, no less.
As far from us as you could possibly get.
Ball still on her.
Right.
It's like 36 hours away by plane.
Damn.
Really?
Yeah.
Wow.
So we have Jason on today to talk about voluntary human extinction or not necessarily
voluntarily human extinction, but just whether humans should be around in general.
Are humans the worst thing ever or just almost the worst thing ever?
I guess would be the question we have to pose to Jason.
The first time we met Jason, he mentioned a thought experiment that I thought couldn't
go undiscussed. So it's been a long time coming, but I wanted to have it on the show.
Actually, I'll let you go ahead and take it from here.
I don't remember what thought experiment you were talking about.
Killing everybody?
Under what?
I mean, there's a lot of ways that you can get to killing everybody.
Pressing a button to end all human life?
Oh, yeah. So are there other ones that you might be that you could get behind her?
I mean, I don't know that I could necessarily get her mine.
You have a lot of utilitarian arguments end in or can end in either something awful like a
utility monster or killing everybody. Or as I prefer, I think my ultimate goal would be Brave New World.
Um, which when I read Brave New World, I didn't get all the way to the end because I just kind
of got disgusted with it. I was like, how is this not a wonderful ideal society?
Everybody's happy. There's like, there's drugs. There's free drugs for everybody.
Everyone is born knowing what their purpose in life is and that they are the best person to do
this thing. There's no like stumbling around trying to figure out what you're supposed to do
or how you can contribute to society. It's just all they're laid out for you.
Except, uh, well, except the one guy.
Yeah, he, there was an accident when he was born.
There was, there was one liberal arts major who, who decided that the world was not,
world was not as good as it could have been. You needed more pain and suffering.
It's obviously shitty for him because, you know, he was, you're,
you're optimized for one specific role and then you're the rust and something else.
No, but it would be like if the David Bowie person got stuck in a job for like, uh,
engineer types, accounting or programming or something. And that was what he was forced to do
for his whole life. Yeah, he'd be miserable. And it really sucks for him, but I didn't see how
this book was supposed to be an argument for the society is shitty. Like one guy got fucked due
to a genetic accident. It's society itself is still pretty good.
Everybody, everybody was happy. Everybody happy. And I think that's kind of,
that's, that's where a lot of these arguments end up with, right? You have most utilitarian
arguments kind of take a summation, right? And obviously we're generalizing. Most utilitarian
arguments take a, take a summation of total happiness and well-being of humanity. And then
they, they, they want to overall increase that, you know, that, that total sum. My take on it
is a little bit different. First off, I, I include animals. A lot of utilitarians include animals too.
Yeah. And because we're trying to do a lot of, of explaining like I'm five, I think we've covered
exactly what utilitarianism is. It's an ethical system that says the right thing to do is that
which increases the happiness for the greatest number of possible participants, depending on
what kind of utilitarian you are. I think going all the way back basically included animals.
I don't know if it did it. Jeremy Bedtham's utilitarianism. I don't know. I know at least
it's in John Stuart Mill. Because that would be on the way back. So the idea is that, okay, well,
should I donate to this charity or this charity? Or should I, I mean, it could be even more
broad than that, right? Do I stay at home all day and watch TV or do I go to work so I can pay bills
and give some to charity or something? I think at its most basic, it boils down to
what you could put it is, is it better to give one person $5 or to give 10 people 75 cents?
And, you know, you add up 10 people times 75 cents is more than $5. So even though they are not
going to be as happy as the one guy who got $5, total happiness is increased more if you go with
the 75 cent round. And that's basically what utilitarianism is. It's, you know, it deals in
terms of utils, which are these imaginary measurement units for measuring how happy someone is points.
Yeah, points basically. And there's no way to actually measure them. But in theory, it's, you
know, if something would make one person unhappy, but the other and someone else more happy than
that unhappiness, then the net net utility is positive. And it takes basic axioms that most
people agree with like happiness is good, unhappiness is bad, people want to be happy,
they don't want to be sad. Yeah. And the the sound bite from Jeremy Bentham is the greatest
happiness or the greatest number. And then you describe the difference between total aggregate
and average. So that I think was in the last century where people broke down. Do you take
the total number of utils or happiness of everybody on earth and add them together and say,
okay, cool, it's at seven billion, or I guess that assume everyone had one. So whatever it's at
seven trillion, the higher the better, or the average utils are at like three billion.
So depending on how you approach it depends on how you might answer some various questions on
what you should do. All right, we should we should actually cover that as the the pouring
conclusion problem. But before we want to before we get to there, I did want to point out that
conversely, you know, it sounds like it makes perfect sense when I put in terms of cents and
dollars per person. But conversely, there's the the now famous fat pushing a fat man in front of
a trolley problem, where a trolley is on a track to hit five people. And do you push a fat man in
front of the trolley, which would kill him, but would stop the trolley before hits the five people.
And generally, the utilitarian answer, at least the classical originally utilitarian answer was
yes, because one person is dying, and that is less bad than five people dying. And most people
have some severe issues with that particular conclusion. And there is lots of discussion
about why that may not be, in fact, the optimal thing to do. This this is the typically the
second question asked in the chain of questions that when they put people to the experiment,
either under fmri or however, the first question is much less controversial, you're standing by
the switch on a railroad. And for some reason, down one way, the way that the train is currently
going, you see five people change to the train, change to the train tracks. And then if you pull
the switch, the train will diverge and go down another track where there's one person tried
tied to the train tracks. Most people will say, yeah, I'll pull the switch, five is better than one.
When it comes to pushing someone in front of the train, or trolley because one person couldn't
stop a train, but trolley is is more believable. People have a gut instinct and reaction against
taking active hand and murdering somebody as opposed to a passive one. So I think it's something
like 95. It's some overwhelming majority of people will pull the switch and move the train from one
to the other. But it's radically less, less than a quarter, I think, if people will will push the
person. And mainly, I just bring that up to to demonstrate that it isn't always naturally intuitive
that utilitarianism is the best way to do things. But I think 90% of the time, most people lean
utilitarian. I don't think they do. I think people who think about it may, but deontology also has
some strong arguments to it. And there's there's a lot of different ethical systems out there.
We should we should point out that in the in the lesser on community, right, you've got,
I think on the on the show notes for your last show, you you gave kind of a breakdown of different
different ethical different schools of ethics at the lesser on community believes in and it's
it's mostly consequentialist, which it's I think it's fair to say that most utilitarians are
consequentialists. And then it's got a very small percentage of deontologists. The thing with
deontology that always bugged me was that constant pick his rules arbitrarily, and then say, go to
you know, you have to do those things, he chose consequentialist arguments, and then say, you
have to do this all the time. So it starts the consequentialist reasoning and then just has no
rule breaking. But yeah, we can do our own long episode on ethics, if you want. Right. We are not
here about ethics. We're here about killing all the humans, which has nothing to do with ethical
theory or doesn't we do we just we just we just push somebody into a train. So just say you five
more though. Yeah, would you push everyone in front of a train to save nobody? Seems like a true
question, Brian. Well, what if those even what if those Christ, why did I just call you Brian?
Did I do that before? What if what if those five people were were in severe pain?
And it would actually make their lives better because they would no longer have negative
utility. Right. So the reason a lot of people choose to, you know, commit suicide if they're
ready, terminally ill, and, and in a lot of pain, they're like, the rest of my life is just going
to be pain. I should probably stop it now. The important caveat, I think, between the word suicide
and euthanasia is that if you're just suicidal, my life's pain. And that's how you're you're feeling
at the time. And the things can get better. But that's just kind of how your mental state is.
But that's not necessarily the case. You could be, you know, treated and felt better,
whether or not you're sick or have a or whether or not I guess you're have a deteriorating illness
or a mental illness. If you if you have a prospect of getting better, I become less and less in favor
of euthanasia slash suicide. That's a difficult call to make, though. I mean, I am with you because
I am a very optimistic, hopeful person. And I think yeah, there's always hope. But it is a difficult
call to make because you can't a lot of you know, a lot of mental disorders don't map well, you can't
you can't see, you know, what they're going with the prognosis is. And so for someone to say,
you're definitely going to get better is not necessarily true. This could be a case where
someone is just going to be in pain forever. And and maybe it would be better for them to kill
themselves. It's why I'm we got to put trigger notes for suicide on this episode, because I hate
saying that. And I, I don't think it's true in general. But I have to admit that in theory,
it is possible that there are people who will have negative net utility throughout that, you
know, from now to the rest of their lives. But at that point, it's like a guessing game, right?
Yes, exactly. So you're saying no, there is a chance if you're if you have a say a major
but depressive disorder, that no therapy will have any improvement for you. Right. Anything from,
you know, the smallest dose of Simbalta to straight up to lobotomy, nothing's going to
make you any better. So that there might be a human on earth that that's true for. But that's
different than saying like, you're dying of cancer, you've got six weeks to live,
we've exhausted all the options, it's just going to get worse, and you're going to die.
Well, at that point, it's kind of like, well, can I check myself out now? I say yes.
Oh, I totally agree. But I, the precautionary principle generally says that if you aren't sure,
you err on the side of not killing yourself. Agreed. But you know, I don't think it's
necessarily a good idea to make that a law and deny the people that option. Because then you're
just saying, I know you better than you do, which maybe is the case, but isn't always like, I know
you're going to get better. And my judgment overrules your judgment, even though you are the person
who generally has the best insight into your own brain. And in that case, I kind of think the
precautionary principle almost says, don't interfere with other people's decisions.
I was, I wouldn't say that I know better than someone does. But I would say that I think there's
a good chance. And until you've tried everything, it's probably worth it, because you just don't
know what you're missing. You know, once you get through a depressive disorder, things become much
more enjoyable. And then when you're in the midst of it, the idea of like, why would I want to
keep doing this just seems like a natural thought. But then you don't realize that that's part of
what being depressed is. And I want to say, I'm not encouraging suicide in any way, because when
I was in my late teens and early 20s, I was extremely suicidal. And I, you know, I almost,
I came very close to death a couple of times. And so I'm about to agree with a lot of things
you're going to say, because I felt all those things too. So, but now I'm much happier with life.
And on the whole, I'm extremely glad that I stuck it out, because things have gotten
amazing. And I'm extremely happy. And what exactly is your socioeconomic status?
I currently make above the median income for our country. I make slightly above the median income
for the country, America. And in your white, what people would identify as male, at least?
Yes, I am a white male. I am extremely privileged. I'm basically straight. I am cis. I look like a
normal person. I don't have any major disabilities. So if I had to take a guess as to whether or not
you were going to be happy and know nothing else about you, other than that, I'd probably say
that you're, you're going to be happy. You're at least going to give yourself a seven, which is
kind of the number that the like world happiness values like the that the UN does in, you know,
Protestant countries and higher income countries. Are you basing that number off of
statistics you read? I'm pretty sure that that depressive disorder is less common
among developing countries and among a lot of poor populations that it's a lot.
Once you have like the affluence of luxury to be depressed, then that's when you get depressed.
Yeah, you might have more depression, but as far as people just rating their happiness,
as in on a zero to 10, which is obviously, you know, not fallible, you generally range between
four and like up to up to eight in some like Sweden and Finland, whereas places like Jordan and
Greece, Greece isn't pretty good right now either. Even considering that those places have a lot of
of high rates of depression. That's kind of interesting. I wonder, I guess I'll have to
look into it more if I can find a link. Yeah, we can put on the it's a UN, the UN puts out a
report every couple of years. Okay, I would like to see other questions. We're gonna totally apply
you for links after this, because that's what we like to do with our show. Sure. As long as we're,
you know, still starting out a little bit, do you want to talk about it yourself at all or
or how you came to be in the recording studio? So I get how I how I came to push the button.
Oh, that's a good title right there. That is a good title.
I uh, or how I came to the room or a little just about you if you want to talk about it.
I entered, I entered, I entered a code into the front to the front door and then I followed
the elevator up to the to the east hallway and then. I believe technically Steve entered the code.
Yeah, no, start from birth. Yeah, there I was in my mom's uterus. Just kind of chilling.
My rationalist history is I was baptized Catholic. When I was 14 I read the Bible and
didn't make sense. And I, you know, that seems to be a common common enough jumping off point
for a lot of rationalists. So I jumped into existentialism and nihilism and throughout
like high school. And then through enough willpower, I eventually settled on some some objective
realities. In this case, you know, the existentialist and the nihilist took hold. And that's where I
get my utilitarianism is that it's the problem that that most mainstream utilitarianism has is
that it values positivity too much. So you have these utility monsters, which the concept there is,
you know, you said, you know, let's say we give everybody or let's break it down into money,
right? Let's say we want we want to raise the the happiness of everybody. So we split the money
equally, right? The problem is that that doesn't make everybody happy because some people like
money a lot more. So you should give your own all your money to them. Which means, you know,
that the spikes happen there. So you have, you have interview Ash, who really likes money.
And so it's on the salesman. So I guess I like money more. So I like money way more
than you guys do. So you guys should just be broke. And even though you'll be unhappy being,
you know, below the poverty line or in this case, I've taken all your money because I love money
so much. If we're if we're taking the aggregate or the average, it's super high because I have
all the money and money makes me really happy. So the problem is the problem with a lot of
utilitarian arguments is that they end like that. I even if we have negative 100 utility each,
you've got like 10,000 utility. So yeah, both aggregate and average is much higher than it
would be if the money was split evenly. I think one of the the best criticisms of
utilitarianism that can sort of stand is that consider individuals to considers the broad
scope. And so in many aspects, it can be considered to fail because it doesn't consider
individual people. And so like you said, the differences between people is that the value
thing is differently. And that treating everyone the same and doing it like that doesn't just
doesn't work. Well, and in our aggregates, like is the problem I came to was, is happiness equal
to unhappiness, right? Is pain and suffering? Does it have an equivalent to happiness? And
I'd like to say so just because when you're when you're happy, it means you're not unhappy. But
it's not a one to one ratio. And so I split the two. And I say, instead of just having an
overall aggregate, where you can have these utility monsters, we should put more weight on
the pain and suffering. Because I was trying to come up with my objective reality, the only thing
I could really find that was universal is that pain and suffering is bad. Right? So did you
find the corollary true that happiness and pleasure is good? It's good. But as not as suffering is
bad. Correct. Gotcha. Yeah. Do you have a breakdown? Like, I think like you said, it's like one to
one. Do you have like a three to one? Or do you just kind of say it's just worse, like to some
extent? Nope, it's just weighted, just more weighted on the negative side. Fair enough. So if we take,
you know, if we take that to a to a large scale, we end up, we end up pushing the button on a smaller
scale. The goal should be to, to decrease pain and suffering, right, to make the world suck less.
But the reason that you had me on today is because when you take that to a large enough scale,
and you have the option to push the button, my moral philosophy would say that you should
definitely take, you should definitely push the button because then you're going to guarantee
that there is no more pain and suffering. Well, like what if I just, if I just came up to you and
without any kind of context said, I have a button that can, that can get rid of all pain and suffering
in the world. I can now see why you like sales. We don't just have you on because, because you
push the button, by the way, this button that we're talking about kills everybody or all life
on earth. That's right. That's, that's the trick. It has to destroy all life on earth. I think a lot
of the, the, the, the, the, the, the, the, the acronym stands for voluntary human extinction
movement. Uh, their, their big deal is that we should, the human race should, you know,
voluntarily get rid of itself. And that does not go far enough for me because I think in a, in a
relatively, at least in a universally relatively short enough amount of time, something else is
going to evolve and it's going to suffer just as much, if not worse. And on the way there, it's
definitely going to go through something like we went through, which is 200,000 years of, of, of
shit. Yeah. I wanted to direct really quick that we're not just having you on because you press
that button. We're also having on because you're a cool dude. So yeah. There we go. Yeah. Well,
there's, there's a bunch of, you know, cool ideas that I want to process, like, uh, why happiness
is less weighted than, or, or, you know, why suffering is weighted higher than happiness.
But, uh, before we get into any of that, the very last thing you said that you would want to wipe
out all life because eventually something would evolve up to human levels again and have the
same amount of suffering that we have, which first of all was also the conclusion I came to
when I was in your position. Uh, but it brings up the interesting question. Humans feel a
uniquely high amount of suffering then in your opinion because of our, uh, ability to self-reflect
and be conscious. Yeah. Yeah. You have creatures that will don't respond to any kind of stimuli,
right? Um, you have higher level creatures that'll, you know, we're saying higher level just
for the, because we're, we're, we're eco-centric, uh, closer to us. I am more than happy to call
that higher level. Uh, you have higher level things that do respond to stimuli and then you
have things that have physical pain and then you get to us that has mental pain and then, um,
I, I would have a sinking fear that if we, if there doesn't already exist, there would exist
to the future a some kind of pain that is worse than a mental pain that maybe we just can't perceive
or that, because we're not smart enough. Once we, because we think out how pain comes along, right,
it's, um, it's moving us to not die and spread our genes, right? And then once we figured out how
to stop being in pain, um, something else had to come along to, to, to make us not die and spread
our genes. And that was mental anguish, right? So you have, you have, you have societies that
come together and you want, um, you want the society to exceed. And so then you have mental
anguish. Um, so I wouldn't, I wouldn't be surprised and I wouldn't give this a high probability,
but I wouldn't be surprised if there was a, if there wasn't some kind of greater pain that was
past that, maybe like an empathic pain that, I mean, we could, we could certainly end towards,
right? I suppose if it's, if it is just outside of our conception, then I, then I'll agree, but
it seems like between physical pain and mental pain, we've covered the two kinds of things that
there are, right? So either something's going on in just your, you know, in your mind or in your
body and it's affecting your mind, but other than that, depository third kind of pain might be to
suggest some third kind of thing. Well, the, the ant can't fathom mental pain. Right. So your
position is that pain comes in, pain is inevitable, uh, as you get smarter, the more conscious you are,
the more pain there will be. And there's no way around this. There's no way a life form as advanced
as us could evolve without having that sort of pain. Um, no way around it. I mean, we could,
we could certainly work to work towards it. I mean, that's what, that's what ideally what I'd
like to see. In fact, an empathic pain might make us better people. Um, but I could, I could
certainly see in the future ways that we can get around both physical and mental pain. Um, but as
it is, and, um, the way that evolution seems to produce creatures, at least on the earth, um,
pain is a driving force for, uh, for the, for the changing of the species.
I just last week stumbled across an article about aversion learning AIs, which, um, you know,
artificial systems, which are goal driven. And the way they learn is when they fail to meet their
goals, they're given a disincentive. They, you know, it's not pain in the technical sense, but
it is a situation they want to avoid. And apparently that really helps with learning.
And I was like, there is absolutely no way that aversion driven learning can go wrong.
Well, we should totally pursue this. It also doesn't work in child rearing as it turns out.
No, no. So even when, when rearing humans or dogs, like it's better to reward good behavior.
Hold on. It seems to work in image recognition though. That's a should. But, but I think,
I think a thumbs up would work. I guess, you know, we're talking about alien minds. This is who knows.
But I, I feel like, especially if we're going to try and scale this all the way up to super
intelligence, let's choose one that we made happy and not made sad on its way to Godhood.
Yeah. No sadness and pain seems to be very powerful motivators.
Could we, could we, could we create a program that did not, you know, that, that was not
advanced by pain and suffering? Yeah. I think we could do that. But then we get into like
transhumanism and then we have to, and then we're, then we're destroying all, all humans anyway.
Oh, just in a different way. Oh, okay. So you wouldn't be against destroying all humans by
evolving into a transhumanist or not evolving, but developing into a post humanity.
No, no, I wouldn't be, wouldn't be opposed to that. Okay. That's interesting. Because I thought that you
were, that like the kill all humans was your ideal solution or kill all things. No, it's not the
ideal solution. It's a, it's a, it's a hypothetical. It's if I had the button that like plummets the
earth into the sun, would I push it? Yeah. But now until then I would like to find more productive
ways that don't end up killing everything. Oh, no, I should totally talk about that too,
because you're one of the few actual effective altruists that I know in real life. But by saying
that if you had that button right now, you would plunge the earth into the sun. Isn't that also
implicitly saying, saying that you see that there is an extremely small chance that we'll
ever get there. And it's overwhelmingly more likely that we'll be suffering forever.
Because you're basically killing all future inhabitants of the earth as well when you push
that button. Right. Right. And I would, if I had to put a probability on, I would say that the
that things are going to get better. I mean, currently they're, they're getting better for
humans. And we're, we've got more and more cows that are suffering and stuff like that. But
I think in general, the everything will get better. But there will also be more people.
So when we're adding up all the, remember that we're separating the columns here,
when we're adding up all pain and suffering, we might have an overwhelming amount of
happiness. But just because we're going to be having a lot more people as well.
I mean, if I could, if I could eliminate that column is what I want to do. Right. If like,
if I can make sure that nobody would have to suffer ever again, at least in as far as earth
goes, I would certainly do that. Right. So you've read those who walk away from
Emmaus then? No. Oh, okay. Well, it's, it's a very short story about an ideal town of,
I don't know, a few thousand people. And it is basically utopia. Everyone is entirely happy.
Life is fulfilling and wonderful, except in the basement of one house, there is a child that
lives in abject horror and pain and suffering. And it does that for, you know, its entire life.
And there always has to be one of that. And the, the story is basically about there are some people
who, when they find that out, leave the village and never come back. Because regardless of how
much total happiness there is in the village, you cannot make up for that one child's afford
the suffering. Right. So are you saying that if the entire planet was really happy and utopic
and very fulfilled with their entire lives, and then there was like one old dude who was kind of
curmudgeonly and grumpy and said, you know, TV really sucks. I cannot watch anything that's good.
And my leg hurts. And I am just life is awful. And I wish I was dead. You're willing to kill the
entire human race. So that amount of suffering doesn't exist. Can I just get rid of him? No,
I can't get rid of him. No, then I have to have, then I have to have a coefficient. Then I have to,
I have to have a, for that, I have to know how much pain there is, how much suffering there is,
and how equal they are, right? How, rather, how they, how, how correlated they are. So there is
a level of suffering you're willing to accept if there's enough happiness to balance it out.
That's a rough one. Yeah. I don't, I, I, at that point, if it was the entire world and then
one curmudgeonly old man, I would, I would, I would hesitate to push the button.
Okay. Once a curmudgeonly old man and three really sick cancerous children.
Yeah. Yeah. Then the button gets pushed.
Oh, shit.
Push that very far.
But the, I mean, we, we, we don't have that. Just like we don't have the button,
right? Just, just, I mean, I think a lot of the, a nuke certainly wouldn't do that.
It would have to be something that would destroy the earth or the sun in order to do that. I mean,
if the, the, the, the, whenever we take things to extremes are, they're difficult. That's how
we get, that's how we get to the button, right? Because we take utilitarianism, we say, what happens
when you make the extremes, right? And that's what we do. I mean, that's, that's a good way to,
that's a good tool in general, right? And so in physics, when you're testing things out,
you put big numbers in, you put small numbers in, you say, does this work in both these? Yeah.
Okay. I believe it's the correspondence principle in quantum mechanics. You, you, you say, okay,
when we put these, when we put the, when we put the numbers into quantum mechanical equations,
do they turn into classical mechanics, right? Because if they don't, then they can't be true.
Right. So, so taking, taking, taking things to the extreme is a really good tool, just in general,
in general. Going to hyperbole is one of my favorite intuition pumps, just in general.
Sort of like slippery sloping, someone's argument or straw manning it. If done responsibly,
it can be a good intuition pump. Yeah. Okay. Well, if we turned this up to 11,
would things still make sense? And the answer is no, that's a good indication that they wouldn't
make sense at whatever level someone's proposing it at. You, you are definitely more extreme than,
than I was or that I thought you were, because originally I had thought when, when I was in
the same, yes, I would like to destroy all humans face, I was of the opinion that it was impossible
for there to be more happiness than there is suffering. And at the time I hadn't read this,
because I don't think it had actually been written at the time, but there's a great post over at
the view from hell, called the transdimensional justice monster, which it posits a godlike
being that can alter where utility is to make, to make the universe as, as net positive as possible,
that, you know, there, there may be more suffering in the past, but at least a greater
happiness in the future due to gains from productivity or, you know, like the industrial
revolution, it was kind of shitty to be a factory worker, like really fucking shitty,
you died from black lung, you got rickets, you got to work 12 hours a day, seven days a week,
so on and so forth. But it led to so much economic growth that the great, great grandchildren of
those workers aren't in a much better position. Now they, you know, can sit in their cushier
conditioned office and work, you know, 12, 15 hours a week and surf the net for the rest of the
time. But, um, there's, there's probably more happiness than unhappiness. Okay, I think it's
gotten better. And I think that it's gotten the transit justice, that transnational justice
monster post was basically arguing that when you even everything out that, you know, we had to take
our happiness by making the past more miserable, that we take our happiness in the developed world
by stealing resources and happiness from the undeveloped world. And no matter how you try to
balance things, no matter how you shake it out, net utility across the entire species across
all time is always negative, that you cannot get more happiness than pain and as a total. And so
the best solution is always to wipe out all life because net utility is always negative. And that,
that is like what I consider the strong form of the argument. But the fact that you're saying that
even a tiny amount of suffering overcomes all happiness. That's like really strange. Why do
you value suffering so much more than happiness? Because it's the only thing that I could find
that was intuitive. I mean, intuitive is the wrong word. When I, because it used to be,
I think it's the only thing that we can agree on. Right? I know I mean everybody. You were going to
find people who, you know, think that pain and suffering is, is the way and then it's, it's
valuable. And I'd argue that it might be necessary in some circumstances. But I think it's the only
thing I can, that we can universally agree on, at least as a human species, which is what defines
ethics. That is bad. I was going, I was going to make an argument that then made me sound like
someone could just change a couple words and make it sound like a defense of the problem or a defense
of evil, like, you know, in the problem of evil with religion. But then I just realized that we
can just look at it a little differently and say, so one of the comebacks to the problem of evil
in religion, which is the problem of if you take the Christian goddess traditionally conceived,
how do you justify the existence of the amount of evil that there is in the world? If God's
supposed to be all powerful and all knowing, why doesn't he go out there and fix things or at least
not think, let things, let bad things happen. Right. Yeah. So one of the comebacks is while
suffering is good because it, you know, contrasts with bad or excuse me, it contrasts suffering
contrasts with pleasure and lets you appreciate pleasure more. And I was going to say something
to that extent, but then I realized that maybe sound like a evil apologist. So to bring it home,
apologist for evil, the rejoinder to that then is why is there so much suffering? Couldn't
every human learn how great it is to not be in pain just by stopping their toe once in their life
rather than like getting their arm blown off and then having phantom limb pain for their
for the remaining 60 years. So that's, that's the classic rejoinder to the comeback of while
suffering is good because it makes people appreciate appreciate not suffering more.
So have you seen Scott Alexander's reply to that? I haven't. It's the most brilliant thing in the
world. Let's hear it. Okay. And I find it really ironic that an atheist had to come up with the
only good rebuttal to this, but it takes place in the in the form of a conversation between Job
and God because that's kind of guy Scott Alexander is. And if you're familiar with the Bible story,
Job basically asks God, why is there suffering in the world? Why do good people suffer? And God
says, Hey, did you make the world cause the sun to rise? No, shut the fuck up. And it was basically
in the Bible, it was basically a bet with Satan, too. Yes, yes. And it was like, I'll bet this guy
also love me if I fuck with him, bet bets on bro. And then he keeps fucking with him more and more.
And Joe stills devoted. So but the core of the story is why do bad things happen to good people?
And the answer is because I'm, you know, fuck you because we have the capacity to feel bad.
Yeah. But the in the in the rewritten conversation between Job and God that Scott Alexander wrote,
Job asks God, why can't you just make a world where everyone is happy and there is no pain?
And God says, I did that. Job's like, No, there's a lot of pain. And God's like, No, no, no, not
this world. There's a different world where there's all happiness, no pain, everything's great. It's
like, Oh, well, why did you make another world? It's like, Well, I thought, you know, there's
this is a lot of people, but I could make an entire world where there's tons of happiness.
And just like someone stubs their toe once in all of human history and think of how much more
happiness that is for just a tiny bit of pain. Totally worth it. So I did that. I made so much
more happiness. I was like, Well, could I make a world where just two people stubbed their toes?
And so on and so forth. And basically, it takes the quantum multi world hypothesis
that every world that can exist does exist, except it takes it that every world where there is more
net positive utility than net negative utility exists. And God's like, you know, I'm sorry,
you happen to be in one of the worlds where it's kind of right on the edge. And there's only a
little bit more happiness and sadness. But, you know, take it to heart, feel happy inside,
because in the human race, the species across all time, there will be more happiness than
sadness overall. And so therefore it's better for this world to have existed than not.
That does ring a bell. And I remember when you mentioned the many, many universes,
I thought that was funny. Yeah. So, but then I guess to bring that analogy home,
which is where I was trying to bring it. So say if we could make everyone on earth as happy as
Eniosh. Yeah, yeah. That may not be happy enough. Well, but is that happy enough? You admit yourself
fairly happy most of the time. Really happy most of the time. Yes. So if we could ever
give everybody put everybody at the top of the hedonic treadmill? Well, I mean, they can't,
they can't all fit at the top. But like, I mean, just to raise raise how. Oh, yeah, right.
Depending on politics, of course they can. Like Stephen Colbert, I won't rest till everyone's
in the upper 1%. So, but now I guess just just raise so that the, the, to make Eniosh the saddest
person on earth, right? So would that be happy enough? Would what? Would you still push the
button in that situation? So like, with someone with a fairly, with a fairly high level of profess
happiness, if that's the saddest person on earth, and the animal equivalency jumps up as well,
would that earth, would you still condemn it to be pushed into the sun, if you could?
That'd be pretty good. I'd be, I'd be pretty happy with that earth. So that sounds like a more
attainable bar, I think, than I mean, am I, am I guaranteed that it's going to remain like that
forever? No, but I mean, let's, let's, let's say the odds are nine and 10 that will remain like
that till the end of the universe. Yeah. Okay. So, sorry. So, so this is where you get at,
right? So you have, you've got a decision theory where you multiply the, you take your, you take
your, your utility and you multiply it times the percentage that it'll happen, right? I mean, this
is kind of in standard game theory. And so I think the way I think what my, what I think what I'm
doing when I'm saying I'm pushing the button is I'm taking that unhappiness, that pain and suffering,
and I'm taking the small percentage that it'll continue or that it exists at all anyway, and
I'm multiplying that times the negative utility and that negative utility, like pain and suffering,
like, imagine just the worst thing you can, like just take a person, just, yeah, just an entirety
of torture in a dark room, something like that, like it gets close, it approaches negative infinity.
And so no matter what you multiply times, I mean, that's going to be like for me, that, that, that,
I can't not push the button because I have, I have the possibility that's going to happen.
Now, well, yeah, right, because you have, I mean, like if I could prevent 50 years of torture,
like, I don't know, I don't, I just can't fathom not pushing the button in that situation, no
matter what else it entailed. This isn't, and I'm just really quick, this isn't an endorsement of,
of any suicidal propositions, but there's a nonzero chance that someone is going to pick
up on your drive home today and torture you for the rest of your life. What's to make you decide
to not take your own pill before you leave the room, right? I'm not trying to talk you into it,
for the love of God. So how do I, cognitive, cognitive dissonance is how I do that.
But you would, you would condemn humanity to non-existence on the small, I don't condemn them,
I bet that, well, you would reward them with non-existence at the small chance that everything
could get really bad and say that way versus not extrapolating that to like a case-by-case basis.
Yeah, okay. So just, just cognitive dissonance? Yeah, like if I, if I want to continue, I have to
pretend that doesn't exist for you, I mean, fair enough. I don't, I also don't have any free will,
so like I, that's just how I continue living. Right, and I, I, I maybe shouldn't have said that,
because I, I guess I was just, this is a dark episode. It kind of is. It's a dark episode.
Maybe it won't make it out, it probably will. Trigger warnings on it. Right, definitely. Yeah.
I wasn't trying to be a dick. I, I'm just, I'm trying to better understand it. So like,
you're not trying to be a dick, you're just trying to say, why don't you, why don't you kill yourself,
Steven? I, that's not where I was going. So one, one of the reasons I don't kill myself is because
I assign a very, very low probability to the, to the chance that I'll live the rest of my life
in abject horror. But if you're saying that on the chance that, that humanity as a whole or life
on earth as a whole could, there's a small chance that they can live abject horror forever,
that, that would justify ceasing its existence. Well, this is a closed system too, right? So like,
I'm just confining it to me. If I were to kill myself, I would create a lot of pain and suffering
in the world. Oh, oh yeah. So then, so it's predicated on, you wouldn't press a button that
would kill half of everybody. Right. Yeah. It would, I have to get every, every form of life
and everything that could possibly, like I have to even get, I have to get like amino acids.
Right. I have to destroy the ocean. You got to, you got to supplement the,
you would have to break out the whole universe then too, huh?
Or I can, or I can hope that everyone else will push the button too.
Is the amount of suffering that you would cause by killing yourself today
greater than the amount of suffering multiplied by the probability of it happening,
of you being tortured for the rest of your life another 50 years?
Well, let's see if we take the, I mean, you can't multiply infinities, right?
I kind of worried how far we want to push this.
We push it all the way, Steven. We do not stop until we hit the wall.
So like, like, what?
Push it all the way so Jason doesn't leave this room, right?
Like, like, what if, what if, what if my killing myself made five other people just
extremely depressed for the rest of their life? Or what if I, you know, what if I killed myself
in such a traumatic manner or something that would give people PTSD or, you know, anytime they
I'm not saying that people love me so that, that much that they, that every time they heard my name,
they would just, you know, break down, but they're, they're, it's a closed system.
So that's why we have to, in the button, the button is the entire system.
You know, we're not, we're not taking other life forms into consideration,
anything that might exist on another planet or whatever.
But the button in is the, because it's, because it's everything, right?
It's all everything that can feel, can no longer feel, I can guarantee zero.
I'll, I'll take that. Yeah.
Fair enough. And yeah, so that, that sounds like a good counter-argument to anyone who,
I'm trying to.
I don't, I don't think your argument holds though that suffering is worth that much more than,
than happiness. Because even though you say everyone can agree suffering and pain are bad,
I mean, yes, sure. But pretty much everyone can also agree that happiness is good.
So just because everyone can agree that suffering is bad, doesn't mean that you
shouldn't count happiness. Pain feels worse. Does it?
I think pain feels worse than happiness. I would rather.
I think pain actually has the same hedonic treadmill problem that happiness does.
Because I've, I've been in, I mean, just, just in terms of physical pain,
God, I've gone through some surgeries before and the recovery was fucking horrendous.
I felt like Han Solo when he was being tortured in Star Wars, you know,
it just, it was, it was awful. I took some painkillers and it went away.
But before they kicked in several times a day for about a week, it was insane amounts of pain.
And now I don't like remember it at all. I mean, I vaguely remember that it happened,
but it doesn't register. It fades into the background and you quickly,
you quickly get over it. I hear that's the same reason because women have more than one child.
Because despite the intense agony of childbirth, eventually the pain just kind of recedes into
the background. You're like, okay, whatever. It's the same head treadmill.
Women have more than one children because, I mean, they've evolved to
forget the pain that they went through. Yeah, we've all evolved to forget the pain we've
gone through. Which means that, I mean, if it's, if it's such a, I mean, if happiness doesn't count
because we forget it, doesn't pain also not count because we forget it? But the fact that it has
to be forgotten for anyone to exist doesn't that kind of speak something to the magnitude of pain?
No, I think you should count the pain as it's happening, but it shouldn't
count forever there afterwards. But you don't remember it. I mean, and we're also
continued self here. Like the person who went through that pain, certainly,
I mean, they experienced that. It's a, I wouldn't consider the person who went through the pain,
the same person who is now happy and doesn't remember the pain. It's a very kind of selfish
thing. Then when you have a child, you're going to say, I want a child at the moment,
but you know, I'm not really, you're not getting consent from that future person.
Right. I don't see how that is an argument for saying that pain is so much more of a consideration
though, just because. Oh, no, that was a tangent. Oh, that was a tangent. Yeah. Yeah, I mean, I don't
really have a, I don't have a solid argument for it. I mean, these are very, and these are kind of
very loose. You know, when we get to the extreme hypotheticals, they're very loose ideas, because
we're never actually going to experience such hypotheticals unless anybody out there has the
button. Don't. If anyone out there has a button and presents to Jason, we are ethically obligated.
Stephen, it's not me to kill one or the other of you because I am not down with total genocide
of the human race. I think both just to be safe. But yeah, it's true. Yeah. Even like I would still
say that the percentage, the probability of there being, I think that there's more happiness.
Like it's tough to, like we can take surveys and stuff to involve any kind of animals, but I think
that there's more happiness. That's just a hunch and based on a couple of like world happiness
surveys. But even like, I think if there's a very small percentage, a very small chance that things
will get worse. And I'm fairly optimistic that at some point, pain and suffering will cease to exist.
But, you know, if I had it now, does that happen quick enough? Does that happen soon enough to get
to the point where we're all as happy as Indra Yash? Do we all get to do it?
I'm really uncomfortable being our baseline for a happy person.
I think you're the happiest person in the room.
Am I? Well, I'm sure I'm happier than Jason.
Apparently, because I am a conversation. But I am uncomfortable being called the happiest person
because despite the fact that I am happy often, I am not happy all the time by any means.
Let's backtrack a little bit and say that we were at the point where talking about as happy as you
can get, maybe like not theoretical maximum, but like happiest non-diagnosably or non-happiest
neurotypical person, right? So there's probably somebody out there with the opposite of depression
where their happiness is just, it wavers between nine and 10. They're just doing great. But that
stuff doesn't get diagnosed because you don't go to a shrink's office to complain about it.
So there's a manic pixie dream girl out there, you're saying?
What?
There's a manic pixie dream girl out there in the real world.
Probably? Okay.
What's that?
Oh, you aren't enough familiar with the manic pixie dream girl trope?
No.
Oh, okay. I'll have to link it to you then. It's basically every movie made by a middle-aged
white guy where a depressed, mopey kind of college-age white guy runs into, and you know,
like very straight-laced business, serious kind of type, but she's not happy with his life,
runs into this girl who's just free and one with nature and does whatever she wants. And she's
like, oh, let's go have crepes because I see crepes over there and just always manic happy,
pixie dream girl.
Gotcha. Okay. Now I can picture some examples from stuff I've seen. So yeah.
Yes, those people.
Okay.
So they're probably, I don't know, this is getting way too probably non-existent.
Probably.
We don't, well, they're not in the DSM, it's not a disorder because it doesn't affect your
life in any adverse way.
Right.
Exactly. So this whole giant tangent came about because you know, I didn't want to be
an happiest person in the room.
The happiest person in the room.
But so that's fine. Now I can't remember where I was going with that. It's been five minutes
that I derailed us. I'm sorry. How about this? How about we can work, can we work backwards?
We can say, how about let's do it if you are starting from the, the people who would push
the, push the button, right? In this extreme hypothetical, I would like, I would like to
point it like my, I know that there is a movement out there. I think this just came up in conversation,
right? Like that was, it was at my opening line was, hey, would you kill all of, all of life on
earth? If you, if you could, but like, these are, I don't even give them a lot of thought
because they're just so like that, that's not going to happen. I'd give it zero.
We can work backwards and say, well, what can we do to reduce pain and suffering even if you
don't, from the point of view of, of people who are going to value pain and suffering over
happiness as far as like a negative utility goes. How do they live their life? Right.
So I think something I would have in common with the, with the vehement movement would be
stop having as many babies, right? As many are a period. Well, vehement would say period,
but as many is a step up in their opinion. Okay. Well, we can get to, it's better than
having 20 babies, get to the point where resources balance out or at least resource
distribution balances out. So, but you see the Citadans kind of, you see this in places that
are, that are happy anyway, right? The higher the, the higher the income gets, the fewer
children people tend to have. So, I mean, don't, don't contribute to unwanted children in the world.
I take the, the stance of effective altruism, right? So we can do things that reduce the
amount of pain and suffering. And that's in the case of effective altruism, giving to effective
charities. We don't need as much money as we make, and we can give, we can comfortably give
chunks of that away to organizations that strictly pump that, turn that money into less pain and
suffering. We should quickly define effective altruism because I'm sure not every listener is
familiar with it. Well, first of all, the, the effective part generally means that you compare
charities and you give money to the one that does the most of it per dollar. Yeah. If you have,
if you have two choices, this is the, the consequentialist, right? Yes. Say the, the, the ends.
Yeah. If you could spend $10,000 to save one life or $10,000 to save three lives, you give to
the charity that can save three lives with the $10,000. Yeah. Or as some of the best charities go,
if you get to give $10,000 to save 35,000 lives, right? I don't think it's that much. How much
does malaria net cost? Well, I believe that the three to five dollars. Yeah. I believe that the
standard currently the, the standard estimate is saving a life cost between one and $2,000.
Okay. I was thinking back to the 1976 essay by Peter Singer, 1975 or six or seven or eight.
We don't, we don't need to, we don't need to worry about lives though. We're worried about
pain and suffering. Oh sure. So, so if we're deworming children, that's like 35 cents a pop.
If we are talking about mosquito nets, that's three to five dollars per mosquito net and that can
reduce a huge amount of pain and suffering plus it then allows more stable households
when your parents aren't dying of malaria or your siblings aren't dying of malnutrition.
They can go to school. They can make more money. They can, they can then increase the overall
happiness of themselves and their loved ones and all the people in their area for 35 cents a pop.
Right. And the, yes, I was thinking it was called famine, affluence, and morality. I couldn't get
the title of his, one of his newer books called the life you can save out of my head and I knew
that wasn't it. So we can link to famine, affluence and our morality on the website if you want to.
It talked me into becoming a charitable person when I was 18. I read it the same week I read
Animal Liberia or all lives are equal. Or was that it? All animals are equal. And that week I
became a vegetarian for like five years. So it was, I found it very persuasive. Not everyone
responds to argument the same way when it comes to these sorts of things, but I think effect
vultureism is first of all a much happier topic for the second half of the episode. And second of
all, it's one of those things that is so, I think, obviously clear for anybody who agrees that charity
at all is an okay thing or is a desirable thing. But there's still a lot of kickback to it, which is
weird. People get, I think indignant. I remember seeing someone complaining that the effect of
vultureism was taking money away from arts like opera and sculpture in the cities.
And I would come right back to that person and say, yeah, it's giving it right into the lives
people who need life saving. I think the more interesting part of the family, all that is
interesting too, but I think perhaps the most interesting thing about effect of vultureism
is the earning to give mentality, where it is considered a very high value, a very ethically
good thing for a person to do to get the job that pays them the most that they can possibly
swing with their skills, and then donate a large portion of that directly as money to
effective charities. As long as your job does not make the world worse, make the world a lot worse,
which is can be difficult when pain and suffering pays, right? Like as a salesman, I want to find
products that don't cause more suffering than offsets, or offset by the income I'm going to
make by selling those things. Yeah, I don't remember which way the equation came down,
but I do remember someone looked into whether becoming the CEO of a tobacco company and donating
99% of your earnings was a net positive or not. That sounds like a fun, see that's what I like
jumping to hyperbole, but this is this is this is real life hyperbole. That's awesome. Oh man,
I want to I could like smoke like cigarettes from malaria wouldn't that be wouldn't that just be a
great campaign? Oh man, if every time you light up you're saving a child's life. The three dollars
from every pack you buy goes to save this kid and put a picture of some oh my gosh. Or Marlboro
wants to come up with a new advertising campaign. I think I think they just they just found it. Oh,
we could how about this we could cut we could cut the the tobacco we could cut syntaxes and we could
take uh we could we could keep the same tax in there but just have that go to oh isn't that what
syntax does anyway it's only what syntax does it's just not it's just not very not very effective
yes they do not target the money usually very well yeah that still sounds awesome but I think so
some of the kickback so I guess there is yes someone complains about arts and opera or something
which seems maybe about a little more defensible than some of the other kickback you get from people
so the effect of altruist if you're straw manning the person could be like oh so you're saying the
fact that I give money to my local animal shelter isn't enough and so the effect of altruist would
be like kind of so I think some people find it indignant to challenge them and say you sorry
your your impulse to charity just isn't good enough it's not well calibrated do charity better
and people find that insulting so I don't I think it's fine I would rather be pointed out like if
I was giving all my money to name a popular city charity I can actually look like you might lose
some sponsors here I can name a charity actually I cannot say the name of the charity because I
don't know the name but I there's a charity that every year comes to my work and takes volunteers
and my my you know my workplace will pay people just their regular wages to go and work at this
charity for one day a year if they want to what they do is they take two blankets and they cut
fringes along the edges and then they tie them together that's the charity yeah that that is
that is that feels like an anti-charity to me it is taking productive hours that those people
could have used at a charity that actually does some good to tie two blanks that what what do you
get out of two blanks tied together that you wouldn't have just from layering one blanket on top of
another blanket how about how about like the catholic church I mean it looks pretty weird can we go
to I mean tithing right right that's how I started that's how I started giving was I said okay well
let me let me see what's doable let me let me do 10% right and so that's because that's a tithe yeah
so if you're tithing to the catholic church where is that money what is it going to how much pain
and suffering is it reducing I mean that's probably that's probably one of the biggest one of the
one of the worst charities that you give to if you want if you want to come up with something a
little less agreeable you could say things that go to fight climate change right so dollar per
dollar money that goes to maybe research but try to stop climate change is being spent very poorly
partially because we don't have a partially because we don't have a we don't have a cost effective
way to fix it and that's the other thing right like do you pour all your money into into one big
thing that could possibly reduce pain and suffering on a large scale or put it into a guaranteed
nets and water and food and education I think the general idea of the effective altruism movement
is that we we pour our money into the things that we know we can fix that are guaranteed
some things that reduce suffering yeah there's it's an interesting taste because it feels like
it feels like you're at least being consistent that something like donating to the arts or opera
is something that would increase happiness but you're more concerned about reducing suffering so
at least you get props for being consistent well it's interesting so like the the stop climate
change dollar per dollar ineffectiveness is is one example but as far as long as we're I think the
for me there's something else to sort of undecided on so I tend to split my charity budget between
like the machine intelligence research institute which stands a non-zero chance of more or less
creating utopia in the next century right and something like give well or the against malaria
foundation where I'm saving literal lives and reducing suffering I guess not today but however
long takes that money to get out there so on one hand it's it's it's a payoff between a bet of
some probability that this is going to make everything awesome forever versus I'm at least
doing some some solid concrete work today I also want to maybe point out that I think Miri's
goal isn't necessarily to usher in utopia but to prevent unfriendly air from destroying everything
fair enough I think I think related to that is creating friendly AI right that could make things
better yes I've got a hypothetical for you Stephen let's say that there's a machine that can that
will create utopia right it's going to is this is this for people or is this for is this more of a
on a transhumanist kind of thing yes yes to both yes yes to both do you would not consider
transhumanist people I mean by definition when they not be well they wouldn't be human no they
wouldn't be human but I don't know the definition of people is I think I think of people is
depending on which moral philosophy you ask it's a being that's inside your moral sphere of concern
so to a lot of people so I think they have to break down special terms for like so for me
I feel well scratch that because that's actually definition but it is one that's out there a better
definition is a being that's able to think for itself and not want to die and has some level
of introspection so by that by that measure there are a lot of non-human human animals on earth that
fit that definition like chimps and dolphins dolphins thank you and elephants it's a fun
semantic term what is expanding the word people it's to find people well I mean so like yeah we
could be to hear oh my hypothetical my hypothetical um is this machine creates utopia maybe it's uh
maybe it's I mean just taking out the potential parts that make us unhappy early whatever however
you want to um it puts a chip in us that just feed this constant dope you know whatever whatever
your idea of utopia is sounds a little different but sounds like a horrible place to me but you
have to feed it everything on earth then who's it who's the utopia for it's for the things that
live after that but if you feed everything on earth how's the hard things like I guess I don't
understand the what if it's what if it's to force a what if you have a chip what if you have a
forced chip that you can put into people that will guarantee that they will no longer experience any
kind of pain or suffering I think that would be a terrible thing I think that some level of pain
and suffering is important that if all you were ever I mean are we are we seriously saying that at
the point of just being blissed out on heroin kind of happiness yeah so see I think I think that
would be that would be like hell I think that for there to be a meaning in my life there has to be
a possibility of of being disappointed and I'll put it this way if everything you do has the exact
same uh has the exact same outcome then there's no point in doing anything at all and I'll put
this way for an intuition pump uh at the risk of sounding like an evil apologist
like I don't want to be as happy if my loved one if if my loved one lives or dies right it's like
if I'm going to be the same whether or not they continue to exist in what sense do they even matter
to me it's like the fact that I would be sad at their loss makes that that is that's an integral
part of them having meaning in my life yeah I don't think there has to necessarily be suffering
but there has to be the potential for suffering at least yeah imagine them dying or so I guess I
don't know but like it would just be weird to think that you know a sibling or a spouse or
something dies and the definition of what I you know if someone's saying I would want to feel
just as happy three seconds later like their death I don't want it to impact me whatsoever
it's like what do you what does that even mean to like say that you care about them
what why why why why wouldn't that be like why wouldn't why wouldn't grieving be why wouldn't
getting rid of grieving be be awesome I think I'm not necessarily not getting rid of grieving but
like you know said having the potential I guess it would just be weird to be completely indifferent
to their life or death what's something different you you greatly enjoy their existence you you love
them you care for them but then when they die they just they're they're gone and you don't
feel pain and suffering I think this was tying more into like the the binary so you used the word
whacked out on heroin I think I think the word that we can we can link to with the definition
is orgasmium I think that's quite what you were getting at where some people would argue that
there's not a global obligation to move humanity into orgasmium into an orgasmium state which is
basically just this big basically I don't know if I haven't talked to a heroin user who's as
stoked about heroin as you seem to think some of them are well at the point you've actually
injected it yeah well sure wouldn't wouldn't it isn't this the opposite of so if we're going to if
we're going to value happiness over pain and suffering doesn't isn't this to where we get
don't we don't we is that why you is that why you abandoned happiness entirely and only focused
on suffering because you because of the orgasmium no I think you're there to avoid that the orgasmium
is win-win we know okay there's no pain and suffering and I went in that situation too okay so
but I'd rather do that because remember the the utils are split into two columns it's right happiness
and pain and suffering yeah so I've gotten rid of the suffering column and I've I've boosted the
happiness column to as high as I'll ever get so yeah the super happy world basically yeah now I'm
trying to decide if you know grasmium world would be better or worse than no world at all I guess
better but it's hard I mean because I think it would be indistinguishable from a world with no
value at all so so if I if I um there's two buttons I'm about to implant the orgasmium in every
everyone and everything I would again be ethically obligated to kill you what if you push it what if
the only option you had was to blow up everything at that point I'd be neutral the two worlds are
equally awful everyone experiencing constant bliss is worse than nothing at all uh it's no it's not
worse it's equal to because there is no there is no growth there is no change there is nothing of value
in a world where all you do is feel happy all the time I don't think that the feeling of happiness
is a good in its own right I think if I put the orgasmium into your brain you would you wouldn't
believe that anymore but I wouldn't want to self-modify somebody who would feel that way right
yeah I don't think that pain and suffering is good I mean obviously it's bad but I think that
having nothing but happiness is all so bad or nothing but ultimate happiness all the time right
so like if if everyone's scale was moved so like I don't know just arbitrarily let's say on a scale
from one to ten five and under is suffering and and six and up is is happy if we could move everyone
to have their base level be six and they can still move between six and ten that sounds like
there's some value still there right six and ten is good yeah so but I'm saying is that they could
they could be happy all the time but there's at least predation where some things make them happier
than others they're more happy doing something that they like than something that they hate
so I guess I was just I was coupling with saying making people happy all the time versus making
everyone blissed out all the time so what would the what would say if you're a six what's your goal
what do you what do you want as an individual you want to be made more happy so you want to get
up to a ten yeah and if you're at a ten what's your goal your goal is to stay out of ten okay
I but then if you just put people at ten all the time you take away their ability to achieve any
goals why do you need to achieve goals you've already achieved your goal I I disagree I think
working towards goals and achieving goals is actually an important part of being a human
and if you were just to give someone that happiness all the time that that diminishes
the meaning of what it is to do things and to matter in a universe would you be less happy
I think I well see now this comes down to how we're defining happiness if we're defining happiness
as the chemicals in my brain making me feel happy feelings then obviously no but uh me right now
would say yes I would feel I would find very little value in that I think okay so I'm gonna get
briefly onto what I have heard as the definition of happiness which I I find to be the best definition
at least in my personal opinion so basically we are all we are all executors of adaptations that we
have evolved over time that these adaptations help us to survive and help us have grandchildren
and well what uh what I've heard is that the when you do something which in your ancestral
environment consistently led to greater reproductive success in the terms of having grandchildren
those actions would bring about a feeling that nowadays we call happiness and nowadays this
doesn't necessarily map onto things that actually bring reproductive success like we can be happy
when we gain fame and status because that means more resources for us and our children
even though when we have sex we're you know sterilized or using birth control or something
so that we don't have them we still feel that happiness even though it doesn't increase our
reproductive success but things that in the ancestral environment led to reproductive success
are what triggers that feeling of happiness are all of those actions uh or their modern day
equivalent still good or worth pursuing because I imagine like winning a war yeah definitely oh
I've heard it's still a rush nowadays yeah but that's not something that necessarily is worth
keeping around just for that no not necessarily I'm just saying this is this is the only definition
of happiness I've found that actually made sense to me okay like when I was trying to figure out
what the hell happiness even is things that lead through reproductive success was the best
definition I could find or that lead to that lead to um ancestrally led reproductive success
yes the which and you anticipate leading to the reproductive success so there comes a point where
once you have accrued this is where the treadmill comes from once you have accrued a certain level
of status you're not increasing your reproductive success at all is already at that level we should
we should probably point out about that treadmill is because I I mentioned it and didn't define it
either the hedonic treadmill is the idea that when something happens to make you happy you're happy
for a set period of time and then you revert back to the happiness level that you were at before
that happened studies show that people who win a lottery are happy for a few months and then revert
back to their their roughly base level of happiness and the converse is true of people who are injured
in uh debilitating accidents like uh quite or paraplegic although there has been some interesting
revitalization recently I bet it'll it'll sorry it also it also says that when you hit um that
when you're when you're looking at income levels and happiness people who are in poverty are unhappy
and then people who you know you hit about 70 grand and you're you're pretty happy and that's
about as happy as you're gonna get because after that you start get diminishing you start you start
to get diminishing returns so you can you can make double tripled up that you don't double or triple
your happiness yeah because those are all the things that are good yeah so I mean that's that's
the thing where if you get another girlfriend you're like oh I'm really happy for a short period
for a period of time and then like you know it reverts back to the mean and so then you're like
oh I gotta go get another new girlfriend or you know you publish a book or something that makes
you really happy after a year or two you're back to where you were before and you gotta you gotta
go forth and try again you always refer back to your happiness so you always gotta strive
to get up there uh you know again and again and again so we've we've we've implanted you with
the orgasmatron um so it was called the orgasmatron I think it was orgasmium I like
our gasmatron I like it too um the orgasmatron is barbarella we've it sounds more like something
you do to somebody rather than just like a state of b we we we we give it to anyosh
who is already the happiest person in the the whole wide room in the world he's now happier
we then we turn it off for an hour for repairs okay yeah do you turn do you put it back in
I imagine I would want to at that time but what's the problem because otherwise you're the
protagonist and brave new world uh no I understand uh the problem is that I don't consider that real
happiness I consider that the same kind of happiness that you get from taking heroin
couldn't you do both couldn't you couldn't you be on there couldn't you have the orgasmatron in
there and like well why is this a limiting factor couldn't you oh if I if I could just like you
know take some drugs the evening sometimes I mean I go out and get wasted every now and then yeah
I mean if I could just occasionally plug it in have a have a fun evening then I might be willing
to do that yeah it would just be if someone took away my ability to not be on the orgasmatron then
I would be upset but you wouldn't really be upset I because you'd have the orgasmatron
depends on whether I have the ability to think or not while the orgasmatron is going on kind of
yeah you're just really you're just really happy you're just nine or ten all the time you're in that
you're in that that kind of state that we were talking about earlier sounds too much like um
self modifying to the point where I'm no longer a recognizable person really is not yourself right
yeah so we're assuming that you have a self but that's a that's a whole that's a new episode
that is another episode yeah this isn't philosophy bites we can we can link to an episode on the
self there um so it's kind of like I'll go out and voluntarily take drugs some time but now
say I'm going to start or rather someone's going to start giving me drugs without my
without my informed consent beforehand and they're just never going to stop like that's that's sort
of where it's at right so it's like well you're happier now what's the problem but it's like
well I didn't ask for this and now I feel like I'm a different person than I was and if you'd
ask me before I would have said no well I think that's the reason heroin junkies are so sad uh
to to us I mean I I know I keep bringing up the heroin thing and you're like heroin junkies aren't
that happy and no they're not I had to point at the time of injection to get you know the bliss
which is great but we do consider their life sad because that's all it is just
even if they never had any downsides if it was just a constant bliss that would still be really
sad there's nothing else to their life you could replace any human with the same human and it wouldn't
really make a difference I'm sure that there's plenty of functional happy people who also take
heroin yeah I hear a lot of doctors we should we should we should make up a drug because it's
like the problem with soma can we just call them soma that's good soma that sounds awesome
was the drug from baby world so yeah the issue with heroin is and really any hard drug is that
it stops becoming about this is going to feel great and this is going to make me stop feeling
shitty yeah so it's more of a like it just makes the rest of your life suck and you just you're
trying to get more leveled out as opposed to really enjoying it like your first couple times right
but even orgasmium or soma or whatever we're calling it I would consider that sad because
there's just that one brain state there's no change there's no challenge you could swap out
any human for any other human at that point if they were all just maximally happy it wouldn't
matter if you had six billion people or one person yeah because every person is just a copy of the
same happy person yeah that's not horrifying to you you you would just keep I mean at that point
are you I would prefer a paperclip maximizer where the paperclip is a person in the maximum state of
happiness you could tile the universe with maximally happy person and that would be okay
I mean it's better than pushing the button I'd rather do that than push the button
like I don't I don't know if there's anything so I think the only I think the only thing that we
have to do at that point would be to just copy the amount of people as many times as we can right so
then we wouldn't want to limit growth we'd want to I mean we'd still want to maintain within resource
limits but if we could if everyone was equally as happy then we just want to make as many of
those as possible I would say at that point you had wiped out the human race and it wouldn't
matter if you push the button or not because that's that's not the human race anymore so
so I put some value on being human and having a human race around the definition you like the
definition you like having the label I I stop identifying as human yeah I don't think it's
just the label I think it is it is a ontologically different thing from what you are describing
but if your goal is to achieve happiness and this is constant happiness even if you're not a
even if you're not a human if you even what if you are just transhuman we should have tabooed
the word happiness an hour ago I feel like we're talking about shit yeah we should wrap this up
actually it's getting a little bit too but uh yeah so I feel like we're we're equivocating a
lot on what we mean by happiness so I don't think like the happiness that that humans today enjoy
and that they value it's the same kind of thing as orgasmic right I don't think that would be
happiness anymore that would just be a chemical state I take it I take back the whole button thing
if anyone has an orgasmium I'll take that instead I would take it for a few hours a couple times a
month you know yeah yeah just just just once just just once the first one's free Jesus so so what
do we what have we learned in this episode hypotheticals are when you when you first came in
here talking about pushing the button I didn't think you were a scary person like I know other
people would think like someone who'd wipe out the entire race if they could have scary but to me
it wasn't scary just like to relate to that you know now that you've said that you would be okay
with just reproducing the one happy person throughout the universe I find you a scary person
which is an interesting thing I didn't think this is where I would end the night nor did I know
that about myself until just now maybe next time we'll talk more about effective altruism although
you're still you're still awesome we can have different you know philosophies about the end
state of humanity and still be cool with each other that's the life lesson here people yes if you
can look past your if you can look past your your extrapolated volition differences even if you
don't pull her opposite things you can still be friends because right now you're not you're not
creating you're not necessarily in charge of creating the future but yeah I think next time
I want to I want to talk more about effective altruism I think that that's a fun broad topic
that is very popular in the rationalist community but also I think very transferable to everybody
else so maybe we can get Peter Singer into the studio here oh man that'd be awesome we can try
getting him in over skype I don't know if he's got time yeah well no I doubt he would fly out
today we'd have to I bet you we could do it if we made a big enough donation I don't know if we
have that kind of money yeah but if we did that would definitely be the way to do it because
then his coming here would mean a huge like decrease in pain and suffering what if we could
just all write bad checks just does that justify the mean no I don't think it does at least he
certainly would not say not agree to that and then neither would I do the whole truth thing
which we gotta have a we gotta have an episode about lying at some point yeah I wouldn't necessarily
endorse any there I was more I was just trying to make a joke but yeah well you also just gave away
like I hope Peter Singer isn't listening right now because you just gave away the part secret
yeah damn they're real checks Peter swears he's this is not that much money in the account
no there's times I'm I'm I'm super loaded that two trillion dollar check is totally
wipe out malaria if you come to the show all righty should we wrap it up then
and yeah any any last thoughts anyone want to get too really quick or I really wish that before
we come here like two years ago I read a comment on this wrong called suffering is valueless
which made a good argument I thought at the time I don't remember most the content now
which is why I want to go back and reread it I wish before we came here I'd reread that
because it had a an argument for why suffering should actually be considered valueless in ethical
theories and I will find that and send it to you and ask you what you think and then we will not
talk about it on the air because you know that'll be in the future and right now we're in the present
I want to see it too that sounds interesting okay and anything you want to plug or anything you
want to pitch yeah you should we can put two websites in there we talked about what can you do
right what can you what can you do if you if you think that we should destroy the destroy all life
on earth what can we do instead of that we can put up give well and then there's there's another
website I forget the name of right now but it goes over it's called like 8000 hours I believe
80,000 hours 80,000 hours that's more yeah once you're working part-time right 80,000 hours you
can put that in the in there that that's gonna that's like well you only have this much time
what's what's going to be the most effective use of your time so like what career based on your
skills and what you kind of want as a job in life little if I kind of give you give you a path
there fantastic we will link those both good resources yeah for sure well thanks for sticking
it through if you made it this far in this episode it was interesting if not necessarily a happy
topic yeah so thank you for joining us yeah we'll see you again in a couple weeks that's it thanks
come on fight
