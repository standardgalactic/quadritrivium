money a lot more. So you should give your own all your money to them. Which means, you know,
that the spikes happen there. So you have, you have interview Ash, who really likes money.
And so it's on the salesman. So I guess I like money more. So I like money way more
than you guys do. So you guys should just be broke. And even though you'll be unhappy being,
you know, below the poverty line or in this case, I've taken all your money because I love money
so much. If we're if we're taking the aggregate or the average, it's super high because I have
all the money and money makes me really happy. So the problem is the problem with a lot of
utilitarian arguments is that they end like that. I even if we have negative 100 utility each,
you've got like 10,000 utility. So yeah, both aggregate and average is much higher than it
would be if the money was split evenly. I think one of the the best criticisms of
utilitarianism that can sort of stand is that consider individuals to considers the broad
scope. And so in many aspects, it can be considered to fail because it doesn't consider
individual people. And so like you said, the differences between people is that the value
thing is differently. And that treating everyone the same and doing it like that doesn't just
doesn't work. Well, and in our aggregates, like is the problem I came to was, is happiness equal
to unhappiness, right? Is pain and suffering? Does it have an equivalent to happiness? And
I'd like to say so just because when you're when you're happy, it means you're not unhappy. But
it's not a one to one ratio. And so I split the two. And I say, instead of just having an
overall aggregate, where you can have these utility monsters, we should put more weight on
the pain and suffering. Because I was trying to come up with my objective reality, the only thing
I could really find that was universal is that pain and suffering is bad. Right? So did you
find the corollary true that happiness and pleasure is good? It's good. But as not as suffering is
bad. Correct. Gotcha. Yeah. Do you have a breakdown? Like, I think like you said, it's like one to
one. Do you have like a three to one? Or do you just kind of say it's just worse, like to some
extent? Nope, it's just weighted, just more weighted on the negative side. Fair enough. So if we take,
you know, if we take that to a to a large scale, we end up, we end up pushing the button on a smaller
scale. The goal should be to, to decrease pain and suffering, right, to make the world suck less.
But the reason that you had me on today is because when you take that to a large enough scale,
and you have the option to push the button, my moral philosophy would say that you should
definitely take, you should definitely push the button because then you're going to guarantee
that there is no more pain and suffering. Well, like what if I just, if I just came up to you and
without any kind of context said, I have a button that can, that can get rid of all pain and suffering
in the world. I can now see why you like sales. We don't just have you on because, because you
push the button, by the way, this button that we're talking about kills everybody or all life
on earth. That's right. That's, that's the trick. It has to destroy all life on earth. I think a lot
of the, the, the, the, the, the, the, the, the acronym stands for voluntary human extinction
movement. Uh, their, their big deal is that we should, the human race should, you know,
voluntarily get rid of itself. And that does not go far enough for me because I think in a, in a
relatively, at least in a universally relatively short enough amount of time, something else is
going to evolve and it's going to suffer just as much, if not worse. And on the way there, it's
definitely going to go through something like we went through, which is 200,000 years of, of, of
shit. Yeah. I wanted to direct really quick that we're not just having you on because you press
that button. We're also having on because you're a cool dude. So yeah. There we go. Yeah. Well,
there's, there's a bunch of, you know, cool ideas that I want to process, like, uh, why happiness
is less weighted than, or, or, you know, why suffering is weighted higher than happiness.
But, uh, before we get into any of that, the very last thing you said that you would want to wipe
out all life because eventually something would evolve up to human levels again and have the
same amount of suffering that we have, which first of all was also the conclusion I came to
when I was in your position. Uh, but it brings up the interesting question. Humans feel a
uniquely high amount of suffering then in your opinion because of our, uh, ability to self-reflect
and be conscious. Yeah. Yeah. You have creatures that will don't respond to any kind of stimuli,
right? Um, you have higher level creatures that'll, you know, we're saying higher level just
for the, because we're, we're, we're eco-centric, uh, closer to us. I am more than happy to call
that higher level. Uh, you have higher level things that do respond to stimuli and then you
have things that have physical pain and then you get to us that has mental pain and then, um,
I, I would have a sinking fear that if we, if there doesn't already exist, there would exist
to the future a some kind of pain that is worse than a mental pain that maybe we just can't perceive
or that, because we're not smart enough. Once we, because we think out how pain comes along, right,
it's, um, it's moving us to not die and spread our genes, right? And then once we figured out how
to stop being in pain, um, something else had to come along to, to, to make us not die and spread
our genes. And that was mental anguish, right? So you have, you have, you have societies that
come together and you want, um, you want the society to exceed. And so then you have mental
anguish. Um, so I wouldn't, I wouldn't be surprised and I wouldn't give this a high probability,
but I wouldn't be surprised if there was a, if there wasn't some kind of greater pain that was
past that, maybe like an empathic pain that, I mean, we could, we could certainly end towards,
right? I suppose if it's, if it is just outside of our conception, then I, then I'll agree, but
it seems like between physical pain and mental pain, we've covered the two kinds of things that
there are, right? So either something's going on in just your, you know, in your mind or in your
body and it's affecting your mind, but other than that, depository third kind of pain might be to
suggest some third kind of thing. Well, the, the ant can't fathom mental pain. Right. So your
position is that pain comes in, pain is inevitable, uh, as you get smarter, the more conscious you are,
the more pain there will be. And there's no way around this. There's no way a life form as advanced
as us could evolve without having that sort of pain. Um, no way around it. I mean, we could,
we could certainly work to work towards it. I mean, that's what, that's what ideally what I'd
like to see. In fact, an empathic pain might make us better people. Um, but I could, I could
certainly see in the future ways that we can get around both physical and mental pain. Um, but as
it is, and, um, the way that evolution seems to produce creatures, at least on the earth, um,
pain is a driving force for, uh, for the, for the changing of the species.
I just last week stumbled across an article about aversion learning AIs, which, um, you know,
artificial systems, which are goal driven. And the way they learn is when they fail to meet their
goals, they're given a disincentive. They, you know, it's not pain in the technical sense, but
it is a situation they want to avoid. And apparently that really helps with learning.
And I was like, there is absolutely no way that aversion driven learning can go wrong.
Well, we should totally pursue this. It also doesn't work in child rearing as it turns out.
No, no. So even when, when rearing humans or dogs, like it's better to reward good behavior.
Hold on. It seems to work in image recognition though. That's a should. But, but I think,
I think a thumbs up would work. I guess, you know, we're talking about alien minds. This is who knows.
But I, I feel like, especially if we're going to try and scale this all the way up to super
intelligence, let's choose one that we made happy and not made sad on its way to Godhood.
Yeah. No sadness and pain seems to be very powerful motivators.
Could we, could we, could we create a program that did not, you know, that, that was not
advanced by pain and suffering? Yeah. I think we could do that. But then we get into like
transhumanism and then we have to, and then we're, then we're destroying all, all humans anyway.
Oh, just in a different way. Oh, okay. So you wouldn't be against destroying all humans by
evolving into a transhumanist or not evolving, but developing into a post humanity.
No, no, I wouldn't be, wouldn't be opposed to that. Okay. That's interesting. Because I thought that you
were, that like the kill all humans was your ideal solution or kill all things. No, it's not the
ideal solution. It's a, it's a, it's a hypothetical. It's if I had the button that like plummets the
earth into the sun, would I push it? Yeah. But now until then I would like to find more productive
ways that don't end up killing everything. Oh, no, I should totally talk about that too,
because you're one of the few actual effective altruists that I know in real life. But by saying
that if you had that button right now, you would plunge the earth into the sun. Isn't that also
implicitly saying, saying that you see that there is an extremely small chance that we'll
ever get there. And it's overwhelmingly more likely that we'll be suffering forever.
Because you're basically killing all future inhabitants of the earth as well when you push
that button. Right. Right. And I would, if I had to put a probability on, I would say that the
that things are going to get better. I mean, currently they're, they're getting better for
humans. And we're, we've got more and more cows that are suffering and stuff like that. But
I think in general, the everything will get better. But there will also be more people.
So when we're adding up all the, remember that we're separating the columns here,
when we're adding up all pain and suffering, we might have an overwhelming amount of
happiness. But just because we're going to be having a lot more people as well.
I mean, if I could, if I could eliminate that column is what I want to do. Right. If like,
if I can make sure that nobody would have to suffer ever again, at least in as far as earth
goes, I would certainly do that. Right. So you've read those who walk away from
Emmaus then? No. Oh, okay. Well, it's, it's a very short story about an ideal town of,
I don't know, a few thousand people. And it is basically utopia. Everyone is entirely happy.
Life is fulfilling and wonderful, except in the basement of one house, there is a child that
lives in abject horror and pain and suffering. And it does that for, you know, its entire life.
And there always has to be one of that. And the, the story is basically about there are some people
who, when they find that out, leave the village and never come back. Because regardless of how
much total happiness there is in the village, you cannot make up for that one child's afford
the suffering. Right. So are you saying that if the entire planet was really happy and utopic
and very fulfilled with their entire lives, and then there was like one old dude who was kind of
curmudgeonly and grumpy and said, you know, TV really sucks. I cannot watch anything that's good.
And my leg hurts. And I am just life is awful. And I wish I was dead. You're willing to kill the
entire human race. So that amount of suffering doesn't exist. Can I just get rid of him? No,
I can't get rid of him. No, then I have to have, then I have to have a coefficient. Then I have to,
I have to have a, for that, I have to know how much pain there is, how much suffering there is,
and how equal they are, right? How, rather, how they, how, how correlated they are. So there is
a level of suffering you're willing to accept if there's enough happiness to balance it out.
That's a rough one. Yeah. I don't, I, I, at that point, if it was the entire world and then
one curmudgeonly old man, I would, I would, I would hesitate to push the button.
Okay. Once a curmudgeonly old man and three really sick cancerous children.
Yeah. Yeah. Then the button gets pushed.
Oh, shit.
Push that very far.
But the, I mean, we, we, we don't have that. Just like we don't have the button,
right? Just, just, I mean, I think a lot of the, a nuke certainly wouldn't do that.
It would have to be something that would destroy the earth or the sun in order to do that. I mean,
if the, the, the, the, whenever we take things to extremes are, they're difficult. That's how
we get, that's how we get to the button, right? Because we take utilitarianism, we say, what happens
when you make the extremes, right? And that's what we do. I mean, that's, that's a good way to,
that's a good tool in general, right? And so in physics, when you're testing things out,
you put big numbers in, you put small numbers in, you say, does this work in both these? Yeah.
Okay. I believe it's the correspondence principle in quantum mechanics. You, you, you say, okay,
when we put these, when we put the, when we put the numbers into quantum mechanical equations,
do they turn into classical mechanics, right? Because if they don't, then they can't be true.
Right. So, so taking, taking, taking things to the extreme is a really good tool, just in general,
in general. Going to hyperbole is one of my favorite intuition pumps, just in general.
Sort of like slippery sloping, someone's argument or straw manning it. If done responsibly,
it can be a good intuition pump. Yeah. Okay. Well, if we turned this up to 11,
would things still make sense? And the answer is no, that's a good indication that they wouldn't
make sense at whatever level someone's proposing it at. You, you are definitely more extreme than,
than I was or that I thought you were, because originally I had thought when, when I was in
the same, yes, I would like to destroy all humans face, I was of the opinion that it was impossible
for there to be more happiness than there is suffering. And at the time I hadn't read this,
because I don't think it had actually been written at the time, but there's a great post over at
the view from hell, called the transdimensional justice monster, which it posits a godlike
being that can alter where utility is to make, to make the universe as, as net positive as possible,
that, you know, there, there may be more suffering in the past, but at least a greater
happiness in the future due to gains from productivity or, you know, like the industrial
revolution, it was kind of shitty to be a factory worker, like really fucking shitty,
you died from black lung, you got rickets, you got to work 12 hours a day, seven days a week,
so on and so forth. But it led to so much economic growth that the great, great grandchildren of
those workers aren't in a much better position. Now they, you know, can sit in their cushier
conditioned office and work, you know, 12, 15 hours a week and surf the net for the rest of the
time. But, um, there's, there's probably more happiness than unhappiness. Okay, I think it's
gotten better. And I think that it's gotten the transit justice, that transnational justice
monster post was basically arguing that when you even everything out that, you know, we had to take
our happiness by making the past more miserable, that we take our happiness in the developed world
by stealing resources and happiness from the undeveloped world. And no matter how you try to
balance things, no matter how you shake it out, net utility across the entire species across
all time is always negative, that you cannot get more happiness than pain and as a total. And so
the best solution is always to wipe out all life because net utility is always negative. And that,
that is like what I consider the strong form of the argument. But the fact that you're saying that
even a tiny amount of suffering overcomes all happiness. That's like really strange. Why do
you value suffering so much more than happiness? Because it's the only thing that I could find
that was intuitive. I mean, intuitive is the wrong word. When I, because it used to be,
I think it's the only thing that we can agree on. Right? I know I mean everybody. You were going to
find people who, you know, think that pain and suffering is, is the way and then it's, it's
valuable. And I'd argue that it might be necessary in some circumstances. But I think it's the only
thing I can, that we can universally agree on, at least as a human species, which is what defines
ethics. That is bad. I was going, I was going to make an argument that then made me sound like
someone could just change a couple words and make it sound like a defense of the problem or a defense
of evil, like, you know, in the problem of evil with religion. But then I just realized that we
can just look at it a little differently and say, so one of the comebacks to the problem of evil
in religion, which is the problem of if you take the Christian goddess traditionally conceived,
how do you justify the existence of the amount of evil that there is in the world? If God's
supposed to be all powerful and all knowing, why doesn't he go out there and fix things or at least
not think, let things, let bad things happen. Right. Yeah. So one of the comebacks is while
suffering is good because it, you know, contrasts with bad or excuse me, it contrasts suffering
contrasts with pleasure and lets you appreciate pleasure more. And I was going to say something
to that extent, but then I realized that maybe sound like a evil apologist. So to bring it home,
apologist for evil, the rejoinder to that then is why is there so much suffering? Couldn't
every human learn how great it is to not be in pain just by stopping their toe once in their life
rather than like getting their arm blown off and then having phantom limb pain for their
for the remaining 60 years. So that's, that's the classic rejoinder to the comeback of while
suffering is good because it makes people appreciate appreciate not suffering more.
So have you seen Scott Alexander's reply to that? I haven't. It's the most brilliant thing in the
