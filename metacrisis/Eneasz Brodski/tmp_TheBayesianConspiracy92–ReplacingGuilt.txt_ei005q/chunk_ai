I definitely think that was the context of the sequences.
This was kind of written after new atheism and skepticism became popular.
Right.
I think Steven and I were talking about this one day.
We were just hanging out randomly, but like, I feel like I discovered the sequences after
I kind of went through my like angry atheist phase.
I'm going to argue with believers and then like that kind of got tired of that.
And it's like, this is kind of like the next step, I think.
Right.
Yeah.
That was my journey too.
It's also worth pointing out that Carl Sagan's writings predates the new atheists by like
20 years.
Yeah.
So the, the tone of religion sucks.
Man.
What I mean.
The tone wouldn't have been tolerated 20 years ago.
Exactly.
And to be fair, I think that was basically Sagan's opinion, but he was way nicer about
it.
He had a record saying that actually like being much less charitable when he's like in his
own house.
And also that wasn't 20 years ago.
That was like 35 years ago, but 20 years before the new atheists.
Yeah.
Yeah.
Okay.
Right.
Gotcha.
Yeah.
It just seems like that was yesterday because we're old.
Yes.
Our younger listeners are like, Jesus Christ, man.
That one 14 year olds.
Geez.
Yeah.
Man, these guys are old.
How did you get past the explicit warning on iTunes to listen to this?
Do we have an explicit warning?
I think so.
We save fuck occasionally.
Yeah.
That's true.
I guess we talk about dead babies and dead baby Hitler.
I'm not sure we've, well, okay.
We do talk about dead baby Hitler.
Did we finish summarizing this?
Yes.
Um, empiricism is supposed to prevent us from making this class of mistake because our beliefs
are supposed to pay rent as we just got finished explaining, but then, and this kind of also
ties back into replacing guilt.
Uh, what about the beliefs that you're supposed to believe?
Uh, Eleazar brings up Santa Claus when you're a kid.
And then also has a little Daniel Dennett quote, um, where it's difficult to believe a thing.
It's often much easier to believe that you ought to believe it.
Should.
And then it gets worse.
Nobody will admit to themselves, I don't believe in the ultimate cosmic sky that I don't believe
the ultimate cosmic sky is blue and green, but I believe I ought to believe it.
Not unless they're unusually capable of acknowledging their own lack of virtue, which kind of answers
your, uh, problem with that.
Steven.
Yeah.
People don't want to believe in belief.
They just believe in belief.
They just believe in belief.
Yeah, exactly.
It's hard to say.
It is.
Yeah.
Again, that was people don't believe in belief in belief.
They just believe in belief.
There we go.
Um, what's virtuous is to believe, not to believe in believing.
Yeah.
I think, um, the, I had another thing with Dan Dennett there, but I forgot what it was.
Yep.
It's gone other than if you liked a little witticism of that.
So Dennett's quote was where it's difficult to believe a thing.
It's often much easier to believe that you ought to believe it.
I think Dennett coined the term believe and believe.
I think so too.
Yeah.
He, there's a lot of things floating around there that Dennis actually responsible for.
Um, the only book of his that I actually finished reading was breaking the spell.
A lot of his stuff is, and maybe I should try again.
It's been 10 or 15 years since I've tried.
But it back then, back when I was a teenager, it was too dense to like, I read it's like,
this seems really cool.
And I'm just like, Oh, this is getting hard and you're not keeping my interest, but I bet it would now.
So, um, anyway, yeah.
So belief shouldn't should include unspoken anticipation controllers.
Belief in belief should include unspoken cognitive behavior.
Guiders, uh, the dragon claimant anticipates as if there's no dragon in the garage and makes,
makes excuses as if they believed in the belief.
Um, that's what I was gonna say about Dennett was that I remember he had another quote that
there are more people that believe in belief than there are people who actually believe
because he's met like, um, I don't know, people like pastor struggling with like,
not believing in stuff anymore, but thinking that they should.
And kind of by default, every believer believes in belief.
And as long as there's one person who just believes who, or rather every,
every believer like believes, right.
And there's at least some people who believe in belief that like,
I should still be believing this.
And that's like, that that's a defective premise of belief.
So he, he, I was, it was just fun to imagine.
Like, yes, there by necessity, more people who believe in belief than there are people who actually believe stuff.
It's not that interesting, but just again, it was one of those other little Dennett kind of,
he has this way of being able to put on like metaglasses whenever he wants.
I think it seems like actually like a rationalist virtue to like be able to have beliefs,
but not believe in those beliefs of like, have the belief,
but then like be ready to just like throw it away as soon as you see any counter evidence that is convincing.
Yeah. I have beliefs that you can update.
Right. I think update is better than just throw away as soon as you see any.
Yeah.
But relinquishment is one of the 12 virtues of rationality.
Yeah. Yeah.
But relinquishing with enough, with enough costs.
Right.
Yeah. We haven't done the 12 virtues.
Maybe that'd be, we'll do that at some point.
Yeah.
All right. The 12 virtues is this short post that I won't go into any of them,
but it's the, as the title suggests, the 12 virtues of rationality.
And it's all written in like preachy high talk that if this was your first exposure to the community,
you'd be like, oh, this guy's insane and they're all cultists.
But it gives nice flowery language to like refer back to and say thus it was written,
or you know, hence it was written that this,
and as long as you know you're doing it tongue in cheek as everyone does,
unless you're coming across one of these things in the wild,
then it's a lot of fun to have kind of that kind of highfalutin language to point things at.
Yeah.
Eleazar says that it's not psychologically realistic to say that the dragon claimant believes it's beneficial to believe that there's a dragon,
but it is realistic to say that they, the dragon claimant anticipates as if there is no dragon in the garage
and makes excuses as if they believed.
All right. And then we had one more sequence.
Basie and Judo.
Which is just a little story.
It's fun.
It's a fun story.
So the true tales of Elias Yucosky.
So Elias Yucosky is at a dinner party when someone said to him,
You're, you're,
Oh, that's right. I'm the doofus.
Doofus.
I don't believe artificial intelligence is possible because only God can make a soul.
Eleazar replied,
You mean if I can make an artificial intelligence that proves your religion is false?
When realizing that he'd made his hypothesis vulnerable to falsification,
he then said,
Well, I didn't mean that you couldn't make an intelligence just that it couldn't be emotional in the same way we are.
To which Elias replied,
Well, so if I make an artificial intelligence that without being deliberately programmed without any sort of script,
starts talking about an emotional life that sounds like ours, that means your religion is wrong.
I, I guess we may have to agree to disagree on this.
Then Eleazar brought up Omen's agreement theorem, which shows that no two rationalists can agree to disagree.
If two people disagree with each other, at least one of them must be doing something wrong.
Finally, he said.
Well, I guess I was really trying to say that I don't think you can make something eternal.
Well, I don't think so either.
I'm glad we were able to reach agreement on this as Omen's agreement theorem requires.
To which Eleazar stretched out his hand and he shook it and then he wandered away.
I love the line.
The one realized he had just made his hypothesis vulnerable to falsification.
But dude, who would have come up with, with that reply?
Well, that's why I liked in the, in the longer version.
He says, at this point, I must have become divinely inspired because I instantly responded.
You mean if I could make artificial intelligence that proves your religion's false?
I would have thought of something like, well, that's stupid.
God doesn't exist or souls don't exist or something.
You know, like this will never be an issue because there are no souls.
But I was like, oh, I can disprove your God with your own claim.
You got to have a shower thought like in real time and have that conversation.
Yeah, that is beautiful.
I also want to say that I, the reason I put these three in for this week instead of just two like we normally do is because this is directly ties back to making beliefs pay rented anticipated experience.
It's, it's one of those things that what does your beliefs not allow to happen, right?
If this guy really believes that only God can make a soul, then you cannot create an AI.
So that is actually a belief that pays rent in anticipated experience, except that he walked it back right away because, you know, can't, can't have your God be falsifiable.
But then assuming that he stuck to it and actually updated, he then changed his belief to be, you know, different.
So his new belief is, his new belief would be there's no God once there's an AI.
Exactly.
Which isn't actually going to happen.
But, but those, that was a beautiful example of beliefs paying rent.
Yeah.
And using that to, you know, Bayesian judo someone into atheism.
Like your beliefs can't kind of just be a bunch of nodes unconnected from each other.
It's really easy to actually like point out contradictory thoughts.
I think, isn't that what people do when they're doing street epistemology is not even really tell the person things, but just ask them questions, leading them to realize that the things they believe are contradictory.
Yeah.
The Socratic method.
Yeah.
Or at the very least that you don't really, and I think our street epistemologists, his name was also Stephen, wasn't it?
I don't remember.
It's been like two, two and a half years.
I'm 90% sure, because that's a great name.
And I think it stuck with me.
Okay.
I think that it's, I think street epistemology is practiced in a way that's slightly less aggressive than the Socratic method.
But that's sort of the same thing.
