yeah the motivation comes with having that virtue consummate a very uncompelling argument that if
you're a really rational person you'll be motivated towards deontology yes uh that that that's the
rational approach brandy or that's the that's the approach to to action that any rational agent would
do that's a very common um common thing for people to claim any round to claim the exact same thing
you have to perfectly rational person would be uh follow her morality which is why it's
objectivism because it is the objectively rational uh morality yeah when you get to name your own
stuff you get to sound really smart right um so let's think of another virtue does every it does
every moral theory claim that um probably not i i have from moral relativism i was about to say
yeah not relativism i don't even call that a moral theory that's like that's like not collecting
stamps is a hobby right so um well relativism is more descriptive than it is prescriptive yeah
i felt that that was the same with the consequentialism uh no consequentialism is absolutely
prescriptive okay uh they they're gonna say you should do these things and you should do whatever
will maximize happiness but i wanted to finish on virtues yes let's think of one more and then
we can find the common theme i'll leave you to you guys think of what is a virtuous thing uh to
work as a virtuous attribute uh show up on time sure i was gonna say not harm people but show up
on time is good so i mean like not harm people would be i mean there's there's cases where you
would harm people so i mean not how many people can't be the right thing all the time right so
maybe like no but if you have the virtue of of disliking when you harm people that'll prevent
you from harming people unless something much greater outweighs it right or you can turn it to
gather you know turn it down to one the other way and you'll never hurt anybody no matter what
and that also seems like a stupid position right you can end up being a jane and always
sweeping in front of you right yeah so but even janes would have doctors right uh i mean maybe
there's a consent factor involved i'm not sure okay um but like punctuality could be another virtue
too and i think it's harder to poke a hole in that one i mean maybe rigidly showing up at seven on
the dot if the party starts at seven might annoy the host whereas showing up five minutes you know
an hour early isn't appropriate and showing up an hour late isn't appropriate but something in the
middle well it would only annoy the host if it is understood that the cultural norm is for everyone
to show up a half hour late yeah i was i was really which case that wouldn't be a virtue that you
would hold yeah i was really stretching with with that one the the thing with the thing with
virtues is that it's it's a it's a middle point between two extremes and the extremes are both
vices and so Aristotle called it the golden mean or he didn't call it that he said it in Greek um
but that's that's what it's kind of been come to be called uh so you know like we said your your
disposition towards violence might be you know when necessary and not never right those those are the
uh that's kind of the way to put that but there's going to be a different golden mean depending
on the culture for example punctuality the the correct the correct way to be is going to be
an hour late if you're in Peru um correct me if i'm wrong Peruvians but i've heard some things
and uh whereas whereas in the US it's to be on on the dot on time or a few minutes early
no really well for for a meeting not for a meeting yeah okay right so isn't isn't that
relativism yes i don't have punctuality is but that's not relativism the you still want to be
there at the appropriate time it's just that the appropriate time is uh not the time that you are
told yeah but make the appropriate amount of killing people and being violent is different in
different cultures yeah and the appropriate amount of um i don't know sexual forwardness
forwardness is different and like isn't that make it relativism well so that that's one criticism
and that's that's partly why you know Aristotle isn't setting forward like a modern ethical framework
so i don't know if punctuality is is broad enough to be a virtue on its own but it kind of is so
like we kind of work with it um you know as long as you're working within the the appropriate framework
of what's expected you could extend it to um real reliability exactly in in which case it should be
flexible depending on what the the needs are right yeah i mean whoever you're working with that in
general you wanted to be a very strong desire but if there's other things that outweigh it
then it gets outweighed yeah you won't let my like i won't let my obligation to return the
$20 that you let me let my family starve right right i will i'll renege on that for the time being
so that's partly what makes virtue ethics uh not desirable to some people and super desirable to
others is that it's super flexible and uh it is the it is the kind of of approach that you can work
with that lets you adjust culturally right i'm trying to think of another example i guess you
know it's also the approach that i hope leads to less paper clipping of the universe can you
expand on that well uh when you have only one desire to make as many paperclips as possible
then you paperclip the universe but when you have multiple competing virtues and desires
then sometimes uh some of them will outweigh the making of one more paperclip when your need is to
be completely honest then you might have a uh undesirable outcome right right so it's kind of
becoming apparent that that is both the strength and weakness of of the virtue approach like it it
seems you know if you have two competing values how do you decide which virtue to go with uh you
know um do you love your well generally you go with the virtue that is not generally you always
go with the virtue that is stronger due to your programming the the question for the the ethicist
is what is the appropriate level of strength we want for each desire oh when building an ai or
something i meant for like for a human uh if you're building your own moral ethics how do you
where do you go yeah how do you live your life how do you be a good person oh yeah that so that
gets around to the other big greek word that arstall threw around a lot which was eudaimonia
and correct my greek if i'm wrong on the pronunciation that's the that's the good life uh that this
is more about and it wasn't so much of a moral connotation like it would be if we're talking
about stuff today right it was more about how how do you it's the kind it's hard to there's not you
use the greek word because there's not a good english word for it um you could say flourishing
but that sounds kind of like plants but it's more akin to like developing really well into the best
that you can be uh a good paraphrase might be the kind of life that if you could choose you would
choose for yourself and for those you love you know it doesn't have specifics to it um although
there are prerequisites to it but it's more just you know you want them to be as as best as they can
be the downside of that is that there are some prerequisites like you arstall argued and i think
he's mostly right that you can't have the great life that you wish for everybody that you love
and for yourself if you're born you know without a good physical body if you're born super poor
if you're born ugly and people hate you uh there so there are there are factors outside of your
control that come in to having a you down on life but he acknowledges that too i think i wanted to
just finish up on the virtue thing though that i think that it's it's a it's a strength of virtue
theory if the approach that you're looking for is something that is flexible for wherever you're at
so cons would hate it uh the consequentialist would kind of say you're doing it wrong but whatever
um you know you're not even he was saying not even you're not even doing it uh you know that's not
a that's not a moral theory um but it is based on it's in a way when you said it all comes back
to consequentialism it is based on the fact that this at least in my opinion it tends to have the
best consequences rather than trying to sit down and figure out the individual all udolans that
will happen from this action and all the possible knock-on effects for sure that having the the
experimental laboratory of the entire society we tend to have uh found some virtues that work
pretty well yeah i think and i'm not so say you right after our election so yeah i damn it proved
wrong by reality again i'm not a super gumball consequentialist but i think that that general
approach is right like when someone when someone asks if if you should uh murder a toddler
virtuous at this would say no but a consequentialist would say well if that toddler is hitler then
yes the consequentialist would acknowledge that in the real world you don't know if that toddler's
going to grow up to be hitler so that that you know if we're going to be charitable to the
moral realists or the moral activists let's be charitable to the consequentialists to
fairly i'll be fine and but that's the thing too is check all of these would say killing the kid
is wrong right so 75 to 90 percent of the time your moral compass is going to point in the same
direction no matter what your subscription is uh you know generally don't lie generally
don't steal generally don't kill people if you're a dentologist it's never but you know most of the
time you're not doing those things anyway um actually i was reminded of something that jesse
brought up at our lesson at our last meeting which is the idea of having a panel in your head um
that he got from elsewhere but we'll see if we can find the original person you talked about this
you have a panel in your head and it's made up of people with different moral theories
and how do you um how do you invite those people to your panel in what proportion
you might have um you know two seats for deontologists you might have um seven set aside
for consequentialists and you might have five for the virtue ethicists and you never invite the
moral relatives who knows i don't know um but certainly stay away from those fucking divine
command guys where where what you're what you're kind of doing is you're blending some of those
theories um in terms of how to apply it to your own life yeah because you can't if you're a
consequentialist you really can't predict a lot of the consequences to what you're doing so you
might want to take a shortcut through another moral theory and so there are different flavors of
consequentialism one of them is uh like rule or excuse me if utilitarianism there's rule utilitarianism
where you kind of set up kind of like it's it's basically deontology but you acknowledge that
you're doing it for the generally good consequences not for the good in its own action you know so
like you're not going to lie because you know other than like unless you have time to reconsider
that but generally you'll make a policy of not lying because uh that tends to be good so like
just this approach with the with the with the mind panel i know you had a word for it i can't
remember what it was that one works if you have the time to sit and deliberate but you're like
aerosol all acknowledges and and consequences and the consequentialist acknowledges everyone
if you take everyone who gives an actual thought says yeah you don't have the time to think through
every decision you're gonna make some things just come right up you know so you've you you know
you either have quick heuristics that you use to come to the right the right action or uh you try
and think through as many things as you can in advance or you know you have flexible rules or
whatever i think one thing that people try to poke a hole in in consequentialism in utilitarianism
by doing this thing well you can never you know you know the house is on fire you don't have time
to sit down to the pen and paper and figure out who's worth saving but that's not that is not when
the utilitarian is doing their their their calculation they're doing it when they're at home
they've got 10 minutes and they're saying well i've got a thousand dollar charity budget this month
what am i going to do with it well i could you know i could drive down the street and throw a
dollar at my window every you know every few seconds what are my other options right so uh it's not
necessarily about the the knee jerk reaction that you have in immediate circumstances it's about the
reason the moral action and then you do have time to consider as many external externalities as possible
if you're considering that you know if you've already decided it's a charity budget you can decide
what the best charity to give it to is you can give it to extremely very puppy disease you can give
it to an individual homeless person you can give it to give well you can give it to the
against malaria foundation and you'll think about if you have the time and the resources you'll give
as much research as you can into any of your options that are worth considering and go with
the one that you decide is best so based on how much you've been talking about this just now do
you think where people put their charity dollars is the most important moral consideration most
people will have not most people but like most people don't have the money exactly to be the
most important consideration that they have yeah exactly if you do have a substantial charity budget
i think everybody except for maybe conch would agree that it's worth considering where that money
is actually going so i don't know if that's the the biggest moral concern that a person would have
but it's the kind of thing that they they can give a lot of forethought to and that there are
um if you grant the premise that making the world a better place is a good thing so you know like
i find the arguments that like i said earlier attack the first premise to be pretty uninteresting
you know if you if you're gonna sit there and question every term and their first in their
opening sentence you're not really getting anywhere well um okay so every everyone who
thinks about ethics always says i want to make the world better that's that's what people in
general want but there are some people who say that the world is made better if women stay covered
up in burkas and can't drive right so that kind of comes down to yeah you why what why is your
your how do you how is your vision of the world different from theirs why why is your better
better than their better that's that's a good relativist question so um i would first say that
that that isn't everyone's concern that you know makes the world better if you're a doontologist
maybe you're a virtuosist you're a veneaguist that's not really what you're caring about
you're caring about what's good for you and you know what's good for you it's what makes you happy
the utilitarian would extend that and saying well people they see happiness if they're stopped
from it that's bad presumably being forced to live in a cloth bag when it's 120 degrees out in the
desert makes you unhappy presumably being shot in the face for trying to learn to read it makes you
unhappy so that if a consequentialist would say uh if you have to shoot uh someone in the head in
order to prevent all of society from disintegrating and us reverting to the level of barbarism that
you see in uh the movies yeah okay yes yes let's say we're we're losing half of our children before
the age of three to disease and there's people starving to death at fairly regular basis and
the only way to prevent that is to keep society strong keeps civilization running and that requires
making sure that a certain subset such as the ones with female genitalia don't know how to read
that is a worthy sacrifice right yes but that is not the the world in which it's like that that
changes what we were just talking about like so the the the world in which this is actually happening
now isn't that world right right so the problem is they're actually wrong or they don't even believe
wait like that long story promise to get laid out right i think a lot of them do they think the world
will fall apart yeah i thought they're doing it to keep their society strong because if they don't
society collapses i think i think they're mistaken about what makes a society strong well yeah i mean
it's impossible that people are coming from that angle i think we hear some of that from people
yeah my assumption is that in general people do want the world to be good they just are mistaken
on on what makes the world better okay i thought you're making a different argument so like good
is right well so i thought you're making a different point to like the the abhorrent conclusions
of some utilitarian calculations if you haven't played the game the last of us skip the next 30
seconds it's a long awesome story of the the main character and the like teenage girl like that he's
companion with he loses sometimes they're doing the thing at the end she's captured she has a resistance
to the zombie virus it's a zombie game um and doctors are gonna fungus yeah and she she has an
immunity in her and they're gonna the only way to get it is to cut it out of her brain and that
that kills the person so uh that would be the right thing to do they can cure the they can cure
the plague by killing it would not only be the right thing to do but there's a strong suggestion
that that's what she would want to do if she was caught if she knew about it yeah
double spoiler alert the protagonist of the game does not agree anyway so i thought that's
a fitting answer of the pugnic conclusion yeah takes her away and doesn't tell her about it
in order to save her life against her will okay and i totally disagree with his with what he did
because that's not it it's not a consequentialist or it is consequentialist if what you're
valuing is that one girl's life it's egoist in that case i think i think every basically every
human would agree with you that that was an immoral act and yet i've found so many people who think
it's an absolutely moral act and i think that they're probably virtue ethicists and deontologists
i think they're most of the egoists because in the game he killed a lot of people to get her out
of there oh he did so they're not deontologists and they're probably not virtue ethicists how about
care we didn't talk that much about um care ethics so i think that that was maybe that was that
cared about her in fact she was maybe the only person that he cared about
so everyone else didn't matter that's right yeah so care ethics is smaller utilitarianism bigger
egoism it's you care about the people in your network whether that's you know it's as big as
you want it can be me and my two best friends it could be me and my entire family me and my
family and all of their friends it's a lot of crime families work yeah and within the family
there's a very very strong moral stuff but anyone outside the family they're you know their marks
