Welcome to the Bayesian Conspiracy. I'm Inyash Brotsky. I'm Steven Zuber. I'm Jay Sticky.
And I'm Charlie Bradley. Welcome, Charlie Bradley. You are here to talk with us about a thing we will
get into after we can do the less wrong posts. We always do those first. And I like you being
in suspense as to what the subject is. Yes, it is not at all spicy, though. I'm guessing because
it doesn't summarize well, so I'm looking forward to it. It does not summarize well. Exactly. Yeah.
If we're gonna get canceled, we're gonna get canceled for our full argument. In context. Yes.
All right. Cool. All right. So our first less wrong post this week is Pascal's mugging,
tiny probabilities of vast utilities. And this is another one of those classics, which actually,
this one isn't just a classic. It created a whole new, I hate to use the word meme,
because it doesn't mean what Dawkins originally intended anymore as an idea that replicates.
And now it means, means, means, mean jokes. God damn, it even sounds weird.
I've actually heard people using it much more in, well, I don't know. Then again,
I'm in a rationalist bubble. So, but like, I think outside of that bubble, people have started
using the word meme correctly in some contexts. Oh, cool. But this created the concept of Pascal's
mugging, which is now spread through the wider society. And I have heard it used in science
programs and news programs, by people who are definitely not in the rationalist community.
I'm not sure if they're familiar with it, though. They might already be familiar,
they might have just picked it up through cultural osmosis. I found it really cool the
first time, I think it was like five years ago, when I heard it just used out in broader society.
And I was like, whoa, rationalism is starting to get out there. A friend of mine found some like
interviews with Elieze Hukowski on this channel that was just about conspiracy theories, but like,
we like looked up the YouTuber. And it doesn't look like he's in the community.
What was the conspiracy theory that they were talking about?
I was actually just trying to remember that and I was like, oh, no, it was Rocos Basilisk.
Oh, I'm sorry. I'm sorry to the whole rationalist community for reminding
either that happened. I don't know why anyone is worried. Like, it's a thing that happened,
whatever. I saw on our discord, someone snapped a picture of one of those micro breweries where,
you know, they make their own beers and name them and stuff. And there's a different one
every month. And so they write them on the menu with chalk. Please tell me there was a Rocos
Basilisk. There was a Rocos Basilisk brew. What kind of beer was it? Oh, fuck, I don't know, man.
Probably an IPA because micro breweries tend to love IPAs for some reason.
Just trying to think of a pun, but I don't know enough about beer to make a joke about it.
Kind of that you will regret not ordering. There you go. Yeah, almost. Yeah, yeah, it's no good.
Anyways, maybe I deserve to be Pascal's mugged because I don't because we haven't even made
exactly because we haven't talked about Pascal's mugging yet. What does it even mean, man?
Okay, so Pascal's mugging starts after describing a large number three to the to the to the third,
using a certain type of up arrow notation. Basically, it's a very big number.
It is very easy to describe because he did it with five characters and the terrain machine
can compute it pretty easily. But even writing it out in base 10 would require enormously more
writing materials than there are atoms in the known universe, which is a paltry 10 to the 80.
And there's a whole bunch of math about how big this number is to help you visualize it,
but it's very mathy and it's still very hard to visualize. Anytime you say more than there are
atoms in the universe, I get an idea that's a pretty big number. And since I can't even get a
good, I don't know, I have nothing in my brain to equate that number to, I just say big. I put
a couple of very, you know, varies before that. But I don't really care more varies, though, than
there are. Yeah, I don't I don't really care how big it is, because it's just like, okay, very large,
you got it. Yeah, I like how he's in the thing was like, consider newt's can uth's up arrow
notation. And I just sent you the picture of Captain America. No, I don't think I will.
And I'm like, no, skipping the math part. I know I already know what you're going into this.
There's a really cool visualization that I remember we did in kindergarten or first grade,
where I remember when printers always had like, what the hell was it, the dots, the holes on the
side. Yeah, so they could like, I think it was so they could spool it, but anyway, they would feed it
through the printer using those. Yeah. So we, for the entire year, tore the little holes off of all
the printer paper, and had the kids like break them into 10, and then bind 10 together, and then
bind 10 of those together. And we got to a million holes, just like trash bags and trash bags full
of those like, bounded up tiny holes. There's a joke in there. That's a lot of holes. It's impressive
that you got to a million though. Well, it was called the million projects. So that was our goal.
It is surprising how there was like kids like bringing the holes from like,
printer paper at home. If like, if they had computers, which only half of the kids had
computers at home, because I'm old, I was one of the families that did not have a computer.
So what is Pascal's mugging? What is a Turing machine? I have no idea what you guys are talking
about. Turing machine is basically the basic theoretical components of a computer. We should
really have Charlie on more often. Because how often do we just say a thing and not explain what
it is? Yeah, I don't know. I mean, at this point, we're at how many well over a hundred episodes,
right? Yeah. And I know we just discussed a Turing machine at one point. Did we? I don't remember
having done so. Well, we're discussing inferential distance on the next post. So that'll give us
a great opportunity to explain why we're not explaining our terms. But yeah, it's a hundred
third. This is our hundred thirty seventh episode. I know we talked about Turing machines at one
point. I'm sure we had to. Yeah, before I was on the podcast. So if everybody goes back and
listens to the previous hundred thirty five episodes of about one and a half to two hours each,
eventually you'll find a Turing machine description in there. Or Wikipedia and read the
first paragraph. That said, I don't remember anything we've ever talked about in the shows.
Who are you? Yeah. All right. So the Pascal's wager is the formulation of the popular concept.
Sorry, I'm running on. A long time ago. Yeah, I'm running on bad sleep and
what do you call it? Stimulants, caffeine and stuff. So this would be a fun conversation.
Do you want to know? No, I'm good. I won in the afternoon. No way. Oh, I don't know.
Usually, that's about six out of nine. I don't know. In any case,
most people are familiar with Pascal's wager, even if they haven't heard it,
formalized with that name. It's like the short version. Oh, you know, you should believe in
it's in the context of religious arguments. You know, you should believe in God because
it doesn't cost you that much or nothing. And the pay off is huge. So this was a philosopher or
a blaze Pascal Theologian mathematician, actually. Okay. Yeah. One of those crazy God
believing mathematicians, which actually weren't that uncommon since I think God belief was like
the default at the time. Well, not only was it default like mathematicians in particular
tended to be more theistic than other scientists since they dealt much more in these very abstract
kind of realms rather than empirical stuff. We're getting distracted again. We are sorry.
No, you're good. Okay. So that's that's Pascal's wager. Pascal's mugging.
Right. Like if you believe you've so what was it if you don't believe you gain a little bit in life
and you get to be more you get to be more hedonistic and you get your Sunday mornings to yourself.
Yeah, you gain just a little bit in life. If you die and there's no God, well, you know,
you didn't win anything. But if there die and there is a God, you go to hell and you've lost
everything forever. On the other hand, if you're a Christian and you lose your Sundays, you pay a
little bit of cost. If you die, there's no God. Oh, well, you're dead. You, you know, you lost
those days on Sundays. But if there is a God, you go to heaven and oh, everything's wonderful
forever. So due to the fact that it's just a small custom life for ginormous payouts in the
afterlife, you should totally believe in God. It's basically cryonics. Well, yes, with better
probability distribution. Right, right. It's cryonics with nonzero probability. Right. Yeah.
So that I mean, that that's the cryonics, I guess would be the other way with nonzero. Yeah.
Right. So that's that's the that's the pitch I give for cryonics, except you assign
based, you know, nonzero probabilities to these things involved. It's a really good argument
when you can put numbers into it. Yeah. When you're when you're not bullshitting and saying,
you know, oh, it's nothing or it's infinite or whatever, then it's, you know, you can actually
do a do a calculation to determine whether or not it's worth it for you. It's how many dollars
per month and what are the chances? Right. Yeah. The obvious answer, though, was always in my
understanding. But like, okay, but what about the probability that your religion is wrong and
that one of these other ones is correct? Yeah. Then you're still better buying a lot of ticket
than not buying one at all is the standard rebuttal. But that's wrong. But some of them are
mutually like incompatible. Like if you you can't I don't think be both a Buddhist and a Muslim.
So that's why that's why you pick the most vindictive God, because he's the one who's
almost like least likely to forgive you for choosing the wrong one. Like Buddha would
probably be fine with it, you know, if you're just a person. No, I'm serious. This is how the
argument ends. But then it is not a God. The unfortunate end of the unfortunate
like conclusion of that is that when you pick the most vindictive God to worship for your life,
that doesn't come at the very small cost as advertised at the beginning of the pamphlet.
Right. It comes at a huge cost. Yeah, kind of start going and killing all the
compare all the world religions. Yeah, you've got to go to the most extreme because that's the
God who would be most likely to punish you. Anyway, so Pascal's mugging is when you get mugged
by a mathematician. It's someone saying give me $5 or I'll use my magic powers from outside
the matrix to run a Turing machine that simulates and kills three to the to the to the to the three
people. Can we just say a lot? A lot. Well, I also want to throw in that he added in one more
up notation than there was before. So that number before where there were already you couldn't put
enough zeros on a piece of paper with you had all the materials in the universe to write it out.
This number is like it's exponentially bigger than that, like hugely bigger than that. So
it's an extremely large number of people that are going to be tortured and murdered to the point
where like, I don't know, you would need more neurons than there are atoms in the universe
to even comprehend this number or some shit. I'm sure. Yeah, some giant, giant number. Yeah.
And so the basically it's the it's the inverse of a Pascal's wager. And the question is like,
well, Pascal's wager to take them to 11 really. Yeah. Yeah, I suppose no cost $5. And here's the
most extreme. Yeah, fair enough. I to me what's interesting is, I mean, and maybe I'm just a
bad rationalist or I don't, you know, find this argument compelling, but I've never really been
Oh, no human finds it compelling. I suppose. But like, well, he the post is him wondering like,
should I am I failing as a good thinker to not be concerned about the sort of thing?
Well, because because a robot wouldn't or would find it compelling or likely would if you, you
know, was if it was just based off of, I mean, this also makes me think of just,
shoot, that's the thing where like, scope and sensitivity. I wonder that same thing about
like, you know, am I a bad person for caring more about my family members and friends than
people dying in some, you know, other country that I don't know that I've never met, never seen.
This is like scope and sensitivity, like, with really bad eyesight and much bigger numbers,
right? Because it's like, you can't, you know, I mean, would you give 100 bucks to your, your,
your sibling who know, whatever, need really need 100 bucks, or would you give it to save
everybody on the continent of Asia, right? Nobody would call you a bad person if you gave it to
your sibling, because we humans understand that like, okay, there's certain ways that like,
the way that we think makes us human, nobody would call you a bad person for giving a piece of
gum to your brother instead of saving all the people in Asia. I think I said 100 bucks, but
in any case, I meant it as 100 bucks either goes to your sibling who needs it, or you give it to
charity. Right. So I actually meant it in the sense of everyone in Asia will die if you don't give
them the 100 dollars instead. But yeah, so you're right. But most, most of us understand on like the,
you know, just how big your, your care network is that how humans are wired where we're closer
to the people next to us. We can forgive ourselves for being mere humans in most of those cases.
But in the context of like, if everyone was going to die and you know, if two billion people are
going to die, they need to be a monster for not doing it. Like people keep saying whenever the
trials experiment comes up that, you know, they'd save the one person that is close to them rather
than letting the five people die. And I don't, I don't know what I would do in that case. At
least for me, it's not the obvious given that I would save the one person I love. I'll try to
fall on any train tracks near you. Please don't. I mean, I don't know. Like I might, but I just
try to say you say it with like certainty. And I keep having issues with the fact that so many
people just easily will say I'd save the one. I don't need money to charity to people that I have
no idea who they are, where precisely they live. And I don't give monthly money to people I know.
So like, I mean, I don't, I don't think it's like impossible to imagine that people do it the other
way around. It's just like more common. But what if your sibling was in the hospital and needed
money to pay the bill? And you were like, sorry, I have this $100, but I've like,
dedicated, you know, myself to sending it to charity each month. Sure. I'm giving it to my
sibling. Yeah. That's where I think people would call you a bad person. Like, or that they would
that. Well, the thing is, if you have the money, and then like, I don't know, socially, I think
that that would be really awkward, right? If I only have the $100, like what would your parents
think? Well, the thing is that like, you have, you have an extra $100, and you just, you refuse
to give it to the sibling, because you say, no, I always give $100 to charity. So like, you can,
you know, find your own money. I mean, I would, it depends on the sibling, honestly.
There's one particular sibling where I'm like, no, you should have the $100, you wanker. But,
but for, you know, for some of my other siblings, I'd be like, yeah, the charity's just going to
miss out on this $100 this month, because you're in the hospital. Well, and like, people spend
large sums of money up to and including their entire life savings, prolonging their life for
like two more years at the end of life, right? You know, when you're 90, trying to hit 92 or
something. That in any case, we're getting kind of our field. We're getting way a few.
I feel like it's still related, actually. But I think the thing that it this, the thing that's
important here is the sentence, suppose I build an AI, and he goes on with like supposed build AI
that work on some banded analog. But the thing is, Eliezer, at the time of writing this, and to
this day, as far as I know, is attempting to build a machine intelligence, which could theoretically,
you know, based on what you know, maybe take over the world, or if nothing else, have extreme
impacts. Friendly AI. Yeah, yeah, exactly. Like, I don't know if he's trying to build the mine,
but he's trying to solve the problems that building a mine will create. Yeah, the whole,
the whole blog is in the purpose of creating an AI that will not destroy humanity, if it were to be
built. And that the Pascal's mugging is a real problem, because if you're a machine mind that
works on probabilities, and, and, you know, theorems like this that work on numbers, and someone gives
you that sort of a, a mugging threat, like, no matter how low you put the possibility that they
are telling you the truth, that they're actually, you know, a dark lord of the matrix that's going
to do this, it still basically adds up to that huge number. Since zero isn't a probability,
right, then the, the, the, the chance that this could happen is worth five bucks. No,
yeah, no matter how arbitrarily small the chance is, as long as it's not zero, the huge amount of
suffering that you're going to save is worth it. If you're a mindless robot following just pure math.
Well, no, you're a very smart robot with a personality, but that thinks on math. Well,
that's kind of us minus the thinking on math part. But I mean, I'm inclined just to say,
nah, you're, you're a charlatan, get out of here. But I, it, to me, it's fun. This is almost like,
you know, this is the kind of like philosophical, whatever, mental masturbation that I enjoy,
but I don't think really does anything. Well, I think it's important if you're making a machine
mind. Right. But since I'm not, I'm not, I'm not losing this. You know, like, if someone,
this is, I mean, the problem is that the probability is so small of this person being
a dark lord of the matrix, right? But like if I would said, Hey, man, if you give me a thousand
dollars, I will put a hundred dollars, I will distribute it across a bunch of cryptos that I
think might blow up. And in five years, you might be a millionaire and in exchange, I'll keep 50
