Hello Bayesian conspiracy listeners, this is Kyle, the sound editor and sound designer
of this podcast. We at the Bayesian conspiracy are looking for a new musical track to begin
and end our podcast, as the heavy metal track that you've been hearing thus far was just
meant to be a placeholder that we never got around to updating, that is, until now.
So we'd like to ask you, our listeners, some of whom surely have vastly superior musical
composition skills than we do, if you'd like to compose something for us. Your name will
be credited at the end of every show. Send us your submissions in MP3 format to Bayesian
conspiracypodcast at gmail.com, and please keep the track no longer than a minute or so in length.
Thank you so much for being fans of the show, and we are very much looking forward to hearing what
you all come up with. And now, back to the show. Welcome to the Bayesian conspiracy, I'm Enya
And I'm Steven Zuber. And today we are going to do things a little bit differently. For a long time,
I've been wanting to do general episodes on stuff that I've seen in the rationalist blogosphere,
basically, and there's never like a good time to have an episode just on one particular subject,
and then by the time I get to it, it's been like a year since it was posted, and it's kind of stale,
and there's no point. And some of them, they just don't necessitate an entire episode to
think, to talk about. So I'm hoping that from now on, like at the end of every episode, maybe we
can have like a little section like, Hey, here's cool shit I saw in the rationalist blogosphere
recently. Oh man, can we rip off the rationally speaking pick? I sure what is that? I'm a bad
rationalist. Oh man, you guys heard it here first, Enya doesn't listen to rationally speaking. God
damn it. You should listen to at least a few episodes. I have learned a few episodes. Oh yeah,
so at the end, they do their rationally speaking, rationally speaking pick, where back when it was
Julia Galef and Massimo Piglucci, and they were just doing it like together, they would pick one
for each of themselves. But then when they had a guest on to let the guests do it, and now it's
just Julia soloing it with guests. And so the guests typically picks. And it's just like,
what's something that's tickled your rational fancy, whether it be a blog post, or a game,
or a book, or a movie, or whatever. Perfect. Yeah, we should totally do something like that.
Cool. Every episode. We'll have to rip off the name and do something different. But yeah, anyway.
As long as we acknowledge that's where we got the inspiration from, and if it's what you wanted
perfectly, I'm totally down doing that. So I think for that, you texted me on what, Sunday,
with the episode idea, and today's Wednesday, and you had three things. Yes, because I was like,
there's a bunch, let's just use one episode to knock a whole bunch of them out at once,
and then we can do it as a regular feature afterwards. So I think the goal for this is
you're going to drive because I am now familiar with all three, but not fluent in any of the
three. So you can just run it by me and I'll have some concepts to relate it to.
Do you remember what the three I told you were?
Elias Yardkowski on No Fire Alarms for AI.
V's post on Slack and Inadequate Equilibria.
Okay. We should probably just do a whole episode on Inadequate Equilibria at some later point.
We'll see how much time we have. We can at least paraphrase it fairly quickly.
Because I also wanted to do a pick on the less wrong crypto autopsy.
Actually, just earlier today, the episode of Sam Harris' Waking Up where he interviewed Elias
Yardkowski dropped, and so he mentioned the No Fire Alarm for AI in that. So I'm not going to
spend as much time on it, I guess, but I still wanted to mention it because it was really interesting.
He just said that he talked about it in his Facebook post. He didn't really talk about it
that much. Did he? I can't remember. He talked about it.
Yeah, he hit the main concept behind it.
What's the main concept on No Fire Alarms for AI?
The main concept is that, like many of Elias's posts, it starts out with a cool
anecdote about how humans in the real world are less than optimally rational.
No. Elias Yardkowski said that?
He did. I know. Shocking. This was an experiment and I prefaced this with the same
disclaimer that he prefaced it in Sam Harris' interview that there is a replication crisis
in social experimentation nowadays. I'm not sure if this is replicated, but regardless of whether
it has or not, it applies to AI even if it may not apply to real life. Elias Yardkowski said that
he is moderately confident that this one actually has been replicated and is a pretty good encapsulation
of how people act in the real world. Anyways, the experiment was a bunch of people are put in a room
and then smoke starts coming in from under the door. And the researchers see how long it takes
before someone actually is like, hey, shit, guys, there's a fire. We should get out of here.
And what happens is that they wait way too long because most people are looking around and seeing
if anyone else is panicking. And if no one else is panicking, then they're like, well,
obviously this must not be that big a deal. Maybe someone is burning some toast. Maybe
this is just an area of the city that naturally gets lots of smoke or something. Like if there was
an actual fire, something would happen to tell me this is dangerous and I should run.
So I think these experiments, I've seen videos of this kind of thing being done that were like
grainy from the 70s and 80s. I think people have been testing this for a long time. This is partly
bystander effect where bystander effect is where people, well, there's a number of things. But
basically it's like pushing off responsibility in situations of possible social stress. So a
good classic example is if you see somebody stagger into an alleyway and collapse on a somewhat crowded
Friday night street downtown or something, you don't immediately run over and panic
because you look around and no one else seems to be doing that. And you start rationalizing,
oh, maybe they're just drunk and they're going to rest it off for a second or whatever. But you
don't think this person fell over and had a heart attack. And so even when they stage things like
this and somebody grasped our chest and collapsed on a crowded sidewalk, people just walked past
them. And I think it's not because people are like psychopaths. It's because people,
being the social animal that we are, we look at the people around us and say, oh, if something
goes wrong, they'd be freaking out. They're not. So nothing's wrong. But the thing is that they're
also doing that. And since you don't want to look non-calm when looking around assessing the
situation, neither do they. So there's just this blanket effect. It's a feedback cycle.
Everyone is trying to look calm and look at everyone else. And since everyone else is also,
there's this false sense that everybody is calm and there's nothing to worry about.
Right. And so you could do things where you get like two Confederates and one test subject sitting
in a waiting room and then smoke comes through the door or comes from the concurrent of the door
and they just sit there. And I think he said it was like 30% would act up pretty quickly. But
everyone would like to think, I thought there was a fucking fire. I'd do something about it
right away. But you don't look like an idiot. And so, you know, if you get up and like, guys,
is that a fire? You know, you don't want to look foolish in front of your peers, the people around
you, I guess. There's so much suboptimal shit that happens just because people don't want to look
dumb. Yeah. So I'm kind of over that too. I'm trying to work on that. So like I said,
speaking of not giving a shit, so it's really easy for you not to worry about looking dumb.
Because I'd be stressed out all the time. But yeah, so
people will wait until, you know, basically the room's hazy and it's hard to breathe before they
start reacting, which, you know, if there's an actual fire might be too late. So yeah,
very likely could be too late. So how does it relate to AI?
It relates to AI because that doesn't really happen when there's a fire alarm. Even though
the humans already have all the evidence they need for a fire with the room filling up with
smoke, they don't do anything. But when there's a fire alarm, that is, it now has made it public
knowledge that there is a fire, everybody knows there is a fire, you will not look stupid if
you get up and leave now, because there's a fucking fire alarm. And so it breaks that social
awkwardness, I guess, by making it known that everybody knows that there is now a problem.
His assertion is that at least half the benefit of a fire alarm is just that specific thing.
And he says that we have a problem in that there is no fire alarm for AI. And what he means by
that is there are a lot of people who say, look, guys, AI is not a big deal right now. We don't
have to worry about it. Who knows how far off it is, maybe technically impossible. But even if
it's not, we'll have some warning signs and we'll see it coming. And we can do something to address
it once it actually becomes an issue. Right now, people are like crying, oh, the sky is falling.
And, you know, funding their mega research projects off the backs of poor, deluded geeks who are
scared of the AI coming to get them when there's really nothing to worry about.
And Eliezer pointed out that there is no way to break this narrative, that the closest thing he
could imagine would be if a program came from zero knowledge of a system to teaching itself
entirely about a system that humans have been working on for millennia without quite getting it
and surpassing the humans within a day on domain knowledge in that area. That would be kind of
a red flag. So we had a red flag like that somewhat recently, didn't we? It's called Alpha Zero.
He was just reposing this in response to Alpha Zero. He was like, this is a sign that AI could
be coming close. And yet still people are like, you know what, I'm an AI researcher or an AI
programmer. This shit is really hard. I don't see how people can get it done. So I'm not worried
about it. And his assertion is, what would make you worried? That sounds compelling to me. If that's
not evidence, I don't know what is. Exactly. What will you accept as evidence? Do you have to wait
until the nanobots are literally chewing on your leg, turning it into computronium for the AI?
Because at that point, it's a little late. And that was basically the gist of the thing, that we
don't have a fire alarm for AI. It'd be good if we did, but since we don't, we should at the very
least openly acknowledge that there isn't one. Is there anything actionable that we can do with
that information? Like, you know, say if, I don't know, you could run a company-wide awareness
campaign in a world where there were no fire alarms and say, look, if there's smoke coming to the
door, no one will think you're stupid if you get up and check and leave. Like we want to encourage
that. Is there something like that we can do with imminent AI? I don't know if there's much
we can do personally aside from, you know, more publicly saying, hey, this is actually an issue
that people should worry about. And I think things are getting a little better in that respect. There
are very well respected people in the tech field that are coming forward. And even not the tech
field, like Stephen Hawking, isn't actually in the tech field, but he's a respected intellectual,
and he says that he is worried about AI. So it's starting to swing that way. But the fact that
there's no single large, you know, alert or alarm or criteria that would get everyone to say, oh,
okay, we should start working on this is also a problem. What can we do? I don't know. Keep
getting the word out. Keep trying to fund research. I mean, like, we can take it seriously, but what
does that mean? Like, that's not actionable for us. You're right. So that's sort of a drag. That's
disheartening. Yeah, I guess at the very least, you know, use your voice to point out, like if
someone says, oh, I doubt it's really a problem, and you're comfortable objecting, you could say,
no, I mean, you might feel that way no matter what, like, just keep in mind that this is a real
situation that there are many people who think that this is something worth worrying about. So
maybe ask people what would convince you that this is a problem to worry about?
And many people might not be aware of AlphaZero. And then you could be like, oh, you just described
something that happened last year. Yeah. Yeah. Okay, cool. That was pick number one.
Round two. Round two, fight. Round two is Slack, a concept that has been introduced by Zvi
together with Ben Hoffman sort of. So first of all, who's Zvi and Ben Hoffman and where can we
read this stuff? Oh, okay, cool. Oh, yeah. The previous one is from you mentioned this from
Facebook post by Aliezer. And I'll go ahead and link that. So Ben Hoffman is Ben Compass Rose
Hoffman, who is, I don't know, a blogger, everyone who I mentioned is basically just a blogger that
I read in the rational sphere. And I don't necessarily know what they do offhand. So Zvi, who blogs at
the Zvi, the name of the blog is Don't Worry About the Vase, brought up the concept of Slack. Zvi is
actually, I consider him kind of an up and coming in the blogosphere person. He's writing a lot.
He's writing good, interesting posts. The quality of them keeps getting better in my personal opinion.
And they are, they grab you. They're interesting to read, which is really important with blogs.
I mean, that's what has made Aliezer and Scott Alexander so influential, right? That you like
reading their things. So you keep coming back day after day. But unlike Scott Alexander, at least
the one post that I read most of today, the Slack one, it was brief. Yes. So they aren't all brief,
but that's fair. Yeah. But some of them are. This one, it being brief is actually a good
part of the theme of the post. So well, because you only have so much slack to go around and you
want to spend, you know, lots of your precious time reading about it. So let's talk about what
Slack is. Okay. So this was inspired actually by a post from Ben Hoffman talking about the
concept of the Sabbath. He relayed a anecdote about how he went on a solo camping trip.
Originally, I tried to summarize the situation, but I ended up doing a poor job of it. So here's
a direct reading of the relevant paragraphs with some emissions for time. Ben writes,
Recently, I've been feeling too caught up in local social momentum. To spend some time alone,
I asked a friend to teach me how to go camping. On my first solo two night camping trip, I forgot
to bring a backup battery to charge my laptop or phone. So instead, I mostly kept my phone turned off.
Very quickly, I started being able to think about aspects of my situation that had been too overwhelming
to in motion to get leverage on the day before, because I wasn't dealing with them. I wasn't
keeping up with anything. I was just present where I was. I wished I'd done this years ago.
And then I realized, if I had been keeping a Sabbath, it wouldn't have taken years to step
back from social momentum. I'd have gotten a chance within seven days of noticing there was
a problem. And seven days later, another chance and so on. So like the requisite problem is that a
lot of us might be able to unearth similar things that we could work on or fix if we weren't spending
10 hours or not, I guess maybe 10 hours a week on Facebook and Reddit and this and that. Not
because those things are bad, but because there's this pressure to not be out of the loop.
I mean, when I went out to dinner with you guys last night, I hadn't seen the launch of
Elon's spaceship. I'd heard about it because I opened up my Reddit feed when I was leaving work
and it was literally the top five things on my feed from different subreddits.
Yeah, I hadn't seen it yet either. But there's this discomfort of being out of the loop
and having to halt a conversation while your friends catch you up is always uncomfortable.
I mean, there's an incentive like not necessarily through punishment, just through awkwardness
that you want to have the same prior information as all your friends.
Ben makes the argument that work is nowadays we think of work as things that we do for money.
So if we aren't doing something for money, it's not work. But he says really the essence of work
is the activity of producing or maintaining the artifacts that allow you to keep living.
So leisure is the opposite of that. It's when you're not responding to this persistent stream
of demands. When you're having leisure, you aren't having demands from your boss,
no demands from TV commercials, no demands from news feeds. It is purely your own time
to do nothing if you want. Just let your mind wander. This goes into the concept of,
as we were talking when we did our social media episode, in the modern day,
everything wants your attention. Everything wants your time all the time. If you allow it,
it'll take all your attention. Having a time where you are forced to not be able to keep up with
things is good. His real point with all this was, you don't have to do this every Sunday.
But if something like an orthodox Sabbath seems impossible for you, that you cannot take a day
where you can't drive, where you can't look on the internet, where you can't cook any food,
then you are probably in a permanent state of emergency. Zvi picked that up and ran with it.
He said that if you are living so far away from any support area that you can't take one day without
a car, there's probably a problem. If you don't have the time to cook a few meals the day beforehand
and put them in the fridge, there is very likely a problem. That you don't have any room for absorbing
shocks that may hit your system. Do you remember which book he recommended?
I don't. It was in the link, which wasn't that long to read, which we'll link to in the show notes,
but we can also find the book. I know there was a similar book about shock capitalism,
