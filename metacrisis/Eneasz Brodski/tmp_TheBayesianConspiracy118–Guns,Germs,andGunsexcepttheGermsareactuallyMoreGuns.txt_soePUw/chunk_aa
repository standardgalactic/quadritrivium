Welcome to the Bayesian Conspiracy, I'm Iniar Szpracki, I'm Stephen Zuber, I'm Jay Sticky,
and I'm Gray.
Gray, nice to have you back again.
It's good to be back.
Last time you were on, what were we talking about?
I think my last appearance was the Radical Honesty episode back in February.
Okay.
Can't believe it's that long ago.
I know, it feels like an entirely different era, because we could still not be worried
about the plague back then.
So it wasn't an entirely different era.
Yeah.
But welcome back, we are gonna be talking about something completely different this time,
but as usual, we are first going to do our sequence posts.
Yay, sequences.
So let's jump right into that.
As soon as I can pull up the right document, geez.
Who wants to start us off on this?
I'll go, absurdity heuristic is the first one, and absurdity bias.
So absurdity heuristic was, started out describing that heuristics aren't bad, per se.
Usually they're actually pointing you in some direction, like, okay, all of my experiences
have led me to believe that this cause will create this cause, will create this outcome.
So.
Very briefly, it's a mental shortcut.
Yeah.
A lot of things tend to be better quality.
Yeah.
Like, that is one that's obviously exploitable, but it's a general truism to the point where,
like, you can glance at things on the shelf and be like, oh yeah, this one on the bottom,
ten bucks, and then this one on the top, ninety bucks, I'm betting that one's better than
the ten dollar one.
Yeah, it's kind of the thinking fast and slow thing, where you have your heuristics that
do a bunch of mental shortcuts for you, and generally they actually work pretty well.
But sometimes, because you do use all these mental shortcuts, they can cause you to have
biases, is just what he meant by the absurdity heuristic versus the absurdity bias.
So when people say something is absurd as just a sort of knee-jerk reaction to a new
idea, or something that sort of pattern matches to things that seem like they might not be
probable, but maybe this person's not operating on, they haven't sat there and done the math
and been like, oh actually, no, physics say that this probably could happen, or whatever
other examples he brought up.
Yeah, but that's not a bias rather than just a heuristic.
And he was railing against that, like, probably because of people saying that AI is absurd,
or maybe his quantum gravity stuff.
I don't think he started railing yet.
He spoke against it.
I believe the railing will come later.
Oh yeah, okay.
But I don't know, did you read it more as railing?
No, I think I was just exaggerating.
He does seem to be trying to not be exactly evaluative in this first one.
He's just saying, yeah, there are heuristics.
We use them all the time.
They're good, and as Kahneman said, there is a fair amount of content overlap between
the sequences in Kahneman.
Those heuristics are good so much of the time that we don't even notice when they're
right.
We're using them so often on at least daily, or they don't even register as common sense.
They're down to the level of reflex.
That's a good way to describe it.
They can brightly call a pointy animal, get away.
Even though there is a response in your brain from the ancestral environment that gives you
those ideas, you will not be aware of them having happened.
You can, with practice, identify when those heuristics are operating, but that takes some
exercise in mindfulness, and for the most part, heuristics are defined by being things
that we don't see happening, that we aren't aware of executing in our brains.
There was the example, actually, that I think you were maybe talking about Kahneman.
I think he brought this example up of if you see something that looks like a tiger, obviously
there's an evolutionary advantage to just running the fuck away regardless of it's
a tiger or not, as opposed to being like, okay, so it's a big cat, and it's got, I think
it's got stripes.
Is it looking at me?
This might be a Marat, like now you're dead.
So he gives a few examples, well, he speaks about three major circumstances where what
is the absurdity heuristic turns into an absurdity bias instead, where instead of being a good
rule of thumb, it's more of a problem.
And the first one is that he says, the first case is when we have information about underlying
laws which should override surface reasoning.
The example he gives is, if you know why most objects fall, and you can calculate how fast
they fall, then your calculation that helium bloom should rise at such and such a rate
ought to override the absurdity of an object falling upwards.
So there's cases where, even though something sounds really absurd, if you have really good
information about underlying laws, you should not think of it as absurd.
Yeah, although when hot air balloons first became a thing, I remember people thinking
they were like, well, I don't remember, I wasn't there, but I've read that people were
just baffled by them, because here's a very large heavy object, and all you do is you
make a balloon, and they didn't have helium, they were just using heated air.
That's the name.
Yeah, hot air balloon, but it is pretty hilarious thinking about what people's reactions must
have looked like to just seeing this gigantic thing just floating.
This, Eliezer's made a point kind of like this, I don't know if it's actually earlier
in your read order, because most of my sequence reading has been out of the edited rationality
from AI to Zombies, but there's another essay where he's talking about how you weigh arguments
from authority, where if you're actually in a completely unexplored domain of science
at the turn of the 20th century, maybe Lord Kelvin saying that this heavier than air flight
thing isn't possible should carry some weight.
Once Otto Lilienthal has built as a measured lift, and once the Wright brothers have built
a wind tunnel to measure lift in their own way, you've got experimental data that points
us in that direction, but when you actually see the plane fly, Kelvin's authority isn't
something that even factors into your calculation anymore.
I see the way that he's suggesting it, like this should override surface reasoning, it
seems like a good step from this how to stop making this mistake into how to actively not
make this mistake.
One thing I can think of is the Wright brothers looked at birds quite a bit, like diagramming
the wings, and birds are heavier than air, and they can fly, so you've already got evidence.
They have heavier than air though?
Birds are heavier than air, but they do have hollow bones and light feathers, but no, they
have weight.
Dead ones fall out of the sky.
We could do an Archimedes sort of experiment, you know, to figure out the mass of the air
displaced by the bird in a plastic box.
Trying to think of a rhyme of the ancient Mariner joke about the albatross around the
neck, but can't make it work anyway.
He has a second case, which he doesn't give an example for, says, attending to surface
absurdity in the face of abstract information that ought to override it.
If people cannot accept that study show that marginal spending on medicine has zero net
effect, that is a bias because it seems absurd that violates the surface rule that medicine
cures things, but the abstract spending on medicine has zero effect is not quite the
same thing, so their absurdity bias is leading them astray there.
Okay, yeah, sort of like, I guess, global warming, where you can, like...
It's absurd that you could change the temperature of the entire earth by something you're personally
doing.
Or like my dad, who's like, well, the temperature of the earth is always changing.
It always goes up and down.
We're not doing it.
It's just going up.
I'm trying to say every time you fart, the summer comes around.
Come on.
I've actually heard people explicitly make that argument, though, with respect to climate
change.
Like, it's arrogant of man to think he could alter the climate.
And you know, everybody knows that the beliefs held in arrogance are never true.
Absolutely.
Never once has an arrogant person been right about anything.
Certainly not the hero of a certain fan fiction.
I was hoping someone would mention that.
All right, third case.
When the absurdity heuristic simply doesn't work, the process is not stable in its surface
properties over the range of extrapolation.
And yet people use it anyway.
Like the future is usually absurd, scare quotes.
It's unstable in its surface rules over 50-year intervals.
And yeah, it gets more into that.
And I think, I forget if it's the next post or the third one, but we'll get into that.
Yeah, I have an example, but it'll make more sense when we get to why is the future so
absurd?
Okay.
He ends with, over the last few centuries, the absurdity heuristic has done worse than
maximum entropy.
It's ruled out actual outcomes as being far too absurd to be considered.
You would have been better off saying, I don't know.
And the example he gave was a certain person being accused of fraud for selling stock in
the radio telephone company claiming that he could send human voices across the ocean
using radio waves.
Nonsense.
Yes, exactly.
And I believe the Lord Kelvin saying that heavier than air things will never fly is another
example from previously.
And the next one was availability or the availability heuristic, which is judging the frequency or
probability of an event by the ease with which examples of the event come to mind.
So yeah, kind of self-explanatory there.
Yeah, we're all pretty familiar with this one, right?
Or at least we talk about it a lot.
Do we?
Yeah, the more you hear about things happening, the more it seems like that's a thing to
you.
Yeah.
Which is why the constant harping about the sins of a subgroup that someone hates tends
to be extremely bad and toxic because it'll make everyone around you think that those
people are evil when, you know, out of 100 million people, you can easily find the bad
ones and keep highlighting all their bad stuff.
And it'll twist your priorities on what evils you actually want to fix because you will
have a gr...
If that kind of complaining is going on constantly, you'll have a greatly exaggerated, potentially
a greatly exaggerated impression of how influential the outgroup you're dunking on actually is.
He gave a slightly less political example, too, in saying that people thought that accidents
accounted for more deaths than diseases.
When it turns out that diseases are 16 times more likely to kill anyone than an accident
because there's just a lot more of them, but you don't hear about the diseases because
they're not, you know, spectacular newsworthy events.
Yeah.
Or bringing absurdity back into it, I think it was in the Camden Aquarium.
They had a plaque talking about sharks, and they said that, you know, you're supposed
to guess how many people die per year of shark attacks, and then there was a little thing
that you could pull up.
And I forget what the actual numbers were, but it was considerably lower than you would
expect, and they compared it to, like, they said more people are killed by coconuts falling
on their head than shark attacks.
Is shark attacks from the single digits, right?
Shark attack deaths?
Shark attack deaths?
That sounds right.
They're not into triple digits.
And just the whole, like, that's a perfect example of availability because you would,
like, when you're thinking about common causes of death, you'd never think of coconuts.
No.
Actually, I just thought of this, but there is a way that an availability bias can be
literally hazardous to your health, at least in the short term.
It's known among people who've taught medical school classes as medical student syndrome,
where all these people who are tired and studying constantly and reading about a bunch of diseases
they've never heard of will develop the apparently symptoms of the diseases that they've been
