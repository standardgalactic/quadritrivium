Because back then, you know, if you went against the church, you got tortured.
I am supporting the church.
Here are these really flimsy counter arguments to these really good arguments.
But yeah.
Steve, I'm wondering what you think is both the future of street epistemology in general, where it's headed, and also what the future is for you.
In general, it's a growing movement now.
And there's not only can you find at streetepistemology.com, you can find videos, instruction in videos, tutorial videos and examples,
as well as they even have a playlist of the critics of street epistemology that are on YouTube so far.
And they have, they're right now in the process of beta testing in New Zealand and I think somewhere else, an app called the Atheos app.
And I helped to beta test it in its early stages.
It's basically an app where you are asked, you are given sample situations of what your interlocutor could be asking of you,
or stating outright, and gives you multiple choice of what possible responses you might have.
And it has some that are pretty tricky.
And they explain when you pick one that they've selected as the correct response.
They explain to you, and you can read more about why this is more ideal than any of the others.
Because, you know, it sets the critical thoughts in motion one way as opposed to, you know, avoiding the in your face answer,
or the one that might distract them from the topic.
So I find that street epistemology is really growing in that sense.
And eventually it will probably hit a peak.
My hopes for it is that it will first connect much more intimately with the Atheist movement and the irrationalist movement.
And that if nothing else, something better might come of it, like an even better model can be formed after testing this one out for a while.
Do you have any resources for people that like to read?
Because I went to streetepistemology.com and there was almost nothing there to read.
And there were videos and such, but I'm not really a video watching person.
I'm very much the kind of person who would go through the old less wrong website and just read for hours and hours, all these text posts.
Is there anything like that that you could point to?
Well, there's the Street Epistemology Facebook group.
You can ask to join that, and it's not too hard to get accepted.
And so there's people you can talk to through text there.
There's also the book itself, really, that defines and establishes what Street Epistemology is.
A Manual for Creating Atheists.
It's hard to get past the title, I know, but I will assure anybody listening.
The title, A Manual for Creating Atheists, is actually a bait-and-switch tactic where in the book itself, Bogosin even explains that he's not really trying to create atheists,
or he isn't really trying to convince you to be an atheist.
He just thinks that atheism will be a logical result that will spread after people utilize a technique like streetepistemology.
And what he really wants is for you to learn how to engage with others civilly about what they believe and why they believe it.
I've always felt the exact same thing, that if you just teach people to love the truth enough, they'll see that on their own, and eventually they'll get to atheism on their own.
I know, I'm stupid and idealistic.
Yeah, I think that that's how most people become atheists, whether or not it works.
They're just confidence.
But it's also somewhat insulting to imply that anyone who is still a believer is someone who doesn't love truth enough to look at that kind of thing.
So I try not to say that, but I still have that open belief.
The purpose really isn't, the objective isn't to really convince people to be atheists.
It's to be open enough to even find that maybe if you've heard a new argument, you might discover that you want to change your mind and no longer be atheists.
It's just, we know that this technique leads you more down the more rational path, whatever that may be.
Awesome.
So how long, we're getting up to the one hour mark, so we should probably cut this short soon, but how long do you think it would take someone to become versed enough in street epistemology to give this a try?
How much effort does it take?
I think it would take about a half hour of listening to Anthony Magnobosco.
If you're a good study, watch some videos on Anthony Magnobosco's YouTube channel where he engages with people at a college campus in Texas in San Antonio.
And he shows you through the questions that he asks and through his own demeanor exactly what street epistemology is.
So if you're a quick study, I would watch his videos for a while so you get comfortable with it and practice on your own.
Maybe even role play with some of us on Blab or you can also just test it out through tweets.
That's something that helped me to get geared up for it.
Can you send us a few links of a few of your favorite videos and we'll put them up on the website?
Sure can.
Awesome.
That's a good time to mention that anything you want to plug by all means.
Well, I've pretty much plugged everything already that I think is quite relevant.
A manual for creating Atheists by Peter Bogosian.
You can catch some people doing street epistemology on Blab.
Blab.io.
If you just sign up for it, maybe follow myself at Young Idealist or another individual Doug is really good at street epistemology there and has kind of a cult following.
He goes by at P1NE Creek, so Pine Creek with a 1.
And yeah, so I think that's pretty much.
Oh, also streetepistemology.com, great resource to sign up with the newsletter and keep updated with what street epistemology is all about.
Fantastic.
Alright, thank you so much for telling us about street epistemology and for joining us today.
Yeah, this was very enlightening.
Yeah.
I appreciate you coming on.
This was a lot of fun.
Anything else that you wanted to say or anything before we?
No, I had a good time.
I really appreciate the opportunity to be on this podcast.
I'm an Uber fan of NEI's HP Mark podcast and everything that you guys do.
So keep it up.
Thank you.
Okay, thank you so much.
Bye.
Bye.
Okay, we are continuing the episode with the feedback feature, which we now do regularly.
Alright, let's start with Mr. Oliva.
He told us how to pronounce it on the subreddit.
It was oliva.
So yes, very similar.
Oliva.
Overcompensation bias.
In today's episode, it was mentioned that the thought of revering Eleizer is hated to the point that people give him less credence than he deserves.
I find this as a common theme in a subset of rationalists.
Overcompensation bias is characterized by once when realizes that they have a bias or that they were factually wrong, they try very hard to change that belief.
Many, including me in the past, then swing their beliefs far too far to the other direction because they know that it is a natural tendency to believe what they originally believed.
Yes, it is very easy to notice this in yourself and trick yourself into thinking that it's fine to be so far in the opposition because then you will not slip back to your original belief.
This, while having some benefits, is a bad strategy to adopt.
I've never heard it called overcompensation bias before, but I had always heard the term that the newly converted are the most zealous.
Yeah, it seems to be very much a case.
And I think rationalists tend to stick to that much longer than normal people.
Maybe not. Normal people do tend to be zealous for a very long time once they confer.
But it could be very much a sort of thing where you want to not make that same mistake that you've made in the past and so you really work hard to never, like he said, to not be...
To overcompensate.
Yeah, to overcompensate.
I have a little bit of a quibble with some of these biases.
There always seems to be a bias and then there's an opposite bias.
That's...
Right?
Yes.
Which we brought up in the episode briefly. There's the anchor bias where you don't, where you have a first piece of information and then you don't update far enough away from that first piece of information.
And now this is the opposite of the overcompensation bias where you update way too far away from the first piece of information and you go too far.
And I've run into that a lot when I was looking at Wikipedia's list of cognitive biases that there's always a bias and then there's the opposite bias.
I have a problem.
Well, that question is, I think, because it's easy to miss the mark going too far either way, right?
Yeah.
But as far as rationality, having the lack of zealotry that other things might...
It might help that there's a whole sequence on effective death spirals where you're kind of cautioned against taking this and running all the way with it.
But as far as overcompensation bias, I feel like it is a useful technique to employ consciously.
If you're doing it unconsciously, that might be a problem, but anchoring is a good example.
My girlfriend and I were at the mall some months ago when the Super Bowl was about to happen and they were selling jerseys next to the food court.
And I said, how much do you think those cost?
I'm betting they cost, like, you know, 30 bucks.
And she's like, oh, I don't know, 50?
And I went over and checked and then I came back and said, if I didn't say 30, what would you have guessed?
And she thought for a second, she's like, 75, 80?
I was like, they're like 100 bucks.
But the point was that she was anchored on mine and if she corrected away from it, she might have gotten a closer answer.
But again, if she corrected too far and said probably 1200, that's, you're doing it wrong that way too.
But I think deliberate overcompensation is a useful tactic.
I think part of it is also the less wrong community has...
When you first find rationality after having been living in their crazy world for a long time, you tend to get really excited about it.
And so the less wrong community has been called cultish many times.
And it's intensely annoying, but it's also that no one wants to be a cult.
Well, at least no one who considers himself a rationalist.
And so there's very much a strong overcompensation pushback to not be the cult.
It's like the people who immigrate from a different country and then burn all ties and, you know, don't keep any of their cultural artifacts around
and don't let their children speak the home language in the home anymore because they want to not be associated with that in any way.
And I think that's one of the reasons that they overcompensated because people were like, well, Eliyzer is your cult leader.
And we're like, no, no, we hate him. He's a terrible guy and he's the worst.
Right. Yeah. So with the example given that Mr. Aliva gave with the general regard to Eliyzer Gukowski,
I think that there is a happy medium to, if someone says Eliyzer is the true Caliphate,
if they, it's clear that we're joking.
That line, by the way, is not mine. That came from a fun Slate Star Codex discussion.
There's a tendency to overcompensate and say that we, you know, oh, he's just one guy.
In fact, he's not even the coolest, but I mean, and that's, that's, that's possibly true.
But I think it's, we have like, like,
But then it gets to the point where you cannot quote him at all because then you're overrevering him.
Yeah, that sounds like a problem too, right? Yeah, exactly. Yeah.
Shall we go on? I think we shall.
Adam says, first of all, thanks for these questions.
I noticed that our last podcast had a lot of comments in the subreddit and we're not going to get to all of them,
so Adam said, I still don't get why you seem to keep bringing up
interacting with others and getting their esteem as a terminal value.
Yeah, he's aimed at me. It's sort of a random thing, right?
Do you agree that you should care about these things probably for Evo Psych stuff about humans being social
and esteem being useful for survival? Yeah, totally.
That's the thing about terminal values. They are, I mean, they're not entirely random,
but there's no reason or logic behind them.
You just have the terminal values you have and you can't be argued out of them.
They can change over time, maybe, but the whole purpose, not purpose,
the whole thing of a terminal value is that there isn't any reason to it.
It's just what your value happens to be.
And in lots of cases for humans, it is due to things that we evolved in.
There's a reason. A lot of people have sex as a terminal value and it has to do with passing on the genes.
But that doesn't change the fact that it's still something that a lot of people value simply for its own sake.
I'm not sure if esteem for others is actually your terminal value.
I feel like you love others? Yeah, or esteem for, esteem of.
Yeah, I don't esteem anyone.
I feel like you're more excited in being happy and other people esteeming you makes you happy.
Yeah, is that your true objection?
Right, right, but then that always gets back to the only thing anyone actually cares about is happiness.
And sure, okay, but that's stupid to say because what are the things that make you happy?
Those are the things that are actually your values.
Yeah, and strangely in the fact that they get you happy, but I guess like, you know, not...
If you boil it down to all everyone that wants is happiness and nothing else, then you can give them the happy drug
and say that the world is perfect now, now that everyone is blissed unhappy.
That's true. And yes, you just haven't tried the happy drug yet.
It's really good, man.
So this looks like it was a comment on whether or not we were discussing the impact of...
Or gazmium.
Just as different.
No.
Reducing suffering.
I'm moving on to the next question.
Oh, okay.
Yeah, this is, but in the context of when we talked about reducing suffering is possibly easier and more cost effective.
Adam says...
Oh yeah, we said that the internet provides a lot of value, so it technically didn't cost anything to make people happy.
Sure.
But also in the next comment here by Adam, deworming seems like decreasing, seems more like decreasing suffering
and has a large economic benefits through increased school attendance and later productivity.
So I guess it was just a concrete example of...
Him being right and us being wrong.
Well, no, I think it was just a good example.
I was on the side of putting efforts towards reducing suffering.
Yeah, I think we should give them props for pointing out that, yes, lots of times reducing suffering also pays for itself
and has high economic value.
Oh yeah, absolutely.
I thought that was given, but that's absolutely worth spelling out.
Next one.
Sure.
Okay.
Oh yeah, I like this one.
