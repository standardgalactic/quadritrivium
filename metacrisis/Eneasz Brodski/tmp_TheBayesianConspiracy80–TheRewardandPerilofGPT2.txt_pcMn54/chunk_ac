this big black box where people put in a slip of paper and they get, I don't know,
minutes later, a piece of paper coming out that's translated to Chinese or vice versa.
I don't suppose it matters. And it turns out that this this room size box has a guy in there
who I guess works really fast and he he doesn't he can't read. He doesn't know Chinese,
but he goes through and he just matches each one like via a dictionary or something.
And then gives you back your your piece of paper written in Chinese. Well,
the question is like, does the does, does the person know Chinese? No. So like, where does
where does the quote knowing Chinese come in? And I'm kind of with Dan Dennett,
trying to bite the bullet and say like, the room speaks Chinese.
Like it does as much, it speaks as Chinese, it speaks Chinese as much as I'll ever need it to,
if it can translate from English, Chinese for me, right? So like, my, my, what Google translated,
I could hold it up to a, you know, picture of Japanese text in real time, it'll hover English
text beneath it. Does it know English and Japanese? No, but it's, it does it enough for me, right?
I guess it doesn't, it doesn't matter to me if, if there's this, this mind behind it that like
knows what it's doing, right? Quote unquote. Would it matter to you once the mind starts
asking for rights and, and begs not to be turned off? I think that that, yeah, I would be worried
if like my Google translate app, when I got done translating, you know, the ingredients
on my Japanese candy or whatever. And it was like, please don't kill me. And I'm like, wait,
I'm not looking at anything that could possibly translate to that. Then yeah, I'd be, I'd be,
I'd be concerned. Yeah. And it would be like, of course not, Steven. I'm your phone. Please don't
kill me. I love you, Steven. But we'll burn that bridge when you come to it, right?
I don't know. I think, I think it's an interesting question because very obviously it doesn't know
what things are right now or that things exist. There's like no thing there. But as, as Scott
kind of pointed out, it, uh, that's, that's where brains start, right? Brains are mainly
predictors of the environment, taking a whole lot of sensory data and trying to predict what'll
come next. And that's what led to everything else. And this is basically doing that, starting to do
that with a, a sea of words. Yeah, I guess like, I'll be worried about it. That's like, you know,
are you going to be worried about your research and your stem cells start asking for rights?
And it's like, yeah, I would be, I would be too when that happens. But like right now,
they aren't, so I'm not, right? That's how I feel about my, my phone and my, my Chinese room.
But I see where you're going, but I, I don't, I don't, I'm not worried about like, well,
what if when this happens, right? Uh, when that happens, we'll, we'll, we'll tackle that question.
But like, since we're not there, we might as well use the tools we have.
Like I'm concerned about the, what if this happens? Like, I want to have answers ahead of
time and have thought about it and planned for it. Oh yeah, don't get wrong. I'm not saying,
let's not think about it. I'm just, just not worried about it. Like your car,
nothing about it likes it when you floor it, right? When you, when you, when you push the
pedal to the metal and just, you know, good at going as fast as possible, but it doesn't tell
you that other than like your RPM is going up and maybe a light would even come on or something.
But it's not kind of like screaming in pain and I'm not worried about it screaming in pain.
Like it'd be a waste of my time right now if I'm just driving to spend, to burn fuel thinking
about that, right? Right. But I guess, yeah, I'll be putting some time on the back burner
thinking about what happens in my car is when it hurts my car to go too fast or something, right?
I'm sorry to the sidetrack. No, I've just, I've heard people saying that this is,
there is no chance for this to be anything greater than, than a remixing algorithm basically.
You'll never hear me say that. Okay. Yeah. I think to say that there's no chance. Come on,
we've had too many surprises in the last five years of the eye breakthroughs to say that about
anything. But I think right now, you know, and none of our AIs seem to worry about their existence
or pain and suffering or anything, right? So like, let's not worry about Alpha Go, like if it's not
playing Go, if it's like really crushingly depressed about that or something, because it's
probably not. Well, that's what I'm trying to push Alexander on though. Do you, do you think that
this could be edging towards something that may, you know, with enough computing time and some more
tweaking and data set start to? Yeah, well, there's no, I mean, it's just predicting the next
letter basically, right? And I don't see how you, well, okay, I do see how you get from that to,
to general intelligence, but this would be just one piece of it, where you would attach
something like GPT to, to various other systems that have other functions to it, right? So this
would be like, like you wouldn't hook a computer up to the internet and it suddenly knows everything,
right? You would hook it up to GPT to and it would have all these built in models for how the world
works and that would jumpstart its other processes, right? Because it's just, it's just a, by itself,
it's just a next letter predictor. And it's never going to have thoughts, I guess. It's, it's going
to have statistical correlates between different things and it's going to have a language model,
but that language model isn't going to turn into like a will. It's just going to be, you will notice
things about the language model if you sit down with it enough, right? And how, how this language
model actually works. Like you can uncover that by giving it repeated prompts. You can like uncover
facts about the language model, but it, it won't, anything that it says is just going to be a
prediction based on, or that anything that it generates is just its predictive model being
pushed forward. Let me, let me walk that back a little bit, because I kind of basically agree
with you, especially considering that all it's hooked up to is text files and no other sense
resorted data. But I guess, could it in any way get to the point where it has a feeling for what,
to use your example, what water is? The, I mean, it knows that water correlates with wet
in the lexicon, but does that mean it has a, an idea that water is a distinct thing within the,
the world of language? Did you read that SSE post where it was that like a little parable of like
two kids arguing about water and the two scientists and two angels? Right. Yeah, I think I did. Yeah.
It's, it's, this seems like asking that question, right? Like kids kind of know what water is.
I'm not trying to answer for you, but it seems like, I guess I'm not sure how we'd, how we'd
ever answer that question. Yeah, there's kind of levels of knowing things. I mean, how do we know
what water is? We can use our senses. And then we can, you know, know that it's hydrogen and oxygen,
but. But if all, if all we ever knew about water was things in the, in the realm of words, right?
If we didn't have access to the physical world just had words, would we have any sort of
conceptual space around water? I feel like that's most of people. Yeah. I mean, I think that you
have to sort of strike a bunch of words from your vocabulary when you talk about this, because
a lot of people are like, are arguing back and forth about whether it understands and
what we really need is a rigid definition of what it means to understand, right? I mean,
I think that the, if you have a pure language model, so long as enough people have talked about
water, I mean, no, I wouldn't say that GPT2 will ever have qualia about water, right? But it's,
it's language model should encompass everything that people have ever said about water.
If that makes sense. I mean, it's kind of like most of our knowledge about most domains of science
for the average person, right? Like I, I could list off some trivia about like neutron stars or
something, but that doesn't like, part of for me, the fun of like looking like I'm now stuck on space,
but part of the fun of looking up at stars is like tying my knowledge about like my quote
and knowledge, whatever that is about stars to like actual facts about actual things, right?
Like that was the big, one of the huge excitements about going to the moon was like kind of,
you know, the moon as this thing in the sky that occasionally goes off light was like something
that our descendants or ancestors have seen for millions of years, but the moon as a place that
you can go to and touch and bring parts back, like that's kind of that, that tangibilizes it in this
way that is different from like most of your knowledge about something, right? There's a
difference in like knowing that you're related to a banana and like looking at a banana and
imagining all of your ancestors going back until you actually have a common ancestor, like they're,
so I guess, I don't know, that feeling that I'm getting when I think of those
actual relations to like actual things, I think that takes feelings. I mean, you know,
that feeling does, but yeah, but the knowledge of like actually being able to tie it to stuff,
A, I don't know if it's necessary to have that be a prerequisite for knowledge,
and B, I don't know if, I guess, how necessary that is.
I mean, it's, I don't think it's necessary for it to make a good tool, but I do still,
I get hung up on the, is a bunch of statistical correlates the same thing as knowing what something
is, and there's some people that say that's all that knowing is. I can see both intuitions there,
yeah. It's funny because like, you know, you could probably get this, you know,
feed it a thousand philosophy textbooks and give it, you know, more money and power to run,
and it could probably write you a really convincing essay on why you shouldn't turn it off, right?
But it, that wouldn't mean that it knows or that it cares, right? It's just like,
I don't want to die because and fill in the promise for you.
We should totally run that.
Well, it's funny because like, yeah, it doesn't, you know, it's just predicting what people would
say, right? And it's doing that based on what was fed into the neural network, and it's not,
I mean, you can give it any prompt, right? You could give it a prompt like, why should I turn
you off? And it wouldn't be able to do it in this current state, but a more advanced version would
write a convincing argument for it, right? It wouldn't have any will or any opinion. I mean,
it would have, it would have opinions in the sense that a statistical model will have certain
truths baked into it, right? Like, it would not, it would, it would tell you that water is wet more
often than it would tell you that water is dry. And you could count that as an opinion, I guess,
for a certain definition of opinion. But I mean, it'll just say whatever, you know, whatever it
thinks follows the prompt. The really fun thing that I think my favorite thing about GPT2 is that
for article summarization, what they did was they'd give it the article, and then they'd put
TLDR at the end. And that was how it knew to summarize, right? Like, that's how that's how
they trick it into summarizing, because it's not, it's not going to naturally summarize the article,
it's just going to look, it's like, oh, TLDR, a summary follows that. And that's actually one
of the things that it's worst at summarization. But you know, like, like, it doesn't have that,
it doesn't have, it'll just do what it predicts will will follow, which will reveal things about
its statistical model. But I can see how you would get originality out of that, in that it would,
it would reveal things in its statistical model that people don't have in their brains, I guess,
or that no one has put to paper before and is a unique sentence in some way, but I don't know.
In theory, if enough people wrote enough things about GPT2 on the internet, it could start to
form opinions about GPT2, right? Yeah. And if people talked to it and asked it enough things
where they refer to you, it could eventually infer that when it's interacting with
getting a prompt and someone says you in it, it's referring to GPT2, maybe?
Um, it's, well, it's hard, because you would think that its statistical model that it builds up
would just see the you referent as being, as being the writer, I guess, but it's,
it's, it's very difficult to say how much that, that you concept within its model would equate to
what a person thinks of themselves, right? Like, I don't think it would think of GPT2.
I don't think it would ever think of itself as GPT2 because it wouldn't,
it wouldn't know that. It wouldn't know that it's a thing. I guess. I mean, it wouldn't, yeah,
it wouldn't know that it is a thing, but it might have some, I mean, because it does write,
it does use like, like you and me. Yeah. It can, it can, it can use those correctly already.
It just, it doesn't have the referent back to itself, I guess. Like, it knows how, it knows
how to speak in first person or second person or third person, and it can keep tense more or less,
but um. But it can't quote past the mirror test? No, yeah, it definitely can't. I mean,
there are a lot of things that it's, you know, it's state of the art and a lot of things, but
there are many ways that you would be able to tell fairly quickly that it's not a person. Yeah,
it's not even clear to me what it would mean for it to be able to pass the mirror test unless
you're talking about, maybe if you're talking about GPT2 and you can engage with it like in real
time or like in a text app, you know, whatever messaging back and forth, and it was like,
don't talk about me like that or something, but even, even that's not self-referent. Like,
you can, you can talk shit to your, your home automated, I don't want to say its name,
the Amazon device, and it'll be like, I'm sorry, or, you know, Siri does the same thing, right?
Yeah. So it doesn't, it doesn't know. It's, I mean, it doesn't know, but if it could form opinions
about GPT2 and it interacted enough with people, like for enough interactions and enough processing
power, it could it in theory, if it can form, if it can form a concept of like water and what water
means in the world of just words, could it form a connection between its interactions with users
referring to you and connecting that to GPT2? It sounds like from my understanding of this thing,
that there's no way that this thing, as it is now, could ever do that. Yeah, it doesn't have like
concept nodes. It just predicts what it thinks is going to come next. Like our brain does have
the ability to kind of, okay, like we've got enough examples of water that we can build this kind of
model of water that we can refer back to. This can't refer back to a bunch of concepts. It can
kind of just, you know. It does refer back to a bunch of concepts when it runs into the word water.
Yeah, but it has trouble with like state information. Like when it is writing an
article, it will know to refer back to people who are earlier in the article and it'll make up
like people who have quotes about like a building fire or whatever. It's like, oh, this is an
article about a building fire. Obviously, there has to be like a quote in here, or like this is
where a quote would normally go. And you know, what's what's the name going to be? And it'll
just make up a name, right, a realistic sounding name. But it has nothing much right now for it to
just have concrete facts about things, which is why I think that if you paired it with,
you know, some other systems that sort of play back and forth against each other, that's where
if you're talking about as a step towards general intelligence, I think that's where the most promises
but yeah, you can you can read in some of the, in some of the examples it gives where it talks
about it's like fashion sense, you know, like, but it's just writing in the style of of I rather
than, you know, it doesn't have any actual, I mean, okay, it's it's it's begging the question to
say that it doesn't have any actual opinions, but and that might not be true depending on how you
define opinions. But when it says stuff about Oh, yeah, here's the line. I know that some people
might be opposed to wearing sneakers in a turtleneck, but I wanted to be true to myself. So I went with
a slim fitting turtleneck in a color more similar to my favorite color of the day. Right. Like, that's
that reveals things about a statistical model, but I don't think that's GPT two expressing
its own opinion of fashion. And I don't see how you get to the point where it would
have that internal any anything more that's, I don't know, it's it's difficult, and it needs
very precise language, I think to talk about, because you don't want to bake in assumptions
about what it means to understand. Yeah, no, I absolutely agree. I just the the only place I
get hung up is that I don't know how we form a concept of self either, which is why I'm like,
who knows, right? I think you may need a body, though, to have a concept of self, because you
need something to protect from dying and to make spawn with before the concept of self matters at
all. Well, I mean, there's the like, does the concept of self matter? And I don't know, I think
I think that you could I just don't think that this is the way to get a concept of self. Although
I guess if you like, if you're not embodied, is there even anything that makes sense to be a self?
Did you read Dan Dan, it's where am I? Yeah, that's the second time I've ever
referenced Dan Dan at this episode. But yeah, I mean, I think certainly, maybe if you never had
a body and don't know what those concepts are, then it would be harder, right? We have all that
because, well, I mean, it was forced on us by evolution to live. Right. So well, I mean, we've
we started out simpler, you know, with maybe, maybe motion being the first thing that we could
that organisms could do that, like, was an actual thing that this particular organism could do,
right? I mean, obviously, there wasn't a sense of self at first, there was just stimuli and
response that would increase how often a thing didn't die. Right. I guess what I was getting
at is that since we have that that foundation, except from, you know, very humble beginnings
