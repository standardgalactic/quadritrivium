It didn't, it didn't seem to understand, it demonstrated understanding.
Well, great did this person teach.
They said high school.
It's high school.
I don't know.
Like to me, the impressiveness, like again, this, this isn't the most sophisticated author
on the planet.
And so like it's not trying to overblow it, but like my only experience to other things
that could generate text was like the auto text on my phone where I keep pressing the middle
predictive button and it gives me something kind of like an English sentence.
That's usually way all over the place.
And I mean, I should generate one now for fun, but just, you know, some weird garbage thing.
Right.
So type in I died and then just keep pushing, pushing the center one.
All right.
I died and my phone number still works and it was just gonna was a great night.
I wanna is, all right, now it's nuts.
But I mean, so that's how bad that is that that wouldn't get that would get a call home
saying that you could have a stroke writing this paper.
Right.
So I mean, it almost made sense at first.
Yeah.
I died and my phone number still works.
I'm dead.
You could still call me.
Yeah.
It's actually correct.
So this this robot who's what GP what was it?
GP T2.
GP T2.
It needs a less Star Warsy name.
I mean, it's still remarkable for what it's doing.
Right.
Yeah, how revolutionary is this?
So it okay.
So neural networks are logarithmic in terms of what you get out of what you put in.
And this still actually has it under fits the web text available to it.
So it's going to be even better than it is.
The question of how revolutionary it is depends on how much of a skeptic you are, I guess.
But if you were to put in like 100 times computing power, which you could do for like
$3 million, right?
Which is not that's a lot of money, but that's not inconceivable that corporation would do that.
Right.
To get it to the point where you're past diminishing the diminishing marginal utility of a dollar
invested in compute.
Yeah, you can the problem is it doesn't know anything.
And so it's good if you want to generate filler.
Or if you want to like fake data, I guess here's I think this is going to put a lot of like on
the edge writers out of business because I mean, I'm in the writing community in Denver.
So I know several people who I mean, in addition to actually trying to write great works in their
downtime also need to pay the bills.
And to do that, you basically work as a mercenary writing posts for blogs and websites that just
need to churn out two, three posts of content every week to keep people interested, right?
And it doesn't pay much and it's soulless work, but it pays money.
This this absolutely seems like something that this AI could do instead and put all these people
out of work because maybe just have someone quick fact check beforehand because it's it puts out,
you know, mindless filler, which is all they really want for those websites.
Yeah. Yeah. And a lot of that mindless. I mean, if you my wife works in
internet marketing, and a lot of what you're just looking for for that filler content is
keywords and you, you know, it's like for the recipes, no one cares about that stuff before
the recipe. It's just for generating keywords. And if you have a predictive model that can output
a lot of the stuff that you want without having to have human do it. It's kind of like this war
between machines, because you have GPT two or something like it that's just spitting out
a whole bunch of text. And then you have Google's algorithms, which are trying to like comb through
and scrape through that text. And like, it's not producing value for anyone. Right. It's like I
maybe there are people out here who want the recipe or who want the like story before the
recipe instead of the recipe itself. But that has not been my experience of of recipes online and
and how people interact with them. Those things are just for for Google for
it's the primary reason it's done. Yeah, it's because it's helps drive engagement and helps
websites rank higher. If you have like a story instead of just why why in the world would that
make a website rank higher if they make you scroll past crap before you get to what you
actually want. Maybe you're because you have you have more of the keywords that people are typing
in and like just into Google in general. And I'd also imagine there's something to do with the
fact that you've got to scroll past ads to see. Yeah, that's what I was going to say. Well,
yeah, that that's also part of it. Which again, you could just auto generate that stuff. I don't
think it's a huge threat to more intensive writing. But it's kind of what it's kind of one of those
things where we don't know how far it's going to be able to go. Because it can like it does have some
amount of reading comprehension, right there. They're machine learning reading comprehension
tests that it is state of the art on, right? Like if you give it a short story and it can tell you
it can answer questions with some reliability, far below human level, but with it can answer
questions with some reliability about what who the people in the story were and what they were doing.
And it's really neat in that respect. But that's not I mean, the extent that that you need a
computer to do that stuff for you to do automated reading comprehension. I don't know. Yeah, a lot
of the applications that they talk about are what I would call like Centaur applications where you
have computer do a bunch of stuff and then like a human is assisting and supervising because the
computer can't be trusted on its own. Right. Scott Alexander pointed out that it it got I just
lost my train of thought because the image of a Centaur came barreling through my head.
What was it? Oh, yeah, that you could ask it. Who's the author of Art of War? And it'll answer
you correctly, which means it has some some way to correlate author and books and how those are
in related sort of like it doesn't know what a book is or what a person is. But somehow the
statistical correlation shows up enough that it can answer the question.
I mean, yeah, if they can search all of its repertoire of knowledge, it could even just
see enough references to saying, you know, Sun Tzu wrote God of War or wrote Art of War.
You can tell I've got God of War on the mind. It doesn't have to necessarily even know that,
you know, if it's got I have no idea how it sorts its data, but I doubt it's like it has pages or
anything of that nature, right? So it doesn't even really probably put Sun Tzu that close to the
content of the Art of War. But it knows references, I'm guessing I really had no idea any of this
stuff works. Yeah, well, I actually think it's in the way their tokenization is set up for for
like each each word is a token. I actually don't think that it knows Sun Tzu. I think it just knows
that Tzu follows Sun in that context. Okay, right. Yeah, isn't it predicting each letter? It's not
even just predicting like strings of words, it's predicting every letter. You know, it's actually
kind of interesting, because I was trying to figure that out earlier today, and I was running it and
the so I was running it based on I had a list of magic items for like D&D, and I wanted it to
and I wanted it to generate more magic items for me. And there was a Vambrace, which is a
piece of armor in there. And every time it did it did the output for it, it would get
it seemed like it got caught on that and it was talking about vampires a lot. And I think that's
because it saw Vambrace, and it's like, Oh, we're talking about vampires. Interesting, right? Which
was which was weird. Yeah. But so yeah, that I I thought based on the paper that was doing
individual words, but I think it is doing letter by letter. And that's why it gets
vampire from Vambrace. I mean, it would have to if it gets confused by words that look similar.
Did you see that it creates it sort of knows that acronyms are a thing? Yeah, that was in the
when it was a step towards general intelligence, which was one of Scott's two articles on it.
Yeah, it invented a fictional government agency. You know, all the first letters were in caps like
you do for a fictional government agency. And then after it said it for the first time in parentheses,
it had an acronym. And the crazy thing is the acronym is not actually doesn't tie back to the
name perfectly. One or two of the letters are off. But if you're just skimming, you wouldn't be able
to tell because it's very similar. And also that means that it sort of knows and acronyms are a
thing, but it doesn't exactly know that they are supposed to map one to one, the first letter to
the the what you get. Yeah, I mean, to me, this is just like, yeah, it's not as good as a person
could do. But I feel like we're anchoring there in a way that we're kind of at this point to be
trained not to write like this is still really crazy cool. I think it's better. It's better than
kids could do. Yeah, well, I think it's amazing that having never been actually trained or taught
or told anything about acronyms and what they are, it just picked that up from from context from
the internet. Yeah, people do this and knew how to do it. Yeah. In that article, there was also
someone had asked you to tell me what your 10 favorite animals are. And it generated a list of
these really like kind of hilarious and terrifying animals. But the thing that Scott pointed out was
interesting about that was that it lost track of what number it was on. And he was saying that like
basically like the 10 year old across the street can count from one to 10.
It went one through five and then just started doing random numbers between one and five after
that it still got your 10 things, which I think may have been a coincidence. But it was one, two,
three, four, five, two, four, three, four, five. Yeah, that's interesting. I don't know where this
will be when I don't know how you find soon something like this given how it was put together.
But that's all we have. I think you just give it more training dead and more power.
Scott seemed to think that this is the same way we work that human brains work. We're predictive
processors. And this is too, it's just a weaker version.
Yeah, if you look at the, at the, they have a bunch of graphs in the paper. They did four
versions of these with a different number of parameters. And they,
they doubled the number of parameters each time. So it's 117 million, and then 345 million,
and then 762 million, and then 1.5 billion. And you can see they, they chart out how
how much of an increase there is in its scores on the various tests that they gave it for reading
comprehension and for translation. Summarization is one of the, like they weren't training for any
of these specific tasks. So they call it like a zero shot result, because they could fine tune
it to try to pass the tests better. But so you can see that some of them seem to benefit a lot
from the increase in increase in size. And some of them like summarization basically bottoms out
after the first doubling, it doesn't improve all that much at summarization as it gets more,
more parameters and more compute time and stuff. But, but, but it's very interesting. So one of
the next steps is to just increase the amount of power that you're throwing at it. And then
one of the other next steps is to clean up the data a little, which is very labor intensive.
The web text is, I think, well, I have that bit right here. The web text that they
trained it on is 80 million web pages. And so then they they pair that down quite a bit.
Yeah, sorry, the the web text data set is 45 million links. And then they they pair it down
to 8 million documents for it's like 40 gigabytes of text. And some of the
weird things that you see in it are just a result of, of kind of messy data. It'll occasionally
in my test, it'll spit out just like raw HTML that didn't get cleaned. I was trying to do,
I was trying to see how well it handled negation, right? So if you say, like water is,
and then you get the next it, like the next answer is what you take as it's answered to the question.
It will give you wet a lot of the time. If you there are a lot of settings that you can tweak
in it to sort of try to force it to do specific tasks. But and that's part of what they do to
change which tasks it's doing. But so if you do like water is you'll get wet a lot of the time.
But if you do water is not, I want to see how it handled negation, right? Because a lot of what
it's doing is statistical relationships between words. And so what it should do for water is not
is a lot of time the time it should do wet, right? Because it knows that water and wet go together
and it can't like flip its statistical models around just based on that not. But I was trying to
mess with that. And it spit out an IRC log. It spit out like that exact format with like two,
it had like two people in the IRC log talking to each other about like being gay. And I was like,
okay, this is not what I wanted at all. But it was very, it was very clearly like,
Well, water is not that. It doesn't know what IRC format is. But I don't know. It's very, very bizarre
output. But it's partly because the data is not that clean. And so it's, it probably gets to the
point where it's like, decides that there's a name, and then it's like, Oh, we're just doing an IRC
log now. Now, I wonder about you said it was generating HTML, whether it's functional HTML,
or if it's just what it thinks HTML looks like. Oh, no, it wasn't, it wasn't. It was very bad HTML.
But like, I don't know. I also was trying to change one of the parameters and I put it in as
a model prompt. And it's like, Oh, okay, this is just like some CSS, and just like repeats
some stuff a lot, because I assume a lot of the CSS that it got is auto generated CSS,
which means that there's a lot of repeated, repeated stylings on things. And so it just
does a long repetition, because that thinks what's, I don't know, it has a lot of things that could
be cleaned up by you'd have to automate it because it's so much data, but to like manually go through
eight million documents would be just too much work. And part of the what makes
GTP to work is that it has so much data, right? That's what I think is the like breakthrough
mostly is that it's just doing as much data as possible instead of trying to be domain specific
and multitasking from that. But it's so much stuff to clean up and you can see some of it poke
through every once in a while of what's obviously like the ads that it does every once in a while
or the like photo captions, which are which are fun. Yeah, do you have an example of a photo
caption? Oh, I think there was one in the paper. I don't think I saw that I was kind of skimming
through the paper. Oh, no, I don't. And I actually am not at my computer that has virtual machine up,
but it's it's it's the sort of like terse AP style photo captions. And it just puts them in square
brackets and took me a while to figure out what that was. But it's a very interesting,
like messy data part of it that just didn't didn't get cleaned out because there's so much
cleaning that needs to be done on, you know, they're they're eight billion pages. So
what do you have an opinion on what this means for general AI advancement?
I think it needs something else. A lot of what I've seen people talking about is that it has no
reference to the outside world, right? It's just it's hard to say.
Yeah, but I still think that's enough to beat you ago and tell you why you suck.
But it won't know that you're a thing. Does it need to? Well, not necessarily. I don't know.
This is like the Chinese room thing, right? It is very much like the Chinese room. Yeah,
I'm okay with it being as like it knows. I think we talked about I don't want to get
us to derail the Chinese room takes like, I don't know, three full minutes to explain, but
not that long, maybe maybe a minute and a half. Well, the short version is
is this worth our time? I mean, sure. All right. So the because this does kind of relate the
the Chinese room experiment is our thought experiment rather is that you've got basically
