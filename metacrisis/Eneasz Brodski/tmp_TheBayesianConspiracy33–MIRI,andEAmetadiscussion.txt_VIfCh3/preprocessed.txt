Welcome to the Basin Conspiracy. I'm Imiyash Brotsky. I'm Stephen Zuber, and I'm going
to introduce our guest. We have on the phone with us, technically on Skype with us, Tsfi,
who works at the Machine Intelligent Research Institute. Do you want to tell us a bit about
the Machine Intelligence Research Institute, Miri for short. I guess what they do, what you do
there, and... Well, let's just start with what they do. That works. Yeah, so the Machine Intelligence
Research Institute is an independent think-tag style research institute in downtown Berkeley.
And what they do is technical research towards the design of safe artificial intelligence.
And what do you mean by safe?
There's both near and long-term concerns, and Miri focuses on long-term concerns. So near-term
concerns look like, you know, like self-driving cars being dangerous or whatever. Long-term concerns
look like, you know, what is going to happen as AIs become extremely capable along many dimensions
and are given more and more responsibility to make, like, actually important decisions.
I guess just to set the stage a little bit more. I mean, so I'm familiar with Miri,
and I think many people listening probably are. But in the event that they're not, I think you
touched on what their goals are, that is approachable from, like, somebody who has no idea what the
institute does. But if you wanted to talk about your role there, or I'm also kind of curious what
the day-to-day looks like before we jump into more of the specifics on what's going on.
Ooh, let's get background first. How did you first find out about Miri and this whole situation?
My story is, I guess, pretty standard. I read about rationality and AI stuff
on the blog Les Run, and I think I stumbled upon some articles called the AI box experiments.
Really quickly, for the listeners, the AI box experiment is one of the common...
Rebuttals.
Rebuttals, yes, to the AI might be dangerous and can hurt us, is, well, why don't we just put it
in a computer that is not connected to the internet, and it cannot do anything unless it goes through
a human intermediary first who gets to veto whatever it wants to do. The worry is that the AI
will be, quote-unquote, let out of the box. The computer is the box, and the human will be convinced
to upload it or connect it to the internet, let it go free, and the AI box experiment is where one
person pretends to be AI, and the other person pretends to be the human who has physical control
of this computer, and the AI player has, I believe, two hours using nothing but a text terminal to
try to convince the human player to let it go free, and it's always done by a human player
who thinks there's absolutely no way they would ever let an AI go free, and what do you remember
what the results have been?
So, sometimes the AI, the person playing an AI in the box was successful, and sometimes not.
Despite the human saying that they would never let the AI go free, they are some half to two
hours talked into letting the AI go free.
Right.
So, I want to volunteer really quick, though. That seems to be the line of
refutation taken by, unfortunately, people like Neil deGrasse Tyson and Bill Nye.
Every time I listened to Sam Harris' podcast, and I don't know, a year or two ago,
Harris finally got on board with AI risk and all the fun stuff involved, which to me, I think,
is exciting and interesting that mainstream intellectuals are getting more serious about
this conversation, but in his conversation with Neil deGrasse Tyson on the show,
and I've heard Tyson say this elsewhere, but when he asks Tyson about his thoughts on it,
he's like, yeah, I'm not worried about it. We can just unplug it. Why is this even
a question people are worried about? And it seems like Tyson's refused to engage with
the answer to that question that he puts out, like, why are people worried about this?
It's like, well, there's actually an answer to this, and you're just not playing along.
Why should we be worried about it? Why don't you think the AI box is a good enough way to stop the AI?
The interesting point in the development of AI is when the AI makes higher quality decisions
in general than the human operators. So in particular, if you assume, which might not be
a reasonable assumption, but if you assume that the AI wants, for some reason, to get out of the box
or get out of whatever restraints are put on it by the human operators, and the AI can make
higher quality decisions than those human operators, then the AI will be able to figure out a plan
that's like too clever for the humans to have thought of to break out of the box.
Which to me strikes me as fairly plausible, even with was a game four or five of Alpha Go,
where against Lisa Dahl, where apparently made this move, I don't play Go, so I couldn't tell you
why it was breathtaking, but apparently it just blew everyone's minds. And it was apparently the
move that turned the game around. And this was, I mean, this was just a game where people who had
been, you know, experts at this were confused by this move that ended up being very beneficial.
If a move is, if a come from behind winning move is possible in a game like that, it doesn't seem
a huge stretch to think that it'd be possible in another type of game, like I want to get out,
right? Or not even, I mean, I don't like that terminology exactly, but same point.
Well, even the fact that a human has managed to convince another human to let him out of the
box, and we can assume that humans are much less smart than the AI potentially could be.
Right. That's the other, I think, knockdown position. I didn't mean to go too far down that.
I did want to ask you a little bit about, so you heard about Miriam, got interested through
Less Wrong, as I think a lot of us did. Yeah, why did you join Miriam? What was your
path to going from person reading about this on Less Wrong to actually becoming
someone who works at Miriam? Yeah, so basically, I spent some time in college
reading Less Wrong and like studying a bunch of math and CS. And I just like contacted Miriam.
I think I was an intern three summers ago, and then I went back and finished college and then
came out to Berkeley to be a grad student at UC Berkeley and also to do research with Miriam.
So are you a grad student right now as well? Yeah, finishing up my second year.
If you don't mind me asking, what did you do as an intern? It was probably not very interesting.
It was probably largely administrative stuff and like, informally talking to researchers.
And so what do you do now that you actually work there in a more formal capacity?
I do some non-research things like helping write papers. I also, my research activities
are, I work with Scott Garebrandt, who is a research fellow, and we work on decision theory.
And I also, you know, talk to the other researchers and help run a small workshop.
What workshop is it? It's like a, it's like a weekly, not really a workshop, like a weekly
meetup. Are you going to stay with me or do you think once you're done with your grad student?
I have not thought that far out yet. I have like more like a one or two year plan.
I think that that will depend on a lot of stuff yet. What is, what is the ultimate fear that is
trying to be prevented here? When we say AI alignment, what is AI alignment? In a word,
it's like the goal of taking those high quality, high quality, those like systems that make high
quality decisions and like pointing them in a direction that we actually like.
And why, why is that so difficult? Why can't we just say to the AI, figure out how to reduce
traffic in our city? You have all sorts of implementational problems. Like how do you even
say that? If you could say that and you like gave an AI the goal of reduce all the traffic in the
city, one really good way to do that is destroy all the cars. There is that, I suppose. Yeah,
probably speaking, there's this like phenomenon called perverse instantiation, meaning like
if you instantiate a goal, but you do so with like a large amount of optimization pressure,
like searching through a very large space of policies, you'll probably find an instantiation
of that goal that is like perverse. That's not the thing that you want. It's like the genie who's
intent on misinterpreting your wish. Yeah, exactly. So I want to live forever is like a
fairly, I think, trophy wish. And then it turns out that you're going to live forever, but you're
going to continue to get more and more decrepit. And, you know, your last billion years are going
to be the worst possible imaginable suffering. But hey, you're technically alive. I think that's
a fair way to approach broadly speaking the, I guess I was just kind of rephrasing what you were
saying. I remember reading of a Tetris playing program where the goal is to get your score as
high as possible. And for anyone who's familiar with Tetris, it's basically impossible to win,
win. You just keep playing until you lose. And once you lose, your score goes down to zero.
So the AI came upon the strategy of pausing the game and never unpausing it.
Not exactly what we wanted. And the real life analog of that could be terrifying.
Another, another classic example is, I think it's a bird in LASL. They tried to evolve
circuits to like act as oscillators of certain frequencies and they succeeded. They evolved
these circuits, but then it turned out that the circuits actually were not doing anything
in a way that they intended the act. The actual thing was that they had formed a radio and were
picking up frequencies from there. Yes, I remember. Did we talk about this before? Or was it only in
person? It might have been with Patrick. Are you thinking? Isabelle, yeah, something like that.
I don't remember if we talked about it on air, if it was only in person that we were discussing it,
but yes, I remember hearing about that. And it was so specific in how it worked that if it was
moved to a different part of the room, it wouldn't work anymore, right? Right, right.
And correct me if I'm wrong. I guess part of really the whole of why this is a hard problem is
because trying to imagine yourself and then control for every perverse interpretation or
instantiation is very challenging. And I guess there's other levels of complexity, I guess,
other levels of challenge built on top of that. Even once all that gets working,
how do you keep things working through, say, self-modification? Well, couldn't you just,
if the AI proposes something like destroy all cars to fix traffic, couldn't you just say no,
that is a bad, go back and try again? Right, so this is like human in a loop stuff. I mean,
in some ways, that's actually a pretty good idea. The issue is you want to actually get
useful work out of the AI. If you just have every time the AI is going to make a decision,
you're like, wait, okay, let's ask the human. You may as well just have the human make all
the decisions. Maybe the AI thinks of something the human wouldn't have thought of? Yeah. I mean,
to some extent, this is like an open, that's like an open research direction, like plans that you
can get an AI to think of but are still verifiable by a human. This is sort of maybe a complicated
issue or I don't know very much about it, but it's sort of not obvious that you can do that.
For example, in the AlphaGo example, the goodness of a move is not really verifiable. If you asked
AlphaGo, why do you think that this is a good move? It'll just give you this huge, it'll barf out
the huge search tree and it'll say, well, my neural nets thought that these were good moves
to investigate and then when I sampled from the distribution, it seemed like it would turn out
pretty well. That's the reason. So you're kind of stuck just either trusting it or not? Right.
There's a problem where the concepts the AI is using, so to speak, are not concepts that
interface with the way that a human would think about the problem. When I was talking about
Miri with Steven a few weeks ago, which was what sparked this episode, and I think this is a
common misconception, Steven was under the impression that Miri's goal is to quickly create
a AI which will help uplift humanity or something along those lines. That's a fair prayer phrasing.
I think my impression was that I was surprised that wasn't the case was I thought Miri in a nutshell
was trying to build safe AI. That was, I think, what I'd said in the nutshell. I was like, actually,
no, that's not it. So what is Miri's goal if it's not to actually build this machine?
Right. So Miri's goal is mainly to do the research that would enable someone to take that
research, turn it into a design for a safe AI, incorporating the ordinary capabilities
produced by mainstream AI research and then make a friendly AI.
Okay, that's not as surprising of a divergence as I thought Inush was getting at. So it sounds
like they're not writing the code themselves, but they're helping build the theory behind it.
Yeah, and in particular, helping build the theory of how to do it safely as opposed to just how to
do it. Yeah, okay, that stays roughly with, that at least confirms that my conception was
roughly accurate, even if they're not, like I said, writing code themselves on this particular thing.
How are they going about doing this? That's a big question.
Right. Well, so there's two research agendas that Miri's put out. One of them is technical
foundations of agency, and one of them is alignment for advanced machine learning systems.
So the foundations of agency thing actually is trying to answer fundamental confusions
about how, like, rational agency works, how decision making works, the idea being, like,
we don't actually know in principle how to direct a, like, very powerful decision making system.
And part of this feels like it's coming from the fact that we don't really understand
how high quality decisions work at all. Is that what the original point of the whole
rationality thing was to figure out how high quality decisions are made and directed?
The rationality thing. The whole rationality movement. I mean, it's sort of been taken over
by humans trying to make our lives better. But I got sort of the impression that that wasn't
the original goal, that it was more along the lines of trying to figure out how decisions are
made well in the first place so that they can be applied to machines.
Actually, when you're saying rationalities, you mean the, like, broad philosophical field
or the conception of rationality in less wrong?
More the less wrong conception.
Okay. My impression is that the less wrong conception is coming from the synergy between
studying human rationality and studying rationality for abstract AI systems.
Yeah, there's like lots of interesting parallels. It's like it's not entirely uncommon at Miri
to like have an insight into rationality that sort of like bounces back and forth between
human rationality and abstract rationality. So maybe that was sort of like a strange answer.
No, no, no. I think that works.
No, that's fine. Also, I was just, I'm trying to think of whether to take like a sharp tangent
to a different direction or do you have anything else you want to ask about specifically with
regards to Miri's agendas and approaches?
Well, I was still wondering more about how they are going about this because he said,
gee, the first prong was about even figuring out how good decisions are made, which we
still don't know. Is that correct?
Yeah, yeah. And then the second prong is trying to see how you would more like looking at what
sorts of machine learning research that's sort of that looks more like mainstream AI machine
learning research. Could you do to like build up the tools you would want to make a safe AI
out of machine learning stuff?
So how can we even do that second part if we don't yet know how good decisions are made
in the first place?
It's not clear whether we can, but there's still progress to be made.
One line of research that I like a lot is the research done by Paul Cristiano,
who's not at Miri. He's at OpenAI and he thinks about how to like sort of like we were talking
about earlier, how to take human judgments like humans making decisions about like what's a good
thing to do and sort of amplify that and apply machine learning to that dataset, so to speak,
and like make a system that's making decisions that are good in the way that human decisions are
pretty good, but doing so at large scale.
I don't know anything about his research. Is it look promising?
Yeah, I'm pretty excited by it. It sort of, it does run into what appear to be more like deeper
technical problems.
So how many people are working on each prong with that on figuring out how to make good
decisions and figuring out how to implement that sort of thing in machine code?
How many people? Very, very few.
Okay, not nearly enough. I think, did you mean like in professional AI industry or at Miri in
particular? I meant at Miri in particular.
At Miri, Miri is not largely focused on machine learning.
It's more about the decision theory.
Not just decision theory, but yeah, it's more about the like theoretical research.
I mean, I mean, certainly we do some machine learning stuff that's not the main focus yet.
So can you tell us, I mean, why this is really important or is it really important?
Sort of from like stepping back and looking at sort of where we want the world to go,
how we want the world to evolve over the next like 10 or 100 or 1000 years,
one of the key factors is intelligence.
That's the thing that steers the world in particular.
There's like lots and lots of money and talent pouring into
creating computers that are more like creating programs that are more intelligent than humans.
So it's like pretty relevant to everyone's goals, whether or not that goes well.
So right now we have computers that can play chess and go better than humans.
We're right at the edge of having computers that can drive cars better than humans.
There's a lot of things computers can already do that's better than humans.
What's the big deal so far?
It seems to be like this has been a great thing that computers are getting better than us at certain tasks.
Right. So this is like sort of still in the paradigm where like the humans are the top dog.
Things like if I look into my crystal ball, it seems like things will change as AI systems
can make sufficiently like domain general decisions that are on par with humans.
I mean, it's sort of hard to speak about the future because it's like
I have very wide error bars on all my like estimates,
but I could like give scenarios where it becomes an issue that as
are smarter than humans.
I think that sounds fine. I think for what it's worth,
I think Enosh is playing the skeptic just to drive the conversation.
Right. I'm trying to find out why a listener who is not familiar with the
Miri would be worried at all that this is something to worry about.
Oh yeah, I guess I just feel like I'm sort of speaking in like abstractions.
I think that's fair because I mean, we don't have concrete examples to point to, right?
You're talking about, I mean, right now, yeah, we're top dog.
We can still process all the decisions that any AI that we're building,
well, I guess for the most part, we're not yet consulting AI's for decisions about
important things that it's going to give us answers that we have no idea how to interpret.
And so, you know, as long as we're top dog, it's not that scary.
But if we're building the top dog that's going to be usurping us,
we want to make sure it plays well with us.
I'm interested to hear the examples that you're thinking of though.
Yeah, okay, so like one example, your basic like AI breaks out of the virtual machine
that's running on scenario.
I mean, you could imagine that an AI system has goals, like its goal is to cure cancer.
And it like is sufficiently intelligent that it can model the world around it.
And in particular, the model of the computer that it's running on and model the fact that it could
make a buffer overflow attack, gain like root access to the computer it's on,
gain access to the internet, transmit its code over the internet to other computers,
take over those computers, and then use like this vastly more computational resources
to achieve its goals even better, which is what the humans apparently wanted.
Couldn't we turn off the computers that it's on?
You could further imagine that the AI like has a model of the shutdown button,
understands the fact that if the human presses the shutdown button,
then the AI won't get to pursue its goals.
So if the AI is currently pursuing its goals and has a model of this fact,
it has a very strong incentive to disable the shutdown button.
So Neil deGrasse Tyson, I think would say, I'm channeling my inner, his version,
and he would just say, why don't you just shoot the computer?
Or turn off all the computers in the country.
I mean, if it came down to that, I think his, I mean, especially if it's just black boxed,
it might have a shutdown button that's like an emergency stop,
or it's just literally plugged into the wall.
And when you unplug it, you know, there's no, he would just, I imagine he would say like,
well, you know, he's electricity, we control the electricity, we win.
So in the worst case scenario, we could shut down every electricity generator in the US.
And so every computer would come to a stop.
I don't see him, I don't see Tyson granting that it could get that bad.
But it's not an entirely bad strategy.
It's just like, I actually don't know how to implement that strategy like feasibly.
I mean, so one thing that might be said is, well, the human will watch the AI as it thinks.
And if the AI starts to think about disabling the shutdown button or,
or like disabling the power grid, big red lever thing, like the human watches the AI,
think this, and at that moment shuts down the AI.
But this is like not, this is sort of like not a real, this is not a real design.
Especially since we apparently will not be able to see what thoughts I think an AI is thinking.
Yeah, we don't know how, we don't know how to do transparency.
Like we don't know how to know what the AI is thinking.
We don't know how to like scale this thing of having a human watch over the AI.
As I mentioned, if the human is just sort of watching every decision that the AI makes,
this is like not gaining you that much, you're still like taking up the human's time.
So you need some way of like leveraging human oversight more efficiently.
And it's like not, it's not at all obvious how you, how you do that.
So what avenues are being explored to do this?
Right. So there's, there's research on how to make AI systems more interpretable, more transparent.
There's a little bit of this research already existing in mainstream machine learning.
Largely, I think just because people want to know what's going on in their AI systems,
I think you can Google like what a con, what a connet learns or something like that,
which visualizes like if you have a neural net that classifies images,
you can see what a node, what features a node is responding to in an image.
So that's kind of cool. Largely, it's just entirely open to make AI systems have the property that
when it outputs a decision, you can be like, okay, why did you actually make that decision?
What factors influenced that decision?
What's, what's your reasoning? How could it have gone differently? Et cetera.
Well, I was just going to say like, I feel like anyone who's still skeptical of, yeah,
why can't we just read his thoughts? Why doesn't it just print it out in English on the screen?
Well, I mean, if you're not a computer person, it might not be super clear why that's super
impot like borderline impossible with what we can do now as far as I understand.
But the other, I think, impediment to that is even if you're equipped to understand,
you know, again, AlphaGo's search tree of how it came to make a certain decision about a move,
if it's a, if it's a more broad machine and you're going to try and just follow its thoughts
as it's thinking them on the screen. And even if it's, even if it's coming to you in human-friendly
English, the machine's still operating way faster than you are. And so you're running on 150 hertz
and the machine's running on theoretically 300 million. So it's, there's just going to be so
much more content there than you could ever hope to cover.
Well, imagine even just trying to watch a human neural network work like a regular squishy brain,
even though it's thinking at the same rate you are, if you're trying to watch all the little
neural nodes lighting up and interpret what that's happening as signals are shooting back and forth,
it's going to take you a lot longer than someone just thinking things.
Even if you were just like listening to their thought stream in a way that they didn't even
have access to themselves, it's going to be just this huge mess, right?
Because you don't get a thought stream, you get a map of which neurons are being lit up.
Yeah. And by analogy, I was just imagining he had some magic way of listening to their thoughts.
But even if you could, and it was like in your understandable English,
it would still just be this huge mess, right? And if you ask a person why they did a thing,
they don't really know, but that's sort of, that's different than a computer. A computer
should know why it's doing what it's doing, but it's going to be very complicated. Am I
just making it all technical accuracy sense? Yeah, it made sense to me. I mean, right,
as you're saying, we sort of hope that you can do better with an AI system than with a human brain
by default, because the human brain is not at all optimized for transparency, whereas we can,
if we want, try to optimize for transparency. Yeah, the human brain isn't even optimized
for transparency to itself. I mean, you can get people to confabulate answers for why they did
things, even when you know with 100% accuracy that they're making it up. There's all those fun
experiments where... Split brain ones? Or the split brains, or even where they've, you know,
they're intentionally stimulating parts of your brain and your hand goes up. And you ask them,
hey, why'd your hand go up? Oh, I was waving for a bus. Those are the kind of answers you get,
even though they're sitting in a chair in a lab, right? So the person clearly has no idea why
their arm went up. When they ask their brain, why'd you do that? They get back some bullshit answer
that sounds believable to them, even though it really shouldn't, right? So you're right. I think,
ideally, an AI would be easier, or would be more transparent with itself and hopefully with us,
but there's, I'm assuming, just way too much to see. And it's coming at you too fast.
Oh, so what are some of the major limitations that are cropping up in this
field of making AI's friendly right now? Major limitations.
Yes, I know this is going to be a long, broad topic.
Well, I mean, I could give, so some technical problems we're still struggling with are
having a theory of counterfactuals. Can you tell us what that is?
Yeah, so it's a, it would be nice to have a theory of sort of like a general theory of what it
means to say, what if I take this action? Or what if I take that action? Like, what if I run this
code? Or what if I like throw this ball or whatever? It sort of sounds like a silly question
or something, but if you try to write down computer code to answer this question, it's like
a little bit less obvious how to do it correctly in general. It might take a little while to
motivate that, but that's one question that we're still stuck on. The transparency thing is like
a big obstacle. So I could, I could list more technical problems or I could talk about,
on the meta level, what's, what's, what's difficult about this?
Sure, go meta. We'd like meta.
So on the meta level of like, why isn't the research, why can't the research just be easy?
It's just like a, it's sort of like a weird domain where we sort of
working with objects that don't exist yet. Like we're trying to reason about very intelligent
systems. You know, we can do math and then we're interacting with real math,
real logical structure, but it's not clear whether the math is actually representing
what will happen in the real world. We can interact with real world systems,
like current AI technology, but then it's not clear whether the things that are important
when you're working with existing technology are the same things that will be important
when you're working in 10 or 50 years with much more advanced systems.
So it's sort of like we're trying to do science on things that we can't directly get our hands on.
That sounds like a major obstacle. How the hell do you work around that sort of thing?
Try really hard. Stare at the whiteboard for an hour.
We rely a lot on what, what sort of looks like philosophy in the sense that we like reason
at a high level about what we expect will be the important bits of some problem and then like
try to figure out what our assumptions are and like say, well, if we make this assumption,
then what will happen? And then if we make the opposite assumption, what will happen? You know,
and so on. Well, I mean, yeah, I think that that's sounds like a good, or that sounds like
the way to spend time, right? You're asking yourself, you'll say, all right, given this,
what happens? And then you pursue that for as long as until you feel like you've hit a good
stopping point. And then, okay, but what if we didn't do that? We did this instead. So, yeah,
that seems pretty straightforward to me. I mean, well, not obviously the details, but the, the
avenue, right? Yeah, yeah. So what are, what are the, any significant advances or changes in
thinking in the past few years? So Paul's work has been very cool. Another thing is, I guess it was
maybe towards at the end of this past summer, Mary released a paper called logical induction,
which was a major, major advance from our perspective in that it solves
what we were calling the problem of logical uncertainty or solved it to some extent.
I'll try to give a short explanation. So there's actually two kinds of uncertainty,
perhaps. One kind of uncertainty is empirical, which is like, I didn't see enough in the world
to know what's going on. So for example, if I, I show you a box and there's like two buttons on it,
and there's also some lights on the side, and I'm like, okay, if you press button A, which lights
will light up? And if I press button B, which lights will light up? And you're like, I don't know,
I can't see inside the box. How am I supposed to know which buttons hook up to which lights?
So this is an empirical uncertainty. It's like, you know what could be in the box. It could be
like wires or it could just be dirt or it could be whatever. But you don't know what's in the box
because you didn't see what's in the box. You're uncertain about what's inside the box. There's
a different kind of uncertainty, which is like computational or logical uncertainty where I
could show you what's in the box. I could show you in detail what's in the box. Like I could show you
when you press button A, the box is going to run this computer program that does these operations
and then light up lights depending on what happens. Even if you saw exactly what was in
the box physically, you still might not know what will happen because you can't like figure out what
the program will do. Like you can't perform the computations fast enough to know what's going
to happen. So you're still uncertain. So that's different than not just having enough information
saying, you know, if I had more information, I could make this decision better. You're saying
logical uncertainty is if I had more time or more processing power, I could use this information.
I have it all. I just can't work with it fast enough to give you the answer.
Yeah, exactly. Okay. Cool. And how did this help with that problem?
So logical induction was an algorithm discovered by Scott Gerber, which it's not at all a practical
algorithm, but it gives a theoretical characterization of like what we mean by doing well at logical
uncertainty, like what it means to do, what it means to like reason effectively about computations
that are too long for you to run. I'm assuming there's just an inferential gap there that's
impossible to cross in five minutes. Is that right? Yeah, that might be the case. That's
understandable. I suppose is this paper available for us to link to if anyone wants to give
a stab at it? Yeah, it's online. There's a blog post. Awesome. Then yeah, we'll let the listeners
decide if they can make heads or tails of the particulars. So what personally would you like
to see happen over the next, I don't know, five, 10 years? I would like to tie to that.
Not just what would you like to see, but what do you anticipate seeing? What are your expectations?
So what I would like to see is more, so first of all, more people thinking on the object level
about AI safety. So people like tackling technical problems that have been laid out
or people thinking on their own from scratch about how to do AI safety. I anticipate this
happening to a fair extent, but I also anticipate it being hard to like figure out what the field
of AI safety should look like and sort of how to judge advances and how to coordinate between
research groups. I imagine that coordination is probably, I guess, what I'm trying to think of
a more appropriate word than problematic. I mean, there's a lot of people I think that if there's
money to be made somewhere, but especially if it's something like building the first self-driving car,
if it's Google or Uber, someone's going to cash in big, right? So that's obviously not
a general intelligence, but somebody who is trying to win this race probably won't be likely to
help their competitors keep up, right? Or is that a bad guess on my part? Is there actually
more cooperation in the field than I would guess? So you're talking about the AI capabilities
research field? Yeah, I guess. Well, is it one unified field or are there different
institutions working on this that either are helping each other or are not helping each other?
You mean safety or capability? Capability, I'm sorry, yeah.
So capabilities, there's lots of research groups, right? There's like Google,
Google Brain, Google DeepMind, there's OpenAI, there's Facebook, Uber, yeah. I think there's
a fair number of companies that have substantial AI research groups and there's also academia.
That's my interpretation as well. I guess what I was curious about is are these groups helping
each other out? I imagine everything with Google is helping each other out, but if one company
makes a breakthrough, do the others find out about it when their product comes to market,
or do they find out about it beforehand and everyone's trying to help each other up?
I see. That's a good question. I don't know exactly. My impression is something like most
things are at least... Well, I guess I wouldn't really know because if it's secret, then it's
secret. I would sort of guess that a pretty large proportion of things are at least talked about
in publications. I mean, researchers like to publish results. Right, on that makes sense.
I guess I'm not sure what my... I might have guessed that there would be more competition
rather than cooperation involved, but it doesn't have to be really either or. And you're right,
like current secrets, if they're out there, which they may very well maybe we wouldn't know,
but I meant like, is there a history of breakthroughs only coming out after one company has quote,
one erase or something? Or is it... I think you kind of said that there's an impulse to
publish among actual researchers and other people can read their stuff and they want it to be read,
so that makes sense. Yeah, I think we're... At least for the time being, it's not the case that...
Like, I don't remember when the AlphaGo paper was published. I think it was definitely before...
And I'm making this up. I think it was before... It was well before they
had the match with Lisa Dahl, the Go champion, which... Just because they published the paper
doesn't mean someone can go and do exactly the same thing they did and scoop them. But right,
it's not the case that... I don't know that it's been the case that we've seen
external signs of deep mind, for instance, making large amounts of progress,
but not knowing what the technical stuff behind it was.
We did have this thing that we wanted to touch on, though, which you brought up.
I wanted to ask you, in your email to Inyash back here, there was this paragraph that jumped out of
me that I wanted to spend some time basically line by line going through. I don't know if
you can pull it up or if you remember it all, but you'd said, if I have an agenda, it's actually
not about that, which was AI safety research and popularizing that, I think, but you said it's
about the meta, namely promoting the message among EAs and rationalists of actually trying to figure
things out, develop your own inside view, build your own capacity, debug your friends, informed
giving, entrepreneurial spirit, and a handful of others. But that stuff, that really jumped out at
me and I thought that'd be a lot of fun to talk about. And it sounded like you wanted to talk
about it too, so I wanted to get to that. Yeah, that sounds great. Awesome. So we can talk about
however you want. You're welcome just to go through however you'd like to approach this. If you
want us to ask you each one and you can respond in turn, or if you want to just talk about it for
15 minutes, that's completely fine with me too, whatever you'd like.
What is it that you're most passionate about? What is it that really fires you up about this?
Well, I actually have a question. Do you feel like you're well grounded or well in touch with
the broader EA rationality community? Definitely in touch.
Definitely more so than the average person.
Right. The well grounded is an interesting question because I don't know how to judge that.
I'm not like one of the people who spends time on the forums discussing these issues, for example.
I don't even really lurk that much. My exposure is more through reading other blogs and seeing it
referenced and talked about there than actually being directly involved. But I could probably
follow what you're trying to say. If I can't, then I would stop and ask you for elaboration.
Oh, okay. Actually, I wanted to see if I could get information out of you that I didn't know.
Oh, okay. That is unlikely. But if there is, I can try.
Well, I might be sheltered. I wouldn't know if I were sheltered.
Or not sheltered, but like have my head in the hole. Well, so yeah, I guess I'll just jump in,
but you should interrupt me. In a word, the thing is ambition. It would be cool if
EAs and rationalists were ambitious about figuring out what's actually going on in the world,
how it actually works, what we should actually be doing.
What do you mean by what's going on in the world? Because the world is really big and
there's a lot of things going on in the world. Yeah, exactly. But I mean, so there's this effect
where we sort of just like humans by default, it seems to me, copy the ontologies used by other
humans in the sense that if someone has thought through what are the top five causes that EAs
should think about, not just the causes that they produce, but the ways of thinking about
cause prioritization, the ways of condensing the world into a small enough representation
that you can think about it with the human mind, are sort of like overfit to that person's,
to the way that that person is doing it. And then other people sort of copy that,
copy the ontology, copy the lens. Well, that is true. I totally agree with you. But on the other
hand, it's very hard not to do that because someone has already done all this hard work and
identified a problem that needs a lot of work, like for example, the whole elimination of malaria
thing. And you can spend the next however many years trying to find other ways to think about
this thing and find other causes. But when there's something as easy as not necessarily easy,
but when there's something as concrete and something that you can do things about as malaria,
it almost seems like it'd be for the average person a better investment to just try to go out
and fix that rather than if everyone were to spend all their time trying to think about better
ways to think about this, right? Or do you think that everyone should be doing that?
Well, I agree with that, like, certainly in theory, and also just like in practice. Yeah,
for lots of people, the right thing to do is like, make as much money as they can,
live a happy life, and like, give a bunch of money to the cause that seems good. But I think,
I think basically like, right, what I'm trying to do is like a pendulum swing,
where I think that the pendulum is way too far on a long bat axe, like in that direction.
I see that there's nobody left now thinking about these things.
Well, it's not no one, but it's like, inadequate.
Yeah. I think that's a fair assessment, certainly in my view. I don't know.
I mean, there are a handful of things. I probably came across like the idea of
combating aging independently before I heard about Aubrey de Grey, but I don't know if I
would have on my own thought of malaria as a great thing to work on right now with my money.
And I did sort of just delegate that work to other effective altruists,
as far as the cognitive leg work to say, you know what,
malaria, that's what we should be focusing on right now, or at least some of them, right?
So I think I see, is that, does that relate at all to what you were saying when you said
that you were interested in like trying to get promoting the idea of getting people to develop
their own inside view, or is that a different concept? Yeah, that's roughly the concept.
Can I ask you, like, have you done thinking along those lines of where best to spend EA
efforts, for example? Yeah, I do some of this. Oh, cool. I mean, it's sort of like bias towards
what's relevant to my decision making. Yeah. Do you mind if I ask what you think would be a
good focus? I'm super tempted to be like, the focus is actually figuring out what the focus
should be. Oh, okay. Oh, like, I actually think that. Okay. You think that there's other low
hanging fruit that we are missing because people aren't looking at for it? Yeah, I think that the
bottleneck is not really, this is sort of, this is sort of not true and sort of complicated,
but like, to a first order factor, to a first order approximation, the bottleneck is not
money and isn't even thinking of plans as such. It's like having people who can do
stuff and can do stuff that's important and hard and like a little bit weird.
That's a very good point because I know I, for one, don't really feel like going to a different
country and working on things that are really hard when I could be staying here in the US doing,
accounting or whatever and podcasting and hanging out with my friends. Yeah, I don't think that
that's bad. Like, I think it's great to like, yeah, doing things that you want is like,
that you want to do is like, probably a pretty great policy.
Right. If I understood what you said earlier about, you think that a good effort right now
would be actually thinking about what we should be working on rather than just like finding things
to work on, we should actually be dedicating actual time and effort into figuring out what we
should be putting time and effort into. That makes a lot of sense to me in that I could imagine
looking back in 50 years and being like, man, if only we had thought of this 50 years ago,
we could have taken care of this, you know, earlier on or it would have been a lot easier
50 years ago or something like that. Is part of your concern there that we might miss what in
hindsight was an obvious opportunity or something that would have been totally within our grasp,
you know, in the past, in retrospect? Yes, that sounds right to me. I mean, that fits
what's in my head. But like, to some extent, it's not that I think that we are lacking for plans.
It's more that I think that so like, maybe I think like in order to do stuff, you have to have
thought through, like you have to have thought through strategy yourself. Maybe I'm not sure
I believe that. It seems to me that coming up with a new thing to focus on de novo is, I mean,
it's nearly impossible. You would probably be better off just using a random number generator
and using that as a basis where most people, when they find something new to focus on,
it's because they are working in a particular area and they notice something that is, you know,
kind of relevant to their area or adjacent, which other people just haven't been paying
attention to. But it's not like, it's not like you can just come up with a problem that people
haven't been looking at from your living room. It's something you stumble across by being,
by working in a field, right? I want to say really quick, strong disagree.
Yeah. Certainly when you said might as well use a random number generator.
Okay. Like, I want you to weigh on this as well. But I think I can imagine a counterfactual world
in which no one was talking about aging right now. And it just, it didn't come up for another 20
years from now. See, I can't actually imagine that because people have been talking about ending
aging since, fuck, I don't know, the beginning of humanity. It's been a foundation of many myths.
But like, as far as like, this is actually a problem we should start attacking right now.
Like, I think it would be possible to have missed that, that mark. And so,
with maybe a little more reflective thought beforehand, they would have thought to do this
20 years ago, something. Oh, well, I mean, I think they should have started 20 years ago,
but I don't think it's for a lack of people having the idea. I just think it's for people
thinking, well, this is technically a biological impossibility. So why bother?
Well, I think that's the point, though, is that they're not spending enough time thinking about it.
Or, you know, thinking this is currently an impossibility. Let's change that. You know,
it's like, we'll see it and be like this intractable problem, kind of like in,
I hate to always bring up methods of rationality when we're talking about rationality stuff.
But like, every wizard's feelings about death, right? It's just this thing that they look at.
Oh, yeah, that's impossible. And they never question that. So I think that
it's certainly imaginable for me that someone or a possible world where people
went longer before they thought about attacking aging as a particular problem.
Yeah, but I mean, we have a lot of experience with death. It's something we rub up against
all the time. We would have never thought to implement latrines in the South if we weren't
people living in the South and seeing people struggling with anemia. This is referencing an
old book about, you know, when they started implementing large-scale public sanitation
projects in the South. That's fine. I think I'm using a real example because I haven't actually
put in this legwork that Svi was mentioning and thought of an original on myself yet.
So maybe I'll do that. And if I can think of something good, I'll get back to you and be like,
see, this is what I was talking about. I'm glad I thought about this.
It's not just like original is great. And also like, there's like a thing where like,
yes, we've thought we've like known about aging as a problem forever. But like,
there's a pretty big difference between like kind of knowing that it's a problem and like
actually doing, actually like making progress on the problem.
Yeah. But you're talking more about finding new problems to focus on. Yes?
Okay, that's, I think that's not really what I mean.
Okay, I misunderstood you.
I sort of, yeah, I sort of, so in the email, the last sentence in that paragraph was,
this claim is not well articulated in my head.
Is it more about doing research to make sure that we're focused on the right thing?
Well, maybe the real question is just, why haven't we already solved x for various values of x?
I mean, there's some reason. I guess when I trace that back, it doesn't feel like x is a problem
that can't be solved. It feels more like no one is really trying. How does that strike?
I think that makes sense. And that strikes me as, as, as sensible.
It's not like no one is really trying to cure cancer, though. But it's still isn't cured.
I think that's a different, at least I'm painting it differently in my head. I'm just
thinking of like, like you were saying, you know, aging has been an intractable problem forever.
I think it was probably likely that people thought of it as this will be an intractable
problem forever, long after it actually was an intractable problem.
All right. What about poverty?
That was probably a problem longer than, I think that was probably a problem that people
give me making progress towards fixing longer than, for more time than was necessary before
people could actually been working on it, right? Maybe. I think there, I mean, people have been
charitable forever too, but maybe not in an organized, useful way.
I guess maybe I just have a ridiculously high bar for really trying.
Like, like this is totally not about sincerity. I'm sure like many, many people are sincere
about various problems. I think I heard a interesting example of trying in the push
versus pull sense where a celebrity who is like outraged by what's happening in Syria,
they're like, well, what do I have? What tools do I have?
I am a celebrity. I have a microphone. A lot of people listen to me.
So I will get up and I will talk about Syria and I will say we should do something about this.
And that's more like push is I find out what strengths I have and I try to use these on the
problem. And that doesn't work very well because those strengths aren't very applicable to the
problem. But you can't say they're not trying. They're doing what they can with what they have.
Whereas the pull approach is what is the problem? The problem is that there is this civil war
happening in Syria. How can we fix the problem and you look at to what solutions are possible?
And then how can we implement these solutions? And then what can I personally do to help
implement these solutions? And then how can I improve my skills in order so I have the ability
to implement these solutions? And so you work backwards from what needs to be done
and then start working from there rather than working forwards from what do I have
and trying to use what you have. And one of them is a lot easier than the other. But
the pull one really seems to me like the one that is more likely to work.
Is that what you mean by actually trying? Yeah, thank you. That was great.
Yeah, back chaining from the goal to what actions would actually help.
Okay, so poverty. The following might be done because I haven't done any research.
I could imagine a story where it's like, well, poverty, why can't we solve it? Well,
we can just send them 10 people food. That's fine, but it doesn't release. Maybe it doesn't scale.
So you're like, okay, maybe we can make mass produced cheap hydroponic farms or something.
Maybe that's like a bad plan for various reasons. For instance, maybe it's a bad plan because
warlords will just like steal any resources you sent. That's like, all right, well,
how can we deal with that? No, I like that. I think that that's a different framing that,
and I hadn't heard it phrased the way either of you guys put it until just now. And I think
that's a really good great way of thinking about how to solve problems, especially if
it sounds intractable, right? It just sort of do your best to reverse engineer from like a space
from the solution and then say, okay, well, how do we get from there to here? Rather than just be
like, this is a problem and like that's going to be the only thing you do is just declare that
there's a problem, right? Or say the first thing that comes to your mind as far as a solution too,
right? The real problem with that is that it's daunting as fuck. It's like, not only do you
have to do tons of research and work to figure this out, you then have to basically dedicate
your life to chipping away at it. Whereas it's much easier to say, these are the strengths I have,
let me just push with what I got. So are you trying to advocate for people to be more willing to
bite that bullet and dedicate their lives to hard problems like that? Maybe, but at least at the
moment, I think no. I think it's a hard issue. I think that, okay, one hypothesis for what might
be the right thing to do in that situation, like I think it's, you're probably not going to solve
the problem unless you do that, basically, because like people have already tried the obvious things,
that's not really true. Like if you're going to make progress, you need to be doing something
weird and you're going to need to level up your skills. I do have a quite general policy of
only do things you actually want to do. So was the question more about how to want to want to do
something? Yes, but only if you want to figure that out. I mean, at this point, since you're
trying to, you already want that, so it seems like what you're trying to do is figure out a way to
make other people want that as well. I don't want to make anyone want anything. I guess, I guess,
like for instance, one thing I keep coming back to is just like, it's totally fine to keep doing
exactly what you were doing before. But let's just like acknowledge that it's pretty interesting
that like a line of logical reasoning that you can totally follow implies that your actions don't
make sense. It doesn't mean you have to be like, oh, well, here's this line of logical reasoning.
I guess I just have to like quit my fun job and like move to another country or whatever,
like stop doing things that I enjoy and start doing things that I hate, but that are like
the correct action. It's like an interest. It's just like interesting that
like preferences don't align with what we like explicitly reason out as a good idea.
Do you think that's because we have multiple preferences or because we are, as Robin Hansen
says, lying to ourselves so that we can signal our good qualities while remaining, you know,
doing the things that are selfishly good for our genes and pleasing to our emotions?
Probably yes to both. I mean, yeah, everything is super complicated.
So do you think people, do you think the world would be better if people
did accept the reasoning of their logical minds and actually do the thing that their ideals
tell them to do? The image that formed in my mind when you said that was not good. Okay. Like
accepting logical reason, like this is like probably reading too much into the language,
but there's this thing of like, should I accept this logical reasoning and do what it tells me?
Or like, why can't I get myself to go to the gym or something? You know,
there's like the I who thinks I should go to the gym or thinks I should like do AI research or
whatever. And there's the myself who like doesn't want to. The more interesting thing at least
to me at the moment is like staring at that and trying to figure out like, what's the deal that
like, I don't know if this is even possible, but it seems like it might be possible to like have a
person who your logical reasoning is directly hooked up to your system one, like directly hooked
up to your like visceral desires. Can we quickly explain that system one is basically your visceral
desires and your things that you want and do out of reflex, right? And your automatic judgments?
Yeah, system one is a very broad catchall term, which sort of like obscures lots of differences.
I mean, there's like fast heuristic reasoning. There's like fast visualization. There's like
deep desires. There's like, you know, you can go on and on. There's like lots of stuff.
It's the more subconscious stuff. Yeah, yeah, it's a catchall term. Yeah.
Like so, so an example of key coming back to is chess. Like chess involves logical reasoning.
You're sort of like playing out variations in your head. And trying to reason like, well,
my king side is weak. So I should move my knight or my bishop over to protect it or whatever.
Like, how can I get a queen to this position? That'll be a good attack or something. But there's
but at least when I play chess, it's not like I do some logical reasoning and I come to a conclusion.
And then like another part of me is like, uh, yeah, but I don't want to. There's a thing where like
the logical reasoning is directly hooked up with the desire to win.
Such that it's like, it's not even a dialogue. It's just like a full you're like a full stack person
where like the logical reasoning is like, like the system one desire to win
has like climbs into them to the like big robot mech suit of the of like system two explicit
reasoning logical reasoning. Is that what you're is that what you're hoping to get to to be able
to align the two systems on in all cases? I think my ontology is probably wrong. So like that,
that that vision probably doesn't make sense. Well, I mean, it might a prima facia. I don't see
why it wouldn't make sense. I can see that in the actual implementation, there could be problems
if we have conflicting terminal values, which I think we very well might in inside us. But
on the face of it, we may not necessarily. So that could in theory be a thing that could work,
right? Definitely in theory. Yes. I definitely don't know in practice. So we talked, I don't know,
a while ago, not in this conversation, but about the right thing to do after a good logical argument
and what you want to do slash will end up doing anyway. And you know, like this can be listed
with like, you know, a pretty heavy handed thought experiment, like say the house is on fire,
and you're running a daycare. And for some reason, all of your neighbors kids are in a stroller right
by the door, and you can get them all outside safely, or you can run to the other room and
grab your kid and get it outside safely. There's a difference between like what you ought to do,
you know, four babies saved greater than one baby saved, therefore say four babies versus like what
you as a human being could live with and would want for, I guess, your own mental health and
I guess, like generalizing some rules, like, you know, always save the four and forgo your own
would just not really work with like the way the rest of your brain is wired. And I don't know
if you'd even want to rewire yourself to be different given how that would just cascade
through everything else. You know, if you didn't really care that much more about your own kids
than your neighbors kids, you might not want to not want to change the kind of person who would
be that indifferent, right? I think if society is set up well, they would want to encourage you to
do that though. Because from every the outside view, everyone else is going to say four babies
are better than one. So they in a good society, there would be some kind of I would assume if
this is my own personal bias speaking, maybe I'm wrong. But I would think in a good society,
there would be a lot of like hero worship that would go towards someone who saved the four
neighbor children instead of their own child and a lot of praise of that person and how great they
are. And maybe that would help encourage people more to to overcome their own biases towards
their own kid. Yeah, and that might help like just help you overcome the more definite grief of
losing your own kid versus losing four stranger kids, right? But that's just like part of our
wiring, right? We could try and compensate that with society. I don't know if this is worth
diving into or not. But there's another way to like frame like the standard trolley problem where
you're at the you're at the train tracks, you can pull the switch to divert a train track
to kill only one person because it's currently on track to kill five. I'm paraphrasing because it's
I'm sure most people have heard of it. If not look up trolley problem. The standard framing is
that you're the person at the switch. I think another way to think about it is you don't know
which person in this scenario you are, how should people act, right? And that's sort of,
I think, what you're getting at that there's a different, I guess, I don't know if I'm getting,
I think I'm talking in circles. I'm going to just... No, I get what you're saying. You as an
outside person, you want him to pull the switch to divert it to just the one person instead of the
five. Yeah, or, you know, the alternate scenario where they're pushing somebody in front of it,
right? You know, if you're the person being pushed, you don't want to be the you don't want to do
that probably or you might because you feel like saving five is worth more than your own. But like
we're like, like those also kind of just pump some intuitions that we're not necessarily built
in ways that conform to the two solid logical reasoning, right? Not to say that we shouldn't
work to better ourselves, but just the fact that you might just be psychologically ruined for life.
I feel like we're getting pretty far afield here. Sorry. We sort of do that, but I try not to do
that when we have guests on the line. Hopefully I'm completely checked out.
Well, I thought that I thought the issues you were raising are pretty interesting.
Yeah, I mean, there's like the thing I thought that actually I couldn't I'm not sure I knew who
was speaking. Maybe it was Enash was sort of giving a decision theoretic argument for pulling the
switch or like a Rawlsian veil. That was me, dibs. Sorry. Not letting Enash steal my my
intellectual credit there. Oh, yeah. Although I didn't actually invent that idea. So
I stole it too. Anyway, sorry, if we have to credit Rawls, we'll do that.
It's interesting that that's actually, I mean, maybe it's not interesting that at least that
it seems to me to be a distinct argument from the argument about like societal norms.
Oh, yes.
Like there's there's like, I think I don't want to quit my fun job. But I think
decision theoretically, I ought to do this other thing, because people in my I want people in
a situation analogous to me to my situation. I want them to behave in some way.
There's a different thing, which is like society has like installed a norm. And I'm just going
to like follow this norm or else I'll be ashamed. I think that's I see the distinction you're making.
But as far as I don't know necessarily if it's either or, you know, it's it's not like
your life is best spent. I'm not sure what example we're using as an alternative to like
quitting your job or, you know, the alternative to doing what you're doing and
doing some charity with your current lifestyle versus, you know what, like leaving the country
and doing something else. But even if there was like another optimal way that you could maximize
utilize on earth with your life, but it would make you miserable. I don't know if that's the
kind of thing you'd want to generalize either, right? Like, I think that there's there's a midline
to where, and I think that's kind of what effective altruism is all about to where, look,
live your life, be happy, but do as much good as you can while doing that. Don't give, you know,
like the logical implication is kind of like Peter Singer, right? Where you give until it hurts.
And then when you're suffering as much as the people that you're helping, then it's okay to
stop giving. Or, you know, if you're suffering to the point to where there's, I think the obvious
caveat to where, you know, if you can't afford your car to keep going to work to keep earning
money, well, then that's where you stop, right? Or whatever that point is. But that's not what's
good for our personal psyches. You know, that's exhausting. We don't necessarily feel like we're
getting everything we want out of life. So it corrects me if I'm wrong, but I feel like the
whole EA goal is to find that midline. Well, I can't. Yeah, I don't know what EA thinks.
That's fair. Yeah, I didn't mean to make you the spokesperson.
As a blob. But like, at least for me, I guess, so I mean, the thing you said might be right
two things. The less important thing is like, it still does just on general principles feel
sort of bad to have this situation where it's like two systems that have like made a treaty,
like made sort of a tense treaty where it's like, well, I want to like be happy and do
fun activities. Also, also, I'm going to like bow to the social norm of giving as much money as I
can. I wish that was a social norm, but no points taken. It's a normal. That's right. Yeah,
this is probably too much of a tangent, but I'll just like plant a flag that I'm sort of wary of
I'm wary of norms. I'm wary of norms as like as a mechanism of it's sort of like exerting power
over. Yeah, well, I mean, I totally know what you mean, but that's that's exactly the point of
norms. It is to exert power over people. And so do you think that there should be, I mean,
what what methods do we have for exerting power over others? There's social norms where we,
you know, shame them and use other other psychological things. And then there's direct
physical violence. I mean, I prefer the social norm ones, honestly. But are you saying that
no one should have any ability to to modify others through things like social pressure?
Well, I agree that it's better than violence. I agree that you need them for a large class of
things, for example, having a norm against violence, or having a norm against like wandering
around the street yelling at people's faces or something, a norm against like barging into
private meetings, whatever, you know, like, yeah, I mean, it's not that no norms make sense. It's
more that like, like if we're talking about the the ways that EA as a movement or rationality is
as like as a community or movement or like ideology or whatever, if we're talking about the
ways that those things are going to actually like make a difference, like actually actually change
the course of history, that is that seems to me to be heavily, heavily biased towards things that
are like weird and hard. And doing things that are weird and hard seems to me to be very destructively
interfered with by norms, or like, yeah, by being beholden to norms. Okay, so our human nature is
going to fight against these norms is what you're saying? Norms can go against what our system one
wants. They can also go against what our system two wants. They're just like a whole other system.
I mean, is there like a system three? Yeah, this is sort of a joke, but it might be it might just
map on correctly to super ego ego. I don't really I haven't read Freud so I don't know.
Yeah, like super ego is shame, norms, social roles, etc. Ego is like, conscious detention,
verbal explicit reasoning. And then it is like deep desires. So so you're saying that we should
examine the norms more or that norms in general are bad or that norms in general should be
examined much more because they can be destructive? Like, well, so I don't have an opinion because
I'm in the like figure things out stage. I don't know what the right thing to do is but I'm like
very, I'm like, a little scared and like very wary of I think that that makes sense. I mean,
certainly, like, norms work as a great shorthand to like the word not all reinventing the wheel
every, you know, every human life or every five years, however long you want to examine these
things, you know, we shouldn't spend, you know, even an hour, well, maybe, I don't know, like,
we don't want to spend a lot of time thinking, man, how about that that norm against like killing
and eating eating people, you know, some things make sense to leave. But I think that norms are a
great super convenient path to laziness, right? Oh, this is this has been established, you know,
so we're not we're not going to even think about it. So I think that's that to me is the big concern.
And like, you know, this kind of ties back in loosely, you know, thinking about like, aging
sucks, and it's intractable versus aging sucks. And let's try and do something about it. Like,
the norm for a long time was it's intractable, it's impossible, it's not even think about it.
It took somebody at some point to say, no, fuck it, let's actually see if we can why is it intractable?
So I kind of get the feeling that we are being very circuitous and saying meta things without
saying object level things. I guess what I'm getting at is it seems to me like there is at
least one specific norm that is common that you have a problem with. And I want to hear what that
norm is. It's the eating people thing, right? Well, I sort of two processes were running in my
head. One of them spit back. Well, the norm of like thinking that when you have reached a logical
conclusion, then you should immediately force yourself using like willpower overrides to do
something that seems like a pretty strong pervasive norm and also seems pretty destructive.
I have seen some pushback on that norm lately. I think the first time I saw it was a while ago
with one of Scott Alexander's posts back when he was still posting unless wrong as Yovane
Inyash here with a quick interjection. It was actually Phil Goats that wrote this article,
not Scott Alexander. It's probably this sort of attribution error that leads to Abraham Lincoln
having said everything. Sorry about that. And thanks to our awesome audio engineer, Kyle, for
catching this where he pointed out that this is the sort of thing that gets us terrorists,
that most terrorists are actually very well educated and more of the engineer geeky type
that we're used to who instead of when you're a peasant or just some normal person going about
their day-to-day life, you have these religious writings and they say some crazy things in them.
But for the most part, you're like, yeah, this is my religion and I'm just going to go through
my life and do some of the things it says, but ignore the obviously crazy part. Whereas once
you're an engineer and you've gone to Western schools and you learn things like the importance
of rationality and logic, you come back and you're like, well, when I examine my scriptures,
it makes these claims and these are true claims and obviously therefore it follows that I should
blow myself up or something. And so it's taking that sort of logic and following it and embracing
those conclusions. And he called it, I believe, memetic immune disorder, that any memetic system
has this immune system in it as well that attacks destructive bad ideas like let's become jihadists
and keeps them far away from everyone else because there is in society this general immune disorder
that's evolved over the centuries to prevent that. And when you introduce rationality into it without
restarting completely from the very beginning, you have sort of an immune disorder where that
immune system is killed without the original ideas being neutralized and therefore it was kind of
a decree of let's not just immediately go with, we have logic ourselves into this is a good idea
and therefore we should go with it. If no one else is doing this, there might be a good reason for
that. Just a paraphrase, it sounds like the claim was that a lack of indoctrination in the
virtues of logical, I guess of logical reasoning and following logic to its conclusions is one
factor in the small number of religious fanatics. Yes, that most people are just regular people
getting along because of the memetic immune system. So I can see a case for that being made. For
example, the standard case against abortion is like, oh, you're killing babies. If you really
believed that there'd be a lot more people shitting abortion doctors. Yes, the logical thing is
there are murderers that have murder shops that open to fucking stop them. I mean, if there was
an actual murderer that just took people in and killed them for money, we would as a society go
over there and burn that shop down. Huh, I'm curious if you'd weigh in on this. Yeah, that's
really interesting. I guess I don't have a model of terrorists. Right, I do, and mine's a little
different, but I can see totally where that's coming from, but I don't know if I'm on board,
I don't know if that's the ingredient or I mean, I think there's other factors.
I guess what I'm saying is I feel like there has been some pushback over the last few years.
A lot of it spearheaded by Scott Alexander saying, hey, maybe just logical rationalizing our way
through things and following those things that we decide on is a bad idea for reasons that we
aren't sure of yet, but we should just kind of respect that. We don't know why exactly it is
that we like to do this crazy thing and we avoid doing this other thing, but it's worked for
millennia, so let's stick with that even if it looks irrational and stupid. That's a very,
Scott Alexander kind of argument to make, I love it. It really is, and it is also very
unsatisfying because it's the same thing that leads you to keeping the snitch in Quidditch,
even though it is stupid. It's, well, we've done it for so long and we don't want to mess with that
because there might be very good reasons for it. Yeah, it's don't rock the boat mentality,
but the boat needs rocking, so it's really just like, I don't know, maybe not give,
I don't know. Well, it's a problem with transparency. The boat obviously needs rocking,
but since we're not transparent, it's hard to say exactly what effect the rocking will have,
and sometimes it's really good, like when we outlaw segregation and other bad racial practices,
which were just bad. There was no reason for these things, and then other times the effects
are not quite so good. Yeah, fair enough. And it's really hard to tell beforehand which is which.
That's the kind of problem we should be thinking about, how to tell which is which beforehand.
Okay, yeah. Yeah, I think it's all very confusing. Like my handle for this is Chesterton's fence,
right? Like you come across a fence in a field and you're like, well, I don't, I want to like
farm the other side of the field. I'm just going to get rid of this fence, and then you find out
that there's dragons on the other side of the fence, and that's why the fence was there. You
shouldn't like tear down the fence before you know why it was there. So if there's like a
fence around taking violent action because your ideology has implied that you should,
then like you should, even despite all that logic, you should still take great pause before
it's coming to violence. No, that's a solid way to think about it. I like that. So I guess you're
advocating for more ways to figure out what actually is harmful and what is useful? I think
this was slightly tangential to the thing I was trying to say, but I also like this.
I'm sorry, I think I brought us down that path. Did you want to re re, what was the
the original quote that he sent us in the email? Well, I think we've touched on a lot of them. The
one that we didn't talk about, but you mentioned very quickly in passing was like debugging your
friends. I don't know if you had more to say on what we were just talking about a moment ago,
if you wanted to move on to that, but I was curious about, I think one of the first conversations
we had on the podcast was raising the sanity water line, which is not all of my friends are into
this. And so how do you, what is your approach maybe to debugging your friends?
The motivation is like, actually, like we don't know what the right things to do are,
even if we did know what the right things to do are, we're like not strong enough to do them.
That's like fine, or it's not fine, but it's like, it's not a thing to just like be ashamed of
and then not do anything about, which is sort of like my default, which is like sort of,
it's sort of upsetting because like we all want to become strong. Well, not real, like a lot of
people want to become stronger and do more cool stuff and do more important stuff, but like
it's actually pretty hard and you actually need help from your friends. So like, yeah, I mean,
that's the broad. So this is a supporting someone rather than imposing debugging. Yeah,
my most central principle is like, don't do things to people that they don't want to have done to
them. I mean, even if I'm talking about AI, AI risk, I sort of like, I'm less paying attention
to the logical arguments, because the logical arguments are like pretty ironclad, at least in
my opinion, but I'm more paying attention to like, what whether the person actually wants to hear this,
like what they actually want for their lives, like, if they already want to like do the most
important thing, and they just like haven't come across these arguments, then I'll happily like
go through logic. But if they have a good life, it could be that they're susceptible to the like
social norm of following logical reasoning, and then doing the actions applied by the logical
reasoning, or at least feeling guilty if you don't do them. If they have that norm, it's just like
straight up kind of productive for me to like, ruin their day, go into their yeah, like go into
their head and install this like, okay, so you because you feel like all you'll be doing is
giving them more guilt rather than helping them. Yeah, and it's not it's not it's usually not
it's not that useful information, like they could figure it out themselves, or like read
superintelligence or whatever, the book by Nick Bostrom. I have a couple of examples to
give that I think tie into sort of what you're saying with as far as being able to be supportive of
people in of the right mindset in a way that makes everyone better off, like Will McCaskill,
the director of the Center for applied or the Center for effective altruism. He apparently had
some years after he read Singer and was fully on board with like the I should feel bad all the time
because I'm not a good enough charitable person after reading that where he had those years of
guilt. And then he met somebody who was I can't remember exactly whether they were both feeling
guilty and they both decided decided to do something about it. Or I think where McCaskill
was giving silently like he's giving a large portion but he wasn't telling people about it.
And like it didn't really do much to make him feel that much better until he met
one other person who was doing the same thing and then they both just hit it off
and they were they finally weren't you know kind of just like alone doing this.
Now he had a community, he had a tribe.
Exactly. And there was a recent episode of Julia Galev's podcast, the rationally speaking podcast
where she interviewed somebody who donated a kidney. And he had said, yeah, I feel like I did
the right thing, but I wouldn't have done it if I didn't know these two other people who already did.
So I think partly one of the one of the good things about I mean, depending on what you're
and like you said, case by case basis and what means you're talking about installing in people.
But the community aspect makes it a lot easier to do a lot of these things.
You know, if you're the only person who cares, then you know, you have no one to share your
A, your enthusiasm for caring and B, I guess all of your other like all your reasons for
caring about it too, right? So that community aspect that that support network seems to
really helpful. Am I hitting on what you're trying to get at all?
Also, don't you think that if the person was trying to do the most good that they would be,
at least in my opinion, I would be thankful if someone helped me see that this is one of
the most important things to work on. I wouldn't necessarily, I might give more, but just having
the knowledge and maybe reallocating the money that I was giving would be useful for me.
And it would be, you're not necessarily giving them the burden of shame.
You're giving them more information that they can do things with, right?
Yeah, totally. If that's like a thing that they're looking for.
You might be the kind of person that it's safe to do that up that that install on,
but other people might respond more adversely, right? They might just feel worse or something.
Is that?
Well, actually, well, like I guess an install is sort of I sort of use it as a pejorative term,
at least in my head. Information sharing amongst friends makes sense, but like
installing things, installing a meme in someone, I am like much more wary of that.
So what is the difference between installing a meme and sharing information?
It's sort of hard to point to it. It might be incoherent, but like in my head, it's sort of like
one way I would say it is like, if the other person has the agency in the conversation,
then you're less likely to be installing a meme.
Okay. So it's more of like a dark arts kind of thing?
Dark arts would be the most obvious version. Yeah. Yeah. Like if it like I have multiple
modes, like I guess by default, if I'm like arguing with someone, my stance is like,
I know the right thing. I know all the stupid ways that people don't understand the right thing.
And I'm just going to tell you them so that now you'll know the right thing.
It's like it's sort of it's from one perspective. It's like an uncooperative
way to have a conversation. It sort of makes sense. But if you're giving them the tools to
understand it, that's more of a friendly thing. Whereas if you are trying to trick them into
believing this without giving them the tools, then that's that's a totally different thing.
Right. Even if they come to the same conclusion either way.
Yeah. I think I've had a fair amount of trouble talking about this sort of thing in the abstract.
Do you have concrete examples that could make it a more, more understandable?
Yeah. It's a good question. Well, like, let's see. I mean, here's sort of like a simple obvious
example. Like if I'm talking to someone about AI safety, and we like, we like go through the
arguments, and they're like, Oh, but I they like give some excuse why they why they shouldn't do
stuff about it. One thing I could do is like attack that excuse, you know, you know, like be
like, but no, you're wrong. And then they give another excuse. And I'm like, no, you're wrong
because of that. This feels pretty uncooperative, because like, I sort of assume that the thing
that they're doing in the conversation makes sense from some internal perspective, like including
like including deflections. So it almost seems like you're pursuing an agenda instead of just
giving them the tools to make themselves better. Right. Like if my agenda is like, they should
leave the conversation thinking that they should go and do such and so about AI safety.
Like if that's my agenda, then I'm being uncooperative, or at least it's uncooperative
to not say so. And there's, there's, I mean, there is a norm in the in the rational community
that that is a very bad thing. And I take it that you are on board with that norm as well.
Oh, there is. It seems to me that there is that the trying to to doing that sort of thing is bad
and that you should be open if you are trying to change someone's mind in that way to be open
about it. Yeah. As opposed to deceiving them. I didn't, I didn't know about this norm, I guess. I,
I mean, I, I don't know if I've seen it practiced very much. I mean, I mean, deception is sort of,
I agree. I agree that there's a norm about deception. Is there, is there a norm about like
saying, here's my agenda. And then if the person's like, okay, I don't want that agenda,
then you just like don't have the conversation. I, are you, are you saying that sometimes
conversations can go like, Hey, I want to tell you about AI safety. And my ultimate goal here is to
get you to donate money to Miri. Are you saying that's how conversations start or ought to start?
Or that is the norm? Or what are you saying? No, no, no, I'm not saying that, but I get the feeling
that, that it's considered bad form to have that as an actual goal without telling someone that that
is your goal. As opposed to just, Hey, here's a thing that I'm concerned about. Maybe you'd be
concerned about it too, after I talked to you about it. I think I see, is there a, is there a
difference? I think I see the difference a bit, but I think that it's desirable that, that, that
step or that first option, but I don't know if I see that practiced either. Yeah, no. You know,
often I, I mean, just to completely throw this into confusion, you might not be clear what your
goals are before you even into the conversation. You might just be talking about it. And yet you
might accidentally get these things in people's heads or something, right? Well, okay. So what,
what came to my head was the recent kerfuffle around a post called EA has a lying problem,
where someone wrote a very long post about, Hey guys, it seems like people are not representing
data that makes, are not putting forth data that makes them look bad and only saying things that
make them look good and would incentivize people to donate to these charities. And it seems even
more so, like they know this and behind the scenes talk about it and say things along the lines of,
for most people, this is not pertinent information and it will just confuse the issue for them.
So there's no reason to bring it up because we know that this is the right thing to do anyway.
And there was a huge, not huge, there was a significant blow up about that where people
started really questioning themselves and a lot of the EA things that are done. And
because this was considered very bad practice, go ahead and be incredibly open about things and
make all this data available. And I don't think there was any real malfeasance. I think there was
a little bit of people in some people in the leadership starting to acknowledge that
there is a level of diminishing returns of complete transparency and saying it poorly.
So I don't think anyone was intentionally being bad and that there wasn't really a
lying problem, but I thought it was very interesting that as soon as someone picked up on this,
they made a very long public post about it, brought it to everyone's attention,
and the entire community jumped on it and started talking about it because that is something that
as a whole is anathema to the community, that we want to avoid that sort of thing.
We do not want to be like the Coleman Breast Cancer Foundation where they think anything that
you can do to increase funding to our group is good because our group is doing a fundamentally
good thing. And I was very heartened when I saw the movement doing that because to me it felt like
even if it does sometimes stray off course and starts getting into darker area, it has a very
quick and pretty good self-correction mechanism. I mean not incredibly quick, it isn't like days
later, but certainly nothing that goes on for as long as something like the Coleman Foundation
does. It was picked up on and publicized pretty quickly. And that's where I get the feeling
that there is more of a norm to be open and to tell people if you're trying to modify them in
that sort of way. Are you familiar with the EA has lying problem thing? Yeah, I think I
at least saw it and read about it. I probably read some of it and all of it. I didn't get
super deep into it. Well, so the lying post, the EA has a lying problem post was a demonstration
that there's at least some norm, but I guess also that the norm was not sufficiently widespread,
at least allegedly not sufficiently widespread to have already been the case.
Like it's such that it wasn't obvious to multiple people that it's very defective,
very non-cooperative to just intentionally deceive. Yeah, I see what you're saying.
Okay, so we are almost out of time. Stephen, you had some last minute question?
Yeah, last minute question for you, just a double whammy one. Is there anything that you want to
share that we didn't ask about or that you wish that we had and that ties into, is there anything
that you want to plug slash share that we didn't ask you about? That goes without saying too,
that any links you want us to share, you can send us, we'll put on the website too.
I didn't think about this in advance. If anyone is in the Bay Area, there are community
talk nights sometimes at the Seafarm Area office.
Do they share an office? Oh, have they always shared an office?
At least for a couple years, yeah.
Huh, today I learned. I for some reason thought they were like,
I guess, doesn't matter, we're running out of time.
So, is there a website or a link or something that you can share with us to how people can
find out about when those are? Last half of my head, I'll check Facebook.
Oh no, there's no rush. You'll send it to us and we will have it posted in the show notes.
And you have a couple of weeks before this goes online anyway, so yeah, take your time.
I guess as long as we're wrapping up, time-wise, I really appreciate you spending all this time
with us. It certainly, I think, is a very much appreciated use of your time and I know that,
you know, you have stuff to do and this was, thanks again, I guess I'm expressing a lot of
gratitude and I'm doing so poorly. Thank you. You're welcome. Thank you for having me on.
This was fun. Any final things you want to say before we go?
Nope, that seems good. Okay, well thanks for coming on and for
bearing with us with our technical issues last week.
Yes. And we'll hopefully talk to you again sometime in the future.
Great. Thank you. Awesome. Thanks again. Bye-bye.
