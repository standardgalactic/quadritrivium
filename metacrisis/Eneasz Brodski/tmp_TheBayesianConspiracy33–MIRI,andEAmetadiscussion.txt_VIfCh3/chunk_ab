I mean, I mean, certainly we do some machine learning stuff that's not the main focus yet.
So can you tell us, I mean, why this is really important or is it really important?
Sort of from like stepping back and looking at sort of where we want the world to go,
how we want the world to evolve over the next like 10 or 100 or 1000 years,
one of the key factors is intelligence.
That's the thing that steers the world in particular.
There's like lots and lots of money and talent pouring into
creating computers that are more like creating programs that are more intelligent than humans.
So it's like pretty relevant to everyone's goals, whether or not that goes well.
So right now we have computers that can play chess and go better than humans.
We're right at the edge of having computers that can drive cars better than humans.
There's a lot of things computers can already do that's better than humans.
What's the big deal so far?
It seems to be like this has been a great thing that computers are getting better than us at certain tasks.
Right. So this is like sort of still in the paradigm where like the humans are the top dog.
Things like if I look into my crystal ball, it seems like things will change as AI systems
can make sufficiently like domain general decisions that are on par with humans.
I mean, it's sort of hard to speak about the future because it's like
I have very wide error bars on all my like estimates,
but I could like give scenarios where it becomes an issue that as
are smarter than humans.
I think that sounds fine. I think for what it's worth,
I think Enosh is playing the skeptic just to drive the conversation.
Right. I'm trying to find out why a listener who is not familiar with the
Miri would be worried at all that this is something to worry about.
Oh yeah, I guess I just feel like I'm sort of speaking in like abstractions.
I think that's fair because I mean, we don't have concrete examples to point to, right?
You're talking about, I mean, right now, yeah, we're top dog.
We can still process all the decisions that any AI that we're building,
well, I guess for the most part, we're not yet consulting AI's for decisions about
important things that it's going to give us answers that we have no idea how to interpret.
And so, you know, as long as we're top dog, it's not that scary.
But if we're building the top dog that's going to be usurping us,
we want to make sure it plays well with us.
I'm interested to hear the examples that you're thinking of though.
Yeah, okay, so like one example, your basic like AI breaks out of the virtual machine
that's running on scenario.
I mean, you could imagine that an AI system has goals, like its goal is to cure cancer.
And it like is sufficiently intelligent that it can model the world around it.
And in particular, the model of the computer that it's running on and model the fact that it could
make a buffer overflow attack, gain like root access to the computer it's on,
gain access to the internet, transmit its code over the internet to other computers,
take over those computers, and then use like this vastly more computational resources
to achieve its goals even better, which is what the humans apparently wanted.
Couldn't we turn off the computers that it's on?
You could further imagine that the AI like has a model of the shutdown button,
understands the fact that if the human presses the shutdown button,
then the AI won't get to pursue its goals.
So if the AI is currently pursuing its goals and has a model of this fact,
it has a very strong incentive to disable the shutdown button.
So Neil deGrasse Tyson, I think would say, I'm channeling my inner, his version,
and he would just say, why don't you just shoot the computer?
Or turn off all the computers in the country.
I mean, if it came down to that, I think his, I mean, especially if it's just black boxed,
it might have a shutdown button that's like an emergency stop,
or it's just literally plugged into the wall.
And when you unplug it, you know, there's no, he would just, I imagine he would say like,
well, you know, he's electricity, we control the electricity, we win.
So in the worst case scenario, we could shut down every electricity generator in the US.
And so every computer would come to a stop.
I don't see him, I don't see Tyson granting that it could get that bad.
But it's not an entirely bad strategy.
It's just like, I actually don't know how to implement that strategy like feasibly.
I mean, so one thing that might be said is, well, the human will watch the AI as it thinks.
And if the AI starts to think about disabling the shutdown button or,
or like disabling the power grid, big red lever thing, like the human watches the AI,
think this, and at that moment shuts down the AI.
But this is like not, this is sort of like not a real, this is not a real design.
Especially since we apparently will not be able to see what thoughts I think an AI is thinking.
Yeah, we don't know how, we don't know how to do transparency.
Like we don't know how to know what the AI is thinking.
We don't know how to like scale this thing of having a human watch over the AI.
As I mentioned, if the human is just sort of watching every decision that the AI makes,
this is like not gaining you that much, you're still like taking up the human's time.
So you need some way of like leveraging human oversight more efficiently.
And it's like not, it's not at all obvious how you, how you do that.
So what avenues are being explored to do this?
Right. So there's, there's research on how to make AI systems more interpretable, more transparent.
There's a little bit of this research already existing in mainstream machine learning.
Largely, I think just because people want to know what's going on in their AI systems,
I think you can Google like what a con, what a connet learns or something like that,
which visualizes like if you have a neural net that classifies images,
you can see what a node, what features a node is responding to in an image.
So that's kind of cool. Largely, it's just entirely open to make AI systems have the property that
when it outputs a decision, you can be like, okay, why did you actually make that decision?
What factors influenced that decision?
What's, what's your reasoning? How could it have gone differently? Et cetera.
Well, I was just going to say like, I feel like anyone who's still skeptical of, yeah,
why can't we just read his thoughts? Why doesn't it just print it out in English on the screen?
Well, I mean, if you're not a computer person, it might not be super clear why that's super
impot like borderline impossible with what we can do now as far as I understand.
But the other, I think, impediment to that is even if you're equipped to understand,
you know, again, AlphaGo's search tree of how it came to make a certain decision about a move,
if it's a, if it's a more broad machine and you're going to try and just follow its thoughts
as it's thinking them on the screen. And even if it's, even if it's coming to you in human-friendly
English, the machine's still operating way faster than you are. And so you're running on 150 hertz
and the machine's running on theoretically 300 million. So it's, there's just going to be so
much more content there than you could ever hope to cover.
Well, imagine even just trying to watch a human neural network work like a regular squishy brain,
even though it's thinking at the same rate you are, if you're trying to watch all the little
neural nodes lighting up and interpret what that's happening as signals are shooting back and forth,
it's going to take you a lot longer than someone just thinking things.
Even if you were just like listening to their thought stream in a way that they didn't even
have access to themselves, it's going to be just this huge mess, right?
Because you don't get a thought stream, you get a map of which neurons are being lit up.
Yeah. And by analogy, I was just imagining he had some magic way of listening to their thoughts.
But even if you could, and it was like in your understandable English,
it would still just be this huge mess, right? And if you ask a person why they did a thing,
they don't really know, but that's sort of, that's different than a computer. A computer
should know why it's doing what it's doing, but it's going to be very complicated. Am I
just making it all technical accuracy sense? Yeah, it made sense to me. I mean, right,
as you're saying, we sort of hope that you can do better with an AI system than with a human brain
by default, because the human brain is not at all optimized for transparency, whereas we can,
if we want, try to optimize for transparency. Yeah, the human brain isn't even optimized
for transparency to itself. I mean, you can get people to confabulate answers for why they did
things, even when you know with 100% accuracy that they're making it up. There's all those fun
experiments where... Split brain ones? Or the split brains, or even where they've, you know,
they're intentionally stimulating parts of your brain and your hand goes up. And you ask them,
hey, why'd your hand go up? Oh, I was waving for a bus. Those are the kind of answers you get,
even though they're sitting in a chair in a lab, right? So the person clearly has no idea why
their arm went up. When they ask their brain, why'd you do that? They get back some bullshit answer
that sounds believable to them, even though it really shouldn't, right? So you're right. I think,
ideally, an AI would be easier, or would be more transparent with itself and hopefully with us,
but there's, I'm assuming, just way too much to see. And it's coming at you too fast.
Oh, so what are some of the major limitations that are cropping up in this
field of making AI's friendly right now? Major limitations.
Yes, I know this is going to be a long, broad topic.
Well, I mean, I could give, so some technical problems we're still struggling with are
having a theory of counterfactuals. Can you tell us what that is?
Yeah, so it's a, it would be nice to have a theory of sort of like a general theory of what it
means to say, what if I take this action? Or what if I take that action? Like, what if I run this
code? Or what if I like throw this ball or whatever? It sort of sounds like a silly question
or something, but if you try to write down computer code to answer this question, it's like
a little bit less obvious how to do it correctly in general. It might take a little while to
motivate that, but that's one question that we're still stuck on. The transparency thing is like
a big obstacle. So I could, I could list more technical problems or I could talk about,
on the meta level, what's, what's, what's difficult about this?
Sure, go meta. We'd like meta.
So on the meta level of like, why isn't the research, why can't the research just be easy?
It's just like a, it's sort of like a weird domain where we sort of
working with objects that don't exist yet. Like we're trying to reason about very intelligent
systems. You know, we can do math and then we're interacting with real math,
real logical structure, but it's not clear whether the math is actually representing
what will happen in the real world. We can interact with real world systems,
like current AI technology, but then it's not clear whether the things that are important
when you're working with existing technology are the same things that will be important
when you're working in 10 or 50 years with much more advanced systems.
So it's sort of like we're trying to do science on things that we can't directly get our hands on.
That sounds like a major obstacle. How the hell do you work around that sort of thing?
Try really hard. Stare at the whiteboard for an hour.
We rely a lot on what, what sort of looks like philosophy in the sense that we like reason
at a high level about what we expect will be the important bits of some problem and then like
try to figure out what our assumptions are and like say, well, if we make this assumption,
then what will happen? And then if we make the opposite assumption, what will happen? You know,
and so on. Well, I mean, yeah, I think that that's sounds like a good, or that sounds like
the way to spend time, right? You're asking yourself, you'll say, all right, given this,
what happens? And then you pursue that for as long as until you feel like you've hit a good
stopping point. And then, okay, but what if we didn't do that? We did this instead. So, yeah,
that seems pretty straightforward to me. I mean, well, not obviously the details, but the, the
avenue, right? Yeah, yeah. So what are, what are the, any significant advances or changes in
thinking in the past few years? So Paul's work has been very cool. Another thing is, I guess it was
maybe towards at the end of this past summer, Mary released a paper called logical induction,
which was a major, major advance from our perspective in that it solves
what we were calling the problem of logical uncertainty or solved it to some extent.
I'll try to give a short explanation. So there's actually two kinds of uncertainty,
perhaps. One kind of uncertainty is empirical, which is like, I didn't see enough in the world
to know what's going on. So for example, if I, I show you a box and there's like two buttons on it,
and there's also some lights on the side, and I'm like, okay, if you press button A, which lights
will light up? And if I press button B, which lights will light up? And you're like, I don't know,
I can't see inside the box. How am I supposed to know which buttons hook up to which lights?
So this is an empirical uncertainty. It's like, you know what could be in the box. It could be
like wires or it could just be dirt or it could be whatever. But you don't know what's in the box
because you didn't see what's in the box. You're uncertain about what's inside the box. There's
a different kind of uncertainty, which is like computational or logical uncertainty where I
could show you what's in the box. I could show you in detail what's in the box. Like I could show you
when you press button A, the box is going to run this computer program that does these operations
and then light up lights depending on what happens. Even if you saw exactly what was in
the box physically, you still might not know what will happen because you can't like figure out what
the program will do. Like you can't perform the computations fast enough to know what's going
to happen. So you're still uncertain. So that's different than not just having enough information
saying, you know, if I had more information, I could make this decision better. You're saying
logical uncertainty is if I had more time or more processing power, I could use this information.
I have it all. I just can't work with it fast enough to give you the answer.
Yeah, exactly. Okay. Cool. And how did this help with that problem?
So logical induction was an algorithm discovered by Scott Gerber, which it's not at all a practical
algorithm, but it gives a theoretical characterization of like what we mean by doing well at logical
uncertainty, like what it means to do, what it means to like reason effectively about computations
that are too long for you to run. I'm assuming there's just an inferential gap there that's
impossible to cross in five minutes. Is that right? Yeah, that might be the case. That's
understandable. I suppose is this paper available for us to link to if anyone wants to give
a stab at it? Yeah, it's online. There's a blog post. Awesome. Then yeah, we'll let the listeners
decide if they can make heads or tails of the particulars. So what personally would you like
to see happen over the next, I don't know, five, 10 years? I would like to tie to that.
Not just what would you like to see, but what do you anticipate seeing? What are your expectations?
So what I would like to see is more, so first of all, more people thinking on the object level
about AI safety. So people like tackling technical problems that have been laid out
or people thinking on their own from scratch about how to do AI safety. I anticipate this
happening to a fair extent, but I also anticipate it being hard to like figure out what the field
of AI safety should look like and sort of how to judge advances and how to coordinate between
research groups. I imagine that coordination is probably, I guess, what I'm trying to think of
