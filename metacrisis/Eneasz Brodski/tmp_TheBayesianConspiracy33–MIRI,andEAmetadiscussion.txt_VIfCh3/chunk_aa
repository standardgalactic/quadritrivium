Welcome to the Basin Conspiracy. I'm Imiyash Brotsky. I'm Stephen Zuber, and I'm going
to introduce our guest. We have on the phone with us, technically on Skype with us, Tsfi,
who works at the Machine Intelligent Research Institute. Do you want to tell us a bit about
the Machine Intelligence Research Institute, Miri for short. I guess what they do, what you do
there, and... Well, let's just start with what they do. That works. Yeah, so the Machine Intelligence
Research Institute is an independent think-tag style research institute in downtown Berkeley.
And what they do is technical research towards the design of safe artificial intelligence.
And what do you mean by safe?
There's both near and long-term concerns, and Miri focuses on long-term concerns. So near-term
concerns look like, you know, like self-driving cars being dangerous or whatever. Long-term concerns
look like, you know, what is going to happen as AIs become extremely capable along many dimensions
and are given more and more responsibility to make, like, actually important decisions.
I guess just to set the stage a little bit more. I mean, so I'm familiar with Miri,
and I think many people listening probably are. But in the event that they're not, I think you
touched on what their goals are, that is approachable from, like, somebody who has no idea what the
institute does. But if you wanted to talk about your role there, or I'm also kind of curious what
the day-to-day looks like before we jump into more of the specifics on what's going on.
Ooh, let's get background first. How did you first find out about Miri and this whole situation?
My story is, I guess, pretty standard. I read about rationality and AI stuff
on the blog Les Run, and I think I stumbled upon some articles called the AI box experiments.
Really quickly, for the listeners, the AI box experiment is one of the common...
Rebuttals.
Rebuttals, yes, to the AI might be dangerous and can hurt us, is, well, why don't we just put it
in a computer that is not connected to the internet, and it cannot do anything unless it goes through
a human intermediary first who gets to veto whatever it wants to do. The worry is that the AI
will be, quote-unquote, let out of the box. The computer is the box, and the human will be convinced
to upload it or connect it to the internet, let it go free, and the AI box experiment is where one
person pretends to be AI, and the other person pretends to be the human who has physical control
of this computer, and the AI player has, I believe, two hours using nothing but a text terminal to
try to convince the human player to let it go free, and it's always done by a human player
who thinks there's absolutely no way they would ever let an AI go free, and what do you remember
what the results have been?
So, sometimes the AI, the person playing an AI in the box was successful, and sometimes not.
Despite the human saying that they would never let the AI go free, they are some half to two
hours talked into letting the AI go free.
Right.
So, I want to volunteer really quick, though. That seems to be the line of
refutation taken by, unfortunately, people like Neil deGrasse Tyson and Bill Nye.
Every time I listened to Sam Harris' podcast, and I don't know, a year or two ago,
Harris finally got on board with AI risk and all the fun stuff involved, which to me, I think,
is exciting and interesting that mainstream intellectuals are getting more serious about
this conversation, but in his conversation with Neil deGrasse Tyson on the show,
and I've heard Tyson say this elsewhere, but when he asks Tyson about his thoughts on it,
he's like, yeah, I'm not worried about it. We can just unplug it. Why is this even
a question people are worried about? And it seems like Tyson's refused to engage with
the answer to that question that he puts out, like, why are people worried about this?
It's like, well, there's actually an answer to this, and you're just not playing along.
Why should we be worried about it? Why don't you think the AI box is a good enough way to stop the AI?
The interesting point in the development of AI is when the AI makes higher quality decisions
in general than the human operators. So in particular, if you assume, which might not be
a reasonable assumption, but if you assume that the AI wants, for some reason, to get out of the box
or get out of whatever restraints are put on it by the human operators, and the AI can make
higher quality decisions than those human operators, then the AI will be able to figure out a plan
that's like too clever for the humans to have thought of to break out of the box.
Which to me strikes me as fairly plausible, even with was a game four or five of Alpha Go,
where against Lisa Dahl, where apparently made this move, I don't play Go, so I couldn't tell you
why it was breathtaking, but apparently it just blew everyone's minds. And it was apparently the
move that turned the game around. And this was, I mean, this was just a game where people who had
been, you know, experts at this were confused by this move that ended up being very beneficial.
If a move is, if a come from behind winning move is possible in a game like that, it doesn't seem
a huge stretch to think that it'd be possible in another type of game, like I want to get out,
right? Or not even, I mean, I don't like that terminology exactly, but same point.
Well, even the fact that a human has managed to convince another human to let him out of the
box, and we can assume that humans are much less smart than the AI potentially could be.
Right. That's the other, I think, knockdown position. I didn't mean to go too far down that.
I did want to ask you a little bit about, so you heard about Miriam, got interested through
Less Wrong, as I think a lot of us did. Yeah, why did you join Miriam? What was your
path to going from person reading about this on Less Wrong to actually becoming
someone who works at Miriam? Yeah, so basically, I spent some time in college
reading Less Wrong and like studying a bunch of math and CS. And I just like contacted Miriam.
I think I was an intern three summers ago, and then I went back and finished college and then
came out to Berkeley to be a grad student at UC Berkeley and also to do research with Miriam.
So are you a grad student right now as well? Yeah, finishing up my second year.
If you don't mind me asking, what did you do as an intern? It was probably not very interesting.
It was probably largely administrative stuff and like, informally talking to researchers.
And so what do you do now that you actually work there in a more formal capacity?
I do some non-research things like helping write papers. I also, my research activities
are, I work with Scott Garebrandt, who is a research fellow, and we work on decision theory.
And I also, you know, talk to the other researchers and help run a small workshop.
What workshop is it? It's like a, it's like a weekly, not really a workshop, like a weekly
meetup. Are you going to stay with me or do you think once you're done with your grad student?
I have not thought that far out yet. I have like more like a one or two year plan.
I think that that will depend on a lot of stuff yet. What is, what is the ultimate fear that is
trying to be prevented here? When we say AI alignment, what is AI alignment? In a word,
it's like the goal of taking those high quality, high quality, those like systems that make high
quality decisions and like pointing them in a direction that we actually like.
And why, why is that so difficult? Why can't we just say to the AI, figure out how to reduce
traffic in our city? You have all sorts of implementational problems. Like how do you even
say that? If you could say that and you like gave an AI the goal of reduce all the traffic in the
city, one really good way to do that is destroy all the cars. There is that, I suppose. Yeah,
probably speaking, there's this like phenomenon called perverse instantiation, meaning like
if you instantiate a goal, but you do so with like a large amount of optimization pressure,
like searching through a very large space of policies, you'll probably find an instantiation
of that goal that is like perverse. That's not the thing that you want. It's like the genie who's
intent on misinterpreting your wish. Yeah, exactly. So I want to live forever is like a
fairly, I think, trophy wish. And then it turns out that you're going to live forever, but you're
going to continue to get more and more decrepit. And, you know, your last billion years are going
to be the worst possible imaginable suffering. But hey, you're technically alive. I think that's
a fair way to approach broadly speaking the, I guess I was just kind of rephrasing what you were
saying. I remember reading of a Tetris playing program where the goal is to get your score as
high as possible. And for anyone who's familiar with Tetris, it's basically impossible to win,
win. You just keep playing until you lose. And once you lose, your score goes down to zero.
So the AI came upon the strategy of pausing the game and never unpausing it.
Not exactly what we wanted. And the real life analog of that could be terrifying.
Another, another classic example is, I think it's a bird in LASL. They tried to evolve
circuits to like act as oscillators of certain frequencies and they succeeded. They evolved
these circuits, but then it turned out that the circuits actually were not doing anything
in a way that they intended the act. The actual thing was that they had formed a radio and were
picking up frequencies from there. Yes, I remember. Did we talk about this before? Or was it only in
person? It might have been with Patrick. Are you thinking? Isabelle, yeah, something like that.
I don't remember if we talked about it on air, if it was only in person that we were discussing it,
but yes, I remember hearing about that. And it was so specific in how it worked that if it was
moved to a different part of the room, it wouldn't work anymore, right? Right, right.
And correct me if I'm wrong. I guess part of really the whole of why this is a hard problem is
because trying to imagine yourself and then control for every perverse interpretation or
instantiation is very challenging. And I guess there's other levels of complexity, I guess,
other levels of challenge built on top of that. Even once all that gets working,
how do you keep things working through, say, self-modification? Well, couldn't you just,
if the AI proposes something like destroy all cars to fix traffic, couldn't you just say no,
that is a bad, go back and try again? Right, so this is like human in a loop stuff. I mean,
in some ways, that's actually a pretty good idea. The issue is you want to actually get
useful work out of the AI. If you just have every time the AI is going to make a decision,
you're like, wait, okay, let's ask the human. You may as well just have the human make all
the decisions. Maybe the AI thinks of something the human wouldn't have thought of? Yeah. I mean,
to some extent, this is like an open, that's like an open research direction, like plans that you
can get an AI to think of but are still verifiable by a human. This is sort of maybe a complicated
issue or I don't know very much about it, but it's sort of not obvious that you can do that.
For example, in the AlphaGo example, the goodness of a move is not really verifiable. If you asked
AlphaGo, why do you think that this is a good move? It'll just give you this huge, it'll barf out
the huge search tree and it'll say, well, my neural nets thought that these were good moves
to investigate and then when I sampled from the distribution, it seemed like it would turn out
pretty well. That's the reason. So you're kind of stuck just either trusting it or not? Right.
There's a problem where the concepts the AI is using, so to speak, are not concepts that
interface with the way that a human would think about the problem. When I was talking about
Miri with Steven a few weeks ago, which was what sparked this episode, and I think this is a
common misconception, Steven was under the impression that Miri's goal is to quickly create
a AI which will help uplift humanity or something along those lines. That's a fair prayer phrasing.
I think my impression was that I was surprised that wasn't the case was I thought Miri in a nutshell
was trying to build safe AI. That was, I think, what I'd said in the nutshell. I was like, actually,
no, that's not it. So what is Miri's goal if it's not to actually build this machine?
Right. So Miri's goal is mainly to do the research that would enable someone to take that
research, turn it into a design for a safe AI, incorporating the ordinary capabilities
produced by mainstream AI research and then make a friendly AI.
Okay, that's not as surprising of a divergence as I thought Inush was getting at. So it sounds
like they're not writing the code themselves, but they're helping build the theory behind it.
Yeah, and in particular, helping build the theory of how to do it safely as opposed to just how to
do it. Yeah, okay, that stays roughly with, that at least confirms that my conception was
roughly accurate, even if they're not, like I said, writing code themselves on this particular thing.
How are they going about doing this? That's a big question.
Right. Well, so there's two research agendas that Miri's put out. One of them is technical
foundations of agency, and one of them is alignment for advanced machine learning systems.
So the foundations of agency thing actually is trying to answer fundamental confusions
about how, like, rational agency works, how decision making works, the idea being, like,
we don't actually know in principle how to direct a, like, very powerful decision making system.
And part of this feels like it's coming from the fact that we don't really understand
how high quality decisions work at all. Is that what the original point of the whole
rationality thing was to figure out how high quality decisions are made and directed?
The rationality thing. The whole rationality movement. I mean, it's sort of been taken over
by humans trying to make our lives better. But I got sort of the impression that that wasn't
the original goal, that it was more along the lines of trying to figure out how decisions are
made well in the first place so that they can be applied to machines.
Actually, when you're saying rationalities, you mean the, like, broad philosophical field
or the conception of rationality in less wrong?
More the less wrong conception.
Okay. My impression is that the less wrong conception is coming from the synergy between
studying human rationality and studying rationality for abstract AI systems.
Yeah, there's like lots of interesting parallels. It's like it's not entirely uncommon at Miri
to like have an insight into rationality that sort of like bounces back and forth between
human rationality and abstract rationality. So maybe that was sort of like a strange answer.
No, no, no. I think that works.
No, that's fine. Also, I was just, I'm trying to think of whether to take like a sharp tangent
to a different direction or do you have anything else you want to ask about specifically with
regards to Miri's agendas and approaches?
Well, I was still wondering more about how they are going about this because he said,
gee, the first prong was about even figuring out how good decisions are made, which we
still don't know. Is that correct?
Yeah, yeah. And then the second prong is trying to see how you would more like looking at what
sorts of machine learning research that's sort of that looks more like mainstream AI machine
learning research. Could you do to like build up the tools you would want to make a safe AI
out of machine learning stuff?
So how can we even do that second part if we don't yet know how good decisions are made
in the first place?
It's not clear whether we can, but there's still progress to be made.
One line of research that I like a lot is the research done by Paul Cristiano,
who's not at Miri. He's at OpenAI and he thinks about how to like sort of like we were talking
about earlier, how to take human judgments like humans making decisions about like what's a good
thing to do and sort of amplify that and apply machine learning to that dataset, so to speak,
and like make a system that's making decisions that are good in the way that human decisions are
pretty good, but doing so at large scale.
I don't know anything about his research. Is it look promising?
Yeah, I'm pretty excited by it. It sort of, it does run into what appear to be more like deeper
technical problems.
So how many people are working on each prong with that on figuring out how to make good
decisions and figuring out how to implement that sort of thing in machine code?
How many people? Very, very few.
Okay, not nearly enough. I think, did you mean like in professional AI industry or at Miri in
particular? I meant at Miri in particular.
At Miri, Miri is not largely focused on machine learning.
It's more about the decision theory.
Not just decision theory, but yeah, it's more about the like theoretical research.
