I have seen some pushback on that norm lately. I think the first time I saw it was a while ago
with one of Scott Alexander's posts back when he was still posting unless wrong as Yovane
Inyash here with a quick interjection. It was actually Phil Goats that wrote this article,
not Scott Alexander. It's probably this sort of attribution error that leads to Abraham Lincoln
having said everything. Sorry about that. And thanks to our awesome audio engineer, Kyle, for
catching this where he pointed out that this is the sort of thing that gets us terrorists,
that most terrorists are actually very well educated and more of the engineer geeky type
that we're used to who instead of when you're a peasant or just some normal person going about
their day-to-day life, you have these religious writings and they say some crazy things in them.
But for the most part, you're like, yeah, this is my religion and I'm just going to go through
my life and do some of the things it says, but ignore the obviously crazy part. Whereas once
you're an engineer and you've gone to Western schools and you learn things like the importance
of rationality and logic, you come back and you're like, well, when I examine my scriptures,
it makes these claims and these are true claims and obviously therefore it follows that I should
blow myself up or something. And so it's taking that sort of logic and following it and embracing
those conclusions. And he called it, I believe, memetic immune disorder, that any memetic system
has this immune system in it as well that attacks destructive bad ideas like let's become jihadists
and keeps them far away from everyone else because there is in society this general immune disorder
that's evolved over the centuries to prevent that. And when you introduce rationality into it without
restarting completely from the very beginning, you have sort of an immune disorder where that
immune system is killed without the original ideas being neutralized and therefore it was kind of
a decree of let's not just immediately go with, we have logic ourselves into this is a good idea
and therefore we should go with it. If no one else is doing this, there might be a good reason for
that. Just a paraphrase, it sounds like the claim was that a lack of indoctrination in the
virtues of logical, I guess of logical reasoning and following logic to its conclusions is one
factor in the small number of religious fanatics. Yes, that most people are just regular people
getting along because of the memetic immune system. So I can see a case for that being made. For
example, the standard case against abortion is like, oh, you're killing babies. If you really
believed that there'd be a lot more people shitting abortion doctors. Yes, the logical thing is
there are murderers that have murder shops that open to fucking stop them. I mean, if there was
an actual murderer that just took people in and killed them for money, we would as a society go
over there and burn that shop down. Huh, I'm curious if you'd weigh in on this. Yeah, that's
really interesting. I guess I don't have a model of terrorists. Right, I do, and mine's a little
different, but I can see totally where that's coming from, but I don't know if I'm on board,
I don't know if that's the ingredient or I mean, I think there's other factors.
I guess what I'm saying is I feel like there has been some pushback over the last few years.
A lot of it spearheaded by Scott Alexander saying, hey, maybe just logical rationalizing our way
through things and following those things that we decide on is a bad idea for reasons that we
aren't sure of yet, but we should just kind of respect that. We don't know why exactly it is
that we like to do this crazy thing and we avoid doing this other thing, but it's worked for
millennia, so let's stick with that even if it looks irrational and stupid. That's a very,
Scott Alexander kind of argument to make, I love it. It really is, and it is also very
unsatisfying because it's the same thing that leads you to keeping the snitch in Quidditch,
even though it is stupid. It's, well, we've done it for so long and we don't want to mess with that
because there might be very good reasons for it. Yeah, it's don't rock the boat mentality,
but the boat needs rocking, so it's really just like, I don't know, maybe not give,
I don't know. Well, it's a problem with transparency. The boat obviously needs rocking,
but since we're not transparent, it's hard to say exactly what effect the rocking will have,
and sometimes it's really good, like when we outlaw segregation and other bad racial practices,
which were just bad. There was no reason for these things, and then other times the effects
are not quite so good. Yeah, fair enough. And it's really hard to tell beforehand which is which.
That's the kind of problem we should be thinking about, how to tell which is which beforehand.
Okay, yeah. Yeah, I think it's all very confusing. Like my handle for this is Chesterton's fence,
right? Like you come across a fence in a field and you're like, well, I don't, I want to like
farm the other side of the field. I'm just going to get rid of this fence, and then you find out
that there's dragons on the other side of the fence, and that's why the fence was there. You
shouldn't like tear down the fence before you know why it was there. So if there's like a
fence around taking violent action because your ideology has implied that you should,
then like you should, even despite all that logic, you should still take great pause before
it's coming to violence. No, that's a solid way to think about it. I like that. So I guess you're
advocating for more ways to figure out what actually is harmful and what is useful? I think
this was slightly tangential to the thing I was trying to say, but I also like this.
I'm sorry, I think I brought us down that path. Did you want to re re, what was the
the original quote that he sent us in the email? Well, I think we've touched on a lot of them. The
one that we didn't talk about, but you mentioned very quickly in passing was like debugging your
friends. I don't know if you had more to say on what we were just talking about a moment ago,
if you wanted to move on to that, but I was curious about, I think one of the first conversations
we had on the podcast was raising the sanity water line, which is not all of my friends are into
this. And so how do you, what is your approach maybe to debugging your friends?
The motivation is like, actually, like we don't know what the right things to do are,
even if we did know what the right things to do are, we're like not strong enough to do them.
That's like fine, or it's not fine, but it's like, it's not a thing to just like be ashamed of
and then not do anything about, which is sort of like my default, which is like sort of,
it's sort of upsetting because like we all want to become strong. Well, not real, like a lot of
people want to become stronger and do more cool stuff and do more important stuff, but like
it's actually pretty hard and you actually need help from your friends. So like, yeah, I mean,
that's the broad. So this is a supporting someone rather than imposing debugging. Yeah,
my most central principle is like, don't do things to people that they don't want to have done to
them. I mean, even if I'm talking about AI, AI risk, I sort of like, I'm less paying attention
to the logical arguments, because the logical arguments are like pretty ironclad, at least in
my opinion, but I'm more paying attention to like, what whether the person actually wants to hear this,
like what they actually want for their lives, like, if they already want to like do the most
important thing, and they just like haven't come across these arguments, then I'll happily like
go through logic. But if they have a good life, it could be that they're susceptible to the like
social norm of following logical reasoning, and then doing the actions applied by the logical
reasoning, or at least feeling guilty if you don't do them. If they have that norm, it's just like
straight up kind of productive for me to like, ruin their day, go into their yeah, like go into
their head and install this like, okay, so you because you feel like all you'll be doing is
giving them more guilt rather than helping them. Yeah, and it's not it's not it's usually not
it's not that useful information, like they could figure it out themselves, or like read
superintelligence or whatever, the book by Nick Bostrom. I have a couple of examples to
give that I think tie into sort of what you're saying with as far as being able to be supportive of
people in of the right mindset in a way that makes everyone better off, like Will McCaskill,
the director of the Center for applied or the Center for effective altruism. He apparently had
some years after he read Singer and was fully on board with like the I should feel bad all the time
because I'm not a good enough charitable person after reading that where he had those years of
guilt. And then he met somebody who was I can't remember exactly whether they were both feeling
guilty and they both decided decided to do something about it. Or I think where McCaskill
was giving silently like he's giving a large portion but he wasn't telling people about it.
And like it didn't really do much to make him feel that much better until he met
one other person who was doing the same thing and then they both just hit it off
and they were they finally weren't you know kind of just like alone doing this.
Now he had a community, he had a tribe.
Exactly. And there was a recent episode of Julia Galev's podcast, the rationally speaking podcast
where she interviewed somebody who donated a kidney. And he had said, yeah, I feel like I did
the right thing, but I wouldn't have done it if I didn't know these two other people who already did.
So I think partly one of the one of the good things about I mean, depending on what you're
and like you said, case by case basis and what means you're talking about installing in people.
But the community aspect makes it a lot easier to do a lot of these things.
You know, if you're the only person who cares, then you know, you have no one to share your
A, your enthusiasm for caring and B, I guess all of your other like all your reasons for
caring about it too, right? So that community aspect that that support network seems to
really helpful. Am I hitting on what you're trying to get at all?
Also, don't you think that if the person was trying to do the most good that they would be,
at least in my opinion, I would be thankful if someone helped me see that this is one of
the most important things to work on. I wouldn't necessarily, I might give more, but just having
the knowledge and maybe reallocating the money that I was giving would be useful for me.
And it would be, you're not necessarily giving them the burden of shame.
You're giving them more information that they can do things with, right?
Yeah, totally. If that's like a thing that they're looking for.
You might be the kind of person that it's safe to do that up that that install on,
but other people might respond more adversely, right? They might just feel worse or something.
Is that?
Well, actually, well, like I guess an install is sort of I sort of use it as a pejorative term,
at least in my head. Information sharing amongst friends makes sense, but like
installing things, installing a meme in someone, I am like much more wary of that.
So what is the difference between installing a meme and sharing information?
It's sort of hard to point to it. It might be incoherent, but like in my head, it's sort of like
one way I would say it is like, if the other person has the agency in the conversation,
then you're less likely to be installing a meme.
Okay. So it's more of like a dark arts kind of thing?
Dark arts would be the most obvious version. Yeah. Yeah. Like if it like I have multiple
modes, like I guess by default, if I'm like arguing with someone, my stance is like,
I know the right thing. I know all the stupid ways that people don't understand the right thing.
And I'm just going to tell you them so that now you'll know the right thing.
It's like it's sort of it's from one perspective. It's like an uncooperative
way to have a conversation. It sort of makes sense. But if you're giving them the tools to
understand it, that's more of a friendly thing. Whereas if you are trying to trick them into
believing this without giving them the tools, then that's that's a totally different thing.
Right. Even if they come to the same conclusion either way.
Yeah. I think I've had a fair amount of trouble talking about this sort of thing in the abstract.
Do you have concrete examples that could make it a more, more understandable?
Yeah. It's a good question. Well, like, let's see. I mean, here's sort of like a simple obvious
example. Like if I'm talking to someone about AI safety, and we like, we like go through the
arguments, and they're like, Oh, but I they like give some excuse why they why they shouldn't do
stuff about it. One thing I could do is like attack that excuse, you know, you know, like be
like, but no, you're wrong. And then they give another excuse. And I'm like, no, you're wrong
because of that. This feels pretty uncooperative, because like, I sort of assume that the thing
that they're doing in the conversation makes sense from some internal perspective, like including
like including deflections. So it almost seems like you're pursuing an agenda instead of just
giving them the tools to make themselves better. Right. Like if my agenda is like, they should
leave the conversation thinking that they should go and do such and so about AI safety.
Like if that's my agenda, then I'm being uncooperative, or at least it's uncooperative
to not say so. And there's, there's, I mean, there is a norm in the in the rational community
that that is a very bad thing. And I take it that you are on board with that norm as well.
Oh, there is. It seems to me that there is that the trying to to doing that sort of thing is bad
and that you should be open if you are trying to change someone's mind in that way to be open
about it. Yeah. As opposed to deceiving them. I didn't, I didn't know about this norm, I guess. I,
I mean, I, I don't know if I've seen it practiced very much. I mean, I mean, deception is sort of,
I agree. I agree that there's a norm about deception. Is there, is there a norm about like
saying, here's my agenda. And then if the person's like, okay, I don't want that agenda,
then you just like don't have the conversation. I, are you, are you saying that sometimes
conversations can go like, Hey, I want to tell you about AI safety. And my ultimate goal here is to
get you to donate money to Miri. Are you saying that's how conversations start or ought to start?
Or that is the norm? Or what are you saying? No, no, no, I'm not saying that, but I get the feeling
that, that it's considered bad form to have that as an actual goal without telling someone that that
is your goal. As opposed to just, Hey, here's a thing that I'm concerned about. Maybe you'd be
concerned about it too, after I talked to you about it. I think I see, is there a, is there a
difference? I think I see the difference a bit, but I think that it's desirable that, that, that
step or that first option, but I don't know if I see that practiced either. Yeah, no. You know,
often I, I mean, just to completely throw this into confusion, you might not be clear what your
goals are before you even into the conversation. You might just be talking about it. And yet you
might accidentally get these things in people's heads or something, right? Well, okay. So what,
what came to my head was the recent kerfuffle around a post called EA has a lying problem,
where someone wrote a very long post about, Hey guys, it seems like people are not representing
data that makes, are not putting forth data that makes them look bad and only saying things that
make them look good and would incentivize people to donate to these charities. And it seems even
more so, like they know this and behind the scenes talk about it and say things along the lines of,
for most people, this is not pertinent information and it will just confuse the issue for them.
So there's no reason to bring it up because we know that this is the right thing to do anyway.
And there was a huge, not huge, there was a significant blow up about that where people
started really questioning themselves and a lot of the EA things that are done. And
because this was considered very bad practice, go ahead and be incredibly open about things and
make all this data available. And I don't think there was any real malfeasance. I think there was
a little bit of people in some people in the leadership starting to acknowledge that
there is a level of diminishing returns of complete transparency and saying it poorly.
So I don't think anyone was intentionally being bad and that there wasn't really a
lying problem, but I thought it was very interesting that as soon as someone picked up on this,
they made a very long public post about it, brought it to everyone's attention,
and the entire community jumped on it and started talking about it because that is something that
as a whole is anathema to the community, that we want to avoid that sort of thing.
We do not want to be like the Coleman Breast Cancer Foundation where they think anything that
you can do to increase funding to our group is good because our group is doing a fundamentally
good thing. And I was very heartened when I saw the movement doing that because to me it felt like
even if it does sometimes stray off course and starts getting into darker area, it has a very
quick and pretty good self-correction mechanism. I mean not incredibly quick, it isn't like days
later, but certainly nothing that goes on for as long as something like the Coleman Foundation
does. It was picked up on and publicized pretty quickly. And that's where I get the feeling
that there is more of a norm to be open and to tell people if you're trying to modify them in
that sort of way. Are you familiar with the EA has lying problem thing? Yeah, I think I
