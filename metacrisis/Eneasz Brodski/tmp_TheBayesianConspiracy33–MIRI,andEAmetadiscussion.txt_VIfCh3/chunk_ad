the pull one really seems to me like the one that is more likely to work.
Is that what you mean by actually trying? Yeah, thank you. That was great.
Yeah, back chaining from the goal to what actions would actually help.
Okay, so poverty. The following might be done because I haven't done any research.
I could imagine a story where it's like, well, poverty, why can't we solve it? Well,
we can just send them 10 people food. That's fine, but it doesn't release. Maybe it doesn't scale.
So you're like, okay, maybe we can make mass produced cheap hydroponic farms or something.
Maybe that's like a bad plan for various reasons. For instance, maybe it's a bad plan because
warlords will just like steal any resources you sent. That's like, all right, well,
how can we deal with that? No, I like that. I think that that's a different framing that,
and I hadn't heard it phrased the way either of you guys put it until just now. And I think
that's a really good great way of thinking about how to solve problems, especially if
it sounds intractable, right? It just sort of do your best to reverse engineer from like a space
from the solution and then say, okay, well, how do we get from there to here? Rather than just be
like, this is a problem and like that's going to be the only thing you do is just declare that
there's a problem, right? Or say the first thing that comes to your mind as far as a solution too,
right? The real problem with that is that it's daunting as fuck. It's like, not only do you
have to do tons of research and work to figure this out, you then have to basically dedicate
your life to chipping away at it. Whereas it's much easier to say, these are the strengths I have,
let me just push with what I got. So are you trying to advocate for people to be more willing to
bite that bullet and dedicate their lives to hard problems like that? Maybe, but at least at the
moment, I think no. I think it's a hard issue. I think that, okay, one hypothesis for what might
be the right thing to do in that situation, like I think it's, you're probably not going to solve
the problem unless you do that, basically, because like people have already tried the obvious things,
that's not really true. Like if you're going to make progress, you need to be doing something
weird and you're going to need to level up your skills. I do have a quite general policy of
only do things you actually want to do. So was the question more about how to want to want to do
something? Yes, but only if you want to figure that out. I mean, at this point, since you're
trying to, you already want that, so it seems like what you're trying to do is figure out a way to
make other people want that as well. I don't want to make anyone want anything. I guess, I guess,
like for instance, one thing I keep coming back to is just like, it's totally fine to keep doing
exactly what you were doing before. But let's just like acknowledge that it's pretty interesting
that like a line of logical reasoning that you can totally follow implies that your actions don't
make sense. It doesn't mean you have to be like, oh, well, here's this line of logical reasoning.
I guess I just have to like quit my fun job and like move to another country or whatever,
like stop doing things that I enjoy and start doing things that I hate, but that are like
the correct action. It's like an interest. It's just like interesting that
like preferences don't align with what we like explicitly reason out as a good idea.
Do you think that's because we have multiple preferences or because we are, as Robin Hansen
says, lying to ourselves so that we can signal our good qualities while remaining, you know,
doing the things that are selfishly good for our genes and pleasing to our emotions?
Probably yes to both. I mean, yeah, everything is super complicated.
So do you think people, do you think the world would be better if people
did accept the reasoning of their logical minds and actually do the thing that their ideals
tell them to do? The image that formed in my mind when you said that was not good. Okay. Like
accepting logical reason, like this is like probably reading too much into the language,
but there's this thing of like, should I accept this logical reasoning and do what it tells me?
Or like, why can't I get myself to go to the gym or something? You know,
there's like the I who thinks I should go to the gym or thinks I should like do AI research or
whatever. And there's the myself who like doesn't want to. The more interesting thing at least
to me at the moment is like staring at that and trying to figure out like, what's the deal that
like, I don't know if this is even possible, but it seems like it might be possible to like have a
person who your logical reasoning is directly hooked up to your system one, like directly hooked
up to your like visceral desires. Can we quickly explain that system one is basically your visceral
desires and your things that you want and do out of reflex, right? And your automatic judgments?
Yeah, system one is a very broad catchall term, which sort of like obscures lots of differences.
I mean, there's like fast heuristic reasoning. There's like fast visualization. There's like
deep desires. There's like, you know, you can go on and on. There's like lots of stuff.
It's the more subconscious stuff. Yeah, yeah, it's a catchall term. Yeah.
Like so, so an example of key coming back to is chess. Like chess involves logical reasoning.
You're sort of like playing out variations in your head. And trying to reason like, well,
my king side is weak. So I should move my knight or my bishop over to protect it or whatever.
Like, how can I get a queen to this position? That'll be a good attack or something. But there's
but at least when I play chess, it's not like I do some logical reasoning and I come to a conclusion.
And then like another part of me is like, uh, yeah, but I don't want to. There's a thing where like
the logical reasoning is directly hooked up with the desire to win.
Such that it's like, it's not even a dialogue. It's just like a full you're like a full stack person
where like the logical reasoning is like, like the system one desire to win
has like climbs into them to the like big robot mech suit of the of like system two explicit
reasoning logical reasoning. Is that what you're is that what you're hoping to get to to be able
to align the two systems on in all cases? I think my ontology is probably wrong. So like that,
that that vision probably doesn't make sense. Well, I mean, it might a prima facia. I don't see
why it wouldn't make sense. I can see that in the actual implementation, there could be problems
if we have conflicting terminal values, which I think we very well might in inside us. But
on the face of it, we may not necessarily. So that could in theory be a thing that could work,
right? Definitely in theory. Yes. I definitely don't know in practice. So we talked, I don't know,
a while ago, not in this conversation, but about the right thing to do after a good logical argument
and what you want to do slash will end up doing anyway. And you know, like this can be listed
with like, you know, a pretty heavy handed thought experiment, like say the house is on fire,
and you're running a daycare. And for some reason, all of your neighbors kids are in a stroller right
by the door, and you can get them all outside safely, or you can run to the other room and
grab your kid and get it outside safely. There's a difference between like what you ought to do,
you know, four babies saved greater than one baby saved, therefore say four babies versus like what
you as a human being could live with and would want for, I guess, your own mental health and
I guess, like generalizing some rules, like, you know, always save the four and forgo your own
would just not really work with like the way the rest of your brain is wired. And I don't know
if you'd even want to rewire yourself to be different given how that would just cascade
through everything else. You know, if you didn't really care that much more about your own kids
than your neighbors kids, you might not want to not want to change the kind of person who would
be that indifferent, right? I think if society is set up well, they would want to encourage you to
do that though. Because from every the outside view, everyone else is going to say four babies
are better than one. So they in a good society, there would be some kind of I would assume if
this is my own personal bias speaking, maybe I'm wrong. But I would think in a good society,
there would be a lot of like hero worship that would go towards someone who saved the four
neighbor children instead of their own child and a lot of praise of that person and how great they
are. And maybe that would help encourage people more to to overcome their own biases towards
their own kid. Yeah, and that might help like just help you overcome the more definite grief of
losing your own kid versus losing four stranger kids, right? But that's just like part of our
wiring, right? We could try and compensate that with society. I don't know if this is worth
diving into or not. But there's another way to like frame like the standard trolley problem where
you're at the you're at the train tracks, you can pull the switch to divert a train track
to kill only one person because it's currently on track to kill five. I'm paraphrasing because it's
I'm sure most people have heard of it. If not look up trolley problem. The standard framing is
that you're the person at the switch. I think another way to think about it is you don't know
which person in this scenario you are, how should people act, right? And that's sort of,
I think, what you're getting at that there's a different, I guess, I don't know if I'm getting,
I think I'm talking in circles. I'm going to just... No, I get what you're saying. You as an
outside person, you want him to pull the switch to divert it to just the one person instead of the
five. Yeah, or, you know, the alternate scenario where they're pushing somebody in front of it,
right? You know, if you're the person being pushed, you don't want to be the you don't want to do
that probably or you might because you feel like saving five is worth more than your own. But like
we're like, like those also kind of just pump some intuitions that we're not necessarily built
in ways that conform to the two solid logical reasoning, right? Not to say that we shouldn't
work to better ourselves, but just the fact that you might just be psychologically ruined for life.
I feel like we're getting pretty far afield here. Sorry. We sort of do that, but I try not to do
that when we have guests on the line. Hopefully I'm completely checked out.
Well, I thought that I thought the issues you were raising are pretty interesting.
Yeah, I mean, there's like the thing I thought that actually I couldn't I'm not sure I knew who
was speaking. Maybe it was Enash was sort of giving a decision theoretic argument for pulling the
switch or like a Rawlsian veil. That was me, dibs. Sorry. Not letting Enash steal my my
intellectual credit there. Oh, yeah. Although I didn't actually invent that idea. So
I stole it too. Anyway, sorry, if we have to credit Rawls, we'll do that.
It's interesting that that's actually, I mean, maybe it's not interesting that at least that
it seems to me to be a distinct argument from the argument about like societal norms.
Oh, yes.
Like there's there's like, I think I don't want to quit my fun job. But I think
decision theoretically, I ought to do this other thing, because people in my I want people in
a situation analogous to me to my situation. I want them to behave in some way.
There's a different thing, which is like society has like installed a norm. And I'm just going
to like follow this norm or else I'll be ashamed. I think that's I see the distinction you're making.
But as far as I don't know necessarily if it's either or, you know, it's it's not like
your life is best spent. I'm not sure what example we're using as an alternative to like
quitting your job or, you know, the alternative to doing what you're doing and
doing some charity with your current lifestyle versus, you know what, like leaving the country
and doing something else. But even if there was like another optimal way that you could maximize
utilize on earth with your life, but it would make you miserable. I don't know if that's the
kind of thing you'd want to generalize either, right? Like, I think that there's there's a midline
to where, and I think that's kind of what effective altruism is all about to where, look,
live your life, be happy, but do as much good as you can while doing that. Don't give, you know,
like the logical implication is kind of like Peter Singer, right? Where you give until it hurts.
And then when you're suffering as much as the people that you're helping, then it's okay to
stop giving. Or, you know, if you're suffering to the point to where there's, I think the obvious
caveat to where, you know, if you can't afford your car to keep going to work to keep earning
money, well, then that's where you stop, right? Or whatever that point is. But that's not what's
good for our personal psyches. You know, that's exhausting. We don't necessarily feel like we're
getting everything we want out of life. So it corrects me if I'm wrong, but I feel like the
whole EA goal is to find that midline. Well, I can't. Yeah, I don't know what EA thinks.
That's fair. Yeah, I didn't mean to make you the spokesperson.
As a blob. But like, at least for me, I guess, so I mean, the thing you said might be right
two things. The less important thing is like, it still does just on general principles feel
sort of bad to have this situation where it's like two systems that have like made a treaty,
like made sort of a tense treaty where it's like, well, I want to like be happy and do
fun activities. Also, also, I'm going to like bow to the social norm of giving as much money as I
can. I wish that was a social norm, but no points taken. It's a normal. That's right. Yeah,
this is probably too much of a tangent, but I'll just like plant a flag that I'm sort of wary of
I'm wary of norms. I'm wary of norms as like as a mechanism of it's sort of like exerting power
over. Yeah, well, I mean, I totally know what you mean, but that's that's exactly the point of
norms. It is to exert power over people. And so do you think that there should be, I mean,
what what methods do we have for exerting power over others? There's social norms where we,
you know, shame them and use other other psychological things. And then there's direct
physical violence. I mean, I prefer the social norm ones, honestly. But are you saying that
no one should have any ability to to modify others through things like social pressure?
Well, I agree that it's better than violence. I agree that you need them for a large class of
things, for example, having a norm against violence, or having a norm against like wandering
around the street yelling at people's faces or something, a norm against like barging into
private meetings, whatever, you know, like, yeah, I mean, it's not that no norms make sense. It's
more that like, like if we're talking about the the ways that EA as a movement or rationality is
as like as a community or movement or like ideology or whatever, if we're talking about the
ways that those things are going to actually like make a difference, like actually actually change
the course of history, that is that seems to me to be heavily, heavily biased towards things that
are like weird and hard. And doing things that are weird and hard seems to me to be very destructively
interfered with by norms, or like, yeah, by being beholden to norms. Okay, so our human nature is
going to fight against these norms is what you're saying? Norms can go against what our system one
wants. They can also go against what our system two wants. They're just like a whole other system.
I mean, is there like a system three? Yeah, this is sort of a joke, but it might be it might just
map on correctly to super ego ego. I don't really I haven't read Freud so I don't know.
Yeah, like super ego is shame, norms, social roles, etc. Ego is like, conscious detention,
verbal explicit reasoning. And then it is like deep desires. So so you're saying that we should
examine the norms more or that norms in general are bad or that norms in general should be
examined much more because they can be destructive? Like, well, so I don't have an opinion because
I'm in the like figure things out stage. I don't know what the right thing to do is but I'm like
very, I'm like, a little scared and like very wary of I think that that makes sense. I mean,
certainly, like, norms work as a great shorthand to like the word not all reinventing the wheel
every, you know, every human life or every five years, however long you want to examine these
things, you know, we shouldn't spend, you know, even an hour, well, maybe, I don't know, like,
we don't want to spend a lot of time thinking, man, how about that that norm against like killing
and eating eating people, you know, some things make sense to leave. But I think that norms are a
great super convenient path to laziness, right? Oh, this is this has been established, you know,
so we're not we're not going to even think about it. So I think that's that to me is the big concern.
And like, you know, this kind of ties back in loosely, you know, thinking about like, aging
sucks, and it's intractable versus aging sucks. And let's try and do something about it. Like,
the norm for a long time was it's intractable, it's impossible, it's not even think about it.
It took somebody at some point to say, no, fuck it, let's actually see if we can why is it intractable?
So I kind of get the feeling that we are being very circuitous and saying meta things without
saying object level things. I guess what I'm getting at is it seems to me like there is at
least one specific norm that is common that you have a problem with. And I want to hear what that
norm is. It's the eating people thing, right? Well, I sort of two processes were running in my
head. One of them spit back. Well, the norm of like thinking that when you have reached a logical
conclusion, then you should immediately force yourself using like willpower overrides to do
something that seems like a pretty strong pervasive norm and also seems pretty destructive.
