I would like to define it poorly and then have somebody else do it better.
Is that cool?
Sure.
Okay.
So I think the Nash equilibrium is a situation where I would not change my answer.
Even though I know the other person's answer.
Okay.
Right?
Yeah.
So I don't regret, I don't regret my answer no matter what the other person says.
Well, no, you might still regret your answer because one year is better than six years.
Well, I think, but the prisoner's dilemma doesn't have an obvious Nash equilibrium.
It has a mixed strategy Nash equilibrium.
Is that correct?
From what I've heard, it does have a Nash equilibrium of both defecting because the Nash equilibrium
as defined is one person has no incentive to change the strategy assuming that no one
else changes their strategy.
So if the other person is cooperating and you are cooperating, you have an incentive to
defect.
So you get one last year.
Okay.
So the Nash equilibrium is what we actually came to because we're both rational actors
and it's that we both defect and we both get six years in prison, which gives us worse
outcome than if we both cooperated.
But it's not the Nash equilibrium.
So we're going to go and defect on each other.
And thank you for correcting my terrible definition.
No, your definition was actually really almost exactly what I said.
And this is one of the situations where Eleizer would say this is not actually what a rational
person would do.
Because there's super rationality.
I suppose if you want to call it that, but I would just call it irrational to end up with
a six year term when you could have had a one year term instead.
I agree.
So yeah, we could put rational decision makers in our first iteration of the game in quotes.
I don't feel like that's the optimal decision for either of us.
But within the constraints of the game where you're only trying to maximize one thing and
there's no other social pressures or anything outside of the game.
Still within constraints of the game.
We're very selfish.
We're very rational.
Yes.
You don't care about each other.
You don't care about what future favors you may have done for each other.
So as you suggested just now, different things happen when we start to repeat the prisoner's
dilemma with the same prisoners.
We actually commit different crimes.
So we get out of jail.
Here's the deal.
We broke living glass, I guess, and pooped in the toilet in the wrong way.
And then we got caught.
And then we're going to do it again later once we get out.
And then we're going to do it again later once we get out and get caught.
And then over and over again.
But there's different results for how a rational actor is going to play that game depending
on if they know exactly how many times we're going to commit that crime versus not really
knowing and doing the crime and getting caught for it an unknown number of times.
Which is kind of fun to think about, huh?
I'm still hung up on the fact that we defected.
We were talking in a voting episode about what my decision making process is to lead me to
vote, which is that I want other people to vote.
I want other people to inform themselves and vote.
And you alluded to the fact that ties in pretty well with the time of decision theory.
Yes.
And I was thinking more when I made the argument in terms of just deontological reasoning,
that if it's something that I would do and I would recommend it for myself,
I would recommend it for other people as well.
And time-less decision theory is a little more broad than that.
Avoid some of the pitfalls.
So say Katrina and I don't care about each other's well-being as far as who suffers
in prison and who doesn't, but we do respect each other's rational capacities as smart people.
Then in theory, if you think she's smart and if you think you're smart, you would both cooperate.
Unless he thinks I'm just highly rational and also possibly a homo-economicist.
Probably if he thinks you're a homo-economicist, yeah.
But if he thinks that you're actually a rational person, in the sense that you want to have
as little jail time as possible, he thinks that you might cooperate.
I would totally cooperate.
Yeah, me too.
There was actually a great t-shirt from, God, I think it was rational apparel or something
that said, I cooperate in the prisoner's dilemma.
And I bought me one of those.
They also had an effect in the prisoner's dilemma one.
I actually can't wear it around though because it has a little plus on it for cooperating plus.
And it's in green because green means go.
And we live in Colorado and so everyone is like, hey, you're wearing a marijuana t-shirt.
Like, no, I don't really like marijuana.
You're patient.
Yeah, I know.
No one ever reads the words.
They're like, ah, green cross.
Here in Colorado, green cross means you're at a weed dispensary.
It's so different depending on where you are.
Yeah, so since we have things like friendship and actually wanting lower prison sentences
and altruism and faith in our fellow human, a lot of people are going to choose cooperate.
We're going to stay silent, right?
Yes, I think so.
That's why the fact that we both ended with six-year prison sentences is not the rational outcome.
But the rational outcome is where we both realize that both parties involved are rational agents
and that we want to spend as little time in prison as possible
and the way they do that is for us to both cooperate.
Yeah, they call that super rationality for some reason.
I don't understand why.
I have a prediction to make, which is that we're going to get a lot of corrections on this episode.
Sure.
Bring it in.
Yes, do it.
That's why we have a listener feedback at the end of the episodes now.
Yeah.
Great.
So yes, totally agree with you.
In my opinion, like half of morality is just basically trying to get people to always cooperate on prisoner's dilemma situations.
And I don't know if other people think that's accurate, but it seems to be the fact that people keep trying to be like,
hey, you know, be cooperative.
It's better for everyone all the time.
Right.
And then religions show up and are like, when you defect, you go to hell.
Exactly.
You don't want to go to hell.
So that's another way that you can solve the prisoner's dilemma is people have to go to eternal suffering in damnation if they squeal,
if they're not loyal to their partner in crime.
Right.
I don't think that any religion would say that, but...
I'm stressing the analogy our mob boss might kill rats, right?
Yeah, that's the example that Scott Alexander used is that we're both in the same crime family, I guess.
And our mob boss says that people who squeal sleep at the fishes.
So therefore, neither of us want to die.
And we've just been given an excuse to cooperate so that we both have a lower jail time.
So it works out.
Excellent.
Or...
So whenever people talk about cooperating and defecting, that's usually the sort of thing that they're talking about.
Or we have reputations, or we plan on committing crimes again in the future.
And in that case, there's the opportunity of maybe Stephen to punish me if I defect by defecting on me next time instead of cooperating.
Because, of course, the best thing for him to do, from my perspective, is to cooperate every time.
If I want him to cooperate every time and we're only doing this thing once, then I should definitely defect.
But if I want to build up some good faith with him so that he continues to cooperate with me, then I should cooperate too.
As a social species, this seems to be really hardwired a bit into us.
Or, I don't know, maybe it's beaten into us in childhood.
But punishing people who defect, even if it's very costly to you, is a thing that people tend to do a lot.
I think my favorite example is the ultimatum game.
Yes.
Where a researcher takes two people, gives one of them $10 bills, says, split this up however you want.
And then the other person can either accept that, the split, in which case you both get however you split it, or they reject it, in which case neither of you get any money.
You give me back the $10.
So theoretically, if you're a rational actor and I'm given the opportunity to split up this $10, Stephen, then you should accept whatever I give you, right?
Because that's better than nothing.
Right.
But...
So a rational actor, rational in quotes, would always split it 9 for themselves, $1 for the other person.
But when they actually do this in real life...
People reject that.
They're like, I'm willing to forego $1 to punish that guy for being a dick.
Yeah.
Or that girl.
I was listening to a teacher the other day, and he was saying, kids really understand fairness at an early age, and actually so do animals.
There have been a number of studies.
But, you know, ask a kid to divide up a cookie.
And they're going to be very, very careful that that cookie is fairly divided.
Because a lot's riding on that cookie being fairly divided.
Yeah.
There's the way to get around that, which is, say if you're dividing a cookie, or whatever it is, you split the cookie, and I get to pick which one I get first.
Yeah.
That's a good...
Yeah, but we're not doing the ultimatum game anymore.
I know.
I just like that way of getting around.
That particular problem sounds like it has a very easy solution.
But of course, if I'm dividing the cookie, and I'm the only one, you know, if I'm bigger and stronger or something, then I get to pick which one I want.
You're lucky you're getting anything at all.
There's some interesting games, permutations I've found where some cultures tend to be more vengeful than others.
And they don't like people trying to make themselves look good by contributing too much to the community pool.
No, no.
This was really interesting.
They're those vengeful and don't like it when people contribute too much.
Because it makes the other person look really good.
So it's one of those ones where you're passing the pot around, and every turn you can either put in some of your own money and then it doubles.
And at the end, once you're all done, everyone splits it up, or you can take out some money.
There's variations where you can punish other people for what they've done.
So if they take out money, you can spend some of your own money to make them lose money.
Now, you have to spend more than you make them lose.
You'd have to spend $4 to make them lose too.
But people will do it anyway, just because they want to take money away from the guy that was being a jerk.
There's a few cultures, and I don't remember exactly which ones they were, where if you put money in the pot, other people will destroy money to take money away from you because they're like,
well, that guy's just being all highfalutin and trying to, yeah, make the rest of us look like greedy jerks.
Clearly, the optimal rationalists.
Well, no, it's just when the person I read who was commenting on this said, this basically looks like pure evil to me.
You're destroying wealth to destroy wealth because someone was trying to create wealth.
Gosh, that reminds me of somebody I was talking to, a divorce lawyer recently.
And she said that she got a call off hours from a woman saying he took the turkey baster.
Thanksgiving's tomorrow and he took the turkey baster.
That was on my list.
And she told this woman, you know, so you could pay somebody hundreds of dollars an hour to make sure that you get the turkey baster.
And figure that all out or you could go out and buy one yourself.
And she said, no, transfer me to the other lawyer.
I want that turkey baster.
And he told her the same thing.
I will make sure that you get it back and that you are, you know, fairly, what do you call it?
Compensated.
Thank you.
Fairly compensated for your turkey baster.
