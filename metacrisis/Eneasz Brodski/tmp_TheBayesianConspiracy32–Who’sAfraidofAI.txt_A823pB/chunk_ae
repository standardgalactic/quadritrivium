a hundred? Should it be a thousand? I mean, this was a talk I saw a few years
ago. Eleazar Kowski was talking about how there's less money spent currently
funding safe AI research than there is marketing lipstick in New York City.
And so I'm not sure I'm not sure where you draw it. You know, should it be
lipstick in the entire country? Should it be, you know, whatever, but like the
idea of sticks of yeah. So like, but this was mid 2000s. I'll be a little later.
I think I would be happy with a Manhattan Project Style project on AI, because
that was a major endeavor and the government put a lot of resources into
it, but it wasn't, you know, drastic. It was it was still small enough that it
could be hidden in the desert and most people didn't know about it.
What if there already is something we don't know about it?
Good, good, good times. Many thumbs up for me. I approve.
Assuming that they're working on doing it right. Yes. I don't mean like the
military probably has a somewhat of a vested interest in autonomous weapons,
right? Yeah. And I naively hope that they have enough forethought to think
my continued survival, not just goes beyond just winning this war, right?
Or winning this fight or whatever we're building these, these, these AIs to do.
And they're going to think, okay, we should actually think ahead, you know,
five years from now, rather than just like six months to, you know, the end of
this, whatever, you know, the scary part would be a war of annihilation again.
Because there, when they were originally testing the atomic bomb and a few people
were like, this, this, what if this ignites the atmosphere? That would be bad.
We would all die. And there were, there were some other people that said, you
know what? I can cover.
Goddamn. A world where we are ruled by the Nazis is not a world that is worth
living in. Even if it's a just 0.001% chance of human annihilation,
I'm willing to take that risk because I'm willing to take that small risk in
order to not have to live under the Nazis. And you make that gamble enough times
and eventually you're going to lose. And I mean, if, if you've ever seen the
downfall, which is just a fantastic movie about the final days of the third
Reich, all a lot of Hitler's inner circle is, is that, is that what that
mimetic video is where it's him like in his? Yeah. And like people, people sub
over it. Yes. And he, and he, like, yeah, he like takes off his glasses, his
hands shaking and okay, I got you. I should go see that. I should see the
original version to have only seen like the, the programming humor mock-ups.
Yeah, no, I've seen so many different, but a lot of people in, in Hitler's
inner circle poisoned their own children and then killed themselves when it
became clear that they were going to lose the war because they said a world
without national socialism is not a world worth living in. I don't want my
children to grow up in that. And if there were another situation like that,
people were like, well, okay, we're not sure if this AI is going to destroy
humanity or not, but we're willing to take that risk because we'd rather not
live under, you know, capitalism or, or communism or whatever it is.
I hadn't thought about it in, in precisely those terms, I mean, Yash,
and thank you, my, my nervousness level, I think increased proportionately to
the, that, that level of, of concern. Cause yeah, it didn't, it didn't
occur to me. Like I, I goes roughly articulating earlier that I naively hope
that they would think we should probably try and live through this with great
probability, but you're like, fuck it, not everyone thinks that way.
People might, might have different priorities and they're like, you know
what, no lie or, you know, no life is better than life under whatever,
communism or
Better dead than red.
Oh my God. That's a, that's horrifying.
Nervous laughter.
Yeah.
On that happy note, where do we want to go from here?
I think one of the, the biggest debates is just how fast the takeoff will be
because if, if an AI becomes superhuman in a matter of days or weeks,
they could possibly take over the world.
But if it's something that requires a number of years to really marshal the
resources, then humans have a much better chance of trying to fight it and
contain it. And I don't, there's, there's, you know, the bunch of controversy
over how long it would take for something very intelligent to, to take over the
world.
There is reasonable disagreement there.
Where I think since the worst case scenario is fast to take off,
that's the one people run with because we,
exactly plan for the worst case scenario.
Makes sense.
But there, there is the chance, yeah, that it could for some reason take years.
There are a number of people who say that's a silly concern and we shouldn't
worry about that because it'll take a number of years and we will have some
time to, to react.
Yeah. I can see what they're saying, but you got to, I mean, is it how
unreasonable is that concern?
Right.
And it's, to me, it's reasonable enough.
Yeah.
So yeah, that's, that's the thing.
I mean, there's other common kickback to this.
You get like Bill Nye and Neil deGrasse Tyson, whenever I hear them ask someone
that, you know, in an AMA asks them about this or whatever on StarTalk or,
which is Neil deGrasse Tyson's podcast, they're like, oh, well, I mean, if it
gets, like, if it seems like it's going to get scary, we just unplug it.
You know, you just, like you just localize it and then, you know, you just
shoot the box.
As long as we can control the electricity, we're fine.
Well, sure.
Yeah.
Like it seems a lot of the, of these doomsday scenarios involve an artificial
intelligence that is like, ubiquitously connected to the world.
Well, I mean, the, if, if an intelligence is that much smarter than a human, by
the time you know that you should be worried and that you need to unplug it,
intentions, it would already be out in a place where your access to electricity
is not, you know, what is restricting it.
Your, your ability to be, yeah, at that point, unplugging it won't do anything
because it's already secured its own source of power somehow.
I mean, the other thing copied itself somewhere.
Yeah.
And there's just like things to think about, like, you know, I'm not sure what
the interface would be if you, if you just got it in a box, if you're only
going to see text on the screen or if it's going to like vocalize things to
you, you know, like, uh, like Siri on your phone or something.
But again, with how much faster it could be thinking than what you think, if
you had to cross the room and hit that light switch to turn off the power and
I'm the AI that doesn't want the power to go out, I've got like 50 years in
subjective time to think of the one thing I can say to stop you, right?
And I can run all the simulations I need to, to think of what will be, what
will be the most successful thing.
Um,
one of the things humans do with intelligence is convince other humans of
things.
So if, if it's something is much more intelligent than a human, it should,
in theory, at least have the capacity to convince someone to, to let it out of
the box or to not hit the switch or whatever.
Yes.
So, so you get a deaf person to flip the switch, right?
Well, I know, I know, I know, I know the spirit of what you're saying.
I was just being, yeah.
I mean, that's fair.
I mean, I think I, I guess it's not really clear to me how safe, what would it
look like for it to be scary enough for you to unplug and as long as it didn't
want to be unplugged, it would probably not, it would probably try and act like
it's not being scary, right?
Yeah.
The human thinks it's smart enough to unplug the machine when the machine gets
scary and then it goes to unplug the machine.
It turns out that the cat door is locked.
Exactly.
It's like, oh, well, how, what are the chances?
So, and that's the thing too.
Like as far as AI is wanting things, um, I don't want to skip past this in that
like it doesn't want to live the way that we do.
It wants to do whatever you told it to do.
And part of that is living long enough to do it.
I don't think that across all of mind-designed space is an inherent
will to live, but as long as it wants to do stuff, it can't, I guess, think of
whatever random thing you want it to do.
You want it to run simulations on like protein folding.
Well, it can only do that while it's running.
And so if it wants to do that, and it, it also wants to run, yeah.
Don't shut me off.
Oh yeah.
Seriously, I have this no actually seriously look at this.
Yeah.
Well, while I was writing my book, one of my major concerns was what if I die
before I get it done, you know, because that would be really disappointing.
I just want it to be done and out there.
And now I'm dead in the whole, there was no point to it in the first place.
So even if all you wanted to do is write your book, part of getting it done
is living long enough to finish it.
Yes.
And so that's, that's a, I think an okay analogy, right?
Yeah.
So yeah, but also a lot of people seem to think that an AI has to like think
and reason as humans do.
And that's not necessarily the case.
It may not even be a, uh, what we, what we recognize as something
that is conscious, that has, uh, that, that has self-reflecting.
I mean, maybe a will, but you don't need to, uh, be, be conscious or, or have
a self-image in order to optimize your environment.
Even fucking plants do that.
So if you have the ability to optimize your environment and you have a goal
that you're going to, you can be as dumb as a refrigerator, as long as you're
really powerful at optimizing things, humans may be unable to stop you.
Yeah.
I think I always leave the C word consciousness out of this discussion.
It's, it's, it's an important and related topic, but I, it to me is not
see how that would be cause it doesn't necessarily have to be, have a
personality or a thought process at all.
Yeah.
And like things that wants and ways to get them for sure.
And the, the, I guess the really quick bird's eye view of why this is like a
separate, but fun topic is that it could be that if we did create a consciousness
that was scaled up to the proportion that its intelligence was scaled up, we
would create things that suddenly mattered way more than we do, right?
We would create a utility monster that by all real good measures was, you know,
the most important thing in the universe now, right?
Or at least in our local part of the, that we know about, and just as like any
human is worth way more than any ant, you know,
What about, you know, you see this in fiction too, is the, the idea of like
distilling one's consciousness into code, making a model of like a human's
quote unquote, right?
Like, and then, and then like, if, if you can observe what that looks like, then
maybe you could design your artificial intelligence based on that, but again,
then again, if you get to that point, like you probably already super advanced
already, because you're distilling all the contents of the human brain into
something that could be observed.
And we had a fascinating interview with a Robin Hansen who wrote an entire book
about his thinking of what could happen if that were to happen before we get
super, okay, what did he say?
Because I'd be curious to hear that too.
Oh man, it was like in a nutshell, in a nutshell that the world would be very
weird and run very fast and that we would probably have many, many copies of
the most efficient humans rather than a large diversity of humans.
Sure, that makes, yeah.
But if we were to try and use that like on the path of super intelligence, you
could, yeah, if we, if we got human uploading before we got strong AI, then
we could use uploaded humans to, we could have them tweak their own, their own
