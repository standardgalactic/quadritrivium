a sort of pain amnesia that humans have.
And that is the only reason women ever have more than one child because after the pain
is gone, it doesn't stick around anymore.
Seems like a good trait to have.
And I was going to bring that up to evolutionarily sound thing to.
And I think that's tied specifically.
There's there's additional evidence that ties that specifically to childbirth.
And when I said it can't be that bad, I wasn't belittling it in the way like, oh,
I can't hear that, but you ladies are exaggerating.
I didn't get a chance to finish that thought.
I was I was going to go on to say something like what you said, which is that it could be
the worst thing ever at the time.
But then you look back and it's clouded by, you know, this new happy baby and all this
awesomeness and just from the way we were programmed that it just doesn't
register as the worst thing that's ever happened to your entire life.
Right.
So if we take this like pain amnesia thing into account and also the no follow on effects
afterwards, basically what we're what we're going coming down to is
everyone in an almost infinite amount of people losing about a second of their life
versus one person losing 50 years of their life, and I'm sure that person who's tortured for 50
years is going to come out like, all right, well, you know what that was really painful
and terrible, but but I do it all again because infinite people didn't get a speck of dust in their eye.
You thought experiments. I don't know. It's weird then because I guess an infinite number of
seconds adds up to more than 50 years of seconds, right? Yeah, I mean they're just some things.
They're just some things where you can't math your way out of it, right?
Dr s apparently is an actual doctor because the email ended with this is a fundamental actual s
never. There was more than just an s in the name, but I don't know how
how much of their name they want out and that's okay. Yeah, this is a fundamentally different
exercise to the hospital budget question where an administrator has to decide whether to spend
a million dollars saving one life or 10 lives because the lives are valued equally in practice.
Hospital administrators actually have to make more complex decisions about life expectancy.
For example, the equation looks a little different if the choices to prolong one young person's
versus 10 very elderly person's lives, but these equations work because and only because
they weigh up the value of equivalent things. So Dr s is saying that these things are not
equivalent. Oh, okay, cool. We got an email back from the doctor and said, yes, go ahead and quote
me and I am happy to be referred to as Sasha personal preferred pronoun as he. So thank you,
Dr Sasha wanted to give him credit if you wanted it out there. Me being being the the fame whore
that I am if someone's going to read my stuff and like, no, no, put my name on it. Yeah. I think
yeah, it's it's a different way to look at it. But I try to entertain it from the worst possible
imagining to where you are making that decision from the outside. Then that's where it got to the
hazy thing that we that we talked about for like an hour. But yeah, if it was and it wasn't told
everybody or if everybody just somehow innately knew then that becomes a whole different equation.
And I get the feeling that it was supposed to be a from the outside sort of picture because it's
the sort of thing like if you're a super intelligent god, emperor, artificial intelligence,
and you are making this decisions for all of humanity without being able to actually consult
with every single nearly infinite person in the multiverse, what is the decision you make? And
and the decision decision should be based on math, I guess. Yeah, that's the idea. It was
trying to use the idea of shut up and multiply or one of the one of the thought experiments
get around to that. All right, sorry. Next one. One last thing, at least only from what I have,
the one last thing on the dust specs is Shaq Latria Shaq Shaq Latria on our website again
says the argument that convinced me of the dust spec position is the whole time we were talking
about the thought experiment. It was always just a one off decision. What I think about though is
whether or not the position is generalizable. For example, Loki really hates humanity, but can
only affect a tiny percentage of people on his own. So what he does is he kidnaps one person
and tells you that he will torture them for 50 years. His offer is that instead of the torture,
he'll set them free as long as you put a tiny dust spec or equivalent in the eye of all three
to the three subjects in your multiverse, the nearly infinite people sometime within the next
50 years. You follow your rule that dust specs are less bad and you agree to that offer and you
are happy, but Loki just keeps doing this because there's so many people. It's very easy for him
to kidnap multiples per minute. It's still just a tiny fraction of all the people. So pretty soon
the end result is that Loki keeps doing this. You keep agreeing to it and your nearly infinite
number of subjects each get one trillion dust specs or small itches or tiny paper cuts or
whatever you choose as your, you know, minimum thing. This is the same as hundreds per second
for 50 years, which is the equivalent of torture and I would agree that getting hundreds of paper
cuts per second for 50 years is kind of the equivalent of torture, right? Okay, okay, but hold
on. If you if you decide that you're gonna it's like, okay, fine, Loki, you win. Go and torture
that person. What stops him from kidnapping another person and doing the same thing again,
keep offering to torture that same person? Nothing, but then the point is you can multiply it on
the other side too. Yeah, exactly. But then the point is one trillion people being tortured for
50 years is much preferable to almost an infinite amount of people being tortured for the next 50
years, which is what your choice comes down to. But is it but, but, but okay, so you're saying
that is that is where the math actually comes in because we're like, oh yeah, so is one dust
is better than 50 years of torture, but one trillion dust backs is much worse than one
trillion people being tortured. Well, okay, but okay, wait, so sorry, are you saying like Loki
continually makes that trade in someone's like dust backs and then he makes does it again? Yeah,
like dust backs, but he does this like what every five seconds or something one trillion
times across 50 years, which comes out to hundreds per second for 50 years. Yeah. Okay, so
okay, but but we're just multiplying, right? Yes, if one instance of 50 years of torture
is preferable to all those dust specs, then why is it not the case that one trillion instances of
50 years of torture is preferable to a nearly infinite 50 years of torture. So, okay, so
at some point it's to Loki to fuck off. Well, yeah, I mean, yes, at some point the most rational
action is to figure out how to kill Loki, but the point of the thought experiment is that if
you take that, you know, one is greater than the other. If you multiply both sides by a trillion,
that shouldn't suddenly flip it for some reason, but it's yet it seems to emotionally it hits
differently, but you're right. The math, the math stays the same. And so you kind of just be like,
all right, well, fine, Loki have it your way. Or I guess he doesn't have a preference, but
right, you stick with the torture, I guess. I mean, if I were just to come up to you and ask
you, would you rather Loki, right? But if I were just to come up to you and ask you, would you
rather torture an almost infinite amount of people for 50 years or only torture a tiny
percentage of that? Only one trillion people for 50 years, which would you rather do equivalent
to torture on both sides? Again, we're tied. We're basically putting the dust specs, the momentary
like, oh, okay, but that's happening like what every second you're like, oh, I can't give my
eye. I can't give my my eye. Wouldn't you consider a hundred paper clips a second for 50 years to
be torturous? Oh, a hundred paper cuts a second. Yeah. Yes. Okay. Probably. So yeah, I mean,
there you go. I mean, of course that changes things. I was definitely looking at it in the
scope of like one time decision. But yeah, I mean, I don't know at that at that point, if we're,
if it's one of those things where it's like someone like, like we assume we can't kill a god and
it's like a god and it's redoing this, why wouldn't you just torture everybody? Why wouldn't you
just torture everybody infinitely anyway? Because that's kind of his goal is like fucking with
you specifically. Yeah. To make the decision. I mean, I guess the real question is it's a
sadistic God. It's like what are you supposed to like if you can't solve the solution, if you
can't solve the problem by killing Loki, he's just gonna do that. Everything's gonna be tortured.
Yeah. There's nothing you can do. But really, the answer is find a way to kill Loki. Yeah.
But out of curiosity, where do you draw the line where it flips from one to the other?
I'll cross that bridge when I get to a man. Okay. You can give me like a one time decision like
that. I'll admit, yes, like, what was the name of the, was it Chakal? Anyway, Chakalatria? Chakalatria
brings up, has brought up an interesting point by kind of multiplying it more, multiplying the
shut up and multiply experiment, I guess, if you might say it that way. Yes, I was definitely
looking at it from the perspective of a one time decision. And in a one time decision, I would
pick dust specs like always. But if you're doing it like 100 times a second, it's kind of like
that's absurd, right? I mean, I mean, that's basically how money pumps work.
Or microtransactions or the old concept of a money pump. It's also like a Monte Carlo booking
swindler or something. I forget what the other term for it is. But it's when someone has three
things that they value not in the same gradient. Like for example, let's say you like fruits,
you like apples and oranges and bananas. Okay. You like an apple more than a banana. Sure.
So if you have a banana and I'll say, I'll trade you this apple for the banana,
if you give me one penny and you'll be like, yeah, I'm more than happy to do that. I'd much
rather have the apple than one penny. So a banana and a penny I give to you and I give an apple.
Yeah, totally. Right. Okay. But I also have an orange and you like the orange more than
you like the apple by more than one penny. So I'm like, hey, I'll give you this orange if you
give me that apple and a penny. And you're like, yeah, sure, right? Totally. Yeah, totally worth
one penny. And then it turns out that you now have the orange, but you have this weird thing
where you kind of prefer bananas to oranges. And so I'm like, hey, I'll give you this banana if
you give me that orange and one penny and you're like, yeah, okay, sure, sure. And then I close
and then I close the Skype session and I just eat my banana. Right. And then you understand what
the fuck is going on. But and you close the Skype session. But assuming that you cannot
close the spikes Skype session, it's the thing where I'm like, here's an apple. Oh, here's an
orange. Oh, here's a banana and you keep since you since you like these different things
differently, but not in an actual gradient. At some point you'll die if you don't eat. So
yeah, it's that kind of thing. I got you. Okay. So fun times. I think that's all the dust
spec stuff I have. This is just a comment that I don't think we have anything to reply to,
but is an interesting comment on the subreddit slash r slash the basin conspiracy. Massison says
the thousand paper cranes thing is a Japanese cultural cultural tradition, which both goes
in the shell and the Sudoku story reference. So yeah, that is apparently a much older thing
than any of us knew because we're not very good at Japanese story and culturaling things. Yeah,
fair enough. I can take that. But yeah, I like the link. If we're sharing links that listeners
were thoughtful enough to share, there was a great one from someone on the website, Richard J
Acton wrote in on the basin conspiracy dot com on the episode on digital rights and privacy.
I think the European digital rights, the edri or edry, I guess I'm not sure what that abbreviations
for is roughly equivalent to the EFF for your European audience. And they link to edry edri.org
and said that they have a good newsletter with tips and tricks and software software suggestions.
So if this wasn't going to struck a chord and you want some actionable things that you can do
to protect yourself, I haven't looked at this myself, but you have at least one recommendation
to check out edri.org. Cool. European listeners, you also have a resource. I'm not sure if it's
just for them or if we get their stuff here that we can do too. If it's just like chrome extensions,
I'll be for everybody. It's true. This was a fun one JD via email going way back
about our voting episode where we had Tim on and telling us reasons why it's not necessarily
rational to to vote all the time. He writes, you're telling people who have the desire to be
rational that it is rational not to vote. What then do you want to leave voting entirely in the
hands of the irrational people who vote for moral and religious and ideological reasons do not have
this constraint? You told your listeners to not have a voice in this last election. The very people
who have the most respect for your opinions and you're telling them not to vote. Before the next
election, I hope you will tell listeners, yes, it is worth researching the issues and yes, it is
worth taking the time to vote. Otherwise, we have a government that is largely composed of
representatives of the irrational Katrina contacted Tim, who then wrote us a very nice long reply.
But before we get to that, the essence of it is basically that's not exactly what I was saying
is what Tim says. And he's right. But I had a friend call me after the episode aired and he was
like, hey, why is this your guy's message? And so JD wasn't the only person who took it that way,
that it came off strongly enough anti voting that at least two people mistook it for that was our
actual, that was the position that was being supported here. Well, before we dive in, I had
my own take on it too, that I sort of agree with Tim that most people, including myself,
do not really research the issues all that much. And it would be best if most people didn't vote
and just leave the voting up to the people who do research and are actually really invest in this
sort of thing, which is the reason that things like decision markets work, because people then
are putting their own money on the line. And so the only people who are willing to put their money
on the line in these predictions are people who do do the research, whereas just voting for something
does not cost you any of your own money. So you can vote regardless of whether you've done research
or not, and lots of people do. And so in that respect, I think if everybody agreed to only
vote on things that they actually have researched heavily and know a lot about, we would have a
much better world. But I consider this very much a prisoner's dilemma sort of thing where not voting
unless you know something is cooperating and voting anyway is defecting. And if everyone on our sides
cooperates and doesn't vote, unless they're very well informed on the issue, but everyone on the
other side votes anyway, they're defecting. And if you know your opponent is going to defect,
cooperating is stupid. It is the losing move. It is how you get taken advantage of.
You have to defect as well. So everyone has to vote in order to cancel out everyone else's vote,
which is really kind of lame and shitty. And like we all know, the world is always much better
if everyone cooperates, but we can't like force that on people, but actually getting cooperation
is hard. You know, as your explanation there covered both sides of the conversation that had on
the phone with this episode aired with my friend. And so that's that's kind of where we wound up
was like, yeah, it'd be great, but since we don't live in that universe, we need to be out there
voting to. So we did actually contact him as well and Tim replied to us, not in these exact words,
but basically summarizing. He said that he he only told listeners that it's morally okay to not
vote, not that they shouldn't vote for the same reason that it's okay to drive a car or
wear an ugly shirt because it has a very, very small negative externality on the world. And
usually those things are okay. I don't know. Like, like, I don't know that about being a small
externality, though, because it seems like a somewhat of a pervasive attitude to be like,
you know, screw the elections. Who cares that it comes down to our delegates and we just shouldn't
vote because whatever that like voter apathy, like there are externalities to saying that. I mean,
I mean, maybe not so much in the way that he's wording it where it's like, it's morally okay
not to vote. That's a little different, but people encourage each other not to vote too.
There are externalities to that. No, we're living in one. For him, for his opinion,
that would be a positive externality that voting, if you don't know the issue very well and have
researched it heavily, is a negative externality on the rest of the world because you are voting
if you're not well informed. Yeah, okay, okay, I see, which, you know, 99.99% of people, including
ourselves are not on almost every topic, like how much research have you really done? Yeah,
no point. I have done less than a hundred hours of reading on any particular policy question.
Yeah, I mean point taken. He says also that he said that he was telling people it's less
worthwhile to vote if they aren't in a swing state because, for example, in California,
you probably don't even need to bother and then that's where the idea of vote trading came in,
right? Yes, and he pointed out that in the last election, we threw away literally three million
