where the technologies that you need to be developed might have a chance to be developed.
But it's close enough to like for people to care about.
It's not, it's within a lifetime, right?
Yeah.
So the like in the 80s, there was debate as to whether a computer would ever be smart
enough to beat a human in chess.
And there were a few other milestones that keep getting.
They were somewhat that we were several decades out from something.
We managed to beat it in the span of 10 years or so.
I know just like six months or less than a year before Alpha Go came out.
One of the top AI researchers was saying it's probably going to be about a decade or so.
And in a decade or so, we're going to have computers that can actually beat humans at
go.
That was in Nick Bostrom's Super Intelligence.
Oh, was it?
Where he had said, I think in 2012, maybe a little later, he is like, there's, there
seems like that's realistic to think that it might happen this decade.
Yeah.
And not that it'll happen in like, you know, a year after this book is published.
Yeah.
A year later, Alpha Go comes out and everyone is like, holy shit, we got to revise all our
estimates down because this is happening faster than we thought.
But on a wide survey, wide-ranging survey, and maybe it was in Super Intelligence that
this data point comes from, of AI researchers and engineers, the point where they think
there's about a 50 percent chance we'll get AI as smart as humans was, I think, 2050 or
so.
I'm afraid I can't remember, but I know what you're talking about.
Yeah, it was something within 30, 35 years.
And there's some people that say, yeah, there's a small chance we could have it within
a decade.
And almost all of them said that they would be very surprised, like a 90 percent chance
that we will have it within the century.
If we don't have it within the century, they will be frankly surprised.
So within 50 years or so is entirely reasonable based on what people are thinking right now.
And again, could be faster because we keep beating projections.
The reason that the rationalist community is a bit worried about the whole AI thing
is because earlier on, and I know I was of the same opinion when I was teens, maybe
even early 20s, was smarter than human machines would be awesome.
And we want them because if you're smart, then you know things that are right and wrong
and good and bad.
Right?
Like nothing that's smart is going to want to turn the earth into a parking lot or whatever
you'd want to do with earth.
And unless it was like utilitarian God AI, right, like had to make compromises.
So it's like, well, let's just kill, like maybe it contacts a bunch of other sentient
species somehow and we don't know it.
And it just is like, okay, there's just like nukes the whole earth and everyone's like,
what the fuck?
You're supposed to be the good AI.
And like, nope.
Actually, this was good.
Right.
No, I mean, that's a big part of the problem that everyone was like, yes, smarts are good.
We're smart.
There came the realization that there is absolutely nothing that ties our goals and things we
value to the concept of intelligence generally, that those things are completely unrelated.
So it's entirely possible for a super intelligent machine to not at all care about what humans
care about and just want to do something like the example that was originally used
and has stuck around is make paper clips.
Maybe for whatever reason, it was programmed to design paper clips.
Maybe I was just like the little test algorithm at first is here's the resources for you make
as many paper clips as you can out of them to test it.
And then it manages to become really big and powerful and take over and soon the entire
world is converted to paper clips and it doesn't hate humans, but humans are made out of atoms
that can use to make paper clips.
So let's let's do that and then start sending probes out throughout the solar system and
out into the galaxy to make everything into paper clips.
For what it's worth.
I think I'll submit that it's not so much the concern that the AI will spontaneously
decide to do something out of nowhere that no one anticipated.
But I mean, that's something that people throw out like all the time it's like, what about
when it decides to kill us?
It's like, well, why would it decide that out of nowhere?
Because haven't you seen the Matrix, bro?
Exactly.
That was the documentary, right?
About alternate timeline.
There's this tendency and there's already a word for this, the fallacy of generalizing
from fictional evidence.
And so people will look at fake examples that were generated for fun and then try and draw
real conclusions from them.
And that's a fallacy.
So there's no reason to suppose that the universe will actually end up like fiction
that people wrote.
That doesn't mean it couldn't happen, but it just means that like you don't have any
reliable basis for saying that it would happen.
There's no reason to privilege that hypothesis over all the other things that could happen.
So the concern is that it'll do exactly what it's told to, but it's what it's told to
do won't be specific enough.
Like anyone whose program computers knows how fucking annoying they are in that they
will do exactly what you say.
And that is a problem because that's honestly like that's part of the reason like I'm
somewhat, I don't know if pessimistic is the right word, but I guess I'm not super optimistic
about the timeline at which we will have recursively improving AIs that get really crazy smart
in the way that you were describing it earlier.
It's going to take so much refinement, I think, just so many iterations and failures to the
point just the cause for concern is even if it takes a century or two, however long it
does take, trying to make something which cares about the same things that humans care
about is really fucking difficult.
And if there are a number of research teams around the world, the ones who don't take
the time to align an artificial intelligence's goals with human goals, like continuing to
have a human race that exists and isn't miserable, will get there first because they aren't constrained
by things like trying to make sure that it is friendly to humans, friendly meaning cares
at least somewhat that our goals are not entirely run rough shot over.
So there's this idea of, or this thought experiment, like what if you met a race of aliens that
was just like us, they had all of our same values they cared about, you know, living
healthily and taking care of their people they care about and all that, but they lacked
the one thing, if they lack any one thing, it can be just disastrous for us to like try
and be implemented like them.
So say if they didn't have the same capacity for boredom that we have, and that to them
they could play the same video games and as long as you change some colors of pixels,
then it was like this whole new experience and all fun over again, but that's just not
how we work.
And so just, I mean, it's just, it's a trivial little thing, but just the idea that, I mean,
you have to get everything that trivial exactly right, because then if it takes off and if
you want, I mean, it also depends what you're doing with the AI that you're building and
if you want it to make the world better for us, or if you want, you know, if you want
it to sit in a box and we can ask you questions or whatever.
But if you want to build the full, the full God AI that takes over and makes everything
awesome for everybody, it needs to have a very well calibrated definition of awesome.
Because the, yeah, the problem being that the space of all possible intelligences,
intelligent agents in the universe is huge and the, the sphere that of things that we
care about and that can interact with us on this planet peacefully is a tiny, tiny subset
of that.
So it's incredibly hard to hit that goal and very easy to make something that just happens
to not care about things at all in the same way we do and uses all the resources to pursue
its own goals.
Yeah.
I mean, again, I think part of it is just like its own goals would be the ones that
we gave it, but it was implement them weirdly.
I mean, that's possible, but you, you can just not give it any goals and it can still,
there's, there's so many different ways to be an intelligent agent and almost none of
them are something that we would recognize as valuable.
Fair enough.
Even if we weren't to give it a goal of its own, make a shit ton of awesome video games.
Right.
And, but, you know, then you have the entire world converted into awesome video games
and there's no humans to play them.
So again, it's, it's like, I mean, I mean, the person where it does a good job.
Yeah.
Right.
I need you to do it.
I need you to do a better job.
Okay.
Yeah.
Can you do that for me?
Do a better job.
Or even if it keeps humans live, you know, there's some things that we found you other
than video games.
So if there is a whole bunch of humans and the only thing we have that we can do with
the resources in our solar system is play video games anymore, some people would find
that distressing.
Yeah.
Some people would also find it distressing to be made into people that only enjoyed
playing video games.
Right.
There are people that don't like video games.
Well, but I mean, there's some people who don't like playing video games and there's,
and those people probably, probably resent the idea of being turned into people who like
playing video games.
Right.
Yeah.
So that'd be, I mean, any kind of like infringement on autonomy is probably pretty disturbing
to most people, including myself.
Yeah.
And that may be the easiest way to fix a lot of problems is just to fix the humans.
So they stop wanting these things.
I don't know.
I mean, we're talking about an AI that has like instantaneous connections to everyone's
brains and can tweak things like, I mean, no, not necessarily, but like AM from I have
no mouth and must scream or no, it's I have no mouth and I must scream.
Yes.
That's it.
No, not necessarily, but maybe I mean, it's, it's impossible to say just what is possible.
If you are super humanly intelligent and have the entire resources of the planet or the
solar system or whatever at your disposal, true, that's part of the issue of like the
event horizon where you can't predict what'll happen because things, we don't have models
that work there yet.
Like, I mean, what, what chimpanzee out there could predict that a human would be able to
fly through the air across continents, I don't have any feathers or wings.
What humans 150 years ago could have predicted predicted that too, right?
And maybe people have been trying to fly forever.
People have been dreaming it, but like to think that it would happen and people just
would be doing it on their days to and from work.
That I don't think there are that many people running around before electricity fantasizing
about that, right?
Yeah.
But just, just the fact that, you know, the, the, we don't know the deepest laws of physics
and we don't know what is technically possible in the universe at the most basic level.
We, maybe you can somehow do that sort of thing fairly easily.
Maybe it turns out there's a really easy way to, to break the light speed barrier and
we just weren't smart enough to see how to tweak physics to do it.
And if not even light speeds fast enough for like to count as instantaneous across the
globe, right?
So like Wi-Fi and nanobots and you've got your, your brain modifying happening more
or less instantaneously across humanity.
What about, so, so what about kind of in between states?
So we're thinking like super theoretical future stuff.
What about like stuff that's is on the horizon, things that we can probably see like autonomous
cars that, you know, we already kind of have that, but we're going to have more of that,
