Hey, this is The Basin Conspiracy, I'm Inyash Brotsky, I'm Stephen Zuber, and today we
have a guest.
My name is Sean.
Hi, I've been here a couple of times.
We're going to talk about artificial intelligence and who is afraid of it and should we be?
Yeah, pretty much.
I think we wanted to have like a primer episode because it's something that we've intentionally
referred to a few times.
Yeah, it's a pretty major focus of the rationalist community, and we've referred to it a couple
of times, but never really had an episode on it, so we decided now would be a good time
to do that.
Now, I'm actually not familiar all that much with what the rationality community says
about artificial intelligence, like if it really gets in depth with it, is it really
speculative as far as what to be afraid of?
Does it try to extrapolate a lot?
Because it seems to me like artificial intelligence right now is in such an incipient stage that
like the main reason we had you on here is because you don't know that much about what
the rationalist community says, so you can be like the person from outside who is asking
these questions.
I thought I was like, yeah, this Sean is perfect for this.
The focus is not on what is currently happening.
I mean, it is a little bit in that it's a sign of things to come, but it's more along
the lines of worry about what happens if humans do make an AI and what that means.
We're talking like singularity then, right?
Singularity is such a ill-defined concept.
I think I don't like it too much, and I think in general, the community has been moving
away from that term.
Do you want to say what singularity is?
Singularity has at least two or three distinct definitions, and the one that people speaking
broadly in the rationalist community are concerned with is the idea of an intelligence explosion
where say you get an AI that just kind of jumps us right into the point, and I say when
rather than if, I think that it's inevitable that we'll reach this stage of technology.
I don't think there's any physics barriers or concepts that will be impossible to meet
to make this actually happen.
The question is, does it happen in five years or 500 years?
When artificial general intelligence or AGI for short is created, the idea is that it'll
be better at humans than everything that really matters to us, including writing AI programs,
and so it can write new AI programs or it can rewrite itself, and then subsequently,
once it's better at it than we are, if you could say offspring or its next iterations
will be better than it previously was a second ago, lather, rinse, repeat, and you've got
a quick takeoff to just super intelligence.
I think the term was coined by sci-fi writer Werner Vinci.
That's right.
Yeah, in the mid-80s, and it's basically the concept that when all you have is a rock,
you can't do very much with it, but once you have a rock that has been sharpened and attached
to a stick, you now have a tool, and you can use that tool to make even better tools.
Humans have been doing this for millennia now, and as our tools get better, we can use
those tools to make even better tools to the point where now we can see individual atoms
with tunneling electron microscopes, and we can put things up in orbit and people on
the moon if we want to, and we have this worldwide communications network.
One of our tools nowadays is computers and the programs that run on them.
Right now, humans are making those, but once our tools get good enough, they can start
replacing us and things, and yeah, like you said, eventually we can make a program that
writes other programs, and once we can do that, there's no particular reason the program
wouldn't keep making better versions of itself to make programs that are even smarter and
better and get to the point where they are better than humans at anything that requires
intelligence.
I feel like even if it was just specifically designed to do that, or I mean, the other
thing to consider is that even if you have an AI that I think it was Nick Bostrom wrote
the...
Superintelligence?
Superintelligence, the book, and he has this phrase that I really liked that he says,
you know, as AI gets gradually closer to human intelligence, there's no reason to think
that it'll stop at Humanville Station, it'll probably just zoom right by, and part of that
is because if you get an AGI that's running anything like how we run, it's going to be
doing so at 1.5 million times faster than we do it, just as far as like, because our
thinking speed is what, 100...
200 hertz, 1 to 200 hertz.
Yeah.
So, and the theoretical limit for as fast as a computer can do it is like the speed
of light.
So, even if it's thinking at the human level, it's thinking at a rate that dwarfs us like
crazy.
You can get from like ancient Greece to now in an afternoon.
If it's thinking millions times faster than we are, it can cover a lot of ground.
I want to get back to that in just a second, but before we do, what was your understanding
of what the word singularity means?
Oh, I was thinking of recursive improvement of the intelligence question, which is just
like a super abstract, like just conceptual, like it's just, because it's like the seed
for a lot of like science fiction writing, you know, obviously, like everyone's heard
those stories before, the robots take over.
Yeah, because that's fascinating.
Yeah, no, right, like the creator race being destroyed by its creation.
And all parents fear, I guess, I mean, I thought parents were supposed to fear their
children dying before them, but I don't know.
Maybe I see this is why I'm not a parent, I'd be like, I'm not feeding you bastards,
because if you grow tall, you'll beat me up.
Yeah, that's totally want to keep you short liable that that's the version of the singularity.
I'm thinking of it's just the really basic conceptual, you know, explosion of intelligence
like you were talking about.
I don't, I'm not aware of any other definitions, unless we're talking about like incremental
moments leading up to that or something.
So there's kind of hard to figure out when that would even be.
Yeah, no, no, I think, so like in a quick nutshell, there's Kurtz-Wiles version, which
is where like technology growth happens, starts happening really, really fast.
And I'm not sure where he draws the line at singularity happening.
Bridge technology where you can like escape velocity and stuff like that, right?
Like he talks about living forever.
Yeah, yeah, him and Aubrey de Grey, but different avenues, I think they're looking for.
Aubrey de Grey talks about longevity, escape velocity.
I think that might be what you're thinking of, which he does the Methuselah project.
He's looking for like life extension research institute.
And then the third one is the idea that on the other side of intelligence explosion or
whenever you get smarter than human intelligence, whether it's an explosion or a slow takeoff,
you get this, this point where your, your models of being able to predict the future
break down because you can't predict what an agent smarter, what an agent that's smarter
than you will do.
Just like I'm watching AlphaGo play Go and I can't guess what move it's going to make.
Because if I could, I'd be as good at playing Go as it is.
I can, I can anticipate what I think it might do, but it makes a surprising move and it
wins.
I wouldn't have predicted that, right?
So that's, that's where like the, the model breaking down singularity word was borrowed
from.
Charles Strauss has an amazing story about different levels of intelligence in, in agents
where he says that he, you know, has to take his cat to the vet like once a year or something.
And the cat knows that when he brings out the little cat carrying case thing, that means
it's vet time.
And the cat is smart enough to see the case and be like, oh shit, I'm going to get shots.
I am running.
And so the cat sees these things in environment is smart enough to figure out what's happening,
takes action to avoid it.
And it goes running for the cat door and it slams face first right into the cat door
because Strauss has locked it so it doesn't open.
And the cat is always shocked that this one time of all times the cat door happened to
not work.
What the hell is happening?
And Charles Strauss is like, well, I was simply thinking one move ahead of the cat.
I knew the cat would run towards the cat door.
So I locked it beforehand and it's that sort of similar thing where you cannot predict
what someone significantly smarter than you is going to do because then you would be as
smart as them.
Sure.
That makes sense.
So that's, I think, the three bullet points in a nutshell.
I'm kind of curious to do this in the format that I think we're modeling a bit where you
kind of just throw out a question and then the three of us can spit ball around it.
Is that something fun?
Yeah, sure.
I did want to get back to, before we did that, when you said the whole thinking things a
million times faster than humans, first of all, would be incredibly useful because if
I could do 10 years of mental labor in the span of a day, it is a legitimately handy
thing to do.
But as someone has pointed out that even if you gave 10,000 monkeys 10,000 years to come
up with something, they're not going to come up with something that a human can come up
with in just one week because of the difference in level of intelligence.
So the speed is one aspect of it, but it's not the only thing.
It's also the just legitimately smarter than human intellect.
Yeah.
And that's the other thing, like right now with, I don't know how much the two of you
know about neural networks, but even right now, so like neural networks, like the concept
has been around for like a really damn long time, like I believe since the 1950s.
And then it got kind of scrapped and it wasn't very well defined over time.
And like, I think it had a bit of a resurgence in like the 90s, late 80s, 90s.
And then it got dropped again.
People were like, oh, this is crap because it's really not like, like basically the way
they were going about implementing them wasn't up to up to snuff exactly.
So and then now it's had another resurgence and now that's why you have things like Spotify's
Discover Weekly, like that's based on a neural network.
AlphaGo was a neural network.
Right, exactly.
And like deep, you know, like the image, the deep dream or, you know, stuff like that.
But there's this tendency for, if you like, you have to give a neural network like training
algorithms so it like can learn.
But sometimes it's like too smart for its own good where it will take data points and
it'll try to constrain boundaries around those data points instead of using like a line to
divide those like as a, this category, this other category.
So you have to like, you can't let it learn for too long or else it'll over refine it
and become very inelegant or very like, it's not good at all.
Like when humans see faces in clouds and in everything.
Yeah, right.
So on machine scale.
So we're still at that stage where like we're really trying to make neural networks more
efficient and more reasonable to do the things we want them to do.
And unfortunately, even in the best case scenario often requires a lot of human intervention.
So I honestly like, I'm just curious like when, like when do we realistically think there's
going to be this explosion of intelligence that actually is going to take off in every
possible direction and actually be smarter than humans and be able to like do abstract
thinking like that's.
So the, I mean, that's, that's a very speculative question because how do you answer something
like that, right?
Yeah, right, right.
I mean, I'm just kind of thinking aloud.
I mean, I'm not expecting an answer really.
I have sort of an answer.
Okay.
It's not like a date that I'm going to give you.
Okay.
2028.
Exactly.
August 17th.
For the record, I want to interject.
That's one reason I'm not a huge fan of Ray Kurzweil is he keeps putting years, sometimes
months on when this is going to happen.
Do you think he's just projecting a lot of optimism because he wants to live forever?
Yes.
Me too.
But I mean, well, it's probably a bit, right?
Like it, it happens to be during his lifetime and happens to be at like that 20 year ish
mark all the time to where like it's far enough out where there's some flexibility and like
where the technologies that you need to be developed might have a chance to be developed.
But it's close enough to like for people to care about.
It's not, it's within a lifetime, right?
Yeah.
So the like in the 80s, there was debate as to whether a computer would ever be smart
enough to beat a human in chess.
And there were a few other milestones that keep getting.
They were somewhat that we were several decades out from something.
We managed to beat it in the span of 10 years or so.
I know just like six months or less than a year before Alpha Go came out.
One of the top AI researchers was saying it's probably going to be about a decade or so.
And in a decade or so, we're going to have computers that can actually beat humans at
go.
That was in Nick Bostrom's Super Intelligence.
Oh, was it?
Where he had said, I think in 2012, maybe a little later, he is like, there's, there
seems like that's realistic to think that it might happen this decade.
Yeah.
And not that it'll happen in like, you know, a year after this book is published.
Yeah.
A year later, Alpha Go comes out and everyone is like, holy shit, we got to revise all our
estimates down because this is happening faster than we thought.
But on a wide survey, wide-ranging survey, and maybe it was in Super Intelligence that
this data point comes from, of AI researchers and engineers, the point where they think
there's about a 50 percent chance we'll get AI as smart as humans was, I think, 2050 or
so.
I'm afraid I can't remember, but I know what you're talking about.
Yeah, it was something within 30, 35 years.
And there's some people that say, yeah, there's a small chance we could have it within
a decade.
And almost all of them said that they would be very surprised, like a 90 percent chance
that we will have it within the century.
If we don't have it within the century, they will be frankly surprised.
So within 50 years or so is entirely reasonable based on what people are thinking right now.
And again, could be faster because we keep beating projections.
The reason that the rationalist community is a bit worried about the whole AI thing
is because earlier on, and I know I was of the same opinion when I was teens, maybe
even early 20s, was smarter than human machines would be awesome.
And we want them because if you're smart, then you know things that are right and wrong
and good and bad.
Right?
Like nothing that's smart is going to want to turn the earth into a parking lot or whatever
you'd want to do with earth.
And unless it was like utilitarian God AI, right, like had to make compromises.
So it's like, well, let's just kill, like maybe it contacts a bunch of other sentient
species somehow and we don't know it.
And it just is like, okay, there's just like nukes the whole earth and everyone's like,
what the fuck?
You're supposed to be the good AI.
And like, nope.
Actually, this was good.
Right.
No, I mean, that's a big part of the problem that everyone was like, yes, smarts are good.
We're smart.
There came the realization that there is absolutely nothing that ties our goals and things we
value to the concept of intelligence generally, that those things are completely unrelated.
So it's entirely possible for a super intelligent machine to not at all care about what humans
care about and just want to do something like the example that was originally used
and has stuck around is make paper clips.
Maybe for whatever reason, it was programmed to design paper clips.
Maybe I was just like the little test algorithm at first is here's the resources for you make
as many paper clips as you can out of them to test it.
And then it manages to become really big and powerful and take over and soon the entire
world is converted to paper clips and it doesn't hate humans, but humans are made out of atoms
that can use to make paper clips.
So let's let's do that and then start sending probes out throughout the solar system and
out into the galaxy to make everything into paper clips.
For what it's worth.
I think I'll submit that it's not so much the concern that the AI will spontaneously
decide to do something out of nowhere that no one anticipated.
But I mean, that's something that people throw out like all the time it's like, what about
when it decides to kill us?
It's like, well, why would it decide that out of nowhere?
Because haven't you seen the Matrix, bro?
Exactly.
That was the documentary, right?
About alternate timeline.
There's this tendency and there's already a word for this, the fallacy of generalizing
from fictional evidence.
And so people will look at fake examples that were generated for fun and then try and draw
real conclusions from them.
And that's a fallacy.
So there's no reason to suppose that the universe will actually end up like fiction
that people wrote.
That doesn't mean it couldn't happen, but it just means that like you don't have any
reliable basis for saying that it would happen.
There's no reason to privilege that hypothesis over all the other things that could happen.
So the concern is that it'll do exactly what it's told to, but it's what it's told to
do won't be specific enough.
Like anyone whose program computers knows how fucking annoying they are in that they
will do exactly what you say.
And that is a problem because that's honestly like that's part of the reason like I'm
somewhat, I don't know if pessimistic is the right word, but I guess I'm not super optimistic
about the timeline at which we will have recursively improving AIs that get really crazy smart
in the way that you were describing it earlier.
It's going to take so much refinement, I think, just so many iterations and failures to the
point just the cause for concern is even if it takes a century or two, however long it
does take, trying to make something which cares about the same things that humans care
about is really fucking difficult.
And if there are a number of research teams around the world, the ones who don't take
the time to align an artificial intelligence's goals with human goals, like continuing to
have a human race that exists and isn't miserable, will get there first because they aren't constrained
by things like trying to make sure that it is friendly to humans, friendly meaning cares
at least somewhat that our goals are not entirely run rough shot over.
So there's this idea of, or this thought experiment, like what if you met a race of aliens that
was just like us, they had all of our same values they cared about, you know, living
healthily and taking care of their people they care about and all that, but they lacked
the one thing, if they lack any one thing, it can be just disastrous for us to like try
and be implemented like them.
So say if they didn't have the same capacity for boredom that we have, and that to them
they could play the same video games and as long as you change some colors of pixels,
then it was like this whole new experience and all fun over again, but that's just not
how we work.
And so just, I mean, it's just, it's a trivial little thing, but just the idea that, I mean,
you have to get everything that trivial exactly right, because then if it takes off and if
you want, I mean, it also depends what you're doing with the AI that you're building and
if you want it to make the world better for us, or if you want, you know, if you want
it to sit in a box and we can ask you questions or whatever.
But if you want to build the full, the full God AI that takes over and makes everything
awesome for everybody, it needs to have a very well calibrated definition of awesome.
Because the, yeah, the problem being that the space of all possible intelligences,
intelligent agents in the universe is huge and the, the sphere that of things that we
care about and that can interact with us on this planet peacefully is a tiny, tiny subset
of that.
So it's incredibly hard to hit that goal and very easy to make something that just happens
to not care about things at all in the same way we do and uses all the resources to pursue
its own goals.
Yeah.
I mean, again, I think part of it is just like its own goals would be the ones that
we gave it, but it was implement them weirdly.
I mean, that's possible, but you, you can just not give it any goals and it can still,
there's, there's so many different ways to be an intelligent agent and almost none of
them are something that we would recognize as valuable.
Fair enough.
Even if we weren't to give it a goal of its own, make a shit ton of awesome video games.
Right.
And, but, you know, then you have the entire world converted into awesome video games
and there's no humans to play them.
So again, it's, it's like, I mean, I mean, the person where it does a good job.
Yeah.
Right.
I need you to do it.
I need you to do a better job.
Okay.
Yeah.
Can you do that for me?
Do a better job.
Or even if it keeps humans live, you know, there's some things that we found you other
than video games.
So if there is a whole bunch of humans and the only thing we have that we can do with
the resources in our solar system is play video games anymore, some people would find
that distressing.
Yeah.
Some people would also find it distressing to be made into people that only enjoyed
playing video games.
Right.
There are people that don't like video games.
Well, but I mean, there's some people who don't like playing video games and there's,
and those people probably, probably resent the idea of being turned into people who like
playing video games.
Right.
Yeah.
So that'd be, I mean, any kind of like infringement on autonomy is probably pretty disturbing
to most people, including myself.
Yeah.
And that may be the easiest way to fix a lot of problems is just to fix the humans.
So they stop wanting these things.
I don't know.
I mean, we're talking about an AI that has like instantaneous connections to everyone's
brains and can tweak things like, I mean, no, not necessarily, but like AM from I have
no mouth and must scream or no, it's I have no mouth and I must scream.
Yes.
That's it.
No, not necessarily, but maybe I mean, it's, it's impossible to say just what is possible.
If you are super humanly intelligent and have the entire resources of the planet or the
solar system or whatever at your disposal, true, that's part of the issue of like the
event horizon where you can't predict what'll happen because things, we don't have models
that work there yet.
Like, I mean, what, what chimpanzee out there could predict that a human would be able to
fly through the air across continents, I don't have any feathers or wings.
What humans 150 years ago could have predicted predicted that too, right?
And maybe people have been trying to fly forever.
People have been dreaming it, but like to think that it would happen and people just
would be doing it on their days to and from work.
That I don't think there are that many people running around before electricity fantasizing
about that, right?
Yeah.
But just, just the fact that, you know, the, the, we don't know the deepest laws of physics
and we don't know what is technically possible in the universe at the most basic level.
We, maybe you can somehow do that sort of thing fairly easily.
Maybe it turns out there's a really easy way to, to break the light speed barrier and
we just weren't smart enough to see how to tweak physics to do it.
And if not even light speeds fast enough for like to count as instantaneous across the
globe, right?
So like Wi-Fi and nanobots and you've got your, your brain modifying happening more
or less instantaneously across humanity.
What about, so, so what about kind of in between states?
So we're thinking like super theoretical future stuff.
What about like stuff that's is on the horizon, things that we can probably see like autonomous
cars that, you know, we already kind of have that, but we're going to have more of that,
I think.
I mean, they're going to put a lot of people out of work if they work.
Autonomous workers, autonomous cars that drive you ever, like, like right now Tesla
basically has that, I think, but they're not fully autonomous.
They're like, it's, it's like mostly autonomous with some human intervention, like you have
to be in the car.
Yeah.
Obviously you would.
Those things, they're also sort of important, but none of those are extinction level events,
you know?
And not, not just extinction level, but permanently turning a growing sphere of the universe into
something worthless even after we're gone potentially.
Yeah.
But, but as far as like near horizon stuff, I do think that automation is one thing that
we see happening now and we'll see it continue to happen.
Just think of like all the, all the people, all the factory jobs that humans used to do
are all done by robots now and they're done faster.
They work nights.
They don't demand pay and unions and stuff.
And so like that, that takes off pretty quickly.
I mean, I think it's extremely likely that in the next 15 years we'll have self-driving
cars.
So like people won't own cars the way that we own them now, or like if we do, it'll
be kind of like a fringe thing, but you'll just like order a car to, you know, whenever
you need one and it'll drive out from whatever local bay and it'll take you to wherever you're
trying to go, like an Uber, but minus the person.
And like fast food workers will be replaced with, you know, self-order kiosks and robots
built, you know, making your burgers or whatever.
That's definitely something that I see as likely in the next.
I like that 20 year vague number, but I mean, we're seeing beginnings of it now.
So it's not, it's not somewhat speculative.
It's just like, when will this happen?
How wide it'll be and how fast it'll happen.
And like you said, there's, there's no reason to think that a computer can't be
made to, to optimize things as well as a human can, even if it doesn't think exactly
like a human can, there's no boundary that says, well, it can't become this smart.
Given a specific task, like driving and not crashing, you know, like, like definitely
that seems, that, that seems like that's something in reach in the near future.
Right?
Like, say we get to that point 15 years, we got these fully autonomous cars that are
just like totally better than humans at driving, or at least, you know, basically
the, like on par with the best drivers, right?
Just they're really good.
And like, if an accident happens or like you have that, you're like, some
emergency thing happens, it's like the trolley problem and it tries to solve that
and it like kills people.
Like, but like who's accountable in that case?
Cause it's, it's, it's, it's AI.
So, so what, so then what do you do?
Like who gets, who gets in trouble?
Like what happens?
I mean, why do you necessarily have to assign blame in those sorts of cases?
It, I mean, I suppose you don't have to happen, right?
Every now and then bridges, I mean, like the legal system has to acknowledge
it's something, right?
I mean, the legal system might, if there's a human that it was involved and can
be blamed, but the whole point of assigning blame is to change the actions
of other people, right?
If, if someone has done something wrong, you punish them so other people will
do that same kind of thing.
They will stop driving drunk or they will follow the speed limit when they
should things like that.
Well, so nobody, nobody's, nobody's fault, right?
Probably no fault.
Yeah.
Like, I mean, sometimes an earthquake happens and a bridge falls down.
And what, who are you going to, you know, blame for that?
As long as it was built to code, then.
That makes sense.
I'm just wondering like what the, what, what the growing pain, so to speak,
will be like moving toward that idea.
Like someone's going to be looking for someone to blame in a case like that.
Yeah.
The first time that somebody is in a fully self-driving car and
for whatever reason, a family of three runs out in front of it and the car has
to swerve off a bridge to not kill that, those three people and kill the one,
you know, passenger of the vehicle.
Somebody's going to be pointing a finger and like, who do we go?
You know, the family, that person will be like, who do I fucking beat up?
Right?
Or what, who do I, who do I vent at my, my rage at?
And they're not going to, they're not going to vent at the algorithm that
made the decision.
They're going to vent at the company that produced that algorithm.
And, and we're going to do this shit out of them, right?
Yeah.
Or try to get money out of them.
I think it's extremely likely that you'll have to sign very detailed
explanations like what you're getting into.
But then that is sort of a question.
You mentioned like trolley problem things.
Like if I get into a self-driving car, it's different than if I'm driving it.
Like if I'm driving, I might well make the decision to throw my car off a bridge
to save a family.
Yeah.
But having that decision made for me seems like something A that not everyone
would get on board with and B, I think there's this impression that it's different
in an important moral sense, right?
I don't think it is necessarily, but I do see how people will, if they, they'll,
they might want to check a box.
Nope, save me at all costs.
But I don't know if make, I don't even know if adding that option would be moral to
do, right?
I think if there is a large percentage of people who would not utilize self-driving
cars, unless there was such an option, then it should be an option and maybe even
standard in cars for at least the first few decades, because just the amount of
lives that would be saved by taking humans out of the driving.
Yeah.
I know, kidding.
Cause how many accidents are there a year by people who are like drunk driving or,
you know, even sobering.
Like it's like, it's absolutely going to be better.
Like, like if, if we get to a point, like, you know, Thomas cars in the future,
they're going to be like far fewer accidents so that like, it'll be weird
and anomaly when there are accidents, right?
Like in theory, that should be how it ends up, right?
Yeah.
Unless all the cars go apeshit all at once and start rampaging and just
killing everybody, then then it wouldn't decide that they don't like humans anymore.
Exactly.
Strive them all off the cliff.
But, but obviously we're talking about like 15, 20, 30, you know, years of,
of like very focused implementations of, of artificial intelligence that doesn't
involve making, using abstract thought and making big decisions and like seeing
the universe or whatever we were kind of talking about with like super future AI.
Yeah.
Yeah.
I think 30,000 people on average die every year in the US from automobile accidents.
And yeah.
So what's that every day?
Right.
So over 90% of those are due to human error.
Oh yeah.
Well, I'm sure the best, yeah.
I mean, they're being, if, if that was just cars falling apart randomly,
then that's that, that would be a whole big lawsuit thing.
Right.
So, oh yeah.
Um, yeah, it's just people want to go fast in the way to work or, you know,
they think they can change lanes.
They think they can make it to light.
Just little things happen and you can only gamble so many times and before,
you know, you lose, right?
So, uh,
and all those weird trolley situations always like star you out in a thing like
there's a family of three crossing the street and you have to swerve to and kill
yourself or else kill the family.
It's, it's, it's very hard.
Questionnaire, wasn't that like a thing that MIT was doing?
Like, what would you do?
Would you kill these people or these people?
Yeah, but it's with self-driving cars even being put in that position is nearly
impossible.
It's not like the self-driving car does not see the family of three or like it's
barreling down residential streets at 90 miles per hour.
I mean, they have radar.
They can see 360 degrees around them all the time.
It's reaction speed is fast.
Yeah, right.
I mean, every now and then maybe something will happen.
There's some kind of glint off a truck that fuses its sensors, but this is not
going to be a regular occurrence.
Yeah.
No, I totally agree.
And I feel like people are like, so if this impossible situation that happens
once every decade were to happen, no, God, really?
Yeah, you don't need to spend a lot of time planning for those fringe contingencies.
I think you're right.
Yeah.
But the, the, the crazy thing about going back to this super human level AI, the
crazy thing about that is that it is something that is, it's not physically
impossible.
So eventually we're going to get there.
I don't, I don't see how we could not get there because having a super human
intelligence is just such a handy tool.
Someone's going to make it.
I got I reminded of when I can think of an obvious way it won't get there.
Oh, I think it's wiped out.
Yeah, we kill ourselves.
I remember when things don't go terribly.
I remember when CRISPR was, you know, discovered and right away the, the
bioethical board, whatever it is in the US said, we will not be doing any CRISPR
experiments on human embryos.
That is unethical and no one was allowed to do that.
And within one year, China was like, Hey, here's these CRISPR embryos we got
going and is like, yeah, what are you going to do about China or, or Korea or
whatever country it is that things like, Hey, we could use that tool, especially
since we are being threatened by this hegemonic military force.
So at some point, as long as it's physically possible, we will get there.
And if there's a race, we'll probably not get the, the friendly version.
We'll get something that isn't quite aligned with what we want.
And then we all go extinct.
And you'd think that the human race would put some small level of resources
into maybe looking ahead at this.
And it's, it's minuscule.
I remember when, when Trump was elected, I thought that point though, right?
So, I mean, we still have a lot of time to be thinking about it.
Can I, can I, the problem is, well, yeah, go ahead.
Well, this is actually the perfect tie into what I was going to mention
earlier that Stuart Russell wrote the, he wrote the textbook on AI called our
artificial intelligence, a modern approach.
And he currently is on the board of a few institutions and stuff working on this.
And he uses the, the thought experiment or the intuition pump of imagine if
we got a message from space saying, Hey, we're aliens, we'll see in 50 years.
And like people wouldn't have exactly the approach that you gave.
Well, we've still got time.
We'll, we'll figure this out when, as it comes closer.
Okay.
You get a message and it's just, we will see you in 50 years.
Well, say it's unequivocally from aliens.
The idea is that we're told like they're going to come into physical contact.
Well, they're going to come to our planet.
What does that actually mean though?
We're not, that's, that might be part of the point, right?
I just, we know, we know they're coming.
And so if they're just going to look at us and that's it.
Well, but the thing is, how do you prepare?
So like the idea, the, no one would take the stance, well, that's 50 years away.
We can start thinking about it in 20, 25 years.
We've, we've got time.
No one, no one really, I think they would be like, holy fuck, we need to scramble.
We need to be ready for anything when this happens.
And people aren't taking that with the, with the definite, although less certain
horizon of, of artificial intelligence and the thing is these things are really
hard problems and take a lot of time, right?
And it's hard to feel like emotionally grappled with it the way the alien
example catches people.
Well, I mean, when Trump was elected, I thought at that night that, I mean,
maybe one or two percentage points here, there could have swung things, but we
were in a situation where it was close.
There was 50% of the electorate willing to vote for him.
And how the fuck in the past four months, like when all the people I knew were
out there, crazy campaigning and being really part, you cannot make much of a
difference on that wide of a level in a few months.
The place to start would have been back like 15, 20 years ago and addressing
these concerns about the middle class being gutted, the working poor not having
jobs anymore, or even just the financial crisis in 2008, I think is really the
last chance we had to divert the sort of thing.
So it was eight years ago that you would have to start working on not getting
Trump elected to make sure Trump didn't get elected.
And you just can't do it in the last minute.
And I think this is the same sort of thing where you have to get started early
in front of these things because they are hard, difficult problems that take a
long term sustained effort.
And we don't know exactly when it'll be.
It could be in 50 years.
It could be in 90 years, but it could be like in 20 years maybe.
And because it's so hazy, the fact that almost no one is looking at it is
ridiculous. It's like there's less than 100 people in the entire planet, as far
as we know, that is actually working on this problem.
And I'm not saying that...
On friendly AI, not AI in general.
Yeah, on friendly AI.
And I'm not saying that we have to dedicate 10% of our
world GDP to fixing this or something.
But I think maybe having more than 100 people working on it is a good idea.
Where did you get 100 from?
Oh, God, where did I get 100 from?
It was, I believe, a Wait But Why article, now that I think about it.
And he has a reference to where he got that number.
But it's basically, Miri, the Future of Humanity Institute, a few other
small organizations that employ a dozen to two dozen people at most, each of them.
And comes out to less than 100.
I see what you're saying about like, yeah, maybe we should be given this.
It's due focus.
And certainly, if there are only 100 people who are taking this seriously
and actually thinking about it, maybe there should be more.
I would agree with that.
I guess I would also bring up that as time goes on, we're going to have more
of an idea of what's on the horizon, right?
And maybe maybe maybe in like 10 years, for all we know, someone could be like,
oh, shit, guys, like this is actually a thing now.
Like we've seen this happen with AI and this happened with AI guys.
No, seriously, like look out, because this is coming up now that we can't see
this now, but maybe we can see this soon enough to where we're like, oh, crap.
And now we start doing things.
People have been saying that for almost 10 years now.
Okay, and fair enough, right?
It's some people are finally starting to listen, which is nice.
I mean, like Elon Musk's and Stephen Hawking's and Bill Gates's and all those
are finally over the last.
Yeah, mostly it's been happening in the past year, but some people have
been starting to sign on for five, six years now, so more attention is being
drawn to it and I am very happy about that, but it's still slow.
How many more people would you want to see?
I guess on that focused on it and I mean, we already have like the big minds.
We have Elon Musk, we have Stephen Hawking thinking about this stuff and
actually I don't think either of them are actually working on it, because
neither of them are AI researchers.
They are just sure sure to pull attention to it and Elon Musk is pouring
money into it or yeah, yeah, I mean, I mean, like what he's working.
Well, I mean, Tesla Tesla's cars are like almost fully autonomous, right?
Yeah, like potentially like they can be like, so didn't someone get
driven to the hospital like it?
Oh, really?
Yeah, I thought I heard something about someone who like he like injured
himself and he got into he got into a Tesla and it drove him to the hospital
and actually saved his life.
Boy, I mean, don't quote me on that.
I guess that rings a bell for me too.
I mean, I think that's one of the reasons that Musk was an early funder of
of of Miri on other of these initiatives because Miri is machine
intelligence research Institute.
Oh, right, right.
Okay, they basically are trying to solve the alignment problem, figure
out how to create once an AI is created, how to make sure that its values
do not end up destroying all of humanity.
So part of part of the confusion there is like, how do you keep a utility
utility function consistent when through self modification, when you're
doing the self modifying to yourself and you know, so the parts of you
that are changing your utility function or refining it, the idea is how
to keep everything lined up when it goes past your event horizon for what
you can predict it's going to do.
You're going to say, all right, well, make it make it as good as you can,
you know, make it better and then it's going to make it perfect, but you
want to make sure it understands what you mean by perfect.
Did you ever read the the genie post?
Oh, yes, there's no it's a favorite one of mine is going to say make it
perfect, like that does maybe think of genies too.
Yeah, well, so that's that's one of the examples I like to use is like, if
you if you ask it, hey, cure cancer, oh, great, you know, where can't you
live's people people gone, no more cancer.
Okay, so like, so you can use the you can use the intuition pump to get
like some really easy thought, like thought experiments out of this to just
just imagine a very malicious genie.
And is that what you're saying?
Oh, is that the perfect wish that you're thinking of?
Yes.
Okay, yeah, I'm familiar with that.
Yeah, the the the wish machine being you like you have a button, it's a
machine you say I want thing X to happen and it makes things X happen.
And if you aren't happy with the result, you push the button, it
rewinds time to write when you made that wish and you can reword the
wish and then it'll do your thing that and you can keep pushing the
button as much as you want until it gets the wish right.
So the example was like your grandma is living in her apartment and you
see that the apartment is burning.
She's trapped in there and you say, oh my God, I want my grandma out of
the apartment right now and drops or like eight stories and she's dead or
gas main explodes. Your grandma goes flying out of the building because it
cannot break the laws of physics, this machine sure. And so you're like, no,
hit the button and you're like, I want my grandma here next to me
right now and your grandma, yeah, like I said, gets plummeted out the
window and lands next to you in splats and you're like, no, push the button
and you keep her finding your wish down and down and down and it keeps
finding new and more creative ways to fail that are closer to what you
wanted and are always exactly what you said, but are never quite right, but
eventually you hit the button so many times, you get to the point where like
firemen rush in and three firemen die getting your 80 year old grandma out
safe and you're then you're like, well, that's not worth it. My grandma only
has another five years anyway. These firemen's doing die shouldn't die. So
you hit the button. It's like okay. I want firemen one to kick down the front
door to throw his axe at exactly this angle that severs a board from falling
into grandma's head, right? I mean, it can't like do magic and and so then
it's like so your grandma gets saved, but this eight year old girl that was
also in the building has her right arm withered and is no longer useful. The
rest of her life and yeah and then like then you got to start thinking, huh?
How much is my grandma's shitty wish machine though? That's it's not a
lead in here. I mean, eventually maybe you can make it perfect, but but the
point is that the problem with the wish machine is it doesn't have human
values. It just has a goal and it does a goal and you have to keep refining it.
It's like a me six bucks. Yes, yes, and eventually you you you manage to make
a machine that has all your goals and what you're worried about and what you
care about and how much you value your grandma's life as opposed to firemen's
lives as opposed to otherness and people's health along those lines. Is it
okay if your grandma lives, but she's in terrible pain for the rest of her life,
you know, or would it be better just to let her pass from smoke inhalation rather
than that and so at that point you've created a machine that basically has
all your values and then you don't even have to ask it anything because it
already cares for your grandma and cares for other humans the same way you do
and you're just like do your thing machine and it makes the best outcomes
happen and and that is the goal and it's a hard goal to make something that has
the same values as humans and would make those judgments the right way as
opposed to a me six. Sure, sure. So yeah, I think one of the two takeaways from
that or two of the takeaways is that yes, it turns out to be very difficult even
for you to explain or for you to explain what it is that you want and it's only
when you're met with failure over and over that you're like, oh, you know what,
actually value that too. You're right. But the other important thing is that you
don't get multiple tries in real life, right? You don't get a magic wish
machine. You get you get a physics based wish machine and yeah, you only get one
wish basically. So you have to get it right the first time or the human race
goes away. And I think part of the concern that we mentioned earlier, but
that the different companies that are working on building AGI for I mean,
just even for like the reason that, you know, the first owner of an AGI or the
first company is going to, you know, become the world's first multi trillion
error, right? I swear to God, the company that makes AGI first is going to be some
spam company that's trying to break all the ways we have to avoid spam.
They're going to be like, we can kind of figure out all your captchas. We can click
on the four house pictures out of these nine pictures. Oh, man. Yeah. And then
that's that's going to piss me off. Spam will be the undoing of humanity. Well,
maybe, but the other the other concern though is that the companies that are
spending more time thinking about how to, you know, actualize friendliness rather
than just like get the job done. They're going to take a time penalty as far as
like, and it's really like first one there wins. And so that's sort of that's
what I think were some of the nervousness creeps in that it's going to be the most
that's why it's going to be those guys in Nigeria. Yeah. Well, it's going to be
yeah, it's going to be the spammers. But it's going to be the group that, you
know, sprints as fast as they can, regardless of the risk, they're going to
they will likely hit the finish line first. And that's what makes it when you
asked what is like an appropriate number to be spending on this? Should it be
a hundred? Should it be a thousand? I mean, this was a talk I saw a few years
ago. Eleazar Kowski was talking about how there's less money spent currently
funding safe AI research than there is marketing lipstick in New York City.
And so I'm not sure I'm not sure where you draw it. You know, should it be
lipstick in the entire country? Should it be, you know, whatever, but like the
idea of sticks of yeah. So like, but this was mid 2000s. I'll be a little later.
I think I would be happy with a Manhattan Project Style project on AI, because
that was a major endeavor and the government put a lot of resources into
it, but it wasn't, you know, drastic. It was it was still small enough that it
could be hidden in the desert and most people didn't know about it.
What if there already is something we don't know about it?
Good, good, good times. Many thumbs up for me. I approve.
Assuming that they're working on doing it right. Yes. I don't mean like the
military probably has a somewhat of a vested interest in autonomous weapons,
right? Yeah. And I naively hope that they have enough forethought to think
my continued survival, not just goes beyond just winning this war, right?
Or winning this fight or whatever we're building these, these, these AIs to do.
And they're going to think, okay, we should actually think ahead, you know,
five years from now, rather than just like six months to, you know, the end of
this, whatever, you know, the scary part would be a war of annihilation again.
Because there, when they were originally testing the atomic bomb and a few people
were like, this, this, what if this ignites the atmosphere? That would be bad.
We would all die. And there were, there were some other people that said, you
know what? I can cover.
Goddamn. A world where we are ruled by the Nazis is not a world that is worth
living in. Even if it's a just 0.001% chance of human annihilation,
I'm willing to take that risk because I'm willing to take that small risk in
order to not have to live under the Nazis. And you make that gamble enough times
and eventually you're going to lose. And I mean, if, if you've ever seen the
downfall, which is just a fantastic movie about the final days of the third
Reich, all a lot of Hitler's inner circle is, is that, is that what that
mimetic video is where it's him like in his? Yeah. And like people, people sub
over it. Yes. And he, and he, like, yeah, he like takes off his glasses, his
hands shaking and okay, I got you. I should go see that. I should see the
original version to have only seen like the, the programming humor mock-ups.
Yeah, no, I've seen so many different, but a lot of people in, in Hitler's
inner circle poisoned their own children and then killed themselves when it
became clear that they were going to lose the war because they said a world
without national socialism is not a world worth living in. I don't want my
children to grow up in that. And if there were another situation like that,
people were like, well, okay, we're not sure if this AI is going to destroy
humanity or not, but we're willing to take that risk because we'd rather not
live under, you know, capitalism or, or communism or whatever it is.
I hadn't thought about it in, in precisely those terms, I mean, Yash,
and thank you, my, my nervousness level, I think increased proportionately to
the, that, that level of, of concern. Cause yeah, it didn't, it didn't
occur to me. Like I, I goes roughly articulating earlier that I naively hope
that they would think we should probably try and live through this with great
probability, but you're like, fuck it, not everyone thinks that way.
People might, might have different priorities and they're like, you know
what, no lie or, you know, no life is better than life under whatever,
communism or
Better dead than red.
Oh my God. That's a, that's horrifying.
Nervous laughter.
Yeah.
On that happy note, where do we want to go from here?
I think one of the, the biggest debates is just how fast the takeoff will be
because if, if an AI becomes superhuman in a matter of days or weeks,
they could possibly take over the world.
But if it's something that requires a number of years to really marshal the
resources, then humans have a much better chance of trying to fight it and
contain it. And I don't, there's, there's, you know, the bunch of controversy
over how long it would take for something very intelligent to, to take over the
world.
There is reasonable disagreement there.
Where I think since the worst case scenario is fast to take off,
that's the one people run with because we,
exactly plan for the worst case scenario.
Makes sense.
But there, there is the chance, yeah, that it could for some reason take years.
There are a number of people who say that's a silly concern and we shouldn't
worry about that because it'll take a number of years and we will have some
time to, to react.
Yeah. I can see what they're saying, but you got to, I mean, is it how
unreasonable is that concern?
Right.
And it's, to me, it's reasonable enough.
Yeah.
So yeah, that's, that's the thing.
I mean, there's other common kickback to this.
You get like Bill Nye and Neil deGrasse Tyson, whenever I hear them ask someone
that, you know, in an AMA asks them about this or whatever on StarTalk or,
which is Neil deGrasse Tyson's podcast, they're like, oh, well, I mean, if it
gets, like, if it seems like it's going to get scary, we just unplug it.
You know, you just, like you just localize it and then, you know, you just
shoot the box.
As long as we can control the electricity, we're fine.
Well, sure.
Yeah.
Like it seems a lot of the, of these doomsday scenarios involve an artificial
intelligence that is like, ubiquitously connected to the world.
Well, I mean, the, if, if an intelligence is that much smarter than a human, by
the time you know that you should be worried and that you need to unplug it,
intentions, it would already be out in a place where your access to electricity
is not, you know, what is restricting it.
Your, your ability to be, yeah, at that point, unplugging it won't do anything
because it's already secured its own source of power somehow.
I mean, the other thing copied itself somewhere.
Yeah.
And there's just like things to think about, like, you know, I'm not sure what
the interface would be if you, if you just got it in a box, if you're only
going to see text on the screen or if it's going to like vocalize things to
you, you know, like, uh, like Siri on your phone or something.
But again, with how much faster it could be thinking than what you think, if
you had to cross the room and hit that light switch to turn off the power and
I'm the AI that doesn't want the power to go out, I've got like 50 years in
subjective time to think of the one thing I can say to stop you, right?
And I can run all the simulations I need to, to think of what will be, what
will be the most successful thing.
Um,
one of the things humans do with intelligence is convince other humans of
things.
So if, if it's something is much more intelligent than a human, it should,
in theory, at least have the capacity to convince someone to, to let it out of
the box or to not hit the switch or whatever.
Yes.
So, so you get a deaf person to flip the switch, right?
Well, I know, I know, I know, I know the spirit of what you're saying.
I was just being, yeah.
I mean, that's fair.
I mean, I think I, I guess it's not really clear to me how safe, what would it
look like for it to be scary enough for you to unplug and as long as it didn't
want to be unplugged, it would probably not, it would probably try and act like
it's not being scary, right?
Yeah.
The human thinks it's smart enough to unplug the machine when the machine gets
scary and then it goes to unplug the machine.
It turns out that the cat door is locked.
Exactly.
It's like, oh, well, how, what are the chances?
So, and that's the thing too.
Like as far as AI is wanting things, um, I don't want to skip past this in that
like it doesn't want to live the way that we do.
It wants to do whatever you told it to do.
And part of that is living long enough to do it.
I don't think that across all of mind-designed space is an inherent
will to live, but as long as it wants to do stuff, it can't, I guess, think of
whatever random thing you want it to do.
You want it to run simulations on like protein folding.
Well, it can only do that while it's running.
And so if it wants to do that, and it, it also wants to run, yeah.
Don't shut me off.
Oh yeah.
Seriously, I have this no actually seriously look at this.
Yeah.
Well, while I was writing my book, one of my major concerns was what if I die
before I get it done, you know, because that would be really disappointing.
I just want it to be done and out there.
And now I'm dead in the whole, there was no point to it in the first place.
So even if all you wanted to do is write your book, part of getting it done
is living long enough to finish it.
Yes.
And so that's, that's a, I think an okay analogy, right?
Yeah.
So yeah, but also a lot of people seem to think that an AI has to like think
and reason as humans do.
And that's not necessarily the case.
It may not even be a, uh, what we, what we recognize as something
that is conscious, that has, uh, that, that has self-reflecting.
I mean, maybe a will, but you don't need to, uh, be, be conscious or, or have
a self-image in order to optimize your environment.
Even fucking plants do that.
So if you have the ability to optimize your environment and you have a goal
that you're going to, you can be as dumb as a refrigerator, as long as you're
really powerful at optimizing things, humans may be unable to stop you.
Yeah.
I think I always leave the C word consciousness out of this discussion.
It's, it's, it's an important and related topic, but I, it to me is not
see how that would be cause it doesn't necessarily have to be, have a
personality or a thought process at all.
Yeah.
And like things that wants and ways to get them for sure.
And the, the, I guess the really quick bird's eye view of why this is like a
separate, but fun topic is that it could be that if we did create a consciousness
that was scaled up to the proportion that its intelligence was scaled up, we
would create things that suddenly mattered way more than we do, right?
We would create a utility monster that by all real good measures was, you know,
the most important thing in the universe now, right?
Or at least in our local part of the, that we know about, and just as like any
human is worth way more than any ant, you know,
What about, you know, you see this in fiction too, is the, the idea of like
distilling one's consciousness into code, making a model of like a human's
quote unquote, right?
Like, and then, and then like, if, if you can observe what that looks like, then
maybe you could design your artificial intelligence based on that, but again,
then again, if you get to that point, like you probably already super advanced
already, because you're distilling all the contents of the human brain into
something that could be observed.
And we had a fascinating interview with a Robin Hansen who wrote an entire book
about his thinking of what could happen if that were to happen before we get
super, okay, what did he say?
Because I'd be curious to hear that too.
Oh man, it was like in a nutshell, in a nutshell that the world would be very
weird and run very fast and that we would probably have many, many copies of
the most efficient humans rather than a large diversity of humans.
Sure, that makes, yeah.
But if we were to try and use that like on the path of super intelligence, you
could, yeah, if we, if we got human uploading before we got strong AI, then
we could use uploaded humans to, we could have them tweak their own, their own
code and make minor improvements on themselves and then kind of self report
and, you know, introspect and all that.
And you could do that much more reliably if you're not running on.
I think that's a super cool idea.
I think I feel like that's what we should be focusing on.
It is.
And there are people making those proposals.
I think Elon Musk, for example, thinks that the safest way to do this is to
propose, is to, and I don't quote me on Musk, but this is definitely out there.
That the safest way to approach this is to have ourselves be blended with these
machines that are basically, we're, we're inseparable.
And Sam Harris is actually one of the, one of the public intellectual people
that recently got on board with AI and he gave a TED talk on this last year.
And he, he made the point that like, it's weird that that sounds like the
safest way to do this, that usually, usually we know things are safe.
Before we start talking about.
Is it kind of eotistical that we think that this is the safest way to, I don't know.
No, no, no, it's just the idea that aren't things usually safe before we
start talking about sticking them in our heads, right?
Oh, yeah.
As opposed to like, no, no, this is the safest.
No, no, we're sticking our heads into something else.
Yeah.
So like this, this is the only safe way forward is to kind of like meld with
the machines in a way that, you know, we can't really predict what will happen
and et cetera, but that's not the idea that that scary, weird future is one of
the, like possibly safest paths forward is just kind of surreal, right?
Yeah.
And there's, there's again, some debate as to if that is likely to happen before
artificial intelligence, general artificial intelligence, because I mean, it
could be, maybe that's the quickest way to, to get to an intelligence on a, on
a silicon chip, but maybe it isn't.
As someone pointed out, humans tried to make flying machines for the longest
time, just by imitating birds.
And this is very much the same sort of thing as trying to imitate a human mind
on a computer, but the way they actually first did it is to find out what the
fundamentals of aerospace engineering are and then create a thing that looks very
little like a bird.
It has fixed wings and a propeller and does not fly the same way a bird flies
at all.
And so maybe it would be quicker to make a artificial general intelligence that
does not run like humans.
And then, you know, we're, we're again stuck in the race, which is faster,
which, which gets us there without killing us first.
Yeah.
I mean, I mean, I mean, obviously in the one case you're dealing with airplanes,
you're dealing, you're dealing with a machine that has the fun, like the, the,
the intent of that machine is based on the intelligence of the person operating
it.
It's like acquired power, you know, but we're talking about modifying intelligence
itself, which is part of my, my downside of that is that sounds like less safe
than just building a mind from scratch plausibly.
At least we know that human has some human values.
That's true.
But at least initially.
This is operating on the assumption that you're completely making a replica of
someone's actual, like some, like one entity, like one consciousness, actually
able to like, we would know that at least it has human cares and desires and
has some empathy for humans.
And as opposed to seeing them as just another biological replicator on the
earth, that's getting in its way of making paper clips, unless the uploaded
human had the value of like better dead than red, right?
So then then we're just, we're back to where we are now, but no one has the
value of better paper clip than, than red.
I mean, to me, being dead and being a paper clip are kind of the same thing.
Okay, yeah.
Alrighty, Sean, since you were with us for the dust specs episode, there are a
number of dust specs listener feedbacks, and I thought three of them could
address them together, three of us could address them together.
Our website is the Bayesian conspiracy dot com and the mic says the dust spec
comparison is total bullshit.
These things are not on the same level of comparison.
Which of these two would you choose?
Either we do horrible things to a single guy versus doing something shitty to a
whole bunch of people.
I don't think if there's a clear answer to that case, what if the person being
considered is to be harmed as you, would you still realistically choose to be
harmed for the good of humanity?
And I think we've talked about something like that before, that it's really
hard to save when it's you, if you would do the thing, but you kind of hope that
you would, right?
Like you, you kind of hope that you would be Frodo willing to give his life in
order to save all the free peoples of Middle Earth.
Yeah, I think I might have mentioned this on the show before, but I can't
remember.
There's this framing, we talked about the trolley problem, the trolley problem is
posed from the position of you standing next to the switch, not saying you could
be anybody in the situation.
Then what do you want the person at the switch to do, which to me is a more
profound way to look at it and it gets sort of at that point.
And we would want them to do what we would want to do ourselves for the most
part, right?
If we don't know where we're going to be, if I needed to die to save the rest of
human species, I would grudgingly do it or, you know, I wouldn't, I don't know if
I'd even be grudging.
I mean, I would obviously not love it.
I wish there was an alternative, but if there wasn't, it's kind of a no brainer.
I kind of want to lease the statue.
Right.
Maybe a song about me.
So, so could you summarize?
What was it?
A question or it was just like, here's my statement.
The line of thinking is that you should be very clear on what your thoughts on
human sacrifice are because it seems inevitable, then that's the ultimate
position you were discussing.
And I guess it kind of was, right?
The one does the good of the many outweigh the good of the few.
Well, because, because it was, it was utilitarian ethics and the scope of
like suffering, right?
Yeah.
So, so yeah, basically he's right.
That is literally what we were talking about.
When does the needs of the many outweigh the needs of the few?
And I think most people actually argue that the needs of the many are not really
all that needful in this particular comparison, which is where it breaks down
as opposed to no one should ever sacrifice themselves for a bunch of other people.
Moving on, Dark Lord Azrael says,
I feel one major failing of the dust specs torture scenario is that it isolates
effects too much unless the knowledge of the torture is perfectly concealed
or none of the dust spec beings care about all the torture that's happening,
then a negative utility is not being captured.
The beings who are being spared the dust specs would get at least as much
negative utility from the knowledge of the torture as they would from the dust spec.
And that is something I meant to bring up and I never quite got around to a good
point.
Yeah, because if I knew that someone was being tortured that so that I don't have
to get a dust spec, that would be a lot of negative utility to me much more in the
aggregate among all humans than than 50 years of torture would be.
Well, yeah, but I mean, I mean, I mean, I was already like, yeah, go dust specs every time.
So because there is another utility of knowing that like that makes even more
like sense to bring up since we brought up those who walk from Omelus,
which is a case where people know about the torture, right?
I think you're right.
And I think that it was implicit in the original thought experiment that people
didn't know that someone was being that there was a choice being made,
that you're standing outside of all of this and you get to decide one of these
two things has to happen and then which one would you choose?
So when Eliezer wrote this, he basically put forth that nobody knows about the torture.
He didn't actually specify that at all.
Oh, he didn't say it.
Okay.
I think it's assumed, but I think a good optimizer would take into account what
the people getting the specs would want if they knew about the options, right?
And if everybody knew that someone was being tortured on their behalf,
that is a lot of negative utility.
Yeah, that sounds like a different comparison than the original.
It is definitely a different one.
So in email to us, our email is basin conspiracy podcast at gmail.com.
Dr. S said that according to the conditions of the thought experiment,
the dust motes cause a brief several second period of pain, but no other consequence.
We expressly ruled out flow on effects like car accidents.
You would also exclude lost productivity, increased ill temper,
raised blood pressure, etc.
And as she didn't say this, but also the knowledge that someone else is being tortured,
which also hurts as a result.
There is literally no long-term negative impact of the dust mode on its victim.
Conversely, you did not set the same conditions of a person experiencing 50 years of torture.
Indeed, we questioned whether they would ever recover from the experience.
It is a life ruining event.
So it's actually quite reasonable to argue that an infinite number of individuals
experiencing in sequential dust modes would not equal one individual
experiencing 50 years of torture, which I think that read.
Yeah, I don't know because I actually do agree with that.
I do as well that I if you were to actually make them equivalent where,
you know, the dust spec is just a brief bit of pain and then nothing.
There's no other effects than you would have to also to make it comparable.
Torture is just 50 years of intense pain, but then after that, no other effects.
And yeah, they don't specify what the torture is.
I think you're you're supposed to fill in the blanks, right?
Yeah, it's supposed to be like what you would consider to be like significantly painful.
Have you guys ever been like in intense pain like post surgery or something?
I mean, I've definitely been in intense pain before.
Okay, I've had I've had several days of intense pain from post surgery and not several days in a row,
okay, and it's really bad.
And I've heard that giving birth is some of the most excruciating pain ever.
The TriGuy or there's some some dudes on some, you know, like there's some video I saw going
around on Facebook ish and it's like these guys who volunteered to have a bunch of electrodes
placed on their body in places that simulate childbirth and like they kept upping the difficulty
to like full on this is what this is what women giving birth feel.
And like most like pretty much all of them couldn't stick around for all that.
They just they gave up at like level four, level five was the most intense pain.
Okay.
And they were just like 10 seconds.
They were like, nope, I quit.
I'm done, which you don't really have an option when you're giving birth to do that.
Yeah, exactly.
In fairness, I think there's probably more chemicals going through your brain
during childbirth than just having your nuts shocked.
Right.
So there's positive things associated with it that are happening.
You have you have like an approximation, right?
But like if you're if you're a guy, it's like, oh, no, yeah, we can't we can't imagine,
but I just have to imagine that it can't be that bad because people still keep having
people have more than one child.
No, this is actually the point I was driving to when I brought this up.
So having been through that pain myself now that it's in my past, I barely even remember it.
I remember that it I hated it and it sucked, but I can't remember actually the pain is not
visceral anymore.
It's like something I read about almost and I remember hearing that there's there's some
a sort of pain amnesia that humans have.
And that is the only reason women ever have more than one child because after the pain
is gone, it doesn't stick around anymore.
Seems like a good trait to have.
And I was going to bring that up to evolutionarily sound thing to.
And I think that's tied specifically.
There's there's additional evidence that ties that specifically to childbirth.
And when I said it can't be that bad, I wasn't belittling it in the way like, oh,
I can't hear that, but you ladies are exaggerating.
I didn't get a chance to finish that thought.
I was I was going to go on to say something like what you said, which is that it could be
the worst thing ever at the time.
But then you look back and it's clouded by, you know, this new happy baby and all this
awesomeness and just from the way we were programmed that it just doesn't
register as the worst thing that's ever happened to your entire life.
Right.
So if we take this like pain amnesia thing into account and also the no follow on effects
afterwards, basically what we're what we're going coming down to is
everyone in an almost infinite amount of people losing about a second of their life
versus one person losing 50 years of their life, and I'm sure that person who's tortured for 50
years is going to come out like, all right, well, you know what that was really painful
and terrible, but but I do it all again because infinite people didn't get a speck of dust in their eye.
You thought experiments. I don't know. It's weird then because I guess an infinite number of
seconds adds up to more than 50 years of seconds, right? Yeah, I mean they're just some things.
They're just some things where you can't math your way out of it, right?
Dr s apparently is an actual doctor because the email ended with this is a fundamental actual s
never. There was more than just an s in the name, but I don't know how
how much of their name they want out and that's okay. Yeah, this is a fundamentally different
exercise to the hospital budget question where an administrator has to decide whether to spend
a million dollars saving one life or 10 lives because the lives are valued equally in practice.
Hospital administrators actually have to make more complex decisions about life expectancy.
For example, the equation looks a little different if the choices to prolong one young person's
versus 10 very elderly person's lives, but these equations work because and only because
they weigh up the value of equivalent things. So Dr s is saying that these things are not
equivalent. Oh, okay, cool. We got an email back from the doctor and said, yes, go ahead and quote
me and I am happy to be referred to as Sasha personal preferred pronoun as he. So thank you,
Dr Sasha wanted to give him credit if you wanted it out there. Me being being the the fame whore
that I am if someone's going to read my stuff and like, no, no, put my name on it. Yeah. I think
yeah, it's it's a different way to look at it. But I try to entertain it from the worst possible
imagining to where you are making that decision from the outside. Then that's where it got to the
hazy thing that we that we talked about for like an hour. But yeah, if it was and it wasn't told
everybody or if everybody just somehow innately knew then that becomes a whole different equation.
And I get the feeling that it was supposed to be a from the outside sort of picture because it's
the sort of thing like if you're a super intelligent god, emperor, artificial intelligence,
and you are making this decisions for all of humanity without being able to actually consult
with every single nearly infinite person in the multiverse, what is the decision you make? And
and the decision decision should be based on math, I guess. Yeah, that's the idea. It was
trying to use the idea of shut up and multiply or one of the one of the thought experiments
get around to that. All right, sorry. Next one. One last thing, at least only from what I have,
the one last thing on the dust specs is Shaq Latria Shaq Shaq Latria on our website again
says the argument that convinced me of the dust spec position is the whole time we were talking
about the thought experiment. It was always just a one off decision. What I think about though is
whether or not the position is generalizable. For example, Loki really hates humanity, but can
only affect a tiny percentage of people on his own. So what he does is he kidnaps one person
and tells you that he will torture them for 50 years. His offer is that instead of the torture,
he'll set them free as long as you put a tiny dust spec or equivalent in the eye of all three
to the three subjects in your multiverse, the nearly infinite people sometime within the next
50 years. You follow your rule that dust specs are less bad and you agree to that offer and you
are happy, but Loki just keeps doing this because there's so many people. It's very easy for him
to kidnap multiples per minute. It's still just a tiny fraction of all the people. So pretty soon
the end result is that Loki keeps doing this. You keep agreeing to it and your nearly infinite
number of subjects each get one trillion dust specs or small itches or tiny paper cuts or
whatever you choose as your, you know, minimum thing. This is the same as hundreds per second
for 50 years, which is the equivalent of torture and I would agree that getting hundreds of paper
cuts per second for 50 years is kind of the equivalent of torture, right? Okay, okay, but hold
on. If you if you decide that you're gonna it's like, okay, fine, Loki, you win. Go and torture
that person. What stops him from kidnapping another person and doing the same thing again,
keep offering to torture that same person? Nothing, but then the point is you can multiply it on
the other side too. Yeah, exactly. But then the point is one trillion people being tortured for
50 years is much preferable to almost an infinite amount of people being tortured for the next 50
years, which is what your choice comes down to. But is it but, but, but okay, so you're saying
that is that is where the math actually comes in because we're like, oh yeah, so is one dust
is better than 50 years of torture, but one trillion dust backs is much worse than one
trillion people being tortured. Well, okay, but okay, wait, so sorry, are you saying like Loki
continually makes that trade in someone's like dust backs and then he makes does it again? Yeah,
like dust backs, but he does this like what every five seconds or something one trillion
times across 50 years, which comes out to hundreds per second for 50 years. Yeah. Okay, so
okay, but but we're just multiplying, right? Yes, if one instance of 50 years of torture
is preferable to all those dust specs, then why is it not the case that one trillion instances of
50 years of torture is preferable to a nearly infinite 50 years of torture. So, okay, so
at some point it's to Loki to fuck off. Well, yeah, I mean, yes, at some point the most rational
action is to figure out how to kill Loki, but the point of the thought experiment is that if
you take that, you know, one is greater than the other. If you multiply both sides by a trillion,
that shouldn't suddenly flip it for some reason, but it's yet it seems to emotionally it hits
differently, but you're right. The math, the math stays the same. And so you kind of just be like,
all right, well, fine, Loki have it your way. Or I guess he doesn't have a preference, but
right, you stick with the torture, I guess. I mean, if I were just to come up to you and ask
you, would you rather Loki, right? But if I were just to come up to you and ask you, would you
rather torture an almost infinite amount of people for 50 years or only torture a tiny
percentage of that? Only one trillion people for 50 years, which would you rather do equivalent
to torture on both sides? Again, we're tied. We're basically putting the dust specs, the momentary
like, oh, okay, but that's happening like what every second you're like, oh, I can't give my
eye. I can't give my my eye. Wouldn't you consider a hundred paper clips a second for 50 years to
be torturous? Oh, a hundred paper cuts a second. Yeah. Yes. Okay. Probably. So yeah, I mean,
there you go. I mean, of course that changes things. I was definitely looking at it in the
scope of like one time decision. But yeah, I mean, I don't know at that at that point, if we're,
if it's one of those things where it's like someone like, like we assume we can't kill a god and
it's like a god and it's redoing this, why wouldn't you just torture everybody? Why wouldn't you
just torture everybody infinitely anyway? Because that's kind of his goal is like fucking with
you specifically. Yeah. To make the decision. I mean, I guess the real question is it's a
sadistic God. It's like what are you supposed to like if you can't solve the solution, if you
can't solve the problem by killing Loki, he's just gonna do that. Everything's gonna be tortured.
Yeah. There's nothing you can do. But really, the answer is find a way to kill Loki. Yeah.
But out of curiosity, where do you draw the line where it flips from one to the other?
I'll cross that bridge when I get to a man. Okay. You can give me like a one time decision like
that. I'll admit, yes, like, what was the name of the, was it Chakal? Anyway, Chakalatria? Chakalatria
brings up, has brought up an interesting point by kind of multiplying it more, multiplying the
shut up and multiply experiment, I guess, if you might say it that way. Yes, I was definitely
looking at it from the perspective of a one time decision. And in a one time decision, I would
pick dust specs like always. But if you're doing it like 100 times a second, it's kind of like
that's absurd, right? I mean, I mean, that's basically how money pumps work.
Or microtransactions or the old concept of a money pump. It's also like a Monte Carlo booking
swindler or something. I forget what the other term for it is. But it's when someone has three
things that they value not in the same gradient. Like for example, let's say you like fruits,
you like apples and oranges and bananas. Okay. You like an apple more than a banana. Sure.
So if you have a banana and I'll say, I'll trade you this apple for the banana,
if you give me one penny and you'll be like, yeah, I'm more than happy to do that. I'd much
rather have the apple than one penny. So a banana and a penny I give to you and I give an apple.
Yeah, totally. Right. Okay. But I also have an orange and you like the orange more than
you like the apple by more than one penny. So I'm like, hey, I'll give you this orange if you
give me that apple and a penny. And you're like, yeah, sure, right? Totally. Yeah, totally worth
one penny. And then it turns out that you now have the orange, but you have this weird thing
where you kind of prefer bananas to oranges. And so I'm like, hey, I'll give you this banana if
you give me that orange and one penny and you're like, yeah, okay, sure, sure. And then I close
and then I close the Skype session and I just eat my banana. Right. And then you understand what
the fuck is going on. But and you close the Skype session. But assuming that you cannot
close the spikes Skype session, it's the thing where I'm like, here's an apple. Oh, here's an
orange. Oh, here's a banana and you keep since you since you like these different things
differently, but not in an actual gradient. At some point you'll die if you don't eat. So
yeah, it's that kind of thing. I got you. Okay. So fun times. I think that's all the dust
spec stuff I have. This is just a comment that I don't think we have anything to reply to,
but is an interesting comment on the subreddit slash r slash the basin conspiracy. Massison says
the thousand paper cranes thing is a Japanese cultural cultural tradition, which both goes
in the shell and the Sudoku story reference. So yeah, that is apparently a much older thing
than any of us knew because we're not very good at Japanese story and culturaling things. Yeah,
fair enough. I can take that. But yeah, I like the link. If we're sharing links that listeners
were thoughtful enough to share, there was a great one from someone on the website, Richard J
Acton wrote in on the basin conspiracy dot com on the episode on digital rights and privacy.
I think the European digital rights, the edri or edry, I guess I'm not sure what that abbreviations
for is roughly equivalent to the EFF for your European audience. And they link to edry edri.org
and said that they have a good newsletter with tips and tricks and software software suggestions.
So if this wasn't going to struck a chord and you want some actionable things that you can do
to protect yourself, I haven't looked at this myself, but you have at least one recommendation
to check out edri.org. Cool. European listeners, you also have a resource. I'm not sure if it's
just for them or if we get their stuff here that we can do too. If it's just like chrome extensions,
I'll be for everybody. It's true. This was a fun one JD via email going way back
about our voting episode where we had Tim on and telling us reasons why it's not necessarily
rational to to vote all the time. He writes, you're telling people who have the desire to be
rational that it is rational not to vote. What then do you want to leave voting entirely in the
hands of the irrational people who vote for moral and religious and ideological reasons do not have
this constraint? You told your listeners to not have a voice in this last election. The very people
who have the most respect for your opinions and you're telling them not to vote. Before the next
election, I hope you will tell listeners, yes, it is worth researching the issues and yes, it is
worth taking the time to vote. Otherwise, we have a government that is largely composed of
representatives of the irrational Katrina contacted Tim, who then wrote us a very nice long reply.
But before we get to that, the essence of it is basically that's not exactly what I was saying
is what Tim says. And he's right. But I had a friend call me after the episode aired and he was
like, hey, why is this your guy's message? And so JD wasn't the only person who took it that way,
that it came off strongly enough anti voting that at least two people mistook it for that was our
actual, that was the position that was being supported here. Well, before we dive in, I had
my own take on it too, that I sort of agree with Tim that most people, including myself,
do not really research the issues all that much. And it would be best if most people didn't vote
and just leave the voting up to the people who do research and are actually really invest in this
sort of thing, which is the reason that things like decision markets work, because people then
are putting their own money on the line. And so the only people who are willing to put their money
on the line in these predictions are people who do do the research, whereas just voting for something
does not cost you any of your own money. So you can vote regardless of whether you've done research
or not, and lots of people do. And so in that respect, I think if everybody agreed to only
vote on things that they actually have researched heavily and know a lot about, we would have a
much better world. But I consider this very much a prisoner's dilemma sort of thing where not voting
unless you know something is cooperating and voting anyway is defecting. And if everyone on our sides
cooperates and doesn't vote, unless they're very well informed on the issue, but everyone on the
other side votes anyway, they're defecting. And if you know your opponent is going to defect,
cooperating is stupid. It is the losing move. It is how you get taken advantage of.
You have to defect as well. So everyone has to vote in order to cancel out everyone else's vote,
which is really kind of lame and shitty. And like we all know, the world is always much better
if everyone cooperates, but we can't like force that on people, but actually getting cooperation
is hard. You know, as your explanation there covered both sides of the conversation that had on
the phone with this episode aired with my friend. And so that's that's kind of where we wound up
was like, yeah, it'd be great, but since we don't live in that universe, we need to be out there
voting to. So we did actually contact him as well and Tim replied to us, not in these exact words,
but basically summarizing. He said that he he only told listeners that it's morally okay to not
vote, not that they shouldn't vote for the same reason that it's okay to drive a car or
wear an ugly shirt because it has a very, very small negative externality on the world. And
usually those things are okay. I don't know. Like, like, I don't know that about being a small
externality, though, because it seems like a somewhat of a pervasive attitude to be like,
you know, screw the elections. Who cares that it comes down to our delegates and we just shouldn't
vote because whatever that like voter apathy, like there are externalities to saying that. I mean,
I mean, maybe not so much in the way that he's wording it where it's like, it's morally okay
not to vote. That's a little different, but people encourage each other not to vote too.
There are externalities to that. No, we're living in one. For him, for his opinion,
that would be a positive externality that voting, if you don't know the issue very well and have
researched it heavily, is a negative externality on the rest of the world because you are voting
if you're not well informed. Yeah, okay, okay, I see, which, you know, 99.99% of people, including
ourselves are not on almost every topic, like how much research have you really done? Yeah,
no point. I have done less than a hundred hours of reading on any particular policy question.
Yeah, I mean point taken. He says also that he said that he was telling people it's less
worthwhile to vote if they aren't in a swing state because, for example, in California,
you probably don't even need to bother and then that's where the idea of vote trading came in,
right? Yes, and he pointed out that in the last election, we threw away literally three million
votes, referring to the fact that the popular vote went three million in favor of the candidate
who ended up losing the electoral vote, right? For all this paraphrasing, it's worth mentioning
that Tim explicitly said don't read all this word for word and feel free to just, you know,
generously paraphrase. So that's what we're trying to do. Yeah, it was a long reply. Yes,
was it was that reply recent or was that from a while ago? A couple weeks. Yeah, okay. He also
said that this is not meant as an actual recommendation, more that what he is saying is a prediction
and the prediction is people vote stupidly and other things that this prediction models is that
all large democracies will have a large number of stupid policies and politicians in all large
democracies will spend an inordinate amount of time focusing on stuff irrelevant to policy
because it appeals to people's stupidity, like, for example, the size of a candidate's hands
and increasing voting by, for example, really encouraging it or getting voted, get out the
vote drives or making voting mandatory will probably make things even worse, not better
and so on. And that is why he had the sure the the argument that he did that you cannot stop
this sort of stupidity simply by voting alone. You have to change the incentives as well.
And he does say that he doesn't think any particular group, not even, you know,
rationalists or listeners of Bayesian conspiracy are likely to be more politically sensible than
anyone else, because if you're passionate about politics that a size sign already that you've
kind of thrown your rationality out the window and he says that goes for him as well.
That that's an empirical an empirical question that I'm not sure if we know the answer to. Yeah,
I would like to think naively that people who make an effort to stay objective and be are
capable of discussing both sides of an issue without yelling or losing their minds. I would
hope that they're on average better voters than any other person, right? I would definitely want
to think that too. Like there are a lot of there are a lot of like quote unquote team players.
I mean, you know exactly, I'm sure you know exactly what I'm talking about because you see it
all the time where it's just like, well, when you guys had Obama, we, you know, phrasing it in terms
of like, well, I think it's not so much like I think this or I think that it's it's we did this,
our team did this and it's just this weird collective
I think also in practical human terms, if you're going to be spending hundreds of hours
researching something, you have to be passionate about it because if you're not passionate about
it, you're not going to take the time. It is the rare person, I think it could read for hundreds of
hours on just duty alone without any passion driving, right? Or unless they're being paid for
it, right? So yeah, I think that's a kind of high bar to set. I do see what what's him saying,
especially like, you know, in general, I think that was my main takeaway that I didn't like,
but he did a better job of explaining in this and maybe he said this verbatim on the show and it
passed my mind and my friend's mind and the person who wrote in. But I took it kind of like as a,
well, it's basically not worth it because, you know, the odds of your vote changing the
election are so small, etc. And for me, I just didn't like it because that's not an argument
that you can generalize because the people who be persuaded by those kinds of arguments are
exactly the kind of people you want voting. And you see what he's saying is that they might not
be that much better. But I think that, A, that's a question that I'm not sure if he has that we
know the answer to. And B, if we're not, then we can work on being better at it. I mean, many of
us care, if you care more about being right than about your politics, then your politics will change
to what's what you believe is right. And that's sort of the the underlying motto of rationality,
rationality, like I want to be right, not not I want my position to be right. I want
I want to change my permit per my position, so it is more right. Exactly,
or let's wrong. Seeing yeah, I was I was going to let someone else do that one.
I have one other one last one, but it is a big one. What on which episode is it the voting one
to no and you were there for the Nazi one as well. So right let's let's have it. Let's have
at it. Have at it. Have at you. Okay, so GT to us in email says that he really enjoyed our response
to Albionic American. And since the idea seemed to be a new one to us, he would like to lay it out
in full. This is a feedback to the feedback from the the Nazi punching episode. Yes. And so that
that's where this is coming from. And if you're not familiar with what he's talking about, that
was in a previous feedback section. I think it was on the I did not see that coming. Albionic
Mary was was he the one who said like the the people who are alt right or they're they're
they're making a commentary on the human condition. Was he that guy? Exactly. Okay,
so this guy's a follow up to that. Yes. Okay, I'd love to hear this one. He says the right believes
in something called human nature, which is to say basic instincts, desires, biases, etc,
that all humans are born with and will always have to some extent. People may display different
behaviors as a result of different material conditions or the presence or absence of different
ideas. But the fundamentals of human nature are constant. As a result, any utopia scheme that
requires us to fundamentally change our nature is bound to fail. Examples given, if socialism
requires that greed be destroyed, socialism will fail. If feminism requires that distinction between
male and female be be destroyed, feminism will fail. And if world peace requires that in group
loyalty be destroyed, then world peace will fail. The leftist view seems to be that with
enough social engineering, any kind of society is possible.
Dude, this is okay. Like some of it gets objectionable. This sounds like exactly what I was
talking about with that conflict of visions thing, the unconstraining constrained vision, but like
he's parroting exactly what and the thing is those two are. I think he has some not a bad
point here because there is some level of human nature that you can't fundamentally alter, right?
Let me let me. Can I say two things on that really quick? One is that I don't know how
representative this is. He says the right believes and I'm close with a lot of Republicans and we've
had lots of conversations. Something like this, especially the more specific examples that he
gives later never have come up. And to the extent that there might be something known as human nature,
I would argue and this seems to challenge what what GT wrote in that there is human nature
across homo sapiens, not across different shades of homo sapiens. And so there are
not so subtle insinuations later on that there are different human natures for different races
from GT. That's not something I'm saying. And I would challenge that. I would also wonder
how the right somehow found all the secret truths of human nature where the rest of us missed it.
And that if they if they have evidence that isn't just like pointing at, well, like, look,
there's increased crime over here or look IQ test scores are different in this part of the country.
Well, that's that's not strong evidence in favor of like underlying human nature is being different
among those people because you can control for variables and have that cross different,
or I guess cross racial boundaries. And so that can't be underlying for differences between
different pigments of humans, right? So that's that's sort of my main gripe with with that whole
line of thinking there, that if there's something like human nature, that's that's the steel man,
we can take that and make that separate. But I do not get the impression that that was the
point that he was making. Okay, I'm going to I'm going to read the next thing of his that I find
not objectionable. And then we can get to the actual objectionable stuff. Sure. The next thing
he says that I think is not objectionable is the right and I'm not sure again, all right people
or all Republicans or something. I think by the right in this case, he means a more specific
group of perhaps new reactionaries or or some other kind of right that I don't think he speaks
for all people on the right is what I'm saying. But in in his in his words, the right believes
that if such a thing if such things are desirable at all, we will at least need to see some evidence
that we can get there. The right is generally skeptical of the idea of moral progress progress
in general, at least partly because we believe society has actually regressed morally on certain
issues. Consider Jonathan hates moral foundations, is it hate, height, height, height. Okay, if
conservatives value things like loyalty, authority and purity, then have we really progressed from
their perspective? Are people more respectful to authority now than they were in the past?
Are we more loyal to our nations or tribes? Are we more chased in our sexual behavior?
As far as I can tell, not so much. And I think he has a good point there that the people on the
right probably don't think we've made more progress because we have been abandoning those
things that he mentioned. So height doesn't come right out and say this because he's an academic,
you're not supposed to sound so biased. It turns out that moral systems that have like
purity as a really important element to it get really nasty, really fast. These are the things
where like, consider what was it a few years ago, there was that school on fire and some,
forgive me for my bad memory, but it was in some strictly by the book, Muslim country,
and apparently the fathers of the children in there, it was a girl's school, kept the
rescue personnel from going in because they didn't want the girls pulled out without being
properly veiled. And that's the kind of shit that happens if you care more about the purity of
keeping your little girls veiled and whatever purity that does, it leads them to dying in
buildings, right? So like a moral system that would have me have that as an ideological purity
to an extreme. Yeah. And if that's your desired outcome, I'm prepared to say you've got your,
you value the wrong things. And this is, I guess, somewhat controversial. I think it's
not just possible, but it's super common to have wrong values, right? I'm prepared to take a stand
there that if you value purity over like staying, you know, saving a life, your values are misaligned.
So then he gets into areas where I want to make it very clear that these are things he has said
and not things that I endorse in any way. But as examples of things that he thinks the left
claims are possible and the right says they would need some evidence that they're possible because
they think human nature will make these things impossible, says the poor can be made wealthy.
I want to interject really quick and say to GT specifically that if you want,
you're welcome to put this on the website as a comment to any episode that you want,
including this one. And then you can have your entire write-ins public. And I guess,
I don't want to make it seem like we're censoring you and only sharing the parts that we want,
but it's also a brevity thing. But part of it is that I don't feel like engaging parts of it,
but you're welcome to put those out there and we'll leave them on the website as long as things
stay civil. You never got mean and that's really the only line. As long as you're being civil,
I'm happy to engage, just not necessarily with every point when we have a time budget.
He says the poor can be made wealthy, which I think is just a weird...
Yeah, my biggest problem with that is that allegedly there's a very strong correlation
between socioeconomic inequality and criminal activity. It's not like... If you go to a place
where everybody's poor, there's not going to be crime there. Everyone's kind of on the same level,
but if it's like there's a lot of disparity, that's where you get into that. So, I would say,
does the left really even believe that the poor can be made wealthy, or is it like they want to
close that gap a little more and make that disparity less so that there's a basic quality
of life going on? Raise the bottom line. Stephen Colbert said, I won't stop till everyone's in
the upper one percent. That is totally the point I was going to make too. It's a very strange
argument because in historical terms, the poor are wealthy. Our poor have more wealth than the
vast majority of people ever did in the past of ancient kings. Yes, our poor have better lives
than kings did a millennia ago, which is... Well, it's supposed to be relative, right? Like the
disparity at the time. Yeah, so relatively, they are not as... You're still living like kings,
because that's all they knew was living. Right. They have more clothing options,
better clothing, better housing. They have heating and air conditioning. Well, maybe not air
conditioning, but refrigeration of food, more food options, medical care. Their children generally
do not die of dysentery in their young age. The only difference is that they have less jewels
and no servants. Yeah. The poor are in almost every scale measure much better off than the
wealthy were of the past. So in that respect, not only can the poor be made wealthy, the poor
have been made wealthy. Well, yeah, no. And again, I feel like that's a weird way to phrase it,
because it's saying that the left wants to make everyone wealthy, but since when? I'm never like
no, isn't that... The left is more about having safety nets and providing upward mobility for
people who've fallen hard times because of a pink slip at the wrong time or whatever, right?
I think the left's goal is more or less wanting to raise the bottom level of quality of life.
We can't make everyone live like millionaires. We can make everyone live like you're making
40K a year, right? Ideally. You're not making judgments between like,
well, do I want to take the bus to work and have lunch or do I want to like fill my gas tank so
I can drive myself? Like those kinds of equations like, am I going to go hungry today?
Aren't considerations that people want people to have on the left? Ideally, on the left, at least
as far as I can tell, we would like to have a post scarcity world, right? Sure. And that's basically
the definition of everyone is wealthy. Yeah, I guess it depends. Like I wasn't sure if wealthy
meant like, yeah, the purchasing power of Bill Gates and everybody should have that, which is
ridiculous, or that everyone's got the purchasing power of somebody who can, again, feed themselves
and clothe themselves. If we get replicators and we live in a post scarcity society, that's a
different thing. Well, I mean, it always seems to me like he says, by claiming that the poor will
always be poor no matter what due to human nature, that even once we have a post scarcity economy,
there would still be people who are poor, which seems odd to me, but I guess could be possible
in some way. I don't know. I wonder how when they had the time in evolutionary history to
evolve a different human nature than other humans, I mean, these are the same species,
and like, did it happen six generations ago when their family's business went bankrupt,
and they've been poor ever since? That's not the timescale that base nature's evolve on, right?
All of my objections to his points are just on historical record. He says, his next point,
that black communities can be raised to the level of white communities. Okay, so he's
right, as everyone in the room, which is so. So is this him repeating what he thinks the right
believes? It's what he believes to as far as I can tell, because he's representing his group
in this email, and I mean, I believe what he's trying to do is point to the fact that in America
right now, most communities that are majority black are worse off than most that are majority white
in several different measures, but his claim is that due to the fundamentals of human nature,
black communities will always be inferior to white communities, and I'm like no. Just based
off the historical record. There were many times in histories where non white communities were the
superior ones. Well, I like I'm trying to like, like, like, if I'm in this deal, man, I don't
want to steal man. That there's even places in the world right now where in nations where their
black communities are better than white communities. Is he talking about just America, and is he just
talking about like like like human nature in terms of like tribalism, like like, oh, we're
predominantly white, so we're always going to press, you know, people of color. Is that what
he's going for based on what I have read in these sorts of circles before the the argument is that
it is a genetic thing that the white races are just genetically better, and so we'll always
end up having better communities. No, like, like, like I've seen that map. Like I've seen that map.
There's this map of like IQ, like average IQ, but it's like we're comparing countries that don't
have the same educational opportunities. It's really a stupid, stupid map. I mean, also the
thing that really gets to me is that I do believe that different cultures can be better or worse
than others, and that the culture you're raised in has a huge impact on who you are and how you do
in life and what you value and what you value. Yes, but the there are legitimately shitty cultures
out there, and there are legitimately good cultures out there. Sure, and in, but it's not a genetic
thing. No, no, it's not a genetic thing. It may look genetic because the same genetic group settled
in an area and had that culture, but it's not intrinsically genetic. And I think as one of my
examples of this is one of the worst in my personal opinion, cultures. God, worse. It's such a
valuable word. One of the least optimized for modern living cultures in as long as I'm being
weasley and trying to not make myself look like an asshole in in in modern America is the border
or culture or river culture, depending on what term you want to use. So there was a group of
immigrants that came from the there was a border between. I believe it was England and Scotland
where there were constant border disputes and the people that lived on that border basically
were living like in the several hundred years ago version of Somalia for decades at a time.
Both sides kept coming through, raping their land, taking whatever they wanted. They didn't.
They were living in a lawless anarchy land and they basically got by by forming very tight cohesive
family bonds and they were only loyal to their families and everyone else could suck a dick.
And they were rather aggressive and it's the the Reavers. They were actually called Reavers.
They would go out and raid other surrounding areas and take their stuff and bring it back
to their villages because that's how you survive in that sort of situation. And they eventually
came and some number of them came and settled in the Appalachians and the American South.
And that that sort of culture is still pervasive in the US in various areas. You can see it.
It's it's I know at least one person who who comes from that sort of culture
and I do think it's destructive. It's it's a bad culture. It is a low trust community.
It is an aggressive community and it is very much ruled by tribal loyalty to to just the
small group of kin and all outsiders are suspect and evil. Yeah, that's that's not like like
and and they're white. I think you're white. No, no, I have a shitty culture and they're white
and I can see the argument of how some people would consider that you know human nature,
but I don't think it needs to be human nature. It doesn't. I think it's the nature of that
culture, but it has nothing to do with human nature. It's it's not necessarily. No, like the
culture can hopefully be changed and we should be able to override that. I mean it will take.
I imagine it would take generations. You cannot do that in the course of one person's lifetime
right, but like when you grow up with that it is internalized, but
over time. Yeah, I think it could be fixed and that to me it feels like that is a thing that he
would call human nature that you were born to parents who are like this and you will be like
learn behavior. It's not and to me it's a cultural thing. Yeah, I absolutely would buy that. That's
that's a learn behavior. Like this is how it is. This is how we do things and I mean maybe there
are some genetic influence on it too. Maybe people who are naturally more aggressive due
to their genetic disposition do better in those sorts of cultures. So those sorts of cultures
still encourage people to to I would say it's always a little column a little column B. Right.
That's always the answer. I'm saying at least partially that is learned. I'm not saying it's
a hundred percent, but it's at least you know part of the way it's learned. So to be
people up to get food. I mean when we talked about this not too long ago when Sam Harrison had
a Sam Harrison when Sam Harris had his talk about God, what was it? Who was the talk with
with the races with the race issues in America? Yeah, that was with a Greg T. Lowry. I think
okay and and he brought up Greg brought up the point that it was just him and one of the person
was at three people. It was two. He brought up that if you're a young black man in the inner
city on the bus and someone scuffs your shoes, you can't let that go. You have to put up an
aggressive air because otherwise you will be victimized by other people who see oh look that
guy's a pussy. So you have to start a fight over him scuffing your shoes and now there's beef and
now people are going to die and that sucks and in my in my opinion that is another example of a
shitty culture and and I think this is where he gets this thing about black communities can't
be raised the level of white communities that there are certain cities that are majority black
that have that sort of shitty culture but I don't think it has anything to do with race. No, it
doesn't. It's not it's it's it's reinforced within that community like it's it's it's something to
do with crappy cultures. Yeah, it's an it's not crap. I think it's an accident of history and it
could have gone the other way but the fact that it went this way reinforces the position that oh
it is or it is a genetic thing that is it this is just your guys as you know genetic problems
and then that uh you know if that's your if that's your belief and that's your
policy making motivations then you're not going to like try and fund schools in those areas because
well they're doomed anyway and then things will just keep getting worse or at least won't get better
and so like it's I can see where this does come down to like sort of a base ideological difference
but it's one where I I'm I'm fine saying I think uh GT who wrote in was what is wrong there uh
I mean I I'm trying to be as like polite as I can about it but I think you're that I see
I think I can kind of see where you're coming from but I think you're just factually incorrect
like like not just ideologically but like as a matter of history and uh observation sure yeah
no like I like I like I think there's a there's a very strong emphasis on this like nature over
nurture thing when most things are both like you pointed out just a few minutes ago like it's
like yes there are things that are kind of that kind of come naturally to humans but that doesn't
mean like saying it's all human nature it just seems like a cop out like it's it's it's like
well let's not make it any better let's just leave it let's just and if you're gonna say it's just
human nature it's on you to explain why human nature is different among different populations
of people that have nothing in common other than like you know so if it's just among poor people
well what is it like are they genetically poor like if they'd inherited a million dollars would they
would they stay poor like the the argument is that yes they would squander the money and be
poor again within a generation well there's there's different that and the when you said is there
how is it that they're on these timelines they have evolved differently the argument being that
up until very recently and starting again actually there was this thing called a sort of mating
where generally rich people would marry other rich people and generally poor people would marry
other poor people which does in fact usually happen and that is what reinforces these sorts of
bad genetics of of managing money or whatever and in my opinion that's more of the thing of
the the reinforcing a culture staying the way it is because people don't cross contaminate their
cultures at all they they they stay they only know what they know but you know that their argument
is that it's a genetic thing rather than a cultural thing if anyone's curious i can dig this up
there's a really great write up for like why someone who was raised in a poor environment
if they happen to make you know if they somehow get $30,000 that they immediately spend it all
and that they don't have anything to show for it two years later this comes from positions that
make internal sense to the to people and like if you come from a place where it's a whole long
thing but there there is a position there and a an explanation there that doesn't fall back on
well last you know i just thought about it and this would seem to make to make the most sense
you know like i guess i'm not sure what how you would reflect in your own human nature or something
but the decisions that seem rational to people and you can't really fault them for given their
upbringing i think and i i get where like that seems wishy-washy and leftist wishy you know
wishing thinking but i i i challenge that i think that that it's i i guess i see where you're coming
from as far as how the timelines could have worked out but i still kind of call bullshit and
there's there's also the whole thing where when you're poor if you come into some money
you're surrounded by all your friends and family that are also poor i i people often say well
why don't you just save up you know a hundred dollars a week and and over time or maybe that's
too much save up ten dollars a week over enough time you'll have some amount of savings so that
you don't go bankrupt if suddenly you blow a tire and you have to pay spend a hundred dollars on a
new tire and then all of a sudden oh my god your bank overdraft fees are happening you have to borrow
money from a payday loan place they want all this extra money back and and it just is a cycle
but the problem is when when you are poor and you have nothing if someone comes to you and says
dude my tire blew out please help me you're like i'm sorry i can't help you i don't have any money
if you have some money and someone comes up to you you're like well you're my brother you're
my sister-in-law you're my uncle you need this money right now here is my money and so you can't
you end up not being able to save money not because you don't want to save money but because
you're a decent human being and people that you care about need money badly too yeah like how do
you justify putting money into savings like when you know for only 25 more dollars a month you can
help pay your grandmother's rent so that she's not left homeless at 80 right or your eight-year-old
niece can actually have that cavity filled instead of being in pain yeah constantly right so i mean
that's that's just like a matter of like low resources and then yeah that that that generates
habits and that's where like i mentioned yeah and then people are like well just stop smoking and
you'd save a hundred dollars a week on cigarettes and but you wouldn't save a hundred dollars a
week on cigarettes you'd give away those hundred dollars a week on other things like that and then
you wouldn't have the the slight relaxation and relief that comes from smoking cigarettes as well
so you're making your own life worse and you can't save that money anyway yeah which is a bummer
it's a thing i'm not sure how much more there's worth engaging here and let's go and keep going
well the his third thing was same-sex you same-sex unions can acquire the same respect as traditional
marriages which to me is come on weird on a number of levels because first of all the amount of respect
something is given is not a nature of anything yeah biological societal values yeah exactly like
what the fuck is he even saying and also does he not has he not read any history books there were
many times in history we were same-sex unions had just as much yeah respect as traditional quote
quote marriages i mean also like how much respect people have for regular marriages these days
anymore either right right like oh you know gay marriage that's that's abhorrent that's out of
the question but you know if you want to get married three or four or five times go for it that's
there's nothing wrong with that like maybe and that that's not fair maybe i'm i'm maybe on straw
manning someone might actually take a stance against both equally but i don't think that yeah
that the number the amount of respect that a person gives something is a relate to the whole
to anything that could be called human nature i don't know if i feel you i feel you're taking
a snapshot of the current conditions and using that to like kind of apply that to the idea of
human nature and like something that's immutable yeah my thing's very myopic yeah he uh i'm not
even going to bother going on he made he i mean do i don't want to like paint him in two negative
of a light but he kind of made a pro lynching argument which i'm not going to repeat because
lynching first of all killing people ever is bad but extra judicial violence we've we've already
said we really are not a fan of which is what lynching is and lynching is literally terrorism
and i cannot get behind state approved terrorism against one's own populace it's just fucked up
so anyways he goes on to say that on any issue where there seems to have been moral progress
an honest conservative will say that the progress is illusory or it came as a result of external
factors or it was actually regress or it was genuine but very small and to him it makes far
more sense to hold people to normal human standards of morality that we know are reachable rather than
to be disappointed that humans are not acting ways in the way that your pet utopia requires them to
act that's a huge cup out well and you can't you can't explain away any contrary evidence by
saying well i have these four categories of of fully general counterarguments that'll fall into
so if you point out well like look for example violence went down in the last century you know
people are way less likely to die of violent deaths than they were 500 years ago oh well that's
that's uh that's barely progress or that's illusory or you know like you can't you can't take any
sign of progress and shove it in any of those buckets kind of a priori right like you're
declaring yourself immune from contrary evidence i want to say that i thought this was sort of a
decent point it it hit me close to home anyway because in our ethics episode i had i put forth
the position that any system of morality which breaks the people that adhere to it is is doomed
to fail and you can't expect humans to hold on to it that it's it's just going to die out like
the shakers who who believe that people should not have sex period and they died out after generation
because they didn't have any children um it's that sort of thing and i i use it specifically
for the sort of morality that i have seen harm people in my life where they they put the welfare
of other people including strangers in such a high regard that they neglect their own lives and it ends
up destroying them either financially or emotionally and that really sucks and that is the sort of
thing i was speaking against so yes i do think that a morality that is impossible and that damages
the people that hold on to it is a bad thing so i agree with him in that regard but i mean on the
other hand but but you have a much more higher bar for where you think people can reach yes exactly
like i mean it works in your mouth but it's it's it's it's almost like he's putting forth this
this weird dichotomy of like of like well either you're just accepting things as they are or you're
striving toward utopia right where it's like it's like okay but i mean things progress is incremental
happens very slowly like like even if you don't perceive it to be a net good that does that mean
it's not and we can focus on specific problems you know so like we can we can eradicate specific
diseases that are killing people and that's progressed by any measure you're dead people it seems
like this weird like fundamentally philosophical view of of how things should be when it's like well
there's like a shit ton of nuance going on here that's why i feel like it's such a cop out i you
know to me it's it also feels kind of like a cop out because i mean you can you can say that a
system of morality which says theft is immoral is one that is not uh not one that functions for humans
because due to human nature people will always steal or something and i kind of think that at
that point it's bullshit your system of morality should say that theft is unacceptable and once
you get to the point where well it's human nature to steal and so any any system of morality that
says theft is bad is doomed to fail like you know let's screw you there is a level of things that
we can reach for and that we we should reach for and that really humans can accept without breaking
themselves so it seems to me like he is arguing that something very simple like give a modicum of
human respect and decency to people who are gay is like beyond what humans can achieve and and
it is a pet utopia to try to enforce that and like no just be a decent fucking human well it's also
it might be impossible to universalize like peter singer level charity but it's clearly not impossible
for humans to do because humans have done it at least a few but then then you can just point to
like oh they're just violating human nature well then if you can violate human nature that's the
same as subverting it and saying that you know we don't need to fall like there's there's the really
there was an entire community of people that subverted human nature to the point where they
didn't have sex anymore yes well and i mean you know violent resistance to oppression might be human
nature and yet uh gondi and his his group didn't or did nonviolent resistance the whole that's kind
of my other problem with just human nature arguments if you can it it's human nature when it works
for your position and it's you oh they're just violating human nature when there's a counter
example and if you can't or maybe they could do maybe they could have some convoluted way of
coming up about how on how like that supposed subversion of human nature was actually linked
to some other form of human nature and that that that form was stronger than the other human nature
i don't know like it seems like a needlessly complex and convoluted model right yeah you want
to put it to power and explanatory power i'm just i'm just thinking can set like like total
total like i'm making this up this isn't what i believe but like i'm just using a total bullshit
example like gondi you're like okay so so if it's human nature for violent protests and then but
then we say like oh well gondi was you know he was totally a pacifist and he resisted and you
could say oh well it's also human nature to want fame and he just wanted fame and that's why he did
and his his his his want for fame was higher than his his want for violence or something like that
so they i mean they could probably come up with some kind of work around to that and then it all
kind of stems the same thing but i you know it doesn't really change how i feel about it it seems
like if your philosophy says that it is human nature to dislike people of different skin color
and we can never change that but uh it's also human nature to be violent yet we expect you not to
murder people it seems inconsistent to me yeah that if you can expect people to not murder others
then you can expect people to be decent to people who are a different skin color
so there's there's an argument that and i'm gonna just turn this up to 11 just to draw
drive this point home um it's likely true that in the ancestral environment rapists had more
offspring than non rapists because you could impregnate a woman i'll say rape i'll say male
rapists probably had more offspring because you could pregnant a woman never see her again
she would have a kid and you could go on to do that as much as you want and yet that's not the
kind of human nature that i would think even gt would be behind right um and i mean we can make
this less disgusting and just talk about like other primates right um okay so it's a it's a
nightmare possibility of historical fact that this probably went down somewhere either in our
ancestry or in a cousin's ancestry um cousin species but that's still not the kind of thing
that you would want to endorse or just like throw your hands up and say well it's human nature we
got to just roll with it especially now well that's that's progress right yeah so uh to say
that all progress is illusory or small i feel like i said you're just explaining away things
that would be counter evidence before you even have a chance to really interact with it uh he
also getting back to the actual thing that sparked our entire debate says that we correctly noted
that spencer does not publicly endorse any violent actions so he asks are we alleging
that he privately endorses violence we spent a lot of time arguing that peaceful ethnic cleansing
is not possible but even if we're correct our argument isn't really relevant to the question
of what he thinks is possible so if someone supports an impossible policy that does not
license you to accuse him of supporting some alternate policy and i think there is there is
a level where that is no longer true if if someone is advocating for something as ridiculous as
deporting quote unquote peacefully one third of the population he has to know that it has to be
done violently uh even if he like didn't understand that at first it would have been brought to his
attention very quickly and then to continue to support that policy despite the reality of its
effects on the ground means that he no longer cares that it would end in violent results you
have to at some point modify your rhetoric when when it becomes apparent what it means right
yeah i i mean i mean you could say well it's only violent if they resist and it's like yeah but
they're going to that's just this is one of those things where it's like you can just look at the
world and like i i think i feel like a lot of these positions that are being brought up are like
things that are just thought about and like no one even considers what it's actually like
out in reality and the reality is that yeah when you try and pull me away from my kid
i'm going to resist and then it's going to turn violent well you know that's on you for resisting
no it's on you for coming in here trying to separate us right so that i mean i think if if
spencer was being honest about wanting a peaceful ethnic cleansing then he would say he wants the
government to raise taxes on white americans to such a level that we can afford to buy out
one-third of the nation of all their property at fair market value plus whatever premium they would
place on the fact that they now have to uproot themselves move somewhere else get new citizenship
learn new languages and all that if you can give some i know there is a level of money where i would
be willing to move to australia or or port not Puerto Rico's in america or columbia or something
give me enough money and i will give you my house and my car and uproot myself and go somewhere
else but it's going to be a fucking ton of money and so if spencer is actually honest about this
he will say yes i want to raise tax rates on all white people in america by 50 for the next
three decades or however long it takes to raise enough money to buy out all these people so they
will happily voluntarily leave the u.s and if he isn't saying that if he's just saying oh we want
a peaceful ethnic cleansing and he knows that what that really means is throwing people out
against their will then he's he's a liar he is avoiding saying the words that we should punch
him and kick him and kill him if need be while obviously presenting that as what's going to what's
going to occur that first version where you mentioned like raising taxes doing it that way
i i find myself liking that in the same way that i like the intellectual honesty of like religious
fundamentalists who are like well every word of the bible is true i think they're wrong and i
disagree with them but i at least like the fact that they're consistent and so that like i don't
like what that vert that argument say if someone is making that i wouldn't like what they're trying
to do but damn i admire your consistency and your willingness to think it through and you know
bite the repugnant conclusions and just run with or not the repugnant conclusions the unfortunate
consequences yeah you know so like hey if you're gonna do all that i just agree with you but props
for thinking it out right at least you actually are advocating for a real peaceful cleansing right
i feel like that's plenty of very time unless we want to give him something unless there's
anything else we want to get on i have one last thing you got it sorry to keep trying to take
it off and no no i know you're you're tired of this but i think yeah i predict i won't be the
only one who feels that way you're okay so he said that since we personally literally asked
saying what we would be interested to hear what comments of ours were quote unquote horribly
backwards he says upon further reflection i probably shouldn't have used the word horribly
and i apologize the main topics you discuss are actually pretty good at being neutral with a few
exceptions he lists even more so than uh journal mainstream journalists which kind of seems like
damning with frank praise to me yeah it's like thank you yeah right okay you're better you're
better than boz feed well i mean you're better than sounds like even like cnn or whatever you're
better than box yay but on the other end he meant it in in i know a good spirits i know yeah
thank you for that i'm just making light of it he says the what usually what gets to him isn't
and the any specific topics but mostly things that are tossed off as an aside in the middle of a
podcast that bother him that betray a left-wing perspective some examples are when i opined
that human society is pretty good at figuring out what's moral and katrina joked that maybe this
is in doubt considering the election results and everybody laughed and he's like oh because
everyone's obviously anti trump haha i was like okay i mean well everyone in the room was yeah
and i'm not going to pretend that i'm not leftish and anti trump because i am and at least i am
putting my biases out there so you know them sure sure he said that in ash mentioned that he uses
emojis and work email at katrina suggested that his ability to do that and still be taken seriously
is due to his being a man and in ash readily agreed i thought about that i don't know i mean i did
readily agree at the time does he have a point that that's bullshit and it has nothing to do with
gender i mean i think another factor that we didn't discuss in the time because we did it really
quickly it was like different jobs that you have if you work in a small office yeah like my last
job it was a very small office me and two other people and there are both women and i was the
only guy there would be some some emojis back and forth yeah but that i mean that was we only
wrote emails kind of as a courtesy because we could literally just shout to each other so being able
to be taken seriously is like there's more there's one one factor to it than just yeah if you're
writing an email to corporate and it's way out there i do see what katrina is kind of getting
at though yeah i mean i also see what he's getting at two that i automatically went oh yeah i do get
more respect because i'm male i get the feeling i probably mail people generally do get more respect
though i will say that i will say that i take anyone who writes in comic sans not very seriously
regardless of their gender no do not use comic sans in your emails if you want someone to think
that you are being professional um and then he lastly says that uh when steven asked what the
manic pixie dream girl trope is i described it as used by white male directors because obviously
white people are boring and uncreative and in our shop and said well at one point in our said old
white men are the worst and fuck these white dudes ruining shit and he says because that's not
anti-white right even though you would never say that about any other when you're when you're
standing on top of the ladder the only people you can yell to other people are standing on top of
the ladder right yeah but no i'm just i i i see his point and i think that's actually a very good
point and i feel bad about doing that now and you are right i will try to lay off on saying that
sort of shit because that is also bullshit yeah i know i'm saying old white men are the worst yeah
yeah no like like i i feel like that's going to spur a whole other conversation but i do
like i i definitely have thoughts about the use of white men as kind of an invective i understand
why it's there like i do i really do i'm definitely brought up and it bugs me because i'm like if
you replace white men with literally anything else even if you'd say white jews it's the most
socially it's the most socially acceptable group to slam because of privilege it's it's it's it's
it's the whole punching up thing basically you're not you shouldn't be there should be no group
that is socially acceptable i kind of agree like i i think that like it's i don't know for a while i
didn't really it didn't really bother me but you see it so much and you see it included in really
intellectually dishonest arguments and it's part of the reason i've kind of like like it was cute
the first few times but it's been going on for decades now well yeah but i mean it's against
like negative sweeping generalizations of anybody right yeah yeah and so uh it could be that someone
might say oh you're just as a white guy you don't like hearing people say shitty things about white
guys like well hold on but you don't know me i also don't like hearing people say shitty things
about other people yeah and so i like again i like the rule of like being able to generalize my my
morality as much as possible and yeah i mean so drawing exceptions is weird i've heard people say
yeah this white guy was really loud annoying at the meeting i'm like wow you would not say that if
he was black yeah well if you said if he did you'd be an asshole but like it's okay if you say that
he's white and that like that is sort of weird it is a great double standard that can see where
that bugs people and i'm on board with that and i do recall just not too long ago hearing
someone complaining about someone who was kind of loud and obnoxious at a meeting
i think in the same thing yeah and then and then ended the sentence with it was a white guy of course
i was like what do you mean of course would you have said that if someone was being kind of stingy
with a tip and he was a jew of course well then well then he wouldn't have said anything right
yeah so no i your point is well taken and i will try to not do that sort of thing in the future
for what it's worth i took the impression that you're saying that's somewhat tongue in cheek
but it is the sort of like joke that there's no reason to keep making so yeah and we can
debate this but do you want to just decide now that we're not going to engage any more of his
stuff on the air and that if you want so you can i feel like that's part of the problem with like
discussing things is there's a little more no that's it okay i mean like future correspondences
because then it sort of just becomes like spending a lot of time engaging this one person with their
one ideas and it's not like giving way too much air time to ideas that we think are kind of gross
play but i don't know play but you're because because like it's it's it's kind of cool to have
someone with a differing viewpoint being like yeah you know like i like like i i consider myself
like i don't know i guess a left leaning moderate but like you know having someone who actually
believes like stuff on the right it's kind of like i'm not gonna this sort of call them a
bunch of names but like this sort of talking with people who are not on your side is really
important right it is what keeps society peaceful and then specifically one person because it seems
like in the last it seems like in the last two or three months like a 25 percent of our airtime
has been engaging this one person oh really okay and so okay i thought this was his first
email back to you from i i see your point i think that we don't want to give too much air time to
something like this but i do think it's it's good to engage it at least somewhat and the first
couple times yeah and to be like we're not running away from your issues we disagree and here's why
whereas opposed if someone was just to be ignored they'd be like well obviously they have no kind
on arguments which is why they're just ignoring me well that's why i invited him to post this on
the website or the subreddit okay and then that way you can be engaged by everybody
and if people are being dicks to you for whatever reason i'll try and shut them down too
uh that's the whole thing be nice to everybody and you know well it can if you find their
positions are pugnant say i find your positionary pugnant not you're a disgusting person yeah yeah
yeah no i'd like like that that's definitely the field of the podcast and like and isn't that kind
of the rationality thing is is like hear everyone out you know yes it is so oh we do have one last
thing in our last episode mr oliva wrote in saying hey what's up with katrina haven't heard her in a
while uh so we should probably mention at this point that katrina has become more and more busy
with her work to the point where she's basically working two jobs right now it seems like two and
a half ish yeah so she has not had time to be on the podcast lately and she says that looks like
it's going to be the case for the foreseeable future so unfortunately katrina is stepping back
from the podcast and she'll still be on every now and then for maybe certain specific issues that
she's really passionate about but uh it sounds like she's no longer going to be a regular feature
of the show she actually let us know this through like a quick message like i think she's that tied
up and we tried to get her to like record something that we could attach which we might be able to do
today for this episode it's it is unfortunate and i missed her contribution so we've been trying to
have at least a third person on for every episode to have somebody to engage with but it's not the
same and if you're noticing the lack uh we are too it was a very different perspective that she
brought to things and i really appreciated that because steven and me tend to agree on a lot of
things which makes for less of a diversity of opinions yeah it's it's less fun i guess that
sounds like about it i do have one thing to say that we have a few patreon supporters thank you
very much that's super awesome you guys rock yeah i mean it makes a big difference i think our
hosting costs are now covered yes our hosting costs are now covered for the website awesome so yeah i
mean we we still don't make any money basically off of this but at least it's not costing us a
monthly fee anymore it's not for what it's worth in yosh is paying everything so it's not costing him
any money right uh yeah but uh please feel free to support us on patreon if you want just a dollar
an episode or leave us a review on itunes that also helps or just let people know if you like
these episodes and thank you everyone has always has already done so yes and you can comment in
the various me places we mentioned on the subreddit or on our website great cool goodbye everybody
take care later
