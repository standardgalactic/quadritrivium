code and make minor improvements on themselves and then kind of self report
and, you know, introspect and all that.
And you could do that much more reliably if you're not running on.
I think that's a super cool idea.
I think I feel like that's what we should be focusing on.
It is.
And there are people making those proposals.
I think Elon Musk, for example, thinks that the safest way to do this is to
propose, is to, and I don't quote me on Musk, but this is definitely out there.
That the safest way to approach this is to have ourselves be blended with these
machines that are basically, we're, we're inseparable.
And Sam Harris is actually one of the, one of the public intellectual people
that recently got on board with AI and he gave a TED talk on this last year.
And he, he made the point that like, it's weird that that sounds like the
safest way to do this, that usually, usually we know things are safe.
Before we start talking about.
Is it kind of eotistical that we think that this is the safest way to, I don't know.
No, no, no, it's just the idea that aren't things usually safe before we
start talking about sticking them in our heads, right?
Oh, yeah.
As opposed to like, no, no, this is the safest.
No, no, we're sticking our heads into something else.
Yeah.
So like this, this is the only safe way forward is to kind of like meld with
the machines in a way that, you know, we can't really predict what will happen
and et cetera, but that's not the idea that that scary, weird future is one of
the, like possibly safest paths forward is just kind of surreal, right?
Yeah.
And there's, there's again, some debate as to if that is likely to happen before
artificial intelligence, general artificial intelligence, because I mean, it
could be, maybe that's the quickest way to, to get to an intelligence on a, on
a silicon chip, but maybe it isn't.
As someone pointed out, humans tried to make flying machines for the longest
time, just by imitating birds.
And this is very much the same sort of thing as trying to imitate a human mind
on a computer, but the way they actually first did it is to find out what the
fundamentals of aerospace engineering are and then create a thing that looks very
little like a bird.
It has fixed wings and a propeller and does not fly the same way a bird flies
at all.
And so maybe it would be quicker to make a artificial general intelligence that
does not run like humans.
And then, you know, we're, we're again stuck in the race, which is faster,
which, which gets us there without killing us first.
Yeah.
I mean, I mean, I mean, obviously in the one case you're dealing with airplanes,
you're dealing, you're dealing with a machine that has the fun, like the, the,
the intent of that machine is based on the intelligence of the person operating
it.
It's like acquired power, you know, but we're talking about modifying intelligence
itself, which is part of my, my downside of that is that sounds like less safe
than just building a mind from scratch plausibly.
At least we know that human has some human values.
That's true.
But at least initially.
This is operating on the assumption that you're completely making a replica of
someone's actual, like some, like one entity, like one consciousness, actually
able to like, we would know that at least it has human cares and desires and
has some empathy for humans.
And as opposed to seeing them as just another biological replicator on the
earth, that's getting in its way of making paper clips, unless the uploaded
human had the value of like better dead than red, right?
So then then we're just, we're back to where we are now, but no one has the
value of better paper clip than, than red.
I mean, to me, being dead and being a paper clip are kind of the same thing.
Okay, yeah.
Alrighty, Sean, since you were with us for the dust specs episode, there are a
number of dust specs listener feedbacks, and I thought three of them could
address them together, three of us could address them together.
Our website is the Bayesian conspiracy dot com and the mic says the dust spec
comparison is total bullshit.
These things are not on the same level of comparison.
Which of these two would you choose?
Either we do horrible things to a single guy versus doing something shitty to a
whole bunch of people.
I don't think if there's a clear answer to that case, what if the person being
considered is to be harmed as you, would you still realistically choose to be
harmed for the good of humanity?
And I think we've talked about something like that before, that it's really
hard to save when it's you, if you would do the thing, but you kind of hope that
you would, right?
Like you, you kind of hope that you would be Frodo willing to give his life in
order to save all the free peoples of Middle Earth.
Yeah, I think I might have mentioned this on the show before, but I can't
remember.
There's this framing, we talked about the trolley problem, the trolley problem is
posed from the position of you standing next to the switch, not saying you could
be anybody in the situation.
Then what do you want the person at the switch to do, which to me is a more
profound way to look at it and it gets sort of at that point.
And we would want them to do what we would want to do ourselves for the most
part, right?
If we don't know where we're going to be, if I needed to die to save the rest of
human species, I would grudgingly do it or, you know, I wouldn't, I don't know if
I'd even be grudging.
I mean, I would obviously not love it.
I wish there was an alternative, but if there wasn't, it's kind of a no brainer.
I kind of want to lease the statue.
Right.
Maybe a song about me.
So, so could you summarize?
What was it?
A question or it was just like, here's my statement.
The line of thinking is that you should be very clear on what your thoughts on
human sacrifice are because it seems inevitable, then that's the ultimate
position you were discussing.
And I guess it kind of was, right?
The one does the good of the many outweigh the good of the few.
Well, because, because it was, it was utilitarian ethics and the scope of
like suffering, right?
Yeah.
So, so yeah, basically he's right.
That is literally what we were talking about.
When does the needs of the many outweigh the needs of the few?
And I think most people actually argue that the needs of the many are not really
all that needful in this particular comparison, which is where it breaks down
as opposed to no one should ever sacrifice themselves for a bunch of other people.
Moving on, Dark Lord Azrael says,
I feel one major failing of the dust specs torture scenario is that it isolates
effects too much unless the knowledge of the torture is perfectly concealed
or none of the dust spec beings care about all the torture that's happening,
then a negative utility is not being captured.
The beings who are being spared the dust specs would get at least as much
negative utility from the knowledge of the torture as they would from the dust spec.
And that is something I meant to bring up and I never quite got around to a good
point.
Yeah, because if I knew that someone was being tortured that so that I don't have
to get a dust spec, that would be a lot of negative utility to me much more in the
aggregate among all humans than than 50 years of torture would be.
Well, yeah, but I mean, I mean, I mean, I was already like, yeah, go dust specs every time.
So because there is another utility of knowing that like that makes even more
like sense to bring up since we brought up those who walk from Omelus,
which is a case where people know about the torture, right?
I think you're right.
And I think that it was implicit in the original thought experiment that people
didn't know that someone was being that there was a choice being made,
that you're standing outside of all of this and you get to decide one of these
two things has to happen and then which one would you choose?
So when Eliezer wrote this, he basically put forth that nobody knows about the torture.
He didn't actually specify that at all.
Oh, he didn't say it.
Okay.
I think it's assumed, but I think a good optimizer would take into account what
the people getting the specs would want if they knew about the options, right?
And if everybody knew that someone was being tortured on their behalf,
that is a lot of negative utility.
Yeah, that sounds like a different comparison than the original.
It is definitely a different one.
So in email to us, our email is basin conspiracy podcast at gmail.com.
Dr. S said that according to the conditions of the thought experiment,
the dust motes cause a brief several second period of pain, but no other consequence.
We expressly ruled out flow on effects like car accidents.
You would also exclude lost productivity, increased ill temper,
raised blood pressure, etc.
And as she didn't say this, but also the knowledge that someone else is being tortured,
which also hurts as a result.
There is literally no long-term negative impact of the dust mode on its victim.
Conversely, you did not set the same conditions of a person experiencing 50 years of torture.
Indeed, we questioned whether they would ever recover from the experience.
It is a life ruining event.
So it's actually quite reasonable to argue that an infinite number of individuals
experiencing in sequential dust modes would not equal one individual
experiencing 50 years of torture, which I think that read.
Yeah, I don't know because I actually do agree with that.
I do as well that I if you were to actually make them equivalent where,
you know, the dust spec is just a brief bit of pain and then nothing.
There's no other effects than you would have to also to make it comparable.
Torture is just 50 years of intense pain, but then after that, no other effects.
And yeah, they don't specify what the torture is.
I think you're you're supposed to fill in the blanks, right?
Yeah, it's supposed to be like what you would consider to be like significantly painful.
Have you guys ever been like in intense pain like post surgery or something?
I mean, I've definitely been in intense pain before.
Okay, I've had I've had several days of intense pain from post surgery and not several days in a row,
okay, and it's really bad.
And I've heard that giving birth is some of the most excruciating pain ever.
The TriGuy or there's some some dudes on some, you know, like there's some video I saw going
around on Facebook ish and it's like these guys who volunteered to have a bunch of electrodes
placed on their body in places that simulate childbirth and like they kept upping the difficulty
to like full on this is what this is what women giving birth feel.
And like most like pretty much all of them couldn't stick around for all that.
They just they gave up at like level four, level five was the most intense pain.
Okay.
And they were just like 10 seconds.
They were like, nope, I quit.
I'm done, which you don't really have an option when you're giving birth to do that.
Yeah, exactly.
In fairness, I think there's probably more chemicals going through your brain
during childbirth than just having your nuts shocked.
Right.
So there's positive things associated with it that are happening.
You have you have like an approximation, right?
But like if you're if you're a guy, it's like, oh, no, yeah, we can't we can't imagine,
but I just have to imagine that it can't be that bad because people still keep having
people have more than one child.
No, this is actually the point I was driving to when I brought this up.
So having been through that pain myself now that it's in my past, I barely even remember it.
I remember that it I hated it and it sucked, but I can't remember actually the pain is not
visceral anymore.
It's like something I read about almost and I remember hearing that there's there's some
