artificial intelligence, a modern approach.
And he currently is on the board of a few institutions and stuff working on this.
And he uses the, the thought experiment or the intuition pump of imagine if
we got a message from space saying, Hey, we're aliens, we'll see in 50 years.
And like people wouldn't have exactly the approach that you gave.
Well, we've still got time.
We'll, we'll figure this out when, as it comes closer.
Okay.
You get a message and it's just, we will see you in 50 years.
Well, say it's unequivocally from aliens.
The idea is that we're told like they're going to come into physical contact.
Well, they're going to come to our planet.
What does that actually mean though?
We're not, that's, that might be part of the point, right?
I just, we know, we know they're coming.
And so if they're just going to look at us and that's it.
Well, but the thing is, how do you prepare?
So like the idea, the, no one would take the stance, well, that's 50 years away.
We can start thinking about it in 20, 25 years.
We've, we've got time.
No one, no one really, I think they would be like, holy fuck, we need to scramble.
We need to be ready for anything when this happens.
And people aren't taking that with the, with the definite, although less certain
horizon of, of artificial intelligence and the thing is these things are really
hard problems and take a lot of time, right?
And it's hard to feel like emotionally grappled with it the way the alien
example catches people.
Well, I mean, when Trump was elected, I thought at that night that, I mean,
maybe one or two percentage points here, there could have swung things, but we
were in a situation where it was close.
There was 50% of the electorate willing to vote for him.
And how the fuck in the past four months, like when all the people I knew were
out there, crazy campaigning and being really part, you cannot make much of a
difference on that wide of a level in a few months.
The place to start would have been back like 15, 20 years ago and addressing
these concerns about the middle class being gutted, the working poor not having
jobs anymore, or even just the financial crisis in 2008, I think is really the
last chance we had to divert the sort of thing.
So it was eight years ago that you would have to start working on not getting
Trump elected to make sure Trump didn't get elected.
And you just can't do it in the last minute.
And I think this is the same sort of thing where you have to get started early
in front of these things because they are hard, difficult problems that take a
long term sustained effort.
And we don't know exactly when it'll be.
It could be in 50 years.
It could be in 90 years, but it could be like in 20 years maybe.
And because it's so hazy, the fact that almost no one is looking at it is
ridiculous. It's like there's less than 100 people in the entire planet, as far
as we know, that is actually working on this problem.
And I'm not saying that...
On friendly AI, not AI in general.
Yeah, on friendly AI.
And I'm not saying that we have to dedicate 10% of our
world GDP to fixing this or something.
But I think maybe having more than 100 people working on it is a good idea.
Where did you get 100 from?
Oh, God, where did I get 100 from?
It was, I believe, a Wait But Why article, now that I think about it.
And he has a reference to where he got that number.
But it's basically, Miri, the Future of Humanity Institute, a few other
small organizations that employ a dozen to two dozen people at most, each of them.
And comes out to less than 100.
I see what you're saying about like, yeah, maybe we should be given this.
It's due focus.
And certainly, if there are only 100 people who are taking this seriously
and actually thinking about it, maybe there should be more.
I would agree with that.
I guess I would also bring up that as time goes on, we're going to have more
of an idea of what's on the horizon, right?
And maybe maybe maybe in like 10 years, for all we know, someone could be like,
oh, shit, guys, like this is actually a thing now.
Like we've seen this happen with AI and this happened with AI guys.
No, seriously, like look out, because this is coming up now that we can't see
this now, but maybe we can see this soon enough to where we're like, oh, crap.
And now we start doing things.
People have been saying that for almost 10 years now.
Okay, and fair enough, right?
It's some people are finally starting to listen, which is nice.
I mean, like Elon Musk's and Stephen Hawking's and Bill Gates's and all those
are finally over the last.
Yeah, mostly it's been happening in the past year, but some people have
been starting to sign on for five, six years now, so more attention is being
drawn to it and I am very happy about that, but it's still slow.
How many more people would you want to see?
I guess on that focused on it and I mean, we already have like the big minds.
We have Elon Musk, we have Stephen Hawking thinking about this stuff and
actually I don't think either of them are actually working on it, because
neither of them are AI researchers.
They are just sure sure to pull attention to it and Elon Musk is pouring
money into it or yeah, yeah, I mean, I mean, like what he's working.
Well, I mean, Tesla Tesla's cars are like almost fully autonomous, right?
Yeah, like potentially like they can be like, so didn't someone get
driven to the hospital like it?
Oh, really?
Yeah, I thought I heard something about someone who like he like injured
himself and he got into he got into a Tesla and it drove him to the hospital
and actually saved his life.
Boy, I mean, don't quote me on that.
I guess that rings a bell for me too.
I mean, I think that's one of the reasons that Musk was an early funder of
of of Miri on other of these initiatives because Miri is machine
intelligence research Institute.
Oh, right, right.
Okay, they basically are trying to solve the alignment problem, figure
out how to create once an AI is created, how to make sure that its values
do not end up destroying all of humanity.
So part of part of the confusion there is like, how do you keep a utility
utility function consistent when through self modification, when you're
doing the self modifying to yourself and you know, so the parts of you
that are changing your utility function or refining it, the idea is how
to keep everything lined up when it goes past your event horizon for what
you can predict it's going to do.
You're going to say, all right, well, make it make it as good as you can,
you know, make it better and then it's going to make it perfect, but you
want to make sure it understands what you mean by perfect.
Did you ever read the the genie post?
Oh, yes, there's no it's a favorite one of mine is going to say make it
perfect, like that does maybe think of genies too.
Yeah, well, so that's that's one of the examples I like to use is like, if
you if you ask it, hey, cure cancer, oh, great, you know, where can't you
live's people people gone, no more cancer.
Okay, so like, so you can use the you can use the intuition pump to get
like some really easy thought, like thought experiments out of this to just
just imagine a very malicious genie.
And is that what you're saying?
Oh, is that the perfect wish that you're thinking of?
Yes.
Okay, yeah, I'm familiar with that.
Yeah, the the the wish machine being you like you have a button, it's a
machine you say I want thing X to happen and it makes things X happen.
And if you aren't happy with the result, you push the button, it
rewinds time to write when you made that wish and you can reword the
wish and then it'll do your thing that and you can keep pushing the
button as much as you want until it gets the wish right.
So the example was like your grandma is living in her apartment and you
see that the apartment is burning.
She's trapped in there and you say, oh my God, I want my grandma out of
the apartment right now and drops or like eight stories and she's dead or
gas main explodes. Your grandma goes flying out of the building because it
cannot break the laws of physics, this machine sure. And so you're like, no,
hit the button and you're like, I want my grandma here next to me
right now and your grandma, yeah, like I said, gets plummeted out the
window and lands next to you in splats and you're like, no, push the button
and you keep her finding your wish down and down and down and it keeps
finding new and more creative ways to fail that are closer to what you
wanted and are always exactly what you said, but are never quite right, but
eventually you hit the button so many times, you get to the point where like
firemen rush in and three firemen die getting your 80 year old grandma out
safe and you're then you're like, well, that's not worth it. My grandma only
has another five years anyway. These firemen's doing die shouldn't die. So
you hit the button. It's like okay. I want firemen one to kick down the front
door to throw his axe at exactly this angle that severs a board from falling
into grandma's head, right? I mean, it can't like do magic and and so then
it's like so your grandma gets saved, but this eight year old girl that was
also in the building has her right arm withered and is no longer useful. The
rest of her life and yeah and then like then you got to start thinking, huh?
How much is my grandma's shitty wish machine though? That's it's not a
lead in here. I mean, eventually maybe you can make it perfect, but but the
point is that the problem with the wish machine is it doesn't have human
values. It just has a goal and it does a goal and you have to keep refining it.
It's like a me six bucks. Yes, yes, and eventually you you you manage to make
a machine that has all your goals and what you're worried about and what you
care about and how much you value your grandma's life as opposed to firemen's
lives as opposed to otherness and people's health along those lines. Is it
okay if your grandma lives, but she's in terrible pain for the rest of her life,
you know, or would it be better just to let her pass from smoke inhalation rather
than that and so at that point you've created a machine that basically has
all your values and then you don't even have to ask it anything because it
already cares for your grandma and cares for other humans the same way you do
and you're just like do your thing machine and it makes the best outcomes
happen and and that is the goal and it's a hard goal to make something that has
the same values as humans and would make those judgments the right way as
opposed to a me six. Sure, sure. So yeah, I think one of the two takeaways from
that or two of the takeaways is that yes, it turns out to be very difficult even
for you to explain or for you to explain what it is that you want and it's only
when you're met with failure over and over that you're like, oh, you know what,
actually value that too. You're right. But the other important thing is that you
don't get multiple tries in real life, right? You don't get a magic wish
machine. You get you get a physics based wish machine and yeah, you only get one
wish basically. So you have to get it right the first time or the human race
goes away. And I think part of the concern that we mentioned earlier, but
that the different companies that are working on building AGI for I mean,
just even for like the reason that, you know, the first owner of an AGI or the
first company is going to, you know, become the world's first multi trillion
error, right? I swear to God, the company that makes AGI first is going to be some
spam company that's trying to break all the ways we have to avoid spam.
They're going to be like, we can kind of figure out all your captchas. We can click
on the four house pictures out of these nine pictures. Oh, man. Yeah. And then
that's that's going to piss me off. Spam will be the undoing of humanity. Well,
maybe, but the other the other concern though is that the companies that are
spending more time thinking about how to, you know, actualize friendliness rather
than just like get the job done. They're going to take a time penalty as far as
like, and it's really like first one there wins. And so that's sort of that's
what I think were some of the nervousness creeps in that it's going to be the most
that's why it's going to be those guys in Nigeria. Yeah. Well, it's going to be
yeah, it's going to be the spammers. But it's going to be the group that, you
know, sprints as fast as they can, regardless of the risk, they're going to
they will likely hit the finish line first. And that's what makes it when you
asked what is like an appropriate number to be spending on this? Should it be
