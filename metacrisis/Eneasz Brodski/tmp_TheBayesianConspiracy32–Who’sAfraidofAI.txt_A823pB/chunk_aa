Hey, this is The Basin Conspiracy, I'm Inyash Brotsky, I'm Stephen Zuber, and today we
have a guest.
My name is Sean.
Hi, I've been here a couple of times.
We're going to talk about artificial intelligence and who is afraid of it and should we be?
Yeah, pretty much.
I think we wanted to have like a primer episode because it's something that we've intentionally
referred to a few times.
Yeah, it's a pretty major focus of the rationalist community, and we've referred to it a couple
of times, but never really had an episode on it, so we decided now would be a good time
to do that.
Now, I'm actually not familiar all that much with what the rationality community says
about artificial intelligence, like if it really gets in depth with it, is it really
speculative as far as what to be afraid of?
Does it try to extrapolate a lot?
Because it seems to me like artificial intelligence right now is in such an incipient stage that
like the main reason we had you on here is because you don't know that much about what
the rationalist community says, so you can be like the person from outside who is asking
these questions.
I thought I was like, yeah, this Sean is perfect for this.
The focus is not on what is currently happening.
I mean, it is a little bit in that it's a sign of things to come, but it's more along
the lines of worry about what happens if humans do make an AI and what that means.
We're talking like singularity then, right?
Singularity is such a ill-defined concept.
I think I don't like it too much, and I think in general, the community has been moving
away from that term.
Do you want to say what singularity is?
Singularity has at least two or three distinct definitions, and the one that people speaking
broadly in the rationalist community are concerned with is the idea of an intelligence explosion
where say you get an AI that just kind of jumps us right into the point, and I say when
rather than if, I think that it's inevitable that we'll reach this stage of technology.
I don't think there's any physics barriers or concepts that will be impossible to meet
to make this actually happen.
The question is, does it happen in five years or 500 years?
When artificial general intelligence or AGI for short is created, the idea is that it'll
be better at humans than everything that really matters to us, including writing AI programs,
and so it can write new AI programs or it can rewrite itself, and then subsequently,
once it's better at it than we are, if you could say offspring or its next iterations
will be better than it previously was a second ago, lather, rinse, repeat, and you've got
a quick takeoff to just super intelligence.
I think the term was coined by sci-fi writer Werner Vinci.
That's right.
Yeah, in the mid-80s, and it's basically the concept that when all you have is a rock,
you can't do very much with it, but once you have a rock that has been sharpened and attached
to a stick, you now have a tool, and you can use that tool to make even better tools.
Humans have been doing this for millennia now, and as our tools get better, we can use
those tools to make even better tools to the point where now we can see individual atoms
with tunneling electron microscopes, and we can put things up in orbit and people on
the moon if we want to, and we have this worldwide communications network.
One of our tools nowadays is computers and the programs that run on them.
Right now, humans are making those, but once our tools get good enough, they can start
replacing us and things, and yeah, like you said, eventually we can make a program that
writes other programs, and once we can do that, there's no particular reason the program
wouldn't keep making better versions of itself to make programs that are even smarter and
better and get to the point where they are better than humans at anything that requires
intelligence.
I feel like even if it was just specifically designed to do that, or I mean, the other
thing to consider is that even if you have an AI that I think it was Nick Bostrom wrote
the...
Superintelligence?
Superintelligence, the book, and he has this phrase that I really liked that he says,
you know, as AI gets gradually closer to human intelligence, there's no reason to think
that it'll stop at Humanville Station, it'll probably just zoom right by, and part of that
is because if you get an AGI that's running anything like how we run, it's going to be
doing so at 1.5 million times faster than we do it, just as far as like, because our
thinking speed is what, 100...
200 hertz, 1 to 200 hertz.
Yeah.
So, and the theoretical limit for as fast as a computer can do it is like the speed
of light.
So, even if it's thinking at the human level, it's thinking at a rate that dwarfs us like
crazy.
You can get from like ancient Greece to now in an afternoon.
If it's thinking millions times faster than we are, it can cover a lot of ground.
I want to get back to that in just a second, but before we do, what was your understanding
of what the word singularity means?
Oh, I was thinking of recursive improvement of the intelligence question, which is just
like a super abstract, like just conceptual, like it's just, because it's like the seed
for a lot of like science fiction writing, you know, obviously, like everyone's heard
those stories before, the robots take over.
Yeah, because that's fascinating.
Yeah, no, right, like the creator race being destroyed by its creation.
And all parents fear, I guess, I mean, I thought parents were supposed to fear their
children dying before them, but I don't know.
Maybe I see this is why I'm not a parent, I'd be like, I'm not feeding you bastards,
because if you grow tall, you'll beat me up.
Yeah, that's totally want to keep you short liable that that's the version of the singularity.
I'm thinking of it's just the really basic conceptual, you know, explosion of intelligence
like you were talking about.
I don't, I'm not aware of any other definitions, unless we're talking about like incremental
moments leading up to that or something.
So there's kind of hard to figure out when that would even be.
Yeah, no, no, I think, so like in a quick nutshell, there's Kurtz-Wiles version, which
is where like technology growth happens, starts happening really, really fast.
And I'm not sure where he draws the line at singularity happening.
Bridge technology where you can like escape velocity and stuff like that, right?
Like he talks about living forever.
Yeah, yeah, him and Aubrey de Grey, but different avenues, I think they're looking for.
Aubrey de Grey talks about longevity, escape velocity.
I think that might be what you're thinking of, which he does the Methuselah project.
He's looking for like life extension research institute.
And then the third one is the idea that on the other side of intelligence explosion or
whenever you get smarter than human intelligence, whether it's an explosion or a slow takeoff,
you get this, this point where your, your models of being able to predict the future
break down because you can't predict what an agent smarter, what an agent that's smarter
than you will do.
Just like I'm watching AlphaGo play Go and I can't guess what move it's going to make.
Because if I could, I'd be as good at playing Go as it is.
I can, I can anticipate what I think it might do, but it makes a surprising move and it
wins.
I wouldn't have predicted that, right?
So that's, that's where like the, the model breaking down singularity word was borrowed
from.
Charles Strauss has an amazing story about different levels of intelligence in, in agents
where he says that he, you know, has to take his cat to the vet like once a year or something.
And the cat knows that when he brings out the little cat carrying case thing, that means
it's vet time.
And the cat is smart enough to see the case and be like, oh shit, I'm going to get shots.
I am running.
And so the cat sees these things in environment is smart enough to figure out what's happening,
takes action to avoid it.
And it goes running for the cat door and it slams face first right into the cat door
because Strauss has locked it so it doesn't open.
And the cat is always shocked that this one time of all times the cat door happened to
not work.
What the hell is happening?
And Charles Strauss is like, well, I was simply thinking one move ahead of the cat.
I knew the cat would run towards the cat door.
So I locked it beforehand and it's that sort of similar thing where you cannot predict
what someone significantly smarter than you is going to do because then you would be as
smart as them.
Sure.
That makes sense.
So that's, I think, the three bullet points in a nutshell.
I'm kind of curious to do this in the format that I think we're modeling a bit where you
kind of just throw out a question and then the three of us can spit ball around it.
Is that something fun?
Yeah, sure.
I did want to get back to, before we did that, when you said the whole thinking things a
million times faster than humans, first of all, would be incredibly useful because if
I could do 10 years of mental labor in the span of a day, it is a legitimately handy
thing to do.
But as someone has pointed out that even if you gave 10,000 monkeys 10,000 years to come
up with something, they're not going to come up with something that a human can come up
with in just one week because of the difference in level of intelligence.
So the speed is one aspect of it, but it's not the only thing.
It's also the just legitimately smarter than human intellect.
Yeah.
And that's the other thing, like right now with, I don't know how much the two of you
know about neural networks, but even right now, so like neural networks, like the concept
has been around for like a really damn long time, like I believe since the 1950s.
And then it got kind of scrapped and it wasn't very well defined over time.
And like, I think it had a bit of a resurgence in like the 90s, late 80s, 90s.
And then it got dropped again.
People were like, oh, this is crap because it's really not like, like basically the way
they were going about implementing them wasn't up to up to snuff exactly.
So and then now it's had another resurgence and now that's why you have things like Spotify's
Discover Weekly, like that's based on a neural network.
AlphaGo was a neural network.
Right, exactly.
And like deep, you know, like the image, the deep dream or, you know, stuff like that.
But there's this tendency for, if you like, you have to give a neural network like training
algorithms so it like can learn.
But sometimes it's like too smart for its own good where it will take data points and
it'll try to constrain boundaries around those data points instead of using like a line to
divide those like as a, this category, this other category.
So you have to like, you can't let it learn for too long or else it'll over refine it
and become very inelegant or very like, it's not good at all.
Like when humans see faces in clouds and in everything.
Yeah, right.
So on machine scale.
So we're still at that stage where like we're really trying to make neural networks more
efficient and more reasonable to do the things we want them to do.
And unfortunately, even in the best case scenario often requires a lot of human intervention.
So I honestly like, I'm just curious like when, like when do we realistically think there's
going to be this explosion of intelligence that actually is going to take off in every
possible direction and actually be smarter than humans and be able to like do abstract
thinking like that's.
So the, I mean, that's, that's a very speculative question because how do you answer something
like that, right?
Yeah, right, right.
I mean, I'm just kind of thinking aloud.
I mean, I'm not expecting an answer really.
I have sort of an answer.
Okay.
It's not like a date that I'm going to give you.
Okay.
2028.
Exactly.
August 17th.
For the record, I want to interject.
That's one reason I'm not a huge fan of Ray Kurzweil is he keeps putting years, sometimes
months on when this is going to happen.
Do you think he's just projecting a lot of optimism because he wants to live forever?
Yes.
Me too.
But I mean, well, it's probably a bit, right?
Like it, it happens to be during his lifetime and happens to be at like that 20 year ish
mark all the time to where like it's far enough out where there's some flexibility and like
