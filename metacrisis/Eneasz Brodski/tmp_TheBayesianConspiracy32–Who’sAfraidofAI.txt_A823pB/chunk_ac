I think.
I mean, they're going to put a lot of people out of work if they work.
Autonomous workers, autonomous cars that drive you ever, like, like right now Tesla
basically has that, I think, but they're not fully autonomous.
They're like, it's, it's like mostly autonomous with some human intervention, like you have
to be in the car.
Yeah.
Obviously you would.
Those things, they're also sort of important, but none of those are extinction level events,
you know?
And not, not just extinction level, but permanently turning a growing sphere of the universe into
something worthless even after we're gone potentially.
Yeah.
But, but as far as like near horizon stuff, I do think that automation is one thing that
we see happening now and we'll see it continue to happen.
Just think of like all the, all the people, all the factory jobs that humans used to do
are all done by robots now and they're done faster.
They work nights.
They don't demand pay and unions and stuff.
And so like that, that takes off pretty quickly.
I mean, I think it's extremely likely that in the next 15 years we'll have self-driving
cars.
So like people won't own cars the way that we own them now, or like if we do, it'll
be kind of like a fringe thing, but you'll just like order a car to, you know, whenever
you need one and it'll drive out from whatever local bay and it'll take you to wherever you're
trying to go, like an Uber, but minus the person.
And like fast food workers will be replaced with, you know, self-order kiosks and robots
built, you know, making your burgers or whatever.
That's definitely something that I see as likely in the next.
I like that 20 year vague number, but I mean, we're seeing beginnings of it now.
So it's not, it's not somewhat speculative.
It's just like, when will this happen?
How wide it'll be and how fast it'll happen.
And like you said, there's, there's no reason to think that a computer can't be
made to, to optimize things as well as a human can, even if it doesn't think exactly
like a human can, there's no boundary that says, well, it can't become this smart.
Given a specific task, like driving and not crashing, you know, like, like definitely
that seems, that, that seems like that's something in reach in the near future.
Right?
Like, say we get to that point 15 years, we got these fully autonomous cars that are
just like totally better than humans at driving, or at least, you know, basically
the, like on par with the best drivers, right?
Just they're really good.
And like, if an accident happens or like you have that, you're like, some
emergency thing happens, it's like the trolley problem and it tries to solve that
and it like kills people.
Like, but like who's accountable in that case?
Cause it's, it's, it's, it's AI.
So, so what, so then what do you do?
Like who gets, who gets in trouble?
Like what happens?
I mean, why do you necessarily have to assign blame in those sorts of cases?
It, I mean, I suppose you don't have to happen, right?
Every now and then bridges, I mean, like the legal system has to acknowledge
it's something, right?
I mean, the legal system might, if there's a human that it was involved and can
be blamed, but the whole point of assigning blame is to change the actions
of other people, right?
If, if someone has done something wrong, you punish them so other people will
do that same kind of thing.
They will stop driving drunk or they will follow the speed limit when they
should things like that.
Well, so nobody, nobody's, nobody's fault, right?
Probably no fault.
Yeah.
Like, I mean, sometimes an earthquake happens and a bridge falls down.
And what, who are you going to, you know, blame for that?
As long as it was built to code, then.
That makes sense.
I'm just wondering like what the, what, what the growing pain, so to speak,
will be like moving toward that idea.
Like someone's going to be looking for someone to blame in a case like that.
Yeah.
The first time that somebody is in a fully self-driving car and
for whatever reason, a family of three runs out in front of it and the car has
to swerve off a bridge to not kill that, those three people and kill the one,
you know, passenger of the vehicle.
Somebody's going to be pointing a finger and like, who do we go?
You know, the family, that person will be like, who do I fucking beat up?
Right?
Or what, who do I, who do I vent at my, my rage at?
And they're not going to, they're not going to vent at the algorithm that
made the decision.
They're going to vent at the company that produced that algorithm.
And, and we're going to do this shit out of them, right?
Yeah.
Or try to get money out of them.
I think it's extremely likely that you'll have to sign very detailed
explanations like what you're getting into.
But then that is sort of a question.
You mentioned like trolley problem things.
Like if I get into a self-driving car, it's different than if I'm driving it.
Like if I'm driving, I might well make the decision to throw my car off a bridge
to save a family.
Yeah.
But having that decision made for me seems like something A that not everyone
would get on board with and B, I think there's this impression that it's different
in an important moral sense, right?
I don't think it is necessarily, but I do see how people will, if they, they'll,
they might want to check a box.
Nope, save me at all costs.
But I don't know if make, I don't even know if adding that option would be moral to
do, right?
I think if there is a large percentage of people who would not utilize self-driving
cars, unless there was such an option, then it should be an option and maybe even
standard in cars for at least the first few decades, because just the amount of
lives that would be saved by taking humans out of the driving.
Yeah.
I know, kidding.
Cause how many accidents are there a year by people who are like drunk driving or,
you know, even sobering.
Like it's like, it's absolutely going to be better.
Like, like if, if we get to a point, like, you know, Thomas cars in the future,
they're going to be like far fewer accidents so that like, it'll be weird
and anomaly when there are accidents, right?
Like in theory, that should be how it ends up, right?
Yeah.
Unless all the cars go apeshit all at once and start rampaging and just
killing everybody, then then it wouldn't decide that they don't like humans anymore.
Exactly.
Strive them all off the cliff.
But, but obviously we're talking about like 15, 20, 30, you know, years of,
of like very focused implementations of, of artificial intelligence that doesn't
involve making, using abstract thought and making big decisions and like seeing
the universe or whatever we were kind of talking about with like super future AI.
Yeah.
Yeah.
I think 30,000 people on average die every year in the US from automobile accidents.
And yeah.
So what's that every day?
Right.
So over 90% of those are due to human error.
Oh yeah.
Well, I'm sure the best, yeah.
I mean, they're being, if, if that was just cars falling apart randomly,
then that's that, that would be a whole big lawsuit thing.
Right.
So, oh yeah.
Um, yeah, it's just people want to go fast in the way to work or, you know,
they think they can change lanes.
They think they can make it to light.
Just little things happen and you can only gamble so many times and before,
you know, you lose, right?
So, uh,
and all those weird trolley situations always like star you out in a thing like
there's a family of three crossing the street and you have to swerve to and kill
yourself or else kill the family.
It's, it's, it's very hard.
Questionnaire, wasn't that like a thing that MIT was doing?
Like, what would you do?
Would you kill these people or these people?
Yeah, but it's with self-driving cars even being put in that position is nearly
impossible.
It's not like the self-driving car does not see the family of three or like it's
barreling down residential streets at 90 miles per hour.
I mean, they have radar.
They can see 360 degrees around them all the time.
It's reaction speed is fast.
Yeah, right.
I mean, every now and then maybe something will happen.
There's some kind of glint off a truck that fuses its sensors, but this is not
going to be a regular occurrence.
Yeah.
No, I totally agree.
And I feel like people are like, so if this impossible situation that happens
once every decade were to happen, no, God, really?
Yeah, you don't need to spend a lot of time planning for those fringe contingencies.
I think you're right.
Yeah.
But the, the, the crazy thing about going back to this super human level AI, the
crazy thing about that is that it is something that is, it's not physically
impossible.
So eventually we're going to get there.
I don't, I don't see how we could not get there because having a super human
intelligence is just such a handy tool.
Someone's going to make it.
I got I reminded of when I can think of an obvious way it won't get there.
Oh, I think it's wiped out.
Yeah, we kill ourselves.
I remember when things don't go terribly.
I remember when CRISPR was, you know, discovered and right away the, the
bioethical board, whatever it is in the US said, we will not be doing any CRISPR
experiments on human embryos.
That is unethical and no one was allowed to do that.
And within one year, China was like, Hey, here's these CRISPR embryos we got
going and is like, yeah, what are you going to do about China or, or Korea or
whatever country it is that things like, Hey, we could use that tool, especially
since we are being threatened by this hegemonic military force.
So at some point, as long as it's physically possible, we will get there.
And if there's a race, we'll probably not get the, the friendly version.
We'll get something that isn't quite aligned with what we want.
And then we all go extinct.
And you'd think that the human race would put some small level of resources
into maybe looking ahead at this.
And it's, it's minuscule.
I remember when, when Trump was elected, I thought that point though, right?
So, I mean, we still have a lot of time to be thinking about it.
Can I, can I, the problem is, well, yeah, go ahead.
Well, this is actually the perfect tie into what I was going to mention
earlier that Stuart Russell wrote the, he wrote the textbook on AI called our
