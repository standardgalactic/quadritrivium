Hi, this is the Bayesian Conspiracy. My name's Katrina Stanton.
I'm Inyash Brotsky, and I'm Stephen Zuber, and Inyash, do you want to introduce our guest for the day?
Yes, today we have on the line Robin Hansen, who is a professor of economics at George Mason University.
Yay!
Hello, everyone.
He's also the blogger at OvercomingBias.com, which is very popular among our circles anyway, and recently authored the book The Age of M.
That's right, with many other credentials besides that. For example, Robin Hansen also invented prediction markets.
Well, at least certain variations on them.
Certain variations of prediction markets.
And for a while, you had a prediction market running with DARPA, right, as a sort of a research thing?
So most recently, it was IARPA, which is a spin-off that's kind of like DARPA.
We had a research grant with them, most recently on science and technology questions, a little bit before that, on world event questions.
That's in the last six or seven years. A long time ago, I had a DARPA project in 2003 that hit the news when it got killed quickly.
I remember hearing about that. I didn't actually realize it was you at the time.
I don't think I knew about the whole less wrong rationality scene yet at that time.
Well, we're really excited to spend most of our time together talking about your recent book, Age of M.
But I was actually hoping I could ask you some other questions about your involvement in rationality.
Sure. It's all fair game, until I say it's not.
Okay, well, that's fair. So when you started the blog Overcoming Bias, what was the state of the rationality community at that time?
There wasn't one, I guess. There's lots of people in the world who think they want to be rational, but they didn't see themselves as part of a community.
Did you have correspondence with those sorts of people frequently before Overcoming Bias became a thing?
Well, I mean, I've always had correspondence with lots of people, and many of us share common interests.
I'm actually not so sure it would be that different a group of people if they identified them around some other theme.
So once upon a time there were people interested in futurism and futurist things, and then there's rationality and more recently effective altruism.
And these various labels attract somewhat different groups of people, but honestly not that different.
They're mostly relatively smart, sincere, mildly contrary, and people interested in the wide range of topics.
What was your goal when you started the blog Overcoming Bias?
Well, my colleagues had started a blog a little while before, and I was being left out of the whole blog thing.
So I wanted to have something to talk with my colleagues about, and it did seem like an interesting process.
I had just gotten tenure, and so I just struggled for many years to achieve the sufficient number of publications of sufficient prestige,
and then having that big thing blow up in the news actually helped about the policy analysis market.
And then this blogging thing is tempting because you can just read all sorts of different interesting subjects,
and then spend a few hours crafting a few paragraphs that in your mind at least contributes to a subject that makes a new point and insight,
makes it clear, communicates it to people, and then after a few hours you're done and you can go on to the next topic.
So people like me who really like to think about lots of different topics, it's very tempting to think that you are contributing that way,
that you are instead of spending months to produce a paper, you spend a few hours to produce a blog post,
of course it's not as deep as a paper, but it could still contain insight and the net effect of blog posts today over many months
can be a lot even more than the number of papers you would have produced at that time.
Especially because it's so much more accessible. I can read a blog post, I can read several dozen blog posts every day,
but reading a single paper a week tends to be a problem.
Yes, although the key question is whether you build on it.
So in my mind there's a big difference if you write a newspaper column and 100,000 people read your column and they understand it,
they still might not do anything with it, and if you publish an academic article and 10 people may read it,
three of them may do something with it.
In some sense you have more intellectual descendants when people do things with your idea,
not just nod and say yes that sounds interesting, but build on it, accumulate.
Do you have examples of people reading your blog posts on overcoming bias and then making something beautiful,
building on your ideas, your original ideas?
Well, I've definitely noticed times when they seem to have been influential and I appreciate that.
I'm influential in the sense that I've taken some ideas from other people and passed them on, such as construal level theory,
so a large number of people appreciate construal level theory perhaps more than they would have if I hadn't blogged that.
I once blogged the idea that if you want to look at a bunch of empirical studies, say regressions and decide which coefficients you believe,
you often might do better to look at a bunch of studies that use the variable that you're interested in as a control variable.
It wasn't the focus of attention of that article and look at those estimates.
That's less subject to selection bias whereas when there's a parameter that everybody has a strong opinion about what the parameter value should be,
then it's hard to publish parameter values that differ from that and the publications can have selection bias.
I heard some other economists at some point say, yeah, I start using that technique now.
I don't have any rationality related questions.
Oh, it's okay. I was actually going to ask a little bit more about the prediction markets.
Okay, yes, let's do that.
Okay, so I went to dinner with my dad recently and I was talking about prediction markets.
My father has been in politics for his entire adult life and he just had this look on his face just kind of scrunched up
and he was talking about how polling is so great and really good polls are more useful for predictions than prediction markets in political races and legislation.
So I was wondering if you could help out by letting me know how you would convince my dad that prediction markets are a very useful tool.
So I would distinguish three levels of things that you can do.
One level, the first level is data, sources of data and a poll is a source of data, definitely.
A second level is a kind of analysis. What do you do with the data?
And there are of course standard ways that people analyze polls statistically so they don't just take the poll literally.
They process polls in order to produce better estimates.
But there are many ways to process data to produce estimates and people often disagree about how to do that.
Statisticians don't all agree and analysts don't all agree about how to analyze data.
So a third level of analysis is how we can set up a forum or a competition where different people who have different kinds of analysis and perhaps also different sources of data can compete
and the rest of us can just sit back and not make very many judgments about who to believe but just trust some overall process to produce an estimate that we can all believe.
And that's where our prediction market lies.
It's that that third level of a forum or a competition or a process where different people with different expertise play in that world
and the rest of us can just look at the market odds and believe that we don't have to decide how much we trust 538 because they got it right in 2012 or something like that.
In your experience, how have prediction markets fared against expert poll takers?
There are polls and then there are experts who process polls and then there are prediction markets that have those experts participate in them.
But again, I wouldn't want to put the prediction market up against the experts.
The prediction market is a process by which experts can express themselves.
So there are many different experts out there who have polls, which ones do you believe about what?
That's a key hard question and you'd rather not have to make those judgments.
You'd rather have some process that you can use that aggregates all of those different experts together in a reliable standard way and you can just go to that standard source.
Can non-experts participate in prediction markets?
Well, it depends on what you mean by the word expert.
If you mean by an expert, a credentialed person, a person with a degree or some sort of institutional affiliation such that we nod and say, ah, yes, that must be a good person because they have that sort of degree or institutional affiliation, then yes, of course.
Because as most of us know in the world that we know best, the people who have degrees and institutional affiliations don't always know best.
For any particular question that you might ask, the key real issue is how much has someone thought about and studied the relevant pieces of information for that particular topic.
Whoever has studied a lot and the most in another sense of the word expert, they are the experts.
And what you want as a process is where the real experts participate and other people back off and shut up.
And prediction markets are such a process that gives people a strong incentive to think about whether they are the real experts and participate if they think they are competitive with the real experts and then back off and shut up if they don't actually think that.
Because they would be risking their own money on their predictions.
Right.
They will lose on average relative to the other people who are the real experts and either people know whether or not they're the real experts and back off immediately or they're wrong.
And then they start participating and they find out by losing that they aren't as good as they thought.
In practice, has that happened?
Yes, yes, quite consistently.
How do you measure the accuracy of what comes out of the market versus, let's say, what is it, 538?
Yeah.
Versus 538.
So most of these institutions are modern institutions and the things they put out are probabilities.
That is, they don't just give you a number.
They give you a probability distribution over the various outcomes that could happen.
So when people put out probabilities, there's a key question about whether they're well calibrated.
So a well calibrated expert, well, when they say 70%, 70% of the time, I mean, when they say 70%, if you ask what fraction of the time are they right when they say 70%, it should equal 70%.
Somebody like that is well calibrated.
Now, you can be really stupid and ignorant and be well calibrated.
Being well calibrated is not the end all of everything, but it's a nice thing to be.
Because if you are well calibrated, then the answer to the question how useful or accurate are you is in the probabilities.
The source that can consistently give more confident probabilities and still be well calibrated is more accurate.
So that means you can just go to any well calibrated source, such as a prediction market, and if it says that the current odds for Trump being elected are 27%, then you can say, well, that's how much they're telling me.
You know, there's three-quarter chance that they'll lose and a quarter chance they'll win.
So if you have different sources and they disagree about probabilities, as long as they're both well calibrated, you, of course, will prefer the source that gives you more precise estimates.
If you had another source that told you Trump either had a 90% chance of winning or a 2% chance of winning, then you'd say, oh, that's a more accurate source because it's telling you it's more accurate if it's well calibrated.
So the key question is, is your source well calibrated?
So people who do statistics carefully tend to have well calibrated sources and prediction markets have consistently been proven to have well calibrated estimates, but many pundits and other sources you might go to are not well calibrated.
It's because they have incentives to skew things in their favor.
Yeah, they have an incentive to pretend to have more accuracy than they really do and people aren't really checking them on it.
Speaking about incentives to participate in the prediction markets, we had a question from our forums from a listener.
You said before that on the question of manipulating the price markets, that the fact that an event may be manipulated would make the price more accurate because other traders would come in and try to take that person's money since they're trying to manipulate a price and the actual probabilities don't change at all.
So the heightened possibility of there being such manipulation going on induces more trading, which allows us to have more relevant info and the effect is more accurate prices.
Now, the user, not without incident, asks that as soon as the prediction market is tied to a policy or the prediction has any other real world impact, they don't understand how this could still hold, particularly in cases where manipulators are also able to manipulate the outcome of the event in question.
How would a prediction market deal with that sort of situation?
So your initial quote was about manipulating a price that is influencing a price to distort estimates, which is different than manipulating an event.
So I will use the word sabotage to distinguish. I will say that sabotage is where you go change the world in order to look good in your forecast, whereas a forecast manipulation is where you go change the forecast to get the decision you want.
Those are two very different sorts of problems.
So the things I said are true about forecast manipulation that in a prediction market, as long as some people suspect that other people try to manipulate the forecast, they will come in on the other side and at the end result is the prices actually get more accurate as a forecast.
So it's not a problem for manipulation of the forecast in order to induce better decisions.
Sabotaging events, of course, is an entirely different problem. So it's generically true that predictions and actions interact.
So this is just always a caution to be careful about any process by which you're making predictions.
So first of all, if you make a prediction, then people might use your prediction and react to it and change the world, and that might reduce or increase the accuracy of your prediction to see what happens.
But you need to take that into account when you make predictions that if people hear your predictions, they will change the world in response.
That's why I tend to recommend trying to make conditional forecasts about what happens if people do things, and that is less subject to that problem.
If you say if we do this, then this will happen, and if people choose to do that or not based on your forecast, you can still make that as a reasonable forecast and your forecast isn't changed by the fact that people chose to make a certain decision.
That's one set of issues. Another set of issues is as this person alluded to, which is if you're in a contest to forecast things, one way to win forecast is to change the world to make your forecast come true.
And unfortunately, often it's easier to change the world by making it worse than making it better.
So if you have a large complicated system like a company, it's generally hard to make that company better by making its products better and its service better, etc.
It's often easier to make that company worse by somehow sabotaging its products or its distribution or something.
So there is always a risk that by giving someone an incentive to forecast well, you're giving them an incentive to sabotage some process.
So I definitely think that's generically true for all ways that you might encourage people to forecast things, and it's also true about prediction markets, and there are a number of very straightforward standard cautions to take into account if you want to deal with that.
How would you prevent this sort of thing from happening, or is this a fatal flaw that would keep prediction markets from ever being implemented?
Well, again, if it's a fatal flaw, it's a fatal flaw in the idea of predicting.
So all systems of prediction suffer this flaw, which is that if you give people an incentive to predict the future, you might give them an incentive to change the future.
Yes, but like Nate Silver, for example, probably doesn't have enough incentive to go out and try to assassinate someone to change the outcome.
So it matches his prediction, whereas a line worker at an assembly plant might have incentive to make a few thousand dollars by sabotaging his product to make sure it comes in at his lower prediction.
So now you're changing several things at once in your two examples.
Okay.
So you're talking about the election versus a particular product on an assembly line.
So the particular product on an assembly line is just a more influenceable event that anybody could influence, including Nate Silver.
So if you ask Nate Silver to predict that event, he might also be tempted to mess up with that assembly line.
So several considerations.
One is who has what ease of influencing the event.
So in general, if there are some people who have particular potential for sabotage, you might want to make their actions more visible, including their predictions and to watch for them sabotaging.
That's a standard thing you might do.
You could also try to adjust their incentive so that they don't want to sabotage.
So for example, I recommend that if you have a project forecasted a company and you want to say predict whether the project makes a deadline.
If you allow lots of people to participate, you have the risk that they might sabotage the project to make it not make the deadline in order to gain from that forecast.
And so you probably just want everybody who has an ability to sabotage the product to have an overall positive interest in the project.
So what you can do, for example, is you give everybody $100 if the project makes the deadline.
And you let them bet that $100 if up or down, but never below zero.
So you make sure they can take the $100 if down to $50 if or down to $20 if, but never negative $20 if.
So they all still have a positive incentive to make the deadline, but they can still express their probability beliefs by moving that amount up and down as long as it stays positive.
So that's a straightforward way to handle sabotage.
Again, I recommend that for any system by which you allow people to participate in some sort of forecasting process where there's a risk of sabotage.
It's not particular prediction markets. It's just part of the general idea that people, if they have an incentive to forecast and an ability to influence, they might be tempted to sabotage.
Okay.
Thank you.
Yeah.
That was very educational.
So are you guys okay with moving on to the age of M?
I am, I am.
Yes.
Okay, let's do that.
Actually, can we try something?
Since we're about to get to the most important part.
This is an earbud mic.
Oh, that's better.
That is definitely better.
Really?
Yeah.
How disappointing.
This is the cheap one and this other one was the expensive one.
Oh, yep.
Well, I've been carrying around this more expensive mic with me to make sure I have it available to use. Maybe something went wrong with it recently.
All right now, this is the built-in mic.
Actually?
This is by Mile, I think the best.
Yeah, this is the best one.
It's got a lot of echo to it though.
Oh, it's got a little echo but it doesn't have annoying wine and that's important.
Okay.
I find the wine less annoying than the echo.
All right, we'll keep this one.
I am, I voted two to one here.
I wouldn't be too discouraged.
Maybe your other mics are optimized for other situations rather than like third-party Skype recording apps or something.
That's the main, that's my main use I got this for.
Oh, well.
I've been doing a lot of podcast so.
I have noticed that the really expensive mics are fairly delicate.
I've destroyed at least one just by carrying it back and forth without putting proper padding on it.
I mean, it took a lot of driving to get it to where the quality was significantly lower,
but I did just through not being ultra-gentle with it ruin one.
Microphone research can be our next episode.
I guess I could have done that.
I am excited about getting into Age of M though, so who wants to take us there?
Do you want to just, I guess.
Age of M.
Do you want to just, I guess, give us your quick thesis and go as long or short as you want?
No, no, no.
Or not.
Could we get a quick summary about the book from here?
Yes, of course.
Thank you.
So the subtitle is Work, Love and Life When Robots Rule the Earth.
So it's like a science fiction novel, except there's no plot and there's no characters.
All there is is the setting, but the setting supposedly makes sense, which to you, if anybody
who's ever sat and thought about their favorite science fiction novel and realized that it
doesn't really make much sense, this is for you.
So I've taken a very particular scenario in science fiction, a particular technology that's
been often discussed in science fiction and futurism, and I assume that particular technology
is realized and pretty much nothing else, and I just try to work out in great detail
all the consequences in 350 pages to prove to you that it's possible to work out a lot
of detailed consequences from one of these standard scenarios and have it make sense.
And the particular scenario that you're talking about is mind uploading, yes?
That's right.
Also known as brain emulation.
The standard story where you make a computer model of a particular person's brain and now
it can live on a computer, it can be sent at the speed of light through telecommunications,
you can make copies of it, all those sorts of things, and I work out lots of consequences
of that.
Okay, there are some priors that you come in with.
Could you explain those?
I am an economist, and so I'm going to use standard economic tools.
I'm going to use supply and demand, which is our standard first cut analysis, which tends
to assume there's a lot of buyers and sellers of most things and there's not that much regulation.
I assume some rough equilibrium that it's after the early transition to this era.
Transitions can be rough and hard to predict because during transitions, people aren't sure
where they're going.
So, but once the world is there and people are used to it, then it's easier to talk about.
I also presume that these brain emulations are relatively simple and which makes sense
in the early era and make less sense later.
So, a brain emulation, again, you took a particular person, you made a model of their brain and
you now have it on a computer, you can turn it on, turn it off, copy it, erase it, run
it fast or run it slow, and that's about it.
You can't do lots of other fancy, you know, stick a probe in and make somebody like chocolate
ice cream or combine one person's music with another person's Christmas vacation memory
and you just can't do all those things.
All you can do is this limited set of actions.
I have a question about that prior.
So, you mentioned that this is a time period that could potentially be a thousand years
for the uploaded minds.
So, after the first couple hundred subjective years, is it reasonable to think that they
are able to appoint, if they're being very productive, that which I think is something
that you also assume that the economy is doubling very quickly, that they are able to do that,
that they are able to go beyond being simple emulations?
So, today and for the last few centuries or even thousands of years really, growth has
primarily come via innovation.
So, the main way we are able to double the economy is by innovating enough in a wide
range of technologies.
That's where growth comes from.
So, growth is so far typically rising up a ladder of innovation and if we rise that
ladder faster, we still might rise at the different ladders of technology advance at
a similar relative rate so that we still have to go so many steps up the ladder before
we can reach certain achievements.
So, if today, you might say we're doubling our economy every fifteen years, if there's
some advance you think would take another hundred and fifty years to get to, then you
are thinking it's ten doublings of the economy, say let's call it ten steps of the ladder.
We have to go ten steps of the ladder up to get to that achievement because it's hard
and we're a long way off.
Now, if the M economy speeds up and grows faster, then we could reach any particular place
in a shorter amount of time but not necessarily a smaller number of doublings.
We still might need to rise this ladder of technology in order to make these developments
so that we can achieve various things.
So, if you want to argue that some kinds of technological developments will become easier
relative to others, that would be an argument for saying we'll go up some ladders faster
than other ladders once the age of M shows up.
That's an argument you could make, but if you're going to think that we just rise up
all these ladders at the same relative rates as we did before or the same relative rates
as we would have without the age of M, then you should expect we would reach any particular
level in the same number of doublings of the economy, which might happen in a lot shorter
time but still a lot of distance in terms of economic growth.
I've got a question about what happens to the humans in this M economy.
It seems that in your writing you emphasize that the people who are most likely to get
emulated and to have many, many copies of their emulations running because employers
really like employing them are people who are very uniquely suited for their jobs.
They really enjoy that sort of work.
It is their goal just to do that work.
That's obviously not a huge portion of the population.
What happens to the humans who are not suitable for becoming a standard worker?
The key parameter is because it's so easy to copy emulations, the emulation economy
will focus on the few hundred best.
Whether they're best in terms of being workaholics or whatever it is, doesn't really matter that much.
The point is the M economy mostly looks at the few hundred best and the other 7 billion
not best are much less in demand.
That's a bust feature of just having a very small number of the best that you can make lots of copies of.
What happens to those other 7 billion people who no longer have jobs because of the 100 best people
and all their various clones have taken them?
The age of M plausibly only lasts a year or two in objective time.
That is, if the economy doubles every month and the age of M lasts through a similar number of doublings
as the past few eras have, i.e. the industry and the farming and the foraging eras
then it would be over in a year or two and something else might happen.
So the entire age of M may only last a year or two in objective time.
So there's not a lot of time for cultural change of humans.
A small number of big things happen to humans and then they're pretty much what you would think humans would be like
given those changes without needing to think of anything else because there's not much else that can happen.
So the two enormous things that happen to humans are, as you indicated first, they lose all their ability to earn wages.
They're just out of jobs. They must all retire.
The second big thing that happens to humans is humans start owning pretty much all this new economy.
If it doubles every month, their wealth doubles every month.
So collectively, humans are getting really rich really fast.
Collectively, humanity would be yes, but there are a lot of people out there who have no net assets at all
and so their wealth would not be doubling because they have zero and they couldn't just retire because they don't have anything to retire on.
I think that Robin says they're SOL. Is that a good prediction?
Just to be very clear, very straightforward, if you lose your ability to earn wages
and you don't have assets or insurance or sharing arrangements, you are in trouble.
There's just no escaping that.
Isn't that kind of a recipe for worldwide revolution though?
Because I know a lot of people can't go two years without any income at all.
So the obvious prediction is that this will vary from place to place.
That's what we've seen in the past. We've definitely seen in the past the degree to which people have assets, insurance or sharing arrangements has varied a lot.
And so I just predict it will vary in the future.
I don't predict the age of M will be uniformly distributed across the globe.
Most likely, emulation economy is concentrated into a small number of very dense cities, which could fit in a very small part of the globe.
And the nations and areas that encompass those em-cities can get rich, but the rest of the world not necessarily so much.
So there are global risks here.
These insurance or sharing arrangements need to be global if they're to be effective.
If you and the people in the few hundred miles around you decide to share with each other and you don't end up actually being something this new economy makes much use of,
then you're not actually insuring against the real risk here.
So I definitely recommend and I've tried to make this clear in all the talks I've given that one of the most obvious policy recommendations I have is that people should try to set up,
you know, diversify their assets and or buy insurance and or set up sharing arrangements so that they can deal with the sort of risk and those things need to be global in some indirect way at least.
They need to have access to assets and resources that would be valuable in an age of M that is not at all equally distributed across the globe.
You were talking about how these emulated minds would be only have a few people and then maybe only have a few hundred very productive people.
And I was wondering Robin, what you think of the value of diversity in work?
This is a question that comes up often.
I think if we look at most product markets and most industrial markets in our economy, we see a relatively low demand for diversity.
That is, most product markets, say toasters or whatever are supplied by a relatively small number of producers.
And that's how most people seem to like to buy most products from a relatively small number of producers.
If you even look at employers, you see that when they're looking at employees, they tend to prefer employees that come from relatively standardized sources like elite colleges that produce a relatively standardized product.
And they're not that interested in variety, honestly, and in fact, variety they often see as a cost.
Now there are some professions, say advertising where they put an especially high value on creativity and variety, but in most industries, they don't.
So most employers have to accept a lot more diversity today than they really prefer because that's the product they're stuck with.
These humans that actually vary quite a lot, but most of them don't really want that much diversity.
And in the age of them, they will choose employees the way most of us choose toasters and everything else in our lives with a limited amount of diversity.
Now we have a literature on productivity and diversity of teams.
And so we know that there are some kinds of diversity that are productive on teams and presumably a competitive economy will look for enough of that kind of diversity, but other kinds of diversity are actually not so much.
So in fact, the main kind of diversity that is useful is different ways of thinking and especially different sources of information or kinds of analysis.
So it's good on a team to have people who come from different professions, different disciplines, different styles of thinking in terms of their background, that is useful.
But holding that constant actually teams seem to do better when most other demographic variables are not diverse.
They do better with common ethnicities, genders, ages, all the other sorts of variety. Most teams are actually more productive when those other parameters are more common.
I have a question about to go back up again. If I don't trust my local government area to provide me with that income over the few years, which I don't.
So I'm trying to pick up assets right now. What would you recommend as the assets in particular that you think are best to purchase to go through this to get through this phase of human history?
You actually looking up yourself.
Yes.
It's most likely to be useful in this.
Back on what I call, I may have a section on policy, which I talk about what we should do together that would be good for all of us. And then I have a section on success, which is about what you should do to be good for you.
So I do try to address both of those.
So the standard economists answer, which I think is roughly right, is that you shouldn't make bets unless you actually know better. So index funds are ways to just diversify your investments without really betting on which classes of assets will perform better than which other classes of assets.
So I do recommend that you mainly purchase index funds.
If you happen to know something about a particular asset relative to other people, then that might justify you speculating on that asset, but that's obviously not going to be a general useful advice for me to give people here in an interview.
The only variation on that I'd say is if you could construct insurance that was targeted to this particular rich risk that would be more useful than just generically having assets.
So if you could convince an insurance company to actually offer a product, which was it we pay in this situation, but only in this situation, then it'll be cheaper for you to insurance sure against that risk by buying that insurance paying that premium.
As long as we're in back peddling mode, I wanted to ask about there was another predicate or there's other, I guess, yeah, prior to the age of M that we didn't talk about yet, which is you predict that it'll happen before strong AI.
And there was a related point which we can or cannot get into depending on whatever you prefer that you're a proponent of slow take off as opposed to fast take off of, say, super intelligence.
So I think the future is important enough that we should explore a lot of different scenarios. So for example, the future is important enough to be worth having 100 different books that explore 100 scenarios, where mine is just one of those scenarios.
That's how important the future actually is.
So into that case, it would be worth having a book on a scenario that had only a 1% chance of happening if they were mutually exclusive.
I certainly think my book rises above that 1% threshold.
I happen to think it's more likely than others, but I do want to be clear that my book is presenting itself as being expert on what happens in this scenario.
I'm not really presenting myself as an expert on the likelihood of this scenario relative to others. That is, I have some comments about it, but it's not an emphasis of the book.
So clearly I'm not presenting myself as being very expert on that.
I would rather we had prediction markets on these various scenarios and by that we could then decide which scenarios are worth exploring, but it is worth exploring a lot of different scenarios.
So that's my prelude to say, yes, I have opinions on this, but it's not the focus of the book to argue about which scenario is more likely.
I was an artificial intelligence researcher for nine years, a while ago, from 84 to 93.
And I've been in the habit of asking AI researchers that I meet at events who have been in the field for at least 20 years, how far have we come in the last 20 years in your field of expertise relative to the goal of human level abilities in that field?
And they usually say something in the range of 5 to 10% of the distance with no noticeable acceleration.
So at that rate, we're talking two to four centuries, which is short in grand historical times.
It's definitely the sort of thing that's coming and one should think about.
But emulations seem like the sort of thing that could happen even faster than two to four centuries.
And that's a reason why we might at least consider this scenario that emulations come before other forms of artificial intelligence.
And yes, my book does assume that at the time when emulations are feasible, just before that time, humans are still very valuable.
Humans are getting a lot of income.
Humans are very useful.
Almost all jobs have not been automated.
It's still only a small fraction of world income is going to pay for automation of computers and most income is paying for human workers.
When you talk to neurologists and scientists and computer scientists who are doing emulations of living organisms, what did they say about their rate?
What did they say about the rate of progress?
I'm not an expert in that field, so I do not meet those people at conferences.
So I'm not in a position to ask them that.
I would love to hear that.
I hear these things more indirectly.
In the field of artificial intelligence, it is a standard trope that the field is trying to achieve human level abilities in these fields.
So it's a question that everybody understands and is familiar with.
I don't think that in the fields of neuroscience, the question of, you know, when will we be able to emulate a whole brain is in fact a standard question that they ask themselves and that they frame their field as producing.
You would have to introduce the topic to them a little more before you ask the question compared to an ordinary AI researcher for whom that's just a natural standard question.
Okay.
So for emulated minds and these futurist ideas, do you also publish peer reviewed papers?
Or have you?
I'm an expert in neuroscience and I'm not presenting myself as an expert in neuroscience.
If that's what you mean.
No, no.
I myself as an expert in the social analysis of these sorts of scenarios.
Okay.
Have you happened to produce any peer review publications?
Well, this book is peer reviewed.
And so this is my, you know, my large product.
I am in a field where people tend to do articles and books are more rare.
But I like books, especially when you are trying to introduce a large new topic and a article is not quite enough for it.
So I think that is more true here.
I did need a lot of setup in the book to sort of set up the situation and to make the analysis concept plausible.
And so I do think this is a topic that is better suited for books, at least the initial presentation of the idea.
Once people accept the basic idea, then you could more write articles on it.
But the first thing I think more needed to be a book.
What kind of people do that peer review for you?
This is such an unusual and common subject to follow.
What kind of people are they?
Economists or computer scientists are reviewing this.
It makes it expensive, basically, or hard.
That is, when your book or article is within a standard narrow field, then you just go look for people who are experts in that standard narrow field.
When your book covers a wide range of fields, then there are not pre-existing experts in that set of wide range of fields.
Then you more need to get experts in each field and ask them just to judge the parts of the book that are overlapping with their field.
And people actually are more uncomfortable with that.
When you ask a reviewer to review a book, they would rather have an overall opinion on the book and to judge that based on their expertise.
And if there are big parts of the book they feel they can't judge, they are more reluctant to give it any review at all.
So it's more work to find people who are willing to just review the parts that they know.
And then you have to collect more of them so that you can cover the range of topics before you could have that.
So that is what you and many others face as a problem when you say, read reviews of my book.
Each review of my book is by someone in a particular field.
And the reviews that I want to hear are people who say, well, the things I know, check out here.
But there's these other things I don't know.
So I can't really endorse it that well because I'm not sure about these other parts.
I have a question from a listener again.
Is there another time for that?
Yeah, you can ask Holger's question.
Yes, I am.
Okay, go for it.
So you were talking earlier about economy doubling times on the order of possibly a month or less.
Our listener Holger says he doesn't understand the basic assumption of why growth is happening at all in the world.
Because the amount of resources needed to sustain an M are presumably much lower.
I believe that is one of one of your assumptions as well.
So demand in general goes down a lot and that indicates that supply should also go down.
And the economies should shrink, although not necessarily with the loss of quality of life.
So what are the products that are being produced and consumed other than electricity in the M world?
So this is definitely a very basic economics question.
And so I feel very confident stating the very basic standard economics answer here.
But let me show you analogies to make it clear.
Until a few hundred years ago, pretty much everyone on earth lived at near subsistence levels.
And that was the way humans lived four millions of years before that.
And pretty much all animals on earth who have ever lived have lived near subsistence levels.
So the scenario where people living near subsistence levels, i.e. having a wage or income or source of food and sustenance that's just barely enough to keep them alive, is the usual case in history.
It's not a strange unusual case.
We have seen it over and over again and we understand it in some detail.
I am saying that that usual case will return.
And so if you think about a thousand years ago, subsistence farmers, they were pretty poor.
They didn't need that much.
They just needed some clothes and some shelter and some heat and some food and that's about it.
But nevertheless, they needed to work most of the time just to produce those things.
Why?
Well, if there were vastly fewer of them, they wouldn't have had to work nearly as hard.
If instead of, you know, a hundred million farmers, there were only one million farmers around the world.
They could have lived lives of leisure.
They could have focused on the few plots of land that were the most productive for fishing or farming.
Only worked a few hours a week and lived lives of leisure.
But because they kept increasing their population every time it was possible to increase the population, they stayed at near subsistence level.
And that's just what we understand is the standard Malthusian was the first person to give a clear statement of this argument that as long as the population grows faster than the wealth,
wealth per person is pushed down to a near subsistence level.
So that's the key story in this future.
Yes, M's can live very cheaply compared to humans, cheap by some objective sense.
Nevertheless, if they grow their population really fast and they can grow it faster than the economy can grow, they will fall into the same Malthusian trap or subsistence level income equilibrium that we've seen in history,
which is where there are so many of them that on the margin their productivity is low enough that they need to work most of the time to survive.
And again, this is not a strange hypothetical. This is the usual case in history that we understand pretty well.
Right. But what are they producing?
So a thousand years ago, what people were producing was the minimum they needed to survive.
They were producing shelter, clothing, food, heat.
These were the main things that people were producing a thousand years because that's what they mainly needed to survive.
So in an emulation dominated future, the main things they are producing are the things the emulations need to survive.
They need computer hardware. They need energy to run it.
They need cooling to cool it off. They need structural support.
They need real estate. They need communication lines.
They need repair. They need national defense and police.
These are the various kinds of things they need to make this economy function.
And that's what most of their time is spent doing.
At what point do they institute a one copy per couple policy, like a one child policy to prevent everybody from living on a subsistence level?
So this comes back to one of my key prior assumptions that you could gain, which I'm saying in history so far, nobody's been running the world.
All the way in the past up until today, technologies have mostly arisen whenever somebody in the world wanted them.
And if other people didn't like it, that was tough.
So we haven't been voting on or taking some grand global decisions about which technologies to adopt, including which kinds of population growth to allow.
We have just not had global coordination on those things.
When we've regulated regulations have been local, and they've been subject to the fact that if a local regulation hurts a local area too much relative to its global competitors, it loses out.
So I'm just projecting that same situation into the future.
I'm just saying, in the age of M, we have not yet achieved a strong global government.
Therefore, the world is not voting on what happens, what happens as the result of decentralized choices, supply and demand.
And technologies arise whenever somebody wants them.
And there is not a grand global regime to prevent population from rising.
Now, at some point in the future, it may well be possible to have a strong global government.
My guess is that would be past the early age of M, but I could be wrong about that.
And perhaps if people dislike the age of M scenario enough, they would be spurred to try harder to create a global government to be able to enforce things like that.
But local population restrictions would just not be sufficient.
Again, it's very likely that the age of M will not be equally distributed around the globe.
A great many, probably most places will look at it as scants, be reluctant to allow it, at least fully.
They will ask for review and commissions and studies.
And then a few places will perhaps even not a neglect, just let it go wild.
And that's where the new era will be focused.
Do you think we should try to prevent the sort of M scenario?
Again, I want to be clear about my expertise because there really is a risk in a lot of futurism that people like spout off on everything.
And then because you kind of know they can't be expert on all those things, you just don't trust them.
So I'm trying to make a bid for being trusted about the things that I can credibly claim expertise.
And therefore I really want to distinguish where I'm claiming expertise from where I might just might have personal opinions.
So in the book, I'm not focused again on the expertise of whether this scenario happens, but it's about the consequences.
And I'm focused on the just predicting the consequences if we do a little to stop it.
And I'm not focused very much on whether I like it or not and evaluating it and on policy recommendations to change it.
I do have some comments about that toward the end, but the focus of the book is just to try to give you your first cut analysis.
In my experience talking to other people about it, I have no doubt and no fear that people will lack and fail to evaluate the world and have opinions about whether it's good or bad.
What I feared is that people would have opinions so quickly that they wouldn't actually take into account a lot of detail about this world.
You know, if your distant ancestors had heard about your world, they would have loved or hated it based on the first few things they heard, which could have been gone in either direction.
Because your world is just complicated and messy and weird relative to your distant ancestors.
And that's how you should think the future would be too.
So again, whenever I've described this to people, even a small number of quick descriptions, people are very fast to evaluate it to have an opinion whether they love it or hate it.
That just seems to be a very natural thing for humans to do.
So natural that they have these evaluations, I think too fast too early.
You should really immerse yourself in a new strange world before you decide whether it's a heaven or hell and whether you're going to fight it or join it.
And so I recommend that you say, read my book.
Something long and detailed that tries to give you a much fuller description and then try to evaluate it.
But again, I have very little fear that other people won't try to evaluate it, so I feel very little need for myself to evaluate it.
I'd rather take the stance of being this neutral analyst who told you about this world and what's likely to happen if you do nothing and let you think about whether you like it or not.
So is that one of the reasons that you went into such extreme detail in your analysis to try to help people immerse themselves in the possibilities of this world?
That was one reason.
The other and perhaps bigger reason is that people tend to think that you can't do this sort of thing.
So 20% of the books on Amazon.com have the keyword history, but about 1% of the keyword future.
And if you ask people, but isn't the future much more important than the past, they'll tell you, well, we can study the past because we have all these artifacts and we can't study the future.
That's why we study the past.
And I think that's just wrong.
I think we have a lot of theories that we've abstracted from all the data we've seen and all those theories can be applied to future scenarios and therefore we can study the future via the process I used in this book.
And so I would like to inspire or shame people into studying the future more.
And part of that is to show that you can actually say a lot of detail about a particular scenario if you're just willing to sit down and pull up your sleeves and do the work.
I have a question about you were just saying now you'd like to present the scenarios to people and let them decide for themselves, whether it's good or bad.
Scott Alexander at his blog wrote a review of the book, and one of his intentions was that it didn't seem you were providing the most honest view of what the future would be like.
I'm paraphrasing entirely he did not use these words at all.
But for example, you said that a person could be go on vacation, for example.
And then the next day make a thousand copies of that person so there's a thousand people who are all energized and refreshed from having just been on vacation.
They work really great.
Then the next day one of them is sent on vacation again, and a thousand copies are made of that one person so subjectively it feels like you are constantly going on vacations after one day of work and you're always happy.
But in the outside view, this could be one person who is basically always only in this state of flow.
And perhaps it's been said before that athletes and other people who enter this deep state of workflow that they almost don't feel like they are entirely conscious anymore.
They're just doing their thing and they become the work.
And if the world was composed of those sorts of people, particularly if maybe they were emulated just after having taken some Adderall and gaining that really hyper focus that the drug gives you, that at some point isn't a world that is entirely populated by Adderall taking in the flow people and almost nothing else.
Is that even humanity anymore?
Or is that just work producing algorithms with a little bit of human flavor to them?
Well, obviously the essential question about this different future age is to look at the typical person in that world and ask what you think of their lives.
So it's a perfectly valid question, I think, to point out these, there's some sort of description of a typical person and ask what you think of their lives.
I don't think that's dishonest or anything that Scott has a disagreement there, I think, although I'm not entirely sure, but let's explore it.
In our era, as I described in the book, we have moved from farmer to forager values and one of the main forager values that we love is leisure.
Most of our stories take place in leisure, very few stories actually take place at ordinary jobs.
And we tell ourselves that we don't like work and we tell ourselves that we love leisure and we celebrate leisure anonymously and we are very proud of the fact we've had increasing leisure,
so much so that we are tempted to think of worlds with a lot of work as some sort of hell.
A terrible, horrible thing because people are working instead of being at leisure.
But in fact, most people in our society that we admire the most, the people that we most celebrate, the people we put up for awards and celebration, those people are usually working a lot.
Authors and musicians and politicians and business people, all the people we most celebrate are actually very hardworking people.
And most of the people that listen to this podcast are probably relatively elite people, well-educated, highly high incomes.
Most of you listening to this are people who actually work quite a lot and you get a lot of meaning for your job.
It's not that you think all of the time at work is just a wasted loss relative to, you know, kicking off time until the weekend.
There are some people of that attitude, but not most of you.
Most of you get a lot of meaning out of your jobs.
In particular, Scott Alexander is one of those sorts of people.
He is remarkably very hardworking.
I doubt he spends very much time on leisure at all.
And nevertheless, he finds his life worth living and he's productive.
He's an ideal candidate for being an emulation.
So, you know, people who in academia and elsewhere who actually sit down and study like meaning of work and the value people get out of work, they say people do get a lot of meaning in their lives from work.
Work is important to them.
Work feels valuable and real and humans while they're working are quite human.
They're often the epitome of the sorts of human nature that we celebrate and look up to and point to with pride.
So, I think in fact, even though it's not part of our standard cultural story, we find a lot of meaning for work.
We are proud of our product at work. We are proud of who we are when we are working very well.
So a future world of emulations who are working most of the time and doing a spectacularly high quality job of that, I see them as valuable.
But again, that's just me.
I'm not sure I can be expert in saying that they are valuable.
It's hard to be an expert in what to value, but we can at least talk about these considerations and see where it goes.
So a not uncommon trope in science fiction is that consciousness is cognitively expensive.
For example, aliens that are not conscious tend to be able to outmaneuver humans in anything that requires a lot of anything very competitive.
This has been going on for decades, but it's really become popular over the past five, 10 years.
And I think probably because people are scared of these future scenarios as wouldn't there be a great amount of economic pressure on em workers to strip out anything that makes them inefficient such as need for love or even eventually the point of consciousness itself.
So it's sort of an economy without any humans in it anymore just running for its own purpose.
Well, let me make the analogy to middleman and business.
For a very long time, a wide range of the population has thought that the only real value that happens in industry in business is the line workers making things and the final customer buying them and everything in between is just parasites.
Marketing is a parasite and distribution is a parasite and design is a parasite and management is a parasite.
Everybody else in the process of parasites all that really should be given credit as the guy on the assembly line or the guy digging out of the mind and everything else is something that should go away.
And people have somehow the intuition that if only we could get our economy organized better and we could think it through better, we would eliminate all these parasites and just have the real jobs happening.
So then things would be vastly more efficient because in fact, it's only a tiny fraction of people who work at these supposedly ideal real jobs and most everybody else is doing other things.
That's I think the intuition that comes from not appreciating that, well, a lot of stuff has to happen for products to get made and for it to show up just in the store where you want it such that you have an idea that you want it, etc.
And most of that stuff isn't superfluous parasites, it's real things that need to happen in order to make the whole system work.
I think that same sort of attitude is showing up about brains is somehow people have gotten the impression that surely our brains are just this horrible inefficient mess where the vast majority of stuff happening there must be just useless and spinning of wheels and random throwing of circuits around
because surely that's not how you would organize something that was efficient.
But of course, we've been going for a long time people trying to write software that competes with the human brain, then they are just having a very hard time of it and have for a long time.
It's really hard to make flexible software that's remotely as capable as what human grades are capable of.
So I think you got to give brains credit for saying they're not just a random assemblage of weird, stupid stuff that'll all go away when people get their act together.
It's actually a system with a lot of insight embodied in its design.
There's a lot of useful thing that's not to say no changes will be made in it, but that's also like saying, you know, that I'm not saying that no changes will be made in the way we do distribution or marketing or management in our firms.
But still, the claim is that most firm structure is there for a reason and it's doing useful things. Similarly, most brain structure is there for a reason and doing useful things.
Well, I know that makes me feel better about myself.
Well, no, I mean, it definitely makes me feel better, but I'm not sure. I mean, so the claim seems to be that human consciousness and human input is vitally important.
But when we compare that to things like self driving cars, they seem to I mean, they're not there yet, but I can easily see within the next 10 years, a non conscious system doing the driving much better than any human could.
Hey, isn't that the case with that just about anything could be replaced with a deep learning system eventually, where we have humans right now.
Well, we certainly don't know that. So it's just a very basic open question about the future design of minds. To what extent will minds that are useful in the long run, derive some of their architectural and design features from brains versus from very different completely
irrelevant sources. That's just an open question. I don't know that I have any great expertise to offer in it. But you'll notice, if you haven't noticed already, I'm trying to be very careful to just describe the future as I see it without offering much reassurance that I have no justification for.
So, fundamentally, the future is strange. It's out of control. It's uncertain. And I can't make those go away.
So there are things you can worry about. And that's fine. So I can't tell you not to worry about the possibility that other forms of intelligence or automation will displace and completely supplant human like minds in the long term future.
I just think it's not obvious that human like minds lose out as a source of descendants because we are in fact at the moment hands down the winners in terms of being useful.
If there's anything that you would want people to be able to take away from age of M after reading it. What's the one thing.
Well, there's several things I guess I have to pick one. Certainly it's possible to see the future in more detail than you have. So when you've looked at science fiction stories and things like that, people have told you that's the best we can do and it's not.
We can do a lot better than that if you want to be serious about it. Now, maybe you don't want to be serious. Maybe you just like stories fine. Then this isn't for you. But if you want to be serious about thinking through what could happen, it's possible to sit down and do it.
It's especially looking at the social implications. The other big thing I should tell you is that our era is really quite different from the past. If you understood the past better you'd realize just how different our era is from the farming era, the foraging
eras and within each of those eras there's a lot of variety. And it's not just that we have progressed. Progress is a little too simple a summary of history. It's also that each of our eras has been different because we have adapted to differences that our eras present.
So it's not just like everything that we do, all of our ancestors should have done too because it's all just the better way of doing things. The things we do are better for our world but the things they did were often better for their world and if we went back to their world we would have to pick up their ways.
The future you should expect to be something like that. That is it's a new world, a new set of conditions and places and they will inherit some of the innovations that you have developed and bring those on but they will also change ways they do things compared to your world and some of those ways will reject things that you've gotten comfortable with and you were hoping they would continue.
And some of their ways will just seem strange. And the more you've told yourself a story of moral progress where we're all up and up and getting better and better and doing more and more right the more you will be disappointed or confused by how they don't necessarily buy into your story of moral progress and don't necessarily do things the way you think they should because they're in a different world.
I haven't been talking as much as I usually do but I think my co-hosts have been doing a great job of asking all the great questions. I found myself agreeing very loudly in my head that sounds perfectly reasonable and that makes a lot of sense.
So thanks for coming on and saying everything but I did want to say that I had a couple people ask me to ask you about the review of Age of M on Slate Star Codex and I know that Scott Alexander turned down your offer to debate formally his disagreements but were there any points of contention that you haven't had a chance to voice or would like to get out?
Well I'm kind of stuck in part. I'm just grateful that he wrote the review so it's very nice that he did it and it's helpful to get lots of people to read so I don't want to see ungrateful to him.
There's also just a general phenomena that I've noticed that I've even seen other people say explicitly that authors who take reviews and give a very detailed point by by rebuttal just look low status and nice people when they do that.
It's gracious more to nod and accept a review even if you don't accept all of its points but the mere fact that it's a well written and mostly friendly and mostly positive review is something you should be grateful for which I am.
But it's a very long review. It's extremely long compared to what was in because that's the way he writes and so obviously there are great many detailed points on which I would disagree if I were to go into a point by by rebuttal.
My very first response to him was basically to pick one point and disagree because I thought that was not being too rude.
And so I just picked the point of whether or not a life of someone who's working is a life of low value because I said well he's the sort of person whose life of working in his life is valuable.
And then later on I did a commentary on the idea of rapid progress toward other forms of artificial intelligence in the M world so I he seemed to think that that was likely and I've heard that from other people so I made a post where I talk about why I don't think.
It's especially likely and I just outlined some of those points in this block in this podcast already.
But there are many other points I could respond to but again, mostly I'm just grateful that he wrote a nice mostly positive review and that induced lots of other people to think about and read my book.
So what's next for you? Are you working on follow up books or projects? What's going on?
I have a second book that's already completed that's under review at Oxford. It's on a completely different subject.
What is the subject?
On a hypocrisy or something more Hensonian you might think. So the subtitle of the book is Hidden Motives in Everyday Life.
So it's about all the different ways that we're wrong about why we do things.
Standardly stated as X.
X is not about why sort of things.
And so, you know, I try to go over many different areas of that.
I also just started a grant a few months ago from the Open Plan Theory Foundation to analyze a different future scenario in the same spirit that I did the M scenario.
And it's in a sense the more and even more conservative scenario that probably should have been done first.
But that scenario is to say, assume that we eventually have human level artificial intelligence via accumulating better software in the same sort of way we've been doing it for 70 years, i.e. no revolution.
So if there's no revolution then all the patterns we've seen in software for 70 years we can extend those patterns into the future.
And we know a lot about software.
And so I'm doing a literature review to start.
And so the idea is just to collect everything we can say about what does the world of software look like as it gets bigger.
And how does that, the world changes, that world takes over the world basically.
But the sort of world of software that we've seen.
This isn't to say this has to be the truth, but I think a lot of people implicitly assume there's some revolution coming.
And they aren't very clear about what that revolution is, it's just something that's revolutionary.
So the more language and terms I can give and the more I can fill out the details of this default reference scenario of what would happen if there were no revolution.
The more I can turn and say, so you think there's going to be a revolution, can you point somewhere in this detailed structure to where you think the revolution is.
So that they can be pushed to say more precisely what their hypothesis is.
And then we can start to evaluate those alternative hypotheses, because again, we should evaluate lots of different future scenarios.
Sounds fascinating.
Is there anything else that you'd like to promote before we say goodbye?
I say thank you very much for hosting this podcast.
I'm grading to talk to people and I hope people will be inspired to read the book and further to continue this sort of research.
That is the main reason I'm doing a strange new kind of research is in the hope that I could inspire other people to follow up.
So if lots of people read and say that was an interesting book, but nobody really tries to follow up with that style, it'll be somewhat of a failure.
All right, you heard it, the called action.
Thank you so much for talking with us today, Robin.
Take care.
I imagine that if it's half as compelling as you are in the last hour, that I'll be inspired to focus on learning about the future as well or thinking about the future more intelligently.
I can hope with you.
Have a good one. Bye.
Welcome back.
Oh, sorry.
Welcome back.
We just dropped over.
We're starting over.
Oh my God.
Okay, now for the feedback.
By the way, all this feedback this time is coming off of our subreddit, Bayesian conspiracy subreddit on Reddit.
So go check that out.
That's the best place to comment, obviously.
First, we have a comment from Richard J. Acton regarding our thermostats comment on animals again.
Even the simplest single celled organisms are orders of magnitude more complex as stimulus response machines.
I did some work on a predatory bacterium that hunts other bacteria of specific types, detecting their chemical trails and swimming after them in order to get under their outer coating and eat them diving in their husks.
They have a really very complex life cycle involving integration of multiple complex stimuli into decision-making processes about how they should behave and what genes they should express.
So on the grounds of complexity of its decision-making powers, I would say that thermostats rank well below bacteria, let alone slugs and planarians.
For some animals, the behavioral complexity of their cells is arguably much more complex than the behavior of the organisms as a whole.
Simulating a sponge is probably easier than simulating a sponge cell. They do have very interesting cells.
On what level, then, should we weigh their complexity?
Could a sponge cell have more moral weight than a sponge taken as a whole?
I'm inclined towards a position that human and animal-slash-other cognition differ in degree and not kind, but it seems increasingly likely to me that there are some threshold effects, possibly relating to self-awareness, something notoriously difficult to measure.
What an excellent comment! Thank you!
Should I start?
Should I start? Okay.
Well, I would like to say that this is actually exactly the reasons that I go with intellectual complexity rather than behavioral complexity, because I don't think anyone would argue that a bacteria has any sort of experience, like knowing what it is to be itself.
It can feel pain and there is no reason not to take antibiotics regardless of how behaviorally complex the bacteria are.
I mean, obviously, this is fucking cool. I was enthralled when I was reading that, and I want to know more about this bacterium now, but that doesn't mean that it has any necessarily individual moral weight, in my opinion.
It's like AlphaGo, the episode we've done that, is incredibly complex, and I don't think AlphaGo is necessarily a moral being either that we need to worry about.
It's also not as complex as that bacteria.
Can you say that?
Because AlphaGo is pretty complex.
Let's talk to Richard again.
Okay. Patrick?
Richard, the person who studied the bacteria.
We know exactly how AlphaGo works.
Well, we don't know exactly how AlphaGo works. We know the broad strokes of how it works.
People know how AlphaGo works, so yeah.
This ties into something that I wanted to just mention, that I was talking to somebody about our animals' episodes, and it didn't really become clear to me until after we finished recording those, that we weren't necessarily tying up the position that things are complex and things matter morally, even though they kept getting conflated.
Those were two distinct arguments.
Yeah, stop it, Anyash.
So, sorry, it was actually almost completely you who kept saying, look at how much more complicated things are than we thought they were.
And I was like, yeah, but that doesn't make them, like, complexity doesn't tie up with, like, moral stuff, but you weren't saying that it was, but the whole time I thought that we were talking about something that we weren't talking about.
So that said, I have revised my opinions a bit on that, and it was, it just took me clearing up what we were actually talking about to do so.
Our question-answer, our question-asker person, Edgar, I believe, was saying that we couldn't know what was inside the minds of an animal.
We couldn't know what something was thinking, what was going on in there, and my argument was, well, to some extent we can.
People are making great strides on that.
But we couldn't know 100% what a bacteria is thinking, because it's not thinking anything.
Zing? No, but that said, I think that that point that you just made was much clearer to our listeners than it was to me during the conversation.
At least it was to the listener that I talked to about this.
But as far as physical processes being tied up, this kind of comes up again with the complex parasitic bacteria.
Atomic fusion in stars is really cool, and it's complex, and there's lots of big moving parts and small moving parts, and they interact in really interesting ways.
But there's nothing moral happening there. It's just happening.
And I would kind of put bacteria closer to helium atoms than I would to dogs.
And if we want to go even smaller, we don't even know how protein folding works.
It is complex enough that we cannot simulate it, and we've been trying for quite a long time to figure out this protein folding problem, but proteins aren't things that are intrinsically valuable either.
They're just kind of cool.
Proteins, of course, are incredibly valuable to us, and we, as fuel, that's what we're built on.
Well, fuel and materials, yes.
Proteins are incredibly important.
Proteins without proteins is not a world I could exist in.
Wow.
I mean, I think proteins do a lot more than you realize, but, sorry, got off topic here for a moment.
We're the only ones who are, who we know who are assigning moral value to things, right?
Now, it's very possible that, in fact, it's probable that other animals are doing that too.
But it's tough for us because we know it's subjective.
It's not something that exists in the world.
A star can't have moral value because we're not there going, this has moral value for this reason.
We can do it in the same way that we can for proteins and that if stars weren't doing their thing, we wouldn't exist, right?
But I think there's a huge, giant gap filled with white fire between complexity and how interesting and cool it is and things that morally matter.
Morality is weird. I think that we should just let it go anyway.
And eat each other in the streets, right?
Eat each other in the streets.
I'm joking about the, I'm straw, very much straw manning the...
Do you guys know about prions?
Yeah.
I guess they're also called prions.
Prions anyway.
So those are pretty neat because those are pathogenic proteins that, by their folding, cause things like mad cow disease and chronic wasting disease, which is mad cow disease for elk.
Huh.
Now you know Coloradoans.
So don't eat elk if you're in Colorado?
Chronic wasting disease is a problem for elk.
Can humans get it?
I don't know.
Cause I know we could get the mad cow disease from eating the cows.
It's true. And it is very, very similar, so possibly.
I know it's being researched in Colorado on the western slope.
Anywho.
Metal soft don't eat elk, got it.
Metal soft don't eat cows, don't eat elk.
Don't eat cow brains anyway.
Just, I was going to eat a cow brain this morning, but then I remembered not to.
So, if a sponge cell is more complex than a sponge, would you put more weight on individual sponge cells than a sponge as a whole?
Well, I mean, the sponge is also, is also, the whole sponge is its cells too, right?
So isn't that an additive effect?
I don't know.
You can make.
We can kind of, this is a reductionism problem, right?
At what level of stuff do we care about?
So like, when you mentioned how cool and important proteins are for, say, stuff made of squishy things to exist, sure.
But like, if we're running in a simulation, it doesn't necessarily, it's not obvious to me that we need to simulate everything down to the protein level, right?
We can simulate things at the surface level where, you know, in middle universe size, right?
Because there'd be no point in simulating quarks if you're, if you're concerned about computing power.
So, but I think that you could, you could simulate people in a simulation, given our conversation with Roman Hansen, who would still have moral weight, even if they didn't have proteins.
So like, they would be simpler of makeup.
No, you're right.
But that's a devastating rebuttal.
But no, I think that they're still really cool and they can be appreciated for how awesome they are.
But I don't, I don't know that you could simulate somebody without simulating their proteins, which are so deeply involved in communication between cells and all cellular processes and communication between cells.
That's also neural impulses and all of that is, is linked to basic things like proteins and cell membranes and stuff like that.
I think you could simulate a world that didn't even go down to the cellular level, right?
But we'll still have thinking people in it because the, the, the part that you're really interested in simulating is the thinking part.
But I might be blowing smoke.
Oh, that's a good question.
So I guess I, in that episode, I talked a little bit about the different levels, right?
And how sometimes those can be at odds with one another.
Coenah flagellates, I want to say is the name of the ancestor of sponge cells.
And they're very independent.
You can put a sponge through a sieve and the cells will, will be fine and they can reform into a sponge.
Oh, that's kind of cool.
Yes.
So it's more like a corporation.
There's an analogous thing to that kind of might work as an intuition pump.
If you're feeling particularly reflective while watching Rick and Morty during the episode M night Shyamalanma simulation or something.
I forgot what it's called.
Where, where Rick's in a simulation and he's taking apart a rat in the garage.
He's like, this is just shot the craft craftsmanship.
And I think he's saying, he's looking at it.
He's like, this isn't what a real rat looks like on the inside.
This is just bullshit.
And so I think that he might be doing that by saying, look, they're not even simulating all this stuff that goes on the inside of rat.
I mean, what a rat looks like on the outside.
I mean, I, so that was, that was on my mind recently.
So I was kind of thinking about that as far as the level of a simulation you need to do to, you know, have things function.
Those are my two cents.
There is a difference between complexity and moral importance.
And it's also an interesting question as to like, you could probably simulate someone so that they would be indistinguishable from their regular self to an outside observer.
Absolutely.
But does that person, if it is a not very deep simulation, do they have any self awareness?
Any, is there any consciousness there?
Or is it just a very good simulation?
Are you talking about philosophical zombies?
If you're using the C word, we're going to have to put this on hold for a while.
The C word?
Consciousness.
Yeah.
I guess it would be a sort of philosophical zombie.
So on that note, let's change to a different commenter.
Okay.
I am up. Okay.
Was I doing honesty for the birds or doing TVC?
Okay.
TVC grid says, it was interesting hearing about arguments in this overall animal segment that continuing to allow the carnival of horrors of life in the wild is not advisable, though nonexistence where you could have existed should count for some extra high negative utility.
Why not mass genetic engineering to essentially eliminate extinction?
Re-engineer food needs to survive off of more efficient energy, such as solar, question mark, and maybe extravagantly hack at the experienced qualia of all beings above a generous minimum bar so that it's significantly less horrible, etc.
My first response to that is that before we get into the mass genetic engineering thing, I don't think that not having existed deserves a higher level of negative utility.
I think that stopping to exist once you have existed is a very high level of negative utility, but never coming into existence in the first place should not be counted as negative utility because then you suddenly have a moral obligation to create as many beings as possible, which is kind of a problem, in my opinion.
That's the next question.
Yeah, that's a great segue to the next question, but if we want to talk about that a bit more, it's a complex question.
You don't know how happy wild animals are.
Well, as Robin said in the podcast, generally they are living at subsistence levels, just barely getting by and often dying of parasites or being hunted.
But Robin's argument was also that they're happy doing that, which is why his argument was that M's would be happy doing that.
Comedian Louise T.K. has this bit where it's only us that our lives don't end by either starving to death or dying.
But I mean, things can be good up until either of those ends, right?
I suppose I personally do not think that that is that great of a thing to be barely eking out existence and then dying in pain.
I'm not a huge fan of that stuff.
But I guess I am imposing my own theories on the animal.
I mean, I don't know.
During the talk with Robin Hansen, I was thinking about my medieval history course back in undergrad and how all of those peasants, they were writing songs about a heaven where you didn't have to work.
And the river of beer and hot people to have sex with all the time.
But not working is one of the things.
Just enjoying yourself at all times.
And that sounds great until you've been on vacation for a month and you're like, I'm going insane, I need to do something.
I'll stack things just pointlessly, something to do, right?
Whatever.
He hits that threshold after like day, he can't handle three day weekend.
So he goes nuts on on vacations.
I like vacations as long as I have some structure things to do.
If I were just left on my own, I would not be not very happy at a vacation.
Take me on a vacation.
We'll go scuba diving and see some awesome animals and it'll be so much fun.
That would be awesome.
You have to pay for it there.
I'm tagging along.
You see even god damn it.
Invite all your family to while we're at it.
Yes, we have such an amazing time.
She's like, I'm ruined.
I'm going to start having to collect the vacation tax from all you guys.
Think of all the books you can read.
I can't read very many on vacation.
I don't read very much.
What do you mean very fast?
This is going to be a long vacation.
I don't know.
I don't know if I gave TC grid TV TVC grids question the time that it deserves.
What about the second half?
I mean, can so the second half is more or less can why not massive genetic engineering or why not massive genetic engineering to essentially eliminate extinction.
By storing all that information and.
I mean, I mean, I think that we probably should store the genetic epigenetic.
As much information, you know, ecological as much information as we can about all sorts of different animals before they're gone forever.
And so that they can potentially be brought back.
It's just they're so complex.
Yeah, it's not a replacement for having them around.
Yeah.
Well, not a replacement for us, not a replacement for them.
And also, as I was, and I think saying off air, there's all the amazing network of interactions both within the body and outside the body of an individual animal.
You can't reproduce or so difficult because it's it depends on the individual.
Right.
And when he said like re engineering them to be on more efficient sources of energy, solar is actually not that efficient of a source of energy.
It's the I remember reading a XKCV does a what weekly what if and one of them was what if we made our cows green and fill with chlorophyll so they could run off the sun.
And as it turns out, you could not produce enough energy from the surface area of a cow to run and then something the mass of a cow.
You would need to make the cow very thin and stretch it out very over a large surface area to collect enough solar power.
And he said the surface area would need is approximately about as big as the field of grass needed to feed the cow.
So a cow basically is running off solar energy.
And that's no coincidence, right?
Yeah, exactly.
Animals already are running off solar energy, it turns out.
And it turns out that if you want a more efficient form of fuel, eating things like plants is more efficient than solar and eating things like the flesh of animals is more efficient, more calorie dense than plants.
So you'd be probably going the wrong way if you're trying to go for more efficient fuel.
That's just the one thing of saving all the genetic information and as much as much as much as much genetic information as we can just in case it goes extinct so they're not gone forever.
That sounds like a good idea.
Let's start doing that.
It's also a bit of a question like do you still have the same animal if you're hacking it?
Like if you change a leopard so that it doesn't eat its prey anymore and instead it eats plants, you have to change its stomach so they can process plants.
You've got to change its teeth so that it choose the plants and its habits so that it eats the plants and pretty soon you'll have a deer instead of a leopard.
Or you could just...
You just don't have a leopard anymore if you change all those things.
Or you could just introduce it to Soylent.
I literally just meant the first sentence of saving all that information somewhere, just in case.
Yeah, as far as re-engineering them, that sounds like a different order of problem.
And like you said, you'd have to change just about everything about a leopard that makes it leopard-y to make it be able to eat plants.
I think a possible good solution would be if an animal is going through pain and about to die, cut off all its pain receptors and let it die quickly.
If there was some genetic trait that could do that.
That's actually on my short list of things that should be introduced when we get super-intelligence-driven data tech.
Pain management?
Like how cool would it be if like the second that a leopard sinks its teeth into the gazelle, the gazelle just gets rushed into orphans and it doesn't feel anything as it's getting eaten.
Like that's kind of a win for everybody, right?
But then it won't get away.
Let's say there were programs falling off and it's a fatal blow.
Here's the thing about pain.
It develops because it's incredibly useful.
We all feel pain because it helps us actually survive and live another day where we can crawl through the fields of grass being a gazelle.
So we need some sort of monitor as in the culture series of books that can tell whether this is a lethal wound or not.
And as soon as it's lethal, then snap on the endorphin.
Science fiction sounds like it beat me to it, but that sounds perfect.
It would have to be basically a machine god looking over the world.
That sounds doable.
That's totally plausible, yeah.
It's not impossible, but this is what the world would look like if a happy god, if a benevolent god actually existed.
Right. And that you could even imagine a world where pain didn't hurt so much, but you had like this kind of red flag, like a siren going off in your head that didn't like hurt forever, but it just was.
Yeah, but the fact that it hurts a lot is a very strong motivator.
I feel like there's another way to go about doing that.
That didn't have to hurt so damn much, right?
It should be something that hurts.
It should be something that hurts that much initially, but then stops.
I mean, what's the point of continuing to have this deep ache through your leg weeks after you got mauled and it's still healing up?
It's so that you don't put pressure on it.
Always with your answers.
Thanks, Katrina, for being right about things.
That's actually a good answer.
I was going to say the same thing about like getting a burn or something, but yeah, there's another knockdown point.
Next time I want to know the right answer for something, I'll just ask you.
I'm so flattered.
So we did get a lot of good questions, but we did a long episode of the Robin and there's too many to get to.
So we kind of picked one each and honesty for the birds asked us.
Honesty is for the birds.
What did I say?
You said honesty for the birds, which I have to say that's very admirable.
Yeah.
My bad.
I'm asking once we can run a complete simulation of a worm, is it okay to kill off these roundworms?
Adding a very happy roundworm increases the total and average happiness of the planet.
If the cost of running simulation is negligible, should we try to run as many or distinct and or maybe even identical instances of it as possible?
So let's take this in two parts.
Okay.
First off, would it be bad to kill simulated roundworms?
In my opinion.
How happy are they?
Random, not a happy one.
Just any random simulated roundworms.
It's about the same as an average earth roundworm.
So an earth roundworm.
Are you talking about earthworms?
Those are analysts.
Yeah, thank you.
Understood.
I won't go into that.
So in my opinion, it would be if it was a, you know, a sufficiently detailed simulation to the point that they're basically indistinguishable.
Then yes, I would consider them of equal moral worth and stepping on one is just as bad as turning off the other, which is to say my opinion, not.
But that's just my opinion.
I was going to say pretty much exactly the same thing that turning off a simulation of a happy roundworm would be just as bad as stepping on a happy roundworm.
Living aside how bad that is, I think that would be equivalent.
But okay.
So we just talked again to Robin Hansen who just wrote a book in which he predicted that in an emulated intelligence economy, you would wake up in the morning and create a thousand copies of yourself, have them go do different tasks and then delete all of them except for one.
With consent is the key thing there though.
Yeah, if you're okay with it, then it's okay.
I see.
Learns can't give consent.
Right.
Okay.
That is a good reason.
I think I'm going to take that out.
No, no, no. Keep it in.
It's really interesting.
It's the best reason I think.
That is the best thing that you could have said at that moment.
All right.
The second part about...
Katrina never answered.
Oh yes.
What what?
Is it okay to kill off a simulated roundworms?
It's as okay as it is to kill off actual roundworms.
We are in agreement.
We are in agreement.
Let it be known.
We have to creed.
So I can't really see how there'd be any opposition to that, right?
As long as you're saying it's a perfect simulation.
Right.
Like it's the, then for all intents and purposes, it's the same as a meat space one.
I mean, there could be the argument that a meat space worm is doing useful work in the environment.
Oh, sure.
So it would be more, yeah, it would be more useful to us.
That is a good argument.
Yeah.
Thanks for bringing that up.
Sure.
But aside from the use to us just on its own merits, I think that's the same.
Fair enough.
That works.
And that is a good point.
So yeah, the other question, the other additional question was about maximizing the number of
happy flatworms you could simulate.
And you could even do this to like people or something too.
But it brings in the, the question of, or brings in the point that yes, that would raise
the total and average happiness of the world if you could just add more.
Well, I guess it depends on how happy worms could be versus happy people could be or something.
Well, if you, I mean, let's assume that we're, let's assume that we're making it very happy.
Right.
But even as like we made it as happy as it could be, and it turns out flatworms when
maximally happy are 1% as happy as people can possibly be.
So you could, you could raise, you could make a trillion of them and the average happiness
of the world would go down.
Okay.
That's different than the question.
So that's a, so that's an iteration of the question, right?
Yeah.
If you could make, if you could make roundworms that were 1% as happy as it's possible for
human to be, and they were all that happy, should you just emulate a ton of them?
Well, I was saying flatworms the whole time roundworms.
Sorry, wrong shape.
There's, there's actually a really long.
I can't answer that question now.
There's a really long and interesting implication and whole discussion on that exact question,
which I don't want to get into because it's its own podcast episode, but I will link to
the stanford.edu's entry on the repugnant conclusion, which is from Derek Parfitt's
writing about some of the implications of utilitarianism, which basically it amounts
to, and this is, this will just get you to read it because I'm not going to explain how
it gets here, but the, the conclusion is essentially for any population of possible people, excuse
me, for any possible population of at least 10 billion people all with very high quality
of life, there must be some much larger imaginable population whose existence, if other things
being equal would be better, even though its members have lives that are barely worth living.
So it turns out that following that, that line of reasoning gets you there, kind of kicking
and screaming, but it's hard to avoid.
So there's, there's not a great way around the pug, the repugnant conclusion yet.
Well, let's, let's, um, let's, let's live in the least convenient world for your argument,
though, where instead of this leading to the repugnant conclusion of with us throwing all
our resources at maximizing the number of happy fl- uh, roundworms there are, we just say
we will forever dedicate this, uh, computer, large supercomputer, let's say it costs a
million dollars, but that's nothing for our society in general, this million dollars
supercomputer to, uh, simulate as many possible flatworms, extremely happy flatworms as it
can.
Say it's 40 quadrillion happy, uh, flatworms.
Is, does that's something we have a moral duty to do?
I think you should probably do it just in case.
Really?
Yeah.
Oh.
And then you can update it as you continue to, um, be able to stimulate more different
kinds of animals that are more happy.
That would push us into a lot of weird obligations, though.
Yeah.
Uh, it would also put you at risk for looking at all these lives that depend on the power
not going out, you know, one flick of electricity and you've just committed a genocide of 40
quadrillion happy worms, right?
No, cause then you can boot them back up and they'll still be happy worms.
There's a point.
Yeah.
All right.
That is, that's something to chew on.
In my opinion, in my opinion, there is no moral obligation to beings that don't exist.
And so these future beings that wouldn't come into existence unless we made them, we don't
have any obligation to make them.
Uh, so what about AI and.
Or like the future.
I don't think we have any, any obligation to make them come into existence.
I think we probably will because we think it's fucking awesome, but I don't think there's any
moral obligation to create an AI.
Does that, does that.
What happens?
Like not taking care of the earth cause two generations from now is new people and who
cares what the world's like in 60 years?
Cause they don't exist yet.
Oh.
Um.
I want to say no, but I don't have a good reason for that based on what I just said one
time.
I mean, because these are people, I'm saying that you don't have an obligation to bring
people into existence, but since we know two generations from now, these people will exist.
They, we shouldn't make things absolutely shit.
Yeah.
That is the distinction I was looking for.
We don't have that.
We don't have an obligation necessarily to maximize the number of potential persons.
Right.
But given that they're going to exist, let's not fuck things up for them.
And that's why any, she doesn't get to have any kids.
I actually plan on having no kids.
Yeah.
But, but it's not because I want the human race to go extinct.
No, I know.
It's because you don't, you don't have an obligation to create people.
Right.
So that they can be future people who are happy and frolic in fields being gazelles sometimes.
I would donate sperm to that effect, but I don't want to have kids cause I think it
would make me less happy.
Ah.
So.
That's just as good of a reason.
Yeah.
I mean it's selfish, but you know.
I don't, I don't, if you, if having kids would make you less happy, I don't think you
should have kids.
Yeah.
It would be,
Well, obviously, yeah.
Because that's my position.
Right.
Well, I mean, it would be a dream, not just on you, but on the kids as well.
Cause you would present them.
Right.
I mean, I think, I, according to parents.
I think they have very happy lives.
I think, I think parents have this kind of weird compartmentalization where they are
able to, while having a lower than average genetic index for like the duration of parenthood,
I think that they still find that stuff super rewarding.
Right.
So like, you're not as productive doing the stuff that you love, but you're finding so
much joy out of pushing a two year old in the swing that like, that makes it all worth
it.
And that's not, I'm not shooting on that position.
That's just, I think, from what's been described to me, how, what it's actually like.
So
We'll talk about this another time.
There's some rewiring done in there.
And that is a big topic.
So we'll save that one.
Yeah.
I will say that even though I'm not having kids, I'm glad other people are and have
in the past because I really liked the human race and I wanted to continue.
And that's how you came about.
Yeah.
Exactly.
I'm glad that I exist too.
Yeah.
And honesty is for the birds.
You need to check out Derek Parfit's pre-partner conclusion.
It's exactly where this is going.
If you haven't already.
If you haven't already.
Maybe you had, that's why you gave us such a perfect question for it.
But in any case, everyone else who isn't familiar with it, it's a very, very interesting thought
experiment.
Or not that experiment, whatever.
It's a different, it's a, it's an interesting exploration of philosophy.
I didn't want to misuse the word thought experiment.
Okay.
All right.
That's fair.
Go to the subreddit.
That's where all the conversations are taking place.
Send us an email instead.
Send us an email.
We'll talk it out.
Okay.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Goodbye.
Bye.
Bye, everybody.
Sorry.
