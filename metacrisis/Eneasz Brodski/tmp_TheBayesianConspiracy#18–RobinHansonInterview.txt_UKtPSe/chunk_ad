Well, no, I mean, it definitely makes me feel better, but I'm not sure. I mean, so the claim seems to be that human consciousness and human input is vitally important.
But when we compare that to things like self driving cars, they seem to I mean, they're not there yet, but I can easily see within the next 10 years, a non conscious system doing the driving much better than any human could.
Hey, isn't that the case with that just about anything could be replaced with a deep learning system eventually, where we have humans right now.
Well, we certainly don't know that. So it's just a very basic open question about the future design of minds. To what extent will minds that are useful in the long run, derive some of their architectural and design features from brains versus from very different completely
irrelevant sources. That's just an open question. I don't know that I have any great expertise to offer in it. But you'll notice, if you haven't noticed already, I'm trying to be very careful to just describe the future as I see it without offering much reassurance that I have no justification for.
So, fundamentally, the future is strange. It's out of control. It's uncertain. And I can't make those go away.
So there are things you can worry about. And that's fine. So I can't tell you not to worry about the possibility that other forms of intelligence or automation will displace and completely supplant human like minds in the long term future.
I just think it's not obvious that human like minds lose out as a source of descendants because we are in fact at the moment hands down the winners in terms of being useful.
If there's anything that you would want people to be able to take away from age of M after reading it. What's the one thing.
Well, there's several things I guess I have to pick one. Certainly it's possible to see the future in more detail than you have. So when you've looked at science fiction stories and things like that, people have told you that's the best we can do and it's not.
We can do a lot better than that if you want to be serious about it. Now, maybe you don't want to be serious. Maybe you just like stories fine. Then this isn't for you. But if you want to be serious about thinking through what could happen, it's possible to sit down and do it.
It's especially looking at the social implications. The other big thing I should tell you is that our era is really quite different from the past. If you understood the past better you'd realize just how different our era is from the farming era, the foraging
eras and within each of those eras there's a lot of variety. And it's not just that we have progressed. Progress is a little too simple a summary of history. It's also that each of our eras has been different because we have adapted to differences that our eras present.
So it's not just like everything that we do, all of our ancestors should have done too because it's all just the better way of doing things. The things we do are better for our world but the things they did were often better for their world and if we went back to their world we would have to pick up their ways.
The future you should expect to be something like that. That is it's a new world, a new set of conditions and places and they will inherit some of the innovations that you have developed and bring those on but they will also change ways they do things compared to your world and some of those ways will reject things that you've gotten comfortable with and you were hoping they would continue.
And some of their ways will just seem strange. And the more you've told yourself a story of moral progress where we're all up and up and getting better and better and doing more and more right the more you will be disappointed or confused by how they don't necessarily buy into your story of moral progress and don't necessarily do things the way you think they should because they're in a different world.
I haven't been talking as much as I usually do but I think my co-hosts have been doing a great job of asking all the great questions. I found myself agreeing very loudly in my head that sounds perfectly reasonable and that makes a lot of sense.
So thanks for coming on and saying everything but I did want to say that I had a couple people ask me to ask you about the review of Age of M on Slate Star Codex and I know that Scott Alexander turned down your offer to debate formally his disagreements but were there any points of contention that you haven't had a chance to voice or would like to get out?
Well I'm kind of stuck in part. I'm just grateful that he wrote the review so it's very nice that he did it and it's helpful to get lots of people to read so I don't want to see ungrateful to him.
There's also just a general phenomena that I've noticed that I've even seen other people say explicitly that authors who take reviews and give a very detailed point by by rebuttal just look low status and nice people when they do that.
It's gracious more to nod and accept a review even if you don't accept all of its points but the mere fact that it's a well written and mostly friendly and mostly positive review is something you should be grateful for which I am.
But it's a very long review. It's extremely long compared to what was in because that's the way he writes and so obviously there are great many detailed points on which I would disagree if I were to go into a point by by rebuttal.
My very first response to him was basically to pick one point and disagree because I thought that was not being too rude.
And so I just picked the point of whether or not a life of someone who's working is a life of low value because I said well he's the sort of person whose life of working in his life is valuable.
And then later on I did a commentary on the idea of rapid progress toward other forms of artificial intelligence in the M world so I he seemed to think that that was likely and I've heard that from other people so I made a post where I talk about why I don't think.
It's especially likely and I just outlined some of those points in this block in this podcast already.
But there are many other points I could respond to but again, mostly I'm just grateful that he wrote a nice mostly positive review and that induced lots of other people to think about and read my book.
So what's next for you? Are you working on follow up books or projects? What's going on?
I have a second book that's already completed that's under review at Oxford. It's on a completely different subject.
What is the subject?
On a hypocrisy or something more Hensonian you might think. So the subtitle of the book is Hidden Motives in Everyday Life.
So it's about all the different ways that we're wrong about why we do things.
Standardly stated as X.
X is not about why sort of things.
And so, you know, I try to go over many different areas of that.
I also just started a grant a few months ago from the Open Plan Theory Foundation to analyze a different future scenario in the same spirit that I did the M scenario.
And it's in a sense the more and even more conservative scenario that probably should have been done first.
But that scenario is to say, assume that we eventually have human level artificial intelligence via accumulating better software in the same sort of way we've been doing it for 70 years, i.e. no revolution.
So if there's no revolution then all the patterns we've seen in software for 70 years we can extend those patterns into the future.
And we know a lot about software.
And so I'm doing a literature review to start.
And so the idea is just to collect everything we can say about what does the world of software look like as it gets bigger.
And how does that, the world changes, that world takes over the world basically.
But the sort of world of software that we've seen.
This isn't to say this has to be the truth, but I think a lot of people implicitly assume there's some revolution coming.
And they aren't very clear about what that revolution is, it's just something that's revolutionary.
So the more language and terms I can give and the more I can fill out the details of this default reference scenario of what would happen if there were no revolution.
The more I can turn and say, so you think there's going to be a revolution, can you point somewhere in this detailed structure to where you think the revolution is.
So that they can be pushed to say more precisely what their hypothesis is.
And then we can start to evaluate those alternative hypotheses, because again, we should evaluate lots of different future scenarios.
Sounds fascinating.
Is there anything else that you'd like to promote before we say goodbye?
I say thank you very much for hosting this podcast.
I'm grading to talk to people and I hope people will be inspired to read the book and further to continue this sort of research.
That is the main reason I'm doing a strange new kind of research is in the hope that I could inspire other people to follow up.
So if lots of people read and say that was an interesting book, but nobody really tries to follow up with that style, it'll be somewhat of a failure.
All right, you heard it, the called action.
Thank you so much for talking with us today, Robin.
Take care.
I imagine that if it's half as compelling as you are in the last hour, that I'll be inspired to focus on learning about the future as well or thinking about the future more intelligently.
I can hope with you.
Have a good one. Bye.
Welcome back.
Oh, sorry.
Welcome back.
We just dropped over.
We're starting over.
Oh my God.
Okay, now for the feedback.
By the way, all this feedback this time is coming off of our subreddit, Bayesian conspiracy subreddit on Reddit.
So go check that out.
That's the best place to comment, obviously.
First, we have a comment from Richard J. Acton regarding our thermostats comment on animals again.
Even the simplest single celled organisms are orders of magnitude more complex as stimulus response machines.
I did some work on a predatory bacterium that hunts other bacteria of specific types, detecting their chemical trails and swimming after them in order to get under their outer coating and eat them diving in their husks.
They have a really very complex life cycle involving integration of multiple complex stimuli into decision-making processes about how they should behave and what genes they should express.
So on the grounds of complexity of its decision-making powers, I would say that thermostats rank well below bacteria, let alone slugs and planarians.
For some animals, the behavioral complexity of their cells is arguably much more complex than the behavior of the organisms as a whole.
Simulating a sponge is probably easier than simulating a sponge cell. They do have very interesting cells.
On what level, then, should we weigh their complexity?
Could a sponge cell have more moral weight than a sponge taken as a whole?
I'm inclined towards a position that human and animal-slash-other cognition differ in degree and not kind, but it seems increasingly likely to me that there are some threshold effects, possibly relating to self-awareness, something notoriously difficult to measure.
What an excellent comment! Thank you!
Should I start?
Should I start? Okay.
Well, I would like to say that this is actually exactly the reasons that I go with intellectual complexity rather than behavioral complexity, because I don't think anyone would argue that a bacteria has any sort of experience, like knowing what it is to be itself.
It can feel pain and there is no reason not to take antibiotics regardless of how behaviorally complex the bacteria are.
I mean, obviously, this is fucking cool. I was enthralled when I was reading that, and I want to know more about this bacterium now, but that doesn't mean that it has any necessarily individual moral weight, in my opinion.
It's like AlphaGo, the episode we've done that, is incredibly complex, and I don't think AlphaGo is necessarily a moral being either that we need to worry about.
It's also not as complex as that bacteria.
Can you say that?
Because AlphaGo is pretty complex.
Let's talk to Richard again.
Okay. Patrick?
Richard, the person who studied the bacteria.
We know exactly how AlphaGo works.
Well, we don't know exactly how AlphaGo works. We know the broad strokes of how it works.
People know how AlphaGo works, so yeah.
This ties into something that I wanted to just mention, that I was talking to somebody about our animals' episodes, and it didn't really become clear to me until after we finished recording those, that we weren't necessarily tying up the position that things are complex and things matter morally, even though they kept getting conflated.
Those were two distinct arguments.
Yeah, stop it, Anyash.
So, sorry, it was actually almost completely you who kept saying, look at how much more complicated things are than we thought they were.
And I was like, yeah, but that doesn't make them, like, complexity doesn't tie up with, like, moral stuff, but you weren't saying that it was, but the whole time I thought that we were talking about something that we weren't talking about.
So that said, I have revised my opinions a bit on that, and it was, it just took me clearing up what we were actually talking about to do so.
Our question-answer, our question-asker person, Edgar, I believe, was saying that we couldn't know what was inside the minds of an animal.
We couldn't know what something was thinking, what was going on in there, and my argument was, well, to some extent we can.
People are making great strides on that.
But we couldn't know 100% what a bacteria is thinking, because it's not thinking anything.
Zing? No, but that said, I think that that point that you just made was much clearer to our listeners than it was to me during the conversation.
At least it was to the listener that I talked to about this.
But as far as physical processes being tied up, this kind of comes up again with the complex parasitic bacteria.
Atomic fusion in stars is really cool, and it's complex, and there's lots of big moving parts and small moving parts, and they interact in really interesting ways.
But there's nothing moral happening there. It's just happening.
And I would kind of put bacteria closer to helium atoms than I would to dogs.
And if we want to go even smaller, we don't even know how protein folding works.
It is complex enough that we cannot simulate it, and we've been trying for quite a long time to figure out this protein folding problem, but proteins aren't things that are intrinsically valuable either.
They're just kind of cool.
Proteins, of course, are incredibly valuable to us, and we, as fuel, that's what we're built on.
Well, fuel and materials, yes.
Proteins are incredibly important.
Proteins without proteins is not a world I could exist in.
Wow.
I mean, I think proteins do a lot more than you realize, but, sorry, got off topic here for a moment.
We're the only ones who are, who we know who are assigning moral value to things, right?
Now, it's very possible that, in fact, it's probable that other animals are doing that too.
But it's tough for us because we know it's subjective.
It's not something that exists in the world.
A star can't have moral value because we're not there going, this has moral value for this reason.
We can do it in the same way that we can for proteins and that if stars weren't doing their thing, we wouldn't exist, right?
But I think there's a huge, giant gap filled with white fire between complexity and how interesting and cool it is and things that morally matter.
Morality is weird. I think that we should just let it go anyway.
And eat each other in the streets, right?
Eat each other in the streets.
I'm joking about the, I'm straw, very much straw manning the...
Do you guys know about prions?
Yeah.
I guess they're also called prions.
Prions anyway.
So those are pretty neat because those are pathogenic proteins that, by their folding, cause things like mad cow disease and chronic wasting disease, which is mad cow disease for elk.
Huh.
Now you know Coloradoans.
So don't eat elk if you're in Colorado?
Chronic wasting disease is a problem for elk.
Can humans get it?
I don't know.
Cause I know we could get the mad cow disease from eating the cows.
It's true. And it is very, very similar, so possibly.
I know it's being researched in Colorado on the western slope.
Anywho.
Metal soft don't eat elk, got it.
Metal soft don't eat cows, don't eat elk.
Don't eat cow brains anyway.
Just, I was going to eat a cow brain this morning, but then I remembered not to.
So, if a sponge cell is more complex than a sponge, would you put more weight on individual sponge cells than a sponge as a whole?
Well, I mean, the sponge is also, is also, the whole sponge is its cells too, right?
So isn't that an additive effect?
I don't know.
You can make.
We can kind of, this is a reductionism problem, right?
At what level of stuff do we care about?
So like, when you mentioned how cool and important proteins are for, say, stuff made of squishy things to exist, sure.
But like, if we're running in a simulation, it doesn't necessarily, it's not obvious to me that we need to simulate everything down to the protein level, right?
We can simulate things at the surface level where, you know, in middle universe size, right?
Because there'd be no point in simulating quarks if you're, if you're concerned about computing power.
So, but I think that you could, you could simulate people in a simulation, given our conversation with Roman Hansen, who would still have moral weight, even if they didn't have proteins.
So like, they would be simpler of makeup.
No, you're right.
But that's a devastating rebuttal.
But no, I think that they're still really cool and they can be appreciated for how awesome they are.
But I don't, I don't know that you could simulate somebody without simulating their proteins, which are so deeply involved in communication between cells and all cellular processes and communication between cells.
That's also neural impulses and all of that is, is linked to basic things like proteins and cell membranes and stuff like that.
I think you could simulate a world that didn't even go down to the cellular level, right?
But we'll still have thinking people in it because the, the, the part that you're really interested in simulating is the thinking part.
But I might be blowing smoke.
Oh, that's a good question.
So I guess I, in that episode, I talked a little bit about the different levels, right?
And how sometimes those can be at odds with one another.
Coenah flagellates, I want to say is the name of the ancestor of sponge cells.
And they're very independent.
You can put a sponge through a sieve and the cells will, will be fine and they can reform into a sponge.
Oh, that's kind of cool.
Yes.
So it's more like a corporation.
There's an analogous thing to that kind of might work as an intuition pump.
If you're feeling particularly reflective while watching Rick and Morty during the episode M night Shyamalanma simulation or something.
I forgot what it's called.
Where, where Rick's in a simulation and he's taking apart a rat in the garage.
He's like, this is just shot the craft craftsmanship.
And I think he's saying, he's looking at it.
He's like, this isn't what a real rat looks like on the inside.
This is just bullshit.
And so I think that he might be doing that by saying, look, they're not even simulating all this stuff that goes on the inside of rat.
I mean, what a rat looks like on the outside.
I mean, I, so that was, that was on my mind recently.
So I was kind of thinking about that as far as the level of a simulation you need to do to, you know, have things function.
Those are my two cents.
There is a difference between complexity and moral importance.
And it's also an interesting question as to like, you could probably simulate someone so that they would be indistinguishable from their regular self to an outside observer.
Absolutely.
But does that person, if it is a not very deep simulation, do they have any self awareness?
