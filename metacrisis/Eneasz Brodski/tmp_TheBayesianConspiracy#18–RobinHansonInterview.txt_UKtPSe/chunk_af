Well, if you, I mean, let's assume that we're, let's assume that we're making it very happy.
Right.
But even as like we made it as happy as it could be, and it turns out flatworms when
maximally happy are 1% as happy as people can possibly be.
So you could, you could raise, you could make a trillion of them and the average happiness
of the world would go down.
Okay.
That's different than the question.
So that's a, so that's an iteration of the question, right?
Yeah.
If you could make, if you could make roundworms that were 1% as happy as it's possible for
human to be, and they were all that happy, should you just emulate a ton of them?
Well, I was saying flatworms the whole time roundworms.
Sorry, wrong shape.
There's, there's actually a really long.
I can't answer that question now.
There's a really long and interesting implication and whole discussion on that exact question,
which I don't want to get into because it's its own podcast episode, but I will link to
the stanford.edu's entry on the repugnant conclusion, which is from Derek Parfitt's
writing about some of the implications of utilitarianism, which basically it amounts
to, and this is, this will just get you to read it because I'm not going to explain how
it gets here, but the, the conclusion is essentially for any population of possible people, excuse
me, for any possible population of at least 10 billion people all with very high quality
of life, there must be some much larger imaginable population whose existence, if other things
being equal would be better, even though its members have lives that are barely worth living.
So it turns out that following that, that line of reasoning gets you there, kind of kicking
and screaming, but it's hard to avoid.
So there's, there's not a great way around the pug, the repugnant conclusion yet.
Well, let's, let's, um, let's, let's live in the least convenient world for your argument,
though, where instead of this leading to the repugnant conclusion of with us throwing all
our resources at maximizing the number of happy fl- uh, roundworms there are, we just say
we will forever dedicate this, uh, computer, large supercomputer, let's say it costs a
million dollars, but that's nothing for our society in general, this million dollars
supercomputer to, uh, simulate as many possible flatworms, extremely happy flatworms as it
can.
Say it's 40 quadrillion happy, uh, flatworms.
Is, does that's something we have a moral duty to do?
I think you should probably do it just in case.
Really?
Yeah.
Oh.
And then you can update it as you continue to, um, be able to stimulate more different
kinds of animals that are more happy.
That would push us into a lot of weird obligations, though.
Yeah.
Uh, it would also put you at risk for looking at all these lives that depend on the power
not going out, you know, one flick of electricity and you've just committed a genocide of 40
quadrillion happy worms, right?
No, cause then you can boot them back up and they'll still be happy worms.
There's a point.
Yeah.
All right.
That is, that's something to chew on.
In my opinion, in my opinion, there is no moral obligation to beings that don't exist.
And so these future beings that wouldn't come into existence unless we made them, we don't
have any obligation to make them.
Uh, so what about AI and.
Or like the future.
I don't think we have any, any obligation to make them come into existence.
I think we probably will because we think it's fucking awesome, but I don't think there's any
moral obligation to create an AI.
Does that, does that.
What happens?
Like not taking care of the earth cause two generations from now is new people and who
cares what the world's like in 60 years?
Cause they don't exist yet.
Oh.
Um.
I want to say no, but I don't have a good reason for that based on what I just said one
time.
I mean, because these are people, I'm saying that you don't have an obligation to bring
people into existence, but since we know two generations from now, these people will exist.
They, we shouldn't make things absolutely shit.
Yeah.
That is the distinction I was looking for.
We don't have that.
We don't have an obligation necessarily to maximize the number of potential persons.
Right.
But given that they're going to exist, let's not fuck things up for them.
And that's why any, she doesn't get to have any kids.
I actually plan on having no kids.
Yeah.
But, but it's not because I want the human race to go extinct.
No, I know.
It's because you don't, you don't have an obligation to create people.
Right.
So that they can be future people who are happy and frolic in fields being gazelles sometimes.
I would donate sperm to that effect, but I don't want to have kids cause I think it
would make me less happy.
Ah.
So.
That's just as good of a reason.
Yeah.
I mean it's selfish, but you know.
I don't, I don't, if you, if having kids would make you less happy, I don't think you
should have kids.
Yeah.
It would be,
Well, obviously, yeah.
Because that's my position.
Right.
Well, I mean, it would be a dream, not just on you, but on the kids as well.
Cause you would present them.
Right.
I mean, I think, I, according to parents.
I think they have very happy lives.
I think, I think parents have this kind of weird compartmentalization where they are
able to, while having a lower than average genetic index for like the duration of parenthood,
I think that they still find that stuff super rewarding.
Right.
So like, you're not as productive doing the stuff that you love, but you're finding so
much joy out of pushing a two year old in the swing that like, that makes it all worth
it.
And that's not, I'm not shooting on that position.
That's just, I think, from what's been described to me, how, what it's actually like.
So
We'll talk about this another time.
There's some rewiring done in there.
And that is a big topic.
So we'll save that one.
Yeah.
I will say that even though I'm not having kids, I'm glad other people are and have
in the past because I really liked the human race and I wanted to continue.
And that's how you came about.
Yeah.
Exactly.
I'm glad that I exist too.
Yeah.
And honesty is for the birds.
You need to check out Derek Parfit's pre-partner conclusion.
It's exactly where this is going.
If you haven't already.
If you haven't already.
Maybe you had, that's why you gave us such a perfect question for it.
But in any case, everyone else who isn't familiar with it, it's a very, very interesting thought
experiment.
Or not that experiment, whatever.
It's a different, it's a, it's an interesting exploration of philosophy.
I didn't want to misuse the word thought experiment.
Okay.
All right.
That's fair.
Go to the subreddit.
That's where all the conversations are taking place.
Send us an email instead.
Send us an email.
We'll talk it out.
Okay.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Goodbye.
Bye.
Bye, everybody.
Sorry.
