And I imagine some of your listeners won't have either.
So can you finish saying what the question was?
The question was, you have a button you can push, which will make a duplicate earth in a parallel universe or outside of our light cone or somewhere.
So it's not just duplicating effort here.
There's actually going to be, you're doubling the utility, you're doubling the lives and happiness and everything like everything that's good on earth.
51% chance it'll do that.
49% chance that all the earths are destroyed.
So either you spin up a second matrix or you delete this one.
Yeah.
Well, not this one.
You delete this one and all other matrices.
Assuming there are others.
And he said, yeah, I push the button.
And he was like, even though there's a 49% chance, you're going to destroy earth.
And he's like, yeah, it's net positive utility.
He'd be dumb not to.
He's like, okay, well, then after that, you're presented the button again.
And he's like, well, then you push it again.
Now you have four earths.
And he's like, or you have no earths.
You can just St. Petersburg paradox your way into having wiping out all human life.
And he's like, or you St. Petersburg paradox your way into having infinite utility.
Checkmate atheists.
This, this is why I don't like utilitarianism.
That's not, that's not good utilitarianism.
Thank you.
This is somebody who likes, who likes pointing fun at like religious people.
I wouldn't point like I'm, it's, it's high school.
It's Lord to point it like the suicide bomber and be like, that's why I don't like religion.
Like granted, that's an edge case as to why I don't like it, but it's more like that.
That is not the representative sample of the average religious person, right?
Right.
And so this guy, I feel like if he's doubling down on the 51% thing over and over,
I feel like he might just be trolling, right?
I think that's how he lost all the money and wiped out tens of thousands of people and ruined things.
Right.
So if he, if he, if he sticks to his guns in the Bahamas and he's like, no, I made the
right call.
You know, it was this, it was the expectedly good outcome.
Then it's like, all right, this guy is kind of not about his, and he sucks.
Sorry.
Let, let's rewind to the, uh, that's not good utilitarianism thing.
How is that not a blatant, no true Scotsman fallacy?
Like you're just drawing a circle around all the obvious implications of utilitarianism
that suck and declaring them not real utilitarianism, which sure, on that grant, on those grounds,
I guess I'll endorse utilitarianism, but that's basically just going to cash out as virtue
ethics.
So the, but with a lot more computational intractability.
I think in general, my, my thing where like, I'll, I'll draw a circle around something
and say, this is not part of the thing that I consider in utilitarianism is like the things
that often with not a number attached to often produce bad outcomes.
Like this move that this guy, that SBF did, produced a very bad outcome.
And if, if you were the kind of person, if this is the way that you knew the world operated
or you wanted to operate, nobody would invest in companies like that, right?
Um, nobody would save their money at a bank because the bank also invests your money,
right?
Like this, this would actually be terrible.
And so if it has a terrible outcome, I think it's not a very utilitarian thing to endorse.
Right.
The actual utilitarian calculation is if you keep pushing that button, eventually you're
going to get a universe with zero earths because they all get destroyed, which is negative utility.
So you don't push the button.
That's the problem with naive utilitarianism.
It's super naive and, and doesn't do anything.
And it's also why humans can't do utilitarianism because you often can't actually calculate
everything.
Like in this case, it's super simple to calculate.
Oh yeah.
Pretty soon you're going to wipe out everyone.
So that's stupid and don't do it, but there's some cases that are a little bit harder.
I don't think this was one of those hard cases.
I think this was like him just wanting to make a lot of money and not wanting to admit
that his other company was failing and being driven by ego.
And then, you know, post-hoc rationalizing it that this is not utilitarianism at all.
This is just someone being an egomaniacal idiot.
Okay, sure.
But if this isn't utilitarianism at all, and this is the most utilitarianism like thing
that humans can run on our meat brains, then shouldn't we just give up on the project?
Well, I mean, as was said before, humans should not do utilitarianism.
They should do maybe three fourths of the way there and actually stick with things like
virtue, ethics or deontology or something.
I think that that's basically the right approach.
I mean, like, you know, you can do the utilitarian thing while still sticking to like certain
axioms that seem to make a lot of sense.
Like, I'm not going to violate the dignity of, you know, somebody else or a stranger
or a group of them.
I will take no chances in destroying the world while doing my thing.
But I'm also like, if I've got $1,000 that I wanted to give to charity for the holidays
or whatever, I'm not going to give it to the Salvation Army guy outside King Stupers.
Right?
I'm going to give it to an effective charity that will save people.
Right?
Yeah, it sounds like we pretty much agree.
Oh, I think we totally agree.
I'm just, again, I'm still kind of riled up.
I guess if you're on board with that, I don't see why you're not on board with Hoel's argument.
But I guess you were also getting that second hand from the podcast and not just reading
the article itself.
Well, I was getting it firsthand from him.
For the listeners, before we started recording, we were talking about Hoel's argument on a
podcast with Lex Friedman.
Was it Lex Friedman?
No, it was Russ Roberts.
Oh, Russ Roberts.
Okay.
So Eric Hoel wrote a substack article on why effective altruism is bad.
I assume it'll be in the show notes.
Yeah.
It's because they're not giving money to Eric Hoel.
Well, I mean, if maybe the essay is more coherent than him, like talking about it, but he was
just being the most like ridiculously disingenuous straw man wielding person I've seen since
I watched Dinesh D'Souza debate Peter Singer in 2007.
Which is basically what's happening right now with SBF and people saying this is why
EA is our evil.
But also it's a simple enough argument that people are going to be like, oh yeah, this
pattern matches to what all the villains do in movies where they're like, oh, it's for
the greater good.
Therefore I will commit atrocities.
So I have not listened to the podcast.
I have read the article and I found it not particularly straw manny or incoherent.
So probably he's just not not good at doing the interview thing and just like couldn't
present it as a conversation.
Sounds good.
No, I'll definitely read it.
I mean, I'm intrigued.
And he did say a couple of times like, you know, I'm just doing this memory, so I might
be wrong.
But then he would go on.
They would go on to like discuss that for 10 minutes.
It's like you were actually wrong.
And so you're now you're just wasting everyone's time patting yourself on the back for how
right you are to see through this, you know, obvious falsehood when you made up that falsehood.
But you know, when you're writing stuff down, you get more time to look it up and read your
sentence over again and fix it.
And as somebody who can't talk talk good most of the time, I'm very sympathetic to the fact
that he might have misspoken on a couple of things.
So, so that was the whole thing that happened.
You'll hear more about the nitty gritty details on the next Mindkiller episode because I'm
going to put it in there if David or Wes doesn't, we're definitely going to mention it.
Cool.
Yeah, I mean, it's interesting.
It's big news.
It would hurt the community if we learned that Peter Singer was in fact torturing children
in his basement, you know, like, wait, but would it?
I think that because he's an actual icon of the community.
I mean, SPF kind of was not to the singer level, but he was a lot of the money driving
the community forward.
And that's that's fair.
Maybe I, you know, he's mostly set off my radar.
Like I did hear him on Sam Harris's podcast and I was on there in the last few years.
But I only remember that he was on there because Harris opened up his most recent episode
mentioning the crash and said I had him on the show.
And so like he was peripheral to my attention, but he wasn't somebody that maybe maybe he
was this for other people though, whereas like this is what I heard about, you know,
I can do good better.
And I found this person really inspiring and oh shit, he sucks.
Like that actually is a drag, right?
But, you know, we'll see how it shakes out.
Hope it doesn't set things back too bad.
Yeah.
I don't really approve of like movement effective altruism.
Why is that?
Just because I think it's got a lot of the politics and power dynamics that a lot of
movements that start with good principles end up developing approximately immediately.
But can you say what those are?
Because I don't know what those are.
I can't remember any examples offhand, but but like, I mean, an example would be like
a rich asshole rocketing to celebrity because he started throwing a bunch of money around.
Just more like banali, I guess.
People in movement EA tend to be big fans of Rob Wiblin.
I find him a like, I kind of like 80,000 hours, but I find Rob Wiblin himself to be like
a fairly uninteresting thinker.
He's a good interviewer, but like his contributions to his interviews are like,
not really much.
Being a charismatic popularizer is important.
Yeah, I mean, that's more of a like, I really don't get the hype.
And I think it's just that like, he's probably some sort of influential behind the scenes.
Or maybe, I don't know, he just has a really good Twitter game or something.
So wait, you dislike EA because it has people that are popular,
even though they aren't the smartest people?
The smartest people, the people doing the best work, you know, whatever.
But again, that's movement effective altruism.
I'm more on board with like the principles.
But principles without a movement is, as Paul would say, a faith without works is meaningless.
Principles without a movement does nothing.
So one of my big takeaways from virtue ethics that's really like helped me as a person,
just psychologically is, and this is the like virtue ethics is more about being a good person
than about doing good things sort of thing.
Virtue is about you and your struggle and your journey, not about saving the world
with the theory being that like, if the world is populated by virtuous people focusing on
their own personal virtue, then it will be saved.
And so that's like, in that sense, like I see effective altruism as being
something that a virtue should track, namely prudence.
But uh, so like when I donate money, I try to donate it effectively and I try to donate
more than I think I should, just like running my basic money allocation software.
I do think that there's some individual projects like GiveWell trying to identify what those
effective charities are that are useful just because of basic economies of scale.
But all that is to say, I think there's fairly significant risk that this does do some
serious damage to movement effective altruism.
I'm just not entirely convinced that that'll be a bad thing on net.
