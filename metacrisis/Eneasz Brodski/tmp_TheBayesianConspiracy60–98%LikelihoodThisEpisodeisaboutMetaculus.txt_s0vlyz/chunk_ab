benefit there. But yes, I think there are lots of things where we should be using the power of
well-calibrated aggregate predictions to sort of label things much more usefully than we do now,
because we're doing it all the time anyway ourselves, just not very well. Yeah.
So how does Metaculous differ from a prediction market then?
So the way Metaculous works is it's essentially, so it's a list of questions, you go in, you see a
question like, will SpaceX land people on Mars prior to 2030? That's one that I'm looking at right now.
And you say, okay, what probability am I going to attribute to that?
And you might think through what are the different things that have to happen for SpaceX to do that
and so on? What are the timetables? How long is it going to take? So you come up with a probability,
you put it in, you can then come back and update that probability as time goes on.
And then at some point, that event either occurs or not. 2030 is a little bit far off,
but as I said, there are lots of shorter term questions. So then when the question comes out,
either yes or no, you get points. So these are just sort of reputation points based on how
accurate your prediction was. So if you predict 99% and it happens, you get lots of points,
if you predict 1% and it happens, you lose very badly, so on. So this is a feedback system that
sort of rewards and punishes you depending on how well you do and also how well calibrated you are.
So you quickly learn not to give 1% probability to something unless you're really quite sure
it's not going to happen and so forth. So then what happens is after a while, you start to accrue a
track record. Each predictor starts to accrue a track record. And so when we have this question
about SpaceX landing on Mars, there are 1,120 predictions on that question.
The median prediction for all those 1,120 people, at least of the fairly recent ones,
is 83%. Now, this is probably way too high. And the reason it's probably way too high is that a
bunch of people or real fans of SpaceX came and made predictions on this question and they really
wanted to succeed. And I don't blame them, but 83% is probably really high. But if you look at the
prediction that is based on people who have a strong predictive track record in the past,
that have just gotten lots of questions right in the past, the prediction is 39%. I'll tell you now.
This is kind of secret information, but the listeners of this podcast can now go and
they have the inside number, 39%. So much lower. So on the back end, there's a waiting of the,
I guess, the predictions based on what your track record was in the past. Exactly. So it actually
does two things at the moment. One is just waiting on how accurate people were in the past. And the
other is recalibrating people based on how each individual, so each individual will build up a
record of the questions that I said 70% were 70% likelihood for what fraction of them actually
happened. Most people at the high end are overconfident, meaning that most people when they
say something is, say, 95% likely, it's actually more like 80 or 85% likely to happen. But once
you have enough statistics, you can then adjust for this. So when I plug in my prediction of, say,
70% to Medaculous at this point, it immediately knows that what I really mean is 60% or something
like that. And it does that for everybody who's got a track record. And then it takes those
recalibrated numbers and weights them by how well people do and then produces this aggregate
prediction. And the recalibration is unique to each person based on their track record?
That's right. Neat.
Now, it's sort of tricky because if you, you know, there's a little bit of attention in that if
you're continually doing Medaculous and taking it seriously and getting better and sort of trying
to fix your own calibration, then Medaculous may over recalibrate you for a while or something
like that. But eventually it should even out. So it's cool to see. And it's interesting to,
it's an interesting piece of self knowledge that now when I make a prediction of Medaculous,
I can, I can watch it tell me what I actually mean. It's a little bit sobering when I, no,
no, I really do mean that that's 50% likely. And it says no, no, 40.
This is kind of neat because as rationalists, often, you know, we talk about prediction games
and how to recalibrate ourselves. And it takes a bit of effort and Medaculous almost sounds like
it would help track that stuff for you. Can you look up your own record?
You can. So, so you can, it keeps a record of all your predictions as well as the aggregate
predictions for the whole platform. And you can just look and it gives you a little calibration
plot that you can see what your calibration is based on the questions that you've answered.
And it would give you breakdowns like of the ones that I guess 70% actually 62% of them were
correct. That's right. Neat. So it's a, so what I've found, so now we've been doing this for a
couple of years and, and so we have enough sort of data and enough users, although we can always
use lots more to start to see statistics and things. And so a few things I would say have
become clear. One is that essentially the things that I mentioned earlier that that some people
really are good at this, you know, there are people who just keep getting them right. And it's
not clear that they do lots more work. They're just better at it. And it's, you know, and there
are people that I know who are quite smart and really good at lots of things, but not particularly
great at making well calibrated predictions. I think I would be in that camp. This is just how
it is. You know, I'm decent, I found, but, but not at all as good as some of the best people.
It's also clear that doing it trains you to think, you know, in calibrated predictions.
And so when people ask me now, you know, do you think this is going to happen?
Probability just pops into my head. I can't help it. I spent enough time on the system
that I'll tell them, you know, 73% and they kind of laugh. But I'm serious.
How much time do you spend on the system? Is it like an hour every day or more?
Probably less than that. And I actually don't take care to make predictions about everything.
But I like to keep an eye on what's going on, look for, I like to think of new questions to put
up there because that's really fun. Keep an eye on what the community is doing, what the predictions
are and things that I care about. So like any user, except that I'd, I probably produce more
questions and kind of keep an eye on things more than most users would.
Oh, can any user submit a question?
They can. Yeah. So, so if someone has a nice, well-formed question that they submit,
it will probably just go live. If it's kind of just an idea or a kind of mess of a question,
we'll try to find someone who will, you know, a moderator who will turn within a shape.
The, the, one of the things that becomes clear after writing and editing questions is
it's quite hard to write precise questions that really will, you know, turn out to be true or
false. You know, you've got to make them precise enough. You've got to have the data source,
you know, to compare to that, you know, is going to exist when the question resolves.
You've got to think through what are the ways that things could go wrong or just different
than we're thinking, such that this question wouldn't have a clear answer to it. And this
happens all the time you find. And so, for example, you know, we had a question of whether
Trump would try to fire Mueller, right? And the period went by and Trump hadn't tried to fire
Mueller. So it resolved no. But then it turns out that Trump had tried to fire Mueller, but then
didn't succeed, right? He, he told, you know, the legal counsel said he would quit if he tried to
fire him. So, so does that count or not? So fortunately, the way we had written the question
was such that it didn't really matter. But that was the kind of thing that happens all the time,
that, that there's just some way that things can turn out that isn't either the yes or no that
you specified in the question. So you get really good at specifying the question clearly enough
that it's really going to be one or the other. And, and that in itself is a learning experience
to sort of think through, well, what are, what are all the ways that sort of things could pan out?
And what it really teaches you that when somebody asks you, you know,
is this going to happen, you know, where there's like a default, there's a baseline,
and then there's the this, which is something different from the baseline.
The answer to that question is probably not. Okay, whatever that thing is, because it's,
it's almost always less likely than you think it's going to be. There's almost always the thing that
you've called sort of the positive outcome almost always has various ways to, to go wrong. You know,
if you ask, will stay sex land people on Mars prior to 2030, there are just so many things that
can go wrong and that space x can go out of business. It can end up happening in 2031.
You know, SpaceX turns into another company. The government could say it's too risky to
let humans go off the planet. Yes, the SpaceX launches the mission and they're on their way
and the humans die because they're in space with radiation that they didn't, you know,
so there's so many ways that something like that can go wrong. So thinking through those is a really
useful exercise to do and you just get, you get better and better at that with practice, I would
say. So how does anything ever happen? Well, something's always got to happen. But, but when
you name the thing beforehand, you know, then you often forget all the different ways that it
cannot happen. It's really Murphy's law, right? Okay. There are lots and lots of ways that the
world could be. You pick a tiny number of them and say, this is the way I this is where I would
like it to be. Or this is the way I think it's going to be. And then you shouldn't be too surprised
when that goes awry, because because you've picked a tiny fraction of the things that that could happen.
You know, and it's a testament to our capability as, as predictive and effective agents that,
you know, anything good happens at all, right? The world is just working against you all the
time. The phase space of the universe is just immense. And if you just meander around, you're
going to make a mess. So it's a lot of effort to make things happen the way you want them to happen
and to happen favorably. And, you know, that's what we're about. That's what we've spent
millions of years evolving in order to do and thousands of years evolving society and technology
in order to do is to, to fight against Murphy's law in some sense. And, and so that's sort of
what the idea is here too, that as we understand what are the different pathways forward,
how do we avoid that huge phase space of unfavorable possibilities and hone in on the
good ones? God, that I really, that sounds very noble to me. I like to think of myself as an
agent for anti chaos in the universe. Oh, you totally are. I mean, that's what systems are in
general. We work very hard to prevent entropy from increasing in our local environments in a
major way, or at least at least in our bodies. And, and we work very hard to pick goals that
we have for reasons that are, you know, known and decided by us and make those happen. It's,
it's quite amazing that we can. So I have two questions about metaculous then. I guess the
first one would be, what do you do with all this data that you get? Like, what is, what is
the point of knowing that there's a 37% chance that that, or people think there's a 37% chance
that will land people on Mars? Yeah, I would say there are three real purposes. One is to
provide the sort of training that, that you get when you use it. So it's sort of, you know,
it's, it's sort of like an educational exercise that people who use the site for a long time
just get better at doing this. And so it's a, it's a sort of training system for getting people
better at making accurate, well calibrated predictions and thinking clearly about future
events. So I think that's, that's one positive thing that comes out of it. I think a second thing
is that it is a way to figure out which people are really good at making predictions, right? So,
so we now have lots of people with a real unimpeachable track record, positive or negative,
about how they do it predicting things. And this is not something you can fake, right? This is,
there's no, you can have all the degrees you want, but if your points are negative,
your points are negative, you're not good at predicting stuff. So it's an amazingly clean
sort of reputation system, or sort of authority granting system for people who are just good at
what they do. And, and when you think about it, making predictions is at some level a very basic
source of why we believe or trust some people and not others, right? Why do you really, why do you
think science, say, say physics or electrical engineering or biochemistry is given the accord
in society that it is, it's because the iPhone works, you know, the electromagnetic waves carry the
signal from one place to another. The, you know, the, the rules that are created in those fields
are rules that are basically for making predictions about what some physical system will do.
And the, those enterprises have reputation because those predictions work. And I think it's,
it's similar in, if you ask two experts, well, what do you think is good, you know,
what do you think is the better way to do this? Or what do you think happened in the
circumstance? Or what do you think is the better choice for what policy to use?
They're going to give you different arguments and they're both going to sound pretty convincing.
But if one of them really has a track record of actually getting things right in the past,
that's going to weigh tremendously, right? So if, if you've got someone who has, you know,
done many different ventures, say, say their business ventures or some other sort of ventures
and been successful at them, and another person who just hasn't, they've kind of failed over and
over again, and you ask them to make a prediction about something, who are you going to listen to?
You're going to listen to the one who succeeded over and over again, because they effectively,
you know, predict the future and, and they have, you have a degree of trust in them.
So I think there's, there's a huge utility in having a system that can generate a real grounding
of trust in particular people or groups of people or processes that make accurate predictions again
and again. And so that's part of the goal is to say, okay, we've got this system that you can
rely on, and here's why you can rely on it because it's done this in the past. Here's how well you
can rely on it, right? You can, you can look at questions that are like this that have been given
an 80% chance of happening, say, and see that in the past, 80% of those things have happened, right?
Whereas if you just go to some expert at a consulting firm, and they, you know, run the
numbers and tell you 80%, what are you going to do with that number? Exactly. Do you trust that 80%?
I don't know. I would want to, if I were to trust an 80% number from someone, say, well,
what is that based on? What's your model? What's the ensemble that is 80% of what and so on?
So I think that's the second goal is to really identify who are the really good predictors and
create a sort of reputation system for the predictors and for the platform and the aggregate
prediction as a whole. They can provide a source of kind of authority and sort of trusted
insights into things. And then I'd say the third goal is just to actually generate predictions
about things. So to see, for example, for artificial intelligence, what sorts of time
frames should we really be thinking about in, for different levels of capability for robotics?
What probability should we be thinking about in terms of autonomous weapons or
artificial general intelligence or passing the Turing test or, you know, artificially
generated fake news? There are all kinds of things that are coming, right? And we don't know when
exactly. Nobody knows when. But the question is, what is the best level of estimate that we can get?
And I think knowing to the degree that we can have confidence and answers about those questions,
you can really take action based on that. So if I feel like there's
a, you know, 20% chance that artificial general intelligence is coming within five years,
that's a huge piece of information for me. If I think that chance is 0% or 0.001%,
then that's a different piece of information that really would affect what I, you know,
as with my hat of the Future of Life Institute who worries about this would do, right?
Now, to enable for this to become useful, it does have to have the trust of the, I don't know,
greater community. And it seems for, to gain that trust, you need to be,
do something very public correctly. Like I think, what was it? Nate Silver from 528
made a almost 100 or was it 100% spot on prediction, which is what got him all the
attention and people started listening to him. I don't remember what the prediction was. Now,
it was one of the presidential races where he called every state. Is, is there any effort yet
to do something large and showy like that to demonstrate the power of metaculous?
It's a good question. I have two minds about it in, in that at some level, I think it,
it should be and has to be the aggregate track record that you really look at to trust otherwise.
So there, one can imagine, one doesn't have to imagine because it actually happens, but one
can imagine someone who makes predictions regularly and then, and they make very bold
predictions, right? But somewhat quietly. And then when they get them right, they file them all
in a big file. And then after a couple of years, they have this huge file of amazing, bold, correct
predictions, right? Because they've just cherry picked the ones that came out, right? And that's
something you can certainly do. And it looks very impressive, but carries zero information or utility.
