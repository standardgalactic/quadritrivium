Hello, welcome to the Basin Conspiracy, I'm Inie Ashbrotsky, and Stephen isn't going
to be joining us until later today due to a hold up with the brand new house that he
just bought. But on the line with me, I have Anthony Aguirre, and Anthony, can you introduce
yourself?
Yeah, I'm a professor of physics at UC Santa Cruz, and I'm also one of the founders of
the Future of Life Institute, as well as one of the founders of metaculous.com, which we'll
be talking about today, I guess.
Yes, you're here mainly to talk about metaculous, but briefly, I didn't realize that you were
a co-founder of the Future of Life Institute. Could you mention, speak about that quickly
as well?
Yeah, so the Future of Life Institute started a few years ago, and metaculous was actually
sort of a spin-off of that, because when we first started thinking about this really
big question of what does the long-term future hold in terms of these basic new technologies,
biotechnology, old high-impact technologies like nuclear, and we were thinking, how do
we find leverage points now where we can maybe nudge a little bit and steer things in a
better direction? It occurred to me that what we really want to do is have a better way
of predicting things. I mean, we make predictions all the time. The idea that we can't predict
the future is just sort of false. We do it, as a matter of course, all the time in making
pretty much every decision we make, but we're used to making predictions about certain things,
like if I pick this up, it will lift, and if I move my hand this way, it will move that
thing over there. There are all kinds of predictions that we just constantly make, and then when
we have to make decisions, we get a little bit more, it gets a little bit harder where
we maybe plan out, well, if I do this versus if I do that, what will happen with option
A versus what will happen with option B, then predicting what will happen in each of those
cases, I might make a decision as to which outcome is sort of more favorable to my goals
or to my happiness or whatever. But we're doing this all the time, and that's sort of
how one has to make decisions, whether you like it or not, you're making predictions
when you're making decisions.
I think you use the phrase to me before that, human brains are little prediction making
engines, that's what they do.
But a large part of what our brains do is just constantly create a predictive model
of what's going on and then get some data, update our predictive model to make it more
accurate, and then match that against what we would like, what our goals are, and translate
those into decisions about what to do. And we register surprise in sort of exactly that
way, when something fails to meet our predictive model, some new piece of data comes in that
suddenly there's something that really doesn't fit in this environment, there's maybe a bear
in an environment that we don't expect there to be one, we get very surprised and then
we try to make sense of that and we notice, oh, that's not actually a bear, that's actually
a bush that just has the shape of a bear and so on, or it's actually a bear and we're
extra surprised and we suddenly have to take action.
So this is a sort of process that's running in a human mind constantly, and we're very
good at it. We've evolved over billions of years, well, millions for humans with billions
of millions for life to be able to do this sort of thing, but we're always frustrated
because we can't do it quite well enough, especially when we're making long-term decisions
and those decisions are hard. So we can ask, what could we do as a group or as a society
or leveraging technologies that haven't existed in evolutionary history to do a better job
of that, to make better decisions through making better predictions? So this is what
I started to think about when we were starting the Future of Life Institute. What would it
look like to be able to have better predictions, say, of how artificial intelligence is going
to unfold or one of the most dangerous things that could happen in biotechnology or where
is a nuclear war most likely to start and by what chain of events? You'd really like
to know these things in order to make nudges now and there's sort of a staple of science
fiction if you could travel in time. The great power that you have, obviously, is that you
know how the future is going to turn out and so you can go back and you think, well, if
I just make this little nudge here, I can change something with a huge amount of leverage.
And that's exactly the kind of leverage you would have if you could make great predictions,
usually we just don't have it.
Yeah, like everyone would always go back and feel Hitler.
Right, that's the thing that pops into mind, but there are lots of other things that you
might try with the benefit of hindsight. So the question became, the future is hard to predict,
but how hard exactly and can we do better? And it seemed to me like, as with any skill,
prediction is surely something that some people are better at than others. It's surely something
that you can get better at. And it's surely something that when people work together at doing
it and are sort of gathered in an effective way so that they can collaborate and they can sort of
make a sort of community prediction, all of those things have got to be a better way of
making predictions is just somebody sitting around, even if they're pretty good and have a model and
so on. All those have got to be better ways, taking people who are good at predictions, training them
and kind of getting them together. So that's sort of where the idea for this
metaculous effort was born. And as I started to do research, I realized that other people had
thought of this at some level before. There were prediction markets where you use a sort of market
approach to try to sort of aggregate people's predictions about things by just seeing what
they will pay for a given contract that pays out depending on what happens in the future.
And there were efforts to just get polls of people and say, okay,
50 people predict this thing and let's average them together and see what we get.
So there was this fairly detailed body of work showing that those things that I had
wondered about were in fact true. You can find people that are good at predicting and that
persist over time. You can train them and people get better at predicting. And when you aggregate
lots of predictions, you tend to get something that is better than almost all of the individual
predictors and reliably good. Okay, so metaculous is a what a program or a institution that does this?
It's really a web platform. So it's a site that you can go to
the way it's set up at the moment. There's a whole list of questions that are just
some of them are important, interesting questions like when are we going to have artificial
general intelligence? Some of them are whose mind is in the red marble in Westworld. There's a whole
span of topics from kind of entertaining to quite serious and all kinds of time spans from
sort of a week to the longest term question we have currently is will the universe end?
So I have a hard time figuring out of a longer term question to make for that one, but I'll see.
Curiosity, is there what is the general consensus right now on whether the universe will end?
It's mixed. I think 70% last I checked were predicting that the universe would end.
And there's a precise meaning of end that I concocted for this question, which is really about
whether there are a finite or infinite number of computations that can be performed
in the future light cone of Earth basically. So I think this of course is not a question
that you're going to answer or get points for at any point, but it is one that's just fun to
think about and discuss and so on. So there's the whole gamut from things that are happening now
to just long term discussion questions and everything in between.
So that sounds like a survey though, and I think you said that metaculous is a prediction market.
It's not quite a market. So the way a market works is for say will the Senate vote to keep
net neutrality? That's a question that would just resolve yes. So in a prediction market,
you would have a contract that says I'll pay you a dollar if the Senate votes to keep net
neutrality and I'll pay you nothing if it votes not to. Would you like, what price will you pay
for this contract? And so if you're willing to pay 70 cents, that basically means you think there's
a 70% probability that that thing is going to happen. And then you can trade that contract back
and forth and find a fair market price for it. And the presumption being that if markets are
efficient and everybody is using their utmost to make the wisest purchase they can of these
contracts, that that price will settle on the best estimate of the probability of that event
happening. So that's a prediction market and it requires contracts, buyers, and sellers.
And at some level, some kind of currency that you're buying and selling these contracts with.
In some prediction markets use fake currency, some use real currency. There are new ones coming
out with cryptocurrencies or you can use Bitcoin and some. They have a slight disadvantage in the
US that they're illegal. It's a slight problem. They're both illegal because the SEC considers
them unregulated trading of securities. If these are these instruments that you're buying and
selling or thought of as securities and it runs afoul of online gambling laws because you really
are getting paid depending on how some thing comes out. That could be a random thing. So it
falls afoul of both of those. And so the prediction markets that have real money have to be overseas
and it's a big pain for them. And you didn't want to go overseas. We didn't want to do that. And
it's not clear to me that the prediction market is actually the best mechanism for the kinds of
questions we're interested in. So I think. Oh, what's that? Well, markets have their strengths,
prediction markets I think have their strengths and weaknesses. I think they're going to be very
strong where there's lots and lots of people interested in some high profile question like
who's going to be president next. So you have lots of people who are interested and knowledgeable.
And you have people who have a monetary interest and might want to say hedge against
one new president or the other or the outcome of the Supreme Court
decision. You may have a business that stands to lose a lot of money if X comes out of the Supreme
Court. So you might want to leverage, you might want to hedge against that by using a prediction
market to get a large amount of money out if that happens, just like any other hedge. So you
can use it that way. But what's very difficult for a prediction market is if you've got a question
like a small one, will, you know, will Moore's law continue in this little bit or will this patent
accrue 17 citations or not? Or will this scientific paper have some high impact? You're just not going
to find a bunch of people who are willing to lay money on that and create a liquid market for it.
What you might find are a few people with enough expertise to make a solid prediction about that
thing. So so a prediction market just doesn't have a liquidity in general to to deal with these
small questions that are sort of expert based. It's also, I think, not clear to me
what mechanism is the most accurate. I mean, there's there are two mindsets, I think, and one is a
sort of economic mindset that says, you know, it if it's in people's financial interest, it's it's
sort of has to be the most accurate because otherwise you could game the system and gain lots
of money. And that's true if it's a liquid market. There's another mindset, I think, and because I'm
a I'm a scientist, you know, I'm a research physicist, I tend to think of a collaborative
venture where lots of people are trying to find the truth as a sort of collaborative venture where
lots of people are trying to find the truth that you want to design it sort of like our scientific
process where there's some competition, but there's lots of cooperation as well. And there's an
overall incentive where everybody wins if everybody gets it right. Where prediction market,
there's always a winner and a loser, you're never it's never in your interest to share
information with other people, really, because you're just giving them an advantage that you might
have. But if you're buying up a lot of a certain outcome, then that itself is information that
people take into account. If someone suddenly throws down $10,000 on, yeah, sure, that's
information. But what I mean is you, you can't give an argument. There's no, there's nothing to be
gained by, except if you're gaining the system by sharing your sort of deep knowledge of some
situation with other people, you know, what you always want to have a knowledge
asymmetry in your favor in a prediction market.
But isn't that also true of, I mean, obviously, with the scientific method, people are motivated
just for their desire for knowledge and for the truth. But if they weren't motivated by that,
then they also have no incentive to share any information they may have. And so the money
would give them that incentive. Well, I think you can, the money flows in different ways. So
in a scientific field, right, somebody decides, we want answers to, you know, we want valid,
well backed up, well argued, testable answers to questions. And so we're going to put some money
into effectively paying a whole bunch of people to figure out those answers.
And, and so the incentives are aligned where the people want to work together at some level,
you know, there may be some competition because they want to get more of that pot of money than
the other guys. But, but the basic incentive is to take the money, do research, share the
research, work collaboratively with people and, and sort of find the truth, at least when it's
working well. And often it does. The finances behind a prediction market are a little bit
different in that it's people, the people who are making the predictions are subsidizing the market.
Yeah. Right. So, so they're, they're betting against each other. It's, it's sort of a zero
sum game or maybe a negative sum game of the, if the market takes out some, some money or positive
some of the market decides to put some money in. But the fundamental dynamic is really more like
gambling. Whereas a, whereas a scientific method, and this is what Metaculous is actually modeled
after, the idea would be to have some set of resources. So if there were a bunch of money
that were being spent on Metaculous, there isn't. But if there were, what it would be used for is to
subsidize the whole thing and effectively to pay people to make predictions in the same way that
scientists are paid to do research and find out truths about facts. Now this would be payment for
high quality predictors to work hard and find out truths about the future, or at least probable
truths. Okay. One, one last follow up question before we get onto Metaculous then. And I will
admit right up front that this is not my question. I was just reminded of it. Robin Hansen originally
asked something along these lines, but seeing the way you just described the scientific process and
how it differs from a prediction market, what would you think about using a prediction market
to lay odds that a certain paper's result would be confirmed in follow up testing,
and deciding whether to publish in a paper based on the that prediction market's results?
Yeah, I think that would be super interesting. But I don't, but I think it would be super
interesting whether it's a market or some other mechanism for predicting the success of a paper.
So we do this all the time. At some level, you know, when I write a paper, I don't write a paper
that I think is going to be boring and have no impact. I don't like to write a paper that I think
is going to turn out to be untrue or falsified or something that's super embarrassing. Right.
And when I look at a paper, I think to myself, is this paper likely to be high impact and
interesting and important, then I might invest the time of hours or even days or weeks in some
cases to read that paper and really understand it or not. So that's a prediction that I have
to make. And it would certainly be lovely if I were looking at a set of papers about esoteric
physics questions. And there was a predicted probability of this being an interesting paper
produced by a community of physicists that would be super helpful. Right. I could then look at the
ones that were likely to be high impact. And of course, it'd be all kinds of gaming and stuff
that went on in that system if we were to devise it, but it would certainly be much,
much more helpful to look at that set of papers with probabilities for them being great next to
them than just a list of titles, which is what we have now. So in that sense, I think it would be
great. I think there's another sense that, you know, Robin, and this is true of prediction
markets in general, it's a mechanism for sort of putting your money where your mouth is.
You know, when you ask somebody, well, what do you think is going to happen? And they say, well,
this is going to happen. 99% sure. And then you ask them, okay, here's a dollar. Well,
you pay me $99. If it doesn't happen, suddenly that $99, that 99% tends to come down a little bit.
Right. So I think it's certainly true that betting is an interesting way to help people
recalibrate their probabilities when they suddenly think about, well, what am I really that sure?
And I think something similar could happen in prediction markets in general, and for
scientific ones, if people were putting money on the line. So I think it might have some additional
