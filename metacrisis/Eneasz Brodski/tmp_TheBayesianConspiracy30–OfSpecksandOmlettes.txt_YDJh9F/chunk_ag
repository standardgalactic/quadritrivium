I liked you take the orange pill.
Oh, sorry. Yeah.
OK, I don't want to go off of too much of a tangent, but there's this thing
that Scott Alexander wrote about with the pills.
It's like this little fiction thing.
You know those memes where it's like, which pill would you take?
And the orange one was like master every human skill.
I would like 100 percent take that one.
It's awesome.
Anyway, wasn't one of them like superpowers.
Anyway, we'll we'll figure that one out later.
Maybe they were very limited in scope.
We can argue about that next time, because I'm sure there's some way
to munch in one of the superpowers to be way better than anything.
There is a bad munch getting the superpowers.
Yeah, exactly.
It's got Alexander made a very good case for orange.
But anyway, so I agree with him.
I was just trying to think of a positive
to example to maybe pump our intuitions the other way.
Right. Sure.
Sure.
So is there any level of how of a number of people that could exist
seeing one second of cat video that would equal a lifetime of perfection?
Yeah, 40. No, I'm kidding.
I like I like cats more than average person.
So I'll pick something like 46, 46, I'll pick something less.
You know, like it could be like just an extra tiny bit of sweetness,
something they ate or something, right?
I don't know.
Is that one second of cat video improved someone's life so much that the
I mean, it's supposed to improve it minutely, right?
It's relative extremely minute improvement.
Yeah. So I mean, I'm the example of the person
whom a dustbeck would kill in the in a reverse experiment, right?
Because I like cats that much.
Not really, but I'm just ruining the thought experiment because I'm bored.
Would you rather be that many people?
One of them would just be the dustbeck would be the worst thing ever, right?
Would you rather have a happiness specs thrown into everyone's eye exactly
one time versus this one dude just getting smothered in happiness dustbecks?
In this example, my intuition is pump the other way, where I think, you know,
I feel like dispersing that happiness is better.
But if I have actually said it and do the math like I did with the torture
problem, I'm pushed out of the way, right?
There's one person who's actually I would assume the other way around,
but that's theoretically would go for the big 50 years one in that case, right?
In terms of positive utility, I mean, I would right now my answer to that is probably us.
I'm digging up a it seems more appreciable.
It seems to be a more appreciable benefit than some extremely minor like,
oh, that was cool.
Okay, I'm done thinking about a cool that was in the meantime,
you're going to have other like happy and bad things happen in your life.
And comparatively, those happy things, those happiness spikes that you have,
I mean, they're not continuous, but that is a consistent position, right?
Because yeah, the total happiness is more when you have that tiny bit of sugar in your tea.
And so that would be the correct answer to choose.
And yet we're still going with the one with the one person getting all the happiness
because we can see that that effect.
I wouldn't go with the tiny bit of sugar spread out among everyone.
Yes.
Okay.
It's easier, more intuitive to go with Stephen's argument about the dollar amounts.
Like I can't help but think that in all these cases, like unless you know the specifics of
how it's going to affect people or what's going to change,
then it's kind of hard to just make a call.
Like this is always true.
This is always false.
You know, like I would always go with this answer.
I would always go with this other answer.
So like we could, you could just make it as abstract as possible to make that really easy,
right?
You could just say, everyone on earth gets point zero, zero, zero, one plus one or plus,
you know, that that percentage of utils or one person gets a billion or a trillion or
whatever number you want to throw at it, right?
And so then then that way you don't even have to say, well, what about the possibilities
like for sure, or dust bags?
You just think about actual utils, right?
So this relates to a kind of fun thought experiment that is a kind of challenge to utilitarianism
that the author of the webcomic Saturday morning breakfast cereal independently invented called
the utility monster.
Wait, Zach came up with that independently?
Yes.
I'm really surprised because he's so well read.
I would assume he would have run into that.
Was this early on in his career?
I'm pretty sure he was on an episode of rationally speaking and Julia asked him about that and
I'm 90% sure he said he thought of it by himself.
Anyway, so the idea is that you could say one person on earth is just for whatever quirk
of nature just way made way happier by things than the average person is.
And so if you're going to go by like total amount of happiness, this one person, their
utility range accounts for like a third of happiness on earth.
If you go by how much pleasure they can get out of things, like them finding, you know.
It takes someone $10 to be a little bit happier.
This guy for $10 feels like someone who's won the lottery.
Sure.
Yeah.
Isn't that kind of like why you abstract the utility?
I mean, that's what you're just talking about.
Yeah, but I'm getting at just another fun little way of like, so one of these,
this whole problem is just like, you know, do you go with like totals or averages or...
If you're trying to maximize total utility in the universe, you give this monster all
the utility there is.
Everyone else lives in absolute poverty because their poverty is, it's not worth giving them
an extra $10 because no matter who you give the extra $10 to, this guy enjoys it so much more,
you get more utility in the universe if you give it to him.
Right.
And it is supposed to make you make that look because everyone is like,
that's fucked up.
Fuck that guy.
Yeah, I guess sounds like a real asshole and I don't even know him.
Well, but it's not even...
And I even applied to gender.
I assumed as gender.
I'm a shit lord.
So it's, the idea is that this person is just capable of so much more.
And this is actually not inconceivable for like, say, designing minds from scratch, right?
You could design a super intelligence that its range of experience just puts it way above,
like it puts us, it's like relates to us in the way that we do to ants, right?
Well, I mean, I love that you brought that up because I like the argument that we have
met the utility monster and he is us, that humans have decided that we are capable of
so much more intellectually and emotionally in our lives are just so much richer and more
important that we don't care about the other animals really.
They don't count.
Well, they came in new algebra, I mean.
Right, exactly.
The few utils that, you know, a chicken could get throughout his entire life is we get
more utils than that from eating that chicken by an order of magnitude.
So it's totally worth it.
So many vegetarians disagree with that math and that's why they're vegetarians.
But the point could be taken.
And I think that defeats the idea of the one utility monster.
Like it's not one super species.
Or a species of utility monster.
Yeah.
So like there's a difference between that and like just one person hogging all the
utils, right?
So like at least it's divided.
And this is actually a really, this leads into another really fun thought experiment
by Derek Parfit from his 1984-86 book called, of reasons and persons I think it's called,
but now I'm hedging on that.
Anyway, it's the idea of the repugnant conclusion.
And he can daisy chain you along this path where you could say,
I'll try and be brief, but it's really worth diving into.
The idea is that you could say, all right, well, we've got a world populated entirely by people
who are just as happy as possible.
One million maximally happy people.
Right.
And then you could typically get people to agree the way that he lays these things out
that if we introduce some people who are 9.8 out of 10 on the happiness scale to this world,
now there's 1.5 million, well, that's a better world.
There's just, there's more happiness, 9.8 is good, 10 is the best.
And then you can get it to kind of where you just slide the scales down,
where say you get people who are at, you know, you get half populations at six,
another population, the other population is at eight,
but that's still better because there's more of them.
So it does take a couple of minutes to get all the way into, but it's a fun,
and I've mentioned this like three times,
we should just get into it at length later,
but I'm not feeling completely collected right now.
I've heard the same thing.
It's a great thought experience.
Like if one million extremely happy people is great,
two million almost as happy people seems like pretty good still as well,
maybe even better, right?
Is that conclusion that there's a certain level of happiness?
No, you keep adding people, you keep doubling the amount of people,
and every time you double the amount of people,
you decrease the total happiness just a little bit.
Is there a specific line where you stop because it's not worth it?
Right, exactly.
And at some point you get down to very miserable people,
and you can double the amount of people that there are,
and they're slightly more miserable,
but they're still on the positive side of the scale.
Ultimately, you never get to the negative side of scale
because at that point you say, let's stop making people.
But as long as you keep reducing the average happiness of any one person
by a little bit, but you double the amount of people,
you increase the total utility.
And they have no effect on the previous people.
The idea just really quick is also that these people still have lives worth living,
that it's not like they're barely subsisting,
or they're suffering their entire lives,
they're just, things are okay.
There's fewer TV shows to watch in one version of this.
Food tastes a little less sweet.
Things just get a little bit worse,
but they still have overall good lives.
The republic conclusion is that you could end up with
by days changing along this line.
I mean, if you just jump straight there,
then people kind of aren't willing to jump,
but they're willing to walk,
and they're willing to say, all right, cool.
So I'll take a world with 10 billion people
who are only kind of happy for the most part,
as opposed to a world with 1,000 maximally happy people.
So you have a million people with happiness level eight, right?
Why does adding more, why is adding more people better outside of more utility,
