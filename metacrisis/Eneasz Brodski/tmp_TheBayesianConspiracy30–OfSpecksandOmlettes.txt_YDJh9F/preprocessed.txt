Hi there, this is Enyosh with two quick notes before we begin.
The first is an apology.
Very soon after we set up, my microphone got knocked out and we didn't notice.
I was still picked up on a nearby microphone and we boosted my voice on that everywhere
we could.
However, for the entire podcast, I sound distant and echoey and it's kind of hard
to hear me.
I apologize about that and we'll be better about checking our mics in the future.
The second thing is that, before we started recording, those of us on this episode just
kind of bullshitted for 10 minutes.
It's not really relevant to the podcast, but for any of our Patreon subscribers, we've
thrown up about 10 minutes of our miscellaneous chatter and humor, just as a little extra
behind the scenes for anyone who wants it.
Thanks, and now on to the episode.
Welcome to the Bays and Conspiracy, I'm Enyosh Brotsky.
I'm Stephen Zuber and we have two guests this episode.
Hi, I'm Sean.
Hi, I'm Matthew.
Returning from punching Nazis.
Yes.
There is a podcast project that has recently come out to make all the sequences available
in audio format as a podcast.
We've talked about the sequences a lot.
They were the original posts written by Eliezer at LesRom.com, actually started out at OvercomingBias.com,
which kind of kickstarted the whole rationality subculture back in 2007, I think.
Anyways, they're a starting point.
They really explained the rationality much better than I could, and I said in I think
our very first episode that if you really want this rationality thing explained, your
very best resource by far is just to go and read those sequences of posts.
Eliezer went ahead and compiled them into a book because a lot of people were like,
hey, we hate surfing posts on the internet, is there like a book that we can have?
And no, you can't have a bound book because that is really, really big and you'd have
to pay an obscene amount of money to get something that big bound in a single tome,
but it is available as an e-book.
And now there is two guys from Europe who are recording every single chapter, each chapter
is one post.
It is between five and 15 episodes per chapter and putting them out as a podcast.
So two things.
One, the book wasn't compiled by Eliezer, it's compiled by somebody else, I think, like
with permissions and thumbs up or whatever, but there is a podcast out there that I think
you have to pay for.
I bought a couple of the sub sequences, I bought a human's guide to words and reductionism.
They appear to now be defunct.
The castify ones?
Yeah.
Yeah, I looked for them like, I don't know, six months ago and couldn't find them again,
so someone's doing them for free, it sounds like.
Yes, cool.
I know you could make a podcast mandatory pane.
Well, I mean, you can, it's not a podcast, it's audio files and to download them you
pay me.
How would an audio book version of the sequences work since all the posts I've read have like
19 links to other sequences?
I'm assuming you just don't get the links.
And rationality from AI to zombies isn't a bit more at order than the sequences on lesswrong.com.
So there's less of a need to like link to previous things.
I think the links are still there in the book if you buy it, but it's much more in order.
So a little more accessible.
That sounds awesome.
Yeah.
Yeah.
The podcast.
I believe it's rationality AI to zombies.
The podcast.
Hey, there it is.
Yeah.
Rationality from AI to zombies, the podcast.
Subscribe.
Are you shitting me?
Sorry.
Done.
Subscribe.
The reason I bring that up is because at our last, at our last lesswrong meetup, which
we have in Denver once a month, I got talking with Matthew about the torture versus dust
specs post and we'd been kind of drinking and we were like, hey, why weren't we recording
this?
This was awesome.
We should really put it up on for the podcast and so now we're doing that.
All right.
So Matt, do you want to lay out the dust spec versus torture thought experiment as best
you can?
So Yudkowski, I'm not sure how, but he did come to the conclusion that the least bad
bad thing that can happen, something with a negative utility, but the smallest possible
negative utility that's measurable would be having a dust spec and up in your eye.
Part of the hypothesis is that's all that happens.
There are no side effects.
It's not that a dust spec hits you in the eye while you're flying a plane and then
you crash the plane.
Ignore that.
It's just purely dust spec and that's it.
And if you have like calluses on your eye or something and getting a dust spec in your
eye doesn't hurt you, then substitute something that actually does hurt a bit like stubbing
your toe or getting a paper cut or something.
Those trick me as orders of magnitude higher than dust spec.
Certainly magnitudes.
It's kind of this abstract idea of like a really minor inconvenience, like the most
minor inconvenience that you'll acknowledge as a thing.
The least bad bad thing.
But it'll go away in like three seconds, right?
Yes.
A very minor pain.
I don't think it can be just inconvenience.
It has to be some measure of pain.
Okay.
I thought it was just some negative yudelons.
We should have been more taken on this.
Yudelons.
So I should probably mention that I haven't read almost any of the less wrong stuff.
You're fine.
Yudelons, I don't.
Is that a unit of measurement of utility or?
Yes.
Yeah.
Okay.
Positive and negative.
All right.
So it's like kind of the idea.
It's just like a term for measuring utility.
Positive or negative is basically what you were saying.
Yeah.
Depending on how into consequentialism and utilitarianism you want to get, you can start
quantifying it with units called utils or yudelons.
But I mean, for the most part, you're talking about happiness and suffering or pleasure and
pain or something like that, right?
Sure.
And I assume there has to be some measure of pain involved because that's what negative
utility is, right?
Or unpleasant.
I mean, it can be pain if your definition of pain is broad enough to include things
that aren't literally pain.
Do wet socks hurt?
So an uncomfortable thought.
Right.
So like an uncomfortable thought or like a small emotional trauma or like.
Something whose net value is negative.
The thing is, I hate wet socks a lot.
Same.
So that's why I wanted to pick out.
I wanted to pick out an example that was obviously not painful.
But obviously negative utility.
Okay.
Because I'd rather get 100 dust specs to the face than get my socks wet.
So if the least bad, bad thing is having a dust spec float into your eye and give you
the tiniest bit of measurable minor inconvenience, then it would be better for one person to
be tortured for 50 years and three to the third to the third to the third or a Google
plex or whatever conveniently large number fits, not have dust specs flying to their
eye.
Looks really quickly unpack the number.
I think the actual number doesn't matter that much.
But the point is the number is so large that if you were to take the number of all the
atoms in the universe that we can measure, took that to the power of itself, that number
still wouldn't even approach how many people were talking about at this point.
It is an unimaginably large number.
So nearly infinite.
Nearly infinite.
The only reason I don't use infinite is because infinite is seems like such a vacuous concept
that to me it almost feels like nothing.
Whereas when I think of the number of atoms that exist and then multiply that by itself,
that's an actual like large number that infinite I know technically infinity is more, but
infinity doesn't feel like more infinity sounds like, you know, you're stupid infinity plus
one.
Douglas Adams has a good thing about infinity in his checkers guide where Arthur, is that
the main character?
Dude, I don't know.
Whoever he is.
Yeah, it's Arthur Dent.
He's in space and he's in front of a very large object and he looks to his left and
he sees the object go on for infinity.
Oh wait, no, it isn't infinity because he can see a curve ever so slightly which has
a more imposing effect than if it didn't curve because humans can't handle infinity.
Right.
So he can actually see it like he can actually understand like this thing is really, really
big whereas infinity would be incomprehensible basically.
That many people getting a little bit of negative utility.
If the measurement of for a dust spec is 0.00000001 and you multiply that by a impressively
huge number, then the amount of eudelons, whatever 50 years of torture happens to be
would be less than that number caused by dust specs.
I don't think we laid out that the that is being weighed against 50 years of torture
for one individual until just now.
So I guess that's the important crux of the position that it's this dust spec this dust
spec into basically infinite number of eyeballs versus one person being tortured for 50 years.
And the idea is that well, because you're taking an infinitesimal thing times the opposite
of an infinitesimal thing, and you then you just have one, you know, being suffering as
much as possible for five decades, the dust specs add up to more negative eudelons than
one person being tortured.
And on the surface, that does sound kind of ridiculous, as I'm sure most people are not
thinking because we have such an aversion to thinking of someone being tortured even
for five minutes.
But as you pointed out, this started as something that seems entirely reasonable and then is
taken to an extreme.
Yes.
Yes.
Would you like to recapitulate that argument?
Because I thought that was fascinating the way you laid it out.
Should we all kind of say like where we're where we are on that whole thing?
Yeah, sure.
Where are you on that whole thing?
Toads against.
Yeah, like, like, yeah, not only is it, yeah, everyone should get a speck of dust in their
eye.
I think it's a slam dunk and like, why would you even like, sure, we should think of everything.
But in the scenario in which every and this like almost unfathomably large number of people
get a speck of dust in their eye, nobody's tortured.
It's just a bunch of people who get this momentary like speck of dust in their eye for like three
seconds and then for one second or split second or whatever it is.
And then like five seconds later, the whole all those people are fine and nobody gets
tortured.
That is that seems like a very obvious that's better than someone being tortured for 50 years.
And you'd have these point is that feels obvious because our stupid ape brain has a problem
with scale.
I mean, I mean, so the problem with scale works to make this just a problem that we
should know that we can't intuitively address, right?
It can't be, well, intuitively, I would think the tortures the worst, I'm going to just
go with that.
The correct answer isn't to go for the other solution because you know that your intuitive
answer is wrong.
You should look at the other answer and say, well, I can't really intuit that answer either.
So I don't feel like my intuition is a good guide to this problem.
That doesn't mean that you can't solve it, but that doesn't mean that I think you can't
intuitively solve it.
When I first read the post, I was very much against the conclusion as well.
And my reasoning was that I would volunteer to take a duck spec in the eye if it meant
that someone wouldn't get tortured for 50 years.
And I imagine that every person who is worth having around on the planet with me, or even
in the same universe as me, would also make that trade.
It doesn't matter how many of them there are, if everyone would be willing to make that
trade, then obviously it would be better to have that happen than to have the 50 years
of torture.
First of all, I don't think Yucowsky picks a side.
I think he just says the answer is obvious.
He does.
In the post, he only says the answer is obvious, but he does think...
In a comment.
In a comment, he says...
In a discussion, yeah.
Down the line, yeah.
But at the beginning, to make it kind of a thought-broken post, then he just lays out
the conundrum and then said the answer is obvious, right?
And I thought that was a fun way to get the discussion going, and it has some of the
most comments on a lot of these.
So the way this sort of thing starts out, and I don't remember if this was the exact...
I'm pretty sure this was not the exact example used in the post, but it would be something
along the lines of, is it better for one person to have both their legs broken, or for 10
people...
Okay, so this is like a reduction of that big, all-encompassing thing with the near-infinite
people and one person being tortured.
It's just condensing it down to a really small example.
Yeah, a good way to break it down.
So one person having both their legs broken, sorry, and then...
Or 10 people having one leg broken.
Right, so you're supposed to say that one person having both their legs broken because
that's two legs versus 10 legs, right?
Exactly.
Okay.
Okay, and if you keep drawing that out, is it better for one person to have every bone
in their body broken, or for everyone on the planet to have their wrist shattered?
Well, okay, right.
So obviously you're supposed to choose the former.
Mm-hmm.
And would you, though?
Yeah, I mean, at that point, if it's everyone in the world that's six billion people currently,
or more than that...
I think it's like seven billion now.
It's seven billion, right?
Isn't it six billion?
We're closing it on seven.
Okay, we're closing it.
So seven billion people get their wrist broken versus one person having every bone in their
body broken.
Yes, including those really tiny ones inside your inner ear.
Right, so that would cause a lot of problems for the world at large if that were to happen.
Let's assume no knock-on effects.
That's what I was going to say, is that, like, imagine suddenly everyone on Earth is...
It's an upper hand for whatever is really important.
Okay, so there's nothing...
So nothing...
We're still going to be able to go to work.
So it's better to break it down starting out further than breaking one leg versus two
legs, where you say, should one person break one leg, or should a hundred people stub their
pinky toe in a minor enough way that they don't break the bone?
They kick the table and they go, ow!
And then, uh...
And they forget about it ten seconds later.
It hurts them for, like, ten seconds, or ten minutes even.
Well, in that case, I think I would rather go with the toe.
Stub the toe.
Yeah.
Stub the toe, absolutely.
Right.
Because I think the one leg is more than the stubbing the toe.
Right.
And as you expand that to, okay, what if a million people stubbed their toe versus one person
breaking their arm?
Yup, stubbed toe.
What if it's ten million people?
Stubbed toe.
Is there any level where you would not go with stubbed toe?
I mean, if it was the entire population of the earth, and we had colonized the solar
system, so there was, like, a hundred billion people.
I see where you're going with this continuous extrapolation of the problem and expanding
it, but it's like, when you get to a point where it's just, it's a speck of dust in your
eye for, like, two seconds and you forget about it forever, for everybody, versus someone
being tortured for 50 years, that, like, that to me is an insane comparison.
Like, there's this underlying assumption that, like, all of suffering is on, like, a single
spectrum, right?
So we take the furthest point over here and the furthest point over here, right?
But it just seems like a really naive view of, like, utilitarian ethics in general to
say that, like, oh, yeah, of course we should torture someone for 50 years, because math,
like, that just seems like a really...
Well, that is kind of the point.
That's my biggest problem with it.
It's just, it's like, but no, but no, like, you pick the dust specks because then nobody
gets tortured for 50 years, that's insane.
So to give the argument that convinces me to steel man it against the thing that I'm
agreeing with, would I say I prefer to torture one person for 50 years or break both legs
of a million people?
Not torturously, just their legs suddenly break and they get anesthetic, but their
legs are broken.
So that, that to me is how to make this problem interesting.
I didn't find the dust specks original thought experiment too torturous to me.
The dust specks is largely part of the problem.
So that, that was it for me was that to me it is, it's not, it's so small as to be as
to be equal to nothing.
And so, especially if you forget about it three seconds later.
Right, so think of the next...
No, I know.
That to me is that's, that's when it becomes interesting.
So if the original proposition is too weak and easy to dismiss, you've got to scale up
the suffering to something that's not point zero, you know, zero, zero, zero.
It was supposed to be something that is on the same continuum, just the smallest possible
you can get on that continuum.
So, but for me, the key thing is that you forget about it almost immediately with no,
like intervention to forget about it.
And so like, if that's the case, it might as well have never happened, you know, like
even a stubbed toe is kind of on the order where I'm, I probably stubbed my toes a hundred
times in the last 10 years, but I don't remember any single time to me.
The idea that it's easily forgettable and maybe the timeframe, something to do there
as part of this kind of gets shaky, you scale it up to, all right, fine.
A broken bone versus torture.
And like that, then you kind of, at least if that's how hard you have to go for me,
I think you could draw it lower.
But if you, if you put it at almost literally nothing, that to me is exactly literally
nothing, the dust spec, right?
So the two main issues with the argument that I'm thinking three of us have is,
one, dust spec is just negligible.
Yeah, it's supposed to be really close to zero, but to us, it feels intuitively as if
it were zero.
Yeah.
I mean, I would, I would pick dust specs over torturing someone for one year or like
six months or three months or one month or like way less time, like any amount of
time, five minutes, five minutes.
Yep.
Nope.
Versus every being in your, in the universe, getting a dust spec.
Yep.
Nope.
Dust specs.
Well, not just every being in the universe, every being for every atom in the
universe, and then every one of those atoms having an equal amount of beings in them.
And if you're not willing to go with dust spec, then go with, you know, paper cut,
make the problem fun.
Yeah.
Right.
But I do have a problem with Yidkowski's attachment to the dust specs.
He never suggests, well, if you have a problem with dust specs, go to the next thing.
I mean, not in the post itself because he wrote the post and then he put it out there.
But later on in the comments, I said, go ahead, go with stub toe.
If that's your lowest, right, that's still painful.
The second problem we probably have intuitively, and I'd argue rationally, is whether
or not something of a certain magnitude can equal any number of that thing.
And his large argument is his point is that utilans reduce to a metric.
And that's the metric.
And it doesn't matter whether or not it happens an infinite amount of times.
It's still a standard metric and they can be compared.
So something a thousand times worse happening once versus something happening
versus the other thing happening a thousand times are perfectly equivalent.
That is the root of his argument, I believe.
I don't know that I can get behind that one.
And I can't.
I really agree with it as well.
It just seems like the thing that's a thousand times worse than the really minor thing,
in some cases, at the very least, is so many orders of magnitude worse that, again,
it just seems like a slam dunk where it's like, no, it's literally three orders
of magnitude worse if it's a thousand times worse.
OK, OK, sure.
Yeah, but I think if you find something that's actually a thousand times worse,
then you would agree that a thousand one one thousandth of that is the same
multiple multiplied over a thousand.
Because when you think when I think of like a paper cut versus breaking
an arm, they don't seem like they are comparable.
But that's that's because like I'm not saying like a paper cut is one
and breaking an arm is one million.
It would be more like a paper cut is one and breaking an arm is like 27 quadrillion.
And at that point, once I've gotten 27 quadrillion paper cuts,
I may prefer to have a broken arm rather than 27 quadrillion paper cuts.
Is there enough space on your body to have that many paper cuts?
He's a model and it takes 10 seconds to heal.
Yeah. Oh, nice. So he's Wolverine. OK.
I mean, it's just that you're not using the right numbers.
You're comparing something that is not one one thousandth to the one thousand.
And then you imagine to actually, you know, like you said,
actually doing the math to some extent, whether you do it on paper or do it in your head,
just, you know, trying to make it equivalent.
Say, yeah, I can get a paper cut.
I can get 100 paper cuts in my body every second for the next 20 years,
or I can break my arm. Well, I'm going to break my arm in a heartbeat.
That sounds awful, right?
So then you scale it to like that.
That does kind of drive that point home that there is a trade off somewhere.
Right. We're right.
So I'd also tell you, are we are we drawing arbitrary lines or not?
Yeah, I think we probably are because none of us are.
I mean, none of us have a patent paper.
And I don't think anyone's going to do complicated mental math in our head.
But I might take the pain, but not like the physical manifestations
and consequences of being set on fire for three seconds rather than break my arm.
Right. So I'm not sure.
But those are the kinds of things that you can you can weigh
as far as how bad they would be.
And you can actually just you just pick things back and forth and say,
well, we'll weigh these against each other. Right.
So you're talking about like when you say breaking your arm,
is it like you have to get medical care?
Your arm is inoperable for a certain amount of time.
All that stuff that comes with the breaking an arm
or you just feel the pain of it breaking your arm.
And it doesn't have any lasting effects after that, either one.
I mean, I was picturing actually breaking my arm doing eight weeks in a cast
and that sort of thing. But like with the fire one,
I just said, just imagine being in super pain for a second, right?
Three seconds of the crucial curse from Harry Potter.
You know, I would choose that over the paper cuts, you know, drawn
out to that over 10 paper cuts a day for the next year.
Right. Like just because that would be super annoying.
That'd be a constant drag.
You know, I think I'd get over five seconds of the cruciitis, right?
Maybe depending on how bad it is.
They didn't do a great job of painting it as the literal word.
I often just have that burst of pain just for three seconds kind of thing.
Sure. I just want to point out that I slowly pull off band-aids.
I've heard that slowly pulling up band-aids is actually the better way to do it.
I'm sure it is. I go about medium.
There was an actual pain study done.
And I mean, pain is always kind of subjective.
You got to ask people one to ten and all that.
But they found that in general, yanking band-aid off first is
quite a bit worse than slowly pulling it off, even though the little light pain is stretched out.
Are you saying because it causes some sort of trauma to your top layer of skin?
No, well, maybe, I don't know.
But it was just it was more painful than having that small amount of pain stretched out.
And the reason they the conjecture was the reason that nurses and parents and stuff
always tell you to yank it off fast is because the person who's doing the yanking
gets to feel pain as they're yanking as well.
They either watch you squirm while they're slowly yanking it off and they feel really bad
or they yank it off once real quick and you flinch and they're like, OK, it's over.
It's done. Interesting.
So it's the person who is yanking it off gets a benefit from yanking it off quickly
because they don't feel as much pain during the process.
There's a similar point to be made.
There was a study done with getting patients to return for colonoscopies.
If you prolong the procedure at its least uncomfortable point,
you got, I forget the numbers, but these are in the Daniel Kahneman thinking fast
and slow, I think. I heard this, too.
Yeah. So you get more people to come back if you prolong the procedure
while it's at its least uncomfortable, like while it's there.
But like not at the most painful part, because in that way with the look.
So there's the other possibility that explains it instead of like the slow rip
off versus the hard rip off. If you do it a hard rip off and you look back
and like, oh, that really hurt. If you do it slowly, it just sort of hurts.
And so when you look back, like, well, yeah, I mean, it was a little uncomfortable,
even though it might have aggregated to more or the similar amount of pain.
With the colonoscopy thing, the idea is that you look back and you remember,
oh, yeah, it wasn't that bad, despite having time was just uncomfortable.
Exactly. Equal amounts of uncomfortable and painful.
Actually, probably more if you just needlessly prolong the procedure, right?
So it's probably more uncomfortable over the stretch of the entire procedure.
But you're just your memory, your recollection of it is, oh, it wasn't so bad.
Good stuff. So, Inyash, you would defend the original
proposition of dustbex versus torture.
Yeah, I would defend that if we can put numbers up on things,
then we're putting fucking numbers on things, right? Are we or aren't we?
Sure. And so is your guys' proposition that you can't put numbers on things?
That's not my proposition.
No, no, what I would ask in response to that is,
how is that meaningful to apply numbers to this like really weird
scenario of like dustbex versus torture?
Like, what is that going to accomplish?
Like, what is the actual, like, like defending it and being like,
we can condense this down to math and come up with a like an answer
in terms of like the utilitarian perspective.
No one is suggesting there will ever be a situation in which
it's 50 years of torture for an individual versus three to whatever number
of dustbex. Sure. So it's a thought experiment that plays out the idea
that you can condense utility into math and thus you must have the numbers
correct, basically is what you're saying.
And whatever conclusion comes from that should actually be applied to every
decision or most decisions that have to do with.
But aren't there some decisions where like the math kind of, it's like, OK,
you can observe the math, but there's this, maybe that's not the be all end
all, right? In my opinion, the point is that it is the be all end all.
And this is pointing out that humans are very fallible and very likely to,
as soon as something goes against their intuition, be like, oh, fuck, no,
this is we can't do numbers anymore.
And it's trying to make you bite the bullet.
So for example, the where people will be asked,
how much money are you willing to give to have one seagulls scraped of oil
after the big oil still happened?
And it was like $10.
How much money would be willing to give to get 10 seagulls scraped,
clean from oil and they can go about their happy seagulls lives?
They're like $20.
And that's like, why were you willing to give $10 per seagull when there was
only one, but only $2 per seagull when there were 10 of them?
Because you started with one question and they're like, oh, for one seagulls,
sure, I'd give 10. Yeah, you're right.
That's kind of inconsistent because the eight intuition sucks.
Yes, people are really should get intuition.
But at some point, there should be an actual number that given an undefined
number of seagulls, you'd be willing to give this much money per seagull
to clean them, regardless of how many there are, right?
Sure. Or maybe you set like a budget limit and you're like, I'm only willing
to spend $100 toward seagull welfare, you know, or well-being.
This is my seagull oil scrubbing budget.
It's $100 right now.
But if you're actually trying to put a number that it is worth putting
per seagull, this is actually very important because we do have a number
of dollars we're willing to pay per human life.
Well, I was going to relate this to experiments that have been done
with charities towards humans.
Okay.
And so one of the keys to doing this is you ask different groups.
If you ask the same group, how much would you give to say this little girl
and how much would you give to say this little girl and her brother?
The same people will scale up.
I don't know if they'll actually double, but they'll scale up in some meaningful way.
If you ask if you ask different, well, they don't double.
They're being inconsistent, assuming that their budgets aren't workable, right?
But if you ask different groups and you show a picture of a little girl and say,
how much would you be willing to help this little girl out?
They'll give some number and I forget what it is, but we can look them up.
If you show a picture of that little girl and her brother and say, how much
would you, how much would you give to save the pair?
If you ask that to a different group, it's typically less than on
average people will give to save the girl.
So if the one little girl and her family of eight people will give way
less than they will do that one.
And this is sort of a different problem than the suspects thing.
But this idea of scope and neglect that we suck follows from the same principle
that our brains can't multiply emotions.
I mean, and that sort of makes sense.
A, on the fact that we just didn't have an ancestral environment that would
sort of program that.
But if we, A, if we, or B, if we could, we would just shut down every
time a disaster happened.
You know, if we could actually understand how much it would suck for 200,000
people to die in a disaster, we would, all of us would just kill ourselves.
Right.
I could be bad enough for that dust, for us to be able to emotionally
understand that for 10 people.
Which is why I don't want to be a super happy.
Right.
Wait, why is it that you don't want to be a super happy?
In three worlds collide.
But why?
I missed how what Stephen said relates to why you don't want to be a super
happy.
The super happy is find out that the start trekking humans in that story allow
for specific things and everyone except the neutral party on the super happy
ship passed out or blacked out or had it taken nap from the stress.
Yeah.
I mean, that almost sounds like an appropriate reaction if you learn about
something terrible, right?
And so like even the humans on the ship kind of realized that like, man, our
reaction to the babies being eaten was kind of mild compared to what it really
should have been.
Right.
But their reaction was to us allowing the emotion of embarrassment to exist.
Sure.
This is all kind of an aside.
You should read three worlds collide.
It's a fun, what, six or seven chapter short story.
And the first three chapters are hilarious.
The first what?
Three.
And what do the super to the super happy show up at what three or three?
I think it's two.
Okay.
It's also an audio form on the Harry Potter and methods of rationality
podcast, but then you don't get to enjoy the comments per wiki page, which I
think are quite worth it.
All right.
They're well, well, willing to both.
So this thing with the like, like doing the utility math is kind of like when
there's a super powerful AI and there's a singularity, it needs to be able to
solve all the trolley problems, basically.
Well, even we need to solve trolley problems a lot, right?
Well, sure.
Yeah.
And we're eventually, I mean, there's, there's this whole thing about like
self-driving cars and like, if it has to make a trolley decision, like we want
it to be the correct one, right?
And anyone who works at a hospital, actually, I guess anyone who works with
the budget at a hospital has to make trolley decisions.
Every single time budget time comes around.
Sure.
Sure.
That makes sense.
So if you put a number on a human life and we do, then you're saying that these
numbers are important and that we should maybe shut up and multiply how many
lives we can save per dollar as opposed to going with these squishy intuitions.
And if we are saying that, yes, the math is the best way to do things because it
gives us the greatest benefit, then we shouldn't all of a sudden abandon the
math as soon as it says something that we think is not to our intuition.
And if this were in written form, shut up and multiply would be a link to
another podcast.
And it will be, it's like a catchphrase or something.
Shut up and multiply, yeah.
Okay.
From less wrong.
Oh, is it, is it like expand the, the problem to figure out if it's like, if you
have an idea about a small scope problem that you multiply or something?
You shut up and multiply means trust the numbers.
Okay.
Don't, don't go with your.
All right.
So just do the math.
Yeah.
Yeah.
So like, I have a problem.
I do have a problem with that in general.
Okay.
Like an example with the hospital that you know, she was mentioning, say if like one
little girl needed a liver and the hospital director could buy one for eight million
dollars and that would shut down the hospital or have money, how many
dollars a hospital needs to not be shut down or the director could, she could keep
the hospital running and save all the other lives and let the little girl die.
Well, a lot of people will be like, well, you got to save the little girl because
the children and because life and you can't put a price in our life.
But the idea, one of the ideas of shut up and multiply is just, look, you can and
you, you have to and you can.
So let's, let's just not sit here and wring our hands needlessly.
Let's just realize, yes, this little role is going to die and the exchange, the
hospital is going to stay open and do all the great things the hospitals do.
Right.
Yeah, yeah.
Right.
Right.
Like, so, so I understand the concept.
It's just that, like, and then there are probably many, many, many situations in
which doing the math makes a lot of sense and they probably will yield them the
optimal outcome, but aren't there situations in which it wouldn't?
I believe that is the argument that whenever someone gets to a point where
there's something they're not willing to accept based on their squishy feelings,
they say, well, this is one of those cases where the math doesn't apply.
And the, the response which I personally give because I would not put these
words in LA is there is, fuck you, prove it.
If you say the math always works all the time, except for the one case where your
emotions are so important, they overrule the math.
Well, not even just one.
I mean, I'm willing to, if you can, I don't want to put you on the spot, but if
you can think of something like one, we can try and develop a case or we
had all willing to throw out the numbers.
If the whole conclusion of this is that we should trust the math on
utilize in that.
Assuming we get the utilize calculation right, that we can figure out how many
utilize torture actually is.
Sure.
Yeah.
Assuming that the conclusion is we should trust the math on if a
utalon is a thing we can measure, we should go for the more positive
utalons or for the least amount of negative utalons.
And I think I can scale this up and argue against his entire dust spec
versus torture conclusion.
So if all humans have a perfect 50 year lifespan and the starting of this
theory is a 50 year or one entire lifespan of torture versus a huge
number of dust specs, then keep the torture where it is and scale up the
dust specs to everyone getting two dust specs.
Oh, scale it the other direction.
So you're saying like increase the number of dust specs to reduce the
amount of torture you'd have to offset that.
Right.
So what if everyone has to be tortured and everyone like actual
torture to not dust spec tortured, say it's real torture for one year.
I'm pretty sure everyone would argue that being tortured for your entire
50 year lifespan is not a life worth living.
And so if life worth living is a different metric, not directly related
to the utalons from the dust spec argument, then you could reframe the
argument of three to the third to the third to the third lives worth living
versus one life not worth living.
Say everyone has to have dust specs in their eye.
It's just going to happen.
Okay.
Would you like to live in that world where dust specs flying to your eye?
I'm assuming yes, seen as you are in fact here.
Yes, definitely.
Would you want to live a life where you were tortured for 50 years, which
happens to be your exact perfect measurement of lifespan?
No, of course not.
So is there somewhere in between where you draw the line?
Like where you would accept more torture than dust specs?
No, where you would accept being tortured for a good life?
Like most people, I'm assuming, would not be willing to be tortured for 49
years to have one year of wonderful life.
Oh, I see.
I'm going to draw the line at 10 percent just to get the conversation going.
Give me what you can.
Yeah, exactly.
Minimers on my head.
Giving what I can.
I'll give 10 years.
You can get me to the table.
Oh, right.
10 percent.
So it'll be five years.
Well, I imagine living longer than 50 years.
But yeah, you're right.
So I see what you're going up.
I feel like that's a different problem.
It's a different problem, but it attacks the same conclusion.
So where does this leave us?
So it brings us back to you.
I remember while we were talking at the meetup by you.
I am pointing at Sean.
Sorry.
Hi.
Sean at the meetup is one of the people who has read those who walk away from
Amalus.
I don't know if that's pronounced.
Yeah, it's well, I always said Amalus or I think it's a bit of a say,
Omeylus, which is how to tell that Sean is one of the better kind of people.
I guess.
I mean, it's so, yeah, for those who don't know, I mean, most people
probably are aware of the like short story written by really stupid now.
No, no, no, it's not one of those things you either know it or not.
You don't.
It's trivia, right?
It's just a shitty person.
According to trivia gets a free beer.
Ursula, Ursula K.
Le Guin wrote this short story called those who walk from Omeylus or Amalus.
One of the most famous short stories in science fiction history.
Yeah.
Which is the reason all that long.
Oh, sure.
No.
And it's just this allegorical story about how America exploits third world
working wages, basically.
Whoa, whoa, whoa.
Well, that's what a lot of people think.
And that's what a lot of people speculate on.
You are putting a lot of stuff onto that story, which is not in the story at all.
OK, OK, OK, well, let's let's forget I said that.
Those who walk from Omeylus is about a utopian society in which
everyone has lived this perfectly happy life where there's no there are no
inconveniences and everything.
Everyone, it's just peachy keen and awesome all the time.
And it's not like one of those shitty utopias where it turns out to actually,
you know, people aren't really happy.
This is one of those utopias that if you were to make a utopia that is actually
perfect for human flourishing and everyone would want to be there without any
catches, it's that utopia.
Basically, yes.
And and the only way that's what your utopia is.
Sure.
The only way in which it is possible for this utopia to exist in its in the way
that it does is that there's this child, this tiny child that is locked away in a
basement and just basically tortured all the time, just constantly
treated very, very, very poorly.
Do they stay a kid the whole time or do they age?
This is a snapshot of a particular but but I mean, I they don't really cover
that exactly.
But however, the one thing that happens is that I believe everyone in the
utopian society has to go down in that basement and look at the child and see
what happens to it one time in their life, one time in their life.
And some people go down there, look at it, are so disgusted that they walk away
from Omeylus.
Most people don't.
Most people look at and they go, hmm, well, that's unfortunate.
And they go about their lives and they just continue living in a utopia.
It is an acceptable price to pay.
Two quick things.
One, I have heard of the premise of this about even if I didn't know the name.
And two, do they know what they're going to go look at?
And I don't think so.
It's so they do they just say, hey, once in a while, if you have to make a
pilgrimage to my basement.
Yeah, but like it's no there's no like it's the they don't look that it's not
like you're not coming to look at my book collection or coming to look at
something scary because as someone in my book club once pointed out, this is a
story rather unique in that it has no characters and no plot.
And yet it is still a very good story.
It's like everything's spoken about in generalities.
There's no, yeah, exactly.
There's no protagonist.
There's basically just it's just a snapshot.
The author, it's a very conceptual thing.
The author is not specific about anything.
So any of any of the questions you have about like the specific conditions, I
am really unable to answer.
I'll need to read the whole thing, but I guess just for it's really true.
It's like two pages.
That's cool for entertaining it just for the sake of thinking about it.
To me, there'd be a big difference if like you knew what you're going to go
look at and you had to live with that knowledge the entire time you're in
this utopia.
I feel like that would kind of detract from the fun of it.
Well, I mean, given that everyone's not having a bad time.
Once you've seen it and it's implied that it's like sometime early in your life,
sometime within the first two decades or so.
So it can't be your last week.
No.
Oh, OK.
So that's what I was going to say is like, if you're told at birth, early
adulthood or something, maybe it's like, whatever, you can just speculate randomly
when you turn 18, when you're ready to vote or something, whatever.
OK, so that makes sense.
I was going to say it's kind of a rite of passage.
Yeah.
Right.
All right.
So that changes it.
That's interesting.
So then this that's the dustbecks conundrum at large, right?
Yeah.
So it sounds to me like if you weren't living with the knowledge that somebody
was suffering the entire time, because that would detract from your
presumably from how happy you could get.
I wouldn't I wouldn't want to be the kind of person who would be happy
knowing that someone was being tortured for my happiness.
But assuming I didn't know it and I just found it out, you know,
in the last week or something, I'd still have a pretty good life, right?
By the measure of the book, the best life possible.
Pretty much everyone who's read the story wants to think that they would be
one of the people that would walk away, at least from what I've heard.
So does that mean they don't buy cell phones anymore?
Because the microprocessors, you know, I do know where you're going with this,
yes, but no, no, that's that's remote enough.
Could they probably they probably wouldn't visit a cell phone production
factory in the third world, right?
Well, and even if they did, it's hard to opt out of life.
Yeah, but you could become a hermit in the well, they do make ethically produced phones.
They're just ridiculously expensive and they're also like shitty as hell compared
to actual phones that are more harmful to the environment.
So there's that, too.
And again, that's one phone.
Do you still drive a car?
Do you burn oil?
Do you go to McDonald's?
Do you exist in a society where this is the foundations of everything
that make us the society we are fair enough?
So would you stay?
I get the impression that you're going to say you'd stay.
When I first read it, fuck no, I also thought of myself as one of those people
who would walk away from a mallice because that's horrific when you read that.
You're like, yeah, it's a very emotionally impactful thing to have to witness.
Right? How bad is like bother you just so much?
And this this cannot stand.
I will not be party to this.
So if 50 percent of the people of the whole society all at once left
because of that reason, would it make a difference?
Oh, it's implied that very few people actually leave.
But if if 90 percent of the people of that society
left because they didn't tolerate the child being tortured,
would it change anything about the child being tortured?
No.
Then what's the point?
Right.
What the thing that finally changed my mind a little bit
was reading Scott Alexander's post on those who walk away from a mallice
or Omalas or have repronounced it.
Yeah, those walk away from omelettes, right?
Which kind of a monster walks away from an omelette.
Son of a bitch.
At least while it's still warm.
We have a world where children are tortured in basements
and live horrific lives for their entire life.
And we don't get a utopia in return.
No, we don't. We get we get I actually learned recently that Thorn
Ashton Kutcher is the CEO of Thorn.
And what's Thorn?
It is software that helps combat human trafficking.
Like, OK, how?
By scanning the dark net or the deep net or whatever it's called.
It's dark net.
It's it's it's supposed to monitor the dark net for information
so they can hand it off to like authorities to intervene, basically.
That's all I know about it, because the only reason I even know about this
is because I saw a video like a two minute clip of Ashton Kutcher
talking about it.
Does it actually work?
I could not tell you for sure.
I know that they're hiring.
But a minor thing that I want to interject there.
Yeah, the point of the post is that we do have
the kind of world with those children in it and much worse things besides
on just orders of magnitude, more scale.
And we certainly haven't gotten a utopia out of it.
So when you're walking away from a malice, you're walking into a world
that is much, much worse.
And if you were to stay in a malice, there would only be one child
being tortured forever.
And all this other good stuff comes because of it.
I think intent has a large part to do with that, though.
Yeah, intention based morality.
It seems that omelets deals with the purposeful intent
to untorturing the child to maintain the utopia or at least being complicit in it.
Yeah, I think it has more to do with complicity.
Yeah, ours is just.
Well, but I don't even know.
Again, I'll read the whole thing.
It's two pages.
We should just stop while we all read it for three minutes.
But since we're not going to do that, I'll just speculate that
you're not really complicit.
If you you're not making a decision to make it stop if you walk away,
which was my point about if 90 percent of the people walked away,
would anything happen? No.
So like, sure.
But like, and you're not complicit in the fact that like you just have to go
look, you don't have to pick up a knife and go help for two minutes, right?
So you don't you just have to look at you just have to look at what's happening.
Yeah, I think there was something like you're not allowed to say anything nice
to the child or like, consult or make it's like better.
You're you're you're you're supposed to look.
Maybe you don't have to actually actively contribute,
but you can't intervene in a way that alleviates anything.
What a great thought experiment to control for that, too.
That's really interesting.
I get that it's two pages.
They don't specify what life is like outside.
Oh, Malus. Yeah.
The story is two pages.
It is a short short.
Yeah, the first page and a half is basically saying how wonderful their
society is. And then the last the second half of the page is describing
how awful this child's life is.
And then the very last paragraph is some people walked away.
Yes. So I mean, like if they walked away and they managed to recreate
all the utopia without the without the tortured kid, obviously
they're doing the right thing. Maybe.
Well, I guess I mean, it's implied that you kind of need the torture
for some reason, right?
That there's some magic engine that's being turned on this kid's suffering.
And it seems to me like it'd be tough knowledge to bear.
And I mean, I guess like you just have to take for granted that there's no way
to fix it, that there's no way to correct this problem down the line or something
or that you can't work your whole life to make this kid's life any better
no matter what. So you'd have to either like, well, are you going to stay here
or are you going to go forage for dirt in the desert?
It's like, well, I mean, I might as well stay here, you know, given that
there's literally nothing I could do with my entire life to change this circumstance.
He's going to be there whether or not I go to the leave or not.
And am I an asshole for deciding that?
No, OK, because no, well, no, no, like, like, like, I wouldn't say you are.
But I think it's reasonable for some people to be because like if it's
if it's such an uncomfortable and torturous thought for them to
acknowledge that they're part of this, but I feel like they're as much of a part
of it, whether they live in the city or not.
Like, it's just a fact of nature, it seems like.
But they aren't profiting from it if they're not in the city. Right.
Oh, OK. That's right, right.
So I think there's some degree of complicit being complicit.
Well, then that kid's suffering is just going to waste if you're not if you're
not feeling good about it, right? It's true.
So if Omelettes is in a solar system with two planets right next to each other
and both of them are Earth like and full of greenery and the one planet Omelette
is magically fueled by the torture of this one kid.
And the second planet is just like Earth minus humans.
If the option of leaving means you have to go to that other planet,
you're actually causing suffering in some way in that if you're going
to live there, you're going to have to hunt animals and eat them and dig up
the dirt and build a house and start polluting the area and not live in utopia.
Right. Where if you're in the utopia and being there has literally zero
influence on the bad part of that utopia, then it's should be your moral obligation
to stay there. That's sort of what I'm thinking.
And the other thing is that you mentioned the profiteering and I wanted to clarify
that as long as the kid's suffering is a fact of nature and if it's nowhere
known or implied that the suffering and stop assuming everyone left because
and that will give you a way to alleviate the kid's suffering.
You could get everyone out from now would have to leave.
So. Yeah. But I mean, let's I mean, for the sake of making it hard,
we could say that the kids are going to be there suffering, making this perfect
omelette planet forever, whether or not you're there or not.
So it's like, well, as long as the kid's suffering, we can offset the balance
of the universe, the unilands writ large over the course of everyone participating.
If more people stayed in utopia, right?
Well, but I mean, but isn't that kind of diffusion of responsibility where it's
like, well, there's going to happen anyway, I might as well stay.
But if everyone thinks that then.
But there's no responsibility in this thought experiment, right?
That like, it's not like there's anything anyone can do about it,
even if the entire world ganged up to try and stop it, like it's just going to happen.
Yeah, they don't really.
I mean, again, they really don't get into it's not like a huge world building
scenario where they have this sophisticated, like well-defined society.
It's just it's all very, very abstract and very like Ursula does not go
into a lot of detail about it.
It's just an allegory.
It's like, well, I think it is.
And to bring this back to the last episode you were on, it's kind of like
the average citizen of Berlin going to there in the 1940s, going to their desk
job doing accounting every day and realizing that there's Jews being
gassed somewhere.
I don't know.
Do we want to make excuses for for that?
Like, well, it's happening anyway.
I might as well stay in the society.
Right.
But you can have everyone have left.
You what?
You can have an effect on that, no matter how small.
It is a non-zero effect where I think the assumption for Omelette was that
there is a non-zero effect you can have on the child, no matter what.
Or there is a zero effect.
Well, you're not allowed to, as long as you want it, as long as you want to be
a part of the utopia, you have to have a zero effect on the child.
So you can help them.
You can help the kid by leaving.
You're going to attempt to help them?
Like, could you get kicked out of a population of, I don't remember if
there's a part Germans, if one, you were to leave, would you make a
difference to all those Jews that are being gassed?
Potentially, yeah.
How?
Cause that's, so that's actually an interesting question.
Cause they're, I didn't like the original comparison because, yeah, in the,
I'm sorry, I don't want to say omelette.
What was the actual name of the story?
Omelette.
It's O M E L A S.
Right.
But if you say omelette, we all know how to say omelette and we don't
waste time.
Yeah, yeah, yeah.
That's omelette, that's fine.
And in omelettes, I liked math solution.
I'm picturing it just as the worst possible, as the least convenient possible
world, right?
Where, and I'm imagining just you're, you're forced to look through this
window where you see into another dimension that you can't access this
kid being tortured and you're told this kid is fueling omelettes or this kid's
feeling, feeling the utopia, you leaving or you doing anything, there's
nothing you can do.
You can either just stay and enjoy the utopia or you can leave.
Although it's interesting how it can still be utopia when everyone knows
it's run on torture, but that aside, the situation in Germany, you could hide
Jews, you could smuggle them, you could fudge numbers to say, yes, we've
shipped all ours out or something or whatever.
Or you could drag your feet and make the bureaucracy more inefficient.
Right.
But so there are things you could do.
Now, if your only choices were life as normal or leaving, that's actually
an interesting situation, but that, that has to change so many factors, but
what was actually possible, right?
That's a false like common, but given that, with me being contrary, and as
soon as you started saying, you know, I think that since the world is going
to be the same, you have a duty to stay.
I, I, I right away thought, well, do you have a duty to stay in Germany?
Because they're going to keep doing things whether you're there or not, and
your life is better in Berlin than if you were to become a refugee somewhere.
Jumping back to what, what you were jumping back to dust specs.
If you are willing to walk away from all the less, then I think you are one
of the people that chooses dust specs, right?
You might be able to make that comparison.
The only thing I would say is that those who work from Amalus is a microcosm,
like a very condensed microcosm of this thought experiment with a dust
specs versus torture, like to the point where I don't know that you could
actually make that claim.
But conceptually, the people who walk away from Amalus, like, I have a moral
objection to the idea that we should torture someone instead of having
everybody have dust specs in their eyes because I reject the math.
And coincidentally, that would roughly, you know, be equivalent to
someone walking away from Amalus, right?
Do you believe that in principle, there is a certain amount of distributed
suffering that could be worse than 50 years of concentrated suffering?
Probably, yeah.
Okay.
It's just that dust specs, and maybe dust specs is just not a good example
because I can just count as zero for you.
Because it basically, yeah, because it basically counts as zero.
And I feel like that's one of those things that like, and maybe I'm just
projecting my sensibilities onto other people, but it's, but at the same time,
it's like, I think of whatever the equivalent is of an extremely
minor inconvenience, you know, like you said, point zero, zero, zero, zero,
one, you know, negative utilize or utils or whatever.
If you distribute suffering in such a way that everyone's just
going to get over it in two seconds, that there you go.
Like, and done, slimed down.
Do you not think that someone with an infant lifespan couldn't get over 50
years of torture at some point?
Does that apply here?
Because, well, cause, cause, cause are we talking about human lives that,
that have us a finite span for which 50 years is very significant?
I think that as long as we're talking about more humans than can possibly exist.
Okay.
Even if every atom was a universe.
Okay.
So you're saying the perspective might be so different in that case for each
individual human that has these incredibly elevated lifespans, perhaps that
50 years is just like whatever.
That's like two minutes for us.
Not necessarily whatever.
50 years is still 50 years, but if you have a long enough lifespan,
you can get over that, right?
If 50 years isn't 75% of your life and it's 0.1%, then maybe.
Yeah.
Then, then that does change the math, right?
Then you're not talking about destroying someone's entire existence.
You're talking about inconvenience or making them as an uncomfortable as
possible for a small amount of their life, but a large amount of time.
But Matthew, you had a way to rearticulate the way that you're thinking
about comparing what you were saying earlier.
I don't want to paraphrase you poorly.
So the dust spec versus single person torture seems like a false
dichotomy because we are only measuring negative uterilons in which obviously
if we are only measuring negative uterilons, we want less negative
uterilons rather than more negative uterilons.
But in this case, would the 50 years of torture just be so exponentially
more that it's not even comparable to dust specs, even if you multiply it
by this ridiculous number?
Well, I mean, that's, that's not possible for talking about numbers, right?
At some point, unless you're talking about literal infinities, if you multiply
a small number by a large enough number, it gets bigger than a certain other number.
Which is where my argument of life worth living metric comes into play.
If I prefer any number of lives worth living over making one life,
not worth living, then that counters you'd see his argument about dust
specs versus torture, because dust specs is very easy to live with.
I have a way that I was trying to think of how to flip this.
Does anyone know if Zimbabwe dollars are still are worth literally more than zero?
Wasn't it like they were so much less worth in the paper they're printed on?
Was it Zimbabwe dollars or what was it?
I know some currency crash last year.
I wanted something way less than American pennies.
But there have been a number of currencies throughout human history that
have gone to the point where the paper that they're that they're printed on is
worth significantly more than the currency that's printed on.
Wasn't there something just like a year or two ago where that currency collapsed?
Let's let's imagine one where it's worth a thousandth of an American penny.
Just the tangent I remember hearing not too long ago about a place where people
would rush the supermarkets and try to get and stay in front of the guy who's
putting the prices on things because inflation was so rapid that they would
have a guy going around constantly putting the price of things up throughout the day.
And if you got to the thing before he came around to put the higher price on it,
you could take it to cash register and pay the lower price.
You would think if it's that bad, they would just go back to the barter system full stop.
I've probably a lot of it.
I'm assuming that there was a big gray market that wasn't a barter as well.
But it was a podcast that was specifically tackling that econ problem
and how they eventually got over it.
And it was just one of the examples they used was the inflation was well over
100 percent per day. Sorry.
Oh, no, that that was a fun thing.
This was what 19 1920s, 1930s.
Which one? The this one I'm talking about.
Yeah. No, it wasn't even that long ago, like a decade or two ago.
Oh, but not South America.
Oh, OK, yeah, not in the US then. No, no, no.
Or in Europe, there's another big crash like that.
They remember hearing about where there was like there's pictures that exist.
So I know it was not too long ago.
People came in and we all got some money.
Yeah, I believe that was by my Germany or the reasons of toilet paper probably is
where it would have been. Right.
Anyway, so I guess to give I wanted to give a reverse of the dust spec experiment.
So let's say we can give everyone on Earth, whether they have a bank account
or some way of keeping track of how much money they have.
I know a lot of people don't have like a lot of people on earth don't have money.
They barely have an equivalent of money,
but let's say they could get one extra calorie of crop that you know,
that day or something, whatever it is, right?
Or one extra calorie of crop yield.
We could increase their spending power by, let's say, 100th of an American penny.
We could do that to everybody on Earth today or we can give one person
ten billion dollars.
And the assumption is he's going to spend it on himself rather than like donate it to.
Oh, you're using positive utility.
I was trying to think of positive utility, but that's that is the difference
and that I couldn't think of one in the midst of the conversation
without losing track of what you guys are all saying.
So distribute like a like tiny, tiny, teeny bit of like money
versus one person gets ten billion that could do a lot more of that ten billion.
Well, but people getting it as a Googleplex and not seven billion.
Yeah. And I guess the other difference, too, is that it's hard to spend ten billion
dollars without it making someone else's life better.
I mean, you're buying a yacht from somebody or you're buying an island or whatever it is.
So I guess I I was trying to think and maybe I failed to generate a positive
utility example of this.
No, I believe in the comments, there was a positive utility example
where it was a one second cat video was the smallest positive Utah.
And and I don't I don't remember what the maximum Utah was like a life of
an hour long cat video.
You just you just like the entire lifetime of orgasm or something.
I don't know. Yeah.
Basically, you have a harem of cat girls X and that every sexual need, basically,
for your whole life, just jumped a wire heading.
It's simple at that point.
Just, you know, don't just inject a bunch of dopamine into the brain.
I mean, not really.
But you know, so let's say you could just be the very best at the thing
you like doing most because that's an actual orange pill in his.
I liked you take the orange pill.
Oh, sorry. Yeah.
OK, I don't want to go off of too much of a tangent, but there's this thing
that Scott Alexander wrote about with the pills.
It's like this little fiction thing.
You know those memes where it's like, which pill would you take?
And the orange one was like master every human skill.
I would like 100 percent take that one.
It's awesome.
Anyway, wasn't one of them like superpowers.
Anyway, we'll we'll figure that one out later.
Maybe they were very limited in scope.
We can argue about that next time, because I'm sure there's some way
to munch in one of the superpowers to be way better than anything.
There is a bad munch getting the superpowers.
Yeah, exactly.
It's got Alexander made a very good case for orange.
But anyway, so I agree with him.
I was just trying to think of a positive
to example to maybe pump our intuitions the other way.
Right. Sure.
Sure.
So is there any level of how of a number of people that could exist
seeing one second of cat video that would equal a lifetime of perfection?
Yeah, 40. No, I'm kidding.
I like I like cats more than average person.
So I'll pick something like 46, 46, I'll pick something less.
You know, like it could be like just an extra tiny bit of sweetness,
something they ate or something, right?
I don't know.
Is that one second of cat video improved someone's life so much that the
I mean, it's supposed to improve it minutely, right?
It's relative extremely minute improvement.
Yeah. So I mean, I'm the example of the person
whom a dustbeck would kill in the in a reverse experiment, right?
Because I like cats that much.
Not really, but I'm just ruining the thought experiment because I'm bored.
Would you rather be that many people?
One of them would just be the dustbeck would be the worst thing ever, right?
Would you rather have a happiness specs thrown into everyone's eye exactly
one time versus this one dude just getting smothered in happiness dustbecks?
In this example, my intuition is pump the other way, where I think, you know,
I feel like dispersing that happiness is better.
But if I have actually said it and do the math like I did with the torture
problem, I'm pushed out of the way, right?
There's one person who's actually I would assume the other way around,
but that's theoretically would go for the big 50 years one in that case, right?
In terms of positive utility, I mean, I would right now my answer to that is probably us.
I'm digging up a it seems more appreciable.
It seems to be a more appreciable benefit than some extremely minor like,
oh, that was cool.
Okay, I'm done thinking about a cool that was in the meantime,
you're going to have other like happy and bad things happen in your life.
And comparatively, those happy things, those happiness spikes that you have,
I mean, they're not continuous, but that is a consistent position, right?
Because yeah, the total happiness is more when you have that tiny bit of sugar in your tea.
And so that would be the correct answer to choose.
And yet we're still going with the one with the one person getting all the happiness
because we can see that that effect.
I wouldn't go with the tiny bit of sugar spread out among everyone.
Yes.
Okay.
It's easier, more intuitive to go with Stephen's argument about the dollar amounts.
Like I can't help but think that in all these cases, like unless you know the specifics of
how it's going to affect people or what's going to change,
then it's kind of hard to just make a call.
Like this is always true.
This is always false.
You know, like I would always go with this answer.
I would always go with this other answer.
So like we could, you could just make it as abstract as possible to make that really easy,
right?
You could just say, everyone on earth gets point zero, zero, zero, one plus one or plus,
you know, that that percentage of utils or one person gets a billion or a trillion or
whatever number you want to throw at it, right?
And so then then that way you don't even have to say, well, what about the possibilities
like for sure, or dust bags?
You just think about actual utils, right?
So this relates to a kind of fun thought experiment that is a kind of challenge to utilitarianism
that the author of the webcomic Saturday morning breakfast cereal independently invented called
the utility monster.
Wait, Zach came up with that independently?
Yes.
I'm really surprised because he's so well read.
I would assume he would have run into that.
Was this early on in his career?
I'm pretty sure he was on an episode of rationally speaking and Julia asked him about that and
I'm 90% sure he said he thought of it by himself.
Anyway, so the idea is that you could say one person on earth is just for whatever quirk
of nature just way made way happier by things than the average person is.
And so if you're going to go by like total amount of happiness, this one person, their
utility range accounts for like a third of happiness on earth.
If you go by how much pleasure they can get out of things, like them finding, you know.
It takes someone $10 to be a little bit happier.
This guy for $10 feels like someone who's won the lottery.
Sure.
Yeah.
Isn't that kind of like why you abstract the utility?
I mean, that's what you're just talking about.
Yeah, but I'm getting at just another fun little way of like, so one of these,
this whole problem is just like, you know, do you go with like totals or averages or...
If you're trying to maximize total utility in the universe, you give this monster all
the utility there is.
Everyone else lives in absolute poverty because their poverty is, it's not worth giving them
an extra $10 because no matter who you give the extra $10 to, this guy enjoys it so much more,
you get more utility in the universe if you give it to him.
Right.
And it is supposed to make you make that look because everyone is like,
that's fucked up.
Fuck that guy.
Yeah, I guess sounds like a real asshole and I don't even know him.
Well, but it's not even...
And I even applied to gender.
I assumed as gender.
I'm a shit lord.
So it's, the idea is that this person is just capable of so much more.
And this is actually not inconceivable for like, say, designing minds from scratch, right?
You could design a super intelligence that its range of experience just puts it way above,
like it puts us, it's like relates to us in the way that we do to ants, right?
Well, I mean, I love that you brought that up because I like the argument that we have
met the utility monster and he is us, that humans have decided that we are capable of
so much more intellectually and emotionally in our lives are just so much richer and more
important that we don't care about the other animals really.
They don't count.
Well, they came in new algebra, I mean.
Right, exactly.
The few utils that, you know, a chicken could get throughout his entire life is we get
more utils than that from eating that chicken by an order of magnitude.
So it's totally worth it.
So many vegetarians disagree with that math and that's why they're vegetarians.
But the point could be taken.
And I think that defeats the idea of the one utility monster.
Like it's not one super species.
Or a species of utility monster.
Yeah.
So like there's a difference between that and like just one person hogging all the
utils, right?
So like at least it's divided.
And this is actually a really, this leads into another really fun thought experiment
by Derek Parfit from his 1984-86 book called, of reasons and persons I think it's called,
but now I'm hedging on that.
Anyway, it's the idea of the repugnant conclusion.
And he can daisy chain you along this path where you could say,
I'll try and be brief, but it's really worth diving into.
The idea is that you could say, all right, well, we've got a world populated entirely by people
who are just as happy as possible.
One million maximally happy people.
Right.
And then you could typically get people to agree the way that he lays these things out
that if we introduce some people who are 9.8 out of 10 on the happiness scale to this world,
now there's 1.5 million, well, that's a better world.
There's just, there's more happiness, 9.8 is good, 10 is the best.
And then you can get it to kind of where you just slide the scales down,
where say you get people who are at, you know, you get half populations at six,
another population, the other population is at eight,
but that's still better because there's more of them.
So it does take a couple of minutes to get all the way into, but it's a fun,
and I've mentioned this like three times,
we should just get into it at length later,
but I'm not feeling completely collected right now.
I've heard the same thing.
It's a great thought experience.
Like if one million extremely happy people is great,
two million almost as happy people seems like pretty good still as well,
maybe even better, right?
Is that conclusion that there's a certain level of happiness?
No, you keep adding people, you keep doubling the amount of people,
and every time you double the amount of people,
you decrease the total happiness just a little bit.
Is there a specific line where you stop because it's not worth it?
Right, exactly.
And at some point you get down to very miserable people,
and you can double the amount of people that there are,
and they're slightly more miserable,
but they're still on the positive side of the scale.
Ultimately, you never get to the negative side of scale
because at that point you say, let's stop making people.
But as long as you keep reducing the average happiness of any one person
by a little bit, but you double the amount of people,
you increase the total utility.
And they have no effect on the previous people.
The idea just really quick is also that these people still have lives worth living,
that it's not like they're barely subsisting,
or they're suffering their entire lives,
they're just, things are okay.
There's fewer TV shows to watch in one version of this.
Food tastes a little less sweet.
Things just get a little bit worse,
but they still have overall good lives.
The republic conclusion is that you could end up with
by days changing along this line.
I mean, if you just jump straight there,
then people kind of aren't willing to jump,
but they're willing to walk,
and they're willing to say, all right, cool.
So I'll take a world with 10 billion people
who are only kind of happy for the most part,
as opposed to a world with 1,000 maximally happy people.
So you have a million people with happiness level eight, right?
Why does adding more, why is adding more people better outside of more utility,
which is kind of an abstract measurement in the first place?
It's like, so why even add people?
Because if they don't exist, they have no...
I'm going to ask you personally,
does it seem kind of like a dick move
if there's a million people on the earth
for you to say, I don't think I'm willing to take
a tiny decrease in my quality of life
so that there can be another million people
having this wonderful life?
I mean, would you be willing to take
just a tiny decrease in your quality of life
so that another million people have great lives?
Well, certainly.
But in this...
And then you just keep repeating that step at Nazion.
Oh, right. No, no, like I see where you're going.
I'm just thinking in a very specific situation
where it's like, here we are, snapshot of this world.
We have a million people.
We have to make more people in the first place
to elevate them to a certain level of happiness
that is an average of everyone in the world
or whatever it is that...
If there are a million people at level eight
and you add another million people
and you decrease that to like 7.5 for everyone, right?
Obviously, you'd be like, oh, yeah, that doesn't sound bad at all.
But I mean, I'm just thinking in terms of why...
Where do the extra people come from?
Why would there be more people?
I don't know.
They teleported in from the dimension of non-existence.
I mean, they're there for the thought experiment to exist, right?
Yeah, yeah.
Kind of like where do these aspects come from?
Sure.
Oh, yeah.
I mean, that's why it's called the Repugnant Conclusion
because eventually people are like,
well, that's a shitty conclusion.
I'd rather have the first one.
Yeah, yeah, yeah, right.
You're very slowly raising the temperature
and decreasing the temperature.
But every single step along the way makes perfect sense.
Oh, yeah, the baby steps make sense, of course.
It's really not very contentious, right?
Because it's just...
That's why it's one of my favorite ones.
It's not one of those ones that you can just shoot a hole in
if it's delivered right.
I think you have to see it ahead of time.
You'd be like, oh, you're going to keep doing this.
Like after the first step, you're like, oh, yeah, totally.
Totally do it.
And then the second step, you're like, oh, he's going to keep doing this.
And eventually, I see where you're going with this.
Oh, I'm on.
I see you.
As long as the lives are worth living,
as perceived by the people that are living the lives,
I would assume that it should go on indefinitely
until you get to the point where it's not worth living and then stop.
You're taking that bullet then.
So you're going to write it all the way to the bottom
and say lives that are barely worth living are...
If you ask the people at the end of their life,
would you rather have never existed or have lived this life?
And they're kind of like, meh, I guess.
As long as it was on the yes spectrum?
Fair enough.
So yeah, you're going to write that slope all the way to the bottom.
I like it.
Right, but I'm going to stop at that point where the bottom is.
Yeah, I think that's where you're supposed to stop.
And on the other side of that bottom is being tortured for 50 years.
Right.
At the other side of the slope, you get dustbecks
and then you go all the way to Erum being tortured forever.
Well, you go from your 10 out of 10,
but the people after you are 9.99 because they're getting dustbecks.
Right.
Fair enough.
Aren't any of you raised in the Christian background?
I know you were.
Yes, vaguely.
I was definitely...
Kind of Catholic, but I never really bought into it.
After I learned of the utility monster, I kind of had the intuition
that it feels like God is the utility monster.
He matters so much that all of humanity doesn't really matter.
We're here just to praise him and feed him mutils
because he's the only being that matters in the universe.
I've also heard the same thing about a singularity.
It depends on what kind of singularity God you're trying to make,
but I also don't like using the word God for the AGI.
But as far as the God of Christianity,
I think that's kind of the point in that you get a large percentage,
with the Christian God example.
I think that's kind of the point that God does matter that much.
That's why you can't indulge in your petty pleasures like masturbation
because look at how much that'll make God sad.
And his happiness matters so much more than yours.
I'm pretty sure Christians would argue that you cannot make God sad.
In any case, we're getting kind of off track.
Yeah, do we want to listen to feedback?
Do we have any?
Yeah, I got some.
All right, let's do that really quick.
Nice, you guys can chime in if you want to be fun.
So Eddie says on our Punching Nazis episode,
the Superman radio series Clan of the Fiery Cross
is well worth listening to if nothing else than for the old tiny radio ads.
It was hosted on the Superman homepage for a while,
but I couldn't find it.
It is easily found on YouTube.
Thank you, Eddie.
All right, econ detective on our signaling episode says,
in response to the idea that cash is the ideal gift,
I have an alternative theory.
When computers are trying to solve complex maximization problems,
one feature many algorithms have in common is randomization in the early stages.
Without randomization, they tend to get stuck at local optimum
while missing out on some other global optimum.
In our consumption decisions,
we're trying to optimize across the space of all consumption bundles
with a limited knowledge of what's available.
Consequently, we might get locked into a local maximum
without discovering some better option that exists out in the world.
Consider the case where someone discovers a lifelong love of painting
because someone bought them art supplies as a gift.
The value of that discovery could be valuable enough
to cancel out hundreds of useless gifts.
So my theory is that it might be worthwhile
to put up with many suboptimal gifts
because the random factor they add to our consumption decisions
actually helps the discovery process.
I can see the benefit to that.
And it does kind of, it would make Christmas kind of fun.
I could give my entire family 20 bucks each or whatever.
Or I could go out and just throw an arrow
and just like grab something random off the shelf
and be like, you're getting a beach ball.
You're getting an air horn.
I don't know, whatever it is, right?
So I think the random discovery thing
is actually really important.
And I love that he brought that up
because now I am feeling much better about gift giving.
Perfect.
Random discovery is really cool.
I, uh, at least two times in my life,
I have decided that I need to change something.
And so what I did was just with some minor exceptions
for things that are really just obviously terribly stupid ideas.
Anything that I get asked my answer is yes.
Can I have 20 bucks?
I'm not in one of those things.
Damn it.
That's basically a random number generator for life events
because you get asked things at random,
especially if you're on Facebook every now and then,
someone would be like, oh my God,
can you drive, can anyone drive me to the airport
or some shit like that?
And I'm like, yep, right here.
And it has led to experiences that I never would have had
and thrown my life on very different directions.
And I like it.
I strongly recommend everyone try some random number generation
in their life at some point.
You're also choosing which gifts to receive in that example though.
Which gifts to receive?
Um, the gift of being stuck in a car
driving someone to the airport?
Oh, no, I mean, it was literally policy of
unless it is something that could be severely harmful for me,
I don't know, do drugs with me.
Would you help me bury a body?
Anything that isn't that massively negatively impactful
on my life, I just say yes to.
If it's something that I would really hate
and sound shitty as fuck, I'm like, whatever.
I say that.
I'm saying yes to everything, so I'm saying yes.
So that's what you mean by RNG is saying yes to?
It's like-
All of the things.
No, not necessarily, but I mean,
if that is one great way to make an RNG in your life,
no matter what you hear, say yes to it.
Even if you're sure it's something that you will hate,
say yes anyway, because fuck it.
I'm actually convinced by what you just said
and I'm definitely going to try that.
Because that sounds, I don't do a lot.
I pass up opportunities to get out all the time.
If I just did a month or I'm like,
I'm going to say yes to everything.
That's convincing to me.
I'm going to give it a shot and I'll report back eventually
when I get around to doing that.
Yeah.
Awesome.
Hey Steven, can I have $20?
I haven't, I haven't started.
Will you let me know when you have?
Don't let me know.
I haven't started, so I'm going to say no.
Yeah.
Call me.
Ironclad, you're not falling for anything.
Yes.
All the list of things that I'm not going to do,
it's going to be everything Matt asks.
Do we have any-
I think we had a few more.
Sure.
When you and me and Katrina were talking about this,
how the term social justice warrior,
I thought wasn't negative at first.
You didn't- oh, like whether it's like kind of pejorative?
Yeah, yeah, yeah.
It was my impression that originally it wasn't,
because I liked that term,
but it turns out I'm kind of a stupid head or something.
Before people more often I think use it as a pejorative,
but like some people do where it is a badge of mind,
like yeah, I fight for social justice.
Anyone who-
I'm a social justice cleric.
Anyone who would pejoratively be called a social justice warrior,
that person would likely not have called themselves that.
Oh, a challenge.
I feel like a lot of people you can get online
where they can be like, I'm doing social justice,
that's why I'm harassing this person.
It happens sometimes,
because people like to do the whole like, you know,
signally like, oh, I'm a social justice paladin.
You know, they'll make like the,
they'll throw in other D&D classes and stuff,
but that's kind of like jokey too.
I would like to use myself as an example,
because back when I was more into social justice,
before it became what it is today,
I actually did use the term social justice warrior
for myself, because I was like, yeah, that's a good term, right?
We're fighting for social justice.
Why wouldn't we be social justice warriors?
It's because it's done off,
it's an offshoot of like keyboard warrior, you know?
Yeah, exactly.
Which you talked about in the previous one.
Yeah, okay, so I didn't actually listen to that one,
that was just...
Yeah, so it turns out that it was actually always pejorative.
I'm just dumb.
Well, I mean, you took the obvious interpretation,
just not the popular one.
I guess, kind of like the same way that I thought,
when Neil deGrasse Tyson was saying that
7% of scientists still believe in God, and...
It's probably more than that, isn't it?
Well, no, no, 7% of at the very highest level,
at the National Academy of Sciences.
Gotcha.
Still 7% of those believe.
And my interpretation was that he was saying,
what the fuck is wrong with these 7%?
When it turns out he was saying the opposite,
that as long as these 7% believe,
you still got a problem on your hands, atheists.
And I like my interpretation better.
And I don't necessarily know if that's his interpretation.
I think what he's saying is that,
you can be as versed in science as you want,
but that doesn't mean that you're necessarily going to be an atheist.
I don't think he's saying checkmate atheists.
Yeah, yeah, yeah.
I don't think he's even saying check atheists.
I think he's just pointing out that
some people can literally call it business.
Check yourself.
It's more like check yourself, atheists,
rather than checkmate atheists.
Yeah, I interpreted completely wrongly is what I'm saying.
I kind of have a record of interpreting things
that make me happy.
I think most humans do.
Yeah, okay.
It's called bias.
Yeah, yeah.
I have a pro happiness in my life bias,
and it's not served me well.
I have similar issues that we can dive into another time.
Okay.
I don't have to be super gullible.
All right, so we'll dive into a quick example.
Like when I was a kid,
I remember watching on TV some reenactment
and I didn't know it was reenactment.
Someone was being attacked by a dog or something.
It was something that just happened to me on the TV
and my parents didn't screen what was on when I was in the room.
And I'm watching this and I'm like, I don't know.
I was under seven because I think I vaguely remember it being
in my first childhood home when I moved when I was six.
I'm watching this and I turned to my mom and I'm like,
why isn't the person in the camera helping?
Why are they just watching and getting this on tape?
And she's like, they've recreated this for TV.
They're dramatizing it for people to just show a story
of what happened.
And I was like, oh.
And the idea that they would fake it never occurred to me.
I also used to think that when they would do flashbacks
to childhood and TV shows,
that they actually filmed that scene with that actor 20 years ago
and then waited 20 years to film the rest of the movie.
Damn, David.
Like the boyhood movie.
I guess I have this weird compunction for like
assuming that there's no trickery involved.
Yeah, I've been very gullible.
What you see is what you get.
Yeah.
Child logic.
Yeah.
I mean that is child logic,
but even as an adult, I tend to believe people way too much.
Even sometimes when people are just joking,
and I'm like, really?
That's fascinating.
And they're like, no, you dumbass.
You're just not snarky enough then.
I guess.
I'm getting more jaded.
So, hey.
Yeah, we're welcome.
Whereas now we have to fact check everything.
Everything you read is just like, yeah,
I don't know if that's true.
I don't know if I believe that,
no matter how plausible it may seem.
But Econ Detective also says on the SJDW thing,
the way I understand it,
SJW isn't a blanket term for everyone
interested in social justice,
but a specific term for the people
who violate liberal norms of political discourse.
So they're more focused on the warrior
than the social justice?
Right.
As in they're being racist by hating on white people?
Or they're making extremely emotional arguments,
or they're like, they're pro-violence,
or something like that?
Or like, I've seen like a, you know,
again just like picturing the extreme examples
just to make it an obvious case.
Like I saw things picking on the person who played Luke Cage.
Luke Cage, okay.
I think he's married to a white woman,
or dating one or something when this show came out.
And people were like, oh my god,
how could you turn on your people?
And these were like social justice warriors.
What?
So like, you get this horseshoe of people like,
on the KKK.
Those are like the extreme worst people.
Yeah.
So like, but what I'm getting at is that
at the extremes, they tend to share the same bigotry.
Yeah, they're against interracial marriage,
just like racist slave owners.
Right.
Yeah, well, like I think a lot of the problems
with that has to do with,
and I'm gonna be very brief about this,
is that a lot of people conflate the idea of systemic,
like systemic racism or systemic sexism
with like individual level sexism or racism,
and that cuts in all directions, right?
But they conflate those two.
So they're like, oh, whatever,
cry me some more white tears, or you know,
even if they're talking to one person
that they're insulting or berating a lot, you know?
And that's kind of like, well, you're directly,
you know, dealing with one person here
that you are being that way.
So yeah, his entire paragraph is basically examples,
very much like you guys just made.
So I'm gonna skip that paragraph,
and move on to the summary that social justice warriors
are people who think their political and social goals
are so important that they need to jettison
the idea of living peacefully in a society
with people who disagree.
In other words, punchin' notties?
Yes.
That would be among, yeah, among one of them, yeah.
Yeah.
Among many others.
So the guy on Tinder that was talking to Katrina
was trying to lump all people who have similar views
about gender into the category SJW,
which is a little like comparing
all Second Amendment people to Timothy McEvoy.
Mm.
Mm.
It's a fun comparison.
Yeah.
Also goes on to say it's really hard
to have consistent definitions of anything
with respect to the culture war
because people will try to bend definitions
to make their friends seem better
and their enemies seem worse.
It's probably best to avoid buzzwords
like SJW, alt-right, MRA, etc., all together.
Yeah, I think you could just stop saying
social justice warrior and call them a regressive left.
Yeah.
Or just an asshole.
That's a word that we can all agree on the definition of.
Well, yeah, some in some of them are.
I think it's good to point out regressive left
as meaning they're on the left,
which generally aligns themselves with progressivism,
but they're asking for regressive options.
I'm drawing this horseshoe in the in the air with two hands.
And I think the regressive part is when you.
Because when it curves back.
Is when you get curved in right next
in your bumping shoulders with the KKK, right?
No, I think that's an important distinction
because it's like, man, you guys are kind of operating
on the right wavelength, but you're doing it so badly.
You're doing it so wrong.
If the things they're crying for were taken literally,
in other words, if what they argued for became policy,
it would actually have a negative effect overall.
And it would be non-progressive.
Exactly.
Like, I mean, we talked about like none of us like it
when Richard Spencer opens his mouth and says,
you know, asshole things, but none of us have.
We can't just punch him.
None of us are prepared to hit him over it.
So that's what you're trying to get at, right?
You're basically saying, well, then we if we kept that law
and sent that law back in time, we'd lock away Galileo.
Or took it forward and we'd lock away
really anyone who ever made the law does a read with, right?
Right.
So that's that's sort of the problem.
Anyway.
On Reddit, Zeke Aaron says,
if you want to have something to remember a charity gift with,
you could always print out the receipt
and fold it into a small paper crane.
Everyone has a shelf somewhere.
Eventually you might have a whole shelf
of origami representations of donations.
I thought that was a wonderful idea.
It's cute, but you'd have to have enough different shapes.
Otherwise, you have 10 cranes and like I don't remember
what different colors these are.
Different colored paper.
Yeah, they make different colored and patterned
origami paper.
I mean, you can't.
I'm just like, I love the idea of every of having a shelf
of cranes and every single crane is another donation
that someone made for me to a charitable cause.
And like that would be really cool.
I would like that.
Oh, was that for donations or was that just for any gift?
I missed that.
No, no, no.
It was specifically for charitable gift donations.
Oh, I think that's awesome.
Donations that you've given?
No, no, no.
No receipt.
I've given to you.
Oh, okay.
Hey, instead of giving you a $25 gift card to Walmart,
I donated $25 to the ACLU and here's the receipt.
I printed it out.
Sure.
You could put that up on the wall and frame it,
but that would be kind of tacky.
Yeah.
Or the cranes sounds prettier.
Yeah.
Yeah, no, I got you.
Isn't there, I only, my only source on this
is the Ghost in the Shell series,
but isn't there some sort of old, not superstition,
old story that if you fold 1,000 origami cranes
in your life, you get a wish?
That's certainly outside of gifts.
That's not, there's a short story.
Sudaku and the thousand paper cranes, she has cancer.
She's this girl who has cancer, she has leukemia,
and she dies before spoilers.
She dies like two cranes before or something.
That's some really small number.
Okay.
I probably got her name wrong.
Is it 1,000?
That's not a lot.
I thought it was.
I can do 1,000 before I die of leukemia pretty quickly.
It might have been.
It was 1,000 Ghost in the Shell.
I'm assuming it was probably 1,000 in the story as well,
because it sounds like they took it from something
that already exists.
Yeah, it's like a short story, I'm pretty sure, right?
Yeah, I think I heard about it before Gets was an anime.
I don't know, 1,000 is a fair amount.
You could do 1,000 in your lifetime,
but it wouldn't be fast.
Even counting to 1,000 is a pain in the ass.
Well, I think she was doing it herself
and her hand, she was in a lot of pain all the time,
so it was laborious and very difficult.
It was like, you know,
but she had to be the one to do it,
otherwise she didn't get her wish.
Okay, so if we just gave her cyborg arms, she'd be fine.
Yeah, but I mean that's-
Well, like the Ghost in the Shell,
that was the thing, she was still learning
how to use her cyborg body.
That's why I-
Okay, I totally plan to have a shelf
of origami crayons someday.
I think that's a great idea,
and I thought that the original proposition
was just for gift receipts.
Like you said, you don't remember what people give you,
because it's mostly garbage.
So it's like, well, a shelf of garbage paper
isn't going to help either,
but of knowing, being able to look at it
and be like, oh, this is all life-saving money.
That is a great idea, I'm very much in support of it.
Yeah.
Elk12429 says, on the subject of gift cards,
I remember hearing about a psychological phenomenon
that helps it to make more sense.
The anecdote I heard is,
a father wants to buy his daughter and her husband
expensive tickets to a baseball game,
but it's too close to the date of the game
to get the tickets and have them shipped to her.
So he transfers the money equal to the amount
of the cost of the tickets to her,
with a note saying, use this money to buy the tickets.
A week passes, he calls her up,
turns out she didn't buy the tickets with the money,
because once the money was in her account,
the tickets seemed too expensive to use that money on,
even though she'd received the money unexpectedly,
and she thought she'd just keep the money instead.
This phenomenon is used to justify gift giving,
as opposed to giving cash.
It lets you buy something for someone that increases
their utility that they would not have bought themselves.
And gift cards are a useful middle ground.
Giving a person a gift card to a specific store,
rather than cash, gives them a restriction
on how the money can be used,
that enables them to bypass the,
I can't justify using money to buy this thing,
even though I would enjoy having it filter in their brains.
That's a compelling point, if that's what you're going for.
For me, if I was given $50 gift card to GameStop,
I'd go buy whatever video game.
If I was given $50 in cash, I'd hold onto that,
because $50 right now would be awesome.
Not like, you know, but it would just be nice
to have like an extra $50, right?
So, I guess depends on what the gift giver is going for.
Do they care more about like, what you actually want,
or what they want you to have?
And if they want you to have another video game,
but they don't know which one or whatever,
then get them the gift card.
But if they just want you to get whatever you want,
or be $50 happier, they could just give you cash.
And you could be one of three people.
You could be scrounging so much
that it's almost insulting to receive $50 for GameStop
when you could spend it on money so that you could survive.
Or you could make so much money, $50 doesn't matter either way,
and being given the game would still barely matter.
Or you could be the majority of people in the middle
where you're not dying of starvation,
and you could definitely use that.
But maybe you wouldn't use it for the GameStop product.
Yeah, and I think that's where this is targeted for,
is that large middle section.
And I think that makes a lot of sense.
And you should buy the Steam gift card anyway.
Right, assuming you own computer games.
Like GameStop is good for console games, but not so much...
The key is that they know enough about you.
This is downloadable.
The key... I'm sorry, I didn't mean to interrupt,
but I think the key is that they know enough about you
to get you appropriate gift cards.
I heard you like video games, so here's a gift card for GameStop.
It's like, oh, I do all my gaming on Steam.
Then it's kind of a waste, right?
Then they've missed fired,
and then you have to find some way
to exchange that for money that you can turn into Steam money.
I just wanted to clarify.
I'm not keeping a list of what constitutes
the Ubermanche in my mind.
One of them is HasRed, those who walk away from the malice.
The second one is Use a Steam rather than GameStop.
The third is Facial Hair.
Software delivery system.
Exactly.
One of us has a full beard right now,
so that's the person who said that Facial Hair counts for Ubermanchery.
The other one on the couch has some sweet sideburns.
Mudden chops.
Got the chops going.
Where the other half are...
Baby-faced.
Well, we've got one clean shave and one baby-faced.
Yeah, I look like...
Lomp, lomp.
Yeah, I look like a child.
I got carded for a rate at our movie last year.
Oh, really?
Yeah.
That's amazing.
Wow.
I'm 27.
Dude, I don't...
Hold on, hold on, hold on, hold on to that one.
I don't know theaters that card people,
unless they're literally children.
They're like, you look like you're 14, but whatever.
I was carded at 18.
Maybe they were just being funny,
but I haven't been carded since I was too young.
It's not like it's a law.
That's someone who's underage into a rate at our movie.
There are no legal repercussions.
Well, there's the potential repercussion of getting fired,
or whatever, right?
So there's those repercussions.
They're not going to jail, but yeah.
Yeah.
What's the likelihood of managers going to look on someone
who's serving you and be like, oh, you're eating carden?
I don't know.
I mean, maybe that...
I think it's about preventing angry mothers from yelling at you.
I think...
But it's entirely possible.
I don't look 10.
So it could be that the person behind the counter was just having fun.
I don't know what it was, but it did stick out.
I'm going to go ahead and absolve your concerns.
You don't look like you're 10.
Great.
Did it look like that was their first week at the job?
I honestly don't remember.
I just remember looking at my ID and I'm like,
seriously, how do you think I got here?
Like, you know, there's two of us.
You think that...
Whatever, it was just a thing.
She's not my babysitter.
Right.
Unclog mission on Reddit just a few hours ago,
says, regarding our EA episode,
you got at least one person, me,
to finally take the giving what we can pledge
after much needless procrastination.
Nice.
Sweet.
Good job, dude.
Sam Harris did that for me.
Oh, yeah?
Yeah.
Fantastic.
Well, fuck yeah.
That's awesome.
So that's exactly what I was talking about on the show,
was that on the one hand, there's this societal norm
where soliciting your gift giving and how charitable you are
is seen as like dickish somehow.
But if you post every time you send a donation,
you post the receipt to Facebook
and text it to your entire family or something.
But there's...
I think there's a fine line to walk between,
like, I guess being in everyone's face
and about it all the time
and just kind of doing it super quietly,
never telling anyone unless they ask or whatever.
But the idea is that, you know,
if you can break the taboo about talking about it
and get more people, you know, engaged,
again, we've got one more person doing it,
even like a day, right?
That literally adds up to a percentage of a life saved
or possibly an actual person who's not dead, right?
Yeah.
So there's like, Patrick Ruffus had like a world-building,
like, stream-eated ones
and he started talking about Heifer International
and he made a really good case.
He went into detail about what they do and everything
and I thought it was so cool that I was like,
okay, I'm gonna give him some of my money
and forked over some money and it was like...
I gave $100 to Tostan after reading
Ozzie's blog post on Thing of Things
about how it helps prevent female mutilation
and seemed pretty effective.
Uncloggin also says,
late in the episode, you brought up money priming.
The finding that subtle reminders about money
can prompt people to become more selfish
and not donate as much to charity.
One of you, that would be me, asked about replication.
In fact, this effect has not replicated in well-powered studies.
Good.
It then provides several links, two links specifically
that demonstrate and discuss this repeated failure to replicate.
The reason I ask nowadays about replication
for social study, for social research
is because there have been at least a few people
who've said we should start considering the year 2016,
year zero for social research
and anything that happened before year 2016 asked,
did it replicate?
Because there were so, so many things in social research
that someone managed to find an effect once
and published it like crazy and blew it all out in proportion
and no one bothered to replicate
and now that people have started trying
to go back and replicating, they just don't replicate.
One of my favorite ones was the finding that
if you give someone a warm cup of coffee to hold in their hands,
they become warmer emotionally than people are talking to.
Whereas if you give them a cold drink to hold in their hands,
they become more distant and cold.
I really liked that one and I repeated it several times
and then it turned out several years later
they tried it again and it did not replicate.
Sounds like something Socrates came up with.
It does, doesn't it?
Well, it also seems so.
The thing is there was actual research to support this,
data and everything.
It's just that it was one of those data mining
we managed to find an effect by coincidence
and it is this huge thing that we make a big deal out of
that is why everything needs to be replicated
and anything that hasn't been replicated,
I always take with a huge grain of salt
and always ask, has it been replicated?
That's a really important point.
I'm glad that they brought up that this hasn't been replicated
and I didn't actually read the primary research on this.
I had heard it repeated by lots of smart people
and the fact that they all took it and ran with it,
I assumed that there was something to this.
I think smart people like,
I think Will McCaskill and Peter Singer
and I can't guarantee that it was either of those
but definitely in the philosophy literature
about effective altruism.
There's been at least a couple studies
that Eliezer cites in the sequences.
Some of them strongly in one case
which did not replicate more recently.
Did the comments point that out?
No, because I don't think that was a thing at the time in 2007.
People weren't really thinking about that.
It's been more something that's come up more recently
and so yeah, I also have been very often in the past
repeating things which it turned out later did not replicate
and that is why nowadays I kind of have this aversion
always ask, did it replicate?
Well shit.
Because I feel really bad about the things
that I spread in the past.
I think Scope Neglect is replicated among enough things
because now I'm thinking of the one that I brought up tonight
about that girl and the brother and family.
That's the one specific one I can think of
but there's also the one, you mentioned the seagulls
but this was the one in Canada where it was some other oil
spilled some decades ago or something.
Scope Neglect has been replicated quite a few times.
Fair enough.
But any individual study I'll be more careful about
and that is a good lesson for me to be more guarded
about what I take for granted from hearing smart people say.
I'll be more careful.
That's kind of a bummer
because now it's going to put all this extra research on me.
Right.
I'd like to be able to shoulder all off onto them but oh well.
Hopefully that won't be as much of an issue going on
since people do consider that now
and everyone is aware about it.
But still good to be careful.
Well and I mean like a lot of these things too
especially the social science like oh 40 college kids from Yale
had this happens therefore it's true of all humans on earth.
That's sort of I think been a recognized failure since
but I mean it just happens to be that you know
a lot of psychologists work at schools.
Populations that have to participate go to the school
and you don't have to pay them.
So they're really easy targets for study.
So like it makes sense that a lot of stuff is done to them
but then generalize it is to I think to be careful
but I could just hear somebody talking fairly seriously about
a study that involved either 10 people or two groups of 10
and they were talking about it like it was this discovery
and I need to find this.
I don't know if I could dig it back up
but it was just in the last couple of days
and I was wondering the entire time like this was 20 people.
Yeah your sample size is too small.
Way and why this was somebody smart.
I'm struggling and saying that I can't think of
where to place this but this did happen.
So yeah be aware your sample sizes and fail failure to reproduce.
So where do you draw the line between being global
and repeating everything you hear and being so skeptical
you have to fact check every single thing that you hear.
My typical I mean so like it depends like I realized
when you mentioned like being gullible on the internet and stuff
I'd never really had a problem with that.
I think maybe I was introduced to skepticism shortly after
the advent of internet in my house.
So I lucked out there that in my early teens
I was already kind of calling bullshit on everything
which is a great way to kind of sometimes be right
more often.
I guess it's a way to not be gullible
but it's also a way to like be a conspiracy theorist.
You contrarian.
Yeah but.
Well if you say something so contrarian
that you get a response from a bunch of people
who give arguments that you yourself find believable
then you've done something to fact check yourself.
Right I guess it's not like so much of a line
as it is a process.
You know one of it would be investigate when I can
be confident in the expertise of the person I'm listening to
if they're not an expert on it or if they have a reputation
of being outside their consensus of their field
you know take them with skepticism
make predictions ask people put on public forums
or even if it's just Facebook and say anyone hear about this
or whatever.
So I mean there's ways to do it.
It's not so much just about what my filter process is
it's also about like my process after the fact.
But like as far as things they get buried
like I don't remember why I first heard about that study
with the girl and their brother and the family right.
So if that's not true I've heard it either so many times
or from the first place that I've never bothered
to look it up again.
Any more feedback.
I have two on the punch mouses.
All right real quick.
Okay.
The mic and it is the mic says there is a false
equivalence between the German Nazis of the 40s
and the Nazis we have around today.
Absolutely.
Using the using the actions of our grandparents
in this case as a moral center is inappropriate
and hypocritical.
The German Nazis were a group of people who were in control
of a country had an army and more importantly
were committing acts of violence against many people.
Whereas today's Nazis are a group of morons
who yearn for the old days.
Despite the neo-nazis being violent they do not have an army
nor hold office nor influence law.
So I thought we kind of acknowledged that they weren't
exactly equivalent.
We just kind of referenced them in a very limited fashion.
Then you planned out there's like 200 of them?
Yeah.
Or they share an ideology.
Like Richard Spencer has the power to why he like
you can rent out a hotel conference room
like a couple times a year.
I didn't know this when we recorded the episode I found out
more recently that he runs his think tank.
The National Policy Institute that everyone keeps mentioning
that oh my god he's running this think tank
that tries to affect policy.
He runs that out of his apartment.
So I mean these numbers might be a little bigger
than the 200.
They can fill a room of 200 but someone wrote
in a really good comment and we talked about
in the last episode that hasn't aired yet.
These numbers might be considerably higher
but as far as people who are ready to travel.
Just considerably.
Somewhere maybe north of the tens are possibly
hundreds of thousands.
Unlikely to be hundreds I don't think.
But probably tens of thousands.
And I mean I think the number of these like it depends on
idiots.
I just picture some 15 year old with an unshaven child
mustache sharing frog memes right.
Hey don't bring frog memes into this.
They did nothing wrong.
So I mean I think that that probably counts
for some of these people.
But and then there's also like they don't hold office.
They don't they don't have an army.
There's the concern some of these people that people call
Nazi Esk are definitely in high office currently
in the United States.
And so that that is why there's some people that are being
you know raising alarms about this.
So I feel it's really weird to call Richard Spencer
a Nazi.
He's not part of the Nazi Party.
And as he says a lot of me Nazis hate him.
And I kind of think that's legit.
He's a white nationalist.
He's definitely white nationalist.
Right.
And and yeah like people are just kind of.
Nazi's shorthand for that.
Yeah.
Well yeah.
And it's and it's yeah you may be more to Nazism
than just being a white nationalist.
Well I think white nationalism is definitely
a important part of it.
But I think Nazism goes beyond just that.
Well and people I would agree.
I'm just saying people.
Definitely from the short hand.
Definitely from historical context.
Absolutely.
I would when people I think love using that word
because anyone that they can brand a Nazi
is therefore subhuman.
And it's a way of othering.
Absolutely.
It's a way of like not.
I mean yeah othering to a level.
You know I think we talked about this was it on that episode
where like they're they're they're subhuman
once they're Nazis.
They're not just we did.
They're not other groups.
They're they're non people.
Right.
So then it's no matter what happens to them.
It's OK.
So it's very politically advantageous to you
to label your enemies Nazis because then
if you can get people to agree then they agree
that your enemies don't matter.
But that's that doesn't make it accurate.
Right.
Albionic American says people keep missing
the alt-rights real message.
And by now I think this has to happen
because of perverse blindness.
Oh I remember this comment.
Yeah.
And the alt-rights racism doesn't really upset people
but rather it's rediscovery of our ancestors
tragic view of man.
Social progress can't happen
because man has an obdurate nature.
I don't know if that's how you pronounce that word.
Obdurate.
OK.
What is what is the meaning of the word obdurate.
Is it a stubbornly refusing to change one's opinion
or course of action.
So okay.
OK.
So stubborn.
All right.
Yeah.
Because man has an obdurate nature
that you can't reshape like clay
into the arbitrary configurations
demanded by the political correctness
of the current year in all in caps.
We have inequity hierarchy and patriarchy
because of this reality.
We have to construct the best lives we can
under these constraints.
Not long ago our ancestors would have called
this realistic view of human condition
something like wisdom.
Compared with our elites childish utopianism
which denies this reality.
The alt-right people sound more like the mature adults
in the country these days.
Yeah.
So so my dad actually told me about this book
called a conflict of visions written by Thomas Sowell.
I'm probably saying his name wrong.
I think it's like solar cell.
But it's it's S.O.W.
E.L.L.
And in the book he posits that there are these
philosophical differences between it.
They're called visions.
But basically one of them one of the visions
is called the constrained vision in which
the idea is that man is not perfectible morally.
It's like human or mankind humankind
tend to be selfish and self centered
and just smart enough to think
that they can rise above their station.
But they ultimately can't.
Thus we must rely on things like traditional
like religion and stuff like laws
and like to kind of keep people in check
because otherwise they'll run rampant
and kind of do bad things.
And then the other side is this unconstrained vision
which is basically like man is morally perfectible.
I'm abbreviating massively here.
And like we can become enlightened.
We can overcome our nature.
We can overcome our impulses
and use our rational big brain still like improve our lives.
And that eventually we're going to
delegate those kinds of these high level decisions
to experts who can kind of kick down what's best for everyone.
Isn't that literally the enlightenment?
Yeah. It's like people like I would even
I would almost say that like the rationality community
I would assume is largely unconstrained.
Very much like yes man you know
mankind can improve itself.
It can elevate itself.
But the constrained vision is very much like
in despair of humanity in terms of their ability to be
altruistic and good.
They kind of think that people are inherently not great.
People are people.
That's literally what he just said.
Yeah. It just reminded me so much of that.
Because people are shitty.
Yeah. It reminded me so much of that.
Like and I haven't read the book.
I've watched summaries on it
but that's kind of what came to mind.
I thought that was kind of interesting.
I think that is a fascinating difference of opinion.
Like I didn't even know that this was something people thought.
Yeah.
Until just recently like when he says
because of perverse blindness I dispute that
because I don't think I am being you know
perversely blinding myself.
I simply had no idea that there were people who thought that
that people are so shitty that we can't make progress.
It's and it's maybe it's not even like
oh people are so shitty.
They're awful. They're garbage.
It's more like their limits right.
Their limitations to how much we can.
I don't know. It's a perspective.
I don't think I entirely agree with it.
It's just it's like I definitely would consider myself
more unconstrained in this case.
But those that ended up itself could be a kind of a false
dichotomy of visions.
It's supposed to be kind of a spectrum
but you're supposed to fall on one or the other.
The the world view that he laid out and it's
you give out a charitable version where there's like this
spectrum but the one that the right end gave like
that seems to me to be factually incorrect.
If this is the result of perverse blindness
it's not perverse it's just incidental blindness
but I really don't think it is.
We there has been progress over the last centuries
that either you're saying either you're saying
isn't there is bad because it goes against the state of
nature or just because it's it's
unsustainable because it goes against the state of
nature but it's not I think written into the stars
or written into our genomes that we one race would
would dominate another.
It happened to be an accident of history that
some had technology faster than others
and were able to subjugate and other factors obviously
but was that your interpretation of that as well
that it seems to deny the fact that progress has happened?
Yeah well I basically agree with you that I do think
progress has happened that we don't for the most part
we don't torture people anymore we don't have slavery
we don't burn kittens for entertainment
but I guess it also kind of depends on your point of view
like there's a number of people who would say that
we have regressed and things were better before the
Enlightenment.
I've literally read arguments where people saying
the reason people call it the Enlightenment is because
they are perverse and they're trying to hold onto their own
power and they're profiting off this world even though this
world is worse so they're trying to paint it as this
wonderful thing like oh the Enlightenment saved us when in
reality the Enlightenment has brought mankind down into
hedonism and purposed less existence and has just made life
worse in general for everyone on the planet.
That sounds like a different demographic of people than the
one that that person's describing.
I mean so that that is not the alt-right position.
No that that hedonism yeah so
if that's the case fine those people are welcome to unplug and go
scratch in the dirt like our ancestors right?
Nah I'm serious.
Wow.
Like I mean I hope you won't I want everyone to be happy but
like if that's really your position cool try it for a week.
I think go outside without without clothes or bring you know or
wear whatever you can catch right like that's some bullshit.
I think they want to keep the the technological progress
but it's the social quote unquote progress that they
disagree with and say has been social regress rather than progress.
Like they want to go to more like tribal society kind of?
Progress like what like what model do you think is?
I don't I don't think that if I had to guess I would say that the
alt-right communities so far as it exists as a community is probably too
diverse to say that this is what we're support right just like the
the progressive community probably has different ideas about progress means.
You know some of us when they talked about reshaping
men to be better or something whatever the phrasing was in the
write-in some of us wanted that literally with brain augmentation with
computers and others want to do it with drugs or with thoughtful
contemplation or whatever right so being intellectual.
Yeah so like there are different avenues to go about making people better
so the saying that all the all people who are progressive want to make people
better isn't like a unified statement in so far as like there are a lot of people
want to go about that differently.
I think what his intention is is that by trying to live in this ideal world
where humans are perfect and not respecting the fact that we are
constrained by various biological impulses we make things worse.
That if we were to accept the fact that people are naturally a little bit racist
and a little bit sexist then we could work within those limitations to forge
a better system than one that ignores that and pretends that it's not an issue.
Oh I think we do do that and that's what we're doing.
That makes sense like like the way you just presented like that or like that
makes that makes sense I I'm not saying I agree that I'm saying
the way you presented it makes sense as far as what they're saying sorry.
None of you you're you're you're welcome to bask in the in the light of a steel man argument.
So but that's not the world we live in right.
We live in a world where people are aware of our implicit bias
and we make efforts to to correct for them.
We talked a long time ago about getting more female musicians
by just not looking at who's playing the music.
It's like people are aware hey you know what there's no reason for us to select
against women let's so because for some reason we keep doing it let's just make
it so we can't and then we'll just pick the best players.
We're we are acknowledging this failure and correcting for it.
Not not just acknowledging it and running with it.
And it feels like a cop out to just be like this is how humans are so we
should just deal with it we should just allow that to be the case and not
challenge the status quo which it kind of feels like that's what that's saying.
No I think the counter argument would be with that the progressive social left
believes that all humans are intrinsically equal more or less
and they see that the population is let's say two thirds white one third minority
and so they say well since that is the composition of the population
there's no reason that that shouldn't be the composition at every workforce
and so they pass laws that say if you have a college
two thirds of your students are allowed to be white one third have to be minority.
If you are a large employer two thirds of your employees can be white
one third have to be a minority and I think he's saying that
that ends up having more bad facts than it fixes
because by ignoring the other factors the fact that people are somewhat racist
and tribal and simply forcing everyone to have exactly the distribution that we
see in in the population without taking into
accounts things like racism or tribalism or just the fact that maybe
people are less able to speak the language and that makes a difference in
in certain professions that it's making the situation worse
rather than better to ignore that. So you made another really good steel man
I think you're being and so we're talking about a completely different argument now
I'm going to just put that out there that you've still made it to the point where
it's unrecognizable as this as this original position
that was much more coherent than the one that was given.
If they had said for example I feel like there are some failures
with our current implementations of affirmative action that would have been
a much more defensible not necessarily a position I'd agree with but a much more
solid defensible position but they flat out said progress is like impossible
yeah right right like if they have a criticism of affirmative action then
it's like okay well that's where let's improve it right not oh well then
throw the whole thing out because it's garbage. So are they saying that humans
without rules whether it's religion or law cannot progress and get better?
Oh I don't know was that the argument? I don't think that's what it is I think it's
I took it I don't want to be uncharitable but I took it more or less as trolling I
didn't even really think that there was a whole lot of content to that statement
that was again if I'm being what was the person name who wrote in?
Albionic American?
If you want to write in and clarify I'm receptive I'm not trying to be a dick
but whatever point you're trying to make did not shine through in what you wrote
at least not to me and maybe it is blindness but it's certainly not me
being intentional or what was the word they used perverse it's just me not being
able to tell them from but if you want to come out about it a different way we
can have a real conversation but I took it as more or less trolling I couldn't
The thing that struck me that that made me interested in the comment is when he said
you can't force people into the arbitrary configurations demanded by the political
correctness of the year which sometimes you feel that a little bit right?
You can't force them immediately so there's a difference between we can't force people
into these configurations based on what you think you should be doing and just self-examination
right like being like hey maybe this thing we're doing is kind of a problem maybe this
thing we're doing isn't great we should try to improve upon it and those things happen gradually
it's not like we need to force humans to go completely against their natures and it's this
strenuous thing and it's going to cause all kinds of agony I don't know that I like that I don't
necessarily even think that that's the argument there's kind of an area it feels like it kind of
sits in that kind of idea space I guess but that sounds like a legitimate somewhat of a legitimate
philosophical view of the world to take um but I but I did think the way that you presented it was
pretty concise overall I mean I think it's one way of putting it or at least
engageable yeah it's kind of a pessimistic view of humanity like people are going to be kind of
stuck in their ways and and like like people are in or they're intransigent they're just not
they're not that malleable so you might as well just shape society to that tendency rather than to
change society to fit these high concepts I don't agree with that but I but I understand kind of
what they're trying to say I think optimism and pessimism make a big difference in like someone's
day-to-day life I much prefer being optimistic than pessimistic but I think if we're trying to look
at something from a rationalist perspective where we actually look at society you shouldn't one
shouldn't ask is this pessimistic or optimistic one should say is this real is it reflected by
the data and the world and if the pessimistic view is a more accurate one then you should
go with the pessimistic one sure sure oh it looks like we are kind of tired we want to wrap it up
over the night yeah sure okay thank you for joining us we'll see you all in two weeks bye bye cheers everyone
later
