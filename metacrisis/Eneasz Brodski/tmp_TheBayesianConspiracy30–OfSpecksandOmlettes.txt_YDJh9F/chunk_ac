And then you imagine to actually, you know, like you said,
actually doing the math to some extent, whether you do it on paper or do it in your head,
just, you know, trying to make it equivalent.
Say, yeah, I can get a paper cut.
I can get 100 paper cuts in my body every second for the next 20 years,
or I can break my arm. Well, I'm going to break my arm in a heartbeat.
That sounds awful, right?
So then you scale it to like that.
That does kind of drive that point home that there is a trade off somewhere.
Right. We're right.
So I'd also tell you, are we are we drawing arbitrary lines or not?
Yeah, I think we probably are because none of us are.
I mean, none of us have a patent paper.
And I don't think anyone's going to do complicated mental math in our head.
But I might take the pain, but not like the physical manifestations
and consequences of being set on fire for three seconds rather than break my arm.
Right. So I'm not sure.
But those are the kinds of things that you can you can weigh
as far as how bad they would be.
And you can actually just you just pick things back and forth and say,
well, we'll weigh these against each other. Right.
So you're talking about like when you say breaking your arm,
is it like you have to get medical care?
Your arm is inoperable for a certain amount of time.
All that stuff that comes with the breaking an arm
or you just feel the pain of it breaking your arm.
And it doesn't have any lasting effects after that, either one.
I mean, I was picturing actually breaking my arm doing eight weeks in a cast
and that sort of thing. But like with the fire one,
I just said, just imagine being in super pain for a second, right?
Three seconds of the crucial curse from Harry Potter.
You know, I would choose that over the paper cuts, you know, drawn
out to that over 10 paper cuts a day for the next year.
Right. Like just because that would be super annoying.
That'd be a constant drag.
You know, I think I'd get over five seconds of the cruciitis, right?
Maybe depending on how bad it is.
They didn't do a great job of painting it as the literal word.
I often just have that burst of pain just for three seconds kind of thing.
Sure. I just want to point out that I slowly pull off band-aids.
I've heard that slowly pulling up band-aids is actually the better way to do it.
I'm sure it is. I go about medium.
There was an actual pain study done.
And I mean, pain is always kind of subjective.
You got to ask people one to ten and all that.
But they found that in general, yanking band-aid off first is
quite a bit worse than slowly pulling it off, even though the little light pain is stretched out.
Are you saying because it causes some sort of trauma to your top layer of skin?
No, well, maybe, I don't know.
But it was just it was more painful than having that small amount of pain stretched out.
And the reason they the conjecture was the reason that nurses and parents and stuff
always tell you to yank it off fast is because the person who's doing the yanking
gets to feel pain as they're yanking as well.
They either watch you squirm while they're slowly yanking it off and they feel really bad
or they yank it off once real quick and you flinch and they're like, OK, it's over.
It's done. Interesting.
So it's the person who is yanking it off gets a benefit from yanking it off quickly
because they don't feel as much pain during the process.
There's a similar point to be made.
There was a study done with getting patients to return for colonoscopies.
If you prolong the procedure at its least uncomfortable point,
you got, I forget the numbers, but these are in the Daniel Kahneman thinking fast
and slow, I think. I heard this, too.
Yeah. So you get more people to come back if you prolong the procedure
while it's at its least uncomfortable, like while it's there.
But like not at the most painful part, because in that way with the look.
So there's the other possibility that explains it instead of like the slow rip
off versus the hard rip off. If you do it a hard rip off and you look back
and like, oh, that really hurt. If you do it slowly, it just sort of hurts.
And so when you look back, like, well, yeah, I mean, it was a little uncomfortable,
even though it might have aggregated to more or the similar amount of pain.
With the colonoscopy thing, the idea is that you look back and you remember,
oh, yeah, it wasn't that bad, despite having time was just uncomfortable.
Exactly. Equal amounts of uncomfortable and painful.
Actually, probably more if you just needlessly prolong the procedure, right?
So it's probably more uncomfortable over the stretch of the entire procedure.
But you're just your memory, your recollection of it is, oh, it wasn't so bad.
Good stuff. So, Inyash, you would defend the original
proposition of dustbex versus torture.
Yeah, I would defend that if we can put numbers up on things,
then we're putting fucking numbers on things, right? Are we or aren't we?
Sure. And so is your guys' proposition that you can't put numbers on things?
That's not my proposition.
No, no, what I would ask in response to that is,
how is that meaningful to apply numbers to this like really weird
scenario of like dustbex versus torture?
Like, what is that going to accomplish?
Like, what is the actual, like, like defending it and being like,
we can condense this down to math and come up with a like an answer
in terms of like the utilitarian perspective.
No one is suggesting there will ever be a situation in which
it's 50 years of torture for an individual versus three to whatever number
of dustbex. Sure. So it's a thought experiment that plays out the idea
that you can condense utility into math and thus you must have the numbers
correct, basically is what you're saying.
And whatever conclusion comes from that should actually be applied to every
decision or most decisions that have to do with.
But aren't there some decisions where like the math kind of, it's like, OK,
you can observe the math, but there's this, maybe that's not the be all end
all, right? In my opinion, the point is that it is the be all end all.
And this is pointing out that humans are very fallible and very likely to,
as soon as something goes against their intuition, be like, oh, fuck, no,
this is we can't do numbers anymore.
And it's trying to make you bite the bullet.
So for example, the where people will be asked,
how much money are you willing to give to have one seagulls scraped of oil
after the big oil still happened?
And it was like $10.
How much money would be willing to give to get 10 seagulls scraped,
clean from oil and they can go about their happy seagulls lives?
They're like $20.
And that's like, why were you willing to give $10 per seagull when there was
only one, but only $2 per seagull when there were 10 of them?
Because you started with one question and they're like, oh, for one seagulls,
sure, I'd give 10. Yeah, you're right.
That's kind of inconsistent because the eight intuition sucks.
Yes, people are really should get intuition.
But at some point, there should be an actual number that given an undefined
number of seagulls, you'd be willing to give this much money per seagull
to clean them, regardless of how many there are, right?
Sure. Or maybe you set like a budget limit and you're like, I'm only willing
to spend $100 toward seagull welfare, you know, or well-being.
This is my seagull oil scrubbing budget.
It's $100 right now.
But if you're actually trying to put a number that it is worth putting
per seagull, this is actually very important because we do have a number
of dollars we're willing to pay per human life.
Well, I was going to relate this to experiments that have been done
with charities towards humans.
Okay.
And so one of the keys to doing this is you ask different groups.
If you ask the same group, how much would you give to say this little girl
and how much would you give to say this little girl and her brother?
The same people will scale up.
I don't know if they'll actually double, but they'll scale up in some meaningful way.
If you ask if you ask different, well, they don't double.
They're being inconsistent, assuming that their budgets aren't workable, right?
But if you ask different groups and you show a picture of a little girl and say,
how much would you be willing to help this little girl out?
They'll give some number and I forget what it is, but we can look them up.
If you show a picture of that little girl and her brother and say, how much
would you, how much would you give to save the pair?
If you ask that to a different group, it's typically less than on
average people will give to save the girl.
So if the one little girl and her family of eight people will give way
less than they will do that one.
And this is sort of a different problem than the suspects thing.
But this idea of scope and neglect that we suck follows from the same principle
that our brains can't multiply emotions.
I mean, and that sort of makes sense.
A, on the fact that we just didn't have an ancestral environment that would
sort of program that.
But if we, A, if we, or B, if we could, we would just shut down every
time a disaster happened.
You know, if we could actually understand how much it would suck for 200,000
people to die in a disaster, we would, all of us would just kill ourselves.
Right.
I could be bad enough for that dust, for us to be able to emotionally
understand that for 10 people.
Which is why I don't want to be a super happy.
Right.
Wait, why is it that you don't want to be a super happy?
In three worlds collide.
But why?
I missed how what Stephen said relates to why you don't want to be a super
happy.
The super happy is find out that the start trekking humans in that story allow
for specific things and everyone except the neutral party on the super happy
ship passed out or blacked out or had it taken nap from the stress.
Yeah.
I mean, that almost sounds like an appropriate reaction if you learn about
something terrible, right?
And so like even the humans on the ship kind of realized that like, man, our
reaction to the babies being eaten was kind of mild compared to what it really
should have been.
Right.
But their reaction was to us allowing the emotion of embarrassment to exist.
Sure.
This is all kind of an aside.
You should read three worlds collide.
It's a fun, what, six or seven chapter short story.
And the first three chapters are hilarious.
The first what?
Three.
And what do the super to the super happy show up at what three or three?
I think it's two.
Okay.
It's also an audio form on the Harry Potter and methods of rationality
podcast, but then you don't get to enjoy the comments per wiki page, which I
think are quite worth it.
All right.
They're well, well, willing to both.
So this thing with the like, like doing the utility math is kind of like when
there's a super powerful AI and there's a singularity, it needs to be able to
solve all the trolley problems, basically.
Well, even we need to solve trolley problems a lot, right?
Well, sure.
Yeah.
And we're eventually, I mean, there's, there's this whole thing about like
self-driving cars and like, if it has to make a trolley decision, like we want
