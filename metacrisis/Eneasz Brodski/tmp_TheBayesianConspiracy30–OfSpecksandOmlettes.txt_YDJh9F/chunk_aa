Hi there, this is Enyosh with two quick notes before we begin.
The first is an apology.
Very soon after we set up, my microphone got knocked out and we didn't notice.
I was still picked up on a nearby microphone and we boosted my voice on that everywhere
we could.
However, for the entire podcast, I sound distant and echoey and it's kind of hard
to hear me.
I apologize about that and we'll be better about checking our mics in the future.
The second thing is that, before we started recording, those of us on this episode just
kind of bullshitted for 10 minutes.
It's not really relevant to the podcast, but for any of our Patreon subscribers, we've
thrown up about 10 minutes of our miscellaneous chatter and humor, just as a little extra
behind the scenes for anyone who wants it.
Thanks, and now on to the episode.
Welcome to the Bays and Conspiracy, I'm Enyosh Brotsky.
I'm Stephen Zuber and we have two guests this episode.
Hi, I'm Sean.
Hi, I'm Matthew.
Returning from punching Nazis.
Yes.
There is a podcast project that has recently come out to make all the sequences available
in audio format as a podcast.
We've talked about the sequences a lot.
They were the original posts written by Eliezer at LesRom.com, actually started out at OvercomingBias.com,
which kind of kickstarted the whole rationality subculture back in 2007, I think.
Anyways, they're a starting point.
They really explained the rationality much better than I could, and I said in I think
our very first episode that if you really want this rationality thing explained, your
very best resource by far is just to go and read those sequences of posts.
Eliezer went ahead and compiled them into a book because a lot of people were like,
hey, we hate surfing posts on the internet, is there like a book that we can have?
And no, you can't have a bound book because that is really, really big and you'd have
to pay an obscene amount of money to get something that big bound in a single tome,
but it is available as an e-book.
And now there is two guys from Europe who are recording every single chapter, each chapter
is one post.
It is between five and 15 episodes per chapter and putting them out as a podcast.
So two things.
One, the book wasn't compiled by Eliezer, it's compiled by somebody else, I think, like
with permissions and thumbs up or whatever, but there is a podcast out there that I think
you have to pay for.
I bought a couple of the sub sequences, I bought a human's guide to words and reductionism.
They appear to now be defunct.
The castify ones?
Yeah.
Yeah, I looked for them like, I don't know, six months ago and couldn't find them again,
so someone's doing them for free, it sounds like.
Yes, cool.
I know you could make a podcast mandatory pane.
Well, I mean, you can, it's not a podcast, it's audio files and to download them you
pay me.
How would an audio book version of the sequences work since all the posts I've read have like
19 links to other sequences?
I'm assuming you just don't get the links.
And rationality from AI to zombies isn't a bit more at order than the sequences on lesswrong.com.
So there's less of a need to like link to previous things.
I think the links are still there in the book if you buy it, but it's much more in order.
So a little more accessible.
That sounds awesome.
Yeah.
Yeah.
The podcast.
I believe it's rationality AI to zombies.
The podcast.
Hey, there it is.
Yeah.
Rationality from AI to zombies, the podcast.
Subscribe.
Are you shitting me?
Sorry.
Done.
Subscribe.
The reason I bring that up is because at our last, at our last lesswrong meetup, which
we have in Denver once a month, I got talking with Matthew about the torture versus dust
specs post and we'd been kind of drinking and we were like, hey, why weren't we recording
this?
This was awesome.
We should really put it up on for the podcast and so now we're doing that.
All right.
So Matt, do you want to lay out the dust spec versus torture thought experiment as best
you can?
So Yudkowski, I'm not sure how, but he did come to the conclusion that the least bad
bad thing that can happen, something with a negative utility, but the smallest possible
negative utility that's measurable would be having a dust spec and up in your eye.
Part of the hypothesis is that's all that happens.
There are no side effects.
It's not that a dust spec hits you in the eye while you're flying a plane and then
you crash the plane.
Ignore that.
It's just purely dust spec and that's it.
And if you have like calluses on your eye or something and getting a dust spec in your
eye doesn't hurt you, then substitute something that actually does hurt a bit like stubbing
your toe or getting a paper cut or something.
Those trick me as orders of magnitude higher than dust spec.
Certainly magnitudes.
It's kind of this abstract idea of like a really minor inconvenience, like the most
minor inconvenience that you'll acknowledge as a thing.
The least bad bad thing.
But it'll go away in like three seconds, right?
Yes.
A very minor pain.
I don't think it can be just inconvenience.
It has to be some measure of pain.
Okay.
I thought it was just some negative yudelons.
We should have been more taken on this.
Yudelons.
So I should probably mention that I haven't read almost any of the less wrong stuff.
You're fine.
Yudelons, I don't.
Is that a unit of measurement of utility or?
Yes.
Yeah.
Okay.
Positive and negative.
All right.
So it's like kind of the idea.
It's just like a term for measuring utility.
Positive or negative is basically what you were saying.
Yeah.
Depending on how into consequentialism and utilitarianism you want to get, you can start
quantifying it with units called utils or yudelons.
But I mean, for the most part, you're talking about happiness and suffering or pleasure and
pain or something like that, right?
Sure.
And I assume there has to be some measure of pain involved because that's what negative
utility is, right?
Or unpleasant.
I mean, it can be pain if your definition of pain is broad enough to include things
that aren't literally pain.
Do wet socks hurt?
So an uncomfortable thought.
Right.
So like an uncomfortable thought or like a small emotional trauma or like.
Something whose net value is negative.
The thing is, I hate wet socks a lot.
Same.
So that's why I wanted to pick out.
I wanted to pick out an example that was obviously not painful.
But obviously negative utility.
Okay.
Because I'd rather get 100 dust specs to the face than get my socks wet.
So if the least bad, bad thing is having a dust spec float into your eye and give you
the tiniest bit of measurable minor inconvenience, then it would be better for one person to
be tortured for 50 years and three to the third to the third to the third or a Google
plex or whatever conveniently large number fits, not have dust specs flying to their
eye.
Looks really quickly unpack the number.
I think the actual number doesn't matter that much.
But the point is the number is so large that if you were to take the number of all the
atoms in the universe that we can measure, took that to the power of itself, that number
still wouldn't even approach how many people were talking about at this point.
It is an unimaginably large number.
So nearly infinite.
Nearly infinite.
The only reason I don't use infinite is because infinite is seems like such a vacuous concept
that to me it almost feels like nothing.
Whereas when I think of the number of atoms that exist and then multiply that by itself,
that's an actual like large number that infinite I know technically infinity is more, but
infinity doesn't feel like more infinity sounds like, you know, you're stupid infinity plus
one.
Douglas Adams has a good thing about infinity in his checkers guide where Arthur, is that
the main character?
Dude, I don't know.
Whoever he is.
Yeah, it's Arthur Dent.
He's in space and he's in front of a very large object and he looks to his left and
he sees the object go on for infinity.
Oh wait, no, it isn't infinity because he can see a curve ever so slightly which has
a more imposing effect than if it didn't curve because humans can't handle infinity.
Right.
So he can actually see it like he can actually understand like this thing is really, really
big whereas infinity would be incomprehensible basically.
That many people getting a little bit of negative utility.
If the measurement of for a dust spec is 0.00000001 and you multiply that by a impressively
huge number, then the amount of eudelons, whatever 50 years of torture happens to be
would be less than that number caused by dust specs.
I don't think we laid out that the that is being weighed against 50 years of torture
for one individual until just now.
So I guess that's the important crux of the position that it's this dust spec this dust
spec into basically infinite number of eyeballs versus one person being tortured for 50 years.
And the idea is that well, because you're taking an infinitesimal thing times the opposite
of an infinitesimal thing, and you then you just have one, you know, being suffering as
much as possible for five decades, the dust specs add up to more negative eudelons than
one person being tortured.
And on the surface, that does sound kind of ridiculous, as I'm sure most people are not
thinking because we have such an aversion to thinking of someone being tortured even
for five minutes.
But as you pointed out, this started as something that seems entirely reasonable and then is
taken to an extreme.
Yes.
Yes.
Would you like to recapitulate that argument?
Because I thought that was fascinating the way you laid it out.
Should we all kind of say like where we're where we are on that whole thing?
Yeah, sure.
Where are you on that whole thing?
Toads against.
Yeah, like, like, yeah, not only is it, yeah, everyone should get a speck of dust in their
eye.
