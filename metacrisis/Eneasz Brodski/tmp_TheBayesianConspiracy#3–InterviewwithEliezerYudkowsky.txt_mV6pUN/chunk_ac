So for some people who are sort of like being held back by their own sense of
modesty or thinking that, like, um, it is, is like sinful to try to develop
your own beliefs and like not to try to find another source to get your own
beliefs from that is like more authoritative.
Like for those people, the, the realization that like they can actually try
to figure things out on their own.
Might be the most important realization.
And for other people, the most important realization might be that they
should actually start reading the literature and look at what other people
think and, um, have argued, especially when like lots of people agree on what
the critical arguments are and like not try to do everything on their own.
Um, it, it varies by the person.
Do you think that through your work on the sequences and as a blogger that
you've made a significant impact on helping people be better predictors?
Well, I haven't had a significant average impact on the median human being on
earth yet growth mindset, but there's really like a very large number of
readers who like acquired more accurate beliefs and probably more accurate
police finding skills about some important things.
But like, even if the number is pretty large in absolute sense, I don't really
know how large a fraction was.
There's, um, there's like a real sense in which that's wrong with taking
on a very hard problem was like getting people to make better predictions.
And it was like striking this very hard problem with a very soft that like
it'd be the bath is like reading a couple of years worth of blog posts.
And there are people who can be moved by that that, but they're not necessarily
a majority.
So what's a, what's a harder bat?
What's the best way to spread those principles and help people in
general be better predictors?
I'm not sure I have much of an improved answer over that.
Like CIFAR workshops, good judgment project, expanded participation,
betting and prediction markets.
Um, see like universities aren't all about collecting
rents on prestige and employment.
Like there's also a fairly hard problem of teaching people things that
universities solve at least some of the time or they used to.
I don't know, haven't checked to see how, how well they're still doing at it,
but like to actually cause people to acquire a real life skill and like
resistance to common errors.
It feels like what you really want to do is like go to a university that is
like a correctly designed university for six months and less than like
write your blog post a slightly different way.
Okay.
Thank you so much.
I wanted to thank you again for taking the time to talk with us here.
I know that we're coming up on our last few minutes, so I'll skip one of my
questions and if you don't like the question I settled on, I'll go back to
the first question.
I've always wanted to kind of ask you about this since this is, you know, I
think the, the end condition, which is, you know, some years from now, you know,
assuming it's in the next, uh, or, you know, in the foreseeable future, you and
your team have gone over the code, everything looks great and you hit
enter and start the AI program.
I'm in, you know, then super intelligence is born.
And I guess I know that, you know, obviously there's event horizon problems,
but then, you know, just speculating wildly, if you want, what sort of, what's
your imagination of what happens one hour later, one week later, one year later?
Well, I'm going to sort of parenthetically quibble about the, the
part where there's a notion of you to like check the code and then you switch
the AI on, like you, you, you like have an AI that has been running and
learning and you've been moderating, monitoring its operations for a while as
it's been gradually going in strength.
And then you like, at some point, perhaps you like pull out the stopper and let
it do things you would not have let it do previously.
Um, but not like you, you write the code, you check the code, you, you switch
it on, like any AI has to learn a whole bunch of stuff before it can do a whole
bunch of stuff.
So leaving that parenthetical aside, um, I will launch into another
parenthetical, which is the difference between genes and sovereigns.
So like, if you want an AI to just, you want just like free an AI and say, do
whatever you think is best, um, that might be a harder design problem than
if you have an AI that you want to do things like, um, I, I mean, let's paint
the cars paint as an example that we sometimes use, like paint all the cars
paint, just paint and paint.
Don't tile the universe with pink painted cars, like don't maximize the level
of thickness, the point where the pink light is evaporating nearby buildings,
just paint the, the darn cars.
And like this itself is already a very difficult and legal problem or so we
suspect, like if you want an AI that, that does things with that, that like sort
of can like do turn things that you actually meant it to do and not have
an insane number of side effects and not rewrite the programmers in order to
give better orders, et cetera, et cetera, like this itself is already a like
potentially very hard and lethal problem.
And depending on whether you solve the GD problem or the sovereign problem, what
happens a week later, um, probably looks very different.
For sure.
I like, I like the, the pink car problem that sounds like the, the leveled
up version of the paperclip maximizer, but I guess, and I don't want to push it,
but say things go great.
What does, what does great look like?
Um, it looks like concrete instantiation of the untheory statements from what's
wrong.
I'll be honest, I didn't finish that one because he prefaced it by saying
something along the lines of reading this could, uh, what it was, there was some,
there was some intense caution that I was like, you know what, I'll put this off
and I never put it back on.
Yeah, I think I'm still worried about that question.
Like I keep on taking stabs occasionally at writing a story set in that world and
keep on running into obstacles or writer's block, uh, and writing it.
And I don't know if I would like ever actually publish it.
If I could do it, I would like worry that if you like show people someplace
that is actually a nice place to live, they might start to see the present
world and a more negative life.
Um, I think that is like a valid thing to worry about, but I do think we can do a
lot better than, uh, Greg Egan's, um, upload for permutation city style
uploads who have nothing better to do with eternity than to learn exactly how
to carve table legs for 20 years because they have like exhausted all of the
actually interesting things worth doing and are now modifying their preferences
to the satisfied carving table legs.
So the, um, I think we can do better than the culture where people are just
sort of wandering around at the mercy.
Well, not the most I mean, machines have mercy, but the people are basically
useless and they know it.
Like the only reason they exist is because AI is a refraining from doing
certain things in order to give people a reason to exist.
And I think they can do better than that too.
But like, if you want to know what specifically an optimal scenario looks,
looks like, then, um, I think we're, I mean, I have ideas, but I think I would
prefer not to try to encapsulate them in the next 60 seconds.
That's totally fine.
I appreciate the answer.
Uh, it looks like we are right about the end of the half hour.
So, um, we can, we can let you go now if, if you have things to get on too.
Um, I can stick around for an extra five minutes, I think you want me to.
Uh, Steven, did you want to go with that first question that you were
had then?
Um, yeah, if you want to give a quick sound bite answer, um, I've been really
excited to see AI in the news a lot lately, even though a lot of it's kind
of fear intensive, you know, like Elon Musk and Stephen Hawking have been
talking a lot about it.
I was particularly excited that Sam Harris was talking about it.
You know, he has a big audience with the skeptic and, and atheist community.
And I think that he, I think his next book is actually going to be on AI
and this whole thing.
And I, I'm excited to see that the audience is getting bigger.
Do you have any thoughts or feelings about, I guess AI and pop news lately?
Well, I, I feel like it's basically not covering any of the real ideas or
arguments.
Um, like I, I, I'm not quite sure what the.
Deep problem is with journalism in modern society that prevents them from
covering any of the real ideas or arguments ever, but they're still
putting photos of the Terminator marching armies of robots with glowing
red eyes rather than like a more accurate depiction of what the problem is,
which might be like a tiny spear inside someone's bloodstream releasing
putolinium toxin and 10,000 years later, a picture of the Milky Way with a
10,000 light year radius spear gapped out on it, centered where the earth used to
be.
So like you, you still don't see like the real deal.
You don't, you don't see the actual arguments.
You don't see the actual ideas.
You don't see, um, if you don't, you don't, you, you, you still see the concept
of like robotic cars causing unemployment is AI anxiety.
And it's on an equal level with the 10,000 light year spear gapped out of the
Milky Way anxiety.
Like to them, it's all the same thing.
It all blurs together.
They, they don't have any, they don't have any concrete model of it.
They don't have a notion of like an actual world, world corresponding to
the words they're, they're emitting.
Um, if to them, it's all just like words, words and more words about AI
anxiety and all, and all the words are equal and it's just this one big blur.
And that's the kind of impression I get when I read this stuff in the media.
That hand, a lot of people talking without bothering to familiarize themselves
with the arguments that they think they're arguing against.
So one word answer is that it would say it's a positive or a net negative that
it's being talked about more.
I, I'm being like, based on the events that have actually happened recently.
Um, if there are positives that they have yet to materialize and the negatives
are kind of obvious.
So, uh, I mean, like maybe that will change in the foreseeable future, but at
least for now, like the consequences so far have not been without call positives.
Okay.
I appreciate it.
Thanks.
Uh, one last thing before you go, we were wondering, did you want to tease any of
our listeners with hints about the upcoming methods of rationality epilogue?
Like when it might be released or the timeframe of the, the setting of that
blog?
Um, well, the timeframe I had planned was at the beginning of their 70 year
pop-up board.
Uh-huh.
That was bad.
Whoops.
We wrote that question and I realized that I think that was in your announcement
of the epilogue scenario.
Anything, any, any additional information?
So, so your voice suddenly went a bit softer there, but, um, so, well, I mean,
like that in terms of additional information, but I will mention parenthetically
that I do not regret giving, for example, significant digits, a lot of time to run
as like the, uh, best HPMOR continuation thick, um, not competing with the
author's personal conception of what might have happened later.
Cause, you know, no matter, even if I can write a really awesome epilogue, it's
not going to contain like as much net meat as significant digits, the
continuation fan fiction bit.
Um, let's see, uh, further facts I can tease readers with.
Well, maybe this one's kind of obvious, but Luna Loggood will be in there.
Awesome.
I think.
Thank you so much for joining us today.
Um, we really appreciate you taking the time.
