Hi, welcome to the Bayesian Conspiracy.
This is Eneosh.
This is Katrina.
And this is Steven.
And today, we are interviewing Eliasar Yadkowski.
Eliasar is a decision theorist and AI researcher who basically kickstarted the rationality
movement when he founded LessWrong.com.
Let's jump right into it.
Hello?
Hello?
Can you hear me?
Yes, we can hear you.
Can you hear me?
I can hear you.
How about me?
I can hear you as well, but your voice is softer.
What if I speak into the microphone?
Is he connected to the microphone at all?
Yeah.
Okay.
How about this?
I can now hear you better, though your voice is still slightly less loud than Eneosh's.
How do I actually pronounce your first name, by the way?
Oh, yes.
I am Eneosh.
Eneosh.
Yes.
And now I know.
I know.
Hurrah!
I'm Katrina.
Nice to meet you.
Pleased to meet you as well.
Yeah.
Did you just hear a clicking sound, by the way?
No.
Good.
Because I just switched off my phone and I was wondering if the iPhone clicking sound
was being echoed over the connection or if it was just in my headphones.
Oh, okay.
I think it was just on the headphones.
It sounds fine.
Okay.
Great.
And we also have Stephen here.
Oh, hi.
Who's kind of in charge of the podcast.
It's a group effort.
Yeah.
So,
Hello, Stephen.
Thank you very much for agreeing to be on the podcast with us.
We are really excited about this whole, this whole thing.
Katrina, would you like to start since we don't have all that much time?
And thank you so much for spending your time.
Yeah.
So basically we already introduced you before we answered your call.
So.
Should we do it again to make sure that he's on board with this?
Oh.
Yeah.
Sure.
Why not?
Okay.
So I introduced you as an AI researcher and decision theorist who basically kickstarted
the new rationality movement when you founded less wrong.com.
Would you say that's an acceptable summary?
Sure.
I mean, like there was this whole thing with previously overcoming bias and it became
less wrong.com.
But you know, that's enough for government work.
Okay.
Excellent.
Great.
Okay.
So if you're cool with it, we're just going to jump into some questions for you.
Alrighty.
Okay.
So can you talk a little bit about your rationalist origin story?
Are there any real standout moments that you remember from your youth that made you the
rationalist you are today?
If we're talking about like big influences, then surely you're joking, Mr. Feynman, a
step farther out by Jerry Pornell, a large amount of science fiction with just sort of
pro-good reasoning ideals quietly in the background.
If I'm looking for like single standout moments, then there was this one point in my life where
I noticed the feeling of sort of knowing something wasn't true, hiding it away in the corner
of my mind and managed to generalize over the feeling and be like, aha, this is what
it feels like to be hiding something away in the corners of my mind, I'm going to look
for everything now and try to sweep out all the hidden corners just like while I was walking
down the street one day.
Do you remember what it was?
Or you feel comfortable sharing what it was that was hidden in that corner?
It was a, like there had been a childhood game some years earlier where, which basically
involved kids from a summer camp hitting each other with small plastic bowling pins.
And I refused to participate in this game.
And what I told myself about that for some number of years was I'm refusing to participate
in this game because it's a negative sum game, like as a matter of ideals, I'm not going
to like trade being hurt for the chance to hurt others.
And like some years later, I looked back and realized, like, no, I just hadn't wanted
to be hurt.
And like that was the actual reason I'd refused.
I made up the sort of idealistic story afterwards.
And I'd known somewhere in the back of my mind why I'd actually done it.
I'd known the idealistic story was false.
I just had these two beliefs going at the same time.
And I was like, aha, like I see this belief.
I see these, like the truth that I always knew in the corner of my mind being pushed
aside by this thing that I'm saying.
Now I understand this feeling.
That is fantastic.
How old were you?
Probably something in the range of 8-ish during the original bowling pins episode or
actually maybe it was closer to 11-ish.
I'm not sure.
I can't remember if I actually did know about game theory at the time or not.
And like what a positive thumb or negative thumb game was.
If I knew what game theory was, then I was probably closer to 11.
And when I first, like, noticed that feeling of hiding something in the corner of my mind
and generalize and go over it, that would have been age 14 or 15, I think.
OK.
Was I got the feeling from reading the less wrong sequences that you had a very religious
upbringing, which I found is not uncommon among rationalists to break away from that
religious upbringing.
Did that have a big influence, too?
It probably had a sort of initially getting it wrong influence.
Like, I'm not sure that I would say I had a very religious upbringing.
I had a very Jewish upbringing.
It's not quite the same thing.
Like modern Orthodox Judaism.
So there was probably like another watershed moment when I was five or six
when they were telling me to pray to Daven.
And I was like, I don't understand these words.
This is not going to work.
And they're like, no, no, like as long as you, Daven and Hebrew, it's OK.
If you don't understand the words.
And this was so stupid that I figured that there had to have been something lost
in the translation between like God and my being told this thing.
So like from that moment on, I was a heretic.
There were things that I was officially supposed to believe, but I did not believe.
So I still believe in God.
But like I already didn't believe in the exact Jewish religion version of God,
though I wouldn't have like self identified as a heretic out of just been like,
no, they clearly got this wrong somehow.
So I think that the effect this had on me was indeed something of the same thing
that I've seen other rationalists get from growing up with one crazy parent,
which is it breaks your trust and authority.
And there were like larger life traumas than that, which like how to break my trust
and authority. But like if you were to like ask of the like sort of earliest
life memory of rejecting what is being presented to you as the authoritative
adult view of reality, it would be that moment where I was like,
that can possibly be what God wants.
Yeah, although if praying to God is like a function call,
you technically don't have to know what it means as long as you execute the contract.
Well, well, if the modern Orthodox Jewish God existed,
pretend would not be like a function call.
I stand by my younger self.
I mean, it might be a bit of a vacuous truth, but I like stand with my
younger self on this one.
Like a good effort made for good reasons.
Do you think it's very important for good rationalists to have that break
with authority figures?
Because I know that I've I've, I guess.
Seeing you, seeing you talk a little bit about how it's important to trust
people with a great deal more experience than you.
Or when you don't have the information that you need.
I think that is there's a difference between sort of trusting people because
it's an option versus why I trust people because you're scared of what happens
to people who don't trust authority.
And sorry, just a second.
My headphones just told me battery low.
I guess if I guess if that gets worse, then you might have to call me back and
you might have to call me back and I might have to call you back on regular Skype.
But hopefully we'll be all right.
Okay.
That's not a problem.
If we cut out, then we'll just await your return call.
Yeah.
So the, to get back to the previous question, the, I think that it's that
there's an important formative experience wherein you challenge the
authoritative view and turn out to be right so that you get some positive
feedback for it.
You understand that you have the right to try to write your own opinion about
things like, you don't always have to exercise that right.
But like a sense of like, there was this one time I tried to form my own opinion
and it didn't go horribly wrong.
Awesome.
And it's, and religion is a pretty good place to get that formative experience.
I mean, it sure beats trying to do it with a fucking physics, you know.
Right.
It's my experience that religion is a good warm up exercise for basically any
rationalist or skeptic technique.
So I, moving on, there's something I've been wanting to ask for a while.
So there's been a bit of a diaspora in the rationalism sphere since you hung up
your blogging hat.
And I was wondering, have you ever considered taking a blogging again?
And if so, if there's like any topic you'd really be interested in giving
another in-depth sequence about.
So to do my, so like to do the less wrong sequences, like I was doing nothing
else for two years, like other other projects, such as the events singularity
institute did suffer as a result of, like my attention being elsewhere.
And like it, and I was like doing one post every day, rain or shine is
sometimes giving myself little vacations by posting rationality post collections.
But like basically just, just keeping up the case, keeping up the volume.
There was a time of my life when the rest of the world wasn't paying very much
attention to me and I didn't have to take meetings with people.
It would be pretty hard to get up to that momentum again, I think.
I would also want better tools, some of which are underdeveloped.
What do you mean by better tools?
So like if I was doing the less wrong sequences over again with the tools
that I had available at the time, then I would be using both a wiki and a blog.
And when I like wrote about a new concept, I would create a wiki page for it.
And then like link future blog post to the wiki page instead of the
previous blog posts.
So it's like untangle the giant spaghetti mess of dependencies that rapidly grew up.
And today, like there's a friend of mine named Alexi is working on a project
called Arbital, which will be sort of like that only even better.
So if I do blogging again, it will probably be there.
In fairness, the giant Tangle spaghetti mess was one of the fun aspects of it
that really got me through it.
Well, yes, but there's like a right way or wrong way to do the giant
Tangle spaghetti mess in the right way, there's TV tropes where you're
bouncing back and forth between the tropes and the shows.
So like the correct way to do the giant spaghetti mess would have been to
like bounce people back and forth between wiki pages about concepts that
listed blog posts, talking about the concepts and blog posts will look
linked to wiki pages and that would have been the right way to do it.
Okay.
Do you like, I know you don't have the time right now, so you're not going to,
but was there like any specific topic that you'd really just want to get
your teeth into if you had the time?
I mean, I think my, my priorities for blogging, if I was in a blogging
environment would be to just like massive brain dump all the stuff about
the alignment problem.
Cause like bless wrong didn't go very far into that.
It's like only going there far enough to like tell you that there is
such a thing as this problem and not logical decision theory and tiling
agents problem and how we know the earth and like how we know what the
orthogonality thesis is true and introduction to a XI and ideal analysis
of unbounded agents and epistemic efficiency and instrumental efficiency.
And then there's this enormous body of knowledge that's basically being
passed around verbally between people who are actually working in the field.
And I try to block that.
That does sound fantastic.
Uh, did you, did you want to jump to yours or should I do the methods of
rationality question?
Oh, okay.
Uh, well, uh, since I'm expecting to get some listeners, maybe from the methods
of rationality podcast, we, we got to get a couple of questions in about that.
So, uh, I know you've written the original universe works before, since,
you know, I read a lot of them and, and podcasted some of them, but so, uh,
why a fanfic and why Harry Potter in specific, as opposed to some other universe?
Well, Harry Potter in particular, because I had been reading a large amount
of Harry Potter fanfiction online.
And so when this story spontaneously burps itself into existence inside my mind,
it happened to burp itself into existence as a Harry Potter story, like not in the
same form that you saw, but the very first version of the story that popped
into my head had Harry washing in horror in the philosopher stone chamber as
snake, cruciate the face from the back of coral's head into insanity.
Like that was, that was so like, there was a whole lot of, uh, free mind outlining
and like changing things around and adding plot elements and so on.
And the like, the version that you will probably sound a little bit more
familiar to people who have better to watch death note is, um,
Yagami light is older and smarter and has 300 core cruxes, one of which is
the pioneer plaque headmaster L is pretending to be insane.
And then into the middle of their dual wanders, the young miles were
coasting and starting Hogwarts.
So, so like that, that, that version probably sounds a little, a little bit
more familiar, but I thought I should mention the like early, early version,
just to sort of like establish the fact that that plan to change.
And like, you shouldn't think that the original vision for the story is
anything that has to survive.
And why Santhic?
Um, because it was easy to write at the time, like it stayed easy to write up
until the end of the Azkaban arc and then stopping easy for reasons I wouldn't
start to figure out until many years later.
But I was working on an attempted nonfiction rationality book and writing
what's going really slowly.
So I was like, okay, can I write faster?
This is this like completely goofy, just for fun thing.
Like there are these people now making up stories about how HPMOR was an
elaborate plot, like from the very beginning and et cetera, et cetera.
And like these people don't understand how writing works, you know, like it was
just like, okay, I can write this thing really quickly.
Okay.
It's good enough that I, that's worth throwing up on fanfiction.net.
Okay.
It seems to have accumulated 1000 reviews very quickly.
I will put more effort into this and eventually the nonfiction book ended up
going nowhere because I was like forcing myself to write it.
And the thing that I wasn't forced, forcing myself to write ended up being
the super popular thing.
And, you know, this is a story that one finds repeated many other times in
another context.
What, if you don't mind me asking, what was the thing that you found out
many years later was really hard and it slowed you down after Azkaban.
Um, so many years later, I worked out that there were a couple of things I was
doing wrong.
Um, one of the things was that by writing a story consisting of entirely
foreshadowing, I was adding on more and more constraints that had to be
satisfied by the later sections.
Um, and the other thing that I was doing wrong was I had started promising
my readers to post at particular times and dates after, um, finding that
people were frantically trying to refresh the, the story page, but every five
minutes, they wanted their hit of H-C-M-O-R, a phenomenon I would later
compare to being a heroin dealer about to knit all of the heroin out of his
own eyelashes.
So I was like, okay, I will like only post at this particular time of day.
And that was, and that was a mistake.
That meant that I had to rush to get the stuff done by that time.
If I wanted to post it that day, instead of this like calm, relaxing,
relieved, gay feeling of, all right, I finished the thing.
Now I get to go post it.
Now I would finish and be waiting 30 minutes for the reward or I'd be rushing,
rushing, rushing and have to like post it at that time if I wanted to get it
up that day.
So that was, um, that was like the, the small thing that like changed the
reinforcement dynamics of what happened to me when I was writing and was just
like me being clumsy, not understanding how the human reinforcement system worked.
Right.
I just want to jump in really quick on that.
Speaking of, I guess the positive experience of writing it, uh, on a scale
of one to a hundred, how much fun was it from your end to watch the subreddit explode
after the final exam?
Well, I mean, like some of the explosions prior to that were a little bit more fun
than the explosion for the final exam itself.
Like it was, so I mean, like maybe 80 or something, because on the one hand,
it was like nicely evil.
And the other, the other hand, I was sort of like horribly aware of the fact that
this puzzle that I had designed as before chapter one had been published, like
the solution is there in the like, in the like first three lines of HPMOR.
Right.
So it was this puzzle that I had designed with like five years worth of
negative writing skills.
And this, again, subreddit was trying to solve it and coming up with better
solutions that I'd envisioned.
And it was too late.
All the first shadow moves already in place.
I couldn't use their solutions.
I had to use the people for shape on.
Okay.
Well, I, all right.
I've got a little bit of an ego to school question because I've always really
been curious about this, but I never actually asked your permission before I
started the podcast and I still kind of feel guilty about that.
And I was wondering what you thought when you first heard that some random
jerk was podcasting your fanfic and, and if your opinion has changed at all now
that it's done.
Um, probably something like a, um, a flash of pride, a sense of being honored
and some amount of banging my head against the wall about how much easier it
was to get people to like put in large amounts of paid uncoordinated volunteer
work on things related to Harry Potter fanfiction than to saving the actual world.
I mean, like, like we're doing a little better on the second metric nowadays.
Like, um, we now have like better volunteer coordinators, but I was like, sort of
like, gosh, it's like easier to get people to do the sort of like fun, humanly
comprehensible stuff that, that carries immediate emotional rewards.
Then it is to like work on the longer range stuff, isn't it?
Yeah.
And, you know, to this day, I suspect that we that like, I like sometimes wonder
whether like there are friendly problems that could just be solved.
If I could get the same number of mathematicians working on it with the
same intensity was trying to solve the final exam.
So the volunteers that you're looking for, are they highly skilled mathematicians?
Um, those are people coming to workshops.
Like again, remember, like stuff has changed over time.
Like at the time I was thinking of these like sort of various failed attempts that
the then singularity institute had made to get volunteers to, I don't know,
translate stuff or, uh, like clean up after a singularity summit or something.
And like nowadays we are like better about volunteers on that score.
Um, and so like, uh, like we have systems in place for like various volunteer
things where I unfortunately don't know the details or what type of volunteers
we're looking for currently.
But if you happen to be a highly skilled mathematician, then you go to
intelligence.org and go to the contact section and are like, Hey, can I come
to one of your workshops and then you come to one of our workshops and
that's like start down the pipeline for contributing to the research area.
Thanks for the tip.
Yeah.
So, um, taking this in a little bit of a different direction, some of the
audience that we want to reach out to are people who are new to less wrong or
new to the rationality community and maybe know only a little bit about it.
Um, so if someone were to adopt only one or two rational skills or habits,
which do you think are the most vital?
It varies by the person and what they've mastered already.
I think that like for a lot of people, they're very critical realization
where you realize that there is this, um, our form of adapting your belief to the
way the universe is, which is completely separate from the skill of arguing.
And that just because you can come up with a argument for both sides of an
issue doesn't mean that you have like now done your duty to it and analyzed it.
And it's uncertain or undecided or even like, um, like even like the state
is it is that there are arguments for both sides.
There's like a fact of the matter.
You're trying to figure out what the fact of the matter is.
Um, for other people, the most important realization might be the one that Philip
Petlock describes in the good judgment project of like, you can be a better
predictor, like that was the thing that that Philip Petlock's, uh, super forecasters
all had in common.
They believe that super forecasting was possible.
They believe that like you could develop skills about it and that you,
you could have like better probability judgments and other the other people in
the project, uh, or there's like median percent of the project.
So for some people who are sort of like being held back by their own sense of
modesty or thinking that, like, um, it is, is like sinful to try to develop
your own beliefs and like not to try to find another source to get your own
beliefs from that is like more authoritative.
Like for those people, the, the realization that like they can actually try
to figure things out on their own.
Might be the most important realization.
And for other people, the most important realization might be that they
should actually start reading the literature and look at what other people
think and, um, have argued, especially when like lots of people agree on what
the critical arguments are and like not try to do everything on their own.
Um, it, it varies by the person.
Do you think that through your work on the sequences and as a blogger that
you've made a significant impact on helping people be better predictors?
Well, I haven't had a significant average impact on the median human being on
earth yet growth mindset, but there's really like a very large number of
readers who like acquired more accurate beliefs and probably more accurate
police finding skills about some important things.
But like, even if the number is pretty large in absolute sense, I don't really
know how large a fraction was.
There's, um, there's like a real sense in which that's wrong with taking
on a very hard problem was like getting people to make better predictions.
And it was like striking this very hard problem with a very soft that like
it'd be the bath is like reading a couple of years worth of blog posts.
And there are people who can be moved by that that, but they're not necessarily
a majority.
So what's a, what's a harder bat?
What's the best way to spread those principles and help people in
general be better predictors?
I'm not sure I have much of an improved answer over that.
Like CIFAR workshops, good judgment project, expanded participation,
betting and prediction markets.
Um, see like universities aren't all about collecting
rents on prestige and employment.
Like there's also a fairly hard problem of teaching people things that
universities solve at least some of the time or they used to.
I don't know, haven't checked to see how, how well they're still doing at it,
but like to actually cause people to acquire a real life skill and like
resistance to common errors.
It feels like what you really want to do is like go to a university that is
like a correctly designed university for six months and less than like
write your blog post a slightly different way.
Okay.
Thank you so much.
I wanted to thank you again for taking the time to talk with us here.
I know that we're coming up on our last few minutes, so I'll skip one of my
questions and if you don't like the question I settled on, I'll go back to
the first question.
I've always wanted to kind of ask you about this since this is, you know, I
think the, the end condition, which is, you know, some years from now, you know,
assuming it's in the next, uh, or, you know, in the foreseeable future, you and
your team have gone over the code, everything looks great and you hit
enter and start the AI program.
I'm in, you know, then super intelligence is born.
And I guess I know that, you know, obviously there's event horizon problems,
but then, you know, just speculating wildly, if you want, what sort of, what's
your imagination of what happens one hour later, one week later, one year later?
Well, I'm going to sort of parenthetically quibble about the, the
part where there's a notion of you to like check the code and then you switch
the AI on, like you, you, you like have an AI that has been running and
learning and you've been moderating, monitoring its operations for a while as
it's been gradually going in strength.
And then you like, at some point, perhaps you like pull out the stopper and let
it do things you would not have let it do previously.
Um, but not like you, you write the code, you check the code, you, you switch
it on, like any AI has to learn a whole bunch of stuff before it can do a whole
bunch of stuff.
So leaving that parenthetical aside, um, I will launch into another
parenthetical, which is the difference between genes and sovereigns.
So like, if you want an AI to just, you want just like free an AI and say, do
whatever you think is best, um, that might be a harder design problem than
if you have an AI that you want to do things like, um, I, I mean, let's paint
the cars paint as an example that we sometimes use, like paint all the cars
paint, just paint and paint.
Don't tile the universe with pink painted cars, like don't maximize the level
of thickness, the point where the pink light is evaporating nearby buildings,
just paint the, the darn cars.
And like this itself is already a very difficult and legal problem or so we
suspect, like if you want an AI that, that does things with that, that like sort
of can like do turn things that you actually meant it to do and not have
an insane number of side effects and not rewrite the programmers in order to
give better orders, et cetera, et cetera, like this itself is already a like
potentially very hard and lethal problem.
And depending on whether you solve the GD problem or the sovereign problem, what
happens a week later, um, probably looks very different.
For sure.
I like, I like the, the pink car problem that sounds like the, the leveled
up version of the paperclip maximizer, but I guess, and I don't want to push it,
but say things go great.
What does, what does great look like?
Um, it looks like concrete instantiation of the untheory statements from what's
wrong.
I'll be honest, I didn't finish that one because he prefaced it by saying
something along the lines of reading this could, uh, what it was, there was some,
there was some intense caution that I was like, you know what, I'll put this off
and I never put it back on.
Yeah, I think I'm still worried about that question.
Like I keep on taking stabs occasionally at writing a story set in that world and
keep on running into obstacles or writer's block, uh, and writing it.
And I don't know if I would like ever actually publish it.
If I could do it, I would like worry that if you like show people someplace
that is actually a nice place to live, they might start to see the present
world and a more negative life.
Um, I think that is like a valid thing to worry about, but I do think we can do a
lot better than, uh, Greg Egan's, um, upload for permutation city style
uploads who have nothing better to do with eternity than to learn exactly how
to carve table legs for 20 years because they have like exhausted all of the
actually interesting things worth doing and are now modifying their preferences
to the satisfied carving table legs.
So the, um, I think we can do better than the culture where people are just
sort of wandering around at the mercy.
Well, not the most I mean, machines have mercy, but the people are basically
useless and they know it.
Like the only reason they exist is because AI is a refraining from doing
certain things in order to give people a reason to exist.
And I think they can do better than that too.
But like, if you want to know what specifically an optimal scenario looks,
looks like, then, um, I think we're, I mean, I have ideas, but I think I would
prefer not to try to encapsulate them in the next 60 seconds.
That's totally fine.
I appreciate the answer.
Uh, it looks like we are right about the end of the half hour.
So, um, we can, we can let you go now if, if you have things to get on too.
Um, I can stick around for an extra five minutes, I think you want me to.
Uh, Steven, did you want to go with that first question that you were
had then?
Um, yeah, if you want to give a quick sound bite answer, um, I've been really
excited to see AI in the news a lot lately, even though a lot of it's kind
of fear intensive, you know, like Elon Musk and Stephen Hawking have been
talking a lot about it.
I was particularly excited that Sam Harris was talking about it.
You know, he has a big audience with the skeptic and, and atheist community.
And I think that he, I think his next book is actually going to be on AI
and this whole thing.
And I, I'm excited to see that the audience is getting bigger.
Do you have any thoughts or feelings about, I guess AI and pop news lately?
Well, I, I feel like it's basically not covering any of the real ideas or
arguments.
Um, like I, I, I'm not quite sure what the.
Deep problem is with journalism in modern society that prevents them from
covering any of the real ideas or arguments ever, but they're still
putting photos of the Terminator marching armies of robots with glowing
red eyes rather than like a more accurate depiction of what the problem is,
which might be like a tiny spear inside someone's bloodstream releasing
putolinium toxin and 10,000 years later, a picture of the Milky Way with a
10,000 light year radius spear gapped out on it, centered where the earth used to
be.
So like you, you still don't see like the real deal.
You don't, you don't see the actual arguments.
You don't see the actual ideas.
You don't see, um, if you don't, you don't, you, you, you still see the concept
of like robotic cars causing unemployment is AI anxiety.
And it's on an equal level with the 10,000 light year spear gapped out of the
Milky Way anxiety.
Like to them, it's all the same thing.
It all blurs together.
They, they don't have any, they don't have any concrete model of it.
They don't have a notion of like an actual world, world corresponding to
the words they're, they're emitting.
Um, if to them, it's all just like words, words and more words about AI
anxiety and all, and all the words are equal and it's just this one big blur.
And that's the kind of impression I get when I read this stuff in the media.
That hand, a lot of people talking without bothering to familiarize themselves
with the arguments that they think they're arguing against.
So one word answer is that it would say it's a positive or a net negative that
it's being talked about more.
I, I'm being like, based on the events that have actually happened recently.
Um, if there are positives that they have yet to materialize and the negatives
are kind of obvious.
So, uh, I mean, like maybe that will change in the foreseeable future, but at
least for now, like the consequences so far have not been without call positives.
Okay.
I appreciate it.
Thanks.
Uh, one last thing before you go, we were wondering, did you want to tease any of
our listeners with hints about the upcoming methods of rationality epilogue?
Like when it might be released or the timeframe of the, the setting of that
blog?
Um, well, the timeframe I had planned was at the beginning of their 70 year
pop-up board.
Uh-huh.
That was bad.
Whoops.
We wrote that question and I realized that I think that was in your announcement
of the epilogue scenario.
Anything, any, any additional information?
So, so your voice suddenly went a bit softer there, but, um, so, well, I mean,
like that in terms of additional information, but I will mention parenthetically
that I do not regret giving, for example, significant digits, a lot of time to run
as like the, uh, best HPMOR continuation thick, um, not competing with the
author's personal conception of what might have happened later.
Cause, you know, no matter, even if I can write a really awesome epilogue, it's
not going to contain like as much net meat as significant digits, the
continuation fan fiction bit.
Um, let's see, uh, further facts I can tease readers with.
Well, maybe this one's kind of obvious, but Luna Loggood will be in there.
Awesome.
I think.
Thank you so much for joining us today.
Um, we really appreciate you taking the time.
And although you can see us, all of us were nodding constantly while you were
talking and often grinning like idiots.
So, um, yeah, we really appreciate it.
And we hope you have a great night.
You warmed the cold abyss of my heart.
Have a great night as well.
Excellent.
Bye.
Thanks again.
Bye.
So, yeah, I had a question for you guys because I was a little bit confused.
What was that?
All right.
Sorry.
Go ahead.
It's okay.
Um, my question was regarding Eleizer's answer about, um, what a, uh, a
perfect AI world would be, perfect general AI world would be.
And he one explained the sovereign versus the genie.
Yes.
So could you, I guess, for the listeners, explain a little bit what the sovereign
is and what the genie is.
I was actually going to ask him about the sovereign because I was not familiar with
that term.
If I had to guess, so I think don't guess, do you have a good idea?
I, I've, I've heard of what I think fits.
I've never heard it by that name, but I think that there are, uh, two different
scenarios I've heard talked about.
Are we, are you thinking about singleton?
I don't know.
Okay.
I'm thinking one is the, the machine that you can come to and say, Hey,
how do you cure Alzheimer's?
Oh, do this.
Okay.
And the other one is the one that has a more active role.
So the one, one's the genie that can give you the answer to any question you
want and other one, maybe it's the sovereign that has the, the, the heavier
hand and actually does things other than just answers questions.
Okay.
Um, and now I feel bad for the stupid phrasing on my question about you press
enter and start, and I know that you would start it with it, you know, like a,
a closed ethernet and make it think it had access to the real internet.
And then at some point, then you plug it into, to whatever else it needs to get
into, but yeah, we were coming up with questions, you know, it's best we could.
And, and he mentioned that he was disappointed and he didn't quite know
what was wrong with journalism today.
And I don't know if this is a complete answer, but I think it's part of it.
Is that online journalism is written for people who written for the audience
that doesn't know how to use ad block.
Wow.
That's fair.
I think I, I mean, I, I'm just guessing, but I think that's probably part of it.
So like, they're not going to be looking to write for an audience about, you know,
nuanced, uh, discussion of, of the paperclip maximizer.
They're going to post a picture of the Terminator.
Ten ways the world is going to end.
Number four, we'll shock you.
I get a feeling a lot of people just have one general upper bound to anxiety that
they're like, Oh God, the AI is going to take my job and I'm going to starve to
death is as anxious as, Oh God, the entire Milky Way is going to be wiped out
because it kind of feels the end result is the same either way to them.
Right.
Isn't that known as scope and sensitivity?
Yeah, that's what I was going to say.
And I think that's exactly right.
And that just like we feel, you know, just as bad about one person dying as we do
about 10 million dying because like you can only feel so hard.
Yeah.
So like I'm thinking, even if it goes up a little, it does not go up by a factor
of a hundred billion.
When I think about like how much it would suck to like suddenly be homeless
because a robot took my job versus like, man, that would suck.
You know, what stuck a little bit more is that the whole universe is destroyed.
Right.
Yeah.
Man, I loved his story about being a kid and in that game with the the plastic
bowling pins, you know, it reminded me just while he was saying that I had been
telling somebody about how I boycotted professional sports because I didn't
like I didn't like how it was glorifying violence and how people were, you know,
pretty much being paid to give themselves concussions.
And and I thought that was horrible.
And now I'm going back to the moment that I said, that's it, I'm boycotting sports.
And it was right after the guy behind me dropped an entire beer on me.
Is that the real reason it could be and not that idealist?
Maybe that put you over the edge.
So I I liked the general lesson from that, I think, is to make a concerted effort
to know when you're lying to yourself.
So like one example from someone else's life, Sam Harris talked about in one
of his books, actually, this book called Lying, how he was offered the to give
the valedictorian speech at whatever university he attended.
And he was like, no, someone who's been here all four years should probably do it,
you know, because I think he transferred there.
And it wasn't until later that he admitted to himself.
It's because he had a terrifying or he was terrified of public speaking.
And if you've been willing to admit that to himself earlier,
he had to force himself to go through with it.
And there's there's a lot of jobs where we put people through a lot worse
physical things that they get paid a lot less for.
Like coal miners in general have drastically reduced quality and length of life.
And we don't pay them shit compared to what we pay professional sports athletes.
And their jobs will be replaced a lot sooner by robots.
And their lives will probably be much worse as a result.
And like the average football player's job would be right.
But I'm saying no one's boycotting, you know, things made from coal.
Yeah, no, I was just because of the violence it does to the minor bodies.
I think that people are. Are they? Yes. Okay.
I think it's almost impossible to do that because you can't
work out electricity in Colorado. No, but you can.
You can try to get renewable.
You can try to make sure that your your energy in your home comes from renewable
resources and make those efforts.
And there are certainly people who are very concerned about environmental
justice and in what happens to to workers.
There is a really stupid movie about that, too.
So maybe it was at least partly about the violence.
I saw somebody on Reddit talking about.
I think it was called Beneath. Don't watch it. Oh, wait, that's probably rude.
We should cut that out.
OK, if it's a terrible movie, let people know. Don't watch Beneath.
Katrina didn't like the movie Beneath. Yeah, it is beneath her.
That can't be. That's terrible. I'm sorry.
So, oh, speaking of boycotting, there was a post on Reddit about that.
And someone said they were, you know, how hard it is to boycott coal mining.
Someone said they boycott Nestle because they're like,
if you did your best to be like an evil corporation, that's like Nestle.
And someone's like, yeah, good luck.
And they linked to Nestle's Wikipedia page of their products.
And it's like everything that you buy to eat and sometimes things that you don't
even eat, like water. So there's like what?
Six Uber Corps that contain 80 that are responsible for 80, 90 percent
of what we could consume. I haven't heard that, but I believe that.
I believe it's something along that line. Yeah.
So going back to the interview. Yes.
I, you know, I've interviewed a lot of people that I really admire
over the past few years or not an interview, but interacted with.
But I still get that rush sometimes.
Like coming up to here even an hour beforehand, I was like, yeah,
you know, this is cool. I'm used to this. I got this down.
And then as soon as he answers the phone, I'm like, oh, my God.
Still the eyes are, I think I was, I think I was more freaking out
two hours ago than I was during the thing.
And then every because I was just focused on listening.
And then I was like, oh, wait, this is L.E.R.
Zuckowski, that guy who like I listened to on podcast or on, you know,
YouTube and stuff. It's like, oh, that makes it.
It's like when I had that kind of flip, you know, like when you're watching
a movie and then suddenly you realize like you're watching the movie,
protected on a screen, like every time I had that switch to realize
like, oh, this is actually happening.
And like, so that was kind of exciting.
I don't know. I don't want to sit here and narrow it out about it.
There it is. We're excited. It was fun.
Were you excited?
I was so excited.
It was dead board.
Stephen had to kick me to keep me awake.
Stephen didn't kick me. He's lying.
It's true.
I did make an effort not to kick this though.
Yeah. There we go.
We have a I have a water glass next to me and we're,
I don't know if we've mentioned this before, but we're broadcasting
from Stephen Zuber International Studios.
I believe we're calling it Stephen's Closet.
And we we have this this nice little set up
where our legs are next to each other.
And we can let people imagine that we have a studio.
It's up to you guys.
I feel bad. I didn't mean to to push on the A.I.
thing. I hope I didn't come off confrontational.
But even all the time.
Stephen always thinks that he's all confrontational and pushy and everything.
We're even never pushy. No, he's like the nicest guy.
He's like, can I offer you a glass of water?
I'm sorry, that was too pushy.
We're like, Jesus, Stephen.
I felt like this.
I felt like I was the one who challenged him on an answer he gave.
I think it was fine. OK, fine.
Yeah, I mean, I he didn't say, dude, shut the hell up.
That's rude. But I mean, I was like, I did.
I did. I was the only one who said that answer is not good enough.
Can you try again?
So.
But no, and I get that's not a question that I think of
those questions that could be answered easily in five minutes.
It would have been answered easily already.
But I did. I did it.
I can check that off my to-do list of ask
Eleazar Kowsky what the post-singularity world looks like.
So yeah, cool. All right.
So when he was when he was talking about what the post-singularity
world would look like, he mentioned it would look like and then referred to,
I guess, his fun theory sequence.
What is that?
It's a sequence about what makes life actually interesting and fun for humans.
And and why most utopias that people think of are actually really shitty
if you think of them, the PDF version.
I know of all the sequences back before they were like organized.
I could have sworn it opened with a quote saying this could like
radically reduce your utility or something.
Really? I need to find that.
And the funds.
Yes, I remember being spooked away from it.
But now I'm looking at this.
I read I read at least some of this because I knew about that analogy.
I knew about I mean, he talks about how like the idea of like
Christian heaven sounds great if you're just peasant.
Like in your you know, you're working 18 hours a day, your life's terrible.
You always hurt.
So like you go to a place where you don't have to work and you don't hurt
and you get to relax all the time.
But then like a week later, you're like sitting there just bored to death
because like sitting on a cloud, taking heart lessons like gets really dry.
Well, the good news is you get to look into hell and see all your loved ones
who weren't saved suffering.
Wait, I shouldn't have said that.
Christian heaven was always explained to me sounded basically like
an unending heroin hit.
I was like, oh, so we have heaven on earth.
It's not a big deal.
You just inject heroin.
Well, there.
Well, no, seriously, it's just, you know, blissed out happiness.
Right. Guys don't inject heroin.
No, don't inject heroin.
All religious people substitute religions with heroin.
I just I'm saying the Christian heaven that I was taught was basically,
you know, blissed out happy, which which is heroin.
And, you know, the lot of heroin junkies love it and live for it.
But I think there's there's more to life than just an ending bliss.
Yes, I think that now I'm sure I've read some of this
because I remember the counter argument, too, to the idea that
some people argue that we have a moral obligation to make
the future of humanity just one big orgasmatron machine.
All right, the super happy's.
But I think less fulfilled than the super happy's.
They seem to at least, you know, they wanted to fly around.
Maybe they're flying to the sun or the Zipnova just like to get resources
to keep building more super happy's or something.
But I got the impression that they had more to life than just laying in pure bliss.
Right. Right. They're incredibly intelligent, constantly.
Yeah, constantly exploring the universe, making new things,
integrating other cultures, potentially.
I got the feeling that it was really only the God was the term
for the super mother overmind thing that took control really quickly.
Uh, it's a Rugu or something like that.
Yeah, I got the feeling that there was only like
Kiritsugu, Kiritsugu.
Yeah, I got the feeling that the Kiritsugu was the only actual
human type thing we would recognize on that ship.
And everything else was just, you know, orgy time all the time.
They're like, oh, hey, look, there's some people to party with here.
Hey, big fucking network seem like he's ready to talk to.
And I always pictured him like Jack Donoghue from Third Rock.
Oh, my.
I mean, well, it describes, you know,
some immaculate guy in a suit or something or didn't it?
And I'm like, oh, I'm picturing Alec Baldwin. Absolutely.
So now we know what Steven's type is.
Distinguished old man.
The take charge kind. Yeah.
Who doesn't like distinguished old men, right?
No. So what we were what we were referring to is.
I kind of remember Iron Man's real name.
Rob Downey Jr. That's the one. Yeah. Fuck yeah.
What we were referring to was like Tony Stark.
Sorry. It's OK.
Was what was the name?
The Baby Eater's in the Super Happy was the name of three worlds collide,
three worlds collide, which is a fictional essay by Eleazar Yikowski,
the person we just had on.
So you should check that out.
And since you know, I probably isn't going to say so,
there is an audiobook version of that available online.
If you want to just listen to it on your commute rather than take the time
to sit and read what like a chapter is, it's not long. Yeah.
But it's a fun dive into the subject of meta ethics.
Yeah. Oh, shoot.
You know what I wish that we had asked if he had anything to plug?
Oh, but we can still ask that.
So here, I'm charging you, Enya, to contact Eleazar and ask him
what he wants to what he wants to plug, what he wants to promote
so that we can make sure that we include it.
And then we'll insert that right at the end of his interview or right here.
Okay. So Eleazar brought up some interesting concepts,
some of which we feel like we should clarify now that the interview is over.
He brought up, for example, some different kinds of super intelligences
that could exist, including a genie and a sovereign.
So for all of these, we've included links on the website.
So you can go to the Bayesian conspiracy dot com.
And go to episode three and you'll be able to see all of these in much greater detail.
But does anybody want to take a quick stab at defining a sovereign?
What a sovereign AI would look like and what a genie AI would look like?
I will preface this by saying that this terminology comes from
Nick Bostrom's book, Super Intelligence, which is a not foundational,
but fairly important work nowadays in the AI field.
In the AI communication department, absolutely.
I have very basic definitions right here.
If you follow the link, you'll see exactly this.
So a genie is an AI that carries out a high level command, then waits for another.
And then a sovereign is an AI that acts autonomously in the world
in pursuit of potentially long range objectives.
So the distinction is basically, do you want to create a God
that is going to run the universe for you, but do it hopefully in the best way possible?
Or do you want to make one that sits in the box and waits to do things that you ask it to do?
Also, it could be the genie from those horror books that we read when we were kids.
The Monkey Paw side effects.
Yeah. And they're always going to misinterpret what you want and do something evil.
Believe it or not, there is a website and I can find it if we want to link to it
where people have worked to articulate the best wish.
Yeah, that so in the event that just in case you ever run into a genie.
Yeah, if you ever run into an actual exercise and it's along the lines of like,
I wish that I can continue to persist in my current state of mind, etc., etc.
Yeah, it's like seven paragraphs of disclaimers and caveats and things.
So the genie will not misinterpret your words in any way.
Something else that Ali Eiser brought up was the fun theory sequence
about what an ideal world would look like if everything went perfectly.
So I did some reading on that because in the interview,
I'd mentioned that I didn't read that sequence in its entirety.
And then when I was going through reading it again,
or I guess reading it for partially the first time,
I've read at least half of it before because a lot of it's familiar,
but it essentially outlines utopia in a way that doesn't suck.
It's one of its core theses is that standard utopia ideas are terrible.
And they've actually imagined living them for like a month.
Things get really boring, really fast.
So it's a fun distillation of that general hypothesis and the link to it
on the web on our website contains what 31 short summaries of all the posts.
So 31 laws of fun. Yes.
It's also a decent treaties on what makes is that the right word?
Treaties treat us treat us treat us treat us treat us.
OK, on what makes human existence fun.
And, you know, what makes existence for human a human thing as opposed to being a robot?
But in addition to being fun to read,
it's also probably a thing that anyone aspiring to write a utopia
or even dystopia story might want to look into just because it covers a lot of the basics
that sometimes you read a utopia story and you're like, this is stupid.
No one would like this this world.
Yeah, or or or missing really any one of the of the 31 laws of fun.
It's basically a recipe for for dystopia, you know, novel entertainment
as opposed to repeating the same pleasurable thing over and over.
And uses the analogy of, you know, playing a great video game,
but doing the same one more than a few times gets really boring.
And changing the colors of the characters on the screen isn't enough to make it novel again.
So yeah, but little little things like that.
Neat. And then off air, we asked Eleizer if he had anything that he wanted to plug.
And Eleizer said that he would like to plug a new website
that he's been working to develop called Arbital.
We will include a link again on the episode three page.
This is what's up right now is bare bones just launching
doing some beta testing, but this is a resource for explaining things to people.
It's kind of like the he's hoping that it will become one day
the Wikipedia of explaining concepts.
And it starts out with a explanation of Bayes theorem.
And it's supposed to be things that as you're going,
there's hover overs that will, you know, pop up more detail if you want it.
They can explain it at various levels of complexity, depending on how much you already know.
And hopefully we'll one day also cover things like arguments
so that instead of having the same damn theist versus atheist argument
for the 10 millionth time on Reddit, you can just point people at this and say,
like, look, here, there's the problem of evil.
Here's the pros arguments.
Here's the anti arguments.
Just read it so we don't have to have this discussion.
I have a feeling that we'll be using this resource quite often.
I hope so.
If, you know, if it launches and it's useful, yeah, it looks fun.
I love the little I saw of it.
So worth checking out on the website.
Yeah, so the Bayesian theorem explanation was really good.
I read through it.
Yeah. OK, so there you have it.
One reviews in and it's good.
So as always, thank you so much for listening.
You can email us with questions or feedback at Bayesian conspiracy podcast at gmail.com.
Visit our website, the Bayesian conspiracy.com.
And yeah, check us out.
OK, I think that's it.
See you back in two weeks for the next episode of the Bayesian conspiracy.
Bye. Bye.
