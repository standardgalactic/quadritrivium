I could have sworn it opened with a quote saying this could like
radically reduce your utility or something.
Really? I need to find that.
And the funds.
Yes, I remember being spooked away from it.
But now I'm looking at this.
I read I read at least some of this because I knew about that analogy.
I knew about I mean, he talks about how like the idea of like
Christian heaven sounds great if you're just peasant.
Like in your you know, you're working 18 hours a day, your life's terrible.
You always hurt.
So like you go to a place where you don't have to work and you don't hurt
and you get to relax all the time.
But then like a week later, you're like sitting there just bored to death
because like sitting on a cloud, taking heart lessons like gets really dry.
Well, the good news is you get to look into hell and see all your loved ones
who weren't saved suffering.
Wait, I shouldn't have said that.
Christian heaven was always explained to me sounded basically like
an unending heroin hit.
I was like, oh, so we have heaven on earth.
It's not a big deal.
You just inject heroin.
Well, there.
Well, no, seriously, it's just, you know, blissed out happiness.
Right. Guys don't inject heroin.
No, don't inject heroin.
All religious people substitute religions with heroin.
I just I'm saying the Christian heaven that I was taught was basically,
you know, blissed out happy, which which is heroin.
And, you know, the lot of heroin junkies love it and live for it.
But I think there's there's more to life than just an ending bliss.
Yes, I think that now I'm sure I've read some of this
because I remember the counter argument, too, to the idea that
some people argue that we have a moral obligation to make
the future of humanity just one big orgasmatron machine.
All right, the super happy's.
But I think less fulfilled than the super happy's.
They seem to at least, you know, they wanted to fly around.
Maybe they're flying to the sun or the Zipnova just like to get resources
to keep building more super happy's or something.
But I got the impression that they had more to life than just laying in pure bliss.
Right. Right. They're incredibly intelligent, constantly.
Yeah, constantly exploring the universe, making new things,
integrating other cultures, potentially.
I got the feeling that it was really only the God was the term
for the super mother overmind thing that took control really quickly.
Uh, it's a Rugu or something like that.
Yeah, I got the feeling that there was only like
Kiritsugu, Kiritsugu.
Yeah, I got the feeling that the Kiritsugu was the only actual
human type thing we would recognize on that ship.
And everything else was just, you know, orgy time all the time.
They're like, oh, hey, look, there's some people to party with here.
Hey, big fucking network seem like he's ready to talk to.
And I always pictured him like Jack Donoghue from Third Rock.
Oh, my.
I mean, well, it describes, you know,
some immaculate guy in a suit or something or didn't it?
And I'm like, oh, I'm picturing Alec Baldwin. Absolutely.
So now we know what Steven's type is.
Distinguished old man.
The take charge kind. Yeah.
Who doesn't like distinguished old men, right?
No. So what we were what we were referring to is.
I kind of remember Iron Man's real name.
Rob Downey Jr. That's the one. Yeah. Fuck yeah.
What we were referring to was like Tony Stark.
Sorry. It's OK.
Was what was the name?
The Baby Eater's in the Super Happy was the name of three worlds collide,
three worlds collide, which is a fictional essay by Eleazar Yikowski,
the person we just had on.
So you should check that out.
And since you know, I probably isn't going to say so,
there is an audiobook version of that available online.
If you want to just listen to it on your commute rather than take the time
to sit and read what like a chapter is, it's not long. Yeah.
But it's a fun dive into the subject of meta ethics.
Yeah. Oh, shoot.
You know what I wish that we had asked if he had anything to plug?
Oh, but we can still ask that.
So here, I'm charging you, Enya, to contact Eleazar and ask him
what he wants to what he wants to plug, what he wants to promote
so that we can make sure that we include it.
And then we'll insert that right at the end of his interview or right here.
Okay. So Eleazar brought up some interesting concepts,
some of which we feel like we should clarify now that the interview is over.
He brought up, for example, some different kinds of super intelligences
that could exist, including a genie and a sovereign.
So for all of these, we've included links on the website.
So you can go to the Bayesian conspiracy dot com.
And go to episode three and you'll be able to see all of these in much greater detail.
But does anybody want to take a quick stab at defining a sovereign?
What a sovereign AI would look like and what a genie AI would look like?
I will preface this by saying that this terminology comes from
Nick Bostrom's book, Super Intelligence, which is a not foundational,
but fairly important work nowadays in the AI field.
In the AI communication department, absolutely.
I have very basic definitions right here.
If you follow the link, you'll see exactly this.
So a genie is an AI that carries out a high level command, then waits for another.
And then a sovereign is an AI that acts autonomously in the world
in pursuit of potentially long range objectives.
So the distinction is basically, do you want to create a God
that is going to run the universe for you, but do it hopefully in the best way possible?
Or do you want to make one that sits in the box and waits to do things that you ask it to do?
Also, it could be the genie from those horror books that we read when we were kids.
The Monkey Paw side effects.
Yeah. And they're always going to misinterpret what you want and do something evil.
Believe it or not, there is a website and I can find it if we want to link to it
where people have worked to articulate the best wish.
Yeah, that so in the event that just in case you ever run into a genie.
Yeah, if you ever run into an actual exercise and it's along the lines of like,
I wish that I can continue to persist in my current state of mind, etc., etc.
Yeah, it's like seven paragraphs of disclaimers and caveats and things.
So the genie will not misinterpret your words in any way.
Something else that Ali Eiser brought up was the fun theory sequence
about what an ideal world would look like if everything went perfectly.
So I did some reading on that because in the interview,
I'd mentioned that I didn't read that sequence in its entirety.
And then when I was going through reading it again,
or I guess reading it for partially the first time,
I've read at least half of it before because a lot of it's familiar,
but it essentially outlines utopia in a way that doesn't suck.
It's one of its core theses is that standard utopia ideas are terrible.
And they've actually imagined living them for like a month.
Things get really boring, really fast.
So it's a fun distillation of that general hypothesis and the link to it
on the web on our website contains what 31 short summaries of all the posts.
So 31 laws of fun. Yes.
It's also a decent treaties on what makes is that the right word?
Treaties treat us treat us treat us treat us treat us.
OK, on what makes human existence fun.
And, you know, what makes existence for human a human thing as opposed to being a robot?
But in addition to being fun to read,
it's also probably a thing that anyone aspiring to write a utopia
or even dystopia story might want to look into just because it covers a lot of the basics
that sometimes you read a utopia story and you're like, this is stupid.
No one would like this this world.
Yeah, or or or missing really any one of the of the 31 laws of fun.
It's basically a recipe for for dystopia, you know, novel entertainment
as opposed to repeating the same pleasurable thing over and over.
And uses the analogy of, you know, playing a great video game,
but doing the same one more than a few times gets really boring.
And changing the colors of the characters on the screen isn't enough to make it novel again.
So yeah, but little little things like that.
Neat. And then off air, we asked Eleizer if he had anything that he wanted to plug.
And Eleizer said that he would like to plug a new website
that he's been working to develop called Arbital.
We will include a link again on the episode three page.
This is what's up right now is bare bones just launching
doing some beta testing, but this is a resource for explaining things to people.
It's kind of like the he's hoping that it will become one day
the Wikipedia of explaining concepts.
And it starts out with a explanation of Bayes theorem.
And it's supposed to be things that as you're going,
there's hover overs that will, you know, pop up more detail if you want it.
They can explain it at various levels of complexity, depending on how much you already know.
And hopefully we'll one day also cover things like arguments
so that instead of having the same damn theist versus atheist argument
for the 10 millionth time on Reddit, you can just point people at this and say,
like, look, here, there's the problem of evil.
Here's the pros arguments.
Here's the anti arguments.
Just read it so we don't have to have this discussion.
I have a feeling that we'll be using this resource quite often.
I hope so.
If, you know, if it launches and it's useful, yeah, it looks fun.
I love the little I saw of it.
So worth checking out on the website.
Yeah, so the Bayesian theorem explanation was really good.
I read through it.
Yeah. OK, so there you have it.
One reviews in and it's good.
So as always, thank you so much for listening.
You can email us with questions or feedback at Bayesian conspiracy podcast at gmail.com.
Visit our website, the Bayesian conspiracy.com.
And yeah, check us out.
OK, I think that's it.
See you back in two weeks for the next episode of the Bayesian conspiracy.
Bye. Bye.
