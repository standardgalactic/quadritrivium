Those are bad ass words.
Um, rather than argued into it, argued into existence, but to be satisfied
by this answer, one must see rationality in terms of engines rather than
arguments, twight.
Did we want to touch on this thoughts thing real quick?
Yeah.
Okay.
I don't know.
I'm not sure how relevant it is, but this was in response to the thing that, uh,
a priori beliefs are discoverable by mere operation of thought.
Eleazar points out that thoughts exist in the universe.
They are identical to the operation of brains.
Oh, I skipped that part.
Yeah.
Material brains in the real universe.
Uh, so when you add one plus one and get two by thinking, those
thoughts are embodied in flashes of neural patterns.
In principle, we could observe experientially the exact same material
events as they occurred within someone else's brain.
You could see someone else's engine operating materially through material
chains of cause and effect to compute by pure thought that one plus one equals
two.
How is observing this pattern in someone else's brain any different as a way
of knowing from observing your own brain doing the same thing?
When pure thought tells you that one plus one equals two independent of any
experience or observation, you are in effect observing your own brain as evidence.
Checkmate atheists.
That like, I don't know, uh, if you actually use that, I don't think so, but
like it reminds me of the idea of armchair philosophizing.
Yeah.
Where?
Or any sort of philosophizing.
Um, I know, I personally just got a distaste for it.
Now there's good philosophy.
Yeah.
I think there's excellent philosophy.
Yeah.
I think you go that far.
I think there's good and excellent philosophy when you're not like, you're
not trying to use philosophy to generate beliefs about the world, but like, you
know, it, um, you know, virtue ethics is the philosophy about how to live, right?
Yeah.
Stoicism, another good example.
There's also philosophy that takes like evidence into account and.
Yeah.
I mean, what, what can we reasonably infer?
That's, I mean, I don't, I don't like drawing hard lines around what's
philosophy and what's thinking other of other kinds of thinking patterns.
But yeah, I think philosophy, any, any decent philosophy can be replaced
with a good enough Bayesian statistical model.
It's like, so this is why the philosophy channel in our discord is named
math for babies because it, once you apply enough math to philosophy, it
just reduces to, you know, math.
Well, that's just your philosophy, man.
If you don't know math, if you're doing math for babies, you're a philosopher.
I don't know.
There's utilitarian things that, that don't work great.
Like, uh, what is it?
Repugnant conclusions or whatever.
And that depends on which utilitarian arguments you accept, I guess.
Yeah.
And that does kind of come down to philosophy.
I mean, which is why philosophy is bad because it comes down to arguments
that you're trying to make other people accept as opposed to laws of statistical thinking.
Well, like if you're trying to decide what you should do, yeah, you know, I
don't know what laws is laws of statistical thinking I'm looking for.
And I'm deciding whether or not to buy cruelty free milk.
Right.
Well, I mean, when we're talking about the more colloquial sense of, ah, see
good and bad, I like good philosophy, then yeah, sure, that's, that's great.
But when I think of the philosophy, I dislike it's more about the armchair
mental masturbation that gets done in high level universities.
Or just, or, you know, philosophy 101 classes.
I mean, that was my, like, those are some of the most fun classes I had.
If you just sit there arguing this random shit, it's great.
Yeah, yeah, it's a lot of fun.
Yeah, like you're, you know, the first time I encountered like serious
arguments for dualism and stuff were like riveting, if not compelling, right?
I mean, once you have serious people discussing arguments for the existence
of philosophical philosophical zombies, that's what I'm like.
Okay, you've, you've, you've clearly lost your own head on your side, your ass.
I like that.
Do some spelunking.
Go spelunk your way for you from this, this weird belief you've got.
All right, so we have for next time priming and contamination.
And do we believe everything we are told?
Excellent.
And I haven't read that one a long time.
Does that touch on that research that like in order to understand a statement,
you have to tacitly believe it for a second?
I think that was Spinoza who said that.
Oh, maybe.
Yeah.
Like I haven't read it yet and I don't remember exactly what post that was.
So maybe we'll find out.
And if not, that was, I said experiment and then I mentioned Spinoza in the same
sentence, Spinoza was a philosopher, so careful.
This was less of like a, I'm just kidding.
So the word Spinoza, yep, there is Spinoza.
Great.
All right.
So we'll save it for next time.
Yeah.
Come back in two weeks to find out.
Next episode of the Bayesian conspiracy.
We need funny commercials.
This episode brought to you by Givewell.
I, rather than funny commercials, I would like, like existentially creepy
commercials like they had on, um, I feel, yeah, those were the best.
Yeah.
Speaking of a brought to you by Givewell, shameless plug.
So I finally, I started a new job with some caveats.
It's like the same product, but new company and, um, came with a pay bump.
So what I did, and then on the, whatever, first paycheck of February, not the
last one of January, I got finally paid out this bonus for my last job.
My last, like my first week or my last week of, of earned pay at the last
job, my PTO, and then my first paycheck from this job.
And so it was a great, yeah, great Friday.
And so what I did was I finally went to Givewell.com slash rationally speaking
and set up a recurring donation, which is matched the first month up to $250.
So everybody should do that.
And I'm humble bragging.
I'm not really bragging.
I'm just stoked.
I finally got, got off my ass and actually did this.
But everyone else can.
And if you go to, uh, I think, I think, um, Barry vett wizards also has the same
pitch, but if you go there to, like I said, Givewell.com slash whatever podcast,
make sure it, you know, look it up first, but I did rationally speaking.
And then you select it from like the dropdown.
I got an email a few days later saying, yep, your first donation will be matched.
So Tim Ferriss just, uh, plug that as well.
Perfect.
Yeah.
I mean, so lots of, lots of anyone, I guess it's matched by rationally speaking.
I think it's matched by some generous donator with deep pockets.
Um, I don't think, because I know that like apparently Tim Ferriss,
the show is doing it, uh, uh, very bad wizards is doing it.
I think it's just a promotion that they have.
Like, Hey, if you're referred here by one of these people, we'll match your
first donation.
Why did not anybody consider the Bayesian conspiracy for this?
We haven't asked, we haven't talked to Givewell yet.
Oh, do we have to approach them?
That probably makes more sense.
I mean, they have to build the endpoint for, you know, Givewell.com slash
Bayesian conspiracy, right?
So yeah, yeah.
Um, or at least have it leads straight to the same page of all the other ones.
So nobody donate anything yet in two weeks.
There might be something where you can do it through us.
I don't think, I don't think rationally speaking gets anything.
I just emailed them.
That's fine.
Just all I want is the mad props.
It's not about who you're helping.
It's about the props.
Yes, exactly.
Validate us.
That's the only reason I live or do anything.
So when I did check out though, I didn't get a thing that said your first
and the donations match.
So I did a reply because it was like, do you have any questions?
And I was like, yeah, did Julia Galef from actually speaking get like a, you
know, a tick mark on the board for referring me?
Cause I said that she did.
And they're like, yep, totally.
You'll get that email in a few days.
And I did.
So cool.
Yeah.
Just to be clear, Julia Galef is her, you know, give her credit for this.
She's, she's fucking awesome.
Yeah.
And I was like, yeah, she's like the best, like not professional EA activist,
EA activist in the world, as far as I'm aware, right?
And like, I'm like, why am I telling him this?
He knows this.
The guy I'm talking with at give well, I'm sure.
But I felt, I felt the need to justify writing my email in more than just a sentence.
That's true.
He's like, why do you think she's on our site?
Cool.
All right.
So we probably going to get through a bunch of feedback and some other topics.
Probably the other topics first, but I wanted to do this one thing of feedback
before we start in on that, because it relates to what we were just talking about.
Because just a second ago, we were talking about, hey, this post that we're
going to read next time called, do we believe everything we're told?
And I was like, oh yeah, I remember about this whole changing our minds thing
that somebody wrote in about.
So want to touch on that real quick.
Koi from the discord or the poster formerly known as Koi.
And now is two characters that I don't know how to pronounce, but apparently
they're pronounced Koi.
So still Koi, just in a different language, maybe.
I'm not even sure what characters, what language it is.
But yeah, maybe it's not even a language.
I don't know.
I mean, it has to be something deferred to be from build to type it in as a user
