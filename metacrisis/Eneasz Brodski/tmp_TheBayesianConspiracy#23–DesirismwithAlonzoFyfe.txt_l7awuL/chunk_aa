Hello, this is Zenyash, and this is Steven, and we are the base in conspiracy. Katrina
is not with us today. Last episode, we talked about ethical theories. Yeah, moral philosophy.
Moral philosophy. And while we were doing that afterwards, I said, you know, there's
this guy named Alonso Fife who has a blog called The Atheist Ethicist, and he was a
huge influence on my ethical thinking over the past decade or so. And how would you guys
feel about just talking to him for a little while? And I said, yeah, sure, that sounds
like a good idea. So I got into contact with him, and he agreed to come on the show. And
this is our interview, Steven and me with Alonso Fife.
The sound quality was a little touch and go. We ended up taking a break partway through
to go reset the router. We're not sure where the problems are coming from, but do forgive
the subpar audio quality. Anyway, thanks. Hope you enjoyed the episode.
Okay, now that we are actually recording, I would like to introduce you again. This
is Alonso Fife on the line with us. He is the, he blogs at the Atheist Ethicist blog,
and I have been a fan of his for a long time. So as a follow up to last episode, we decided
to call him and talk with him about moral theory. Alonso, I'm sorry I interrupted you
earlier. Could you introduce yourself and briefly tell us about desirism?
Okay, my name is Alonso Fife. When I was in junior high, I got interested in moral philosophy.
What I noticed was that there were a lot of different people making a bunch of different
claims. And I wanted to know which ones were right. So, well, that got me into moral philosophy,
which I've been studying for quite some time. Desirism is the idea that actions really aren't
the primary object of moral evaluation. When people evaluate or make moral evaluations,
what they're actually evaluating are desires. And the evaluation of actions is derived from
that. So, a right act is the act that a person with good desires would do. It's not right
in itself or for its own sake. And usually the first question that gets asked when someone
hears that is, what makes a desire good? What makes a desire good is its capacity to fulfill
other desires. One of the main things about desirism is it holds that desires are the
only, well, the technical phrase is it's the only end reason for intentional action.
The only reason that you have to do something or to avoid something is because it either
fulfills or it forwards a desire. And that also applies to, that applies to evaluating
anything. Everything is evaluated according to its relationship to desires. But that includes
other desires. Each desire is evaluated according to its capacity to either fulfill or thwart
other desires. Is desirism your term or does that go back further? More or less, it's my
term or it's the term that's applied to these ideas that I came from. It came about in part,
I had a couple of people convince me to adopt the term desirism. Originally, I believe
you called it desire utilitarianism. Yeah, it was originally called desire utilitarianism,
which is a version of motive utilitarianism, which came out by R. M. Hare, no, Robert Adams
in the 1974 Journal of Philosophy. Desire utilitarianism, it's pretty much the same
thing, except it holds that desires are evaluated according to their capacity to maximize utility.
And then the question is, what is utility and why does it deserve to be maximized? Ultimately,
when I came to the idea that a desire has to be evaluated according to its capacity to
fulfill other desires, that meant dropping the utilitarianism aspect and just adopting
desirism. So did you become convinced that what utility is, is desires being fulfilled
or did you just change what you actually think the correct evaluation of things is?
Well, desires being fulfilled, there's no such thing as an intrinsic value. You can't
identify something in the universe and say, this has value for its own sake. Everything
has value because of its relationship to desires. And that applies to desire fulfillment itself,
it doesn't have any type of intrinsic value. If, for example, I like chocolate cake, if
I'm eating chocolate cakes, then that's a good thing because it fulfills my desire to
eat chocolate cake, but desire fulfillment itself isn't what has value, it's the eating
the chocolate cake that has value to me because I have a desire for chocolate cake.
So when you use the word desire, is that the same kind of thing that, say, you know,
a lion has a desire to catch prey animals or is it something that we learned in this
little bit on our last episode we were talking about, that in my experience in the field
of moral philosophy, most consideration of things that can be considered moral agents
are rational agents that exclude children, or against human children, mentally infirm
humans and basically nonhumans. So it wouldn't make sense to morally judge a cat or a dog
or whatever. So does desires encompass the desires of nonhumans in that same regard?
Well, animals have desires, which means animals can be harmed or benefited. I don't think
that part is controversial. As far as holding animals morally responsible, that would be
kind of a waste of time because they can't understand the moral claims. So that applies
also to young children or to the mentally infirm. So they're not moral agents, but
they are what's called moral patients. They can be the subject of morality even though
you can't hold them morally responsible for their actions.
That makes sense.
I like the distinction that they still have desires, but okay, I guess I'm skipping ahead
here, that the tools of moral instruction tend to be things like reward and praise and
condemnation. And since they don't have the ability to reward or praise or punish us,
that it makes it very hard for their desires to be taken into our calculations.
Well, yeah, that's right. They don't have the capacity to know that they can modify
our behavior by modifying our desires. The way we do to them, and we do do that to animals,
we do punish and praise animals as a way of modifying their behavior to suit our interests,
particularly that it particularly applies to pets and trained animals.
So I can, I guess I skipped ahead and I want to walk back a little bit. This whole concept
of desires being not just things that are what we, not just things that are evaluated,
but things that are altered by humans.
Yes. Desires can be molded, and that's where the idea of praise and condemnation or reward
and punishment come in, is that's the way that we change the desires of other creatures,
other human beings.
When you praise something, you create in the other people a stronger desire to do that,
which is praise, or if you reward something, it creates an interest in doing that, which
is rewarded. And that gives us a reason to reward such things as charitable actions,
or honesty, or kindness. And the opposite is true with respect to punishment and condemnation.
That tends to form an aversion to certain types of things or certain types of acts.
So we have reason to condemn and to punish things like breaking promises, or taking
other people's property without their consent, or vandalism, or assault. We condemn those
things in order to create an aversion to those particular types of actions.
How do you weigh competing desires? Say, someone's desire to eat a chicken sandwich
and a chicken's desire to not be made, or any other two examples? I guess what I'm asking
in general is, when desires conflict, what's their formula for resolution?
Well, desires do have a weight. You notice that whenever you act, you have conflicting
desires, but you have the capacity to consider some of them being more important than others,
some of them are stronger than others, some are weaker. And that is true on an interpersonal
scale as well. One person has a strong desire for something, another person has a weaker
desire. But I do need to warn against one of the things that desirism doesn't do, is
it doesn't say that you weigh all of the desires and you do whichever fulfills the most and
strongest desires as a moral principle. It evaluates desires according to their capacity
to fulfill other desires, so it seeks a type of harmony amongst desires, it seeks desires
that don't conflict. I can see that. I guess, but when they do, I guess, what am I trying
to say? I come from a utilitarian disposition, so I'm trying to think of a way to not think
about it in the terms of maximization. I'm still curious. So the utilitarian has a pretty
solid stance on eating animal products or something. They'll say, well, the pleasure
you get out of eating whatever hamburger doesn't outweigh the pain that went to making it,
etc. Certain conditions haven't been met or something. Do you, I guess, does desirism
amode, I guess, for how to answer those elements?
Well, desirism would look at what's true about that particular situation, or these types
of situations. Yes, animals have desires. Unfortunately, they can't mold our desires
because they don't understand the concepts of praise and condemnation, so they can't
get us to dislike the things that would hurt them. But humans do have a reason to make
sure that other humans are kind and unwilling to inflict pain on other creatures. If you're
willing to inflict pain on other creatures, you may be willing to inflict pain on other
humans. And also, humans have the capacity to care for animals, and if I care for an
animal, I have a reason to cause other people not to harm that animal. So, we do have reasons
to mold the desires of other humans in order to protect and care for animals.
For sure. I think I'm still trying to keep my head around the goodness in the satisfaction
of a desire. Is that one of the main things that's going on here? I'm still trying to
understand. Yeah, that's actually the biggest problem with desirism, the biggest problem
that people have, is the idea that they want to interpret it as desire satisfaction, having
some type of intrinsic value. What matters is that a desire is satisfied. And that's
not the way the theory works. If I have one of the examples that I use is I just imagine
a planet where there's one being on the planet. I tend to call him Elf, and he has one desire
which is a desire to gather stones. So, him gathering stones doesn't have any type of
intrinsic value. Nothing in that planet in that world has intrinsic value. However, gathering
stones is an action that has value to Elf because Elf has a desire to gather stones.
So, a utilitarian would look at this from the point of an observer looking down and
say that because Elf's desire is satisfied that that has some type of intrinsic goodness.
Desirism says if you're looking down from the outside, nothing has value unless the
person looking down also has a desire. And it's only that desire that anything that he
sees has any value. So, if I'm looking, if the impartial observer is looking down, sees
Elf gathering stones, knows that Elf has a desire to gather stones, he's completely
indifferent to that fact unless he has an interest in desires being fulfilled.
Got you. Yes, I think I understand.
Okay, so, nothing has value unless you can relate it to a desire. It only has value
to the person with that desire, and so far as it fulfills that desire.
Yeah, I think that sounds pretty clear and it sounds appealing. I like the overall idea.
I do wonder, I mean, so yeah, if I have true desires that are conflicting, say my desire
he chocolate cake, I'm also a fan, and my desire to stay physically fit, those desires
can't both be fulfilled at that same time, but I obviously feel more strongly for one
than the other. But I guess, how would I imply that on a larger scale? If there are two people
that desire two different things? Yeah, I mean, or they both desire the same thing,
right? And they didn't care about each other, I guess, whichever one gets it.
If you're talking about a simple situation like that, you have two people, or two beings
that both desire the same thing, then effectively, as a matter of descriptive fact, they're going
to fight about it. They don't have any other option available to them unless you introduce
some other desire to resolve it. But desirism is actually interested in molding desires
or choosing desires. So let me modify my previous example a little bit. I had Alf on a planet
with a desire to gather stones, that's what he wanted. Now, let's introduce a second
person to that planet, a person that I tend to call Bet, and we'll give Alf the ability
to choose Bet's desire. And Alf could give Bet one of two desires, either a desire to
gather stones like him, or a desire to scatter stones. Now, looking at this from Alf's point
of view with his desire to gather stones, and let's make the further assumption that
there's a limited number of stones, so what Alf does is he gathers stones for a while,
then he's got them all in one big pile and he has to scatter them again. By giving Bet
a desire to scatter stones, he can spend all of his time gathering stones. So he has a
reason to give Bet a desire to scatter stones, that's the option, that's the rational choice
for him. So the desire to scatter stones for Bet is the better desire, and that's the
desire that he gives that new person.
Sounds like a great position for Alf.
Yeah, but it also turns out to be a good position for Bet, because Bet gets this desire to scatter
stones, and there's already somebody in the world that's gathering stones for Bet, and
so Bet ends up just being in a good position as well. And these are the types of situations
that desirism looks for.
So what we would want to do in Steve's example is to make someone who has the desire to bake
chocolate cakes, but not eat them?
That would help. Or if they like to bake chocolate cakes, just bake enough chocolate cake for
two people. Let's generalize that, or let's look at something in a larger community.
Let's look at the type of situations that we have. We have reasons to have other people
tell us the truth, because we need true information in order to fulfill our desires. If they lie
to us or they give us false information and we use that information and we're not going
to get the things that we're aiming for.
So we have reason to cause other people to tell us the truth, and the tool that we have
for doing that are condemnation and punishment when they lie, and praise and reward when
they tell the truth.
So we're trying to create desires that actively help people cooperate and get along and fulfill
their other desires, and that's the way we do it, and that's what conventional morality
really consists in, is using these tools of reward and punishment and praise and condemnation
to get people to like and dislike certain things that are more useful for us to have
them like and dislike.
I think one of the things I like about Desirism is that it starts out as being a descriptive
theory first, that with any theory you first have to describe what you are working with
before you can make recommendations in it. Like if someone wants to know how to get to
the moon, you first have to describe how gravity works and how chemicals interact in order
to produce force and what that will do with the materials you're working with, and then
once you have the framework where you can describe everything that happens, you can
make recommendations off that given what we know about physics, this is what you would
do in order to get to them.
Right. Everything in Desirism is what's called a hypothetical imperative. There are no categorical
imperatives in it, which means everything is built on wants. If you want X, what do
you have to do to get X?
And in the moral case, for just about everything that I want, I need other people to be honest
with me. So given that, and the same is true with you and with everybody else, you have
reasons to want others to be honest with you, and then the use of these tools of reward
and punishment is simply the way of causing them to have this aversion to lying, which
is beneficial to you, and they're causing you to have an aversion to lying, which is
beneficial to them, but everything's built on hypothetical imperatives.
So really, what I consider neat aspect of this is that you aren't really working with
acts, so you aren't trying to encourage people to always do the act of telling the truth,
what you're trying to do is give them the desire of wanting to tell the truth, so that
it's self enforcing. You don't have to worry about someone lying to you if you know they
have a deep desire to want to tell the truth all the time anyway, which is why the evaluations
are of desires rather than acts, because then they give the motivation with them.
