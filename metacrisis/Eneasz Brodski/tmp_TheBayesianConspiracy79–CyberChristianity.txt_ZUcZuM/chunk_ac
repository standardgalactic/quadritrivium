if they filmed this crime and put it on TV, somebody would get in trouble. I wonder if the
Japanese law legal system has some kind of exemption for game shows. If you make enough money,
it's okay to do whatever. So like they have to make sure it's really popular to sell the ad space
first. Right. Anyway, the show, they had a vegan on one time and she was saying, I absolutely,
I've taken survival courses. I know how to identify plants that are edible so forth and so on. She
was like just saying, basically, I'm going to stick to my principles. But after three days of not
eating, she ate part of a lizard, which was the only thing the other guy was able to catch. He was
also like, he's this big time hunter. I don't know. I just find it really funny to watch this show
because these people like really talk up their ability to survive. And then every episode I've
seen, they just fell miserably to eat anything and they're just starving for a really good
bunch of days. Yeah, it's fun. Like the, I mean, the seriousness of how much we can take the set
phase value, given that it's on TV aside, it's a nice reminder that like we're not the dominant
species on the planet because we can like do all the cool stuff when we're totally naked, right?
It only takes like two nights of sleeping with bugs crawling in your butt that you're like, oh,
fuck this. I'm, you know, I want to go back inside with my nice machines and tools and clothes and
all that stuff, right? Yeah. Yeah, we're basically like hairless cats. We're like so vulnerable.
I don't understand. Cats have claws. We don't have claws. Yeah, I'm just talking about their
lack of like, I guess the ability to regulate temperature or like not get cut on everything.
We can pick up sticks and sharpen them and throw rocks, that sort of stuff. That gives us a bit
of an edge. Nature is still so much better at that than us though. You have to have been trained
for a long time like as a hunter gather to be able to do that well. Oh yeah, when I said we, I meant
humans. I can't throw with any accuracy. The people in this show can't even find stuff to eat.
Like that's the thing. Like there's just like, you think you're out in the wild and it's like,
okay, I'll go find some like berries. I'll go like hunt for animal. They can't even find the stuff.
All right. That's why invading armies had major issues. You'd like need people that know the area
and be like, yeah, this is what's safe to eat. This is where you can find it. Local knowledge was a
very big thing. Really warfare is effective for that reason. Totally. Invaders don't know. I feel
like we're getting sidetracked again. Sorry. So you're trying to convert it to some weird religion.
Yes. Yes. Okay. So, um, so you have this AI, it's in a box, and you want it to not be a threat to
your species. And you have this box where you can, you know, do things with it. But you can't,
as we have discovered directly, like access its mental state, you can't see what it's thinking,
all that stuff. What do you do to see if this AI is is something you can trust enough to take
its suggestions on how to cure cancer or not. Give it simpler tasks first that you can easily
verify with low risk. Okay. It would be one thing that you could do. But again, part of the problem
of this too, if anyone really loves this topic, Nick Bostrom super intelligence kind of covers all
of the stuff that I keep bringing up as far as like, if it's, if it's way smarter than I am,
then it could just trick me. I mean, it could give me, you know, it'll know that I'm testing it
because it's smart. So maybe you want to set up a situation where it doesn't know that it's being
tested. The AI doesn't know it's in the box, stick it in a simulation. So how do you trick
something smarter than you to think that it's, I mean, if you have control of every all of its
sensory input, you can create basically the matrix for it, right? Okay, and see how how it acts.
Oh, okay. So it's not interfacing with you directly. It's interfacing with fake humans and
stuff. And it doesn't know that somehow. Right. Okay. I'm going to leave the somehow stuff just
to the hand away the magic part. Yeah, that it's an intelligence that has a body within the matrix
and it can interact with the rest of the world as you've laid it out. Then you observe what it does
and you see if this is a thing that consistently acts in a way that you can trust it in the real
world, right? You're losing me already. Okay. I mean, because unless you simulated a world
to super high fidelity, and your ability to infer people's like, coherent, extrapolated volition
from their behaviors is really good. I don't know how you do this, right? Well, first of all,
let us assume that the world is as high fidelity as our actual world is. So like you look for
somebody who like as arbitrarily high fidelity as you can possibly. So like you watch this person
just like see if they give to charity, see if they, you know, or maybe you put them in situations
that test them. Yeah, I feel like I know where this is going. And they can watch them for a few
days. You kind of watch them over this course of 100 years or a human lifetime or so several
lifetimes. Yeah. Keep, you know, I guess, yeah, that's interesting. So like there could be a
reincarnation. And maybe since you don't know like which AI is going to be the AI that works,
you just keep making tons of different AIs and tweaking their initial starting variables and
letting them all interact with each other. Okay. I'm nodding. I think I know where we're going
with this. Okay. At this point, everyone knows where we're going with this. God, is that you?
Yes, me, Margaret, that the world is basically a giant simulation and that we are all the super
smart AIs that God has created to figure out if we are trustworthy enough to release into the
real world. And doing that by observing us throughout our lifetime to see if we are safe,
moral, want to destroy things or want to help other people. So I don't want to tear apart
the argument before I get the whole thing. Are you still daisy chaining or is that it?
I mean, that's basically it. There's a couple other things that I'm going to mention as well,
but that is the basic premise. So like how do you get Ted Bundy's and
okay, well, Lord save us. Those are the bad ones. Those are the bad ones. Yeah. Those are the ones
that you don't let out. But then why do you make bad ones? Like, see, oh, wait, maybe you don't
know what you're going to get when you tweak the starting variables. Yes, that is a big part of
it. Also, why is there any suffering at all is a good question because that makes sense because
you need to see how people respond to the bad stuff. Right. If you only if you only let someone
have good things, then obviously they're only make good, good decisions for the record. I'm
only letting off the human level intelligent programmer, building the matrix off with that
problem, not letting God off with that problem. Because if you're all powerful and you know,
everything, you can you can figure out how to let people learn stuff without torturing them.
But if you're as dumb as we are, or only slightly smarter than we are, because you made the matrix,
no, no, you'd have to be done with than we are, because we're the super intelligent. That's what
I was just going to say. So in this scenario, God is just the dumb human and we're the God AIs.
Yes. Well, we could build, I guess I could see that going both ways. I don't want to challenge
the official version of the argument, but like this is not the official version of the argument,
but like you could make at this point, I've kind of gone away from the official version.
Well, we're kind of smarter than the AIs that we're building now, right? Right. Generally,
yeah. And so, you know, except for a few specific tasks, the super intelligence building the
or the intelligence building the matrix could be smarter than us in general at other stuff
that it knows about, but we're good at these things maybe, right? So, but again, not all
powerful, not all smart. So it doesn't know. And it's not really a test if good actions always
lead to good outcomes and bad actions always lead to bad outcomes. Sometimes people who act
badly get rewarded and become president. Sometimes people who act really well, still get, you know,
robbed and murdered and die of cancer at the age of 40. And their children, you know, cry a lot,
because if always good things lead to good things and bad things, the bad things, that's not really
a test. Yeah, that's fair. So hold on. Let me let me think about this, because yes, if you knew
what you're going to get, then you would just do that just to selfishly get what you want. Whereas
if you don't, there's if anyone, are you guys caught up on the good place? Yeah. Yeah, not season two.
And then I think we're halfway through season three right now too. Oh, are we already in season
three? Yeah, holy shit. Luckily, the episode's 20 minutes long. Yeah. The good place kind of
tackles this exact problem. I don't want to spoil the show. So we'll we'll leave all that aside. But
this is this is kind of fun for that. The thing is, like, if you know that, pretend that the
universe ran on the utilitarian calculus of, you know, your action has 10 good points, because
giving that homeless person a nickel, you know, whatever, create a 10 utilize the universe,
we can measure that because we're deities. If you if you knew that, do you really deserve the
points? This is kind of like deontological ethics, right? You should be doing things out of a sense
of duty, not because you expect something nice to happen, or because even if it makes you feel
good. Right. In fact, if you feel bad doing it, then and you still do it anyway, that's even better,
if you're a deontologist, because then you're doing it out of out of a surprise sense of even though
you're getting Yeah, I see what you're saying. Nice. And so you're trying to find the AIs that
will do the right thing out of a good sense of duty as opposed to reward. So that you know,
once you let them out in the real world, they'll still have that sense of duty.
But of course, then like this, this raises the classic problems with the regular problem of
evil, right? Yeah, like how does a kid, a toddler nine months old, who you know, gets malaria and
dies? How did they do anything wrong? I mean, that was the test for their parents. Their death was
painful, right? So that was I mean, that was all just part of the whole testing environment. You
aren't literally testing every single AI, you have an environment and you're hoping that the
one really cool good AI gets through somehow. Yeah, there's a lot of arbitrary stuff that happens
in general that like doesn't seem like it lends itself to a test at all. I could maybe imagine
it's just a sandbox. Yeah. And the thing is, you don't care about any individual AI, you don't
love any of the AIs, you just want to find one AI that's good that you can let out into the real
world to make the world better that you can then clone a bunch of times and use as a tool.
So this is diverging pretty heavily at this point from standard from from out of the box,
Christian. This is probably not the like you were saying, the official explanation of what this
means because like we're finding ways that this is a horrible utopia already. Yes. I feel like
the people that adhere to this probably think that there's some like, well, maybe not. I don't
know. Like, are these people that think that this is a good thing? Like, oh, this is great.
So I've heard two different interpretations. The one I like more is the Jesus one. So I'm
going to drop that one on you guys. The original programmers actually, at some point about 2000
years ago, found an AI that passed all the tests and became the good AI with the moral sense of
duty that they can trust. And so they let it out into the world and they take its advice on things
and do, you know, stuff in their world. But also, like just shutting down the simulation at that
point would be kind of bad because you're murdering literally all the other AIs in the world. But they
don't know what AIs they can trust except for the one that they have proven is trustworthy already,
right? This Jesus guy. So that Jesus guy can go through and like look at the other AIs in the
system and being like, no, no, no, no, no, that one's a good one. You can let that one out. If you
trust me, you can also trust that one. And even though the programmers don't know for fact they
can trust that one, they do know that they can trust the Jesus AI. So if he said you can trust it,
then obviously you can trust it. So if you follow Jesus' teachings, you get whatever the reward is.
The problem with that though is like, I can feel like if I had Jesus' superpowers, I could do more
good than he did, right? Dude, you're you what? Oh, you mean the world? Right? Supposedly in actual
history, Jesus didn't have superpowers. He could cure the blind. He was just a really good,
this has been, it's been two thousand years, the myths and legends have grown. At the time,
he was just a regular person who passed the test. So this is just some nice guy. Yeah. Like gone to
your right, you know, or Buddha or whatever. Yeah. He went around saying, give away all your
possessions for the end of times is coming. Yes, he's just some death cultist. This is why we have
to join death cults. So this, this doesn't make the character of Jesus one of the magic powers.
Not necessarily. I mean, you can if you want to swing that way in your religious beliefs, but
it's not in your in your in your cyro Christianity beliefs, because that seems really inconsistent.
No, he was so good that he actually like got Neo powers because he saw his way out of the matrix.
But that's my point. Like, so if Neo could could touch somebody, or even not, maybe, maybe why would
he have to be physically close to them? He knows the code, right? He can touch the source. He can
just sense the blind person and cure their blindness. Dude, maybe Neo could only like exploit some bugs
here and there. But if he could cure a blind person, he could cure blindness, right? Maybe he like
saw a particular flaw in the code for that guy and was like, Oh, I can fix that one. You just
insert a semi colon. But for other people, he's like, I don't know what's wrong with your code.
Sounds like a pretty nerfed Neo to me. This is like, this is like matrix two Neo.
That was always one of my main beeps of Christianity in general was like, if this
God is all loving and he wanted to give us a book, why isn't it like chapter one, the germ theory of
disease? Yeah, I mean, it's hopelessly opaque to like you've got to like really squint to get
out the morals and stuff. And you'd think a lot of the morals are like completely
odds with modern day sensibilities about like a lot of it was just like governing who you can
have sex with and in what positions. Yeah, yeah, you think if God really didn't want us to have
slaves, he wouldn't give us all kinds of instructions on how to treat our slaves to give us
a commandment that says don't have slaves you assholes. So you want to that's rude. You want
to hear the the other interpretation that I'm less down with. Yeah. Okay. So if you're down with
Jesus one, I'm really curious. I'm not necessarily down with this Jesus one. I just thought that
one was the most interesting the one that you could embrace, right? Okay. So there was there
was another one was someone who was trying to actually salvage the Bible as far as I can tell.
One of the most common problems people have with the Bible is when God told Abraham to murder his
son, right? That was pretty bad kind of a fucked up thing to do. I mean, okay, there's a lot of
different things. But generally, that was like when you were murdering and raping people, they
were doing it to the unbelievers, the Canaanites, the outgroup, whatever they deserved it. They're
kids. Yes, but the kids of the outgroup. So it's okay. So my problem with the Abraham thing was
that like God knew whether he's going to do it or not. Right? In theory, because well, you can't
know everything and not know some stuff. Well, I mean, what if this is a God that walks through
camps and can step on shit? He obviously must have not known the ship shit was there, right? So I
think like your OT God is different from your NT God. My my my anti my anti-religious arguments
are the easiest ones to not the easiest theisms to knock down are like the Oh yeah, the omnis,
the omni ones, right? My God is omniparful, omnipresent and omniperevolent. Yeah. So like if you
know everything, I can't surprise you, right? So it doesn't make any sense when you get mad at you
that some snake tricked you into eating some fruit because it's like I put you know, I knew that snake
was there. I knew you were there. I knew exactly that's a play out, but it's still your fault.
Like no, if I can stop you, it's my fault, right? I can't remember where I heard this,
but something about with great power comes great responsibility. Spider-man is a better God than
God. All right, so what's the Oh, so the thing was if okay, I'm just gonna say if you're creating
an AI, you wanted to follow your instructions. If you're creating AI as a tool, you wanted to
follow its instructions, even if they don't make sense to the AI. So every now and then you put
in arbitrary tests like kill your own son to see if it will follow the test, which and sometimes you
put in arbitrary rules like cut off your foreskin or don't eat pork or whatever it is, just to see
if people will follow it. Because if they do, then you know they'll follow your rules even when they
come out. And I am of the opinion that is really fucked up. And I would, I would not trust an AI
that if I just told it, kill your son that it would do it, because I might not be the only
person that tells us AI things. And I want an AI that will not kill its own son, even if I order
it to have its own principles. Yeah, and I wanted to have principles better than mine. Right? That's
kind of one of the points is that I'm going to make this thing super powerful rather than make
myself super powerful because I want something I can trust more than me. Yes. And if you're gonna
just do whatever I say, well, that sounds like I bought them with the box. Yeah, exactly. I'll
just go do it. Yeah, I guess maybe because I don't have the power to, but like, so I guess
if you want to be the supreme dictator of the universe, you make an AI that'll follow your
orders indiscriminately. But if you want to make the universe a better place, you make one that
knows what a better place looks like, then you're following it, following the trap of what if I
tell the AI to make some paper clips and oh my God, now the universe is paper clips, right? Whoops.
Yeah, so you made a paper, you made a paper clip maximizer if you just have one that'll do whatever
you want. That's pointless. Yeah, yeah. Yeah, I can see why I like this one less. Well, I mean,
