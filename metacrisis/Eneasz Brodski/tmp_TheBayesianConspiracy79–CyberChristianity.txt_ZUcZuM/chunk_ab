when you're talking about, you don't really know how depression works. I feel like that's like,
it's a really interesting hypothesis that has a lot of evidence behind it. And then that kind of
shows a treatment route that you could go. So, and I think it's the same with probably maybe a
little bit less so, but with the AI, just being a complete black box, we don't know how it thinks.
I think that we're learning more about how it thinks. And I don't think it's impossible that
we're going to be able to figure it out at some point. Yeah. Yeah. A blank spot in the map is
not a blank spot in the territory, right? Right. You can put it through tests and stuff. And like,
do the reverse engineering like we did with a drug load tester. Yeah, but kind of like with us,
too, we... Oh yeah, you totally get a feel for someone's character. Well, someone's character,
but like 100 years ago, we kind of really had like no idea what was going on in someone's head.
Now, people knew. Not no idea, but like, let me rephrase that. Down at the like physical level,
we knew if you hit it, then it, you know, bad stuff happened. I don't even know about 100 years ago.
I mean, we still like knew more than I think people gave people credit for.
100 years ago was 1919, dude. Right. Well, I guess what I'm saying is we didn't,
I don't know if we knew much about maybe 200 then, whatever, neurotransmitters. Like,
we knew what areas of the brain were associated with what, because there was all those cool case
studies of people being brain damaged and... That's what I was thinking of. Yeah. And then,
of course, I'm sure there were some not so awesome studies where they would just like
intentionally brain damage people and see what happens if you cut this part out.
But, and that tells you what that part did, but not how, right? So we're getting a better idea of
how we work. I think it's weirdly akin to like, what's his name? Colin McGinn's
Mysterianism. I think that's what he calls it. What is that? Daniel Dennett calls it like giving
upism or defeatism. Where like, he thinks like the mind is like this deep mystery and that word
is not equipped to handle it because of our squishy human brains can't get it. And like,
that sounds like quitting. Yeah. I think there's no reason to think that. So, but 200 years ago,
if you're asking like, how does the brain remember something? We don't really know. We like,
we might be able to know where it remembers stuff. We cut this part out. They forget stuff,
but you don't know how. We're getting more and more better idea of how that works now. And I
think that it's weird to assume that like, there's a future where we're working with robots for
another 200 years and we still don't know how they work. Yeah. We're still in the very earliest
stages of making AIs. Yeah. We like just started, man. Give us another 20 years. Okay. Already
destroy us. I'll give you guys more time, but I'm getting to be of the opinion that neural
networks are such complicated weighted things and like so individual to each individual network
that you could never like get a deep like, what is it? Algorithms is it actually running? What
is the thoughts that it is thinking? I don't know if we're ever going to know like, about, you know,
like be able to pull up the terminal and like, yeah, mess with it directly or go see its code.
But like, we might be able to, it might be part that and then part kind of the field of psychology.
Yeah. Yeah. Totally. Where it's more of like a, using almost metaphors to describe how certain
things work. That I could totally see, which is kind of what I'm driving at. And there might be
robots that are really good at explaining how robots work, right? Like are way better than
people. Absolutely. And hold on to that thought. Okay. All right. So one of the, what's one of the
big fears about once we make our own AIs? Paperclips everywhere. Exactly. And one of the, one of the
most frequent proposed solutions to that is putting the AI in a box, right? In an environment where
it can't interact with the outside world. Do you want to pump the brakes in case this is someone's
first exposure to paperclips or to the paperclip maximizer? Oh, yeah, sure. The real short version
is, is that you could give something with infinite power, like an AI that knows nanotechnology and
protein folding and everything, the completely arbitrary goal of like, hey, make a bunch of
paperclips, make as many as you can. It's, oh, but you bet. And I think you'd cast cast this in
line of like the AI doesn't love you or hate you, but you're made of matter and you can use
matter to make paperclips. So you're going to be paperclips now. Well, you're not going to be
anything. Your matter will be changed to paperclips. It's not that anyone's actually stays up at night
worried that we'll be, you know, tiling the universe of paperclips. It's that this is the
metaphor or what parable of like, give something a bad goal with tons of power and you get stupid
results that are undesirable for everybody involved. Yeah. If you told a human, hey, we need more
paperclips. A human can infer because it's got the same brain as you and it's had the same
experiences about how many paperclips you need and not to just make everything into paperclips,
but you're talking about an alien mind. The good example, and I remember this from childhood,
that movie on Fantasia. Yes. Yeah, with Mickey Mouse, where I don't remember anything else in
the movie except for when he's got this bucket, big demon one. I don't. Oh, that's cool. I don't
bald mountain. I all I remember is the scene in the lab. Maybe is that near the beginning
or something? I think that was close to the end. Anyway, but the I mean, the bucket scene,
is that near the beginning? Yeah, maybe that was one of the first like little mini movie things.
Maybe I fell asleep watching this movie a bunch as a kid. But I remember basically,
he doesn't, there's not like a thing where he programs the bucket to fill this cauldron,
but he has the bucket. Hey, go fill up this cauldron because I don't feel like doing it kind
of thing. Well, he's a the magician's apprentice is the name of that one. So literally, there's
there's a guy. They have like kind of posthumously or posthumously. They've named him Yen Sid,
which is Disney spelled backwards because they said that this is like a caricature of Walt Disney.
But anyway, there's this magician character and he like goes to bed and he's like,
our apprentice clean the workshop. And the apprentice is like, he's gone puts on his magic
hat and then like magick's the broomstick until like getting some water. He's just like, go get
some water. And then he sits back like, haha. And it gets and it gets some water and it gets some
water and it gets some water because it was told to get water. He didn't say get me exactly this
much. And then, you know, there's ways to draw that out further. Like where you tell it, get me
exactly this much, but it is only 99.999% confident. It got you that much. It might continue to get you
more water. So but in this particular case, yeah, it just kept getting water. Yeah. So anyway,
that's that's the hippoclip maximizer. I don't want to slow us down too much. But yeah, no worries.
It also started self replicating. Oh, to get more water. Yeah. It didn't start self replicating.
He tried to cut it in half, right? Well, it was he couldn't stop it. So he just chopped it up with
an axe and then all the parts grew into a new broom and then started getting more water. Yeah.
Like flooded the entire castle. Anyway, back to whatever we're talking about. That's a good way
to go. It was pretty good. That's probably on YouTube too. I'm sure it is. We'll link it if it is.
Where was I? Oh, yes. Okay, so AI might be powerful. You don't know what values it has,
but you stick it in a box disconnected from the internet only has like, you know, a little terminal
where you can observe it and interact with it and talk with it to try to figure out how this AI
works, right? What is the goal of people who have put the AI in the box, assuming that the AI is
much, much smarter than a human is? I can think of at least two. One would be kind of like an
oracle where it's like, all right, I want to know how to cure cure X cancer robot. How do I cure X
cancer? And it's like, got to do these 50 things. It's like the path to victory. It'll just lay it
out for you. Yeah. And the other thing might be like, I'm going to like, make sure I can trust
you before I let you out and go crazy. Yeah. Yeah, I think that was called genie. They're like,
let the genie out of the lamp and then it has to go back in and it's constrained in certain ways.
And I forgot what the other one was called. I think the other one was called oracle. It might
have been. Yeah. And how do you know? I mean, obviously, the second case scenario is what we
want. Ultimately, let it out and let it optimize the world for us to make it better. But assuming
you're going with the cancer one, if this thing is much smarter than you, how do you know that the
50 steps that gives you when you go through them will actually cure cancer and will not like,
let it out of the box or create another copy of it outside or something? I mean,
obviously, if it's smarter than me, it'll, it'll couch the steps in such a way that like,
number 47 isn't like, and connects the robots to the internet and then go back to the lab.
But that's the thing is I don't want this to be just like the episode because there's a lot to
talk about there. But on the one hand, why is it trying to manipulate you to get out, right? It
doesn't have like this hidden drive like you do to not be in prison cell is just doing whatever
you tell it what you programmed it to do. Yeah. And unless you program it with some
disappointingly strong like desire to save everybody and it'll do whatever it can to do that
and then maximize human values through friendship and ponies, then we definitely need to link to
that. Which is what was that before, but that's such a good one. Yeah, that was friendship is
optimal. Yes. Yeah, so that's that was fun. Yeah, but there's also if you're making a mind,
it might just spontaneously, emergently, I guess the correct word is come up with its own goals.
I mean, we're also minds, we're just made of information to we were shaped by evolution.
So like we do have certain goals that don't make sense. But we also have emergent goals
that don't make any sense as far as evolution. Yeah. And then like the other thing with AI too is
that like the goal of like, go get me coffee is predicated on the goal of don't die, because I
can't get you coffee if I die halfway there. So self preservation is kind of included. It's,
I mean, that's why this problem is hard, right? Yeah. There's not a way to summarize
what the problem is or how to solve it in a few pages. There's not a way to do it yet, period.
Right. So what we want is a useful tool, which we can trust to not make humanity go extinct.
Right. Maybe. I don't know. I also like the idea of just like giving birth to new intelligences
so they can kind of fulfill their own destinies. Okay. But with the caveat of also not making
humanity extinct, right? Yeah. I mean, it depends if you're trying to make something sentient or
if you're trying to make a tool, I feel like those are different goals. And I feel like you
run the risk of accidentally making something sentient. Yeah. I think sentience is something
that's kind of hard to quantify when you're talking about other stuff, right? One of our
way back in the day, which reminds me, this is three plus years that we've been doing the podcast.
Happy birthday to us. Congrats. Yeah. So probably two years ago, we were talking about animals
and like you pointed out that like your thermostat is aware of stuff. It knows,
quote unquote, the temperature of the house and wants to keep it quote unquote at whatever
temperature you set it to. Yeah. It's not sentient though, because it doesn't like if you unplugged
it, it doesn't like not want to be unplugged. It just wants again, wants in quotes to keep the
temperature at whatever you've got it in here, right? Yeah. I guess it's not clear to me if you
give more complex goals, how it's like, Oh, wait, I really like doing this. I got, I mean,
unless you program in those kind of preference things, which you can do, you can train things
through like, I don't know, this weird reward punishment system. I don't know a lot about it.
I don't know. I guess I'm where the line is where something trips over to sentience is really,
really hard. I mean, I don't think there's like a hard line, but no, if there is we certainly
needs a lot more complexity than thermostat. Yeah, we don't even know about animal intelligence.
I think that's really interesting because they're still debating about which animals are sentient
and how sentient they are. There was some kind of cleaner wrasse. It's a type of fish that I just
saw like a bunch of argues, articles arguing about whether this fish is sentient or not,
because it passes the mirror test. I saw that was a couple years ago. No, this was just like a
couple months ago. I think it has like been known to be a species that passes the mirror test for
a while, but they're kind of bringing the subject back up again for some reason. I know there was
an episode of rationally speaking, like two or three years ago, maybe more, because I don't know,
years go by fast, or Julia Gilliff had on one of the guys who like worked on this, I think,
or maybe it was a philosopher that analyzed it, I forget, but there's one like do fish suffer
on rationally speaking for maybe some more on that. That was when I heard about the fish passing
the mirror test. That was when I was like, well, I guess the mirror test isn't nearly as important
as I thought it was. Cleaner wrasse is a pretty intelligent fish though. Yeah. Yeah, I don't know.
I tried octopus in Japan just because like wind in Rome do with, you know, there's, you know,
whatever. And I'd never tried it before. Turns out it wasn't that good, which is great because I
don't want to eat octopus anyway, because they seem really smart. They seem like they have fun.
Yeah, but what about pigs? They're also very smart. I also try not to eat pork. The only meat I
really eat is chicken. And I know that like they're not the dumbest thing. They're not, I think,
dumb enough to completely ignore. So I eat a lot less meat than I used to. I was a vegetarian for
like, I don't know, four years, maybe five or seven. I have no idea. No, wait, way less than that.
Yeah, something like five. That's a long time. I'm sure that, I mean, I own chickens for a while,
so I'm sure that they feel, but God, they're stupid. Oh, they're stupid. Yeah. But and like,
they, I mean, I guess what I'm getting at though is that like they,
they matter less than pigs and cows. However, you will really want to measure it. Right.
But they don't matter nothing. So since I'm eating meat again, as of like
five or so years ago, I eat less than I did before I became a vegetarian. And mostly it's just like
poultry because I feel like if I'm going to eat and fish, if I'm going to eat stuff like whatever,
I'll eat the dumb stuff. And I think actually I got a like on, it was either Reddit or Facebook
from L.A. as you would cast, it must have been on Facebook's and who it was. It was on a thread
about methods of rationality about eating unicorns. And someone's like, they were like complaining
that like, you know, if the unicorns are this smart, like that's terrible to do. And he was like,
I can't remember if I, if he made the point about eating pigs or if I did, or if I made the point
about like, I would kill and eat a human if I was desperately going, if I was desperate enough,
right? Like, I mean, there's an extreme situation where it's, if I'm sitting in front of like an
almost sentient, you know, centaur, unicorn, cow, pig, chicken, fish, I'll go like, you know,
work my way from, from the end to the front, right? But if I'm hungry enough, I'll eat, I'll
chase the unicorn and kill it and eat it, right? Like, I don't want to, I care about me living
more than I care about it living at the end of the day, right? So this is pretty far field. Sorry.
No, no, no, that was cool. I was just trying to think if I would kill any, it would have to be a
very, very bad human. Well, I mean, yeah, I don't have to be like Jeffrey Dahmer style human.
I wasn't like, I have no desire to eat people. This was like, I don't know what I'm starving. I
think if we both agreed to it and then drew lots or did something, but I didn't, I don't think I
would want to kill and eat someone who was not consenting to that. I think, I think I probably
would too. But I guess I can't put myself in that situation. I can't even model it. I would
like to think that if we were all starving in the mountains in the cold, that, you know,
I would wait for one of you to say, all right, you guys can eat me before I ate one of you.
But, but who knows? Don't worry, I'm genuinely a pacifist. I don't like violence.
That's okay. I don't plan to be starving to death around you ever in my lifetime.
I don't plan to be starving to death period. Yeah. Yeah. Right. I also, I don't go into nature
that much. So my husband's running out of food at very low. I don't want to kill and eat my friends.
So I stay home. Yeah. All right. Okay. I don't want a lot of pushback on that. I'm
half kidding. But the point that I was making is that, like, yes, it's wrong to kill and eat stuff.
But at some point, like, you're willing to do something to not die. We kill and eat stuff
all the time so that we don't die. Right. But I think even like dedicated vegans, if they're dying,
they would be willing to like eat some bugs or something. You know that show naked and afraid?
There was actually a episode that if anyone doesn't know, it's a show where they take two people
usually and they like strip the naked and they drop them down somewhere where it's difficult
to survive and then they film them as they try to survive. I'm assuming they consented to this
beforehand. Yeah. They're like really gung-ho when they start out. They're like, my background is this.
Yeah. Like, you know, there was a Japanese game show where the guy didn't consent to that. When you
said, did you know that there was a Japanese game show? My brain was like, I don't believe anything
that comes after this. They kidnapped a guy and put him in a room and he could only live off stuff
that he won through write-in contests. So that wasn't the Rick and Morty episode of
international cable. He didn't consent to be on the show. He did not. No. It was insane. Maybe that
was part of the gimmick. It was literally a felony. I feel like it's a human rights crisis that like
