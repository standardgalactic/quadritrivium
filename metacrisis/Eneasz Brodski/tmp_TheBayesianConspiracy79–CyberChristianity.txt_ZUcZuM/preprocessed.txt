Welcome to the Bayesian Conspiracy. I'm Anya Arsbrowski. I'm Jess Dickey. And I'm Stephen
And today I am going to be telling you guys about Cyber Christianity, which is the stupidest
name ever, but I like it because I like warts with the word cyber in them. I just saw Battle Angel
Alita and like I was just feeling like the entire movie. I was like, I want a robot body. Oh man,
like there's like a bunch of scenes in that movie where people get robot bodies because that's like
the whole theme of the thing. And I was just like looking at the screen and making claw hands.
Like I want that. Me. Give me that. Although not with the creepy big eyes. I don't know. Like
that the super stimulus is kind of interesting in that you get used to it after the first 10 minutes.
And I actually found it to be kind of adorable. Like probably I would assume. I also thought it
was ridiculous though that like these are combat robots and they give them these like armored
bodies that then they keep their like meat head with their giant eyes and it's just like it's
just like you just stick a big target on top of them. Like, hey, my human brain's in here and it's
soft and squishy. Hit me here. That's why I like mannequin and worm. Yeah. Thought ahead. I think
it was a book that I read where they had they had robots with heads like that. The common knowledge
was that when you run into one of these combat robots, you take out the head. Turns out the head
like it has like some hard drive in there with some stuff that's mildly useful to the robot.
But mainly it's there to distract people to go for the head because they'll think, yeah,
if I take the head, it'll go down and then yeah, the robot's fine. Yeah, that's actually the whole
like drive back from the movie. I was thinking about that. I was like, why would they do that?
And I was like, oh, you know what? If you're really smart, if they actually stuck the brain in
like the chest cavity and like the head was just a decoy. Yeah. Left calf or buttock or something
random. It'd be hard to fit there. Yeah, maybe the buttock. I don't know about the leg. Yeah,
the main processing was like in the core surrounded by everything else. You could like cut it up into
quadrants and then stick each one at a different spot, assuming you could still network them together.
I think we need to design some killer robots, guys.
Thanks for bearing with me. We're now returning to our regular scheduled program.
Cyber Christianity. So you were asking in the last episode about how like you could be a
rationalist and a Christian at the same time. And I'm sure there's a number of different ways
to do it. So this is going to be just one of them. But it is one that I found very interesting
due to its focus on AI and singularitarianism. Singularity. Singularitarianism. Why can't I
say it either? Yeah, right? It's a weird word. Say it one more time. Singularitarianism. Singularity.
I don't know. Singularitarianism doesn't sound right either. It does sound weird, but. Singularitarian.
We're going to have to cut this out. Yeah. No, keep it in. It's funny. Oh, God, it's so tedious.
Singularitarianism. Singularitarianism. What the hell's wrong with us? I'm googling this shit.
Not that big a deal. All right. It's also weird to spell. Singularitarianism. All right. All right.
Anyway, so getting back to things. First of all, this is actually tangentially related. You saw
about how, what do they call it? Deep Mind, which was the basis of AlphaGo, which we had the
episode on when it beat the shit out of human Go players, which no one thought was going to
happen with what it did and it did. So they have since then created Alpha Starcraft or Alpha
Craft or Alpha Star, I don't even know what the hell they call it, a program that plays Starcraft,
and that has now beaten some of the top human players in Starcraft, like, handily. It was
nine out of 10 games, I think. Has it had a chance to play with the world's best?
They were, like, in the very top rank. They were, I think, among the top 10,
top 15 players in the world. I'm really happy for, like, the last person to be, like, the best human
before Alpha Star, whatever beats it. And then your title won't be like, I'm the best Go player in
the world. It'll be I'm the best human Go player in the world. But I was the last one to lose to the
robot. Nice. Yeah. And wasn't that also, I might be wrong about this. I think Alpha Star was the
one that proved the previously there have been this thing where if two or more humans teamed up
against an AI, or like if you had a human with an AI, yeah, it was like humans teaming up against a
singular AI, you could still beat it with numbers and then need that like disprove the, I don't know
if that was Alpha Star, there was something recently, were they? Oh, I don't know. Yeah,
previously, I think human plus AI is still superior. But yeah, I think, well, maybe it depends,
I'm not sure. But that's what I've heard is that, you know, the best whatever some of these players
are the ones that are, you know, kind of like cyborg cyborg co units where it's a human helps and
the robot helps. That still kind of gives me hope that like, maybe we can compete if you just, you
know, merge with an AI, like a little parasite. I was when I heard this, I was really shocked. I
did not think it would happen this soon, because there's so many degrees of freedom in Starcraft
compared to something like Go. Just, just so much more you can do in terms of, you know, what units
can do and what units you can recruit and how you use your resources and how you plan, you know,
going forward. Yeah, but what's funny is we're making these exact same arguments three years
ago about like, there's so many more things in Go than there are in chess. You know, that's true.
There's no way it'll be that fast. Oh my God, I was, I was shocked. And the thing is, it wasn't even
like, one of the advantages computers have over humans is that they can do an insane number of
actions per minute. So they can really do their micro, they throttled Alpha Star to be equal to
about how many actions per minute a human can do. And actually, Alpha Star had slightly less than
both its human competitors, which was also fascinating. So there was like, a lot more
thinking, not just brute forcing, right? The one big difference is that Alpha Star could see the
entire map at all times instead of just the, you know, part where the screen is. But they're working
on getting around that and forcing it to only see one, one small part of the screen at a time.
Yeah, that sort of sounds like cheating. Yeah, that could convey a pretty significant advantage
for the AI. But we can't be done once you realize that. Did they like play games where the humans
were also able to see the whole map where they just disabled the fog or something? Oh, no, it
didn't disable the fog. The fog was still there, but it was like, you know how you have the mini map
at the bottom, right? It was like the mini map was at the same resolution of the, as the four human
would be the screen, sort of, more or less, because they don't, the pixels not really mattered to
them. Right. Okay, I got you. Well, they, they should have found a way to correct for that. They,
well, they're working on it now. So it's not like true, true competition would be to allow the
human to see the whole map, right? Well, I mean, because the human can focus on everything at once,
whereas the AI, we won't be able to adjust for that. That's another point. Well, that means
so the point would be to make it so the AI can only focus on a area about the size of a screen,
right? Okay. Yeah. And can really only do most of its actions in that area. Gotcha. But they, you
know, they're working on that. They're pretty confident that given another couple months,
maybe a little bit more than that, they can get around that too and just have an AI that's
better at it than at the entire game than humans. And the, the thing that is interesting about
these deep learning networks, Alpha Star, you know, really drove it home to me, but it's been,
it's been a case for quite a while is that people don't know how they work. You like you set it up,
and then you let it go wild and learn things, but you can't actually like crack it open and
look into it and see like what it's doing. It's just like afterwards you look at it and like,
wow, that's, that's neat. I saw this cool thing about a deep learning network that was trained on
juggalos and clowns. And then asked, you know, given a number of pictures and said,
are these juggalos or are these clowns? And, you know, they don't know how it knows. But
afterwards they were like, tell us the ones you're the least confident on and tell us which one you
think they are so that they can go back and reverse engineer, like what criteria the system came up
with. It was a really neat article. I should link it in the show notes. This sounds pretty cool.
Yeah. From looking at its edge cases and how they, how the AI judged it, they said probably the biggest,
one of the major criteria for juggalos is that they're group photos, because normally they are.
So when there was a group photo of clowns, it wasn't quite sure of that one. And also juggalos
tend to not have red, whereas clowns generally have a lot of red. That's the only difference
that I could think of. And like maybe sharper, like it's not even, I was going to say sharper
eyeliner, but isn't like the whole thing juggalos black and white? Yeah. I would think of clowns
as softer to juggalos having harder edges. Clowns with black around the eyes were much more likely
to be borderline juggalo. And for those ones, they were like the pictures of the clowns from it.
And I'm like, it's basically a juggalo. That's not a clown. Ginger juggalos were the least likely
to be judges ginger as juggalos. Right. So yeah, apparently red is a very high clownness factor
and black around the eyes is a very high juggalos factor. But again, it's not like they could
look in and see databases or anything like that is just the AI learns and then you got to kind of
figure out how it learned from seeing where it goes wrong. That's just like a mind. That's exactly
what I was going to say. It's kind of like, it's kind of like your own mind or someone else's.
Right. So like training, training in AI might be like training a savant child where it's like,
how the fuck did you figure that out? I don't know if I could explain it to you. And then too,
it's like, you know, I don't know how I know the difference between juggalos and although I guess
the difference is that we could think about it and articulate our criteria. But we kind of can.
We're also not very self aware of our own brain's processes. Yeah. Like the clown from it really a
clown. I thought it was a demon. Yeah, yeah. It's that like took the shape of a clown because it was
like making itself look like things that you were scared of. Right. But it doesn't look like it.
I mean, it looks enough like a clown that you can tell it's supposed to be clown ish.
But it also looks different enough from a clown that you can tell this is not your normal clown.
This is a demon hell clown. Like every clown. Like every clown. I never was one of those people
who was afraid of clowns. I see how like they're creepy because they like push down candy valley
with their gigantic mouths and weird eyes and that sort of stuff. But there are people who are
like genuinely afraid of them. And like, actually, I think Rachel doesn't like clowns. She doesn't
want to watch it with me because I liked it. I thought it was fun. You should get killer
clowns from space for her. Okay. It's a great eighties B horror movie. That sounds like something
we could enjoy. But this was one of those things that really depressed me when I because for me,
part of the allure of being uploaded was that once my mind is in software, I can fix what
the fuck is wrong with me. I can just go in tweak some things and be like, ah, no longer suffer
from depression and all these other bullshit that I don't like, you know. And now I'm like, oh,
I can't even tell I'm with you. I don't know how that's changed at all. Well, I assumed that that
would be the case. And now you probably can't actually do that because people be like, well,
your brain's in there, but we still don't know how the fuck anything runs. So I think we're in the
process of like the reason we're developing these synthetic minds is to try to figure out
how our own minds work and to do jobs for us and stuff and to like, you know, yeah, I think it's
more than do hot dog or not hot dog. But there's because we can't even figure out how Alpha star
is doing things, right? Or how the Juggalo disorder is doing things. But like, we have a pretty good
idea neurologically what depression is and how it manifests. That's that's not like saying how do
you sort between like a chair and a seat or something, you know, like relating objects or
categories. This is just like, do you have enough of this in your head? Yeah. And so that sounds
like a really easy knob to tune, right? No, because what if you wanted to because one of the things
you discovered is that redness has a high a clown factor and groups have a high Juggalo factor,
right? How do you go in and change that in the program? You don't. But we're not talking about
adjusting your your category adjustments. We're talking about adjusting chemistry.
Well, there's no actual chemistry in your simulation. I guess you can simulate how much
chemicals are in the brain. There'll be something that's like, what functionally isomorphic to how
neurons work. Yeah, absolutely. How we do this, right? So like, yeah, so whatever, whatever it is
that we're simulating neurotransmitters, probably just the regular electricity. Yeah. And then if
it is, if it just does turn out to be a complete brain emulation, then yes, I can see how you
could simulate different levels of chemicals, but it still doesn't get you over like childhood
trauma and shit. I think we're actually closer to solving that in meat space. Yeah, like I would
bet more that we're going to be able to actually fix depression earlier than we're going to be able
to figure out how our mind works and figure out how to upload it. I think we're closer.
Well, I mean, how would how would you get someone who, for example, cannot form emotional bonds
because of a deep seated fear of abandonment due to losing a lot of people in their childhood to
get over that through tweaking bits? Really quick, though, that's a different
question than solving depression. But I mean, that's that's like a huge specific problem,
whereas like, right, just if the depression probably triggers because the person is lonely,
because they can't form relationships. But like, if if somebody who responds to antidepressants,
I think is like the kind of person that'd be easy to fix in the simulation or in an emulated mind,
this is kind of why we can model life and why antidepressants work. But for like your example,
I've ever tried alcohol for many years.
Yeah, for an example, like the one you were just saying there, I believe they're experimenting
with I forget which chemical it is. And I think it's a former psychedelic or former
street drug with causing you to forget or there might be two different applications. There's
ones that are just causing people to forget trauma. And then I at the psychedelics are
ones where they're trying to kind of do like a hardcore form of CBT, where you're reframing
your trauma. If you're doing it on psychedelics, then it kind of actually is more likely to reshape
those neural pathways. That's kind of like the cancer patients who don't fear death as much,
or at all anymore, after they're given like a high dose of psilocybin.
And depending on how all this works out to with us emulated, so like we could go kind of just like,
you know, again, functionally isomorphic robot brain, or we could do something more like a computer
today. And like, in that's the case, you just got this big box of shitty memories that you can
just throw away, right? You can ask today's computers to do that. Yeah. And then of course,
you get no pointer exceptions with a bunch of other stuff, because a lot of stuff connects to
that. You'll have a bunch of shit to fix. But you're just a broken human throwing out
errors every few minutes. Wouldn't necessarily want to delete things in my past anyway.
Yeah, exactly. I like the approach of reframing the trauma. Or rewriting it. Yeah. Like, well,
you still have it as part of your background. No, rewriting it. Oh, you guys don't like the idea?
That's that's that's basically, I know, I know. You're talking about like going back to like,
inserting like a clown in there. That's like, like brain rape, just going in and altering
yourself. No, it's like brain masturbation if you do it yourself. That makes me think about a
scene in methods of rationality where Harry was trying to like make Neville like less afraid of
everything. So he had like, people giving him chocolate, but they were dressed as dementors.
He's like, stop it. Stop trying to make me feel better. You're just doing such a terrible job.
Yeah, that's, I guess, I mean, I don't know how to respond to reverse mugging. If anyone wants
to try it, that'd be great. And I'll let you know. So where you cost me and throw a bunch of money,
I mean, I'll let you know how scared I am afterwards. It'd probably be proportional to how
much money you give me. So give me a bunch and then I'll tell you how scary it was. That is kind of
like, um, what is it called exposure therapy, though? Yeah, but except plus the it's like,
not just exposing you to it, but making you form positive connections with the thing that you
were phobic of. But if I'm phobic of being mugged, there's a good reason for that.
That usually comes with violence and pain. Yeah, but say if you're traumatized from being mugged.
If you're disproportionately afraid of it to the point where you never want to leave your house.
Yeah, I wouldn't want someone to mug me and give me money. I would want like people to walk past
me in the street and not have bad things happen. They just got me like pushed up against the wall
and like, Hey, here's $200. Exactly. It's the violation that I dislike. Here, have a cell phone.
Have some keys. You want someone else's wallet? There you go.
Yeah. So I guess what I was getting at is that you can't really tell how the AI is running with
the processes are at the root of it, right? You can't like read its mind. You can't read its mind.
I think that like people, it bugs me when people say this to a lot of people, like, well, we don't
understand how brains work at all. And I'm like, no, actually, we're getting pretty close to
understanding it. Like we know a lot about how brains work. Even like predictive processing,
when you're talking about, you don't really know how depression works. I feel like that's like,
it's a really interesting hypothesis that has a lot of evidence behind it. And then that kind of
shows a treatment route that you could go. So, and I think it's the same with probably maybe a
little bit less so, but with the AI, just being a complete black box, we don't know how it thinks.
I think that we're learning more about how it thinks. And I don't think it's impossible that
we're going to be able to figure it out at some point. Yeah. Yeah. A blank spot in the map is
not a blank spot in the territory, right? Right. You can put it through tests and stuff. And like,
do the reverse engineering like we did with a drug load tester. Yeah, but kind of like with us,
too, we... Oh yeah, you totally get a feel for someone's character. Well, someone's character,
but like 100 years ago, we kind of really had like no idea what was going on in someone's head.
Now, people knew. Not no idea, but like, let me rephrase that. Down at the like physical level,
we knew if you hit it, then it, you know, bad stuff happened. I don't even know about 100 years ago.
I mean, we still like knew more than I think people gave people credit for.
100 years ago was 1919, dude. Right. Well, I guess what I'm saying is we didn't,
I don't know if we knew much about maybe 200 then, whatever, neurotransmitters. Like,
we knew what areas of the brain were associated with what, because there was all those cool case
studies of people being brain damaged and... That's what I was thinking of. Yeah. And then,
of course, I'm sure there were some not so awesome studies where they would just like
intentionally brain damage people and see what happens if you cut this part out.
But, and that tells you what that part did, but not how, right? So we're getting a better idea of
how we work. I think it's weirdly akin to like, what's his name? Colin McGinn's
Mysterianism. I think that's what he calls it. What is that? Daniel Dennett calls it like giving
upism or defeatism. Where like, he thinks like the mind is like this deep mystery and that word
is not equipped to handle it because of our squishy human brains can't get it. And like,
that sounds like quitting. Yeah. I think there's no reason to think that. So, but 200 years ago,
if you're asking like, how does the brain remember something? We don't really know. We like,
we might be able to know where it remembers stuff. We cut this part out. They forget stuff,
but you don't know how. We're getting more and more better idea of how that works now. And I
think that it's weird to assume that like, there's a future where we're working with robots for
another 200 years and we still don't know how they work. Yeah. We're still in the very earliest
stages of making AIs. Yeah. We like just started, man. Give us another 20 years. Okay. Already
destroy us. I'll give you guys more time, but I'm getting to be of the opinion that neural
networks are such complicated weighted things and like so individual to each individual network
that you could never like get a deep like, what is it? Algorithms is it actually running? What
is the thoughts that it is thinking? I don't know if we're ever going to know like, about, you know,
like be able to pull up the terminal and like, yeah, mess with it directly or go see its code.
But like, we might be able to, it might be part that and then part kind of the field of psychology.
Yeah. Yeah. Totally. Where it's more of like a, using almost metaphors to describe how certain
things work. That I could totally see, which is kind of what I'm driving at. And there might be
robots that are really good at explaining how robots work, right? Like are way better than
people. Absolutely. And hold on to that thought. Okay. All right. So one of the, what's one of the
big fears about once we make our own AIs? Paperclips everywhere. Exactly. And one of the, one of the
most frequent proposed solutions to that is putting the AI in a box, right? In an environment where
it can't interact with the outside world. Do you want to pump the brakes in case this is someone's
first exposure to paperclips or to the paperclip maximizer? Oh, yeah, sure. The real short version
is, is that you could give something with infinite power, like an AI that knows nanotechnology and
protein folding and everything, the completely arbitrary goal of like, hey, make a bunch of
paperclips, make as many as you can. It's, oh, but you bet. And I think you'd cast cast this in
line of like the AI doesn't love you or hate you, but you're made of matter and you can use
matter to make paperclips. So you're going to be paperclips now. Well, you're not going to be
anything. Your matter will be changed to paperclips. It's not that anyone's actually stays up at night
worried that we'll be, you know, tiling the universe of paperclips. It's that this is the
metaphor or what parable of like, give something a bad goal with tons of power and you get stupid
results that are undesirable for everybody involved. Yeah. If you told a human, hey, we need more
paperclips. A human can infer because it's got the same brain as you and it's had the same
experiences about how many paperclips you need and not to just make everything into paperclips,
but you're talking about an alien mind. The good example, and I remember this from childhood,
that movie on Fantasia. Yes. Yeah, with Mickey Mouse, where I don't remember anything else in
the movie except for when he's got this bucket, big demon one. I don't. Oh, that's cool. I don't
bald mountain. I all I remember is the scene in the lab. Maybe is that near the beginning
or something? I think that was close to the end. Anyway, but the I mean, the bucket scene,
is that near the beginning? Yeah, maybe that was one of the first like little mini movie things.
Maybe I fell asleep watching this movie a bunch as a kid. But I remember basically,
he doesn't, there's not like a thing where he programs the bucket to fill this cauldron,
but he has the bucket. Hey, go fill up this cauldron because I don't feel like doing it kind
of thing. Well, he's a the magician's apprentice is the name of that one. So literally, there's
there's a guy. They have like kind of posthumously or posthumously. They've named him Yen Sid,
which is Disney spelled backwards because they said that this is like a caricature of Walt Disney.
But anyway, there's this magician character and he like goes to bed and he's like,
our apprentice clean the workshop. And the apprentice is like, he's gone puts on his magic
hat and then like magick's the broomstick until like getting some water. He's just like, go get
some water. And then he sits back like, haha. And it gets and it gets some water and it gets some
water and it gets some water because it was told to get water. He didn't say get me exactly this
much. And then, you know, there's ways to draw that out further. Like where you tell it, get me
exactly this much, but it is only 99.999% confident. It got you that much. It might continue to get you
more water. So but in this particular case, yeah, it just kept getting water. Yeah. So anyway,
that's that's the hippoclip maximizer. I don't want to slow us down too much. But yeah, no worries.
It also started self replicating. Oh, to get more water. Yeah. It didn't start self replicating.
He tried to cut it in half, right? Well, it was he couldn't stop it. So he just chopped it up with
an axe and then all the parts grew into a new broom and then started getting more water. Yeah.
Like flooded the entire castle. Anyway, back to whatever we're talking about. That's a good way
to go. It was pretty good. That's probably on YouTube too. I'm sure it is. We'll link it if it is.
Where was I? Oh, yes. Okay, so AI might be powerful. You don't know what values it has,
but you stick it in a box disconnected from the internet only has like, you know, a little terminal
where you can observe it and interact with it and talk with it to try to figure out how this AI
works, right? What is the goal of people who have put the AI in the box, assuming that the AI is
much, much smarter than a human is? I can think of at least two. One would be kind of like an
oracle where it's like, all right, I want to know how to cure cure X cancer robot. How do I cure X
cancer? And it's like, got to do these 50 things. It's like the path to victory. It'll just lay it
out for you. Yeah. And the other thing might be like, I'm going to like, make sure I can trust
you before I let you out and go crazy. Yeah. Yeah, I think that was called genie. They're like,
let the genie out of the lamp and then it has to go back in and it's constrained in certain ways.
And I forgot what the other one was called. I think the other one was called oracle. It might
have been. Yeah. And how do you know? I mean, obviously, the second case scenario is what we
want. Ultimately, let it out and let it optimize the world for us to make it better. But assuming
you're going with the cancer one, if this thing is much smarter than you, how do you know that the
50 steps that gives you when you go through them will actually cure cancer and will not like,
let it out of the box or create another copy of it outside or something? I mean,
obviously, if it's smarter than me, it'll, it'll couch the steps in such a way that like,
number 47 isn't like, and connects the robots to the internet and then go back to the lab.
But that's the thing is I don't want this to be just like the episode because there's a lot to
talk about there. But on the one hand, why is it trying to manipulate you to get out, right? It
doesn't have like this hidden drive like you do to not be in prison cell is just doing whatever
you tell it what you programmed it to do. Yeah. And unless you program it with some
disappointingly strong like desire to save everybody and it'll do whatever it can to do that
and then maximize human values through friendship and ponies, then we definitely need to link to
that. Which is what was that before, but that's such a good one. Yeah, that was friendship is
optimal. Yes. Yeah, so that's that was fun. Yeah, but there's also if you're making a mind,
it might just spontaneously, emergently, I guess the correct word is come up with its own goals.
I mean, we're also minds, we're just made of information to we were shaped by evolution.
So like we do have certain goals that don't make sense. But we also have emergent goals
that don't make any sense as far as evolution. Yeah. And then like the other thing with AI too is
that like the goal of like, go get me coffee is predicated on the goal of don't die, because I
can't get you coffee if I die halfway there. So self preservation is kind of included. It's,
I mean, that's why this problem is hard, right? Yeah. There's not a way to summarize
what the problem is or how to solve it in a few pages. There's not a way to do it yet, period.
Right. So what we want is a useful tool, which we can trust to not make humanity go extinct.
Right. Maybe. I don't know. I also like the idea of just like giving birth to new intelligences
so they can kind of fulfill their own destinies. Okay. But with the caveat of also not making
humanity extinct, right? Yeah. I mean, it depends if you're trying to make something sentient or
if you're trying to make a tool, I feel like those are different goals. And I feel like you
run the risk of accidentally making something sentient. Yeah. I think sentience is something
that's kind of hard to quantify when you're talking about other stuff, right? One of our
way back in the day, which reminds me, this is three plus years that we've been doing the podcast.
Happy birthday to us. Congrats. Yeah. So probably two years ago, we were talking about animals
and like you pointed out that like your thermostat is aware of stuff. It knows,
quote unquote, the temperature of the house and wants to keep it quote unquote at whatever
temperature you set it to. Yeah. It's not sentient though, because it doesn't like if you unplugged
it, it doesn't like not want to be unplugged. It just wants again, wants in quotes to keep the
temperature at whatever you've got it in here, right? Yeah. I guess it's not clear to me if you
give more complex goals, how it's like, Oh, wait, I really like doing this. I got, I mean,
unless you program in those kind of preference things, which you can do, you can train things
through like, I don't know, this weird reward punishment system. I don't know a lot about it.
I don't know. I guess I'm where the line is where something trips over to sentience is really,
really hard. I mean, I don't think there's like a hard line, but no, if there is we certainly
needs a lot more complexity than thermostat. Yeah, we don't even know about animal intelligence.
I think that's really interesting because they're still debating about which animals are sentient
and how sentient they are. There was some kind of cleaner wrasse. It's a type of fish that I just
saw like a bunch of argues, articles arguing about whether this fish is sentient or not,
because it passes the mirror test. I saw that was a couple years ago. No, this was just like a
couple months ago. I think it has like been known to be a species that passes the mirror test for
a while, but they're kind of bringing the subject back up again for some reason. I know there was
an episode of rationally speaking, like two or three years ago, maybe more, because I don't know,
years go by fast, or Julia Gilliff had on one of the guys who like worked on this, I think,
or maybe it was a philosopher that analyzed it, I forget, but there's one like do fish suffer
on rationally speaking for maybe some more on that. That was when I heard about the fish passing
the mirror test. That was when I was like, well, I guess the mirror test isn't nearly as important
as I thought it was. Cleaner wrasse is a pretty intelligent fish though. Yeah. Yeah, I don't know.
I tried octopus in Japan just because like wind in Rome do with, you know, there's, you know,
whatever. And I'd never tried it before. Turns out it wasn't that good, which is great because I
don't want to eat octopus anyway, because they seem really smart. They seem like they have fun.
Yeah, but what about pigs? They're also very smart. I also try not to eat pork. The only meat I
really eat is chicken. And I know that like they're not the dumbest thing. They're not, I think,
dumb enough to completely ignore. So I eat a lot less meat than I used to. I was a vegetarian for
like, I don't know, four years, maybe five or seven. I have no idea. No, wait, way less than that.
Yeah, something like five. That's a long time. I'm sure that, I mean, I own chickens for a while,
so I'm sure that they feel, but God, they're stupid. Oh, they're stupid. Yeah. But and like,
they, I mean, I guess what I'm getting at though is that like they,
they matter less than pigs and cows. However, you will really want to measure it. Right.
But they don't matter nothing. So since I'm eating meat again, as of like
five or so years ago, I eat less than I did before I became a vegetarian. And mostly it's just like
poultry because I feel like if I'm going to eat and fish, if I'm going to eat stuff like whatever,
I'll eat the dumb stuff. And I think actually I got a like on, it was either Reddit or Facebook
from L.A. as you would cast, it must have been on Facebook's and who it was. It was on a thread
about methods of rationality about eating unicorns. And someone's like, they were like complaining
that like, you know, if the unicorns are this smart, like that's terrible to do. And he was like,
I can't remember if I, if he made the point about eating pigs or if I did, or if I made the point
about like, I would kill and eat a human if I was desperately going, if I was desperate enough,
right? Like, I mean, there's an extreme situation where it's, if I'm sitting in front of like an
almost sentient, you know, centaur, unicorn, cow, pig, chicken, fish, I'll go like, you know,
work my way from, from the end to the front, right? But if I'm hungry enough, I'll eat, I'll
chase the unicorn and kill it and eat it, right? Like, I don't want to, I care about me living
more than I care about it living at the end of the day, right? So this is pretty far field. Sorry.
No, no, no, that was cool. I was just trying to think if I would kill any, it would have to be a
very, very bad human. Well, I mean, yeah, I don't have to be like Jeffrey Dahmer style human.
I wasn't like, I have no desire to eat people. This was like, I don't know what I'm starving. I
think if we both agreed to it and then drew lots or did something, but I didn't, I don't think I
would want to kill and eat someone who was not consenting to that. I think, I think I probably
would too. But I guess I can't put myself in that situation. I can't even model it. I would
like to think that if we were all starving in the mountains in the cold, that, you know,
I would wait for one of you to say, all right, you guys can eat me before I ate one of you.
But, but who knows? Don't worry, I'm genuinely a pacifist. I don't like violence.
That's okay. I don't plan to be starving to death around you ever in my lifetime.
I don't plan to be starving to death period. Yeah. Yeah. Right. I also, I don't go into nature
that much. So my husband's running out of food at very low. I don't want to kill and eat my friends.
So I stay home. Yeah. All right. Okay. I don't want a lot of pushback on that. I'm
half kidding. But the point that I was making is that, like, yes, it's wrong to kill and eat stuff.
But at some point, like, you're willing to do something to not die. We kill and eat stuff
all the time so that we don't die. Right. But I think even like dedicated vegans, if they're dying,
they would be willing to like eat some bugs or something. You know that show naked and afraid?
There was actually a episode that if anyone doesn't know, it's a show where they take two people
usually and they like strip the naked and they drop them down somewhere where it's difficult
to survive and then they film them as they try to survive. I'm assuming they consented to this
beforehand. Yeah. They're like really gung-ho when they start out. They're like, my background is this.
Yeah. Like, you know, there was a Japanese game show where the guy didn't consent to that. When you
said, did you know that there was a Japanese game show? My brain was like, I don't believe anything
that comes after this. They kidnapped a guy and put him in a room and he could only live off stuff
that he won through write-in contests. So that wasn't the Rick and Morty episode of
international cable. He didn't consent to be on the show. He did not. No. It was insane. Maybe that
was part of the gimmick. It was literally a felony. I feel like it's a human rights crisis that like
if they filmed this crime and put it on TV, somebody would get in trouble. I wonder if the
Japanese law legal system has some kind of exemption for game shows. If you make enough money,
it's okay to do whatever. So like they have to make sure it's really popular to sell the ad space
first. Right. Anyway, the show, they had a vegan on one time and she was saying, I absolutely,
I've taken survival courses. I know how to identify plants that are edible so forth and so on. She
was like just saying, basically, I'm going to stick to my principles. But after three days of not
eating, she ate part of a lizard, which was the only thing the other guy was able to catch. He was
also like, he's this big time hunter. I don't know. I just find it really funny to watch this show
because these people like really talk up their ability to survive. And then every episode I've
seen, they just fell miserably to eat anything and they're just starving for a really good
bunch of days. Yeah, it's fun. Like the, I mean, the seriousness of how much we can take the set
phase value, given that it's on TV aside, it's a nice reminder that like we're not the dominant
species on the planet because we can like do all the cool stuff when we're totally naked, right?
It only takes like two nights of sleeping with bugs crawling in your butt that you're like, oh,
fuck this. I'm, you know, I want to go back inside with my nice machines and tools and clothes and
all that stuff, right? Yeah. Yeah, we're basically like hairless cats. We're like so vulnerable.
I don't understand. Cats have claws. We don't have claws. Yeah, I'm just talking about their
lack of like, I guess the ability to regulate temperature or like not get cut on everything.
We can pick up sticks and sharpen them and throw rocks, that sort of stuff. That gives us a bit
of an edge. Nature is still so much better at that than us though. You have to have been trained
for a long time like as a hunter gather to be able to do that well. Oh yeah, when I said we, I meant
humans. I can't throw with any accuracy. The people in this show can't even find stuff to eat.
Like that's the thing. Like there's just like, you think you're out in the wild and it's like,
okay, I'll go find some like berries. I'll go like hunt for animal. They can't even find the stuff.
All right. That's why invading armies had major issues. You'd like need people that know the area
and be like, yeah, this is what's safe to eat. This is where you can find it. Local knowledge was a
very big thing. Really warfare is effective for that reason. Totally. Invaders don't know. I feel
like we're getting sidetracked again. Sorry. So you're trying to convert it to some weird religion.
Yes. Yes. Okay. So, um, so you have this AI, it's in a box, and you want it to not be a threat to
your species. And you have this box where you can, you know, do things with it. But you can't,
as we have discovered directly, like access its mental state, you can't see what it's thinking,
all that stuff. What do you do to see if this AI is is something you can trust enough to take
its suggestions on how to cure cancer or not. Give it simpler tasks first that you can easily
verify with low risk. Okay. It would be one thing that you could do. But again, part of the problem
of this too, if anyone really loves this topic, Nick Bostrom super intelligence kind of covers all
of the stuff that I keep bringing up as far as like, if it's, if it's way smarter than I am,
then it could just trick me. I mean, it could give me, you know, it'll know that I'm testing it
because it's smart. So maybe you want to set up a situation where it doesn't know that it's being
tested. The AI doesn't know it's in the box, stick it in a simulation. So how do you trick
something smarter than you to think that it's, I mean, if you have control of every all of its
sensory input, you can create basically the matrix for it, right? Okay, and see how how it acts.
Oh, okay. So it's not interfacing with you directly. It's interfacing with fake humans and
stuff. And it doesn't know that somehow. Right. Okay. I'm going to leave the somehow stuff just
to the hand away the magic part. Yeah, that it's an intelligence that has a body within the matrix
and it can interact with the rest of the world as you've laid it out. Then you observe what it does
and you see if this is a thing that consistently acts in a way that you can trust it in the real
world, right? You're losing me already. Okay. I mean, because unless you simulated a world
to super high fidelity, and your ability to infer people's like, coherent, extrapolated volition
from their behaviors is really good. I don't know how you do this, right? Well, first of all,
let us assume that the world is as high fidelity as our actual world is. So like you look for
somebody who like as arbitrarily high fidelity as you can possibly. So like you watch this person
just like see if they give to charity, see if they, you know, or maybe you put them in situations
that test them. Yeah, I feel like I know where this is going. And they can watch them for a few
days. You kind of watch them over this course of 100 years or a human lifetime or so several
lifetimes. Yeah. Keep, you know, I guess, yeah, that's interesting. So like there could be a
reincarnation. And maybe since you don't know like which AI is going to be the AI that works,
you just keep making tons of different AIs and tweaking their initial starting variables and
letting them all interact with each other. Okay. I'm nodding. I think I know where we're going
with this. Okay. At this point, everyone knows where we're going with this. God, is that you?
Yes, me, Margaret, that the world is basically a giant simulation and that we are all the super
smart AIs that God has created to figure out if we are trustworthy enough to release into the
real world. And doing that by observing us throughout our lifetime to see if we are safe,
moral, want to destroy things or want to help other people. So I don't want to tear apart
the argument before I get the whole thing. Are you still daisy chaining or is that it?
I mean, that's basically it. There's a couple other things that I'm going to mention as well,
but that is the basic premise. So like how do you get Ted Bundy's and
okay, well, Lord save us. Those are the bad ones. Those are the bad ones. Yeah. Those are the ones
that you don't let out. But then why do you make bad ones? Like, see, oh, wait, maybe you don't
know what you're going to get when you tweak the starting variables. Yes, that is a big part of
it. Also, why is there any suffering at all is a good question because that makes sense because
you need to see how people respond to the bad stuff. Right. If you only if you only let someone
have good things, then obviously they're only make good, good decisions for the record. I'm
only letting off the human level intelligent programmer, building the matrix off with that
problem, not letting God off with that problem. Because if you're all powerful and you know,
everything, you can you can figure out how to let people learn stuff without torturing them.
But if you're as dumb as we are, or only slightly smarter than we are, because you made the matrix,
no, no, you'd have to be done with than we are, because we're the super intelligent. That's what
I was just going to say. So in this scenario, God is just the dumb human and we're the God AIs.
Yes. Well, we could build, I guess I could see that going both ways. I don't want to challenge
the official version of the argument, but like this is not the official version of the argument,
but like you could make at this point, I've kind of gone away from the official version.
Well, we're kind of smarter than the AIs that we're building now, right? Right. Generally,
yeah. And so, you know, except for a few specific tasks, the super intelligence building the
or the intelligence building the matrix could be smarter than us in general at other stuff
that it knows about, but we're good at these things maybe, right? So, but again, not all
powerful, not all smart. So it doesn't know. And it's not really a test if good actions always
lead to good outcomes and bad actions always lead to bad outcomes. Sometimes people who act
badly get rewarded and become president. Sometimes people who act really well, still get, you know,
robbed and murdered and die of cancer at the age of 40. And their children, you know, cry a lot,
because if always good things lead to good things and bad things, the bad things, that's not really
a test. Yeah, that's fair. So hold on. Let me let me think about this, because yes, if you knew
what you're going to get, then you would just do that just to selfishly get what you want. Whereas
if you don't, there's if anyone, are you guys caught up on the good place? Yeah. Yeah, not season two.
And then I think we're halfway through season three right now too. Oh, are we already in season
three? Yeah, holy shit. Luckily, the episode's 20 minutes long. Yeah. The good place kind of
tackles this exact problem. I don't want to spoil the show. So we'll we'll leave all that aside. But
this is this is kind of fun for that. The thing is, like, if you know that, pretend that the
universe ran on the utilitarian calculus of, you know, your action has 10 good points, because
giving that homeless person a nickel, you know, whatever, create a 10 utilize the universe,
we can measure that because we're deities. If you if you knew that, do you really deserve the
points? This is kind of like deontological ethics, right? You should be doing things out of a sense
of duty, not because you expect something nice to happen, or because even if it makes you feel
good. Right. In fact, if you feel bad doing it, then and you still do it anyway, that's even better,
if you're a deontologist, because then you're doing it out of out of a surprise sense of even though
you're getting Yeah, I see what you're saying. Nice. And so you're trying to find the AIs that
will do the right thing out of a good sense of duty as opposed to reward. So that you know,
once you let them out in the real world, they'll still have that sense of duty.
But of course, then like this, this raises the classic problems with the regular problem of
evil, right? Yeah, like how does a kid, a toddler nine months old, who you know, gets malaria and
dies? How did they do anything wrong? I mean, that was the test for their parents. Their death was
painful, right? So that was I mean, that was all just part of the whole testing environment. You
aren't literally testing every single AI, you have an environment and you're hoping that the
one really cool good AI gets through somehow. Yeah, there's a lot of arbitrary stuff that happens
in general that like doesn't seem like it lends itself to a test at all. I could maybe imagine
it's just a sandbox. Yeah. And the thing is, you don't care about any individual AI, you don't
love any of the AIs, you just want to find one AI that's good that you can let out into the real
world to make the world better that you can then clone a bunch of times and use as a tool.
So this is diverging pretty heavily at this point from standard from from out of the box,
Christian. This is probably not the like you were saying, the official explanation of what this
means because like we're finding ways that this is a horrible utopia already. Yes. I feel like
the people that adhere to this probably think that there's some like, well, maybe not. I don't
know. Like, are these people that think that this is a good thing? Like, oh, this is great.
So I've heard two different interpretations. The one I like more is the Jesus one. So I'm
going to drop that one on you guys. The original programmers actually, at some point about 2000
years ago, found an AI that passed all the tests and became the good AI with the moral sense of
duty that they can trust. And so they let it out into the world and they take its advice on things
and do, you know, stuff in their world. But also, like just shutting down the simulation at that
point would be kind of bad because you're murdering literally all the other AIs in the world. But they
don't know what AIs they can trust except for the one that they have proven is trustworthy already,
right? This Jesus guy. So that Jesus guy can go through and like look at the other AIs in the
system and being like, no, no, no, no, no, that one's a good one. You can let that one out. If you
trust me, you can also trust that one. And even though the programmers don't know for fact they
can trust that one, they do know that they can trust the Jesus AI. So if he said you can trust it,
then obviously you can trust it. So if you follow Jesus' teachings, you get whatever the reward is.
The problem with that though is like, I can feel like if I had Jesus' superpowers, I could do more
good than he did, right? Dude, you're you what? Oh, you mean the world? Right? Supposedly in actual
history, Jesus didn't have superpowers. He could cure the blind. He was just a really good,
this has been, it's been two thousand years, the myths and legends have grown. At the time,
he was just a regular person who passed the test. So this is just some nice guy. Yeah. Like gone to
your right, you know, or Buddha or whatever. Yeah. He went around saying, give away all your
possessions for the end of times is coming. Yes, he's just some death cultist. This is why we have
to join death cults. So this, this doesn't make the character of Jesus one of the magic powers.
Not necessarily. I mean, you can if you want to swing that way in your religious beliefs, but
it's not in your in your in your cyro Christianity beliefs, because that seems really inconsistent.
No, he was so good that he actually like got Neo powers because he saw his way out of the matrix.
But that's my point. Like, so if Neo could could touch somebody, or even not, maybe, maybe why would
he have to be physically close to them? He knows the code, right? He can touch the source. He can
just sense the blind person and cure their blindness. Dude, maybe Neo could only like exploit some bugs
here and there. But if he could cure a blind person, he could cure blindness, right? Maybe he like
saw a particular flaw in the code for that guy and was like, Oh, I can fix that one. You just
insert a semi colon. But for other people, he's like, I don't know what's wrong with your code.
Sounds like a pretty nerfed Neo to me. This is like, this is like matrix two Neo.
That was always one of my main beeps of Christianity in general was like, if this
God is all loving and he wanted to give us a book, why isn't it like chapter one, the germ theory of
disease? Yeah, I mean, it's hopelessly opaque to like you've got to like really squint to get
out the morals and stuff. And you'd think a lot of the morals are like completely
odds with modern day sensibilities about like a lot of it was just like governing who you can
have sex with and in what positions. Yeah, yeah, you think if God really didn't want us to have
slaves, he wouldn't give us all kinds of instructions on how to treat our slaves to give us
a commandment that says don't have slaves you assholes. So you want to that's rude. You want
to hear the the other interpretation that I'm less down with. Yeah. Okay. So if you're down with
Jesus one, I'm really curious. I'm not necessarily down with this Jesus one. I just thought that
one was the most interesting the one that you could embrace, right? Okay. So there was there
was another one was someone who was trying to actually salvage the Bible as far as I can tell.
One of the most common problems people have with the Bible is when God told Abraham to murder his
son, right? That was pretty bad kind of a fucked up thing to do. I mean, okay, there's a lot of
different things. But generally, that was like when you were murdering and raping people, they
were doing it to the unbelievers, the Canaanites, the outgroup, whatever they deserved it. They're
kids. Yes, but the kids of the outgroup. So it's okay. So my problem with the Abraham thing was
that like God knew whether he's going to do it or not. Right? In theory, because well, you can't
know everything and not know some stuff. Well, I mean, what if this is a God that walks through
camps and can step on shit? He obviously must have not known the ship shit was there, right? So I
think like your OT God is different from your NT God. My my my anti my anti-religious arguments
are the easiest ones to not the easiest theisms to knock down are like the Oh yeah, the omnis,
the omni ones, right? My God is omniparful, omnipresent and omniperevolent. Yeah. So like if you
know everything, I can't surprise you, right? So it doesn't make any sense when you get mad at you
that some snake tricked you into eating some fruit because it's like I put you know, I knew that snake
was there. I knew you were there. I knew exactly that's a play out, but it's still your fault.
Like no, if I can stop you, it's my fault, right? I can't remember where I heard this,
but something about with great power comes great responsibility. Spider-man is a better God than
God. All right, so what's the Oh, so the thing was if okay, I'm just gonna say if you're creating
an AI, you wanted to follow your instructions. If you're creating AI as a tool, you wanted to
follow its instructions, even if they don't make sense to the AI. So every now and then you put
in arbitrary tests like kill your own son to see if it will follow the test, which and sometimes you
put in arbitrary rules like cut off your foreskin or don't eat pork or whatever it is, just to see
if people will follow it. Because if they do, then you know they'll follow your rules even when they
come out. And I am of the opinion that is really fucked up. And I would, I would not trust an AI
that if I just told it, kill your son that it would do it, because I might not be the only
person that tells us AI things. And I want an AI that will not kill its own son, even if I order
it to have its own principles. Yeah, and I wanted to have principles better than mine. Right? That's
kind of one of the points is that I'm going to make this thing super powerful rather than make
myself super powerful because I want something I can trust more than me. Yes. And if you're gonna
just do whatever I say, well, that sounds like I bought them with the box. Yeah, exactly. I'll
just go do it. Yeah, I guess maybe because I don't have the power to, but like, so I guess
if you want to be the supreme dictator of the universe, you make an AI that'll follow your
orders indiscriminately. But if you want to make the universe a better place, you make one that
knows what a better place looks like, then you're following it, following the trap of what if I
tell the AI to make some paper clips and oh my God, now the universe is paper clips, right? Whoops.
Yeah, so you made a paper, you made a paper clip maximizer if you just have one that'll do whatever
you want. That's pointless. Yeah, yeah. Yeah, I can see why I like this one less. Well, I mean,
it was from someone trying to salvage the Bible part of the cyber Christianity, right? And I'm
like, I just know that doesn't work for me. I don't want to say that that would just be a
continuation of the test, like imposing hardships on your AIs. Yeah. Here's a confusing book full
of contradictory rules. Let's see what it does. Right. But I would not salvage the Bible that way,
but but then I would want the AIs that rejected the Bible. Yeah, I would want the AIs that are
not good at that. Well, maybe that's the test, but somebody who actually likes the Bible. Yeah,
I wouldn't want that. And yeah. And at that point, you start getting into questions like,
if the test is which parts of the Bible do you reject, then why are you even bothering with
the Christianity part of anything, right? Just stick with being a good person again.
Yeah, that's my take on both of these is like, it's pretty obvious that somebody has motivated
reasoning to want to, you know, like they're not like coming from an objective standpoint,
like, let's, you know, start from first principles and try to figure out, like, is the
universe a simulation? Does God exist? They have a privileged belief that they're starting from,
and then they're working backwards to try to fit it into whatever, I guess, their beliefs on science
or singularitarianism. I'm totally nodding. That makes perfect sense. Yeah. It's one of those
things, though, that I really enjoy because enjoy in quotes, I guess. I have always had some problems
with reality testing, which is why I hate solipsism. And this is exactly the sort of thing
that will fuck me up and be like, Oh, shit, what if we are all in a simulation? And like,
there's no way to disprove it, right? It's stupid. Yeah, exactly. But this is this is one of those
things that will just this is why I'm worried that this episode might be a bit of a bass list,
right? It can hurt people without any benefit. Maybe it doesn't bug me at all, because I just
have the Occam's razor kind of mindset of like, again, like, I don't reject this any more or less
than I reject a bunch of other things that like are equally, you know, I'm not like taking some other
random. Oh, what if like, you know, this the Assyrian religion is true, and then kind of mapping
that on to and then like getting really worried about it because I don't I mean, I guess you're
going to run into this sort of somehow out there, even if you just run into it in standard
Christian Christianity, you know, that's what I was going to say is everyone's running to this
because everyone's been exposed to religion. Right. It's the exact same. So it's better
coming from like some of us who are like, by the way, this is all bullshit. Don't take it seriously.
Yeah. Then to then to hear from someone who's like, Oh, did you know that we're all actually
programs in the matrix? I think it would be just like those guys that stand around campus shouting
about how Jesus hates bags or something, right? I don't think anyone who's seriously trying to
address the question of like how reality works or where we came from would come again, come to
that from first principles. This is such an obvious style of like religious apologetics of
like, let me just work backwards from what I already believe and build up this edifice in a
language you guys all like robots, AI, matrix, and I think this is particularly dangerous for me
or someone like me because I already reject supernatural and magic and all that stuff as
obvious bullshit. But I think it's entirely possible to have simulated minds, which then
could have simulated environments. And so this has the patina of plausibility, which, you know,
it doesn't for me because I don't think there's any evidence of it. If there was no evidence,
but it's not physically impossible. And anything that's not physically impossible.
There's so many things that aren't, you know, impossible, like you could come up with like
just all day long, you could come up with random theory, all those things bother me.
That's right. That was good. The string theory bother you. Yes. Really. That's what I'm thinking
about. That's actually what this reminds me of. It correct me if I'm wrong, but I don't think
there's actually any evidence of string theory. It just like somebody came up with a cool thought
experiment about how molecules might behave like strings and then they might affect other things.
And it was just like, I could see how for a certain type of mind, this is like impossible to put
down after you've been messing with it because it just like, oh, it's so elegant. This makes so
much sense. This could be real, but like, dude, wait until we get to the quantum theory sequence
sequences. And I start like crying because I'm like, what if I am multiplied an infinite number
of times and nothing matters? I think about that. But I also don't think anything matters. So I guess
what the string theory thing, I'm not sure we'll have to ask physicists, but if it's not string
theory that has no support that people are enchanted with scientifically, there was something else.
Certainly philosophically, right? Parallel universe is thing the questions put like,
how can anything matter if there's a, you know, infinite universes, because then like your
utilize don't count towards anything because infinite is too big. I think there's two things
with that. One, there's like the barrier between our universe and any other parallel universe.
So like, for everything that we care about with our experiences and our actions that impact
sentient beings, it's just our universe, right? Until we open a door to another earth and we're
trading, you know, terrible Star Wars prequel movies with them, then the third universe is like,
it's 2050 and like the universe is like war free and everything because they're prequel
movies didn't suck. Dude, do you suddenly like this philosophy more? Yeah, now, now I'm getting
sold. Now the thing that the worst part is, if there is an infinite universe, then somewhere
out there, there's a case where this scenario actually is true. And what if we're in that one?
I don't know. I don't know about your second premise though.
What if we're in one of any of the others? Yes. I don't know. Like kind of what you said,
like I don't know about the phrasing of well, nothing matters. But like that's actually, you
know, defined matter here. Like I feel like there's when a lot of people do convert from whatever
their religion is, they have this like kind of void of meaning of like, Oh, no, if these things
aren't true, then like, does anything matter? Because like, there's no good or evil, there's
no guiding hand, there's no like ultimate goal. So like, but like they're defining matter in a
really specific way. I don't even know why it would matter if a God did exist, and he had some
kind of grand plan for us, the plan doesn't make any sense. It seems like he's using pretty
terrible methods to get there. And like, I still have the things that I care about, and they're
kind of arbitrary, and they're a product of evolution and whatever. But like, I get to choose
what matters to me. I mean, what God cares about is kind of arbitrary too. Well, I don't know,
because we don't actually know what the grand plan was supposed to be. But it was just to like
create a heaven and then have everybody bliss out forever. But if he cares about what you're
totally cares about, then it seems super arbitrary. Like the most important thing is just to remember
that in the end, everything boils down to normality. The universe is what it is, and don't get like
carried away in any of this crazy bullshit. Yeah. Although I don't want to get too close to like
resigning to the universe as the way it is, because fuck that, the universe sometimes
sucks, and we're going to fix it. Right, right. Yeah. But don't don't embrace craziness like this.
Yeah. Don't be me in my bad moods. Be me in my good moods. Yes. But I think that should be everybody's
mantra. Be the best you you are when you're in a good mood. Okay. What you mentioned about
the whole like, what if nothing matters? I remember specifically those kind of panics when I was a
teenager. Me too. And you know, everybody, I think it's, it's understandable how like,
some of the atheist community to the extent that it still exists gets kind of circle jerky about
like, I broke out of this weird thing, even though it was like scary and confusing for a while,
because it kind of is. And you know, I like to give myself credit that I had a lot of these thoughts
like that I read about later originally, but they weren't hard thoughts to have, right?
Like the idea that life still matters in this sense, I have every sense that I care about,
if anything matters less, if you're in the matrix, right? That's kind of why people,
that's why the movie The Matrix, why people, well, some of most of the people would get out
like being out of the matrix, because like, at least this is real. At least this isn't somebody
fucking with me. It's really weird. When I first de-converted, I was the other way around,
like things seemed much more important. It was because the physical world existed, and it was
all that existed, and our lives mattered, and it was important, you know? Yeah. I'm much more
nowadays struggling with does anything that I do matter, because time passes and eventually I'll
die and nothing I ever did mattered, it matters in any way. So I think, I mean, when I first broke
out, I like, I had a mission, you know? And I don't know, I mean, what you're, what you're
approached at the end is kind of like, what do you call it? Deaths, being scared of death, and
Do it a songst. What? It's a German word that means fear of death. Oh, I like it. It literally means
death fear, so. Yeah, so, I mean, I don't have a good response to death fear. That's why I try to
be immortal, but to the extent, like, there's a lot of solace that I had before I realized that
Chronix wasn't some crazy thing from, you know, TV show Futurama. Like, is that where you were
first exposed to it? I, maybe, I would have been, like, nine or, I would have been about 10 or 11
when Futurama came out, so that's entirely plausible. But, you know, like, Richard Dawkins
opens up, I'm sure this is one of his books, oh, it is, it was in Unweaving the Rainbow, but it
was also in like the 1980 Christmas lectures that he gave at like the fair day, yeah, it was fun.
You used to watch those on Christmas? Yeah, I didn't watch them on Christmas, but I found them on
YouTube, you know, 20 years after they came out. There's, I don't know, this is where like that
gratitude thing comes in that Dana, Dana talks about a lot. There's some awesomeness about being
alive at all. And, you know, Dawkins puts it poignantly, because again, with the biology background,
he's like, the odds of anyone with your DNA coming, coming to exist are vanishingly small.
You know, the fact that your parents met and that they're, that both of their parents met,
and that, you know, whatever the night that they banged, they happen, that you,
that all of these steps happens to make exactly them and then exactly you. It could so easily
have been otherwise. And it almost, it's absurd that this actually happened, right?
And that's awesome. And here we are. And, you know, even if it's only for like a little century,
we get, we get time to walk around the universe and enjoy it. And that's awesome. And while we're
here, we can do our best to make sure that other people can do the same thing. So I want to keep
living and I want to choose when to die if I ever feel like it in 10 billion years. But
to the extent that like, I'm not afraid of nothing I'm, of my life not mattering, I guess to the
extent that like I want to have an impact, everyone does. But what you do carries forward,
you know, even if, even like what you do is forgotten and nobody remembers you particularly.
And it could be that everyone you help in your entire life as Batman in 200 years,
they're all dead and no one remembers Batman or something, right? Even though you did the best
you could. Yeah, like how many awesome people lived in the past that we've forgotten about now,
just because, you know, the oral history carried on for a few generations and then it died out.
Yeah, almost all of them, right? Yeah. But the good effects still propagated forward.
And even if they don't, even if they only go forward for a little while, like that doesn't
mean that they don't matter because those people were impacted. The world, the universe was made
brighter by that, right? That's, that's what gets me like, I don't know, when I look at the numbers
of like how likely I think Chronix is, I'm still happy to be alive if I'm not gonna live forever,
right? Yeah. In my good moods. In my bad moods, I totally get where it's like, this is fucking
pointless. But I'm gonna drink and play video games. Sounds really good to write about now.
But then again, I'm also the guy who plays a nice guy in video games, right? So like,
maybe I'm just eluded. And I know that people in video games don't care, even though like they
yell and stuff. And they, if you go through Skyrim and set the whole town on fire, like they run away
from you and stuff and you chasing down like, but I know that they're not really suffering,
but I still want to play the nice person. And so maybe that's something in real life too. Maybe
I'm just really bad at separating my, my play from my life. But if that keeps me, if that keeps me
saying and happy, that's all that there is to it. Yeah, you'd be the pick then. There we go. Not only
are you like making the right choices in the simulation, but you're making the right choices
in a simulation that you have created within the simulation. This guy doesn't, this guy didn't
harvest a single little sister in Bioshock. Quick, promote him. I was always really torn on that,
because it did not seem to me like they were happy. They seem to be like in pain. And I was like,
is it a good thing to harvest them? It's not the, maybe it's not the best moral choice, because
like at the end of the day, if you don't kill them, all they do is set them loose in this hell
scape of rapture, right? So it's like, they're, do you want me to kill you right now and try and do
it as painlessly as possible? Or you want somebody to like find you and cut you into pieces later
and eat you. And also they kind of seemed evil. And like, at least neutral. To me, they looked evil.
The fact that they looked like a little girl was supposed to like trigger your like parental,
you know, like, oh man, we got to save this. Did you guys play Bioshock too? No, but they looked
like little girls who are skipping around in a hell scape laughing and singing with glowing eyes
and this monster doing their bidding. I'm like, I think these are bad guys. In Bioshock too,
there's a sequence where in Bioshock too, you play as a big daddy the whole time, except for
this small part we're playing as a little sister. And to their weird, gloomy eyes,
everything's all happy and magical and there's butterflies everywhere. And yeah, so they're
really just playing around, like kind of a slow down drug. Kind of like, what's that fun video
game that came a couple years ago? It's like this dystopia where everyone takes their happy pills.
We happy few. Okay. Where you take the joy and like the world lights up and everyone's happy
and chipper. It's kind of like that, but on steroids for the little sisters. I realize
that we're digressing a bit, but the point is, is that... No, I didn't harvest them just because
I read that that was the good choice. So you needed a Bible to tell you what to do.
Well, the stimuli were ambiguous. That's worth bringing up for, like you mentioned,
like what matters and stuff and why do I do anything. I don't remember the horror of this
fully hit me before I saw it on one of the new atheist things somewhere, but the person who says,
oh, without God, why would anyone do anything nice? I've heard that a lot of times.
That's the person you got to worry about. Me too. But I always thought like, that's just stupid.
You guys are ridiculous. Because I think I never really actually believed that somebody was that
serious about it. But to the extent that they are, if they learned that the heavens were empty tomorrow
and they would immediately like start eating their kids or whatever. No, they probably keep
doing what they're doing because they care about their kids. They care about their loved ones.
They want to stay warm and happy. The person who sincerely believes, no, if I, if I learned God
didn't exist, I would just go on a murdering spree till I was gunned down. That seems like
a very small minority of people who actually say that. Well, maybe not who say that, but who
actually would do that. Yeah. I think they're more worried about what other people would do.
Yeah. Other people who profess that, I only do good things because I'm expecting a reward.
First of all, you're losing per the rules of the good place, the TV show, presumably, because you
can't just, you know, if you knew the game and you can't again, you can't rig the game and
except the game master, not to notice, right? I don't know. I'm getting pretty far fielded,
just anti-theism. Yeah, that's the other thing that's weird about the Bible.
Is that like the Bible tells you, like, do these things or you'll go to hell. And then,
like, if you do all these things correctly, you'll go to heaven. It's kind of like, yeah,
how is this a good person now? This is somebody who's explicitly self-motivated.
Maybe that's all you care about because God's also super vain and just wants, like, the attention,
right? I think they just used to define goodness as something else.
If you guys have ever read, um, Hell is the Absence of God by Ted Chang,
I mean, if you haven't read it, totally read it. So good.
Is that on your, uh, rationalist fiction, right?
Well, Ted Chang as the author is, that's not the one that I linked.
Okay.
But, uh, because I don't think that's a rational story necessarily, but it's basically
what someone like us would write after thinking about all these things.
Cool, I'll check it out.
Okay, so that is, that is my thing on what cyber-Christianity is and why
some people might be able to reconcile.
I wonder if there's anybody who didn't come from religious upbringing that buys into this.
I doubt it.
I sort of doubt it too. I feel like this is something that, like, you're raised religious
and you hear about the school rationality stuff and you find the people nice and you like the
idea of polyamory or something. So you're like, I'm going to go join these people and make their
noises, but they don't like the fact that I'm religious. How do I get them to accept me into
their tribe? Oh, I'll change enough of the, the words of my religion to just sneak in there.
I don't think that there's any such explicit thought process.
No, it doesn't have to be explicit.
Well, yeah, but I made it sound explicit, but I don't think, I don't think it was.
I don't, I didn't mean, I didn't mean... It's like what your subconscious brain is doing in the
background. What your conscious brain is really motivated to ignore it. Mike, are they being
hypocritical or are they just being a craze with their beliefs? Yay. That's a good question.
How is that segue? But I'll also just point out that that's sort of the press secretary
metaphor that Robin Hansen uses in Elephant in the Brain, where it's like your, your real
motivations are obviously like, I just want to stay with these, these people and keep doing this
and your press secretary goes forward and gives you good reasons to say all these new things that
you're saying. And in the show notes, I will link a rationalist Mormon who put forward a lot of
these arguments. He has them both in written format and in three podcast episodes where he just
basically spells them out speaking into a microphone. I'd be interested in hearing it.
And, you know, then people can tell me how I misrepresented him and I'm a bad person.
And I probably didn't do it complete justice, but I think I got the both, both the, the heart
of the matter and also what I thought was most interesting about it.
Does he maintain that there was like a magical Jesus?
I don't know. That was the one where he mentioned that you want an AI that will follow
even arbitrary rules like kill your son.
So I'm curious because Mormonism is pretty strict about like all the weird shit they believe, right?
I mean, the dogma is, yeah, but the actual believers can be really all over the place.
I've met some Mormons who are basically atheists.
Yeah, same. I guess I was thinking that like...
But still strong like Mormon.
I think I found much more commonly, and I guess I know more Christians than Mormons,
but I find more Christians who are like essentially atheists. Like, you know,
God is like in my garden when I'm out, you know, potting plants or something.
Like that's, if you said that 500 years ago, we would have killed you.
But I find, I guess I, again, for the small sample size, but I find that Mormonism
in my limited experience tends to be less flexible about that.
You can't call yourself a Mormon and say God isn't real.
Whereas you can do that Christianity kind of.
Well, they certainly don't say God isn't real.
They might say God is a human who invented us as AIs and...
Something like that.
I think it might just be part of...
Mormonism is much smaller than Christianity, and I think that it's a little,
like probably just more extreme for that reason.
It's a smaller and newer religion.
I also think for him, it was more of a metaphor than a literal one-to-one,
we are in the matrix thing.
Okay.
It's like, this is, this is what God is doing, and this is why he might be doing it as a sort
of a testing routine. Not literally, there's programmers out there wanting to find out if
they can release into their world.
So he doesn't even believe this. He's laying this out as like an analogy of why he believes
his religion.
I mean, maybe. I couldn't, I couldn't quite tell.
Although that sounds like the most...
It might be a very good strong metaphor, or it might be, you know,
does it matter?
We talked about it, and we had a cool conversation, and some people probably believe it.
Oh, no, no. That sounds, I mean, as far as that goes, but it's just like that kind of like
obscuring, you know, weird, shady, ephemeral...
I didn't want to put words in his mouth.
Maybe he does literally think that there are the gods of programmer thing.
Oh, no, that's fine.
And I don't even know who this person is, but I'm just thinking like,
this is the exact kind of thing that makes arguing with religious people tires them,
because, you know, you can grant, okay, yeah, that's an interesting metaphor.
I see Ray coming from within religion, and then they'll go around and tell their friends,
and be like, I converted him.
He's on board now.
Yeah, they do a lot of the mountain bailing, too.
Yes, mountain bailing, just, again, I call it ephemeral goalposts,
where like they'll go, not only do they move, but like sometimes they're real,
and sometimes they're not.
So I don't know, it's, it's a whole thing.
But we were talking about acracia and hypocrisy.
Yeah.
A little bit.
Shall we move on to the less wrong posts?
Yes.
Sure thing.
All right.
Our first post that we're going to talk about today is the sequence post hypocrisy or acracia.
Am I pronouncing that right?
Yeah.
Okay.
I think so.
And I think, if I recall correctly, like the word acracia was already in the community,
I guess, in the air, where people had introduced it before, I think.
I think Aristotle coined it.
Or Plato.
But it was not like, it wasn't a commonly used word.
I had never heard it before of the rational, the rationalist community came around.
I took philosophy classes, so I took it the wrong way.
I got it in the wrong order, probably.
It probably came into the rationality community from philosophy.
Oh yeah.
In the post, he mentions one of those philosophers.
The ancient Greeks used the term acracia.
Yeah.
Yeah.
In any case, the idea that you know what you should do and you don't because of weakness of will.
Right.
Yeah.
So it's like, I want to lose weight and be attractive, but donuts.
We've had this conversation.
Yeah.
Yeah.
So I think this is the first post, well, I know this is the first post that actually
defines the term acracia.
And I think it was already being talked about a bit, but this post is a good introduction
for someone who hasn't heard it before.
And just basically says what acracia is.
And how it's not in hypocrisy.
Right.
Or is it?
Yeah.
So you were just before we started the episode, you mentioned that you thought Robin Hansen
had a more extreme definition of hypocrisy than most people.
Yeah, but they go on to kind of define what they're talking about in a nice way.
Although the fact that they're arguing over definitions of words is kind of funny.
Yeah.
So I was thinking that hypocrisy was just saying one thing and not doing it yourself.
Like if my parents told me not to smoke, but they both smoke.
And they say it's bad for you.
You shouldn't do it between puffs on a cigarette.
I thought of it in the ad hominem, what, to coquet, the hypocrisy argument, right?
So it's like the fact that they're not doing what they're saying doesn't make them wrong.
You know, Peter Singer could be, you know, as selfish as Donald Trump,
but they wouldn't make his moral arguments wrong.
It would make him a hypocrite, but it wouldn't make him wrong.
But I think they have a more strict definition, which I think is fine, of like moral hypocrisy
saying, we were talking about religious people earlier, you know, the gay bashing super,
super church or megachurch pastor who gets caught with his pants down,
doing blow off some male prostitutes, Dick or something, right?
So this happened in Colorado Springs a few years ago, right?
I think you generally do them like off their abs or something.
Like doing blow off a dick would probably be hard.
I mean, obviously it would have to be hard.
It would have to be hard.
Anyway, so my colorful language aside, that would be hypocrisy in the sense that they're
talking about. They're saying this is a moral failure, except when I do it.
I've always, I don't know if it's my religious upbringing as well and like how Jesus loved
using hypocrite as an insult, but I always also gave it a moral aspect.
Like if your parents say don't smoke because it's health advice, then it's just a crazy.
But if they're like giving you the impression that smoking is a morally bad thing,
which I always got that impression from my parents, then it would be hypocrisy if they smoked.
Like if smoking has a moral judgment behind it, like it's dirty or it shows weakness well,
you know, all good people don't smoke and they are smoking while they tell you that,
then it's hypocrisy.
Yeah, to be fair, the post then goes on to explain hypocrisy versus acracia and does it
in a way that makes the distinction, I don't know, more palatable and actually easier to
understand what phenomena we're talking about if we use acracia or hypocrite to describe the
behavior. So yeah, I like it.
Yeah, Eliezer says, if I really believe that I ought to exercise at least three times per week,
but I don't always do so, is that properly termed a hypocrite? I mean the term acracia,
meaning weakness of will or failure of self-control seems more appropriate.
Yeah, I mean, I guess that's basically it. I've always, I feel like a douchebag saying this.
I kind of have gotten the impression for many years now that the rationalist community overuses
the term acracia or acracia to as an excuse sometimes.
To justify all the shit that we still do despite us should be knowing better.
Right, I'm like every now and then it's a right to just say, you know what, I'm a crappy human
or something and I'm doing something like giving it the term acracia almost seems too noble sometimes.
Because you're using a fancy Greek word.
So you're explaining like why you didn't do your work. It's like, well,
it's because I was playing Red Dead Redemption for four hours.
Right.
And it's like, oh, that was, that was the craze of this weakness of will. It's not a problem with me,
you know, as a person that moral character is a weakness of will.
Yeah. And I'm like, dude, you wanted to drink and play video games. I feel you. I've been there too,
a lot. All you have to do is admit that there's nothing wrong with that once in a while, right?
Yeah, as long as you're not like missing bills or, you know, not, you're getting,
you know, losing your job or something because you're playing too much video games, then go nuts.
Yeah, I don't have a problem with it. I think that it's generally correct. And I also don't think
that people are like, Oh, it's okay that I was drinking and playing video games because it was
just a craze. It's like people, when they say that it's not like they're giving themselves a free
pass. I think it's just an actually true definition of what was going on in their brain. They probably
sat there drinking, playing video games and every 20 minutes thinking, I should go do the dishes.
I should go walk the dog. And then they felt bad about themselves afterwards.
That's my life.
I think that's generally how that happens. There's, I think very few people that
does me like five days a week don't feel bad about like doing behaviors that they know they
don't really want to do or that like are actually like short term shooting themselves in the foot.
Okay. Then I was being a douchebag, right?
Well, maybe just miss under, maybe like miss modeling them because,
you know, I think if you're consistently making the acracia like thing or not excuse, but
explanation for your behavior and like you, you notice that acknowledge it, but don't do anything
about it, then what's the point? Like, so I tend to not to feel bad about my video game binges,
you know, assuming I'm not putting off important things, right? Yeah, I don't have much to add
to that other than why did they start this debate in the first place?
Hansen and Yudkowski.
Oh, well, I assume it was because Hansen was doing his typical economist revealed preference thing.
Yeah.
Where people are like, well, they obviously don't really care about their health.
They're just signaling that they care about their health and they're smoking.
Right.
And yeah, and all the answer was like, that's just weakness of will. It's not necessarily.
It's Robin Hansen human.
I love Robin Hansen.
No, me too. He's amazing. I think he's great.
But it's funny because maybe he, I don't know how he has this ability maybe to see through
the bullshit. Maybe he doesn't live with the bullshit. Maybe it's some court.
He's always wearing the one ring from the Lord of the rationality.
Right.
Yeah. So that's the distinction. I think maybe the reason that it's in this at all is because it's a
valuable thing to notice about like, why aren't you doing the thing that you want? Well, you know,
it's not because you suck. It's just because maybe it's a weakness of will sometimes.
And you can, you can train your will that you can't maybe train your moral character as easily.
I'm not really sure what they're getting out there. But
Oh, it's also, if you're going to be using that term, it's good to have a post that defines it.
Yeah.
I mean, this is a sort of sequence of things that is supposed to be starting from bedrock.
Wait, is that why it's called the sequences?
I've made the effort to intentionally choose the Acrasia argument in the past.
I didn't define it that way.
But I went around for a while, kind of having this like,
dark view of the world going, yeah, people suck. The life sucks. And I kind of wanted to
improve after a while. So I started an experiment where I was just giving people the benefit of
the Dow. Like, for example, I was driving for many years. I was commuting to college for five
years. And it was like 45 minute drive both ways. And then for work, it was the same distance.
And just like on a highway and people cut you off and New Jersey drivers.
So when people would cut me off, I would be like, what a terrible person,
what a dick. And it would just make me miserable. And I kind of tried to change my mindset where
I would give people the benefit of the doubt. Like, okay, they probably cut me off because they
didn't see me. Or because maybe they've got an emergency, they might be speeding somewhere
because something really bad is going on with them or they're really stressed out or whatever.
I went through the same sort of transformation.
Yeah.
And I think it was around the time I don't think that this kicked it off, but there's
a great distillation of half of that attitude, which is called Hanlon's razor.
Never attribute to malice. That was just equally explainable by stupidity.
Yeah.
So it's like someone who cuts you off, it could be a malicious asshole, but more than likely,
they're probably just stupid. Or they're a bad driver.
Yeah, they're not paying attention. They've got other shit going on.
I was using stupid generically.
I feel like you're not giving them that much of the benefit of the doubt there.
Yeah, I was like, that's not quite what Jess was going for.
But it's hard to be mad at somebody for not noticing something.
If you felt like they're trying to run you off the road, which sometimes they are,
then they're their dicks.
I feel like it's almost never that anyone's actually trying to run you off the road.
No one cares that much about you to try to run you off the road.
I'm not saying that. I meant to say not never.
And there are people who they'll see you and then scoot up so you can't merge.
Like I mentioned last episode where difficulty entering the highway.
Yeah, people definitely get road rage.
Yeah, some people are being assholes. Don't get me wrong.
But most of the time, they're probably not. They're just not aware.
But there's the other thing that helps me get over that same sort of people suck attitude
of inferring the moral character from their actions.
And what's that? The fundamental attribution error where you're
snippy with somebody, you know, at getting coffee or something,
because you just had like the worst day ever.
You were in two car accidents this week and, you know, you've got rocks in your shoes.
You're getting sick in the hospital.
And yeah, all this crap, right?
And then so like, you're, you're snippy with the barista or something.
And then a week later, your life's great.
And then the person in front of you is snippy.
And you're like, that person must be a fucking dick.
I'll bet he kicks his dog. I'll bet he, you know, I'll bet he litters.
Like, you just start referring a bunch of mean things about their personality.
Probably the opposite political orientation.
Exactly. You guys are mean.
I'm saying that this is what humans do.
I didn't coin the phrase fundamental attribution error.
This is next, this is a, this is the perception that people,
or this explanation that people see, we see our history.
Really a lot of people like that.
I mean, I've met one or two people like that, but I think everyone's like that to some extent.
I always thought it was my impression that that is the exception,
that generally people are assumed the best of others.
Maybe you're a saint. Yeah.
Because don't get wrong, I don't walk around angry people now,
but I did, especially as a teenager, like, why is everyone being so mean?
And it's like they're not, they're just doing what makes sense to them in their moment.
But that's not, I think, an intuitive realization for most of humanity.
Yeah, understanding the elephant and the birdie actually does help.
I think you, I don't know.
At first it seems like, oh, the people are all like,
Homo economicus is not the right word, but just trying to maximize stuff for themselves
and they're lying to themselves all the time.
And at first it makes like, you feel really angry about everybody,
but then like, once you take the moral judgments out of the picture,
it starts being like, you can actually give people more of the benefit of the doubt,
because you understand that everybody has these flaws and they're marined.
And it's not that they're intentionally like trying to be malicious towards you.
You can explain things away through brain bugs too.
I eventually, I started doing the accepting the Acrasic explanation because
I realized I was making myself miserable.
And what I noticed the more I started doing the opposite was,
if you go around thinking that people suck and that life sucks,
you know, people don't want to be around you because you're depressing.
So then you end up, you don't get friends, you don't have like romantic relations,
you miss out on like job opportunities and then life does suck.
And then you're like, it's all those other people's fault.
So like, if it becomes a self-fulfilling prophecy and you get the other benefits of
giving people the benefit of the doubt, sometimes people are jerks,
but like, most of the time they actually do have something terrible going on in their life,
or they're, you know, they've got some kind of extenuating circumstance,
or they were just not paying attention and people like you,
if you're the kind of person that extends the benefit of the doubt,
you see more of the good side of humanity.
And like everyone's got stuff.
And that's, that's kind of what you were saying too, that, you know,
this isn't like necessarily just an insight from like rationality or psychology or something,
you know, the, the observation that like everyone's got struggles that you're not aware of is been,
you know, noticed and marked down by tons of people throughout the ages.
But being mindful of that, I think keeps you from getting annoyed at people a lot of the time.
I really quickly at the end here, I noticed the post called me out,
which I thought was really impressive that he managed to write this 10 years ago to call me
out today, who says that people who claim to care about truth and then deceive themselves
really don't care about the truth.
Why not say they really care about the truth as is right and proper,
but they aren't living up to their own morals.
And I know on more than one occasion I've said that like,
all you really need to do to get people to accept the scientific method and,
and, or reject religion is just make them care about the truth.
And then they'll find out on their own because they want to know the truth and that'll motivate
them. And I'm like, now I'm like, hmm, that sounds, that sounds like maybe I am wrong.
In your defense, I think you probably formulated that during your years of arguing with religious
people and where the, where the, for the sentence, I don't care if that's true or not is used a lot,
right?
It wasn't used very often with the people I argued with.
I tended to argue with other people on forums who were also there for the intellectual sparring.
Because I mean, I've definitely seen, I guess you're a lay religious person, especially,
where it's like, fine, I don't care if it's true, you know, it makes me feel good or
I hope it's true. And so like, to many people, truth isn't that high of a value.
But just like trying to hammer into love the truth more to people is not going to be the answer,
I guess.
I think it would work on a large number of people.
Again, anyone who's saying that I care if it's true or not, it gives them hope.
You know, something else like, doesn't have to be religion, it could be like an alternative
therapy for a terminal illness.
And it's like, I don't care if it's been not been, if it's been shown not to work,
it gives me hope is something that I think your average,
probably mostly say an adult could say, right?
I think it's not that they don't care about truth.
It's that they care about it less than having that hope that this alternative therapy will
cure their child or that they'll go get to live forever in heaven or whatever.
I don't know. I think if someone's child has cancer, they really do care about curing that
cancer. And they care about the truth in that regard, but they just don't have the tools already
to know that this homeopathy is bullshit.
Or that they've already tried all the standard care and they have to resort to the homeopathy.
Yeah, that's fair. I mean, they wouldn't be giving them any sort of alternative medicine
if they didn't expect it to work.
So like, they care about what's true, they just not, that their map is wrong, right?
It's really hard to come to someone when their kid is dying of cancer and telling them
this is bullshit and this is why. It seems like this is one of those things that you
should work on before the cancer happens, because once cancer's in the picture,
it's you're a real asshole if you're being like, hey, about that homeopathy.
It's the same thing with the religion. Say someone's kid has just died and then they're,
you know, you can't go up to them at that point and be like, oh yeah, but like, you know what,
you should really care about the truth. God doesn't exist. Your kid's gone.
That's like a bad time to have that argument.
Yeah, I remember people trying to give me, again when I was back in my religious debating
days of like, well, would you like talk to somebody about their religion on their death bed?
And I'm like, no, I'm not an asshole. What could they do with this new information?
If they're going to die in 30 minutes, nothing, right? I'm not going to ruin their last minutes.
If they were making a bunch of wrong decisions and they were like, you know, 19,
they're going to decide, do I want to go to college or do I want to go off and, you know,
do whatever, some pointless religious thing. Not saying all religion stuff is pointless,
but you get my point. That would be a good time to have that conversation.
But not at the point where you're ruining their life or ruining, yeah, I mean,
you could retroactively ruin someone's life in their last 30 minutes, right?
I don't think you'd ruin their life, but you'd ruin the last 30 minutes of it.
But they would, yeah, you'd ruin the last 30 minutes and they would look back and say
my whole life was pointless. If you somehow managed to convince them that this whole thing was
bullshit in their last 30 minutes, then like, they might have said, I regret everything and
then die, right? I don't think they would ever really do that because there's still everything
else in their life. Well, we talked about how hard that transition can take more than 30 minutes,
right? Yeah. All righty, on to, do you know how to pronounce this? Tsuyoku Naritai.
Yeah. Do that again? Tsuyoku Naritai.
Tsuyoku Naritai. Neat. Okay. The next episode, Tsunoku Naritai. I want to become stronger.
This is explaining the fundamental growth mindset. Oh God, I just used that term
of rationality and that people want to be better. Cool. Sidebar. Why don't people like,
I remember growth mindset was popular for like five years and then it wasn't. Why?
Didn't it not replicate or something? I don't know how you could test that. I think there was
something where they did some battery of psychological tests and showed that it didn't
actually work. All right, what's growth mindset?
Growth mindset is the idea that I can always get better, stronger. I don't have intrinsic
limits. Yeah, exactly. So I think there's like, I can't do this because I'm too dumb. It's like,
I can't do this now, but I will get better. Okay. Wait, I think I'm thinking of something other
than growth mindset that didn't replicate. That's what it was because you were supposed to not tell
kids that they have intrinsic limits. It's more like you can still be better. Oh, you can be anything
you want to be. Yeah, exactly. Well, so like, that seems on one, that seems like a deepity,
right? In the sense that like one reading of it is just trivial and obvious. The other half is
like profound, if true. But like, so. I think that's why it started became sort of a joke
eventually ish. Well, but on the one obvious side, like I'll never win an Olympic gold medal
for deadlifting, right? Because I'm almost 30 and I can deadlift right now like maybe nine pounds.
So that's that's just never going to happen. But to say that that's it's able
never ever happen, because I'm a transhumanist, I think that one day we'll all be able to, you know,
maybe not win gold medals, but I'll be able to compete with today's gold medalists. No problem.
Maybe I need to think about this, this whole growth mindset thing more and not keep us
down that route all for too long. Yeah, I think they rejected it too strongly.
I think it's like anything that becomes too popular too fast that after a while there's a
backlash. I think it's just yeah, that hipster backlash going on. Anyways. So Eliezer says,
the orthodox Jewish belief that all Judaic law was given to Moses by God at Mount Sinai.
After all, it's not as if you could do an experiment to gain new,
halachic knowledge. The only way you can know is if someone tells you who heard it from someone
else who heard it from God. Since there's no new source of information, it can only be degraded
in transmission from generation to generation. I still thought to myself, Torah loses knowledge
in every generation. Science gains knowledge with every generation. No matter where they
started out sooner or later, science must surpass Torah. The most important thing is that there
should be progress. So long as you keep moving forward, you'll reach your destination. But
if you stop moving, you'll never reach it. I love that I cut out for brevity, but he said that like
this was the thing that occurred to him as a kid. Even though I'm all religious and stuff,
at some point, science is going to be better than God. Yeah, he was like, I wasn't a full blown
atheist yet, but I did have this thought that kind of implanted the seed. Well, in their defense,
I mean, he was, you know, already, I guess an above average logical person as a child. But
those two things are obviously true, right? If science is learning more and the Torah is
learning and we're getting less from the Torah, and I'm drawing lines with my hands that are,
I'm not drawing good lines. One's supposed to go down, one's supposed to go up. Yeah. Yeah. So
what's the what's the message? The message. So it's called Tsuyukunara. Tsuyukunari-tai.
Because it's a Japanese term, meaning I want to become stronger. He points out that it's much
more prevalent in Japanese media than American media. And specifically, I'm thinking about anime
because I see it a lot in anime. But I think that's a good point. We don't have it as much in America.
No, it was the entire theme of the anime growing lagoon. If anyone's familiar with that one,
humanity was like living in this dystopia where everyone lived underground and they were imprisoned
by these like animal creatures. And then like the end of the show was them like two galaxies
fighting other galaxies. Like it just, it was this like theme of growth that was just so ridiculous
and over the top that it was really adorable. But I mean, it's it's a theme in a lot of anime.
Like for Rony Kenshin, he needs to be strong enough to beat Shishio and goes on his, you know,
little quest to find his true strength. Inuyasha, which is like 190 episodes, which I dropped off
to like 120 because every season was exactly the same, where they'd find the bad guy and then he
would, you know, hurt him. And then the bad guy run behind some MacGuffin wall, and then he's not
strong enough to get through the wall. So then the next season he goes off, gets strong enough to get
through the wall, and then he gets through the wall, hurts the bad guy again, guy goes behind
a different wall, lather ends repeat for like six years. I'm like, all right, that's, it's all the
same. You're never going to get the stuff. Yeah. Attack on Titan, a kid starts out like not knowing
a lot, but really wanting to avenge his family and kill all the Titans and protect his society.
So yeah, he just keeps learning and getting better and has this drive to become the person
that can defend his, his society. Yeah. And I stopped watching that too, because it started
becoming just episodes and episodes of filler. I can't do this. What's wrong with you? Just like
a pretty strong undercurrent of Japanese culture. You see it also like not just in anime, but like
you'll see it in martial arts and even in like, I don't know, I like to watch the NHK and they
always show like there's a part about craftspeople. So like somebody making clay pots and you watch
this. Yeah. It's basically the Japanese BBC. For some reason, I can't think of what the
acronym is, but they have the English version of it too. And you can stream it from their website.
They have some pretty neat programming. And this is like, yeah, one that I really like is like,
they follow craftspeople. And so there's like a guy making clay pots. But like, he's trying to become
the best person who ever made clay pots ever. And like, he's already achieved mastery. And
Eleazar talks about this too. It's like, you could say Siyoko Naritae, if you're a new go player that
wants to ascend, or if you're getting pretty good, but you still want to be like, in the top ranks,
or if you're already the best person alive, but you still think you could do better.
Yeah. And in that sense, that kind of growth mindset is true that like, I don't want to
belabor on that point, but there's always the capability to say, I want to do better, even
if you're the best. And even if you really suck, you want to suck a little less. I think, yeah,
you made the point about just like lifestyle stuff, you know, I mean, I've seen documentaries
people making like kitchen knives. And that I didn't see the whole documentary, but that
sushi one. Oh, Jiro dreams of sushi. Yeah, like the mentality in Japan, or in Japanese culture,
is different in that like here, it's like, I want to be good enough to make the most money
in my field for my area or something. I don't really care about being the best. I just, you know,
I want to be not necessarily, but that's, that's maybe more common attitude. It's like,
I don't really care about being the best at this in the world or the best at this,
even in my city, I just want to make, you know, a very competitive salary or something.
Yeah. So you're going to retire, I think has a kind of a leg up on maybe growth mindset,
or maybe like this was the problem was that people were kind of mischaracterizing growth
mindset. But you kind of think of again, like the skinny kid who wants to like become an Olympic
athlete, and it's just never going to happen because of genetics. Whereas I think so you can
already tie is kind of comparing yourself against who you were, which I think is more of, you know,
a rationalist value. I like that distillation alive. That's that sounds perfect to me. It's
not that I'm comparing myself to Arnold Schwarzenegger. I'm comparing myself to me of a month ago.
And I want to be stronger with him in a month now and be stronger than me now, right? That sounds,
I like that. Speaking of anime, there's the anime called Mob Psycho. And there's like this one
aspect of it that I really enjoy. It's this ridiculous show about a kid with psychic powers,
and he sees ghosts and stuff. He's like this, this skinny weak kid otherwise, and he's not popular.
And he decides to join the body improvement club at one point, because he has a girly
likes and he realized that no matter how good he is at psychic stuff, she's not going to like him
unless he's got a good body. And it becomes this really funny thing of like, it's this team of like
the most ridiculously drawn jocks who are like running laps and lifting stuff. And then there's
like this ridiculously weak kid kind of lagging along behind them tripping over himself, like
unable to like lift a marshmallow onto sticks. And it's like, people are like, why are you doing
this? You're like this great psychic, you could, I don't know, one character who is this other
great psychic who he beat in a fight. It's like, you could rule the world with your power. And he's
like, well, yeah, but I couldn't, you know, like make that girl notice me. So
I thought that that was really funny. It's just really adorable. I would recommend this anime a
lot. What was it called? Mob Psycho. It just, it takes a lot of weird anime tropes and kind of
like turns them on their head. That's what this writer is good at. It's the same person who did
One Punch Man. Oh, cool. I've definitely heard of that. I saw the first episode. I don't know
what I'd get into if I got sat down for two seasons of it, but it sounds interesting.
No, that sounds fun though. The, the, personally mentioned that I still forgot the name of again.
Mob Psycho. Mob Psycho. Okay, there it is. Yeah, there's a lot of that theme running through it too,
where characters or various characters are trying to become better for like bad reasons
versus good reasons. So it gets a little bit more into that. That reminds me back on the
post though, that the message isn't, I don't know, don't worry about being the best. Just if you
have this drive, which I think a lot of people do, you want to be just better. And that's always
attainable, basically, you know, within physical limits, et cetera. But it's almost like it's
emphasizing a particular virtue for people who are virtue ethicists. That this is a great virtue
to have the will to become better. Yeah, you could say it's something that gives life meaning.
Yeah. I mean, I'm just thinking of like, and you could do this anything. It doesn't have to be,
you know, I'm going to hone my rationality skills or, you know, be an amazing doctor.
There's people who've beaten Dark Souls on the Guitar Hero controller, right? And like,
they're not doing that for money. They're not doing it for probably not for the prestige.
Maybe there's some prestige with that community, but they're doing it to see if they can, right?
That might already be the best. But like, all right, now I wonder if I can do it blindfolded.
I saw a video of a girl beating Pontiff Sullivan, one of the bosses in Dark Souls 3, blindfolded.
Up audio cues? Yes. But the audio cues, to me, aren't that obvious. Certainly, you don't know
where you are. Because you haven't done it like every day, all day long. Exactly. But I watched
you do this and it was fucking amazing. I love seeing stuff like that, too. It's so, I don't know,
like, there's sometimes I watch videos and be like, humans are the best when you see, you know,
like somebody do a ridiculous skateboarding trick. Those humans are awesome videos.
That's what I'm thinking of. Yeah, the humans are awesome videos. Like, it's just, it's adorable.
There's a subreddit called humans being bros. So like, most of my news feed is like positive
things like that. I keep the negative things in there, but I have it like at least three to one
on good stuff. And it's like videos or clips or whatever of people like this guy walking past
this homeless dog. And he's like, there's this kind of like scrap of like cloth in the street that's
big enough and he goes and gets it and he lays it on the dog. It's like a security camera that,
you know, just nice little thing, a little little warm fuzzy. Sometimes you need warm fuzzies.
Yeah, the essay post says Tsuyoku Naritae is the driving force behind my essay,
The Proper Use of Humility, which we did a few episodes back in which I contrast the student
who humbly double checks his math test and the student who modestly says, but how can I ever
really know? The student who double checks his answers wants to become stronger. He also goes
on to have another cool anecdote from his religion of birth. Each year on Yom Kippur, Yom Kippur?
I've heard of both ways. Okay. An Orthodox Jew recites a litany which translates,
we have acted shamefully, we have betrayed, we have stolen, we have slandered,
as you and so on. As you pronounce each word, you strike yourself over the heart in penitence.
There's no exemption whereby if you manage to go without stealing all year long, you can skip
that word and strike yourself one last time. It does not end, but that was this year and next
year I will do better. This ritual bears a remarkable resemblance to the notion that the way
of rationality is to beat your fist against your heart and say we are all biased, we are all irrational,
we are not fully informed, etc. Take no pride in your confession that you are too biased.
Do not glory in your self-awareness of your flaws. If your ignorance is a source of pride to you,
you may become loath to relinquish your ignorance. It's perfect. Yeah, I've got nothing to add.
He ends it with the important thing is to do better, to keep moving ahead, to take one step forward.
So you could not even die! There you go. Yeah, you nailed it. All right. I like this one in the
context of the previous post, because it's an interesting thing to contrast against. The other
one was kind of giving people the benefit of the doubt, saying they're not doing these things
because of crazy, but then you can kind of ask why are they not? So you could not eat high,
not eat high eating. Maybe the orange just takes a while. You don't get to become super sane in
a single episode. It takes 100 episodes of filler. I don't know. What I like to do is to give other
people the benefit of the doubt, but like hold myself to a high standard, and I don't expect
other people to do it. It's nice if they do. There was a quote from the previous post where
Eleazar had said, what would you do if someone came to you saying that like, I have all these
flaws, I've done all these things, but I want to become stronger, please help me. And I feel like
that's the point where you would turn that person towards the Siokunari Tai philosophy.
But I do kind of wish that it was more of a value of our culture. I think that like it's kind of
Japan is like a high conscientiousness culture. And it's neat that they value that. I think like
in America, we tend to kind of idolize rock star types. And we also tend to think of it as like,
it's weird, because we're supposed to be like the country that thinks that anybody can be whatever
they want. But I think that like when you actually talk to Americans, they tend to betray more of
an idea that people are kind of born with something or not. Yeah, they had that spark. That's why.
And I've talked to people and have expressed that dichotomy in real time where it's like, okay,
sure, Martin Luther King Jr. had this this spark of, you know, care for compassion for humanity
and whatever his vice is. But I don't think that detracts from the good things he did. Someone else
was born the poor child of abuse of drug addicts or something. And they have less opportunity to
be Martin Luther King Jr. And yet the whole again, American, what do you call it, dream is that
anyone can do anything, right? Yeah. So it's like, oh, no, that person can do it too. It's like, well,
can like within the limits of like physically possible, sure. But and this isn't really,
I guess, tying to the difference between our two cultures. It was just something something with that
mentality trips me up, right. And that's the whole other thing. And it's obviously,
I'm not being very coherent about it. So it's hard to be coherent about
because like, there's a sense in which I think both of these ideas are correct. Obviously, I'm
giving people the Acrasia explanation. I think it's true. Like, I generally think that
in the last post, somebody linked to a Brian Kaplan article where he was talking about,
what if there's an alcoholic who just decides to go to the bar after work every day and get
smashed instead of going home to his wife and kids. Now, maybe it's true that he actually
likes alcohol more than his wife and kids. But you can't say that without suffering social
consequence. So he gets to, you know, maybe go to some AA and like walk around saying how he
feels really bad for his behavior, and he wants to change. And then he gets social sympathy and
he gets to keep drinking. So that was like kind of a point against the whole like Acrasia. Like,
there's social consequence to admitting your true self or your true beliefs. And there's,
you get like, socially rewarded, I guess, for saying the things that society thinks you should
say like, I should exercise, I should diet, I should do my homework. Sometimes maybe people
are just kind of hiding that they don't actually want to do those things. And just saying like,
and you know, you know, there's a sense to which I do think that that's probably correct.
But I also think that the, you know, hypothetical alcoholic probably does actually feel bad about
what he does. Maybe he has a really hard life. Maybe his job really wears him out every day. He
has to make the choice. Do I go to the bar and feel like instant gratification, I feel better
and have a few drinks? Or do I go home, fight with my wife, like change my kids diaper, take
out the trash, walk the dog and feel miserable. And the thing about like, in this case, a drug
dependency is that like it becomes less and less of a choice every day, right? Because, you know,
if you're in it, and you have a dependency, then like when you decide not to, it's not just where
you stay like where you are. All right, I'm going to go home and be like, only as miserable as I
am, you're going to get more and more miserable throughout the day and following days, because
your, your body needs, needs and quotes really, really wants whatever it is that you're addicted
to, right? And I put in mind the super stimuli that, you know, the people who died playing 003
and whatever Dota 2 or whatever games it was, they probably didn't make the decision like,
I'd rather do this for one more hour than live out the rest of my life. They, they just had a
weakness of will and I guess lack of attention. It's obviously super rare. So it's not like if
this was happening to one in 200 gamers, they'd probably, there'd be a more of a panic about
it. It does happen to people though. Yeah. Like you hear about a crazy behavior in casinos and
whatnot, like there's some people that just have such strong addictive tendencies that they will
completely like ruin their entire lives to continue pushing that button. That's a really good point
that gambling is just as it gives, it's different kinds of stuff than I'm assuming drug dependency,
but it's similar. But it, but yeah, that, that feedback of like, I've almost got it in that rush
when you win is awesome. And I can totally see how that that's super appealing or rather common.
Yeah, we all have varying levels of that too. It's not like there's a few people that just have
superhuman willpower like Elon Musk, I guess, and they're in the other like top 1% in the other
direction. But the rest of us like, you know, the majority of people are somewhere in the middle
of that floating around. Yeah. Okay. For next time, we have Tsuuko versus the Eglaterian Instinct.
Oh, that sounds like kind of what we're talking about.
And statistical bias in quotes. We will post links to both of those on our website,
TheBazingConspiracy.com. And you can find everything else that we talked about there as well.
And on our subreddit r slash TheBazingConspiracy, by all means feel free to throw us a buck on
Patreon if you feel like it slash have the ability to, if not no problem. You're also welcome to,
you know, whatever, share an episode, talk about it, you know, anywhere on a Reddit comment and
YouTube comment to your friends in real life. Write us a review on iTunes, give us a rating.
That definitely helps too. And we have a patron, I think, this week for people who are currently
supporting the podcast. We do. We would like to thank John Pedersen for helping to provide this
podcast and bring it to all of you. Yeah, thanks, John. This was awesome. Thanks, John. Thanks,
man. It helps. Okay. We ran out of time again. So we will just probably do a full feedback
episode next time or something to catch up. Feedback's piling up. I'm not just stalling
for time, Zayla Volkan. I will address that until the dark web stuff I've been saying.
All right, then we'll see you all in a couple of weeks. Cheers. Bye, everybody.
