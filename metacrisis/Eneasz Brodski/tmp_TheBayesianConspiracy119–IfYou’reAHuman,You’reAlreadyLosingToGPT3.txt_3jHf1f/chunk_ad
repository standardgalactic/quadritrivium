or is that something that you can really like explain like in five?
Yeah, I can try.
It's kind of, it would involve a lot of hand waving.
It's, it's kind of like one of those,
what do they call it, emergent things?
No, no, no.
So it's, it's really like, it's actually kind of straightforward.
It's just, it has a lot of, it has a couple of parts that are hard to say
without using a lot of math words.
So I mean, it has a bunch of matrices that use to model,
a form of attention over, over data.
So it's like, you can think of data, data comes in,
data is multiplied by a sequence of matrices,
and then functional transformations happen between those,
those matrices acting, being multiplied by the data.
And then that happens over and over and over again.
So it's like, it's, that's about as, as, as vague as I can be without like
literally writing equations.
But it is just a very specific way of multiplying your data by matrices
and then updating those, the weights of those matrices based on what happens
So it's kind of just feeding it lots of data
and then it's trying to sort of figure out what's important here.
Yeah, I would say that's true of kind of neural networks as a discipline.
Like just the way, the way neural networks are, are used,
are found to solve tasks looks something like, you know,
you grab a whole bunch of enormous matrices, you feed in a bunch of data,
you, and then you get some error signal out,
out the other side of the neural network that says
your neural network did not model this thing well enough.
And then you, you'd apply another procedure to update the, the like little numbers inside,
the numbers inside all the matrices.
So it's just, it's just a big old, big old bag of matrices
with very specific transformation rules between them.
And it took, you know, about 10 years of kind of wandering around the space of matrices
and transformation rules before people found a very generally useful rule.
That, that, that kind of body of rules is just codified in a transformer architecture.
Was I correct in my understanding that it's sort of like a neuron in that when
it hits on positive feedback, whatever process it used to get there,
it gets strengthened and when it gets negative feedback, it gets weakened.
Yeah. So for the like atomistic neural network primitive,
they, they vary commonly and also, also in transformers have this, this,
this property that they kind of gate data as, as it comes in.
And kind of the whole magic of neural networks as a discipline
is kind of intelligently stacking these primitives in a way that
the, the gating mechanisms work to your favor.
So like a very, very, very rough picture of this is like, you could think of something like,
you know, you're trying to write a hand digit or, you know, a cat, a cat dog recognizer.
So you give it a picture of a cat, give it a picture of a dog.
And you want it to say like zero if it's a cat and one if it's a dog.
And if you use something like a neural network architecture for this,
you would find that if you actually like inspected the, the, the stacked
primitives that, that make up the neural network that does,
that does this sort of operation, you would find things like, you know,
in the early layers of this neural network, if it was, if it was a bunch of stacked layers,
the early layers would be doing things like detecting whether or not an edge was somewhere
in the image. And then as you kind of move deeper, you would, you would find that these
primitives were doing things like detecting combinations of, of edges and being like, oh,
this is, this is clearly a dog-like ear or a cat-like ear. And, and that there would be some
neuron kind of midway through the network that was kind of hand-wavely detecting the presence
or absence of dog or cat-like ears. And then by the time you get to the end, you have literally
two neurons. One of them lights up when it says this is a dog and one of them lights up when it
says this is a cat. And that's, that's a thousand foot view what's, what's going on in pretty much
every neural network architecture. Okay. So GPT is more, has greater innovations on top of that.
Yeah. So the, the particular, yeah, specific innovations for GPT are many, many transformers
stacked together and a whole bunch of kind of infrastructural tricks to even train such a thing.
So I mentioned GPT one was 2018. GPT two was 2019. And it was quite large for, for by most
language model standards. I think it's, it's like a billion parameters or so, like a billion numbers.
Like it's, it's however many matrices, it's some number of matrices, which add up to about a billion,
a billion numbers. And, and that was a pretty big deal when it came out last year, because it,
it was quite good at generating text, like, like surprisingly human looking text. Y'all,
y'all talked about it. You saw what it could do. It can, can do kind of crazy things.
I've got a quick interjection there. Speaking of us having talks about it, I was conflating the
episode on GPT two with the one that we did on Alpha Go. Oh yeah. Who did we have on for GPT two?
Was it anybody? I think it may have been Patrick again. Okay. That's why I conflated them. Okay.
So yeah, then here's my, do you remember? No, that's not it, but I'm definitely wrong. No,
no worries. I was just making sure I wasn't maintaining that confusion. And so from a high
level view, the other reason I brought that up is because I forgot that was just last year. I
thought it was four years ago, like whenever GPT or whenever Alpha Go was, so yeah. Yeah, GPT one
was 2018. Here's the other problem is that 2019 subjectively the last eight months that felt like
three years. So why, I mean, GPT two hit the news, you know, my radar, and I think the
rationalist communities radar, like a nuke, did anyone notice GPT one or what made GPT two so much
more exciting? And then we'll get into what makes GPT three is much more exciting than two.
Yeah. So GPT one was, I mean, it was exciting. It, it, it, I think it did pretty well on a whole
bunch of language benchmarks. I think the real, the real movement happened with GPT two because it
was, it was one a lot larger and trained on kind of unstructured text. So I should say a little bit
about how this model is actually trained and what that even means. So typically when, when people
have tried to solve language learning tasks, they do something like find a data set, which is like
those aforementioned tasks where you say, you know, it's literally a sentence and the sentence is like
put a ball on the table, put a book on the ball, where is the ball? Like, and then there's a label
for that particular sentence that says, you know, the ball is under the book in text or something
like text. And usually when people would train their language models, they would take a huge
data set of such sentences and, and do neural network training, which is an algorithm, just
an algorithm one can perform that, that tunes the weights of the matrices of the neural network,
so that it is very accurate on that data set. This is, this is me just describing how people,
how people train machine learning models, 1000th of view. What was pretty surprising about GPT2,
even at that point, is that it's actually an unsupervised model. And what this means is you
don't actually really have this notion of there being data that with labels, it's not like you
have a sentence that's like a question and then an answer to the sentence that your network is
trying to predict. It's more like you just have a huge unstructured bucket of sentences. And then
training, and I'm air quoting here, it is training, but it's like what you do is you, you take those
sentences, and then you like knock out a word, and then you say, what, what word should go here,
GPT2? And then you do that over and over and over and over again, until it, it understands or it has
kind of an understanding of that, that huge corpus of, of sentences that you've thrown at it.
That's kind of a pattern match. Yeah, it's, it's just a completely unstructured pattern
matcher, just, just giving it sentences and basically saying, what is, what is the next token
given that you've seen this much sentence so far? What, what should the next word be? And you don't,
you don't need labels for this because you just have, you have sentences and you can just chop out
words of those sentences and be like, well, I know what the next word is, I just chop it out.
You don't, you don't have to work very hard at all to come up with, with such a dataset because
it's just, it's just data that you take and, and, and blow words out of. And no one really expected
this to work this well, because it, after doing that, GPT two was like even better at a bunch of
these other tasks. And then you can take GPT two with this pretraining done. And this is what the
P means in the GPT is that it's, it's been pretrained because it's, you've, you've fed it like
millions of documents and you've done this like knockout procedure to be like, all right, what
is the next word? And you can take that model and then do supervised training on top of it. And
this is the more standard, feed it a dataset with labels and have it try to learn that dataset,
like try to, try to do the standard given, given the sentence about balls and books being stacked,
whereas the ball, and they showed in GPT two that taking the pretrained thing and then fine
tuning it on supervised datasets beat everything. It was just like state of the art across the
board on like all kinds of stuff. And this was, this was pretty, pretty crazy. So I can, I can go
to GPT three there. Do people have more questions about where GPT two is? I think I'm good. I have
a question about the, the pre-training because like, I remember would have been sometime within
the last year or something. Again, times have been fuzzy for a while. At a meetup, we did a,
like we were, some of us are playing AI dungeon on our phones and it knew what vampires were. It,
like one of us did, I think there's like a post-apocalyptic one and I was doing like a
fallout playthrough basically. Like it's not trained. It doesn't know. So I guess what I'm
getting at is super mutant plus. Right. I think it brought up super mutants. So where is it,
what is, where does the pre-training data come from? I think they just did an internet crawl.
So I think they looked at like literally fanfic.net and Reddit and, and just that was part of the
corpus that it was trained on. And they probably for AI dungeon. Yeah. Yeah. And I think they,
they did a light amount of filtering for like safety reasons because I don't want it to just be a
horrible monster. A racist machine. But yeah, they, they, they took a whole bunch of internet stuff
and they took, they probably did a little bit of extra fine-tuning on top with like maybe even
biasing the pre-training data set to be like stories or story-like and like not, not doing
things like, you know, technical manuals of tractors. And, and then it, it was a good storyteller
after that. So, and this maybe will lead into GPT-3 or not, but like I, I guess I'm curious,
if you just trained it on every physics paper and book in the last century or since the inception
of physics and then asked it a physics question, it'd probably be able to give you whatever the
gravitational constant on earth or something. But so I guess what my proposal is like,
feed it all the physics and then ask it a question that we don't know the answer to
and then test the answer it gives us and see if it's right. If it was that easy, someone would
have done it by now. So what's going on there? It doesn't sound easy. Yeah, yeah. So what is going
on there? So if, I think this is a good opportunity to talk about GPT-3. So it was, it was performant.
GPT-2 was performant, but it wasn't, it wasn't like, it's not like an AGI, right? Like it's, it's
literally the model is you give it a sentence and then it spits out the next token of the sentence.
It's literally just trying to find what the, what the most likely next English word token is
for the sentence that it has been given. And GPT-2 was not nearly large enough to ingest
all of, all of human knowledge. It was, you know, only about a gigabyte. GPT-3 is 100 or so times
bigger. So it's about 175 billion parameters. And it's just, it's just crazy at all kinds of things.
So it's, it's, what's, what's maybe most surprising about GPT-3 is that one, it's more or less
identical in architecture to GPT-2, just much, much larger. Two, it was trained on a
outrageously larger amount of data. In fact, they didn't even, during the pre-training step,
they didn't actually even complete one pass through the entire crawl of data they had.
And they crawled like many, many terabytes of, of internet text as, as just the stuff to be
doing this kind of knockout procedure on. And just from that point, it started, started being able
to do things that are kind of crazy. So remember, I said, like, the way people usually try to use
language models to solve tasks is they do this supervised procedure where you, you know, you,
you have pairs of sentences and labels. GPT-3 doesn't even really need that. You can just kind of in,
in human words, explain what you want to do, or provide maybe five examples in text, like literally
you're just typing text for like what, what the sort of relationship is that you want GPT-3 to
model, and then just hit like the next token button, and it will be able to do the sort of thing you
wanted to do. So like, we could literally like, I actually have it up right now, we could, we could
do some of these examples live if you want. But like it's, it's just wildly capable without needing
to do anything, but just take data and, and unsupervisedly knock words out and try and predict
what they are. So if you took that kind of thing, and you fed it, you know, literally all of physics,
and like literally all of, all of the scientific edifice of knowledge necessary to understand
that physics, it's, it's not completely insane that it would be able to answer difficult out of
domain questions, if you will. I would be surprised if, if it like could figure out new physics,
just because I think figuring out new physics is a slightly orthogonal problem than what GPT
is actually doing under the hood, perhaps, which we can talk about more. But it, it, it would probably
be, do a really good job of kind of being able to explain what, where, where physics is right now,
at least, and, and could even maybe make plausible suggestions for what to do next. But that's,
that's a little like, they haven't trained GPT3 on all of physics for one. It's, it's hard to
ingest all of the scientific articles you might want to do for such a thing. But like, I think
that's like, I think people take seriously the idea that is possible now, which no one did before.
No one thought you could just shove all of the human knowledge into one, you know,
couple terabyte model, and then have it be able to do these kinds of things.
Well, like, I mean, it sounds like it's just a problem of scale for that part of it. I mean,
I understand how, like, I take your point that, you know, answering new questions might be orthogonal
just to like having the existing edifice of knowledge. But like, I've got saved on a hard drive
somewhere, the 64 gig backup of Wikipedia, in case the internet explodes, and I want to learn
how to do something, right? So granted, Wikipedia isn't everything in physics, but it might be
enough to make a, make a, like, so I, I guess I'm not sure where I was going with this. I feel
like I had something actually useful to contribute. No, no. So I think, so you're, you're hinting at,
like, it seems to be the case that Wikipedia is, is sufficient knowledge for, say, a human to, to
start, you know, contributing at the forefront of physics. And I think I mostly agree there. Like,
if you take a human, you give them a pile of textbooks and Wikipedia, they can probably
actually start contributing to like new knowledge pretty quickly. I think this is hard for things
like GPT to do, because it's sort of a different grain size than, than what it's doing. Like,
again, it's like literally doing next token prediction. And what, what forefront of science
work looks more like is kind of random search in a, in a space of things that you internally
have built models for. And, and while GPT definitely has some kind of weird metaphorical
analog of a model inside of it, of many different sub-disciplines, I don't think it's,
it's internal models are quite sophisticated enough to do what a human researcher can do.
But could it replace like a, what can I think of words today?
It could replace a lot of boilerplate stuff.
Well, I was going to say like someone who works at a call center.
Like, yeah, absolutely. So like, yeah, yeah, I think like, I mean, they have these cool demos
where like you, it's, it can do things like you ask it, you know, what's the bash command for like
finding a file with this name, and then it'll just, it'll just give you the bash command.
