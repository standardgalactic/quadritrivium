Like it, it, it, you can, you can talk to it like it's a thing, and it can provide
reasonable answers to your questions in an almost knowledge agnostic way, which is kind of nuts.
Do you have any children? I know that's weirdly personal, but I'm about to lead into something.
No, I do not.
I was wondering if you did have children, would you trust GPT three to act as a replacement
teacher for them, and perhaps even storyteller and some sort of a young lady's permer device?
Yeah, no, this is man.
I want to like tell you that YouTube to do that.
And I think that anything that works better than YouTube, because the algorithm is basically designed
to like, well, yeah, there's, I want to share a video, I don't know if I've like had it on
here before, it's kind of an aside, but it was, I think called something like the horror of children's
YouTube. Yeah, yeah. But yeah, yeah, that's, that's, YouTube, YouTube algorithms, man, they
kind of go weird sometimes. So I, I think so, like that, that might be controversial.
It can definitely tilt into air quotes, unsafe things. If you, if you guide it, like, I mean,
this, this is, this has come up a couple of times in the community, but it's like, you know, you
train a language model on literally all human knowledge, and then you ask it to be horribly
racist, what do you think is going to happen? It's, it can do a really good job of being a
horribly pretending to be a horribly racist person. And this should surprise no one based on
its capabilities. I think maybe one would want to do something like put in some safeguards if
you're going to put it in front of children. And OpenA has actually already done this. So if,
if tokens start coming out of GPT-3 that it can detect are toxic in some way, it's like,
are you sure you want to continue? It's, it's kind of adorable, actually. But this is, I mean,
I think this is a hard problem in the total abstract, because it's, you could certainly,
if you were a human talking to another human over a chat program, I'm sure you could fool any such
detector unless that detector was as smart as a human. And maybe, maybe even then, if you were
careful enough, right? So it's like, there's, there's this problem of, you know, massive language
models that are, that are multimodally capable of representing human knowledge and telling stories
in any sort of way. And then there's the problem of making sure that, that, that thing doesn't say
something extremely dangerous to someone, which is just kind of the general problem of AI safety,
which I think you can, you can put, again, you can put guardrails, but you can't solve in general.
Not just dangerous, like GPT-3 doesn't know when it's telling your child wrong information,
like that elephants are blue or something. Right. So I would, I would strongly encourage my child,
my, my hypothetical child to, to verify, to, to create with, to create cool stories or whatever
with GPT-3, but realize what they're actually dealing with is not necessarily going to tell
the truth. But since you're the one with the, the insight here, or maybe Matt as well, because
I know you've also played around with it a lot, how likely is it to actually give wrong information
if you ask it something factual like you would ask a teacher? Oh, that's a good question. Yeah,
I mean, I'd say, I'd say I can't really give you odds on that, but if you've primed it correctly,
and it's about the kind of factual information that you would expect a kid up through high school
to know it's probably going to give you a right answer. Really, the thing that I would add in,
interject here, especially when it comes to talking to kids is, is that the priming is just so
important. And what, what that means in this context is, is basically loading the GPT-3
sort of conversation window with enough text so that it actually understands what conversation
you're having. So, and this is one thing I see all the time online where, you know, there'll be a
sort of crappy low level debate about, you know, GPT isn't that impressive. And it's, it's very
often from someone who, who's not really quite understanding that when they're playing with it,
they are, they're prompting it in a way where it's, it's filling in the next token completely
appropriately because it thinks it's having a stupid conversation. It's answering the question,
it thinks it's being asked, in other words. And you have to, you have to be quite careful
in, in how you frame the conversation. And the thing, like I've, I've sort of, you know, been
sitting there and had my, my son be like, you know, okay, now say this, and I type in what he says.
And then the thing is, it rapidly just degenerates into complete nonsense because it, it doesn't
know like, oh, I'm talking to a child, but it's like the text that's being represented here is
pure chaos. And so I'm just going to quite correctly just say nonsense. And not even,
not even really like clever nonsense, just like garbage, because the internet is full
of nonsensical garbage anyway. Yeah, it works, it functions much better as if you're already
familiar with kind of tropes of human storytelling, it's a good improv tool, because you can, you can
kind of play off of it, and it will play off of you. But if you, if you, if you don't really have a,
a base of knowledge that you can kind of recognize what it's going for, then
like, like Matt said, it's, it's kind of a chaos. And so in that way, like,
while I'm not worried about a child using it, I don't think a child would have much fun until
they're like, maybe a teenager. There's also some concepts where it's sort of,
um, like, like it seems really obvious to me that it could get this, but that there's just not
enough data for it to have really quite nailed it down. Like for example, I was running some
questions by it where I was like, you're facing north, the mountains are to your left,
what carnal directions are the mountains? And it would, it would get these rights sometimes,
but I was, I was actually surprised at how often it got it wrong. And it's like, but that's an easy
question. It's like, well, it's an easy question. If you have an innate spatial sense that you have
anchored the directions to, but maybe it just didn't come across the idea that, I mean, because
left does not mean west. Left means west only when you're facing north, right? So it's a,
there's an if then quality to that, that it probably just didn't pick up because it doesn't
have a logic. It doesn't, it, it doesn't have, that's an interesting question actually. I think
that it can do something like reasoning, but not in the way that we do. And it doesn't have,
like it definitely doesn't have formal logic. It definitely doesn't, but, but it definitely
can kind of like step forward in a thought process, you know?
Yeah. So for, for example, I have a, I have a kind of nuts example. So I was trying to get it to
emulate text adventure games where, so I provided as context a text world, which was a line,
and it was a line of like underscores to represent the world and then like an X character to represent
the player and then like some more underscores and then like a G for a goblin. And then I primed it
with like, you know, go left, go left, stay put as, as commands. So this is all like literally the
input text to GBD three was one such text world with the command go left. And then the next frame
was, you know, the little X character move to the left. And then, and then, you know, I did this a
couple of times. And then my next, for the thing that I actually asked you to say was, I said,
okay, go right. And it successfully recreated my text world with the character move to the right
once. I was just like waving my hands around in the air because it's like, this is, it is doing,
it's doing something like it's actually building a quite sophisticated probabilistic model of,
of what you have given it. And it's, it's manipulating those weights in a way that's
quite high level. Like it's not like that you can't memorize. Like I, I invented that whole
thing. Like it's not that that exists nowhere in the internet. It had to have a notion of like
movement on the line, meaning something having to do with the words going for left and right. And
like, I'm just, I'm actually waving my hand. I'm really curious that it moved the X and not the G.
Oh, so I had, I had provided examples of like, I had, my examples were always moving the X character
when I gave it commands. So I, yeah. And then, and then the final line was now you do it machine,
and then it did it correctly. Yeah. I mean, that blows my mind, honestly. I mean, that, that,
I know you told me about that before, but I'm still kind of like grinning over here because
I wouldn't have expected that it could do that honestly, you know, and
Did you ever give it the command to go right or did it figure that out on its own?
Yeah, I had not. I literally, I put the text to go right and then it generated the world with it,
with the character move. That's fucking nuts. Yeah, I would love it if you would like attack
the goblin, just like, just like, see how far you can push it into extrapolating what, what you might
mean by that, you know. Yeah, I did. It started to break pretty fast because this is pretty,
pretty out of domain for what GPT is usually used for. But like it can, I mean, it would come up
with plausible, so like you can just put the, push the button and keep generating text, and it would
come up with like plausible, you know, fantastical sounding next things to do, like, you know,
because some versions of this, I would, I would provide a short text description of what was
going on in the world, like you're standing next to a goblin, you hear wind in the distance,
stuff like this, and it would like continue in the style of that when, when generating like
new frames and new commands for itself for where to go in the text world. It wouldn't always do
what they said, but it would, it would continue the pattern quite faithfully. I have heard that you
can do some pretty nifty things as long as you like, like you said, play around with it within
its rules and like massage it as you go. I heard someone was trying to recall four distinct type
of art styles that were different and she like couldn't quite remember the name of the last two.
And so she like went into the AI dungeon and it was just interrogated a character about like, you
know, give me, give me four different art styles. And it gave her three and then a fourth one, but
the fourth one wasn't what she was looking for. It was like, no, not that one, like another one.
And the character was like, yeah, let's go do this AI thing, this dungeon thing. And so she went and
the next person she got to was like, someone else, she talked a little bit, they give her a
different art style. That wasn't what she wanted either. But then she met a spy. And the spy was
trying to convince her to go on this mission. She was like, it's really important for my mission
that I know these various art styles so that I can serve our government correctly.
And the spy was like, Oh, well, I've heard that this art style, it might be what you're looking
for. She's like, that's it. That's the word. Yeah, I mean, just talking about the awesome
stuff that people have gotten it to do could probably fill up a two hour conversation all by
itself. Honestly, I mean, one thing that I was having fun doing was just pasting in interview
transcripts from podcasts, like an interview with Hugh Jackman. And then I step in and I'm like,
now I'm the interviewer, I'm asking Hugh Jackman a question. And then it answers and it's like,
no human could tell that this is not just the next answer that Hugh Jackman would give here,
it's perfectly in like the sort of polite, friendly Australian tone that he's using.
It's the kind of thing he would say, like you ask him about his workout routine,
he starts talking about the importance of nutrition. Because like it knows who Hugh
Jackman is, it knows that Hugh Jackman's in movies and works out. And that's the
thing is you can ask him about stuff that's not in the transcript that you gave. And it still
knows. I know who Hugh Jackman is. Yeah, that's so fucking cool. I've got my own thing I want to
contribute as far as the fun story of what I've seen it do. I saw a video where they were basically
give me a React UI that has six buttons in the color of the rainbow. And I think it took a couple
nexts for it to do it, but it did it. And then it has the code for it too.
So as a not very skilled front end developer, luckily that's not most of what I do, but I'm
like, oh, this has me out of the job. But then my next thought was immediately like, well,
you need a deterministic background for what's making your application. So luckily, I think you
can't just have it randomly build shit. Because if it does break, it's going to be complete nonsense,
more or less to the person that's going to fix it if it's not built with any standards.
And you still need a person to tell it what you're looking for and be like, no,
not like that. You still at least at this point would need a human who understands how
UI is supposed to work and look and replace 20 different humans.
Yeah, that's not the engineer necessarily. That's going to be like the product,
you know, customer liaison. They would just rule out those pesky engineers, the ones that,
you know, want all this money to do all this annoying shit.
I don't know. That's kind of what you wanted. Well, that's different topics.
Not what I want.
I mean, I'm pro automation as long as we get, you know, the fruits of it.
UBI.
Yeah. And all seriousness, like you can just push the automation up the chain though and be like,
and I'm not saying this is trivial or that GPT-3 can do this, although I think
GPT-N might be able to where you, you know, yes, right now you have to say, okay, I want a button
with the colors of the rainbow, but maybe the next iteration you say, I want a good UI for a
website that sells shoes. And it's just like, got it, you know, because at a certain level,
it actually understands what that is and the way to go about it.
Right. In fact, yeah, I don't think it would be able to do that right now, but you could probably,
I am curious to see what would happen if someone were like, generate a prompt.
You know, this is a couple of generations down the line, maybe, but it's like,
generate a prompt for GPT-3 that would cause GPT-3 to make a UI that would be good for this
situation. And then it generates the prompt that you then feedback into it, you know, like you.
Yeah. I mean, or, or just even like literally like in feedback, telling it what to change,
right? Like it's, it's interactive. It's like, you just, you don't like something,
you tell it, and then it, it fixes it, right? And yeah, I was going to ask that actually,
like, is there a way to give it a reward signal or a dis-reward signal punishment?
Yeah. So you can, you can absolutely, you can like, you can do the fine-tuning process. So like,
and this is, this is what actually gets GPT-3 to be state-of-the-art on a lot of these benchmarks
that language modeling researchers care about is you take, you take the already insanely capable
unsupervised model, and then you do that process where you take some, some, a data set with,
with targets and labels and, and you train it to very accurately compute things about stacking
balls on cups and stuff. And then it just beats everything at that task, because it's like,
it's, it's leveraging, it's like perfect knowledge of language. And then applied to the,
the very specific sub-problem of like, you know, hard, hard stacking of object problems.
I, I wanted to go back to Steven's question real quick. Like, it apparently can do some
programming on its own where you just tell it, you want some stuff and it'll give you the code
to do those things. How, how much, because rations are highly overrepresented in the
programmer community, how much can it replace right now? Like, how worried should we be about
this or about GPT-4 coming and just taking over a lot of the programming work?
Um, so I, I don't think you should be worried yet. I, I think it will like demonstrably automate
a ton of, a ton of the sorts of programming tasks that we already all hate. So like, like, you know,
when you have to like write the same sort of data structure for the thing over and over,
or you, you want to do the same sort of transformation to the thing, or even just like
define, define a network like for, for my kind of job, like I want to build a neural network to
do something. And I, it's a ton of boilerplate to like write down what to do that. And I would
probably go look on the internet and find it or some internal template to use. I think a lot of,
a lot of short-term tasks of that form will just fall to this straight up. Like in the same way
that IntelliSense helps us now where you, you're writing code and then the, it just comes up and
says, oh, did you mean this function? It's going to be that, but for like entire function methods.
I think, I think maybe 10 years from now, this is going to be insane enough that it's going to
automate lots of software and lots of everything. Like I, I'm, I'm like pretty bullish on this kind
of tech, honestly. Like I think it's, it's, it's both more insane than people realize already,
and will be more insane faster than people are willing to predict it will be. So be a little
worried is, is all I have to say. I wanted to, to kind of interject here. And like this is something
I said the day earlier today where I was like, I really want to make sure in this conversation
that I don't make the same mistake that I see a lot, which is like moving directly from GPT3 is
really cool to like how cool the next thing is going to be without a clear line between the two.
Because like, yeah, GPT3 is really cool and do all these cool things we just talked about.
It's not an AGI. It doesn't know that West is left when you're facing North most of the time.
It, it, you know, it's got limitations. Nobody here is saying it doesn't have limitations. But
