that it was more like a, a well-calibrated, a very strongly calibrated, like
text prediction or in my understanding, which I think very well, maybe wrong, but
if it's right, it doesn't have like a confidence node that it's like, I'm
pretty sure of this, that just has like, it just has the next word and maybe some
of the previous ones, but anyway, go on.
Yeah.
I'm going to skip just a little bit and continue for me, all of this adds up to
the following humans are actually pretty dumb.
The things we think are hard are not hard, certainly not as hard as we think
they are. The clutch that is the human brain is a clutch or Cluj, I never
was sure how to pronounce that word.
The only word that I saw was one that you skipped and that was copacetic, which
I liked the Taylor shout out because she used that word 15 times in worms.
Okay.
Maybe exaggerating, but she used it a lot.
So that's the most times I've ever seen the word copacetic.
In any case, I want to shout out that I noticed that Matt.
So well done.
The Cluj that is the human brain is good at doing a wide variety of things, but
any individual thing we do ultimately requires less CPU power than you find on
your cell phone. Yes, the training phase for these systems tends to involve huge
amounts of compute, but we're only getting more efficient at that part.
And the train system ends up being something that, as Alexander Wales noted,
can fit inside a virtual machine on a net book.
Yeah, it was four gigs and I'm not sure how many gigs though human brain is,
but to me, I mean, four gigs is well, it's funny because like now it's nothing.
And yet I remember 15 years ago when like the new large 256 megabyte flash drive
was $60.
So I remember the first time I heard the term gigabyte and I thought someone was
like pulling my leg because it sounded stupid.
Like gig, sure.
Whatever, man.
Right.
Okay.
The only reason GPT two isn't obviously scarier than it already is, is that it wasn't
really asked to do anything other than predict, than predict text, give and text.
So imagine this, another AI system trained purely to identify logical incoherence
or contradiction in written text, not that hard to do.
I wouldn't think there are easy versions of this that can be made without even
needing deep learning, but a full deep learning system with lots of training
data would do an even better job.
Now imagine connecting this system with GPT two, such that GPT two generates a
large number of possible essays rather than just one and the logical
coherence spot judges which essay is the most logically coherent.
Usually such systems are trained in tandem, such that they get even better at
doing their joint job.
Now you have a new system, which does what GPT two does, except doesn't make nearly
as many of the kinds of obvious mistakes that GPT two does.
Now just think about other types of simple things that could be plugged into
Alpha Star or GPT two, and you start to see the road to AGI.
Yeah, I like that.
I think that that sort of hybrid thing is what we're both kind of converging on here.
This tool on its own is cool, but it's not on its own sufficient to do the job.
And yet training up a logical consistency bot, and I mean that's the thing too,
is the training times with these is not like training a person where you can give
them a few years in school and read a bunch of books and write a bunch of essays.
It's like hours.
Subjectively it's hundreds of years, but whatever.
For us, at our scale, that's all we care about.
It's pretty wild.
I'm excited about it.
We'll see what shakes out.
One final comment from Googleplex Bank?
Actually, I had one here on.
So we kind of took these out of order.
We read Trebotron's first, then we did more dint of males,
which was the parent comment of his.
Oh, was that what it was?
Yeah.
Oh, okay.
Oh, wait, no, I'm sorry.
They kind of had a bit back and forth here.
So there was Trebotron on his own comment, and then there was another one
underneath Matt's comment here.
So I'll read that one really quick, too.
Mm-hmm.
In the child to Matt's comment was another Trebotron one where he quotes,
it clearly has the complex of understanding of conceptual relationships.
It knows water is wet, that wet things are slippery, that slippery things
make people fall, et cetera.
And then Trebotron says, no, it doesn't.
It knows none of that.
The only thing it knows that wet, the sequence of character is not a concept,
often is seen around, quote, water.
Another sequence of character is not a concept.
It's just a statistical model.
It's not any deeper than that.
No conceptual knowledge at all.
Quote, is that it wasn't really asked to do anything other than predict text given text.
Unquote.
It literally cannot do anything else.
Arguably, you can take that size neural net and train it on cat pictures,
and it would be able to probably generate a decent cat pic,
like this person does not exist thing that we talked about.
It wouldn't be able to do anything, text prediction or anything else.
This is why GPT2 is not scary.
Its main innovation is parameter big spacing enough to encode a decent statistical model
of English and nothing more.
So then Matt kind of grabs out that, no, it doesn't, no conceptual knowledge, quote,
and then says, you're just a machine that models statistical relationships.
We have more context for our statistical relationships.
At this point, we're just reading their thread.
We are.
And it was really good thread.
So this isn't worth reading thing.
This is just a time constraint thing.
And it's just we don't want to be reading other threads.
Yeah, this thread exists.
It's on under episode 80 on the subreddit, the reward and parallel GPT2.
Check it out.
It was a great, great discussion.
One of our most highly commented posts in the last few months.
So, and I got to plug really quick too.
If you like that level of like thoughtful analysis and good use of words,
Matt can talk like that in real time too.
That's true.
So I once again want to plug the We've Got Warm and We've Got Ward podcasts,
as well as the Doofcast episodes of, well, all of them,
but they're good with or without Matt too.
But Matt and Scott discuss Warm and Ward, a great web serial fiction,
if you haven't read it, as strongly recommended,
in just that poignant and deep and thoughtful analysis.
And what's great too is that Wild Bo was playing at the same level as the author.
Like everything that he's doing is clearly on purpose.
And that's the sort of stuff that I could write a short story
with a beginning, middle, and end.
And it would be like, yep, things happened.
And it told the little thing I wanted it to.
I could never put in without some serious time,
and it would suck for me to do it.
Symbolism and implied meanings in this and that.
Like I just, I don't have that.
And so Wild Bo's a brilliant author,
and the editors work brilliantly.
So everyone check that out if you haven't yet.
Excellent.
To wrap up the GPT-2, and I think also this episode,
Google Plex Byte says,
I am saying that given enough data and computing power,
GPT-2 could understand abstract mathematics
as well as a mathematician does.
Abstract mathematics doesn't require any reference to the physical universe.
So GPT-2 would have every tool it needs to understand it
as well as a mathematician can.
I think we'll leave that to computer scientists to dissect there.
Without trying to taboo the confusing words,
like understand and that sort of stuff and knowledge maybe.
I mean, I guess, yeah, you can write something
that'll give you the resultant equation of throwing a baseball.
It doesn't have to know what a baseball is.
That's not abstract mathematics though.
That's fair, yeah.
So, but I mean, GPT-2's language analysis isn't abstract either.
It's giving the actual words, give an actual input, right?
I'm sure I'm missing stuff.
I have a job, well, I work in the field of computers,
but my computer science is drastically lacking.
And if that's not obvious, well,
if that wasn't obvious before, it is now.
You were not an AI programmer is what you're saying?
Or a computer scientist, okay.
I went to a boot camp.
They don't train us computer science.
They train us on how to code,
which is, that's the main difference to get out by, for example,
actually I've had a couple people ask about that in email
and in Patreon comments and stuff.
It's worth pointing out, I guess, if anyone's curious,
the analysis of like, post computer science graduates
and post boot camp graduates for software development,
it tends to be that boot campers can code faster and better
than the average computer science graduate.
What they can't do is like, describe data structures
or different data models for,
or like different models for building applications on,
or really any computer algorithmic, any stuff.
That's all skipped over in the like, how to do this course.
So if you want a good, long understanding,
do the long, hard way.
If you want to be able to hit the keyboard
and get a job in six months, maybe do a boot camp.
Oh, we do have one comment from me.
Yeah.
I said, this happened pretty fast
and linked to Alphabet Google and anything that Alphabet made.
Alphabet made a Chrome extension
that is designed to tune out toxic comments.
It just will scan the comments, rate them on toxicity
based on its understanding of the language
and not show you, you know,
past a certain level that you choose.
I prefer the plugin that just changes
all the YouTube comments to like Meow and like Woof
and all that stuff.
Right.
But that was, that was pretty fucking impressive
that like, it could in some way have a rating of toxicity
based on reading a comment and, you know,
I want to show them or not.
I wonder what like the success overlap is
