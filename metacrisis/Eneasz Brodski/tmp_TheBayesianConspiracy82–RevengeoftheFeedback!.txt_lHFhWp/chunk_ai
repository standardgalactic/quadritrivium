And I take that back.
Actually GPT two doesn't seem to get us closer to being able to have, I don't
understand it well enough to say whether or not if GPT two, or if GPT five, if we
could have a conversation about the Lord of the Rings with it, where it could
teach me something I didn't know, other than like a parrot, a passage I missed
or something.
Cause it, I don't think it quite, it doesn't work on a holistic understanding
of the, the, of the, if I was talking about the rings of the trilogy, right?
Right.
It works on like word distributions and how fun I see these things with this
things.
Yeah.
So it, I don't even think yeah.
GPT five in my, I'm sure dad, that's two is even like the second version of this.
I'm sure I'm just torturing the hell out of this.
I don't think that version 5.0 will be able to like challenge Steven Colbert
on like thoughtful and sighfulness on Lord of the Rings.
Now use him cause he's a big nerd with this.
Yeah.
So I mean, it could probably, I would say certainly challenge him on trivia about
like who, who's related to who and how.
I think there's a difference between trivia and like thoughtful analysis though.
I agree that there, I agree with the second part that there's a difference between
those two things, but does it even have a real running memory of like, you know,
our, our, what's the daughter's name?
The daughter elf that Erwin Erwin or Eowyn Eowyn Eowyn was the Rohan guy.
My bad.
No, that's Aomer.
Who cares too many names?
The elf daughter.
If, if, if, uh, if Liv Tyler is, is, uh, Oh my God, you know an actress's name.
Yeah.
I know, right?
That's crazy.
She is related to the guy from Aerosmith.
Yeah.
Um, is his daughter.
Yeah.
Actually, I think that it wasn't, they weren't even raised together.
It was like, he had, he had her with a groupie.
Um, really?
Don't correct.
Don't, don't, I thought it was like a model.
Well, I mean, she, she is related to him.
I think, but I don't think that they grew up together.
I don't think that he was like her father.
I think that he was, I think that, I think that he was her biological father, but
not like her in Yandu's words.
She was, he was their father, but not her daddy in the timeless words of Yandu
from Guardians of the galaxy too.
It was Arwen, the name of the, that's right.
In any case, if you find out if my trivia is right, but then, uh, if
Agent Smith's daughter was Arwen that, or Elrond, I don't know who that
guy's name is, then she was the daughter of a, of a model and singer, which
makes me think it wasn't just some random groupie.
Cause generally, Oh, huh.
Yeah.
She named her after their last name was after, uh, Todd Rudgeran, who was
she claiming that Todd Rudgeran was the biological father at age.
You tend to relive and live met Steven Tyler and figured out he was her father.
Okay.
Yeah.
And now we all know more about Liv Tyler than we ever thought we would
before we started this podcast.
Well, you already knew this apparently.
I just didn't know it.
I don't know if it counts.
Yeah, funny.
I don't know if this counts as knowledge because that wasn't really sure.
But, um, yeah.
So what I, but this is my long way of getting to, I don't know if GPT two
knows that Arwen is Elrond's daughter.
I mean, it's certainly knows it knows that those go together sometimes.
But like, I think it, it would be easy enough to be like, it, there's some
concept of a daughter is a relationship.
It doesn't have concepts.
It just has, it has word mappings.
Yeah, but what I'm getting at is, I don't know if this would be enough.
I don't think that this kind of neural net is the thing that would engage
you in a good conversation about this until what a daughter is in the real
world, but it knows that how the words daughter, father, and all those are
related in terms of words, I guess.
I was just curious if it would be a daughter is a thing that comes from a
father and a mother.
I guess maybe I wasn't sure if like GPT two ever had like probabilistic
estimates where it could be like, no, no, I like, if I, if I challenged it,
it's like, no, no, it was actually someone else's like, no, no, I'm pretty sure.
I don't know if GPT two has that kind of, of like insight into its own
probability distributions, or if that's even how it works.
No, you're right.
I mean, we're using, we shouldn't use the word no, because there's a lot of
implicit assumptions with the word no right now.
Right.
But we have maps of concepts in our brains that cover a lot of things.
But I think our two maps of concepts are just a lot thinner and I think only
work on words.
And I guess what I was getting at is that my impression was that they did,
that it didn't quite work in a way that like, if I challenged it, if it could
say, I'm pretty sure that this is the case.
I mean, I've been able to give me text examples or something, but it couldn't
say really, I read this and I gave it like a 99% confidence level that this was
the case.
Right, right, right.
I think it's just like, I saw the word Arwin, Elrond and daughter together a
lot.
Yeah.
Yeah.
I don't know much else.
I think I kind of lost myself other than, oh no, I was agreeing with, with
what, uh, Chebotron was saying that this isn't AGI stuff.
Um, and then I was briefly thinking for a while that this might be, Hey, I can
have a cool conversation with a robot about a book, but then I walked it back
on that and said, wait, no, I don't think that this is, this is that either.
This is something that it can give me very often accurate trivia about the book.
Right.
It certainly couldn't have a conversation with you about it.
Cool.
Uh, Mordinnum male responds or not responds, but Mordinnum male has a
counterpoint for me, Alpha star and GPT two were a one two punch that made me
much more concerned than I had been and moved my AI time tables up slightly.
Before now, it wasn't obvious that Starcraft two is a game that can be played
super humanly well without anything that looks like longterm planning or
counterfactual reasoning.
The way humans play relies on a combination of past experience, narrow
skills, and what if mental simulations of the opponent building a superhuman
Starcraft two agent out of nothing more than LSTM, which law short for long
short-term memory, uh, nothing more than long short-term memory units indicates
that you can completely do away with planning, even when the action space is
very large, even when the state space is very large, even when the possibilities
are combinator, combinatorially enormous.
Yes, humans can get good at Starcraft two with much less than 200 years of time
played, although those humans are usually studying the replace of older masters
to bootstrap, but I think it's worthwhile to focus on the inverse of this
opt for surfacing that a sophisticated problem domain, which looks like it
ought to require planning and model based counterfactual reasoning actually
requires no such thing.
What other problem domains seem like they ought to require planning and
counterfactual reasoning, but can probably be conquered with nothing more
advanced than a deep long short-term memory network.
That's a provocative thought-provoking question, right?
Cause like this looks fucking impossible.
Doesn't it?
Oh wait, we did it.
Yeah.
What else looks like that?
Lots of things, right?
Yeah.
Yeah.
It also reminds me, brings me back to various science fiction stories,
including most recently, I think Peter Watts has been doing this that kind of
assert that consciousness is on the net, a drain, and any sufficiently advanced
species will either evolve out of consciousness or remove it on purpose
in order to be more competitive.
This brings to mind like the most ridiculous, but to me, one of the like most
not ridiculous sounding, but also kind of one of the most like it seems startlingly
plausible hypotheses of like why people sleep or why everything sleeps basically.
Cause like in the evolutionary long-term view of organisms and individual bodies,
we're just vehicles for our genes to ride around and reproduce, right?
Well, we only need to be like awake long enough to do that.
And the rest of the time, we should just be doing nothing and staying safe.
To conserve energy.
To conserve energy and to stay safe while we then like go out, reproduce and then,
you know, return to that state.
So like sleeping is more than a natural state there.
And so in that sense, consciousness is like, other than the fact that I guess
sleeping is also beneficial or whatever.
But I thought that was kind of like hauntingly compelling.
Yeah.
And Dan, now he's saying basically the same thing.
It looks like long-term planning and consciousness isn't that important for
solving a lot of things that we used to think it was important for.
And people were making that argument about driving 10 years ago, you know,
and like now we know driving, robot driving is a solved problem.
Or if it's not 100% solved, it's a, no one denies now that, well, I don't
think decides that it's a solvable problem, right?
Yeah, driving is complicated.
There's lots to do, but it turns out it's not that hard.
So, or it is that hard, but we can, we can solve it.
Right.
It's brute force.
Doesn't mean, yeah, it's not intractively hard for a robot to do.
Yeah.
More did a male continues.
And now we have GPT2.
People keep calling it a language model.
It is obviously more than a language model.
It is a concept model.
It clearly has a complex web of understanding of conceptual relationships.
It knows that water is wet, that wet things are slippery, that slippery things
make people fall, et cetera.
It knows, quote unquote, and again, we're using that word, this abstractly and
without the kind of concrete reference that you and I have access to, but I
would argue that it still counts as knowledge and conceptual understanding.
Much like I was saying that I think knowing that a daughter is the way daughter
is related to father is something that is encoded in that web of concepts, even
if it doesn't know what a daughter or a father is in the real world.
Yeah.
See, I guess I'm, my understanding of GPT2 isn't sufficient because I'm, I
didn't spend a lot of time researching it and I get the impression that there's a
lot of groundwork I have to cover to get my head around it, but I was thinking
