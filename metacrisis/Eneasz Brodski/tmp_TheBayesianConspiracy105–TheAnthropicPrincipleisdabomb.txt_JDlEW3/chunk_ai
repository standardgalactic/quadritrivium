And based on stuff, we do have evidence for probabilities that we can calculate.
Like how the, it's a better question of like, you know, how likely was the cold,
how were we to have survived the cold war?
Because there have been other wars and other types of crises that we can look
at, get data from and then actually do statistics on the, those are the relevant
parts of that question, the like, how likely is this specific event in a bunch
of other universes, this exact, like, you know, that it starts to get into like,
this isn't good math, like this, this isn't going to solve the problem.
This will maybe be like really, really super extra meta thing that we can do
thought experiments on.
But the fact is that we survived the cold war.
There were factors involved.
We can look at those factors and, and if we actually care about trying to like
pre-mortem future conflicts, then we should be doing this more than we are, I
think, I think this stretches the usefulness, at least of my capabilities as a
Bayesian, um, like how likely are we to survive, uh, fast AI takeoff?
We have, we have no answer for that, right?
Because we don't have any priors.
That doesn't mean that we shouldn't be careful.
That doesn't mean that we shouldn't make some certain safeguards, but like to
say I have no priors, I can't answer that question.
I could still say it's probably low.
Like, I think there are way more ways to get it wrong than there are to get it
right, and therefore if you're just throwing a dart at random, you're more
likely to miss than you are to get it, than you are to, are to hit it.
Also a small caveat.
There's a George Carlin joke that near misses are misnomers.
They should be called near hits.
It's like, oh, look at that car accident.
They nearly missed, but they didn't.
Um, like a third of his comedy for like 15 years is just word play.
Um, so like, I, I, I struggle with the fact that just because we don't
have priors doesn't mean that we can't make reasonable, um,
no, I definitely think when we're thinking about potential crises that
could happen, that we should look into how likely is this crisis based on
like what other similar sorts of data we can observe.
And then regardless of that, like the fact that this might happen and how bad it
would be if it did happen means that we should try to come up with like strategies
around it.
That's definitely like, I'm not going to say that because the anthropic
principle is not a valid question, then we should just throw out worrying about
AI or anything ever.
I didn't think you were saying that I was more just trying to solve it myself
because like a century ago, there was no concern of nuclear war wiping out the
species, right?
Um, right now, at least as of the time of recording at 428 on Sunday, the 23rd,
there's no chance, let me, let me rephrase that as a five minutes ago,
there was no chance of AI wiping out the planet in the next five minutes.
Um, that doesn't mean that there's a, that it's the kind of thing that we
can't worry about, but it is the kind of thing that I can't put a probability
on going smoothly or poorly.
Um, so like trying to find statistics on how much to be concerned about this.
It doesn't seem valuable.
How many, I guess one of my big questions would be how many crazy
coincidences do you have to see before you start getting worried?
Like how many times would you personally have to come very close to dying before
you start saying, maybe, uh, maybe I should change what I'm doing.
One, because I want to live forever.
You know, I definitely have been more careful about crossing streets since I
was going to buy that car a few years ago.
And that wasn't my first time almost getting hit, but it was the oldest I was,
I think almost getting hit, right?
Like when you're a teenager, you're not worried about that sort of thing.
So like I, you know, if I saw the exact same thing next time I'm on the
intersection downtown, if somebody's leaving something in the street, I'll
let it get run over.
But you turned on the LHC three times or tried to anyway.
And the chances of you getting hit are much higher than the
chance of the LHC not turning on.
How about, uh, you can, you can conceive of events that might be harmful to you.
And you can then think about how bad is it going to be if this happens and
then kind of run, make a decision on what things you need to prioritize
caring about.
Like I look at, okay, what things are likely to kill me and I worry about
car accidents and like the things that are the most common things that kill
people.
I worry about those before I start worrying about getting eaten by a shark
when I swim in the ocean.
And most people are more worried about getting eaten by a shark in the ocean
than they are dying every time they drive because we're bad at statistics.
Um, something like AI takeoff is like, okay, how many times have people
again killed by rogue AI in the past?
Well, not because we don't have any evidence, but how bad would it be if it
happened?
It would be really bad.
So maybe that is worth, you know, escalating up the chain of things to worry
about.
Um, the, a lot of people have died from being hit on the head by coconuts.
It's statistically one of the more likely ways that people die.
Do I have to worry about that?
I mean, there's definitely a bunch of other things that are higher on that
list.
Uh, also, I don't live anywhere near a coconut tree.
So, you know, there's other, there's definitely ways you can think of things
that might be dangerous and might kill you and then try to decide how important
is it that I come up with counter strategies for this thing or worry
about it?
And I don't think the anthropic principle has to come into play.
But what if Google was like turning on a new server farm or something that
had some of their, they were like making alpha Starcraft or something, right?
An AI to play Starcraft really, really well, uh, the newest version, but
every time they tried to turn it on, some really crazy wacky shit would happen
where it didn't turn on.
At some point, would you start to think maybe they shouldn't turn this on
because anthropic principle says it's extremely unlikely this would have
kept happening.
So we must be in some.
Yeah.
Um, in that case, like, if it's useful sometime, if it's something like where
I'm noticing confusion about the, like the universe seems to be behaving in a
way that I wouldn't expect, um, then you start to be like, okay, something is
happening.
I notice I'm confused.
I'm going to generate hypotheses for why this might be and then decide what to
do from there.
So it's once the universe is behaving in ways you wouldn't expect.
Yes.
So in short, once you're surprised by how events are going, yes, but you're not
surprised that the cosmological constants are exactly what they are.
Why do I have a cause to be surprised about that?
I know how.
Because unless there is no other option for them to be what, besides what they
are, then it's extremely likely they would be just what they are.
Intuition about how likely it is for something to delay Google server
fires to like turn on over and over and over.
And like the more like unlikely and the more common that is, the more you can
be like, this is not the way the universe behaves, but I don't know anything
about how often life evolves or something like that.
Or about what states look like.
Yeah.
Yeah.
I mean, we have no idea what the constants can be, but seeing that there's no
reason they couldn't be something else.
Is that not at all anything that influences your thinking?
I know enough about how the universe works to be confused when it's not working.
I don't know anything about how often life evolves or any of the big cosmological
questions.
So I don't think that that I can be surprised or not surprised by it.
I think that's a succinct way of putting it for me too.
Like I don't find the cosmological constants, I don't find the mind tuning
of the universe surprising for some reason.
Because again, we haven't delivered the universe where that didn't happen.
Right.
I guess I'm failing to make the thing click into place if there is one.
I think I'm with right now I'm like, just saying that if I'm trying to
paraphrase just to put this in my own head correctly as well, like I have some
expectation of what it looks like for this machine to keep failing to turn on.
Like because I have a model of the universe where, you know, when you hit
this switch, things happen and if things keep stopping that from happening,
that is surprising because I have already an understanding about those things work.
What I don't have is an understanding of how universes are started or stopped or
whatever.
Right.
Yeah.
How the constants are determined.
Right.
Yeah.
So like one of them, I can't find surprising because I have good reason for priors.
The other one I can find interesting, but I'm able to dismiss it.
Dismiss it anthropically without losing any sleeve because it's like, well, yep,
we live in a universe that allows life.
Yeah.
Um, I don't like, do I find that answer satisfying?
I'd be another kind of question.
Like, do I find it satisfying that?
Well, we live in a universe where that's the case.
I find not knowing things unsatisfying and I want to know more about them.
I mean, I don't find it all surprising that we live in a universe where that's
the case because obviously topologically we could only live in a universe that
supports us.
Tautologies are tight.
The question is, is there infinite universes with infinite constants or is
there only one where that just happened to have these constants?
So that, if that's the question, um, I mean, that's basically the
anthropic principle question, right?
Cause if there's infinite universes, then the anthropic principle is in effect.
There's almost infinite universes where you are dead now.
I think maybe we have no way to go about starting to find
out the answer.
So right now I can be like, huh, that's a question.
Yeah.
Right.
But what do you think is more likely that there's infinite universes?
I don't know.
I literally can't tell you which thing I think is more likely
because I have no evidence.
I think multiverse theory is more likely because a lot of other smart
people think it's likely for other, for also other reasons.
And so it sounds like the scientific consensus is either already or is
moving towards a direction where multiverse theory will be the predominant
view of the multi cosmos.
And I also think that's the most logical explanation because for
there to be only one universe seems absolutely ridiculous to me.
It would be weird.
Yes.
It would be, it would seem counterintuitive, maybe.
But if there's only one, but if there is an infinite number of universes
and we're in just one of them, then the anthropic principle comes into
effect in everything you do.
You're running off of the anthropic thing of like, or the, I don't know,
