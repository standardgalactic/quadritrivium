Maybe cause the only time I've seen him talk about it, he sort of stopped the, he's like
kind of stopped the conversation at that point.
But if he, if he had said what you, what the next thing that came out of your mouth afterwards,
which is what's going on there, that's really weird.
Let's figure out what's going on there.
Um, I think I might have come to that way.
I don't know why I took it the other way, but I think your interpretation is probably
more, more likely.
Well, I think.
Any beat cop like knows how probability of evidence works, you know, that's, that's
what I don't know, maybe not a beat cop, but a detective at any rate, uh, knows how
to weigh.
Well, this means that this likely happened and this is just beyond the pale.
If they see a bullet casing on the ground and they see a guy with a bullet hole in his
chest, they assume that the casing probably came from the bullet that shot the dude.
Well, it's not like it's, um, it's not like they have to work to compartmentalize.
No, that is the natural, natural order of things is what you learn in one field sticks
to that field and it's difficult for people to generalize, generalize knowledge and learning.
And I believe that actually is Eleizer's point that the sanity waterline, that the
general level of people applying their skills to everything in life is so low that these
ridiculous beliefs pop up even among people who shouldn't have these beliefs.
And so it should be in theory fairly easy to raise the general sanity level in the population
if we can just get people to, to, you know, apply these tools to all their thinking.
It'll be so easy if we can just do this incredibly difficult thing.
I think that that's, that's what I was going to say too.
So that does sound like it would solve or go a long way towards solving some of those,
the society wide rationality issues.
But that's a rather hard task.
There was a great anecdote.
There was a talk at the 2014 amazing meeting.
It was a panel discussion with Julia Galef, Dan Dennett and two other people who I'm not
familiar with, but we'll link to the talk in the description for this episode and on
the website.
And they discussed this very issue, can you teach rationality and like most 45 minute
panel discussions, they don't come to a consensus.
But one of the panelists, I forget her last name, first name was Barbara.
She talked about how she taught probability theory to a lot of her students and they could,
they nailed it, you know, you talking about drawing marbles out of urns and that they
had it just fine.
And I'm not sure she didn't sound like she was making up the, she didn't sound like
she was making this a hyperbole, but she said the second that she switched the examples
to M&Ms, all their skills fell apart.
Oh no.
And so if it's that hard to generalize colored balls from M&Ms, I think generalizing standard
rational practices to everyday life might be a very arduous task indeed.
I think that's kind of what this episode is about because you guys are saying like it's
the most impossible thing, but how did we come to this, you know, realization and get
these skills?
I read a series of blog posts on the internet.
It's a good thing we're perfectly sane.
No, I'm saying if all I had to do was read some blogs by a guy who could write entertainingly,
it shouldn't be that hard to impart, right?
Did that cause you to lose your religion?
No, I'd lost my religion long before that.
What happened?
So what happened when you read those blog posts?
What did you generalize that to?
That's what I want to know, Imyash.
What do you mean?
What did I generalize it to?
So we're talking about, you read those blog posts, you're like, sounds good to me.
Right.
Now, how did it impact your life and those other compartments that we're talking about?
How do you use it in work?
How do you use it, yeah, just on the day to day, how do you use it when you're doing
physics?
You know, I don't actually use it at work because my work is fairly simple.
I'm an accountant.
It's all, you know, basic math stuff.
So I don't use it at work at all.
I use it more in my day to day life and in how I think about things and assign probabilities
to beliefs.
Like I think one of the major things before was I grew up in a very fundamentalist religion
and things were generally very black and white.
This is good.
This is bad.
There's no in between.
This is true.
This is false.
You know, God created the earth.
But here's the funny thing about Jehovah's Witnesses.
They're creationists, but they're not young earth, right?
So God created everything exactly how it is, but he did it billions of years in the past.
So what's their version of history?
Well, you know, when creatures enter the evolutionary record, that's not when they evolved into
that form.
That's when God decided it's about time to make some lizards.
And so he made some lizards then.
I can totally see that.
That sounds...
And after all, he's like, this planet's kind of born without humans.
Let's make humans now.
And that's when humans showed up in the fossil record.
I'm going to say that sounds distinctly less probable than just standard young earth creationism.
Really?
Because saying...
They don't have to fight against the carbon dating people.
Exactly.
They don't have to fight against the fossil record.
Right.
They have to specifically fight against the fossil record because you get transitionary
species and you get species and different species.
There are no transitionary species that allow me to correct you before you go any further.
Oh, that's right.
Whenever you find...
Whenever you fill a gap, you just make two more smaller gaps.
Right.
Yeah.
No, so the interesting religion has, I guess, not that much to do with what I was saying.
But before, I was thinking very much black and white, truth or false.
And once I lost my religion and started thinking more empirically, and I guess more just general
liberally, I guess would be the term, because you associate liberal-liberalness with this
sort of wishy-washy gray area, and I think it's quite certain.
And I didn't know how to handle that.
So I was just like, I guess anything is possible, and I guess everything is equally valid.
And so it was just basically everything, just this one shade of gray, and you can't really
make any judgments or decisions.
And then stumbling across this, the Bayesian thinking, where you can actually put different
probabilities on different things and adjust your probabilities as you encounter more evidence,
I was like, oh, so I can say with very high certainty that God doesn't exist without having
to say things like there's absolutely no life in the universe or whatever.
I guess that has nothing to do with God.
I don't have to say there's absolutely no chance Jesus as a historical figure didn't
exist in Judea around 30 BC.
So and a lot of people have to seem to be like either I have to fight for there was a historical
Jesus or have to fight against there was a historical Jesus.
I can be like, you know, there's a decent chance that there was some rabbi that went
by not the name Jesus, but was it Yeshua or something that went by that name and eventually
his legend was transfigured and there were a lot of messiahs around that time and I'm
sure they all so you can have these probabilities in your head and you don't have to be yes
or no for one or the other and it really makes life a lot easier to approach when you you
admit that your view is uncertain, but you get it as certain as you can.
What you what you just we're talking about makes me think of two points.
One is that we do all have examples in our lives of times that we transfer knowledge
or what we've learned about rationality or even another subject, maybe statistics, maybe
something else where we transfer that to something else in our lives to our benefit,
I guess, but I think that what we're lacking is general transference, general and measurable
transference that's not, you know, largely anecdotal, but that's a skill that can be
taught if you keep trying or can be improved anyway, if you keep trying at it, right?
I want to say too that that's what's evidence is a really good question because it's I think
it's really hard to tell how good someone is, how good they're getting slash how much
better they are than they were when it comes to that sort of skill like it for myself.
I think I said last episode was I try to say I tried to not because I want to say it as
often as it's needed, but I want to not need to say, man, I was stupid, you know, every
other day. So but I wanted to say about what you said in Yash, the switch from binary to
Bayesian thinking and that was what was really interesting. I'm not sure when that came to
me before or after last wrong or before, I guess, or during, but I'm not sure maybe we're all
told everything's binary or we're kind of preprogrammed that way, that things, you know,
that's either true or it's not makes it easier. It does. But yeah, but I guess it makes it much
easier to form into coalitions. If you can say, you know, these people are for us, these people
are against us, or these people are for this idea or against this idea. And since so much of our
history is about forming coalitions, I think that that's probably at least part of the right
explanation. You know, when I went into college and I took a chemistry course, and everything I
learned was different from high school chemistry, which before I thought was the absolute real
thing. I guess that really threw me for a loop. And everything turned into more shades of gray at
that point. But when I learned about lesser wrong and rationality, it was mostly pretty much what
I already knew intuitively. And I credited that with growing up always wanting to be a scientist.
And, you know, learning science and being immersed in biology, and kind of learning what we don't
know and what the holes are about our world and what we assume. We assume so much that we just
take as knowledge, but nobody's ever actually looked into it. So everything just fell in together
really easily for me. And the example that I told you guys before we started recording was my husband
Tim, he grew up as a young earth creationist. And the rest of his family is still young earth, 8,000
years old, you know, creationist. It's six to eight.
Oh, okay.
It's something they have some room in there.
It's like October 24.
Well, there's no there's no you asked, but there was a guy, yeah,
well, there's an established year for a lot of these people, some 6700 years,
I don't believe they had an established year. But he said that, you know, he got away from
young earth when he took a geology course. And he's like, Oh, well, guess that those two things
don't square up. And he dropped God, he dropped theism after he took some statistics.
Why statistics?
Because he started to think in terms of probability, I think. We'd have to ask him directly.
I can imagine one avenue would be, how do I know which, you know, there are so many gods, what
would make me think this one's the right one? You know, my odds are like one in a thousand.
That's, I mean, I find that convincing. I don't, I don't know if that's the way that he went.
I wanted to say really quick, that it's interesting, when you mentioned that Tim took a geology
class and said, wow, my religion's false, maybe not, you know, after day one or all at once.
But he started to question and it fell apart for him. I'm wondering what discerns people like Tim
from people like I met in college, and I met people like Tim as well, but I met people who
go to college. That's probably some of their first time coming into some of these ideas,
because they're you know, homeschooled or private schooled. And evolution loses out, geology loses
out. It's doubled down. Well, so yeah, but that's, so that's the thing is that it doesn't seem like
enough just to give them the information. Tim had some sort of a disposition that allowed him to
to update the direction. He's a very intelligent person on his, he wrote a little blog about
the transfer of learning. And I'm just trying, I'm trying a couple of the things that he suggested,
which I don't know that I completely agree with this, but he suggested that intelligent people
make connections better. Right. I know so that and it's possible that when we as relatively
intelligent people think about all the connections that we've made and all of the major changes in
opinion that we've come to in our lives, then based on, you know, our education and rationality
or science or whatever, then maybe we're kind of able to do that and not everyone is. I don't
think I agree with him on that necessarily. I think, again, that people are are able to update
all the time and that it's how strong being in a it's being in a I think it's probably being in
a situation where you have the motivation to update that makes the difference rather than the
intellect to update or the intellect to to do that knowledge transfer draw. So I am saying that
based on not much. Although I believe that they're probably, I mean, I'm sure there is research on
knowledge transference and learning transference and motivation. It's just, I don't know of it.
Everything is incentive gradients, right? You need some incentive to do this.
And I was nodding when you were illustrating that example about needing an incentive to change
and or a motivation. And I had a couple thoughts. Well, you can be highly intelligent and still be
