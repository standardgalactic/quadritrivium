And we're always arguing about a thing that could change either of our minds.
And this is often a very productive way of having disagreements.
And she says something like double crux is a great, but I'm looking for here is something like an aesthetic double crux, where people can try and like share parts of their aesthetics and share what would change how they feel about those aesthetics and try and have that sort of disagreement and sort of elicit, you know, some of the nameless things that are affecting our decisions and affecting our beliefs.
Now, is it possible to change your aesthetic through argumentation?
I actually recall, I'm pretty sure it's not a proper full story.
I'm pretty sure it's the story of, I think it was two Seafar staff members, one of them who likes eating onions, and one of them who did not like eating onions.
And, you know, I'm on the second side.
And the one who did not.
Same, onion here's unite.
Yeah, love onions and they're good for you.
And, and so they tried, you know, they said, what do you like about eating onions.
And they tried to, I wasn't present for the conversation, but I imagine they talked something about the taste or something about the like, feeling ahead in the math and encourage the other one to try taking a bite and paying attention to those things.
I think overall the other person, I'm going to use the phrase learned to enjoy eating onions, or felt that they could appreciate why the person would enjoy eating onions and could imagine themselves becoming that sort of person.
In a way that they did not expect going in that they would be able to get out of such conversation.
And I think this is a style of conversation I've tried to have more and more, where someone like doesn't like a decision or acts in a certain way or something.
And I'm trying to say what feels salient to you about this and communicate both there and my sort of aesthetic toward the thing or feelings toward the thing.
It's really cool. I'm going to have to read that.
It's relevant to some stuff that my partner and I have been talking about where, and also that like Guild of Servants just did their aesthetics class.
And, or it was like, just about how to cultivate a sense of style, like fashion style, or just like, I guess the accessories you surround yourself with, et cetera.
Can you tell me, Jase, what the Guild of Servants is?
Can I?
Apparently it's a secret society. I don't know if I'm allowed.
I do have to say I'm talking of secret societies.
I've been in the little restaurant community for almost a decade now, and I've been waiting for the sort of owl or some form of male to invite me to the Beijing conspiracy.
And here you are.
And so if you would like, you can just turn all of your questions into probabilistic estimates and I will answer them.
If at any point you would like me to give a probabilistic estimate, I am ready and I'm willing.
Excellent. Yeah, the Guild of Servants was described in episode 117 when two of the founders, David and Alex came on.
I think in a couple of sentences, the goal is to curate actual life betterment skills that actually work, which for me, I kind of encompass everything that actually works into the umbrella of rationality.
In addition to, in a broader way, solving the PR problem that rationalists occasionally run into.
I wouldn't say that's even a primary goal, but the one long-term goal is to have large community impacts where the people who get to claim responsibility say, yes, we're with the rationalist community via the Guild of Servants.
Nice.
Interesting.
Nice.
But if you're interested in more, like I said, check out episode 117 of our podcast, which was just like, I guess bi-weekly, it would have been sometime this summer.
Alright, listen to it.
So Ben, did you have a post you really liked?
Oh, alright. I think I really love the Babylon prune stuff.
So it's three posts that come from a sequence called Babylon prune by Alkeja.
She's a great writer on the site.
In fact, just two weeks ago, I wrote some of the best stuff this year.
The Babylon prune is just again, another very simple model of how his thinking works, which is that, you know, similar to like brainstorming and then like picking the best ideas.
It's like coming up with quite a lot of like strings or like ideas or sentences or something and then just picking the best ones to say or something.
You can see the similarity in Scrabble where like people try and like start with smaller like sub sentences, sub words from what they've got and build it up and open.
And he felt like a lot of his rationalist training had been like on his prune skills of being able to go, this idea isn't good enough.
This idea isn't good enough.
This idea isn't good enough.
And you know, he like listened to people say such eloquent and interesting things.
He's like, oh, I understand those.
Why don't I say interesting and eloquent things?
But it's sort of internal setup just kept whatever he thought to say, it would just go not good enough, not good enough.
And then he just sort of not be able to say anything.
And so it's given me a sort of tools about practicing having good filters, having good generative and filtering processes.
It's in fact led to quite a lot of practical use.
In fact, just recently, Jacob Lagras, who has been collaborating with Lessrong and is the primary other person who is responsible for this book set being built,
did a great babble challenge on Lessrong where for once a week, for seven weeks, he would say, OK, here's what you're going to do.
You're going to babble 50 solutions to the problem I present you.
And he had great problems like how to light a candle.
How to light a candle was the last one.
You need to give me 50 solutions to how to light a candle or how to get to the moon or how to hide a notebook so that no one will find it.
Or I think how to hide a pen that it will only be found by a specific person like 100 years later or something.
And it was great, a bunch of Lessrongers who I think have never written comments before went, I think I can do this.
And they came up with such inventive and creative explanations or solutions to the thing.
And obviously a ton of insane ones.
And this is now just like a technique that like me and Jacob Van Olli and people use around like work in the office and Slack where they're like, oh, we have a problem.
All right, here's 10 solutions.
Just I will babble 10 solutions, which I think has been previously something we've been kind of scared to like say a lot of bad ideas or something.
But I think it's been a good kind of practice.
So anyway, yeah, I'll just write some really product stuff about different filters he finds in his own mind and different levels of prune and different gates and ways of increasing your babble and ways of
proving your prune and this sort of concept.
But yeah, I know it's permeated my thinking a lot over the last three years.
I think prune is kind of self-explanatory.
But by babble, does that mean just generating lots of options and not like without self criticism?
Yeah, I think he's opening example is I think how babies learn to speak, which is not that they like imitate as much as they just produce a lot of noise.
And they notice what sort of gets positive feedback or something.
The interesting thing about babies that is relevant to the Babylon prune analogy here is that babies can produce all phonemes that are present in modern languages.
They just like babies are better at pronouncing the th in English and on and on.
And then as they learn the language of the world around them, they lose the ability to pronounce certain phonemes and prune down the set of phonemes to the set of things that are actually being used.
And they generally produce just a lot of noise and then prune it down to the subset of noises that are correlated with a certain language.
And that's like a very interesting way in which like you can imagine getting that language where you can imagine doing the opposite thing where babies don't know how to produce any phonemes and then learn
iteratively at it at once that belong to a certain language that they're surrounded by.
That's fascinating. I now kind of wish we had a language that just used all the phonemes so that we wouldn't lose any of them.
Someone who's been struggling for like six years with this horrible th in this language.
I would also be glad about this.
German doesn't have the th sound then, huh?
It does not. We only have the rolling R, which I now have taken myself to repeatedly using in lots of words and those various expressions of emotions in order to
slightly compensate for my inferiority of not being able to pronounce all of your phonemes by demonstrating that I pronounce phonemes that you cannot pronounce.
I'm kind of bummed you didn't roll one R when explaining that.
Yeah, I enjoyed it really hard, but it was hard to find one.
Americans do have rolling R beautiful.
He just makes that noise when he goes into rooms now, by the way.
He just does it.
People have different catchphrases and nullies is just
Yeah, I was about to say it sounds like the noise you make when you want to get pets.
Yeah, yeah, he is the rightest girlfriend the most.
But yeah, I think there's a bunch of other essays in the book.
One of the one I would want to mention because of how much I like it is an untrollable mathematician, which is a cartoon.
In fact, yeah, I guess the curiosity book as a whole is it's a curious book.
I can't believe I just said that, but it is about it's not about curiosity.
Like curiosity, we care about a lot because as a motive is very pure and true seeking and like doesn't.
And it's like, there's that great earlier as a line, a burning itch to know is higher than a solemn vow of caring for students or to pursue truth.
And so I just love people getting real curious on this one just about sort of any subjects and they often find very fascinating things and they do great reasoning.
And so it's just a collection of essays of people getting curious about some aspect of the world.
The essay has got a title, which is a question like Scott Alexander's got is science slowing down.
And my ancestry has got what motivated rescues during the Holocaust.
He was trying to understand the sort of motives of the people that like did various sort of heroic things.
And Abrams is Abram Demsky is the author of an untrollable mathematician who's a researcher at Mary.
He was trying to get curious. He was curious about this a complicated relationship between probability theory and logic.
They have a weird probability theory.
Classically assumes a certain amount.
Sorry, assumes logical amniscience.
It assumes, you know, the consequences of your beliefs such that you can just do a simple update of like, oh, I have now seen this evidence.
Therefore, I can say that these beliefs are false and these beliefs are true.
And I can rebalance those probabilities.
Whereas, for my logic, you never know all the consequences of your beliefs like as a bounded rational agent in like with limited compute and limited time.
I cannot compute every logical implication of everything that I know.
This would just require a lot more compute and time that I have access to.
And one thing something girdle.
And so there's a setup where you're trying to like weigh these things where each one of them is like, yeah, actually has a very beautiful diagram.
I think of some scales trying to weigh probabilities.
But each probability is a plant that is growing like quickly.
And you're like, I don't know which of these plants is going to go quicker.
I don't know which way it's going to finally end up.
And one of the issues he is noticed is that in principle, I can start telling you if I went to troll you and you were like this mathematician trying to dutifully combined logic.
And probability theory, I could keep telling you consequences of your beliefs, logical implications of your beliefs that weigh on one side of these scales.
And I could repeatedly do that to make you balance these scales at in principle, any balancing I like.
I could keep telling you facts that are implications of what you believe or logical facts where you go, oh, that's a good point.
That makes me think slightly more likely that this statement is false.
And I can end up in principle getting you anywhere on this set of weighing scales.
And so his question is, is this necessary?
Can I come up with a way of thinking of combining logical probability that doesn't have this trollable nature?
I won't necessarily tell you the inclusion.
Maybe I will.
But it's just a fun cartoon that ends up very conversationally and with beautiful sort of diagrams.
Just telling you these little interesting things about logic and how it works and how probability works and how some of him and his friends got curious about it and how they were on a train together talking about it.
And so it's been one of my favorite posts that came out in 2018.
And I really, I also just like the idea of someone reading the Curiosity book going, oh, I think I know what's happening to me in this book.
We had his science slowing down.
We had what motivates rescues during the Holocaust.
These are people doing some like research and writing it up.
And then you turn the page and it's just a cartoon chapter headings anymore or something.
You're just in the middle of a hand drawn cartoon that's just chatting about girl serums and so on.
So yeah, it's one of my favorite essays in the book.
I really like it.
Awesome.
Yeah, I also really like it.
In some sense, I want to bring this up point because we hear the Bayesian conspiracy.
I think the biggest change and the most interesting thing that from my perspective has happened in kind of the development of theoretical rationality in the last two
Watch what you say here, Oliver.
This is the character.
The fact that I think just like I moved away and I think a lot of other people I respect have moved away quite substantially from the classical Bayesian model of rationality.
Because like actually just like the concerns around being a bounded agent just actually have all of these these very substantial obstacles that make reasoning like that, that make it very hard to even think about what it would mean to reason like a Bayesian.
The Bayesian studies really, really weird things where I think think Abram said it really nicely.
I don't know if it was controllable and additional and one of his other comics where he said that the fact about a Bayesian is that a Bayesian must have the ability to represent the complete territory as a like within their brain as a single hypothesis.
And like, in order to just represent a single hypothesis about the universe, you need to be able to have a hypothesis in your brain or in your representation in your model of the world.
That is as complicated as to fall universe.
And that itself is like very weird statement to make about any object.
It like, well, how would they even make that statement about something that is embedded in the universe?
Does it have a perfect self model?
Then you end up with all of the kind of classical problems with like, well, how can you have a perfect self model?
Can you predict what you will do in the future?
What does it even mean to make a choice in that context?
And all kinds of other weird logical confusions that happen.
And so just, just in order to make a slight jab at the Bayesian conspiracy, in some sense, I have substantially moved away from from thinking of the art of rationality as being the art of becoming a better Bayesian and think that now there is a much, much more richer and more complicated art here that we have started to discover in a lot of the writing that is kind of exemplified by Abram's controllable mathematician and embedded agency and other things in that space.
Are you one of these post-rationalists that is trying to schism us?
No, no, no, please.
I saw someone on Twitter the other day say, the dirty secret of all the post-rationalists is that they still read less wrong.
No, the thing that has made me excited about this is that we have found traction in a theoretical and grounded extension of our art of rationality into this domain of bounded computation.
I think I felt really stuck for many years between like 2013 and 2016 or 17, where I kind of knew that, well, okay, but I'm not a perfect Bayesian.
But I didn't really have any traction.
I didn't know what alternative model I would even start to consider.
And it's not like we're that much proper ahead.
But over the last three years, I feel like I've really gotten some traction and what, how I can formally think and how I can rigorously think about a system that is computationally limited and tries to, is not logically omniscient.
And I think that kind of, if I, I don't want to really bash on the post-rationalists, but I think that kind of makes me think that I wouldn't necessarily describe myself as being in a post-rationalist tradition,
because I'm still interested in a grounded kind of formal rigorous handling of the relevant philosophical and epistemological puzzles.
And I think we have successfully broken into that space.
And now I feel like there's this richness of new, interesting things to do in that space that feel very much in the realm of building another probability theory, as opposed to deeply meditating and uncovering the real truth that was inside of my soul all along, or something, something,
it's a strong and a post-rationalist.
I, I am, I mean, you already had me sold on this book series, I was planning to buy it an hour ago, but I am now like even more interested and slightly worried about some intellectual contagion that may take me over, but I'm going to read this anyway and take that risk.
Change the name from Bayesian conspiracy to embedded agency conspiracy, then I will, I will take credit.
It's still an awkward name.
It is really not a good name. Don't do that. Would be horrible for your brand name.
Yeah, we just, we just launched some merchandise. We can't go changing the name now.
Right.
Although the merch doesn't actually say that Bayesian conspiracy does it.
No, it's just the art.
You already, you can come ahead. Ready to change your mind.
Yeah, I think the relevant parts of this, I don't expect we'll, my epistemic state is not that we will like, we will not look back on Bayesianism.
We will not be like, ah, that was mistaken.
We will, there are some parts of the formalism that I think do not apply to agents that are bounded.
I think these confusions are best pulled out in another one of Abrams essays, which he co-wrote with Scott Garabrent in the, which is partly in the book, but it was in fact itself the length of, I think, two books.
We didn't include the whole thing, which is his embedded agency sequence where he just starts, and I think it's in a very fine manesque way.
You know, this is, this is quite complex and philosophical and mathematical questions.
But he just sort of draws a couple of cartoons and diagrams and you get a pic.
You go, oh, yeah, I think I get what he's talking about where he points to the sorts of confusions we have about what it means to be a rational agent.
Some places where our theory does not fully explain or fully account for some of the actual problems we face.
I think a lot of the classical ways people have used certain parts of Bayesian probability theory.
His fundamental contrast he makes in the book is between an agent that is playing something like a video game where the agent is not part of the video game, they're outside the video game.
The things that happen in the video game cannot affect the agent's mind and the agent in principle could understand everything that's going on in the video game within its own mind.
If you imagine just the relative like where the agent is in fact computationally got way more confused so they can think about all the possible states of the video game.
