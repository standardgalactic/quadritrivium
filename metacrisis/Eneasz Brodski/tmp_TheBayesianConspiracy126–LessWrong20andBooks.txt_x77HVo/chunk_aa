Welcome to the Basin Conspiracy. I'm Minjash Brodsky. I'm Stephen Zuber. I'm Jay Sticky.
Hello, I'm Oliver Hebraker. Currently, I run the team behind LessWrong and LessWrong 2.0.
And I'm Ben, and I also work at the LessWrong team. Oh, Ben Pace is my full name.
Welcome, guys. And I recognize both your names. And like, I see your names often enough that I kind of think you guys were there from the 2007 beginning period.
But then like, I also realize how long it's been, and maybe it's just been more activity lately. How long have you guys been at LessWrong?
I think I discovered LessWrong when I was about 13, 14, which is now about 10 years ago. So I discovered it about 2010, 2011.
And the sequences have been finished. I remember being very excited because Alieza came back to write highly advanced epistemology 101 for beginners.
And I was there for the first time while it was being written. And then, yeah, I think I followed LessWrong.
I think I've just, I initially found an introduction to intuitive explanation of Bayesian reasoning by Alieza.
A friend showed up at me at school, and then I think I just read the sequences into the night for like months.
It was like, until like 3am every morning going, wow, this stuff's great.
And then, yeah, I think I applied to go to CIFAR when I was like 16. And they were like, yeah, that sounds good.
And then we couldn't organize it for like three years. And then finally, when I was 18, I think I pretended to be ill for like prom and graduation so I could fly out to San Francisco to go to my CIFAR workshop.
Priorities, I love it.
Yeah, yeah. And then, yeah, I just been sort of following the community and part of it since then.
Awesome.
I do want to at some point get someone from CIFAR on here to talk about it. Did you find it to be a valuable experience?
Oh, yeah. It was quite life changing. I remember, I think it was just my first evening there.
I had a conversation with someone where they were like, I was like, I'm trying to figure out what to do with my life and I didn't even know how to think about that.
And they were like, I think you should stay here in the Bay for another month. I was like, ah, but I have like a flight away on like Tuesday after this workshop or something.
And he said, have you asked yourself the question, if I made it my goal to stay here in the Bay for longer, could I achieve?
And I said, no, I have not asked myself this question. And then I stopped for about two minutes.
And I thought about it. And then I turned back and I said, huh, I think I could achieve that goal.
And then I did. And I, yeah, I think generally, like, it was a great place for like reflection on what I cared about and what my goals were and how to act in the world.
Oli, how was it for you?
Yeah, I mean, I guess, let me briefly answer your initial question, which is I've been also around since I think 2000.
I've been around since I think the 60th chapter of H.P.M. Wall came out. I think that's when I got. So that must have been around 2011, 2012.
That's my rough guess.
That's right.
So yeah, I'm sadly not one of the very old timers that has seen the original writing of the sequences, but been around for quite a while.
It does seem harder for me to have noticed it even earlier. I think it was also like 14, 15 or 16 around the time.
Yeah. And then, yeah, for me to story, like if I continue with Ben's strength of giving a quick rundown of kind of how I showed up here,
I ended up kind of finishing all the finished H.P.M. Wall, finished the sequences.
And then I sent Miri at the time and email being like, Hey, how can I help?
And then Miri was like, Well, I don't know, you can read all of our papers that we've released.
And then I responded and was like, like a week later, I had actually read like a large fraction of the papers they sent me, which wasn't the most wrong stuff, but it was technical stuff.
And I didn't understand everything, but instead like a pretty decent fraction.
And then I think Marla responded with like how I've told us to like 20 people, but nobody had ever actually done this.
I usually do this as like a method of like somehow not having to answer all of these questions.
And then we talked a bit more back and forth and then I ended up getting off at Intenship at Miri, I think a year later.
And then I was, I was living in Jimmy at the time, but then I flew over to the Bay and then turned out I wasn't actually that useful for me.
So I mostly ended up interning at CIFAR.
And so my first CIFAR workshop was actually done while I was interning at CIFAR, which was a very interesting experience.
Oh, excellent. Are you still working with CIFAR?
So technically what's wrong is part of CIFAR, it's definitely like fiscal sponsorship, like we don't have any like shared decision making structure or anything like this.
But we are like directly below CIFAR's office, like we render our office space from CIFAR.
And so definitely still collaborating with them quite a bit.
And of course fiscal sponsorship means today is like some obligation for them to check what we do in order to make sure that it fits with their share of objectives and stuff.
Ali actually had a especially fun time because he, he's German and I think the sequences was perhaps one of the first major works he read in English.
You just think certain words are in the dictionary that aren't in the dictionary.
So that's like, I like learned English like while reading.
I was watching Dr. Who.
I was watching Dr. Who.
My two sources of learning English was watching Dr. Who with subtitles and reading the sequences, which gives you a very weird set of vocabulary letters.
And so I was like, I had like, I was like fluent in English.
I was speaking English with many of my friends and I could talk to you completely fluently about stuff like quantum mechanics because that's one of the things that I had learned in English.
But at some point somebody else asked, like, can you get a broom?
And I was like, what is a broom?
That's standing.
What, do you remember any of the words that you thought were in the dictionary that actually aren't?
Well, I had all of these, like, there's definitely this thing where just like, I had this, this.
Sometimes it gets like some words, some somewhat like idiosyncratic meaning and I just totally thought that that was like the standard meaning.
So I think like, what was the biggest one here?
My brain is blanking on rationalist jargon for some reason.
I definitely got, I definitely wasn't very good at using semicolons, but that's mostly because Elisa does not know how to use semicolons.
Nobody in America knows how to use semicolons.
They are a lost art and sort of arcane and weird.
So did you get, I'm assuming there's a lot of cultural overlap between the US and Germany.
Did you understand references to like Star Wars and English pop culture?
Yeah, totally.
Like Germany, like, just like history wise, Germany is just like really, really deeply entangled with the US, even more than most other European countries.
Just because like Germany was like, like West Germany was just like occupied by the US for like, I mean up until like 30 years ago, like basically.
And so just the influence there is really massive.
I really love the fact that you were the only one who read all their papers.
Hopefully somebody came off to me.
I have some hopes, but yeah.
It just, it feels like that should be in one of those samurai origin story things.
It's almost the timing that people are like, well, I want to do this, but I don't want to like have to work hard.
And they just didn't read the, you know, do the back reading.
That's awesome.
Well, wait a rock.
You said you taught a Seafar workshop.
What's your focus?
So, sorry.
I don't know.
I don't know how to answer that question.
I think I guess I don't really have focus.
I guess if I were to take your class, what would I take away from it?
So the current Seafar curriculum is like pretty like the classes that I taught while I was like active as a Seafar instructor.
I think my favorite class was just like trigger action plans and I think Fermi estimates.
So both of those were kind of like the things that I like to like to teach.
Trigger action plans is really pretty straightforward.
It's just the way I like to phrase it is like there's a way to model a brain as just like a very large pile of statements.
And it's sometimes a very useful abstraction to like apply to like how your system one works.
We're just like, you can be like, like it's kind of in psychology, like in classical cognitive psychology is usually referred to as implementation intentions and something else.
It's like the basic idea where like in order to like reliably build habits, the basic thing that you want to do is you want to like concretely visualize a very specific trigger in your life.
Like when I like, like you want to reliably floss your teeth.
So you want to make it so that like when you decide that you want to floss your teeth, you're like very specifically and concretely visualize your bathroom and you visualize like the part of your bathroom where your floss is stored.
And you want to be like, this is the trigger that I now visualize very concretely.
And then I want to decide on a very, very precise specific action that I want to take.
And then in a class actually very frequently, we encourage people to take some trigger that they can use and then repeat it just physically right there a few times.
So you might go to the bathroom and literally just like stand in front of the mirror, open the drawer and grab the floss and then put it back kind of in order to get that trigger right.
And there's like the part that's kind of like more the self-helpy part where like, okay, cool, that's a neat trick that I can get you to learn habits.
But the part that I really like about it was this feeling of, I think when I first learned about it, it was this first kind of universal concrete cognitive model of how a brain works.
Where like, you know, I didn't really ask myself the question of how brain work, like how brains work that much before I kind of like encamp it that stuff.
And very frequently when we teach at Cifa workshops, it's kind of like, you can be like, well, he is one way to model your brain.
It's not the ultimate way. It's not the only way, but you can model yourself as being a large pile of if statements.
And that for the first time gives people something to like slot into their like, kind of section of them with the world of how brains work.
And that then allows really interesting other conversations to happen.
Would it be like, oh, I can now actually talk about like how people reason in general, and that opens up kind of everything up to the broader like, out of rationality and like, how does thinking work?
And why do I believe what I do believe?
So that's kind of really why I like that class because it combines this really, really practical benefit with this very universal lesson about rationality that I've seen have a really big effect on lots of people.
That's awesome. Man, I've never like, I've had this vague inclination like, yeah, maybe one day I'll take a Cifa workshop.
I don't know exactly what it was about what you said, but I've never had been more strongly inclined to actually start looking into sessions, maybe in the next couple of years when the plague is over.
I would love to. That sounds awesome. Yeah, all I could think of is like, I want to subscribe to your newsletter.
Well, I do run a newsletter. It's called less wrong.com.
Oh, great.
Curated emails, two to three times a week.
Oh, I actually didn't know about that feature.
Yep. Yeah, if you sign up, there's a small checkbox that says if you create an account, you can subscribe to created emails.
And that's actually how a pretty large fraction of people interface with less wrong.
Yeah, I think one of the things we've got a box, probably if you have an old account, we like didn't want to like suddenly take all the accounts from like, like there's like 50,000 accounts that were like registered before we created let's run 2.0.
And I didn't want to suddenly stop blasting all of them emails three times a week.
That's valid.
But yeah, I think created emails are like part of our, I feel like I don't know if I'm using the term technically correctly, but something like positive selection feels like an important part of intellectual progress where we're not just like weeding out the like bad posts, but we're like,
regularly saying this was great. Well done. This was people should read this sort of stuff.
And that's the wider audience.
The rationalist newsletter is just called that.
I'm less wrong. You sent it for created emails. I don't know that we have a thing called the rationalist newsletter.
Okay, maybe I'm thinking of something else.
Someone else does the rationalist newsletter.
Okay.
Yeah, but yeah, I think, I think when we initially we're getting started working on less wrong. We did a bunch of user interviews with people.
And they told us about like the first couple of times.
Like I specifically record Kai Satala, who's a great less wrong and has written great sequences about psychology and emotions in the brain and multi agent models of his own brain.
And yeah, and he gave a story about, I think his first time he wrote a post on that strong. He's very nervous. He'd been reading for like a couple of years.
And he just got a little comment from Ellie as I go, Hey, this was a great post.
A bird.
And he's like, Whoa, this was so much better than I expected the response. I expected some sort of like critical explanation of like why I was mistaken.
And so I think we took that to heart and we just like three times a week now we write on someone's post. This post was great. Let me tell you some reasons why.
And then like mail it out to a couple of thousand people.
So yeah, I think we generally, and over even over the years I've like updated more towards this model. I think there was a point.
I wrote a post about it about, I think two years ago, where Ollie and I looked into some of the scientific journals from like the twenties and thirties to try and figure out what they were doing.
And specifically, I think we've looked over all the papers in the journal where Turing first published Turing computers and Turing machines, his paper, when he was I think just either an undergrad or a PhD and also into a journal I think Einstein was
Yeah, my German came came in handy during that time.
The Einstein was curating or something like this.
And I remember being surprised, like I quite actively shocked that through the entire journal, through every page of every paper, there wasn't a single sentence of criticism.
There wasn't this person's wrong and let me show you every single way in which they're wrong.
They just were like, here are some papers I thought were good, and I'm going to build on them.
Or here are some other papers that I thought were good. At one point there was someone saying, I'm not going to take quite the same approach as this person.
But it was primarily a method of like promoting the best ideas as opposed to like cutting down the worst or something like this.
The pre internet culture.
Yeah.
And so yeah, we've done a lot and I think of trying to like, and similarly with the book, which I guess we'll discuss is like trying to reward the best stuff as opposed to just like cut down the worst or something.
How correct me if I'm wrong, but this is less than five years old the less run 2.0, right?
Yeah, we are, I think I've never been working on it for three and a half years, and I always forget that seems about right.
Okay, because I'm pretty darn sure it didn't exist yet when we first started basing conspiracy. And that's been almost five years now.
So yeah, when it first came online, I was like, this is awesome. I really hope it works out, but I don't want to get too excited right now.
And I'm really happy that it did work out because, you know, lots of times things don't and obviously it was in good hands.
Yeah, I won. I won a surprising number of bets at the time. Like we definitely were like presented with with a lot of criticism. I think I won. I won like a $500 bet with Nate.
See a machine talent research institute was just like, I mean, I think you're just obviously going to fail. And then, and then he said he would publicly announced that I was right.
If you take a bet that Ryan Kerry, I recall him predicting that we'd fail.
Yeah, I remember Ryan Kerry, I also had an informal bet with Critch.
Has he already publicly announced that you were right?
Yeah, I think he did on Facebook.
What was the like deciding factor in you winning that bet?
Did you like anchor on some metric like number of posts?
I think we had set up a system that was like, if it turns out to be like disputed, we would like ask like, we had some foot party agreed on, I don't remember who it was.
But in the end, he just sent me a message from being like, yo, okay, seems like you're right.
I love it. Yeah, on the, on the Beijing Conspiracy Discord, we have a all bets bear witness channel. And half the fun of winning is that typically the loser has to declare like, yep, this person was right.
Nice.
So that's almost, you know, that's what that's worth more than the $10 or whatever, you know, small amounts we tend to bet over.
Yeah, I really love taking bets on like major projects I'm working on or something with the, again, we'll get to the, I made some bets on the books, I'll tell you about them in a bit.
But I won those.
Well, we can, we can move to the books if whenever you guys are ready.
Sure.
Always ready.
It's not true. I'm definitely not always ready.
Do we.
So quick question, do we want to divert to the less wrong post we normally do at the top or since we're on a roll here, just keep going into the books.
Actually, you make a good point. If we're going to get to him, we should probably talk about near the beginning, huh?
Yeah, let's do it.
Yeah.
Yeah.
Yeah.
Yeah.
All right.
Well, we could start with Einstein's arrogance.
Yes.
And this is the in a nutshell, the post where basically when Einstein was asked, Hey, if Eddington's observations that he's going to go do during an eclipse, don't, or, you know, don't demonstrate the predictions of your experiment, what would you, what do you have to say?
He's like, Oh, well, I feel bad for Sir Eddington because my theory is right.
No, no, no, it's better than that.
Since Eddington was going to be measuring eclipses to try to confirm the theory of relativity.
When he was asked if the observations failed to match his theory, Einstein says, I would feel sorry for the good Lord, because the theory is correct.
Oh, wait, because he was Lord Eddington.
Oh man, I thought that he was saying he would feel sorry for God.
I mean, another level of arrogance that would be useful.
Well, I feel sorry for God that he said the wrong universe because my model is metaphysically correct.
That's what I was thinking that he was like, I'm sorry, got screwed up the eclipse, man.
That's delightful.
