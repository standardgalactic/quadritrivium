Yeah, that would be one more, another rung on the arrogance ladder.
That said, he was right.
So one Lord was wrong, at least one.
Lord Eddington wasn't trying to disprove him, but he was just trying to test it.
So that's the question.
Sorry, go ahead, Jayce.
I just was remembering, wasn't it Einstein that said God doesn't play dice?
Yes.
At least I remember being attributed to him, even while I have to flag that whenever anything gets attributed to Einstein, I'm not supposed to.
Oh, that's true.
I know Stephen Hawking repeated that attribution in at least one of his books.
I had to reread the original papers to be sure Einstein had even come up with relativity.
So much gets attributed to this guy.
He suddenly feels much less arrogant to me because just disagreeing with somebody else's empirical data,
maybe they wrote it down wrong, maybe their observations were incorrect.
I thought, when it says Einstein's arrogance at the top of the post, I was like, that is hubris.
That is the sort of arrogance I was thinking.
I want to add one other observation I see from the post, which is that a journalist asked Einstein why he'd do.
And I'm like, oh, I can imagine, in fact, being slightly more arrogant than usual towards journalists.
I don't know, they're often very critical.
It could be.
I just miss, like, you know, the journalist twisted the quote to make it sound kind of headlining.
Yeah.
I would feel really sorry for Einstein if he had to deal with the level of press.
Yeah, I guess it was largely less bad at the time.
I don't know.
They like had yellow journalism back then, which I think that's basically the same thing we have nowadays, just with a different name.
What was yellow journalism?
It's the same thing where people greatly distorted the facts and would say things that were in some cases just blatantly untrue in order to push their own agendas.
Scientists baffled.
Interesting.
Yeah, so moving through this one real quick, the that just is so since he's not in fact saying that the Lord himself is is wrong.
He's challenging Lord Eddington or he's challenging the possible outcome of the experiment failing to prove relativity or to demonstrate it rather.
So the question is, like, is he being super arrogant by saying that?
And I guess in a sentence to summarize, not really like the fact that he had postulated the theory of general relativity at this point and demonstrated it mathematically like that is more than him just saying, you know,
that already has so much weight behind it than it does if it was just him guessing on a dice roll or something, right?
The amount of bits of evidence it required to pull these particular equations out of the space of all equations.
There was an immense amount of like evidence already pinning down what the possible outcomes could be.
So we've done this far, he must already have had and given that he was correct, he must already had an immense amount of evidence suggesting this thing.
To the point where he felt confident that if someone said them perfectly, it didn't come out that they had messed up with their empirical tests.
This post ties very well with the previous one, how much evidence is necessary, because that one specifically talked about bits of evidence, which we kind of skimmed over because it was more technical and we don't have, you know, written format.
But he does say that to assign a 50% probability to a correct candidate from a pool of more than 100 million possible hypotheses, you need at least 27 bits of evidence.
And then says that just coming up with, what was it?
How likely is it that Einstein would have exactly enough observational evidence to raise general relativity to the level of his attention, but only justify 55% probability?
And it was like 27.3 bits.
And he was like, how likely is it that he had 27.3 bits of information rather than 28 or 29?
And he was probably pretty damn sure that it was true from all the info that he did have.
Anyone else on this one?
I have one unrelated thing, but I do want to, I wonder if he wants to say anything.
We were chatting a bit about Einstein's general like reasoning methods being also kind of surprising at the time.
Like it was somewhat controversial the way he came to his physical theories.
There wasn't like, like each step of it was pinned down by evidence.
He did a lot of reasoning from his intuition and building up sort of mental models, which was also kind of interesting.
I think that was sort of controversial at the time as a way of doing physics.
Yeah, there's just, I guess there's like two interesting directions that I can imagine going with this post.
We're one of the question of, okay, but why does traditional rationality as LEs are called that are just kind of traditional science?
They have this norm of like that kind of being bad form.
And number two is like, I think it's very interesting to like think about what kind of mind Einstein had that like caused him to be like arrogant in this way.
If we want to like call it arrogance.
I generally like, I mean, of course, Einstein has like to discover of like two biggest paradigm shifts and like physics of the last century.
I always find them a very, very central like kind of like data point in when I'm trying to figure out how good reasoning is supposed to work, which I guess is a bit, of course, like everyone tries to do that, but I still find it very interesting.
I've had a book since I was a teenager that my sister got me and I haven't actually read it, but it's downstairs called how to think like Einstein.
So you've reminded me that that book exists.
I will read that and get back to you if I actually learn how to think like Einstein.
Great.
I think this post is a very interesting contrast between how humans think and how perfect Bayesian reasoners think. And I think like Elia is actually at a point to doing that.
I don't know, maybe I'm wrong. But he does point out in this that if you have like enough evidence to find this hypothesis and hypothesis space that you need like a huge amount like he said 27 bits.
From a Bayesian perspective, you need an amount of evidence roughly equivalent to the complexity of the hypothesis just to locate the hypothesis in theory space.
If there's a hundred million alternatives, you need at least 27 bits of evidence just to focus your attention uniquely on the correct answer.
So like for a Bayesian reasoner, you need a lot of evidence.
And so you would be really sure that once your attention has been drawn to one particular hypothesis out of several hundred million that you've got the right one.
But like I don't think humans necessarily work that way.
Yeah, at the end.
Since the human brain is not a perfectly efficient processor of information, Einstein probably had overwhelmingly more evidence than would in principle be required for a perfect Bayesian to assign massive confidence to general relativity.
And he just didn't even use a number there.
So just a lot.
He does say that.
I'm not sure.
I don't know, because I think humans cut out a lot of steps and will give probabilities too high of a weight if they, you know, if it is convenient for them.
And I don't think any human would really consider 100 million possible hypotheses.
They already start with a vastly curtailed hypothesis space.
So I don't think a human.
I think an average.
But this is Einstein.
He might have had a lot of evidence.
For a lot to put anybody on a pedestal, it's Einstein.
I mean, I think the difference that kind of comes through in the post is that it just matters really drastically.
Like your perspective really matters a lot.
Like, I don't think it is it is bad for someone to ask Einstein to provide more evidence if they are not convinced, because of course, like, they don't have that those bits of evidence.
And I think there's kind of this.
I find this part of when I think about how science works, I find it very interesting that like, in particular in substantial parts of modern science, it's like very hard to have your internal credences diverge very much from what you think your public credences are.
Where like, it's very hard to be like, I am 99.99% certain of this.
But like, I think you should only be a 30% certain on this.
This is like a very like complicated mental operation and like verbally very hard to get around.
We're usually like, when I say that like, this is 99% likely and kind of asserting that you are also supposed to believe it is 99% likely.
And like, then if the other person is like, what, I don't think it's 99% likely.
We don't actually have a lot of like very elegant scripts to be like, Oh, I think I have the relevant evidence, even though you do not.
And I think a way in which like science has, has helped with this is kind of to make it so that like, when you assert confidence to something, you kind of assert the confidence that is known for the complete audience that you're like, like that, that is kind of like achievable via consensus of the scientific community or the full
audience you're talking to or something like that.
Okay.
So when you say that something is 99% likely, I think what it usually gets translated to is we have a set of evidence that we can all agree on that, like, makes this hypothesis 99% likely, which is supposed to drastically for a statement and saying, I think I have evidence that makes this 99% like
And I've recently been thinking about this where I was just walking home yesterday and I was mentally, I don't remember how I got into that, but I was mentally analyzing my internal affect for how I would react to a bunch of sentences prefixed with I think versus not I think and how drastically those changed
my, my affect towards them, like I was like,
So, um,
I think sounds so much less confident. Like, it's weird, because if someone says I believe if you know if you're talking to a and I don't want to put words in your mouth, but for myself, like, if I if I'm talking to Inyash and he says I believe this, I'm like, okay, he's really sure about that.
If I'm talking to, I don't like my mom, she says, I believe this happened. Like, that sounds way less sure than if she just said this happened.
Like, it's, it's a so like if someone says, Oh, I think this is it rather than this is it.
To me, it like cuts my confidence what they're saying and they're, I think we're trying to communicate the confidence what they're saying like it just cuts it way down.
I remember when I first started writing, I was still in the habit of speaking kind of like a rationalist, and that actually makes for terrible writing because we often couch things with I think I believe and this stuff.
And the first thing you got to do is get rid of all that unless you're specifically writing a character who is.
Didn't you didn't you want all of this important epistemic information about my epistemic state.
I just wanted to know what you were thinking about today.
This was so much more accurate before you asked me this.
So I do think it's not sorry.
I just sort of on the side.
But there's this book that is about a girl with OCD and I really related to this one of my childhood books because I also had a CD and it's the first time I ever actually saw it portrayed.
One of the things that she does is when journaling she realized that I can't ever be sure that something is true so she would start every sentence with I think.
And then this became a ritual where she would write just a bunch of I think I think I think at the top of the page.
But then that didn't feel good enough.
So she like took the words I think and made them into a symbol and then would draw the symbol repeatedly around the page.
So it's a really really make sure that she wasn't claiming to know something that she maybe didn't know.
And it would be things like I think I went to school today.
I think there's a lot of interesting neurological stuff from that.
But anyway.
What was the book called?
It's called growing any way up.
Okay.
In case anyone wants to read it.
Yeah, it's like a YA book but recommended it's really I thought it was really interesting.
Yeah.
Yeah, so back to one of the other points that we were talking about.
I think it is important for like healthy communities that are either intellectual or scientific communities or even like.
Building communities that like trying to build useful things like startups.
This is an important point to realize that people can have a lot of evidence that you do not have and they can be much more confident in a thing than you can be.
And not have norms where this is like inappropriate or something.
I think it's often the case that, you know, small teams of startups have like interviewed like hundreds of users and understood like a pretty accurate model of what problems are facing and is like tested trying to fix a little bit and making a prediction about how it would go and just being validated to the point where they
just have overwhelming evidence that there is like a product here that a lot of people sort of want to use.
And then they're very confident in their business, which out from the outside, I think people often call it kind of overconfidence.
Whereas I think you can just in fact, similar to Einstein here, although not exactly the same methodology, reach quite a high level like get quite a lot of evidence about a thing, but not in a way that is like easily communicable in like a short conversation or something.
Yeah, like I find it interesting about the I think distinction there's just like, I think English was just like well it kind of communicates a different level of credence but I find it very interesting that I think actually even when you explicitly
include the credence, my internal reaction to it is still drastically different where like the statement like that's at most 50% likely versus the statement I think that's at most 50% likely feel drastically different to me.
Yeah.
Like if you come to me in a conversation and say that's at most 50% likely, that sounds to me like you're asserting that we both are supposed to know that it's at most 50% likely.
The second one says this is my belief, this is what I think.
I do observe when you said that my internal responses to the two with the person who said, I think that's 50% likely. I was like I had some impulse to like asked about how they were viewing the thing and like get a better understanding of their
perspective or share a bit more of mine. And the person who said, that's no more than 50% likely. I just want to offer them a bet.
I can't help it. Well, I was going to digress a bit with the annoyance of trying to get some people to agree to bets but I find that people who aren't raised with the or who aren't raised necessarily but who aren't having adopted the like mentality of sticking your
out to defend a belief. If you offer them like, I'll just give the brief story. My manager was, for some reason brought it up like twice in a week. He wanted to say like, there might be some legitimacy to the false election claims going on.
And we should really just sit and you know, none of us have all the evidence. I think it's putting worse, you know, paraphrasing. I think it's epistemically humble of us to just kind of, you know, wait for the information to come in.
And I was like, I tell you what, I'll bet you $500 tier 50 that it comes out this way. And his response. Oh, I don't bet. I'm not a gambling person. And I'm like, you don't want you don't want $500? Like then, I mean, come on.
This has been one of the benefits of having prediction markets is that all of my housemates and friends have just been making money up.
Oh, yeah, it's been so insane. I'm so confused. I have so many thoughts on this.
I mean, you've seen there's been a bunch of prediction markets still sticking like until I think the last couple of days, but still for like a whole month after the election, put it at like 12% that Trump is still the president.
And so I think it just a bunch of my friends have put in a couple of thousand dollars into all the different prediction markets and just been making money.
Which is how I learn how to do that.
Isn't that technically not legal?
No, there's some legal prediction markets. So predicted is legal. And there's one other one that's legal in the US.
Okay, that's what had been stopping me because I kept hearing that betting on election results is completely illegal in any way.
No, definitely predicted has a specific. So I have looked into this good amount because one of our good friends, Jacob, who like currently is collaborating a bit with less wrong has a he is for a while been trying to get
No action letter.
Yeah, no action letter from the FTC, whatever, whatever the thing is that's supposed to like limit like betting and various various currency related things in the US.
A letter that like is the thing that predicted God that allows him to run a betting market on various interesting things.
And so I've read a good amount about all the regulations in the space because I've been thinking about things.
Okay.
That's a good update to have.
But I'm sorry, Jason, I think you were going to ask a question.
I was curious what like what things he wanted to include as things you could bet on, but also realized that we made we should like get through the first sequence.
Fair enough. I'll just I'll hold us up for one more second. What was the name of that website? You said predicted.
Predicted. Yes.
Just predict it.
Gotcha.
Well, I need to start making some money. So I'm going to bookmark that for now.
Can I hit one last thing on this post before we move on?
No, of course.
That was not an answer I was expecting.
It reminds me of the time we met with Robin Hansen. Remember, he came out to dinner and like the third time I started a like a topic with him.
Hey, can I ask you something? He was like, you can stop asking me that because you always.
Like he said it really nice, but I was like, oh, man. Yeah, it was a verbal tick of mine when when it's like someone I really respect.
I was like, oh, I don't want to use your time. Can I ask you this thing?
But no, the thing is, there's a post later on, which always never quite sat right with me where Eliezer says that a very powerful true basing predictor could come up with the theory of reality.
The probability of watching just three frames of a video of, I believe, a water drop falling on and bending a blade of grass.
And he said if it was powerful enough, it might be able to do it with just two frames.
And like that always seemed really ridiculous to me.
But reading this, especially at the end when he says that since the human brain is not a perfectly efficient processor, Einstein probably had overwhelmingly more evidence than he would need in principle.
Like maybe the more I think about more like maybe in three frames of video, you can get 27 bits of information.
And if you were a obscenely powerful, Bayesian predictor processor, maybe you could get general relativity out of that.
And that's what he was setting up with this post on Einstein's arrogance.
Yeah, I've been that far experiment is one that I've actually been thinking about for like basically the last decade, just like at least once a month or so.
It's been like one of the big ones.
And I've had so many disagreements with lots of people I respect on it who think that the number is much, much, much higher.
And even like, like I think I had at one point a disagreement with Luke Milhauser, where he thought that it was pretty likely that maybe not a perfect Bayesian predictor, but at least anything we're going to get in the form of AGI or something to that, still has probably will have to run experiments in order to like figure out to write like model.
Like at least extending beyond relativity or something. And I'm, I still think that's wrong, but it's like definitely I've talked to many people I respect to who disagree with kind of that free frame number, at least in the AGI case quite a bit.
It feels absurdly low to me, but I am no longer at the point where I think it's obviously wrong. Like it could maybe be possible.
It reminds me of, I think the guy's name is Douglas Hubbard, who wrote How to Measure Anything, which is a cool book I read as a teenager. And it's, there's one example in that which always stood out to me, which was people often talk about how you need more data and lots of data before you can like feel very confident about
hypothesis. And he was sort of counterintuitively making the claim that like, the first data point is most of the information you need. And his example was about guessing the weight of jelly babies. And if you went to someone like me, for example.
What are jelly babies?
Or sorry, jelly beans.
I'm really reminded of Free Worlds Collide, oh God.
It's like a jelly embryo, really. It's a baby at four days of gestation.
Do you guys not have jelly babies? Is it just a British thing?
