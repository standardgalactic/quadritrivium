Welcome to episode number 46. Today on the show we have Daniel Schmocktenberger again. This time
he's talking about existential risks and how to avoid them. We've already done several other
episodes with Daniel before and they were always some of our favorites, a very mind-expanding
and inspirational, but I think this one is particularly important because he talks about
some of the most pressing issues that we face on our planet today and proposes some ways that we
can solve them. If you want to check out the previous episodes that we've done with Daniel,
go to futurethinkers.org slash Daniel. To get all the books, resources, and the other things that
we mentioned in this episode, go to futurethinkers.org slash 46. This episode is brought to you by
BCDC. They're working on several blockchain-based apps to help save the environment. One of them is
called EcoChain and it's a crowdfunding and investment platform for renewable energy projects
where people can invest in new clean energy installations and get an ROI. So BCDC is doing
a token sale which is now happening in November 2017, but the pre-sale is already happening now.
You can see our report and listen to the interview we did with them by going to
futurethinkers.org slash BCDC. You can also find all the links to their website, white paper,
and bounty campaign there as well. Last but not least, before we get into the episode,
we want to thank our recent patrons, Ray and Dean, for supporting us on Patreon. Alright, let's get
started. Welcome to futurethinkers.org, a podcast about the evolution of technology,
society, and consciousness. I'm Mike Gilliland and I'm Yuvia Ivanova. To get notified of new
podcasts and videos, go to futurethinkers.org and subscribe to our mailing list. You can also find
us on iTunes, Stitcher, and YouTube. If you like what we do and you want to help us make more podcasts
and videos, give us a like or a review, share it with your friends, and consider becoming a patron.
Go to patreon.com slash futurethinkers. If you're looking to get serious about meditation,
check out the meditation app that we've created with Negupta called the Cutting Machinery.
Go to cuttingmachinery.org slash app. Last but not least, if you want to get access to more
content, hidden episodes, or if you just want to chat with us and previous guests, then check out
our community at community.futurethinkers.org. Daniel, welcome to the show. It's good to have you
on again. So today we're talking about existential risks, which is a topic I know you're quite familiar
with that, you know, you've brought up in past episodes with us. So this is something I've actually
really been looking forward to getting into with you. Great. So why don't we start with the big
questions? What are some of the biggest existential risks that we are facing on the planet right now?
So for those who aren't familiar with the term existential risk means some risk that could stop
us from existing. So a species-threatening event, there's also plenty of catastrophic risks that
would just really suck even though we wouldn't be completely wiped out as a species. So there are
versions of World War III that are existential kinds of nuclear holocaust that nobody makes it
through. There are kinds of World War III that are catastrophic, meaning some group of people in
remote areas make it through, but the atmospheres hold uranium and, you know, most of the people
did very poorly. So we're kind of interested in preventing all of those things, right? So what
are the categories first? There's different ways of kind of chunking in different ways of doing
taxonomy. One thing we can look at is start by saying there are kind of human induced things,
and then there's kind of natural phenomena. The natural phenomena are things that either arise
from within our earthen biosphere or outside of it. So outside of it is things like solar flares
and Carrington events and asteroids that can be catastrophic or existential. And obviously,
you know, within our atmosphere most of the natural phenomena that would occur would not
be existential, but there are some super caldera kinds of scenarios that would be. We're obviously
interested in any of them because, you know, if there was an event that we just couldn't avoid here,
then Mars Colony moves up on the priority list, right? But surprisingly, most of these things,
if we have more intel about them, many of them can be avoided or at least mitigated against. So for
instance, if we look at something like a Carrington event where we're looking at solar flares or
coronal mass ejections that besides the just effect that they would have on biology directly
would also have effects on electrical circuitry. As far as biology goes, there might be an impact
on one part of the planet. People can go underground. It's not that big a deal. But as far as frying
the circuitry goes, if we're talking about frying the circuitry of nuclear cooling stations on the
power plant, so then they turn into nuclear volcanoes, that's a pretty big deal. Now we could,
of course, do things to mitigate against the sensitivity there like hardening the
nuclear cooling stations and, you know, other forms of key infrastructure. So even with regard to
things that we can call natural phenomena, obviously, you know, there are projects that are
looking at asteroids and other near earth objects, because depending upon what we see, they might
actually be movable. And there are a number of projects focused on that. Then we move our attention
to, you know, human induced things. And we can look at the ones that go through environmental
pathways. Basically, we either through biodiversity loss, or climate change, ocean acidification,
or other kinds of effects on our biosphere, we create a biosphere that is uninhabitable to humans.
But we don't even have to get to full uninhabitability for partial uninhabitability to
lead to the beginning of cascade effects. You know, when we look at what happened in Syria,
we see that a major part of what happened, factoring that there were many, many factors
simultaneously, but one part of it was droughts in an area that had not had droughts led to
subsistence farmers moving into the cities that taxed the resource capacity of those cities beyond
their limits that led to resource wars that led to factions and that whole kind of thing.
So when we start looking at climate change, creating massive numbers of refugees,
and the refugee dynamics leading to war dynamics and resource shortage wars, economic collapse,
and then, you know, cascading wars, you can have scenarios where environmental phenomena
are the first step in a series of cascades that can occur. So you've got all of the
environmental phenomenon, the effects of on coral of ocean temperature, ocean acidification,
the loss of big fish and the trophic cascades, key species collapse throughout many different
dynamics, pollinators, like there's a lot, there's a lot of things in there. Then we can look at
other reasons for just human induced violence towards each other. So all of the reasons that
something like a World War III could happen. And we see right now, the United States leaving its
position of clear supremacy as a superpower, we see China moving up into that position, we have
rarely seen transitions of power go smoothly in the history of civilization. And we've never had
transitions of power be at this kind of global level and with existential level technologies,
and with over 7 billion people and with a biosphere near collapse and near fragility points, like
we're in a very different place. But even when just there was a local hegemony where the power
was moving from one kind of local power to another, those were usually not easy transitions. So
lots of different scenarios. And then there are the exponential tech scenarios where different
kinds of exponential technology, exponential technology means exponentially increased power
to affect stuff. And whether we're talking about through AI, or through CRISPR and biotech, or through
nanotech, you know, or through robotics, if we have exponentially increasing ability to affect stuff
without exponentially increasing good choice making, that is just self terminating, no matter what.
And that is the scenario we have right now. So exponential tech can lead to existential scenarios
on purpose or on accident, right? You can have just much more powerful kind of capacities,
military capacities, and much more powerful capacities that even smaller actors are able
to get. Nuclear bomb started off as such a difficult thing that only huge state actors
could have it, those state actors could watch each other, you could have a mutually assured
destruction system, so nobody could use it. But as you start to have exponential tech that
becomes distributed, where smaller and smaller actors have access to larger and larger potential
to affect things, the ability to avoid that happening gets harder and harder. And then you
also have exponential tech that leads to existential scenarios accidentally. So most of the AI risks
that Boston lays out in superintelligence, the gray goo risks that Drexler lays out, you know,
these are mostly where we create an auto-poetic system, we create a self-fueling system that
has a faster feedback loop, is more adaptive than biology and not commensurate with biology.
And so we have to avoid accidental extinction, as well as on purpose extinction, you know,
that's kind of a rough lay of the land of the categories of existential risk. Now, if we take
a step back from it, we can say all of these categories have some things in common, some deeper
underlying drivers that are actually the real existential concerns, because at this level,
the scenarios are basically countless. But if we go deeper, one way of saying it is that the real
existential risk is a loss of sense-making, a loss of the ability to actually make sense of the world
around us, and what is worth doing, what the likely effects of things will be, what the effects of
our actions are now. And so when we think about it, how long do we have with ocean acidification
before the coral die? Right now, the thoughts on that vary so widely that it's either the most
important issue, or we have quite a bit of time. And is Fukushima really close to releasing a lot
of radiation or not? And can North Korea's nukes really reach certain places or not? And what are
the real risks with AGI, parasitic AI, et cetera? Like if we don't have better ability to make sense
of these things, then they just all fail scenarios, right? Because we don't know how to prioritize,
what should actually get attention paid to it, and what could be successful and what's not likely
to be successful. So we can say that a lack of information coherence is one of the things that
is at the heart of driving them. And now to take a deeper step, we say, well, within capitalism,
and we can step even further back in a moment, but let's take that step. Within capitalism,
where we're playing a competitive game, we're playing a win-lose game, usually zero, sometimes
small positive sum, but still fundamentally win-lose, information is a source of competitive
advantage. So we are incented to both hoard information and to create disinformation. We
actually, like if we succeed at that, we will actually do better within the system. So as you
have exponential technologies, the ability to do more media, spread more different info through
more channels, et cetera, how do we have exponential information tech with the incentive to disinformation
and actually be able to have enough coherent information to have any idea what the fuck is
going on? Well, basically, we don't. When you think about getting towards the tipping point of
climate change, where if we get to a positive feedback loop with warming or something, and if
we don't stop a dynamic before a certain point, we won't be able to afterwards, we're actually pretty
close to the point of peak discoherence, where if we get to that place, we might not actually be able
to recover coherence. And so I would say that's one way of thinking about what is actually essential.
Another way of thinking about what's essential is, like we said, exponential technology. It's very
hard for people's intuition to grasp what an exponential curve is, because most of the things
we experience don't feel like exponential things, but most of technology that is exponential is
increasing our ability to affect something with our choice, but it's not increasing the quality
of our choice. Now, when you multiply these together, actually decreased information coherency
with increased impact, that is a self-terminating scenario.
So what are some of the ways of avoiding that? Especially, I liked what you said about our
loss of sense making and the exponential curve of that also. That's a really interesting factor
that I think a lot of people aren't talking about. It relates to one of the questions I was going to
ask. What do we do about the institutionalized ignorance of people to what is actually going on?
You mentioned the corporate interests and producing disinformation, but there's also bad
educational systems that are outdated or motivated by different factors that have nothing to do with
actually giving people good information, and then there's research that serves corporate needs,
and then there's media propaganda and politics and all this stuff. So what kind of tools do we
actually have to change these systems in a way that they serve the planet, rather than serving
these individual interests? We could say, well, when you talk about sense making, shouldn't that
be science? Shouldn't science be sense making? In the same way that we could say, shouldn't
journalism be sense making? Shouldn't the intelligence agencies of the world be sense making?
We've seen scenarios where a scientist publishes a scientific paper in a prestigious scientific
journal where the paper that they wrote is complete gibberish. They made it up with technical sounding
words, but it's actually gibberish, and it goes through the publication process because they
came from the right university or whatever, and they were doing that just to show that the peer
review process is broken. We've also seen where stats like 50% of the articles posted in places like
Journal of American Medical Association five years later, the findings are found to have been
misinterpreted or wrong or had bad methodology, et cetera. 50% wrong means that if I'm reading a
cutting edge medical journal, my chances of knowing what's true are 50-50. It's like pulling
tea leaves would give me as much of a sense of what's true. That is a broken sense making system
within science. If you want to think about what the heart of science is, like what the essence
of the philosophy of science is, the essence of it is earnest inquiry, like earnest desire to
understand the nature of reality. Whatever the speed of sound actually is, I don't care
if it is different than I think it is, I want to find out. There's a certain reverence for
reality and respect for the nature of what is beyond my own ideas about it or how I can benefit
from it that is at the heart of the scientific impulse. But the scientists that need paid and
the equipment has to come from somewhere. If there are some of the answers that are more profitable
than other answers, then it's easier to put money into it because I'll be able to get that money
back and if an herb or a plant has some medicinal property, but I will never be able to patent it
and I won't be able to recoup the money that I get from the small margins that I would put into
the research, how can we afford to pay for it? But we will with, say, some synthetic thing,
I can get a patent on, etc. What actually gets researched within that structure and how it gets
researched? You have to factor how deep those kind of perverse incentives end up trickling
through the whole thing. Then you ask a good question, which is, within a system,
within a fundamentally win-lose game-theoretic system, how do you prevent these issues and
be able to have real coherency when we are incented to withhold information and even,
you'll see in soccer or football, whatever, someone will fake left to try and get the other
person to go left and then they'll actually go right. That's disinformation, intentional
disinformation so that you can throw the opponent off so you can win. This is not different in
corporations or nations, right? Intel and counterintel and intentional disinformation, same thing,
but with global level consequence. Here's another good way of thinking about it. Think about early
tribes as competitive teams, almost like a sports team, where they have to work really well together
and be very coherent with each other to be able to compete with other teams in military conflicts
when they occur and compete with the other teams for scarce resources in the shared environment,
the shared commons. Say they weren't military, right? They're just doing their own thing,
there's plenty of abundance, but then if any of them realize that they can do this military thing,
go kill another tribe, take their stuff and get the riverfront that they had, or get whatever
kinds of things they had acquired and developed, that actually worked for them, worked better.
Everyone else has to build defensive militaries at least, otherwise they lose by default. One
of the things to get in the history of win-lose game theory is that, one, it worked. You could
actually go kill the other people, take their stuff and make stuff better for you and your people,
and two, if you didn't play win-lose game theory and someone else did, you lost by default,
which is why most of the more peaceable, less militaristic cultures got wiped out.
Then we say, one of the tribes is just bigger and stronger than us, we're not going to be able
to compete with them, but if two or three of us smaller tribes band together, we can, but then,
of course, the other side has to compete, so they band together. Now we move from tribes to villages,
right? Then we can move up to kingdoms and to nation states and to global economic trading
blocks. We can think about those evolutions of what we think of as civilization structures,
as evolutions of competitive teams within a win-lose game theoretic structure that have
more and more power to be able to out-compete the other one, and that's both have more people that
can be coherent with each other against the other one, right? As a nation, we're a team competing
against the other one, we have a shared military-industrial complex paid for by taxes, whatever,
or as a religion or a race, so we can have some overlapping teams. But once you get to the point
where all of the teams are stepping up simultaneously, that's the kind of evolutionary
driver, they're stepping up in their power, once you get to the point that you have exponential
