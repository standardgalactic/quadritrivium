technological power on multiple teams, where the amount of power that it would take to actually
win would require destroying the playing field, which becomes inevitable, right? Inevitably,
power will keep increasing until it's actually bigger than the playing field can tolerate,
and then winning pretty much means losing because there is nothing left to win, right?
So right now, we're at a place where the superpowers of the world cannot win a war against
each other. When you have hundreds of times the nuclear capacity necessary to kill all life on
earth, there is no winnable war, but we keep building more fucking military capacity. We just
spend a trillion and a half dollars on the F-35 because we didn't have badass enough jets with
the military that already has all the technical capability that it has in the US, right? And
think about what a trillion and a half dollars of resources means to other kinds of things.
And so when you say, why did we ever get more than five times as many nukes as we needed to
kill all life on earth? Like, after you've got enough nukes to kill life on earth five or ten
times, why do you keep building more? Well, you have hundreds of times. Was that ever a really
just strategically smart idea? Or was that because we had a for-profit military industrial complex
that makes money when that happens, where the people who are in positions of decision-making
power also happen to be shareholders or have vested interest in those structures? And this
is not blaming the people involved. These are structural, right? These are structural dynamics.
And so can you have lasting peace and a for-profit military industrial complex at the same time?
Of course not, right? Like, that's supply and demand 101. If there is a massive capacity for
supply, it has to protect the demand. And then you think about where those perverse incentives
go everywhere, right? Healthcare, where if people weren't sick, then the system loses all of its
resource. And so how could it ever really invest in prevention deeply when its profitability
depends upon symptom management? And if you even cured people quickly, that's not profitable. But
if you manage symptoms ongoingly, it's profitable. And if the symptom management causes other issues
that need more symptom management, i.e. more meds, that's just like an upsell or a cross-sell,
maximizing the lifetime revenue of a customer that happens to be a patient. So when you think about,
you know, win-lose game theory within that kind of incentive structure, where we can't win at the
wars anymore. And yet we're moving closer to the kinds of tensions where war becomes more and more
probable. And we also can't win at competing for extracting the scarce resource. When we've already
extracted so much scarce resource, we're almost at the biosphere's limits, right? We're almost
at the limits of what can be handled in the extraction of oil or fish or grazing land for
cattle or so many things. And so either we continue with win-lose game theory, and it becomes omni
lose-lose catastrophically, or we have to figure out how to have that power not turned against
the other power and figure out what does omni win-win mean, where no one has incentives that
are misaligned with the well-being of others in that system that has that much power. And that's
really the fork in the road we're at as a species for the first time ever now, is we either figure
out what omni win-win means economically, as a worldview, governance-wise, we either figure out
what that means, or we continue with win-lose and it becomes omni lose-lose, which pretty much means
we either step up into a radically higher level of quality of life existence with a new kind of
collective intelligence, or we have a catastrophic step down, existential or near existential. But
in the next not very long time, that's really the fork in the road.
And what evidence do you see of us going down one path or the other with more preference?
If we just plot curves, it doesn't look good. If we just plot the curves of what has happened so far
with regard to how much harm came from new power that we developed, and we look at how
fast we are developing new power on exponential curves, if we look at the population curves and
we look at biosphere issues, it all looks pretty bad from that point of view. And you can try and
take the techno-optimist route and say, but look at the rate at which we are developing solutions.
But here's the problem with that. Here's another way of thinking about what the driver of the
existential risk and all the catastrophic risks are. So one way of thinking about it is that
win-lose game theory is driving the whole thing, right? Win-lose structures are driving the whole
thing with win-lose game theory multiplied by exponential power and exponential populations.
Another way of looking at it is a lack of information coherence. I mean, there's information
coherence and coherence as beings and agents with each other. Another way of thinking about it is
a lack of coherence around value measures. And this is actually a very deep and essential way of
thinking about it. Think about a tree in an ecosystem and we say, well, what is the value of that tree?
Well, it's providing a home for a bunch of pollinators and birds. It's providing food for
them with flowers. It's stabilizing topsoil. It's symbiotic with the micro-rysia and the fungus
and bacteria and the soil. It's providing shade for all of these things. It's pulling out CO2
and producing oxygen and providing food for animals. And it might have millions of value
metrics as part of this kind of complex ecosystem. But then we cut it down to make it $10,000 with
a 2x4 and its value is $10,000. And the 2x4s, they're not sequestering CO2 and stabilizing
topsoil. They are serving as a structural strut. It's this one really simple thing,
right? Structural strut. So we took this very complex thing and we reduced the complexity
of it radically. We made it a very simple thing. So we downcycled the shit out of it
because what the metric we were seeking to optimize was dollars in my account. And so a
dollar is a value metric. But I get the $10,000 from that tree or I get $10,000 from the elephant
tusks, right? Or I get $10,000 from this servicer. Well, it's like, how do I relate the value of an
elephant tusk or a person's art or a tree? Like these should not be fungible. These should not
be inter-exchangeable. These are fundamentally different things. But because I remove all of
the information from them and all of the context, I want them to be simply exchangeable in terms
of capital so I can maximize the ease of transaction to grow pools of capital. That is an
extinctionary dynamic is that we're taking complex value and turning it into simple value.
That almost sounds like a gray goo scenario of itself. It is.
We're taking all of these super complex things and reducing them to this homogenous goo that is
monetary value. I actually just saw a really good article that was called Capitalism as a
paperclip maximizer. And it's actually a really fun thought experiment. Paperclip maximizer,
if the people aren't familiar, was, I don't remember who came up with it first, but it was in
Nick Bostrom's book on AI issues and just said we could have an AI whose job was working
at a paperclip company to maximize paperclip production. And it also had the capacity to
upgrade its own capacity. And it ends up getting into a place where it makes all these paperclips
and upgrades its own ability to make more paperclips and then starts competing,
taking all of the resource that humans needed to make paperclips because that's its algorithm.
And as it's increasing its own capacity, it's being able to out-compete us. And then it realizes
that we're made of atoms, that it can make paperclips out of, that it's this continuously
growing, this auto-poetic capacity that's just turning everything into paperclips.
And so they were saying, this paper capitalism is a paperclip maximizer because
capital is making more capital. That's the gist of this distributed system. Rather than as a
central AI, it's this distributed collective intelligence system that uses these distributed
human bioprocessors within it. But because we've reduced the value to capital and having more
capital makes it easier to get more capital. The capital gains interest faster than the overall
economy grows, gives you access to more financial services, etc. Then that's the goal. So to say
capitalism is a paperclip maximizer is a reasonable thing. And Drexler's model of gray goo was based
on another exponential tech, which was nanotech, the ability to rearrange things at a molecular or
atomic level. If we do that right, it's pretty awesome. If we do it right, it's like the replicator
from Star Trek, we can take trash and turn it back into rad stuff at the level of just atoms.
And that actually might be the future of the materials economy is that we have quantum computing
that has enough sophistication, give or take a million qubits to be able to properly direct
nanotronics to take old stuff and turn it back into new stuff at atomic level, creating a closed
loop materials economy that can upcycle indefinitely. But if we get it wrong, then you can have these
machines that are just turning everything into goo. So Drexler had this model called gray goo.
And in a way, UV, you're right, capitalism as gray goo is we take this tree with this
radical contextualized complex value and take it out of its context and give it this reduced,
abstracted, simplified value metric. And we've done that to 80% of the old growth forests that
the earth spent billions of years developing, and 90% of the large fish species in the ocean.
And then what does that capital really do other than continue to do that auto poetic thing?
And so when people think about like, what is capitalism? And honestly, we can say communism
and socialism were really actually subsets of this kind of resource concentration system.
It's a process of abstracting value. So we go from complex value to abstracted value,
and then extracting it and accumulating it. That particular model came from a colleague,
Forest Landry, and capitalism does that. But socialism and communism have other versions
of doing that. But that's the core thing. That's the ring of power that has to be broken is abstraction
of value and specifically a reductive abstraction, extraction. So you remove the content from its
context and accumulation. And that's how you take a complex system that is resilient and
turn it into a complicated system that's not resilient, that's becoming progressively simpler
and kill it. This brings up something I find to be fascinating, which is the use of blockchain
to tokenize abstract value. Do you have thoughts about this and whether it's actually possible
to do this? I can get more specific if you need. Yeah, please do. So we had an interview last year
with Vince Means, and he talked about tokenizing these abstract values. Actually, he used the
tree example. So what if you can tokenize all the extra abstract value that a tree provides,
have a token for oxygen converted, that sort of thing? And what if you can optimize those
different tokens in the same way that capitalism optimizes capital? Can we use these other systems
of value and use them as currency as well? I say that loosely and optimize that using the existing
capitalist system. I don't think so. Laser-affair capitalism always kind of goes there. It says
the tragedy of the commons is because if nobody owns it, then they'll fuck it up. But if someone
owned it, they take responsibility for it. So really, the answer is to have everyone own everything.
Every coral reef, everything is owned, and we have some way of creating capitalist value in
the process of owning it. This breaks down for a number of reasons. So why is air not worth
anything and gold is worth so much? Because in a win-lose game model, something that everyone
has access to and can't not have access to and I can't get any more of doesn't give me competitive
advantage. So everyone has access to air, so I don't need to value it. I don't need to pay
attention to it. Now, if I cut down this tree, of course, I'm cutting down something that produces
oxygen and sequester CO2, but I'm not cutting down enough to fuck up everything. I'm not cutting
down enough to really affect my experience at all. Now, of course, as you get 7 billion people
with that mindset, it becomes a different story. But distributively, they're all thinking about
their own action and that if I cut this tree down, I have $10,000 worth of 2x4s in my pocket
and I need to feed my family and I don't have any measurably less oxygen, but multiply that by 7
billion and we all die, right? But if I don't cut this tree down, that tree in the commons is worth
nothing to me in terms of differential or competitive advantage. And I have a system where
pre something like basic income, even before competitive advantage, I just need to live,
right? So if I can take something out of the commons to live, like I'm going to do that.
So gold or diamonds or whatever are worth a lot because they were perceived as scarce when
we started the valuation schema. And if there wasn't enough for everyone to have lots of it,
then some people could have it and others wouldn't and those who had it had something that was unique
where they had some differential advantage. But it's still fiat, right? Like whether it's just
printed dollars or gold that we ascribe that value to that is not based on the actual material
value of the gold. And then we set it in vats. It's still like, it's a value metric that we made
up. And so we'll have gold worth, you know, however much per ounce. And because of that,
if there's gold under some trees, we'll cut down a lot of trees to get the gold out and burn up the
oxygen or damage it in the process, because we have a system that is valuing differential value,
not systemic value. That's if you start saying, well, how do we actually advance systemic value?
You can't really tokenize that because as long as there are separate balance sheets,
and I have some number of tokens and I'm trying to advance the number of tokens that I have in
competition with other members, we're going to keep getting all these win-lose dynamics.
So the post-capitalist move is a deeper move than that. Now, does that mean that tokens and
blockchain can't serve a transitionary role? No, of course they can. Right now, we make up the
currency units, you know, through a central bank. Well, you can have another group make up a different
kind of unit, and you can build some better structures into that unit. And that can be
valuable, totally. But as long as we still have private ownership of those units, that if I have
more of the units to begin with, it's easier to get more of them. If I have less to begin with,
it's harder to get more of them. So I have widening wealth gaps built in. All of the other shitty
aspects of capitalism, all of the derivatives and complex financial instruments and fucked up
markets will end up arising from those competitive structures. Those are just basically inexorable
ways to game the game. So then you're like, oh, fuck, so what do we do? We do need a completely
new economics. We do need a completely new system of sense-making. We do need a completely new system
of choice-making, governance, at an axiomatically redesigned level. And when most people, when
you ask them what is economics, they think, well, it's a system of trade, system of barter.
They're already assuming private ownership and trade and barter that is then mediated by some
kind of currency. But you have systems that don't have private ownership, that don't have trade,
right? Because I don't own shit to trade to you. We have some kind of systemic value. We just didn't
like those ones, because for the most part, we thought of those as communist socialist systems,
where if I didn't privately own stuff, then the state was giving me what I needed, but then the
state was also forcing me to do shit. Because as long as there are shitty jobs that nobody wants to
do, that we don't have intrinsic motive to do, but the society needs done, how do you get the
people to do the shitty jobs? Well, if we kind of meet everyone's needs throughout this thing called
like a state, then the state has to force some people to do the shitty job. So we call that
imperialism, and we don't like communism because it's imperialist. So we say, well, let's let the
free market force them, which is if they can't be smarter and educate themselves better to do a
better job, then they get the shitty jobs. And if they don't, the state isn't forcing them, but they
will still be homeless and starve. It's just where do we switch the imperialism, right? We switch it
from the state to the market. But basically, we still have to have a system of extrinsic incentive
to control people to do shit that they're not intrinsically oriented to do. Well, okay, so that's
been part of what we've had to work with. And Marx had thoughts on it and Smith had thoughts on it.
But technological unemployment is changing that whole fucking story, where you can start to automate
the shittiest jobs. So you can have a system where the things that people actually could have
intrinsic incentive to do are what are there for humans to do, the things that you had to
extrinsically incentive them for, you don't have to like that's a new thing. That means an axiom,
which is how do we deal with the labor force and axiom is changing. And so we have to go back and
rethink all of our economic ideas. And with regard to sense making, there's a lot of core things
that are changing in terms of our capacity for sense making. We have the ability to have
IoT sensors, right? Internet of Things sensors that are giving us real time data about air
chemistry and water chemistry and soil chemistry and fisheries and the commons and etc. And we have
deep learning systems that can be synthesizing that information. We never had sense making systems
like that. We didn't even have the technological capacity for things like that. And choice making
systems. So basically we are at the face of deeper issues than we've ever been at the face of before,
but also with deeper capacities. So the same increased technological capacity that if we keep
using it to take complex things and make them simple, simple or complicated, instead of complex.
And we keep using it to fight against each other and win and competitive win-lose games.
If we keep doing that with these increased technologies, that's existential. But if we
use those technologies to actually obsolete those core social structures and create coherency-based
social structures where that technology can be used for, not against, then, I mean, we really
