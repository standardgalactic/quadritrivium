Welcome to Future Thinker's podcast, episode number 57.
One of our favorite guests, Daniel Schmocktenberger from Neurohacker Collective, is back on the
show today.
Daniel is talking about a concept which she calls the generator functions of existential
risks.
So these are the deep causes from which the existential risks we face today come from.
This interview is in three parts that should be listened to in chronological order to
get the full understanding of the concept.
In part one, Daniel explains what generator functions of existential risks are and how
they affect the world.
In part two and three, he elaborates on the generator functions through a few different
lenses and reflects on what we need to do to solve these underlying issues.
To listen to all three parts and get all the links and show notes, go to futurethinkers.org
slash 57.
One quick announcement before we get into the show.
We are hiring a social media manager for Future Thinkers and our other podcast, Crypto Radio.
If you or somebody else you know might be interested in this position, go to futurethinkers.org
slash social to find out all the info.
This episode is brought to you by Kualia, a premium nitropic supplement that helps support
mental performance and brain health.
It's specifically designed to promote focus, support energy, mental clarity, mood, memory
and creativity.
UVA and I have both used it in the past, we really like it and we actually met the founders
and interviewed them on Future Thinkers and you can check out those interviews through
one of our favorites at futurethinkers.org slash Daniel and futurethinkers.org slash
Jordan.
They've got a new formula up called Kualia Mind.
It's got more natural ingredients and you can get it at futurethinkers.org slash brain
hack and you can get 10% off if you use the code future.
All right, let's get into the show.
Welcome to futurethinkers.org, a podcast about the evolution of technology, society and consciousness.
I'm Mike Gilliland and I'm UVA Ivanova.
If you're new to the show and you'd like a list of our top episodes and resources, go
to futurethinkers.org slash start.
And if you like our podcast, leave us a rating and a review on iTunes and elsewhere.
It really helps others find the show and we really appreciate it.
In one of our previous episodes with you, we spoke about how to assess and prevent existential
risks and we got a lot of positive feedback on that episode.
People really loved it and I know you've spent a lot of time thinking about the subject
since then and you've refined your position.
So I wanted to ask you, what is your current thinking on existential risks and how to deal
with them?
All right, great.
I'm very happy to be back with both of you.
It's valuable for us to kind of go up a level when we're thinking about risks, existential
or just any kind of catastrophic risk.
It's not just a weird fetish topic and it's not just purely there is some risk and we
want to survive.
The deeper topic, in future thinkers, you guys are looking here at the future of economy
and the future of sense making, the future of technology, the future of healthcare, the
future of ethics, the future of lots of things and in the presence of exponential technology
that makes possible changing things that were never possible to change before.
What is the guiding basis for how we utilize that technological power well and make good
choices in the presence of that?
We've always had a sense that human nature is a certain thing fixed kind of genetically
or at whatever level it's fixed and then we're looking at good behavior within that
framework and as soon as we're at the level of genetic engineering, humans as a real,
at least thought experiment could be a real possible technology, we could work to genetically
engineer all people to be sociopathic hypercomputers who didn't feel badly about winning at win-lose
games at all and we're optimized for it.
We could work to ship our brains into AIs that could take us even further in that direction
of being able to win those games.
We could engineer aggression out of ourselves completely and so then we start to say like,
wow, those are really different realities.
What do we want?
Well, we want to win the game.
Why do we want to win?
There's an assumed game that we're playing against whomever, right?
Some in-group is playing against some out-group and whether it's US versus Russia versus China
or whatever in-group out-group dynamic, we say, well, what happens when you keep playing
that in-group out-group game forever?
Well, those games have always caused harm.
They have a narrow set of metrics that define a win and everything outside of those metrics is
externality.
So when we're playing a win-lose game, whether it's person on person or team on team and
whether the team is a tribe or a country or a company or a race or whatever it is, right?
The in-group competing against an out-group for some definition of a win, we're directly
seeking to harm each other, right, to cause the lose to each other and we're also indirectly
causing harm to the commons, which is we're competing for the extraction of scarce resource.
We are externalizing cost, pollution, whatever it is to the commons.
If we're having, if we're competing militarily, there's a harm that comes from warfare, right?
We're polluting the information ecology through disinformation to be able to maintain the
strategic competitive advantage of some information we have.
So, level risk games necessarily cause harm.
So, if you keep increasing your harm-causing capacity with technology and technology that
makes better technology, it makes better technology.
So exponentially, exponential harm-causing eventually taps out, right?
Eventually, it exceeds the capacity of the playing field to handle and the level of harm
is no longer viable.
And it's an important thing to think about that.
When we think about the risks of weaponized drones or CRISPR bio warfare or any of the
like really dreadful things that are very new technologies that we could never do before,
they're not really different in fundamental type than the things that have always sucked.
Like when we first came out with catapults versus not having those or cannons or whatever it is,
they're just a lot more powerful of the things that have always sucked.
But it's important to get that we have been murdering other people in mass and this thing
called war is a reasonable way to deal with the difference or to get ahead for a long time,
for the whole history of the thing we call civilization.
And we have been unrenewably, you know, plants out of the ground in terms of agriculture or
cutting trees or whatever in ways that lead to desertification for thousands of years.
All the early civilizations that don't exist anymore don't exist anymore because they actually
led to their own self termination in really critical ways.
So it's not like war and environmental destruction and et cetera is a new topic.
It's just, if the Mayans fell or the Byzantine or the Mesopotamian or the Roman,
even the Roman Empire fell, it wasn't everything.
There was a lot, but it wasn't everything.
But when you have the in group, out group dynamics keep getting larger.
Tribe to groups of tribes to villages, fiefdom, kingdom, nation state, global economic trading
block. So as to be able to compete with a larger team that, you know, keeps having the incentives
to do those with larger weaponry, extraction tech, externalization tech, narrative tech,
information and disinformation tech, you get to a point where you have, like we have today,
a completely globally interconnected supply chain and globally interconnected civilization
dynamics because of scale, where the collapse ends up being really a collapse of everything.
And the level of warfare possible can actually make a non habitable biosphere, right?
Can actually not just be catastrophic for a local people, which previous wars always were,
but catastrophic for people. And so yet that rival risk games have always caused the behaviors of
humans that have sucked, but exponential suck is existential. That's an important way of thinking
of it. And that means that we have to be different than we have ever been in the history of the thing
we call civilization to simply not extinct ourselves. Because the way we have always been
has been a smaller scale of the same thing that at this scale is now extinctionary.
That's a big deal because it means that the solutions we're looking for do not look like
previous best practices, because those were practices that how to win it when lose games,
where winning at when lose games is now the omni lose, lose generator, right? It is now the thing
that we can't keep doing. So we don't like to think this deeply about things. We like to take
whatever things have been like the good best practices and figure out how to like iterate a
tiny bit and run with those things, except the entire tool set of best practices we have are
actually why we're at the brink of a gazillion different x-risk scenarios that are the result
of using those tool sets. And so words like capitalism and science and technology and democracy
are like our favorite words because they give us a lot of rad shit they did. They also created a
heap of problems and problems are now catastrophic in scale where the solutions need to be new stuff.
Now people freak out because if you say something other than capitalism, they think that you mean
communism and you want to take their stuff and have the state force everyone to do shitty jobs.
And if you say something other than democracy, again, it's assumed that it's going to be like some
kind of fascist terrible thing and something other than science means like regressive religion. No.
I want to be very clear that I'm not proposing systems that suck worse than the current systems
we have. I'm proposing deeper level of insights and deeper level of what we would actually call
innovation and novelty than have been on the table so far. So for instance, democracy. You know,
when Churchill said democracy is the single worst form of governance ever created,
saved for all the other forms, he was saying something very, very deep, which is democracy is
the best form of government we've ever created. And it's fucking terrible. But all the other ones
are even worse because the idea of government or governance is this really tricky thing where we're
trying to get lots of different humans to cooperate or agree or make choices together. And like we
just suck at that. And he was admitting something very important and true. And Jefferson said similar
things, which is we were able to get a lot of people to care about each other, to see through
each other's perspective, to make agreements if it's a tiny number of people. And this was tribes,
the history of tribes. And that's why they kept out at a very small size was above the size at
which you could care about everybody, know about what was going on for everybody, factor their
insights, share the same base reality where if you heard anyone, you were directly hurting someone
that you've lived with and loved and cared about. As soon as you start getting to a size where you
can hurt people without knowing it anonymously through some supply chain action or voting on
something or whatever, it starts to become a totally different reality. Anonymous people and
we're willing to give up some freedoms for people we also depend on and care about and have this
close bonding with. But as soon as we get to larger than tribe dynamics, we have had a real hard
time doing that in any way that doesn't disenfranchise lots of people. We have democracy that says,
okay, there's no way everybody's going to agree on anything, but we still have to be able to move
forward and decide if we make a road or not or go to war or not or whatever it is. So let's come up
with a proposition of something to do and let's at least see that more people like it than don't
like it. At least it seems to represent the majority of thinking. That seems like a reasonable
idea. And whether we have a representative or not or it's a 67% majority or 51% or a voting
currency, they're all different versions but basically have the same idea. But let me explain
something about how bad the idea actually is, the catastrophic problems that it creates for
democracy will stop being like this really wonderful word. And it doesn't mean we don't
give it a do for the beautiful place that it's served in history. It's just, it is a place that
is in the rear view mirror in terms of if it continues in the forefront, we actually can't
navigate. It's not an adequate tool for the types of issues we need to navigate. And again,
remember, I'm not going to propose any other system ever proposed, proposed things that don't even
sound like governance, but that sound like a different method of individual and collective
sense making and choice making. But democracy is a process where somebody or somebody's make
a proposition of something, right? We're going to build the bridge this way or go to war, whatever
it is. And they make a proposition to benefit something that they're aware of, that they care
about. But they're not aware of everything and they don't care about everything that's connected
equally. So other people realize, hey, the thing that you want to do is going to fuck stuff up
that I care about that bridge that you want to build so that you can get across the river without
driving all the way around is going to kill all the owls in this area and mess up some fisheries.
And I care about that, owls and fisheries. And other people are like, fuck you and the
environmental owl fisheries stuff, like we need to get to work. And what you have now is a basis
where if the proposition goes through, it will benefit some things and harm other things. And
if it doesn't go through, the thing that would have been benefited now isn't benefited, but the
thing that would have been harmed now isn't harmed. And so you will get an in group of people that
care about the one set more, who then banned against the out group of the people who care about the
other thing more. This will always drive polarization. And eventually the polarization
becomes radicalization. And we didn't even try in the process to figure out what a good proposition
that might do a better job of meeting everybody's needs was. We didn't even try and do a context
map of what are all the interconnected issues here? What would a good design that everybody might
like look like? And can we even try to find a synergistic satisfied? That's not even part of
the conversation, right? Maybe rather than make the bridge there, we could make the bridge just
a slightly different area and there's no owls there. Maybe we can move the owls. Maybe we can
use pontoon boats. Maybe we don't even need to make a bridge because all the transportation
back and forth is for one company and we can just move the company's headquarters. Maybe,
but the sense making to inform the choice making is not a part of the governance process.
So if I'm making choices blind or mostly blind based on a tiny part of the optics,
then other people who have other optics are like, wait, that's not a good choice. And right now,
we both know that if one eye shows something, but my other eye shows something else, I want to
pay attention to both eyes. I don't want them in a game theoretic relationship with each other,
right? They do parallax and give me depth perception and my eyes and my ears don't want to make
each other the same. They actually really want to do different functions, but they also want to
pay attention to each other because if I hear something and I think that it was over there,
I'm going to go away from it, but my eyes tell me it's actually somewhere else. Like I want to
pay attention to all of my sense making. So my brain is doing this really interesting process of
taking all of this different sensory information, putting it together and trying to pay attention
