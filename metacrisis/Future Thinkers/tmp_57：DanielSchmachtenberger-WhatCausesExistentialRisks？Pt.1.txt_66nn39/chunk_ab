all of it to make a choice that's informed by all that sense making. We have never been able
to think about governance processes like this, where we start with how would a group of people
that are inter affecting each other that are inter affecting within a particular context
as sense making nodes be able to share their sense making in a way that could create parallax.
It could actually synthesize into a picture that could create design criteria of what is actually
meaningful, good, desired, etc. by everybody that could work towards progressively better
synergistic satisfiers that are based less on theory of trade-offs that will always create
some benefit at some harm, which will lead to some in groups fighting some out groups that will
lead to increased seeking of power on both sides that will eventually turn into a catastrophic
self-terminating scenario. And when we look at it, we see that this type of scenario has always
led to left, right polarization that eventually becomes radicalization that ends in war to
stabilize, but you can't keep having war with exponential tech where non-state actors have
existential level tech. You just can't keep doing that. And so the thing that we've always done,
we can't ever do anymore. This is a big deal, but we also are able to think about how do brains
put the information from eyes and ears together? How do a bunch of neurons come together on neural
networks to make sense in a way that none of them individually do? How do 50 trillion cells
as autonomous entities operate in your body in a way that is both good for them as individuals
and good for the whole simultaneously, where they are neither operating at their own benefit,
at the expense of the ones that they depend on, nor are they damaging themselves for the other,
like they are really in a kind of optimized symbiosis because the system depends on them and
they depend on the system. And that means that they depend on each other and the system and etc.
Can we start to study these things in a different way that gives us novel insights into how to be
able to have higher level organization, collective intelligence, collective adaptive capacity,
collective sense-making, actuation capacity? And the answer is yes, we can. And we can see that
that's not the thing that we've called democracy. The thing that we've called democracy, some process
of a proposition based on some very limited sense-making with some majority that is always
going to lead to polarization, that's going to lead to the group that doesn't get it feeling
disenfranchised and then having antipathy against the group as a whole and then the warfare that
occurs between them of whatever kind, whether info warfare or actual warfare. And so the reason I'm
bringing this up is because like democracy was great compared to just like a terrible fascist
dictator, right? But it definitely is not adequate to the complexity of the problems we have to solve,
nor can we continue to handle the kinds of polarization and problematic dynamics that
inexorably creates. The same is true with capitalism, the same is true with the thing that we call
science and tech, which is probably the hardest one, I'll get to that one last.
So the thing that we call capitalism, we all know the positive story there that it incents people
being self-responsible and being industrious and seeking to bring better products and services
to the market at a better value and those who do will get ahead and they should get ahead because
they're going to be good stewards of resource because they got the resource by bringing
products and services at a value that people cared about and etc. Like we know that story, right?
There's some truth to it, like there was some truth to democracy story and it served evolutionary
relevance and it most certainly can't take us the next set of steps. So, you know, for instance,
the thing that we call money and ownership, you can actually think of it related to governance as
a type of choice-making process. We don't think about deeply enough the nature of what these types
of structures are. If I have a bunch of money, it means I have concentrated choice-making capacity
because I can now want to make a choice that I can extend through a bunch of employees,
right? I can have a bunch of them working aligned with my sense-making on my behalf to increase
my actuator capacity or I can get physical resources to be able to build something that extends my
actuator capacity. And so, we recognize, oh, a system that ends up determining who has resource
and then the resource ends up being a way that some people are actually directing the choice-making
of other people is a choice-making system. And then we say, well, is that actually a shitty
choice-making system? Because the idea that those who have the money are better choice-makers for
the whole is just silly, right? Like it's just really silly. Even if someone did bring good
products or services to market effectively, that doesn't mean they're kids who inherited it do.
And it doesn't mean that I can't make money by extraction rather than production that is net
debasing the environment. And it doesn't mean that I didn't externalize. I figured I had
externalize more harm and cost to the commons than somebody else did, which drove my margins up. So,
I got more of it. And I didn't mean that I didn't do war profiteering, right? And so, we start to
realize, okay, well, that's just actually a shitty choice-making system. So, we stop even
thinking about what is the future of economics and the future of governance because the words are so
loaded that we just can't help ourselves but boot bad concepts when we think of those words.
So, we start thinking about, all right, us humans are making choices individually and in groups
based on information that we have towards some things that we value and hope to have happened,
right? So, how do we get better on value frameworks, what we really seek to benefit? How do we get
better on our sense-making processes? And how do we get better on our choice-making processes?
And what is the future of individual and collective value framework, sense-making, choice-making
processes? And that ends up being what obsolete the things we call economics and governance now.
Now, we come back to capitalism for a minute. Okay, so not only is it just not a great choice-making
system and not only does it end up actually being a pretty pathological choice-making system because
it's easier to extract than it is to actually produce and it's easier to rip off what someone
else worked really hard on than it is to work really hard on making the thing and it's easier to
externalize costs to the commons than not and etc. But we say, okay, it's not just that, it's that
we've got $70 trillion worth of economic activity, give or take, everyday trade hands,
spend on what you consider a dollar. Where pretty much all of that externalizes harm at some point
along the supply chain, meaning the physical goods economy required mining and turns it into trash on
the other side to the linear materials economy is destructive to the environment, causes pollution
along the whole thing, etc. Causes marketing to be able to drive it that is polluting people's minds
with a bunch of manufactured discontent and etc. Drives disinformation of some companies trying to
disinform other ones to maintain strategic advantage is running on dollars that are supported by
militaries. And you just think about the whole thing and like, wow, okay, even the things that I think
are good, like that just a movement of what those dollars are is externalizing harm in an
exponential way that is moving towards catastrophic tipping points. All right, well,
that can't work. And then even worse, we say, all right, let's take a look at Facebook for a minute
because Facebook, since last time we talked between Russia and Cambridge Analytica and
Tristan and etc. There's a lot more understanding of the problem of platforms that have a lot of
personal data, but also have their own agency with respect to you. And we say, okay, so Facebook
wants to maximize time on site, because they make money by selling marketing. And the more
users are on site, more often the more they can charge for the marketing. So they figure out,
they use very complex AI analytics, split testing, etc. to see exactly what will make
online stickiest for you possible. So you happen to feel really left out when you see pictures of
your friends doing shit without you. And that makes you click and look and it makes you feel
really bad, but they optimize the shit out of that in your feed, because it's what you actually
stay on. And other people, it's the hypernormal stimuli of the girls that are all airbrushed
and photoshopped that makes them stay on or the whatever it is, but it'll always be a hypernormal
stimuli. And if you think about the way that McDonald's wanted to make the most addictive
shit or Coca-Cola or Phillips Morris, if I am on the supply side of supply and demand,
I want to manufacture artificial demand. So I want to maximize lifetime value of a customer,
right? Lifetime revenue. And so if I can make you addicted to my stuff, that's the most profitable
thing. So when you look around at a society that has ubiquitous rampant addiction of almost all
kinds everywhere, you realize that's just good for business. That's good for GDP. And so we see
that Facebook is just by its own nature of doing what it's supposed to do, public company,
for you share responsibility to the shareholders, maximize profit, blah, blah, blah, is they're
going to maximize your time on site. And maximizing your time on site is going to work better by
hypernormal stimuli that make you addicted than by things that make you sovereign work because
you actually realize that your life is better when you get to fuck off Facebook and go hang
out with real people. And so it has to drive addiction and discontent and whatever else.
And it's really good at a bunch of crafty tricks for how to do that. And so we see then that
corresponding with that is the rise of bulimia and anorexia and all kinds of body dysmorphia.
We see that corresponding with it is depression and probability for suicide rates. We see that
the hypernormal stimuli of polarizing news grabs people more than like non-polarizing news because
fights in an evolutionary environment are really important things to pay attention to. They're
kind of hypernormal stimuli. And so the most polarizing names are going to show up on YouTube
videos and get the most traction. And you notice whenever there's a debate and it's supposed to
be a friendly debate, you look at the name of the YouTube video that gets the most shares and it's
so and so eviscerate, so and so, right? Yeah, destroys this person. Yeah. And that hypernormal
stimuli grabs us in the basal gangly in the worst way possible, makes the worst versions of us by
their fucking maximizes time on site. And so now you look at you say, okay, so the net result of
Facebook is increased radicalization in all directions, increased broken sense making,
like decreased sense making, increased echo chamber and antipathy at a level that will
increase both the probability of civil wars and world wars and everything else, and increased
teen depression and suicide rates and shopping addictions. And you're like, wow, this is fucking
evil, right? Like this is a really terrible thing. Does Facebook want to do that? We could almost
make up a conspiracy theory that it's like Facebook has optimized how to make the world
war says fast as possible. No, Facebook doesn't want to do that. Facebook is just trying to make a
dollar, right? And justify to itself that it should continue to exist. And it just happens to be that
how it makes a dollar has the externality of all that stuff in the same way that when Exxon was
making a dollar or when the military industrial complex or a sick care industry that makes money
when people are sick and not when they're healthy, like, fuck, okay, so look at that whole thing,
right? So capitalism is inexorably bound to perverse incentive everywhere. And
at an even deeper structural level, if we tried to fix that, we'd say, okay, we're competing to
own stuff. And the moment you own something, I no longer have access to it. Even if you're not
using it, even if you hardly ever use that, whatever it is, right drill, that you don't even
remember where you put it, I don't have access to it. And as a result, and there's a scarce amount
of the stuff. And the stuff that is more scarce, we make more valuable. And so then you own it,
I don't have access to it. But because you want to be able to provide for your future and whatever,
and there's uncertainty, you want to own all the shit that you can and pull it out of circulation,
put it in safes and security boxes. So that takes a lot of resource from the earth,
where everything you own is just bothering me, because it's being removed from my possible
access. So we are in a rival risk relationship with each other, because the nature of the good
itself is rival risk because of the ownership dynamic, right? And the valuation on scarce things
in particular. And everybody can say, well, it makes sense why we value scarce things, because
if there's enough for everybody, it's not doesn't have the same type of advantages. If there's not
enough for everyone to kind of make sense, except the problem, of course, is if we make decisions
based on an economic calculus, which we do, right? The CFO looks at the numbers and says,
no, this quarter, we have to do this. And they're only paying attention to the numbers they're
paying attention to. Then if air isn't worth anything, because there's enough of it for everybody,
and I can't increase my strategic competitive advantage over you by hoarding more air,
then air is worth nothing. Even though we all die without it, literally is valueless to us,
our economic calculus. So we will pollute the shit out of it and burn the shit out of it,
fill it full of CO2, pull the O2 out of it in the oxidizing of hydrocarbons, because we don't
factor it, because it's not scarce and doesn't provide competitive advantage. Whereas the gold
on the other hand, the gold we will fight wars over, we'll destroy environments and cut down
trees to mine it out that we're actually putting the oxygen in the air that we don't give a shit
about, so that we can put the gold in a safety deposit box that we don't ever look at and doesn't
get to do anything other than be noted on my balance sheet as some increased kind of competitive
advantage I have over you, because if there's not enough for everybody to have it, then I get
some competitive advantage by having a new doubt. And the value is proportional to that,
not the real physical asset of what that metal could do, which is why the metal is not actually
being used in electronics, it's being in gold bars and Fort Knox and whatever else, right?
This is all insane. And of course, we see that the moment that we make abundant things worthless,
even if they're the foundation of life, and we make scarce things worth a lot, even if they're
meaningless, then it creates a basis to artificially manufacture scarcity and avoid abundance everywhere.
If I have something I supply and I make it abundant, then all of a sudden it's worth nothing,
which is why if I make some software that I could give to the whole world for free ones,
I've made it, I just have to make enough money to have made it. No, no, no, I'm going to patent
protect it and come sue you if you figure out how to pirate it so that I can keep charging you,
even though I have no unit costs, right? I've actually solved the scarcity problem, and I'm
going to artificially manufacture scarcity to keep driving my balance sheet approach.
The Kimberly diamond mines burn and crush their diamonds because we thought diamonds were scarce,
we made the price high, then we realized they weren't scarce. If people had the price high,
didn't want that to be known, and you know, et cetera, et cetera, right? So if we want a world
of abundance for everybody, we can actually make it technologically engineer the scarcity,
I'll create abundance everywhere is a feasible thing to do. We can talk more about that later,
but you can't have an incentive on scarcity and engineer it out at the same time, right?
So now there's scarce stuff, we're competing for it, you own it, which means you possess it and
you remove my capacity for access. And so I want to own it faster than you own it. And so then I
know, and now we get into a tragedy of the commons and a multipolar trap saying, say I go cut down
a tree in the forest. I don't need that many trees right now, but you're in this other tribe and
you're going to go cut down some of the trees. I would like there to be a forest rather than a
total clear cut environment because forests are beautiful and they grow up with the forests and
I like the animals that are there. But I know that if I don't cut down the trees and I leave the
trees, there still won't be a forest because you're going to cut down the trees. And since you're
going to cut down the trees and the other guy, the other tribe knows that and they know that I'm
going to. And so we say, fuck it, I got to cut down all the trees as fast as I can because if
there are going to be no trees anyways, I might as well get them rather than them get them because
if they have increased economic power over me, they're going to use those pieces of wood against me
and more in an economic competition. And so now we all increase the rate of the destruction of the
forest as fast as we fucking can, even though we'd all like a forest because we're caught in a
multipolar prisoner's dilemma with no good way out. So the tragedy of the commons is a multipolar
trap. The arms race where we say, you know what, we should just not make weaponized AI. We should
just not do that. That's it. Everyone in the world thinks that the idea of facial recognition,
weaponized AI drones is a world that would be better not to live in. Like nobody wants to live
in that world because it doesn't matter how rich or powerful you are, you're not fucking safe to
a bunch of bad scenarios in that world. But we're all making them. Like everybody's advancing the
fucking tech. Why are we doing that? Why don't we just make a treaty not to do it? Because
if we make a treaty, we secretly know that the other guy's going to defect on it secretly. Because
if he gets the tech first, he'll rule the world and he'll beat us. And he knows that we're going
to defect on it. So either we make the treaty and we all defect on it secretly while agreeing to it
publicly, while trying to spy on the other guys, while trying to disinform their spies about what
we're actually doing, or we just don't even fucking agree to it. So we move forward towards
a world that increases the probability of extincting all of us every day
