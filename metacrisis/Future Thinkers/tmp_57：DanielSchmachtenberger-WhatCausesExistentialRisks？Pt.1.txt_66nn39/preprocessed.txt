Welcome to Future Thinker's podcast, episode number 57.
One of our favorite guests, Daniel Schmocktenberger from Neurohacker Collective, is back on the
show today.
Daniel is talking about a concept which she calls the generator functions of existential
risks.
So these are the deep causes from which the existential risks we face today come from.
This interview is in three parts that should be listened to in chronological order to
get the full understanding of the concept.
In part one, Daniel explains what generator functions of existential risks are and how
they affect the world.
In part two and three, he elaborates on the generator functions through a few different
lenses and reflects on what we need to do to solve these underlying issues.
To listen to all three parts and get all the links and show notes, go to futurethinkers.org
slash 57.
One quick announcement before we get into the show.
We are hiring a social media manager for Future Thinkers and our other podcast, Crypto Radio.
If you or somebody else you know might be interested in this position, go to futurethinkers.org
slash social to find out all the info.
This episode is brought to you by Kualia, a premium nitropic supplement that helps support
mental performance and brain health.
It's specifically designed to promote focus, support energy, mental clarity, mood, memory
and creativity.
UVA and I have both used it in the past, we really like it and we actually met the founders
and interviewed them on Future Thinkers and you can check out those interviews through
one of our favorites at futurethinkers.org slash Daniel and futurethinkers.org slash
Jordan.
They've got a new formula up called Kualia Mind.
It's got more natural ingredients and you can get it at futurethinkers.org slash brain
hack and you can get 10% off if you use the code future.
All right, let's get into the show.
Welcome to futurethinkers.org, a podcast about the evolution of technology, society and consciousness.
I'm Mike Gilliland and I'm UVA Ivanova.
If you're new to the show and you'd like a list of our top episodes and resources, go
to futurethinkers.org slash start.
And if you like our podcast, leave us a rating and a review on iTunes and elsewhere.
It really helps others find the show and we really appreciate it.
In one of our previous episodes with you, we spoke about how to assess and prevent existential
risks and we got a lot of positive feedback on that episode.
People really loved it and I know you've spent a lot of time thinking about the subject
since then and you've refined your position.
So I wanted to ask you, what is your current thinking on existential risks and how to deal
with them?
All right, great.
I'm very happy to be back with both of you.
It's valuable for us to kind of go up a level when we're thinking about risks, existential
or just any kind of catastrophic risk.
It's not just a weird fetish topic and it's not just purely there is some risk and we
want to survive.
The deeper topic, in future thinkers, you guys are looking here at the future of economy
and the future of sense making, the future of technology, the future of healthcare, the
future of ethics, the future of lots of things and in the presence of exponential technology
that makes possible changing things that were never possible to change before.
What is the guiding basis for how we utilize that technological power well and make good
choices in the presence of that?
We've always had a sense that human nature is a certain thing fixed kind of genetically
or at whatever level it's fixed and then we're looking at good behavior within that
framework and as soon as we're at the level of genetic engineering, humans as a real,
at least thought experiment could be a real possible technology, we could work to genetically
engineer all people to be sociopathic hypercomputers who didn't feel badly about winning at win-lose
games at all and we're optimized for it.
We could work to ship our brains into AIs that could take us even further in that direction
of being able to win those games.
We could engineer aggression out of ourselves completely and so then we start to say like,
wow, those are really different realities.
What do we want?
Well, we want to win the game.
Why do we want to win?
There's an assumed game that we're playing against whomever, right?
Some in-group is playing against some out-group and whether it's US versus Russia versus China
or whatever in-group out-group dynamic, we say, well, what happens when you keep playing
that in-group out-group game forever?
Well, those games have always caused harm.
They have a narrow set of metrics that define a win and everything outside of those metrics is
externality.
So when we're playing a win-lose game, whether it's person on person or team on team and
whether the team is a tribe or a country or a company or a race or whatever it is, right?
The in-group competing against an out-group for some definition of a win, we're directly
seeking to harm each other, right, to cause the lose to each other and we're also indirectly
causing harm to the commons, which is we're competing for the extraction of scarce resource.
We are externalizing cost, pollution, whatever it is to the commons.
If we're having, if we're competing militarily, there's a harm that comes from warfare, right?
We're polluting the information ecology through disinformation to be able to maintain the
strategic competitive advantage of some information we have.
So, level risk games necessarily cause harm.
So, if you keep increasing your harm-causing capacity with technology and technology that
makes better technology, it makes better technology.
So exponentially, exponential harm-causing eventually taps out, right?
Eventually, it exceeds the capacity of the playing field to handle and the level of harm
is no longer viable.
And it's an important thing to think about that.
When we think about the risks of weaponized drones or CRISPR bio warfare or any of the
like really dreadful things that are very new technologies that we could never do before,
they're not really different in fundamental type than the things that have always sucked.
Like when we first came out with catapults versus not having those or cannons or whatever it is,
they're just a lot more powerful of the things that have always sucked.
But it's important to get that we have been murdering other people in mass and this thing
called war is a reasonable way to deal with the difference or to get ahead for a long time,
for the whole history of the thing we call civilization.
And we have been unrenewably, you know, plants out of the ground in terms of agriculture or
cutting trees or whatever in ways that lead to desertification for thousands of years.
All the early civilizations that don't exist anymore don't exist anymore because they actually
led to their own self termination in really critical ways.
So it's not like war and environmental destruction and et cetera is a new topic.
It's just, if the Mayans fell or the Byzantine or the Mesopotamian or the Roman,
even the Roman Empire fell, it wasn't everything.
There was a lot, but it wasn't everything.
But when you have the in group, out group dynamics keep getting larger.
Tribe to groups of tribes to villages, fiefdom, kingdom, nation state, global economic trading
block. So as to be able to compete with a larger team that, you know, keeps having the incentives
to do those with larger weaponry, extraction tech, externalization tech, narrative tech,
information and disinformation tech, you get to a point where you have, like we have today,
a completely globally interconnected supply chain and globally interconnected civilization
dynamics because of scale, where the collapse ends up being really a collapse of everything.
And the level of warfare possible can actually make a non habitable biosphere, right?
Can actually not just be catastrophic for a local people, which previous wars always were,
but catastrophic for people. And so yet that rival risk games have always caused the behaviors of
humans that have sucked, but exponential suck is existential. That's an important way of thinking
of it. And that means that we have to be different than we have ever been in the history of the thing
we call civilization to simply not extinct ourselves. Because the way we have always been
has been a smaller scale of the same thing that at this scale is now extinctionary.
That's a big deal because it means that the solutions we're looking for do not look like
previous best practices, because those were practices that how to win it when lose games,
where winning at when lose games is now the omni lose, lose generator, right? It is now the thing
that we can't keep doing. So we don't like to think this deeply about things. We like to take
whatever things have been like the good best practices and figure out how to like iterate a
tiny bit and run with those things, except the entire tool set of best practices we have are
actually why we're at the brink of a gazillion different x-risk scenarios that are the result
of using those tool sets. And so words like capitalism and science and technology and democracy
are like our favorite words because they give us a lot of rad shit they did. They also created a
heap of problems and problems are now catastrophic in scale where the solutions need to be new stuff.
Now people freak out because if you say something other than capitalism, they think that you mean
communism and you want to take their stuff and have the state force everyone to do shitty jobs.
And if you say something other than democracy, again, it's assumed that it's going to be like some
kind of fascist terrible thing and something other than science means like regressive religion. No.
I want to be very clear that I'm not proposing systems that suck worse than the current systems
we have. I'm proposing deeper level of insights and deeper level of what we would actually call
innovation and novelty than have been on the table so far. So for instance, democracy. You know,
when Churchill said democracy is the single worst form of governance ever created,
saved for all the other forms, he was saying something very, very deep, which is democracy is
the best form of government we've ever created. And it's fucking terrible. But all the other ones
are even worse because the idea of government or governance is this really tricky thing where we're
trying to get lots of different humans to cooperate or agree or make choices together. And like we
just suck at that. And he was admitting something very important and true. And Jefferson said similar
things, which is we were able to get a lot of people to care about each other, to see through
each other's perspective, to make agreements if it's a tiny number of people. And this was tribes,
the history of tribes. And that's why they kept out at a very small size was above the size at
which you could care about everybody, know about what was going on for everybody, factor their
insights, share the same base reality where if you heard anyone, you were directly hurting someone
that you've lived with and loved and cared about. As soon as you start getting to a size where you
can hurt people without knowing it anonymously through some supply chain action or voting on
something or whatever, it starts to become a totally different reality. Anonymous people and
we're willing to give up some freedoms for people we also depend on and care about and have this
close bonding with. But as soon as we get to larger than tribe dynamics, we have had a real hard
time doing that in any way that doesn't disenfranchise lots of people. We have democracy that says,
okay, there's no way everybody's going to agree on anything, but we still have to be able to move
forward and decide if we make a road or not or go to war or not or whatever it is. So let's come up
with a proposition of something to do and let's at least see that more people like it than don't
like it. At least it seems to represent the majority of thinking. That seems like a reasonable
idea. And whether we have a representative or not or it's a 67% majority or 51% or a voting
currency, they're all different versions but basically have the same idea. But let me explain
something about how bad the idea actually is, the catastrophic problems that it creates for
democracy will stop being like this really wonderful word. And it doesn't mean we don't
give it a do for the beautiful place that it's served in history. It's just, it is a place that
is in the rear view mirror in terms of if it continues in the forefront, we actually can't
navigate. It's not an adequate tool for the types of issues we need to navigate. And again,
remember, I'm not going to propose any other system ever proposed, proposed things that don't even
sound like governance, but that sound like a different method of individual and collective
sense making and choice making. But democracy is a process where somebody or somebody's make
a proposition of something, right? We're going to build the bridge this way or go to war, whatever
it is. And they make a proposition to benefit something that they're aware of, that they care
about. But they're not aware of everything and they don't care about everything that's connected
equally. So other people realize, hey, the thing that you want to do is going to fuck stuff up
that I care about that bridge that you want to build so that you can get across the river without
driving all the way around is going to kill all the owls in this area and mess up some fisheries.
And I care about that, owls and fisheries. And other people are like, fuck you and the
environmental owl fisheries stuff, like we need to get to work. And what you have now is a basis
where if the proposition goes through, it will benefit some things and harm other things. And
if it doesn't go through, the thing that would have been benefited now isn't benefited, but the
thing that would have been harmed now isn't harmed. And so you will get an in group of people that
care about the one set more, who then banned against the out group of the people who care about the
other thing more. This will always drive polarization. And eventually the polarization
becomes radicalization. And we didn't even try in the process to figure out what a good proposition
that might do a better job of meeting everybody's needs was. We didn't even try and do a context
map of what are all the interconnected issues here? What would a good design that everybody might
like look like? And can we even try to find a synergistic satisfied? That's not even part of
the conversation, right? Maybe rather than make the bridge there, we could make the bridge just
a slightly different area and there's no owls there. Maybe we can move the owls. Maybe we can
use pontoon boats. Maybe we don't even need to make a bridge because all the transportation
back and forth is for one company and we can just move the company's headquarters. Maybe,
but the sense making to inform the choice making is not a part of the governance process.
So if I'm making choices blind or mostly blind based on a tiny part of the optics,
then other people who have other optics are like, wait, that's not a good choice. And right now,
we both know that if one eye shows something, but my other eye shows something else, I want to
pay attention to both eyes. I don't want them in a game theoretic relationship with each other,
right? They do parallax and give me depth perception and my eyes and my ears don't want to make
each other the same. They actually really want to do different functions, but they also want to
pay attention to each other because if I hear something and I think that it was over there,
I'm going to go away from it, but my eyes tell me it's actually somewhere else. Like I want to
pay attention to all of my sense making. So my brain is doing this really interesting process of
taking all of this different sensory information, putting it together and trying to pay attention
all of it to make a choice that's informed by all that sense making. We have never been able
to think about governance processes like this, where we start with how would a group of people
that are inter affecting each other that are inter affecting within a particular context
as sense making nodes be able to share their sense making in a way that could create parallax.
It could actually synthesize into a picture that could create design criteria of what is actually
meaningful, good, desired, etc. by everybody that could work towards progressively better
synergistic satisfiers that are based less on theory of trade-offs that will always create
some benefit at some harm, which will lead to some in groups fighting some out groups that will
lead to increased seeking of power on both sides that will eventually turn into a catastrophic
self-terminating scenario. And when we look at it, we see that this type of scenario has always
led to left, right polarization that eventually becomes radicalization that ends in war to
stabilize, but you can't keep having war with exponential tech where non-state actors have
existential level tech. You just can't keep doing that. And so the thing that we've always done,
we can't ever do anymore. This is a big deal, but we also are able to think about how do brains
put the information from eyes and ears together? How do a bunch of neurons come together on neural
networks to make sense in a way that none of them individually do? How do 50 trillion cells
as autonomous entities operate in your body in a way that is both good for them as individuals
and good for the whole simultaneously, where they are neither operating at their own benefit,
at the expense of the ones that they depend on, nor are they damaging themselves for the other,
like they are really in a kind of optimized symbiosis because the system depends on them and
they depend on the system. And that means that they depend on each other and the system and etc.
Can we start to study these things in a different way that gives us novel insights into how to be
able to have higher level organization, collective intelligence, collective adaptive capacity,
collective sense-making, actuation capacity? And the answer is yes, we can. And we can see that
that's not the thing that we've called democracy. The thing that we've called democracy, some process
of a proposition based on some very limited sense-making with some majority that is always
going to lead to polarization, that's going to lead to the group that doesn't get it feeling
disenfranchised and then having antipathy against the group as a whole and then the warfare that
occurs between them of whatever kind, whether info warfare or actual warfare. And so the reason I'm
bringing this up is because like democracy was great compared to just like a terrible fascist
dictator, right? But it definitely is not adequate to the complexity of the problems we have to solve,
nor can we continue to handle the kinds of polarization and problematic dynamics that
inexorably creates. The same is true with capitalism, the same is true with the thing that we call
science and tech, which is probably the hardest one, I'll get to that one last.
So the thing that we call capitalism, we all know the positive story there that it incents people
being self-responsible and being industrious and seeking to bring better products and services
to the market at a better value and those who do will get ahead and they should get ahead because
they're going to be good stewards of resource because they got the resource by bringing
products and services at a value that people cared about and etc. Like we know that story, right?
There's some truth to it, like there was some truth to democracy story and it served evolutionary
relevance and it most certainly can't take us the next set of steps. So, you know, for instance,
the thing that we call money and ownership, you can actually think of it related to governance as
a type of choice-making process. We don't think about deeply enough the nature of what these types
of structures are. If I have a bunch of money, it means I have concentrated choice-making capacity
because I can now want to make a choice that I can extend through a bunch of employees,
right? I can have a bunch of them working aligned with my sense-making on my behalf to increase
my actuator capacity or I can get physical resources to be able to build something that extends my
actuator capacity. And so, we recognize, oh, a system that ends up determining who has resource
and then the resource ends up being a way that some people are actually directing the choice-making
of other people is a choice-making system. And then we say, well, is that actually a shitty
choice-making system? Because the idea that those who have the money are better choice-makers for
the whole is just silly, right? Like it's just really silly. Even if someone did bring good
products or services to market effectively, that doesn't mean they're kids who inherited it do.
And it doesn't mean that I can't make money by extraction rather than production that is net
debasing the environment. And it doesn't mean that I didn't externalize. I figured I had
externalize more harm and cost to the commons than somebody else did, which drove my margins up. So,
I got more of it. And I didn't mean that I didn't do war profiteering, right? And so, we start to
realize, okay, well, that's just actually a shitty choice-making system. So, we stop even
thinking about what is the future of economics and the future of governance because the words are so
loaded that we just can't help ourselves but boot bad concepts when we think of those words.
So, we start thinking about, all right, us humans are making choices individually and in groups
based on information that we have towards some things that we value and hope to have happened,
right? So, how do we get better on value frameworks, what we really seek to benefit? How do we get
better on our sense-making processes? And how do we get better on our choice-making processes?
And what is the future of individual and collective value framework, sense-making, choice-making
processes? And that ends up being what obsolete the things we call economics and governance now.
Now, we come back to capitalism for a minute. Okay, so not only is it just not a great choice-making
system and not only does it end up actually being a pretty pathological choice-making system because
it's easier to extract than it is to actually produce and it's easier to rip off what someone
else worked really hard on than it is to work really hard on making the thing and it's easier to
externalize costs to the commons than not and etc. But we say, okay, it's not just that, it's that
we've got $70 trillion worth of economic activity, give or take, everyday trade hands,
spend on what you consider a dollar. Where pretty much all of that externalizes harm at some point
along the supply chain, meaning the physical goods economy required mining and turns it into trash on
the other side to the linear materials economy is destructive to the environment, causes pollution
along the whole thing, etc. Causes marketing to be able to drive it that is polluting people's minds
with a bunch of manufactured discontent and etc. Drives disinformation of some companies trying to
disinform other ones to maintain strategic advantage is running on dollars that are supported by
militaries. And you just think about the whole thing and like, wow, okay, even the things that I think
are good, like that just a movement of what those dollars are is externalizing harm in an
exponential way that is moving towards catastrophic tipping points. All right, well,
that can't work. And then even worse, we say, all right, let's take a look at Facebook for a minute
because Facebook, since last time we talked between Russia and Cambridge Analytica and
Tristan and etc. There's a lot more understanding of the problem of platforms that have a lot of
personal data, but also have their own agency with respect to you. And we say, okay, so Facebook
wants to maximize time on site, because they make money by selling marketing. And the more
users are on site, more often the more they can charge for the marketing. So they figure out,
they use very complex AI analytics, split testing, etc. to see exactly what will make
online stickiest for you possible. So you happen to feel really left out when you see pictures of
your friends doing shit without you. And that makes you click and look and it makes you feel
really bad, but they optimize the shit out of that in your feed, because it's what you actually
stay on. And other people, it's the hypernormal stimuli of the girls that are all airbrushed
and photoshopped that makes them stay on or the whatever it is, but it'll always be a hypernormal
stimuli. And if you think about the way that McDonald's wanted to make the most addictive
shit or Coca-Cola or Phillips Morris, if I am on the supply side of supply and demand,
I want to manufacture artificial demand. So I want to maximize lifetime value of a customer,
right? Lifetime revenue. And so if I can make you addicted to my stuff, that's the most profitable
thing. So when you look around at a society that has ubiquitous rampant addiction of almost all
kinds everywhere, you realize that's just good for business. That's good for GDP. And so we see
that Facebook is just by its own nature of doing what it's supposed to do, public company,
for you share responsibility to the shareholders, maximize profit, blah, blah, blah, is they're
going to maximize your time on site. And maximizing your time on site is going to work better by
hypernormal stimuli that make you addicted than by things that make you sovereign work because
you actually realize that your life is better when you get to fuck off Facebook and go hang
out with real people. And so it has to drive addiction and discontent and whatever else.
And it's really good at a bunch of crafty tricks for how to do that. And so we see then that
corresponding with that is the rise of bulimia and anorexia and all kinds of body dysmorphia.
We see that corresponding with it is depression and probability for suicide rates. We see that
the hypernormal stimuli of polarizing news grabs people more than like non-polarizing news because
fights in an evolutionary environment are really important things to pay attention to. They're
kind of hypernormal stimuli. And so the most polarizing names are going to show up on YouTube
videos and get the most traction. And you notice whenever there's a debate and it's supposed to
be a friendly debate, you look at the name of the YouTube video that gets the most shares and it's
so and so eviscerate, so and so, right? Yeah, destroys this person. Yeah. And that hypernormal
stimuli grabs us in the basal gangly in the worst way possible, makes the worst versions of us by
their fucking maximizes time on site. And so now you look at you say, okay, so the net result of
Facebook is increased radicalization in all directions, increased broken sense making,
like decreased sense making, increased echo chamber and antipathy at a level that will
increase both the probability of civil wars and world wars and everything else, and increased
teen depression and suicide rates and shopping addictions. And you're like, wow, this is fucking
evil, right? Like this is a really terrible thing. Does Facebook want to do that? We could almost
make up a conspiracy theory that it's like Facebook has optimized how to make the world
war says fast as possible. No, Facebook doesn't want to do that. Facebook is just trying to make a
dollar, right? And justify to itself that it should continue to exist. And it just happens to be that
how it makes a dollar has the externality of all that stuff in the same way that when Exxon was
making a dollar or when the military industrial complex or a sick care industry that makes money
when people are sick and not when they're healthy, like, fuck, okay, so look at that whole thing,
right? So capitalism is inexorably bound to perverse incentive everywhere. And
at an even deeper structural level, if we tried to fix that, we'd say, okay, we're competing to
own stuff. And the moment you own something, I no longer have access to it. Even if you're not
using it, even if you hardly ever use that, whatever it is, right drill, that you don't even
remember where you put it, I don't have access to it. And as a result, and there's a scarce amount
of the stuff. And the stuff that is more scarce, we make more valuable. And so then you own it,
I don't have access to it. But because you want to be able to provide for your future and whatever,
and there's uncertainty, you want to own all the shit that you can and pull it out of circulation,
put it in safes and security boxes. So that takes a lot of resource from the earth,
where everything you own is just bothering me, because it's being removed from my possible
access. So we are in a rival risk relationship with each other, because the nature of the good
itself is rival risk because of the ownership dynamic, right? And the valuation on scarce things
in particular. And everybody can say, well, it makes sense why we value scarce things, because
if there's enough for everybody, it's not doesn't have the same type of advantages. If there's not
enough for everyone to kind of make sense, except the problem, of course, is if we make decisions
based on an economic calculus, which we do, right? The CFO looks at the numbers and says,
no, this quarter, we have to do this. And they're only paying attention to the numbers they're
paying attention to. Then if air isn't worth anything, because there's enough of it for everybody,
and I can't increase my strategic competitive advantage over you by hoarding more air,
then air is worth nothing. Even though we all die without it, literally is valueless to us,
our economic calculus. So we will pollute the shit out of it and burn the shit out of it,
fill it full of CO2, pull the O2 out of it in the oxidizing of hydrocarbons, because we don't
factor it, because it's not scarce and doesn't provide competitive advantage. Whereas the gold
on the other hand, the gold we will fight wars over, we'll destroy environments and cut down
trees to mine it out that we're actually putting the oxygen in the air that we don't give a shit
about, so that we can put the gold in a safety deposit box that we don't ever look at and doesn't
get to do anything other than be noted on my balance sheet as some increased kind of competitive
advantage I have over you, because if there's not enough for everybody to have it, then I get
some competitive advantage by having a new doubt. And the value is proportional to that,
not the real physical asset of what that metal could do, which is why the metal is not actually
being used in electronics, it's being in gold bars and Fort Knox and whatever else, right?
This is all insane. And of course, we see that the moment that we make abundant things worthless,
even if they're the foundation of life, and we make scarce things worth a lot, even if they're
meaningless, then it creates a basis to artificially manufacture scarcity and avoid abundance everywhere.
If I have something I supply and I make it abundant, then all of a sudden it's worth nothing,
which is why if I make some software that I could give to the whole world for free ones,
I've made it, I just have to make enough money to have made it. No, no, no, I'm going to patent
protect it and come sue you if you figure out how to pirate it so that I can keep charging you,
even though I have no unit costs, right? I've actually solved the scarcity problem, and I'm
going to artificially manufacture scarcity to keep driving my balance sheet approach.
The Kimberly diamond mines burn and crush their diamonds because we thought diamonds were scarce,
we made the price high, then we realized they weren't scarce. If people had the price high,
didn't want that to be known, and you know, et cetera, et cetera, right? So if we want a world
of abundance for everybody, we can actually make it technologically engineer the scarcity,
I'll create abundance everywhere is a feasible thing to do. We can talk more about that later,
but you can't have an incentive on scarcity and engineer it out at the same time, right?
So now there's scarce stuff, we're competing for it, you own it, which means you possess it and
you remove my capacity for access. And so I want to own it faster than you own it. And so then I
know, and now we get into a tragedy of the commons and a multipolar trap saying, say I go cut down
a tree in the forest. I don't need that many trees right now, but you're in this other tribe and
you're going to go cut down some of the trees. I would like there to be a forest rather than a
total clear cut environment because forests are beautiful and they grow up with the forests and
I like the animals that are there. But I know that if I don't cut down the trees and I leave the
trees, there still won't be a forest because you're going to cut down the trees. And since you're
going to cut down the trees and the other guy, the other tribe knows that and they know that I'm
going to. And so we say, fuck it, I got to cut down all the trees as fast as I can because if
there are going to be no trees anyways, I might as well get them rather than them get them because
if they have increased economic power over me, they're going to use those pieces of wood against me
and more in an economic competition. And so now we all increase the rate of the destruction of the
forest as fast as we fucking can, even though we'd all like a forest because we're caught in a
multipolar prisoner's dilemma with no good way out. So the tragedy of the commons is a multipolar
trap. The arms race where we say, you know what, we should just not make weaponized AI. We should
just not do that. That's it. Everyone in the world thinks that the idea of facial recognition,
weaponized AI drones is a world that would be better not to live in. Like nobody wants to live
in that world because it doesn't matter how rich or powerful you are, you're not fucking safe to
a bunch of bad scenarios in that world. But we're all making them. Like everybody's advancing the
fucking tech. Why are we doing that? Why don't we just make a treaty not to do it? Because
if we make a treaty, we secretly know that the other guy's going to defect on it secretly. Because
if he gets the tech first, he'll rule the world and he'll beat us. And he knows that we're going
to defect on it. So either we make the treaty and we all defect on it secretly while agreeing to it
publicly, while trying to spy on the other guys, while trying to disinform their spies about what
we're actually doing, or we just don't even fucking agree to it. So we move forward towards
a world that increases the probability of extincting all of us every day
in this scenario, the tragedy of the commons, these multipolar traps. And we'll come back to this.
I was on capitalism, but I'm going to come back to this multipolar trap. So please remind me to do
it. One of the things we have to solve at a generator function level is multipolar traps as a
category, meaning all instances of them categorically, because a multipolar trap basically means
where the local optimum for an individual agent, if they pursue it, leads to the global minimum
for the whole. But if I don't make the nukes or the AI or take the tree down, I'll get fucked
right now. If I do it, I don't get fucked right now. But as we all do it, we all get worse fucked
later. So you made a scenario where the incentive of the agents, short-term locally,
is directly against the long-term global well-being. That is the world writ large right now, and
capitalism is inexorably connected to that, because it drives these rival risk dynamics,
and rival risk dynamics are the basis of multipolar traps. And so I could just sound depressing,
except there are actually solutions to all of these things. There is a basis for how we deal with
physical stuff. It is not you owning some scarce thing and removing my access. We all know examples
of it. We know that when you go to the grocery store and you use a shopping cart, you don't bring
your own shopping cart that you own, which would be a major pain in the ass. And I don't do that.
You have access to a cart where there's enough carts that during peak hours, the busiest hours,
there's enough carts for everybody, and enough leftover for repairs. So your access to the cart
does not decrease my access to carts. So I'm not upset that you got a cart. We're not in any competition
over carts. And because I only need enough for there to be enough during peak time, which is maybe
200 people, I only need 200 carts, 220 carts. Even though that store might service 10,000 people a
month. So think about 200 carts versus 10,000 carts, how much less resource it is from the
environment, how much more efficient it is. So now you start saying, all right, well, let's look at
other places where this thing could happen. We say, well, we've owned cars as a good. And when you
own a car, I no longer have access to it. But then you're mostly just going to leave it sitting and
hardly ever use it. You'll use it now and again, but it's going to spend 95% of its life just sitting
places. And as a result of that, there's a lot of fucking cars to not provide that much transportation.
And that's a lot of metals taken out of the earth and a lot of plastics and a lot of, you know,
actual environmental costs to be able to make those to have most of them never in use.
Now you start to say, okay, well, we look at car sharing like Uber and Lyft and whatever,
and we start moving there from a possession of a good to an access of a service. That's pretty cool.
And we still got to pay for it. It's still kind of shitty because there's not enough of it to have
given me good access everywhere. But it's pretty easy to imagine that that thing takes over, right?
And then you use something like blockchain to disintermediate the central company that's pulling
the profits out of it. And so now it's cheaper access for everybody and puts more resource back
into the quality of the transportation. And then it becomes self-driving cars. So it doesn't actually
even have that cost. It's just a self-maintaining dynamic. And you say, okay, now it takes a tiny
fraction of the metals coming out of the earth to provide higher quality transportation units for
everyone, where you having access to transportation as a Commonwealth service does not decrease my
access to transportation as a Commonwealth service. But when you use transportation to go to school and
learn stuff or to go to a maker's studio and make stuff or go to a music studio and make stuff,
you're going to make stuff that also becomes part of a Commonwealth and riches the Commons that I
have access to. So we move from a rival risk ownership of scarce goods to an anti-rival risk
access to shared Commonwealth resources. Now we went from rival risk, not just to non-rival risk,
meaning uncoupled, right? And in rival risk, we're anti-coupled. Your well-being is directly
against mine. But now to anti-rival risk, which is a coupled good, where as you have access to more
resources that make you more generative, what you generate enriches the Commons that I have access to.
So I am engendered. I am incented to want to give you the maximum to support you having the maximum
access to generative resources. Now I'm not explaining all of what the future of the economic
system looks like right now, but to just start giving a sense that when we think about the problems
of capitalism, there have been problems associated with it forever. But the scale of the problems is
just more catastrophic now. But I'm also sharing examples of ways that we can start shifting some
of the things that we couldn't shift previously, some of the things that neither Mark nor Smith
had available to them. And this is very interesting. We have to shift off these systems and we can at
the same time. And this is to me a very interesting developmental insight when I look at biology in
particular, is that if we look at the 40 weeks of a baby in utero, it can't actually, we've talked
about this before, but I'll bring it up again in this context, it couldn't be born much earlier.
It would be pre-mean and without an ICU for whatever it would die. But it also couldn't stay
much longer. If we get too big and never be able to be born, kill the mom and kill itself,
it comes out in a pretty narrow window where both for the first time can, it actually now has the
capacities and has to can't stay in the nail longer. And it's interesting that this, it's got 40 weeks
on one growth path, right? It's growing the whole time. There's a growth curve. It's not going to
stay on that growth curve forever. If we tried to forecast its future and just continue the progression
of 40 weeks, around 50 weeks, it kills itself in the mom, right? So that's not the thing. It goes
through this discreet non-linear phase shift that if I had never seen it before, I'd have no idea how
to predict. And out of the birth canal, umbilical cord cuts, respiration through the lungs, stuff coming
in through the mouth rather than the belly button, everything's different. And it does it in a way
that's unprecedented to the whole 40 weeks of its existence previously. So if I tried to plot the
curve of what has been, it is not bad. And it does it both when it has to and can. And if we look at
any type of developmental phase, chicken developing inside of an egg, if it tried to come out earlier
is still goo. If it tried to stay in later, it starts. If we look at a caterpillar to butterfly
through chrysalis, same thing. If it tried to just keep eating, it would eat itself to extinction.
If it tried to go into the chrysalis earlier, it doesn't have enough resources to make a butterfly,
it would die in the chrysalis as partial goo. This is really interesting that when we look at
discrete nonlinear phase shifts where there's one phase, right, the caterpillars getting bigger and
bigger and bigger, eating everything in its environment. And if I just forecasted, it keeps
being that I forecasted eats all the environment and it does. So that's not what happens. It does this
really different thing that is not an extension of the previous curve. And so when we try to take
our capitalism curve, our nationalism curve, our science and tech curve and keep extending them,
it's just fucking silly. And that's why we come up with silly stuff like it asymptotes into infinity
with the singularity. It's just, you know, on one side, if we take all the things that seem like
they're good, then we asymptote into a singularity. If we take the things that look bad, then it just
goes into self extinction. It's neither of those curves, right? Because shit is getting exponentially
more powerful, which means better and worse at the same time. But that means it's neither of
those curves. That means that this phase is just coming to an end. It's destabilizing. The things
that are getting exponentially worse will end the phase before the things that are getting
exponentially better will change the nature of those things. But that means that what we're
actually going to get something altogether other if it is other than extinction. And
it's very interesting that we're at the place where these types of dynamics both have to change
right now because we have catastrophic level tech that we never had to have the same things that
always sucked be unsurvivable and can change because we actually have the level of insights that
make possible, things that are different at a deep enough axiomatic level that it's kind of like
when you've got a bunch of cells competing against each other and then there's this metamorphosis
where now they're inside of a slime mold and they are part of a multi-celled organism where
they're participating together as part of a shared identity. That's like a really deep level of
reorganization and people going from that shift as separate, rival-risk, competitive agents,
modeling themselves as apex predators, trying to compete with each other to be the most apex
predator and predate the environment. So we modeled ourselves as apex predators, right? That was an
easy thing to do. We look around at nature and we're like, we're kind of more like lions than
gazelles. We look a lot like chimpanzees. They're kind of badass hunter apex predator. They coordinate
et cetera. So we're like that. Cool. We'll do the apex predator thing. So we both want to see where
we can get in the dominance hierarchies. We get to add the prestige hierarchy, but
how we do within our tribe, then our tribe as a whole, our species as a whole relative to the
other species is in this kind of apex position. There's a reason why we cannot think that way
anymore. And again, we felt this way forever and we cannot think this way anymore. When I say we
felt this way forever, I don't mean every indigenous tribe felt this way because they didn't, right?
They had a web of life and were merely a strand and in types of thoughts. But the ones that thought
this way became more apex predators and killed those other people. And so it was effective to
try and do this apex predator thing up till now, but now it also destroys everything. So again,
the thing that has been adaptive is now anti-adaptive, which is why the thoughts that have made us win
are the thoughts that make us extinct now. And so if I think an apex predator, how its ability
to be a predator, its ability to kill other things and compete with the other predators for
whose most badass is pretty fixed and it's pretty symmetrically bound with its environment. So
lions can't get better at killing faster than gazelles get better at running. And so they
co-evolve where the lions slowly get a little bit better at some things and the gazelles also get
better at other things because if the lions just rapidly got way better, they'd kill all the gazelles
and then debase their own capacity to keep living and then they'd be extinct. If great whites could
make my alone drift nets and just take all the fucking fish out of the ocean and reproduce at
an exponential rate like they would have already self-induced their extinction, but they can't.
All the years, the great whites never got a drift net, but we have drift nets and we have
nuclear weapons and we have D9s and we have the ability to technologically extend our predatory
capacity and to do so in a way that is exponential and that makes us completely asymmetric with the
environment that we depend on. So again, I come back to the lion and I say, all right,
the most badass lion or the most badass gorilla is not like 10x more badass than the next most
badass gorilla. It's like marginally better, right? Like marginally can win at a fight and only for a
pretty short period of time before the next guy takes him. But then you look at like a Putin
or a Trump and you say, how much military capacity does that one person have to bear if
they wanted to or economic capacity compared to say a homeless guy? And you look at the
spread and you're like, oh, this is a very different distribution of power than any other species has.
Other species did not have a million X power dynamics within the same species or billion X,
like it was freaking tremendous or that much more power relative to their environment. So if the
lions could get technologically more advanced in their predation faster than gazelles could,
it would debase the stability of the entire ecosystem. The thing to realize is like,
if a cancer cell starts replicating in a way that's good for it, it's actually getting more
sugar and replicating faster inside the body. If it keeps doing that, it kills its host and kills
itself. It is ultimately suicidal. Its own short term success is suicidal. Viruses that kill people
too quickly don't propagate for very long because they kill their host and they don't get a chance
to propagate. The viruses that are less lethal end up being the ones that get selected for over a
longer period of time because they get a chance to propagate. So if there was a species that was
so good at hunting that it killed everything in its environment, then it would go extinct.
So it's not the most competitive advantage that makes it through. It's self stabilizing ecosystems
that make it through. This is such a way more complete understanding of evolution, which is
individuals within a species don't make it through because they wouldn't have survived without the
whole species. And species don't even make it through because they wouldn't survive without
other species. Whole evolutionary self stabilizing niches make it through. That's fucking important,
right? Yeah, this whole idea of survival of the fittest is kind of challenged with this concept
because at no point is survival of the fittest taking the whole system into account going forward
and infinitely. It has to go through that phase shift. Yeah, survival of the fittest was something
that had a local truth but was not the only global phenomena that was operating because there was
also a tremendous amount of cooperation that was happening and cooperation within members of a species
with each other and between species and interdependence on each other. But again, it was like almost
the idea of competition, it's a hypernormal stimuli. It was like an early hypernormal stimuli
hijack like sugar and porn and airbrush pictures and likes on Facebook because in an
evolutionary environment, fights stand out even though they're not mostly what's happening,
right? Like mostly if I am in a forest, there's a gazillion interactions happening every second of
aerobic and anaerobic soil bacteria having a relationship with each other and gas exchange
between me and the plants. It's just boring, but it's like almost everything. But then I see a
couple lions fighting and like, oh shit, that's really interesting, right? Survival of the fittest.
So there is this hypernormal stimuli that made us actually misemphasize what was happening
as a part of the phenomena that was not all of the phenomena,
misemphasize it. Now there's also this thing that is we're moving forward right now.
The way we have been applying that thinking, which is that some individual agents or some
in groups, countries, companies, races, whatever, some in groups can be more fit to survive than
others through better militaries or better economic extraction tech or better info and
narrative tech. That has always been true. I'm not criticizing that that was always true and
even necessary because if one tribe killed another tribe and their life got better because now the
other tribe wasn't competing for pigs with them and now they got all the pigs and they got like
all the stone tools that their tribe had made, whatever they're like, oh shit, I realize that
this like killing other tribes thing is actually a pretty good evolutionary strategy. Now all the
other tribes have to build militaries or die by default. The win-lose game becomes obligate.
So one, the win-lose game worked. It actually worked if you were good at it and two, it was
obligate, which is if you didn't do it, you got killed by Genghis Khan or Alexander the
Great or whoever the fuck it was. When we look at cultures that did not focus on militaries but
focused on the arts and humanities and education and healthcare and et cetera that out competed
the other cultures in terms of quality of life, but that wasn't where the thing actually got decided.
They all got murdered and the really effective murdering cultures combined and combined and
made it through and that's us today. And yet the tools of murdering and the tools of environmental
extraction are going up and up until we're at a level where the playing field just cannot handle
that game anymore. You can't keep extracting more stuff from the environment when you've already
got peak resource, when you've got the biodiversity issues and species extinction issues and et cetera.
You can't keep polluting an environment when you've got dead zones in the ocean from nitrogen
runoff and CO2 levels in the air and et cetera getting to the point of cataclysm and you can't
keep doing increasing military tech following exponential tech curves where then non-state
actors can have fully catastrophic level tech and you can't even monitor it. You just can't keep
playing that game. And so this is the gist is that the thing that has always defined the game is that
it's always been a rival risk game theoretic environment and the rival risk game theoretic
environment if it can produce tech that keeps increasing will always self-terminate at a point
and we just happen to be in the eminence of that point. So this is the first generator function of
ex-risk. Now we tie this all the way back to the beginning of the conversation and I obviously got
long-winded. The beginning of the conversation we said why are we focusing on risk? Well if we're
focused on how do we design a civilization that is actually good, it's beautiful, it's desirable.
Those are hard terms and we'll get to that in a minute. That starts to get to some of the inadequacy
of the thing we call science right now and it's incommensurability with ethics and we'll get to
that. But what does beautiful civilization look like? Well the first thing we can say kind of
easily is that it doesn't self-terminate. If it self-terminates we can mostly all agree that's
actually not a desirable thing. So if it is inexorably self-terminating it is structurally
self-terminating not just kind of like one little accent that we can solve but over-determined through
many vectors because of underlying generator functions that's not a good civilization design.
So the first design criteria of an effective civilization is that it's not self-terminating.
So then we say well what are the things that cause self-termination and what we find is that even
though there are a gazillion different ways that it can express, ways that it can actually happen,
it's from some very deep underlying dynamics that if we understand those and we solve them at the
dynamics level we fix all of them. So the first one is this topic we've been talking about and we
talked about previously which is that rival risk games multiplied by exponential technology
self-terminate because rival risk games cause harm with power and more power ends up being more
harm until it's more than is hand-able by the playing field. We got that. So exponential tech
whether we're looking at a scenario of everything getting fucked up by AI or by biowarfare or by
nanotech stuff or by so many different types of scenarios those are all the same types of
choices humans have been making just with those exponential powers at it. Given that we cannot
put those technologies away we cannot get the world to stop making them much as I often wish we could.
We either figure out how to move from a rival risk to an anti-rival risk environment that is
developing and deploying those technologies or we self-terminate. So this is the first design
criteria is that we have to create rigorously anti-rival risk environments. It doesn't end
up being all of it. I'll do two generator functions that's one and that is what we see in terms of
all of the either exponential tech risks or war risks or economic collapse leading to failed state
scenario risks they all come from things like that. Now all of the like environmental biosphere
collapse stuff is also related to tech getting bigger right as we're fishing out more fish as
we're putting more CO2 it also relates to that but it's a slightly different thing that we look at
which is whether we're talking about CO2 in the air or mercury in the air or the water or
microplastics in the water or a continent of plastic in the ocean or nitrogen effluent in the
river deltas or those are all toxicity dynamics those are all basically stuff that has accumulated
somewhere that it shouldn't have right we call that pollution. So we don't need to solve the dead
zones issue or the CO2 issue or that we have to solve all of those categorically that we're not
creating accumulation dynamics and on the other side of that same coin is depletion dynamics so
cutting down all the old growth forests fishing out all of the fish species extinction biodiversity
loss peak nitrogen peak oil etc those are all where we are using something in a way that depletes it
then it gets turned into trash or pollution on the other side where to accumulate somewhere
so we can define toxicity formally as depletion or accumulation because of an open loop in a
network diagram and if the loop was closed the things wouldn't accumulate they would go back
to the source of where we would get something from so it wouldn't have to deplete and we notice
that when we see any kind of natural system we go to an ecosystem whether it's a coral reef or a
forest or whatever and go to a forest there is no trash the body of something dies it's compost
there's feces it's compost something gets cut and bleeds it process it doesn't matter what it is
anything you can think about that is part of that environment from an evolutionary sense the
environment designed to be evolved to be able to process and there's also no unrenewable use of
anything right anything that is utilized is utilized in a closed loop fashion to one of the things we
see in complex systems and natural systems is comprehensive loop closure one of the things
we notice about the human design systems is open loops everywhere the materials economy itself
linear materials economy that takes virgin stuff and turns it into trash after using it for a very
short period of time is a macro open loop but there's micro open loops everywhere we have to
categorically solve that right we have to basically close all of those loops and another way of saying
what this generator function of issues is is that nature the stuff that nature makes is what we
call complex right it's self-organizing self-creating self-organizing self-repairing and the stuff we
make design tools is complicated it's rad right like the computer we're talking on is rad but
if it got broken it would not self-repair and it didn't self-organize it was made it was soldered
whatever made from design from the outside we can think about complex stuff comes about through
evolution complicated stuff comes about through design two different types of creative processes
with fundamentally different characteristics and complex stuff has comprehensive loop closure
everywhere because it couldn't externalize something and still be selected for adaptivity
right the adaptiveness factors everything whereas if i'm building something i might make it for one
reason or two or three reasons but it actually affects a lot of other stuff but the other stuff
wasn't what i was trying to optimize for so it ends up being where externalities occur so even this
computer i'm talking to you on right now was not optimized it was optimized for a bunch of things
so it's really cool but the fact that it's got a backlit screen that is 2d that's at a fixed focal
length from my eyes where i'm getting macular degeneration from spending too many hours on it
was one of the things that my eye health was not one of the things it was built to try and focus on
or the fact that it's fucking up my posture ergonomically by me looking down at the screen it
wasn't one of the things i tried to focus on or the you know a gazillion other things right what
happened to the environment and the supply chain process of getting the metals to make it or the
making of this computer affected a gazillion things that were not part of its design criteria
which means the making of it required externalizing a bunch of harm i.e a bunch of open loops
where it affected stuff that was not internalized to the process we can see that if a forest burns
it repairs itself if a house burns it does not repair itself and if my computer gets broken
it doesn't fix itself but if i cut myself it fixes so there's this fascinating difference the reason
we're bringing this up is to say if for something to be antifragile it has to be complex complexity
is the defining origin of antifragility and complicated things are all ultimately fragile
more or less fragile if we have a situation where complicated systems subsume their complex
substrate then this means continue to grow right so basically we're converting the complexity of
the natural world into the complicated built world and it's continuing to grow it will eventually
collapse because the complicated world and if you know it's just like my computer the water
infrastructure is complicated not complex the pipes don't self repair they can break easily
they're subject to being broken on purpose or accident right the same is true with the
everything right the roads the energy grid the everything now when i look at globalization
i say i've gotten increasingly interconnected complicated world that is increasingly more
complicated where the failures anywhere can trigger failures in more places because nowhere can
actually make its own stuff anymore because it should have complicated enough s to be made
across this whole thing right like this computer we're speaking on took six continents to make
and so if china died everywhere's fucked if the u.s died if there's so many places if if if mining
wasn't accessible in africa everywhere's fucked and we see an increasingly interconnectedly
complicated which also means increasingly fragile built world that we're trying to run
exponentially more energy through in terms of human activity dollars etc etc and that's happening
while decreasing the complexity of the biosphere far enough that its own anti fragility is going
away we're getting to a place where rather than climate being self-stabilizing it can go into
auto destabilizing positive feedback cycles where the biodiversity is getting low enough
that you can get catastrophic shifts in the nature of the homeostasis this made it possible to
have a biosphere like the one that we've lived in if you have a world where complicated systems are
subsuming their complex substrate and continuing to grow they will eventually collapse so these
are two different generator functions where we can say if i'm trying to solve ocean dead zones or
plastic or species extinction as one offs i will certainly fail because at most i move the curve
of collapse a year but there's so many other scenarios for fail that are over determined if
i don't solve the generator function of all of them i haven't actually got it the having a right
relationship between complex and complicated and having loop closure within the complicated
and creating anti rival risk basis it is a safe basis for exponential technology is the first
level of assessment of necessary design criteria for viable civilization
all right that's it for this episode to continue listening to the second and third
parts of this fascinating interview go to future thinkers.org slash 57 and to check out our sponsor
qualia go to future thinkers.org slash brain hack if you're new to the show and you like a list
of our top episodes and resources go to future thinkers.org slash start if you want to sponsor
our show go to future thinkers.org slash sponsor if you like our podcast leave us a rating and a
review on itunes and elsewhere it helps others find the show and we really appreciate it thanks
for listening and we'll see you in the next episode bye
