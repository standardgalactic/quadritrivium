Welcome back to Future Thinkers podcast, part two of the three-part series with Daniel Schmucktenberger on the generator functions of existential risks.
In this episode, Daniel gets into the past and present systems of sense-making and choice-making that we have as a humanity,
and how those will need to change as we transition into the future.
To get all the links and show notes from this episode, go to futurethinkers.org slash 57.
You can also find all three parts of the series there in one place.
This episode is brought to you by Qualia, a premium-neutropic supplement that helps support mental performance and brain health.
It's specifically designed to promote focus, support energy, mental clarity, mood, memory, and creativity.
UVA and I have both used it in the past, we really like it, and we actually met the founders and interviewed them on Future Thinkers.
You can check out those interviews, they're one of our favorites, at futurethinkers.org slash Daniel and futurethinkers.org slash Jordan.
They've got a new formula up called Qualia Mind, it's got more natural ingredients,
and you can get it at futurethinkers.org slash Brain Hack, and you can get 10% off if you use the code FUTURE.
Alright, let's get into the show.
Welcome to Future Thinkers.org, a podcast about the evolution of technology, society, and consciousness.
I'm Mike Gilliland, and I'm UVA Ivanova.
If you're new to the show and you'd like a list of our top episodes and resources, go to futurethinkers.org slash start.
And if you like our podcast, leave us a rating and a review on iTunes and elsewhere.
It really helps others find the show, and we really appreciate it.
Can we talk about some of the specific examples of the generator functions and what they look like in a society?
Okay, so I want to look at these same two generator functions through a couple different lenses that end up mapping to the same thing,
but these lenses are valuable.
So one way of saying what needs to happen is that we need systems of collective intelligence and collective sensemaking and choicemaking
that increase with scale effectively.
And I'll say why that makes sense.
If you look at Jeffrey West's work out of Santa Fe Institute, his book Scale is a classic example.
We see that you've got productive capacity or intelligence capacity, design capacity of a person.
Then you bring a few people together and you get increased productive capacity.
And for a little while, you'll actually get an exponential up curve where more people give you a lot more ability because they are sharing new capacities.
And this is the startup phase or something.
But pretty soon you get an inflection point where adding more people starts having diminishing returns per capita.
And then you get to this tabling part of the S curve where adding more people is not increasing adaptive capacity really at all.
And so if I have a curve where more people don't keep adding adaptive capacity well, then those people will always have an incentive to defect against that system
because they'll actually have more adaptive capacity per capita if they defect against it.
And so as long as I have a system of collective intelligence that cannot scale with the number of people, it can't include everybody.
And it will always force its own collapse, its own defection.
So far we've found that all the things that we call like intentionally made human systems, countries and companies have this type of curve.
And so again, this shows why none of those can make it.
And this is a design criteria thing of if I'm making, if we're creating a social system, if it doesn't have sense making and choice making,
able to increase at least linearly with number of people through some process, then the people will defect against that system as soon as you pass the inflection point.
And now they'll either defect against it, make their own thing completely unless the big system, even though it's less efficient per capita than them,
and it would take them out, in which case it's not safe to overtly defect, so then they covertly defect.
They defect while staying within the system, which starts to look like everything you see in the world today where someone looks at,
okay, what is my particular bonus structure and how can I optimize getting the most bonuses here even though it's not what's good for the company?
They have now defected against the well-being of the whole because their own incentive and the well-being of the whole are actually anti-coupled,
or at least mis-coupled. And we can see that almost everywhere people within systems have actually defected against being optimally aligned with the integrity of the system.
They're basically preying on the system in some way and while continuing to look like they're serving it, because that's where the incentive is.
And that's going to, of course, make the system radically unadaptive and speed up the rate of its inevitable decline.
And so if I could get a system that could scale its actual adaptive capacity with the number of people that we're in,
the people being in the system would always be better for them in participating with it than defecting against it covertly or overtly.
One way of looking at it is of what we have to solve is that sense-making and choice-making processes that scale adaptivity,
where the adaptiveness scales with number of people.
We'll come back to this in a minute, because this ends up being a very central way of thinking about it.
When we talk about rival-risk dynamics creating problems, anti-rival-risk dynamics, coherence dynamics of agents with other agents are core to the solutions that we're looking at.
It also ends up being that coherence is what solves a lot of the problems of the open loops in tech,
because just how when we were talking about in democracy the process of making a proposition,
somebody's sensing something they care about, they make a proposition that benefits it,
but it harms other things that they might not have even been sensing.
How do we bring all the sensors together to say, what is everything important connected to this?
Hold those as design constraints and then go into an integrated design process that is trying to meet all of those design constraints.
That sounds like the future of governance, but it also sounds like the future of technology design.
To make technology that's not externalizing harm, and that also means infrastructure design.
Coherence between all the agents that are sensing a part of the system
means that all the information from all the parts of the system get to be internalized to the decision-making processes,
to the choice-making processes, which means that externalities get internalized.
Interpersonal coherence at scale ends up being a central way of thinking about this.
Another way of thinking about it is the generator function of all the x-risks.
One thing we just said is the generator function is social incoherence.
That was actually what we just said as a generator function,
spoke through the language of collective intelligence's ability to scale.
Another way of looking at it is that the source of all of the risks and also the things that have always sucked,
the risks are just the things that have always sucked bigger,
is the relationship between choice and causation,
an inappropriate relationship between choice and causation as two types of change.
This is going to get into a very tricky philosophical area,
which is partly why I said the thing that we call science and tech is necessary but not sufficient.
Science is a theory of causation.
When we study law, what we're studying, the laws of physics, the laws of any domain,
are the rules that create causal change.
When we study, and even more purely in computer science,
computation means rule-based transform, and so something is changing,
it's transforming from one state to another state in a perfectly predictable way,
governed by a rule set, a law set, so causation.
Now, choice, we don't have a theory of choice,
which is why every philosophic conversation gets into free will and determinism and gets stuck there,
and Sam Harris and Dennett debated out and get nowhere,
and at the end of the day, say, we just have different intuitions on this,
and that's happened since forever in philosophy,
and we could actually get into that topic at depth at some point,
does require some nuance to do well, but we are making choices.
We are at minimum operating as if we were making choices,
and we are making choices that are extended by tech's knowledge of causation.
So before I got the tech and I was just a predator, I could hit somebody with my fist,
but then I could understand causation and say,
the heavier the thing is and the faster it goes and the harder it is,
causal principle, some more damage it causes,
and so I can extend my fist through a stone hammer,
then I can extend it through a sword,
then I can extend it through a gun, then I can extend it through a...
And so I'm taking my knowledge of causation, science,
and creating technology, applied science,
that allows my choice still to be extended through levers,
very powerful levers.
Science is giving me a theory of causation that can create applied causation
without giving me a theory of choice that tells me how to use that increased causal power.
And this has been a classic thing in the philosophy of science from the time of Descartes,
that science says what is not what ought,
that it is the realm of objective this is,
but we can't say anything about what ought because we don't deal in subjectives
and through most interpretations of physicalism,
the entire thing is meaningless and deterministic anyways.
And so ought just means something that we can't even make sense of.
Well, now there's a real bitch in here, there's a real problem,
which is science gives us the ability to understand the physical world very deeply,
and to create technology that can change the physical world very deeply.
And so it is the most powerful avenue for affecting the physical world,
technology, which is applied science,
but it has absolutely no compass for how it should do that.
And not only does it have no compass,
it says that all compasses are gibberish,
because it's going to be some religious idea, some moral, some whatever,
but they're not science, they're not objective.
We've equated objective with real and subjective with gibberish,
and the relationship between subjective and objective we don't even really take seriously,
because we don't know how, we don't have good tools for that, intellectual tools.
Then we say, okay, well, if technology is the power to change,
like to create nuclear bombs that can blow up the entire fucking world,
to create all kinds of dystopias or all kinds of protopias, right?
If it's all this power to do anything, what determines how we use that power?
And it's not an ethical framework, it's not a theory of choice.
What it ends up being is, well, who paid for the science?
Who can pay for the tech? How did they get the money to pay for the tech?
Remember, money is a concentrated choice-making system.
So what we end up getting is, well, capitalism.
Well, what that ends up meaning is social Darwinism.
That means game theory.
And so when whose game theory still ultimately guides the development of all the tech?
And so if we're now growing exponential tech with no basis for how to use it
other than to keep winning at win-lose games,
where every time we increase our technological capacity,
so do all sides in a multipolar way.
And so we're just upping the ante at the playing field.
This is a very important principle.
It is impossible to have a kind of technological asymmetric advantage
and maintain it indefinitely once you employ it.
You can maintain it while you don't employ it.
In which case it's not really advantage, it's only potential advantage.
But the moment that you deploy it, then everybody else sees it.
And it's much easier to copy than it was to do the initial innovation.
It's easy to iterate and find some other examples of.
And so all you did was up the playing field,
which is why you went from one country with nukes to lots of countries with nukes.
And somebody with AI to lots of places with AI and et cetera, et cetera, right?
And so the idea, well, we're going to develop this technology for our good purpose.
That's just silly because it's going to be used by all players for all purposes.
So this is why I call naive technical optimism naive
is because could technology solve some problem?
Sure.
But have we addressed loop closure and complicated systems
where it's not externalizing harm somewhere else?
No.
And have we recognized that that same technological capacity
will be used by everybody for all purposes,
all purposes that are incented in a system that has ubiquitous perversion incentive?
All right. Well, that's a problem.
The answer is not just that we technology ourselves out of it.
We actually have to change that underlying basis.
What that means is, okay, if we got all this power, what should we do with it?
Well, how the fuck do we answer that?
Ultimately, that's an ethical question.
It's an existential question.
And this comes to the forefront and we don't like to ask this
because we only know physicalism, which says this is not even a question, right?
Basically, physicalism, we have a couple of different versions and they all suck, right?
They're all nihilists unless you do some mental gymnastics to try and pretend that they aren't,
which I would say is not intellectually that honest,
because either I get consciousness as an epiphenomena of brain,
but that makes it a causal, right?
So the brain is a causally closed physical system
where voltage differentials in the brain move ions across membranes
and neurotransmitter goes one way versus the other.
So you think you love her or not, or had this idea or believe this thing or not.
And it's all basically controlled by particle physics and your consciousness is epiphenomena for some reason,
but could not be causal because what is consciousness that is not physics that could causally affect physics, right?
If physics is causally closed, okay.
So you don't really have choice.
So your experience of yourself as a choosing agent is ultimately an adaptive illusion,
which is also a problematic argument because why would it be adaptive to have that thing
if that thing doesn't affect causality at all?
And what is the weird metaphysics of how first person pops out of third person?
David Chalmers speaks about it in a very interesting good way when he says,
okay, so you've got a bunch of atoms, they're non-experiential,
and you arrange them in a particular way and experience pops out of it, right?
They've got position and momentum and mass and shit like that.
And now we've got like feeling and emotion and it's like a different type of stuff.
But if I have tools to study third person, I'm going to find third person because that's my tools, right?
So if my epistemology is measure shit and then do math across the stuff that I measured,
I'm going to come to a belief in that reality is measurable shit.
If I went the Buddhism direction and my epistemology was inquiring to the nature of my own experience,
I can do the exact opposite.
I can say, I actually don't know that there are any particles here.
I might be dreaming.
