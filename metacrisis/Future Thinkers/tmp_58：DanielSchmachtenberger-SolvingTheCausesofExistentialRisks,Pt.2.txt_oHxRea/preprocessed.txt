Welcome back to Future Thinkers podcast, part two of the three-part series with Daniel Schmucktenberger on the generator functions of existential risks.
In this episode, Daniel gets into the past and present systems of sense-making and choice-making that we have as a humanity,
and how those will need to change as we transition into the future.
To get all the links and show notes from this episode, go to futurethinkers.org slash 57.
You can also find all three parts of the series there in one place.
This episode is brought to you by Qualia, a premium-neutropic supplement that helps support mental performance and brain health.
It's specifically designed to promote focus, support energy, mental clarity, mood, memory, and creativity.
UVA and I have both used it in the past, we really like it, and we actually met the founders and interviewed them on Future Thinkers.
You can check out those interviews, they're one of our favorites, at futurethinkers.org slash Daniel and futurethinkers.org slash Jordan.
They've got a new formula up called Qualia Mind, it's got more natural ingredients,
and you can get it at futurethinkers.org slash Brain Hack, and you can get 10% off if you use the code FUTURE.
Alright, let's get into the show.
Welcome to Future Thinkers.org, a podcast about the evolution of technology, society, and consciousness.
I'm Mike Gilliland, and I'm UVA Ivanova.
If you're new to the show and you'd like a list of our top episodes and resources, go to futurethinkers.org slash start.
And if you like our podcast, leave us a rating and a review on iTunes and elsewhere.
It really helps others find the show, and we really appreciate it.
Can we talk about some of the specific examples of the generator functions and what they look like in a society?
Okay, so I want to look at these same two generator functions through a couple different lenses that end up mapping to the same thing,
but these lenses are valuable.
So one way of saying what needs to happen is that we need systems of collective intelligence and collective sensemaking and choicemaking
that increase with scale effectively.
And I'll say why that makes sense.
If you look at Jeffrey West's work out of Santa Fe Institute, his book Scale is a classic example.
We see that you've got productive capacity or intelligence capacity, design capacity of a person.
Then you bring a few people together and you get increased productive capacity.
And for a little while, you'll actually get an exponential up curve where more people give you a lot more ability because they are sharing new capacities.
And this is the startup phase or something.
But pretty soon you get an inflection point where adding more people starts having diminishing returns per capita.
And then you get to this tabling part of the S curve where adding more people is not increasing adaptive capacity really at all.
And so if I have a curve where more people don't keep adding adaptive capacity well, then those people will always have an incentive to defect against that system
because they'll actually have more adaptive capacity per capita if they defect against it.
And so as long as I have a system of collective intelligence that cannot scale with the number of people, it can't include everybody.
And it will always force its own collapse, its own defection.
So far we've found that all the things that we call like intentionally made human systems, countries and companies have this type of curve.
And so again, this shows why none of those can make it.
And this is a design criteria thing of if I'm making, if we're creating a social system, if it doesn't have sense making and choice making,
able to increase at least linearly with number of people through some process, then the people will defect against that system as soon as you pass the inflection point.
And now they'll either defect against it, make their own thing completely unless the big system, even though it's less efficient per capita than them,
and it would take them out, in which case it's not safe to overtly defect, so then they covertly defect.
They defect while staying within the system, which starts to look like everything you see in the world today where someone looks at,
okay, what is my particular bonus structure and how can I optimize getting the most bonuses here even though it's not what's good for the company?
They have now defected against the well-being of the whole because their own incentive and the well-being of the whole are actually anti-coupled,
or at least mis-coupled. And we can see that almost everywhere people within systems have actually defected against being optimally aligned with the integrity of the system.
They're basically preying on the system in some way and while continuing to look like they're serving it, because that's where the incentive is.
And that's going to, of course, make the system radically unadaptive and speed up the rate of its inevitable decline.
And so if I could get a system that could scale its actual adaptive capacity with the number of people that we're in,
the people being in the system would always be better for them in participating with it than defecting against it covertly or overtly.
One way of looking at it is of what we have to solve is that sense-making and choice-making processes that scale adaptivity,
where the adaptiveness scales with number of people.
We'll come back to this in a minute, because this ends up being a very central way of thinking about it.
When we talk about rival-risk dynamics creating problems, anti-rival-risk dynamics, coherence dynamics of agents with other agents are core to the solutions that we're looking at.
It also ends up being that coherence is what solves a lot of the problems of the open loops in tech,
because just how when we were talking about in democracy the process of making a proposition,
somebody's sensing something they care about, they make a proposition that benefits it,
but it harms other things that they might not have even been sensing.
How do we bring all the sensors together to say, what is everything important connected to this?
Hold those as design constraints and then go into an integrated design process that is trying to meet all of those design constraints.
That sounds like the future of governance, but it also sounds like the future of technology design.
To make technology that's not externalizing harm, and that also means infrastructure design.
Coherence between all the agents that are sensing a part of the system
means that all the information from all the parts of the system get to be internalized to the decision-making processes,
to the choice-making processes, which means that externalities get internalized.
Interpersonal coherence at scale ends up being a central way of thinking about this.
Another way of thinking about it is the generator function of all the x-risks.
One thing we just said is the generator function is social incoherence.
That was actually what we just said as a generator function,
spoke through the language of collective intelligence's ability to scale.
Another way of looking at it is that the source of all of the risks and also the things that have always sucked,
the risks are just the things that have always sucked bigger,
is the relationship between choice and causation,
an inappropriate relationship between choice and causation as two types of change.
This is going to get into a very tricky philosophical area,
which is partly why I said the thing that we call science and tech is necessary but not sufficient.
Science is a theory of causation.
When we study law, what we're studying, the laws of physics, the laws of any domain,
are the rules that create causal change.
When we study, and even more purely in computer science,
computation means rule-based transform, and so something is changing,
it's transforming from one state to another state in a perfectly predictable way,
governed by a rule set, a law set, so causation.
Now, choice, we don't have a theory of choice,
which is why every philosophic conversation gets into free will and determinism and gets stuck there,
and Sam Harris and Dennett debated out and get nowhere,
and at the end of the day, say, we just have different intuitions on this,
and that's happened since forever in philosophy,
and we could actually get into that topic at depth at some point,
does require some nuance to do well, but we are making choices.
We are at minimum operating as if we were making choices,
and we are making choices that are extended by tech's knowledge of causation.
So before I got the tech and I was just a predator, I could hit somebody with my fist,
but then I could understand causation and say,
the heavier the thing is and the faster it goes and the harder it is,
causal principle, some more damage it causes,
and so I can extend my fist through a stone hammer,
then I can extend it through a sword,
then I can extend it through a gun, then I can extend it through a...
And so I'm taking my knowledge of causation, science,
and creating technology, applied science,
that allows my choice still to be extended through levers,
very powerful levers.
Science is giving me a theory of causation that can create applied causation
without giving me a theory of choice that tells me how to use that increased causal power.
And this has been a classic thing in the philosophy of science from the time of Descartes,
that science says what is not what ought,
that it is the realm of objective this is,
but we can't say anything about what ought because we don't deal in subjectives
and through most interpretations of physicalism,
the entire thing is meaningless and deterministic anyways.
And so ought just means something that we can't even make sense of.
Well, now there's a real bitch in here, there's a real problem,
which is science gives us the ability to understand the physical world very deeply,
and to create technology that can change the physical world very deeply.
And so it is the most powerful avenue for affecting the physical world,
technology, which is applied science,
but it has absolutely no compass for how it should do that.
And not only does it have no compass,
it says that all compasses are gibberish,
because it's going to be some religious idea, some moral, some whatever,
but they're not science, they're not objective.
We've equated objective with real and subjective with gibberish,
and the relationship between subjective and objective we don't even really take seriously,
because we don't know how, we don't have good tools for that, intellectual tools.
Then we say, okay, well, if technology is the power to change,
like to create nuclear bombs that can blow up the entire fucking world,
to create all kinds of dystopias or all kinds of protopias, right?
If it's all this power to do anything, what determines how we use that power?
And it's not an ethical framework, it's not a theory of choice.
What it ends up being is, well, who paid for the science?
Who can pay for the tech? How did they get the money to pay for the tech?
Remember, money is a concentrated choice-making system.
So what we end up getting is, well, capitalism.
Well, what that ends up meaning is social Darwinism.
That means game theory.
And so when whose game theory still ultimately guides the development of all the tech?
And so if we're now growing exponential tech with no basis for how to use it
other than to keep winning at win-lose games,
where every time we increase our technological capacity,
so do all sides in a multipolar way.
And so we're just upping the ante at the playing field.
This is a very important principle.
It is impossible to have a kind of technological asymmetric advantage
and maintain it indefinitely once you employ it.
You can maintain it while you don't employ it.
In which case it's not really advantage, it's only potential advantage.
But the moment that you deploy it, then everybody else sees it.
And it's much easier to copy than it was to do the initial innovation.
It's easy to iterate and find some other examples of.
And so all you did was up the playing field,
which is why you went from one country with nukes to lots of countries with nukes.
And somebody with AI to lots of places with AI and et cetera, et cetera, right?
And so the idea, well, we're going to develop this technology for our good purpose.
That's just silly because it's going to be used by all players for all purposes.
So this is why I call naive technical optimism naive
is because could technology solve some problem?
Sure.
But have we addressed loop closure and complicated systems
where it's not externalizing harm somewhere else?
No.
And have we recognized that that same technological capacity
will be used by everybody for all purposes,
all purposes that are incented in a system that has ubiquitous perversion incentive?
All right. Well, that's a problem.
The answer is not just that we technology ourselves out of it.
We actually have to change that underlying basis.
What that means is, okay, if we got all this power, what should we do with it?
Well, how the fuck do we answer that?
Ultimately, that's an ethical question.
It's an existential question.
And this comes to the forefront and we don't like to ask this
because we only know physicalism, which says this is not even a question, right?
Basically, physicalism, we have a couple of different versions and they all suck, right?
They're all nihilists unless you do some mental gymnastics to try and pretend that they aren't,
which I would say is not intellectually that honest,
because either I get consciousness as an epiphenomena of brain,
but that makes it a causal, right?
So the brain is a causally closed physical system
where voltage differentials in the brain move ions across membranes
and neurotransmitter goes one way versus the other.
So you think you love her or not, or had this idea or believe this thing or not.
And it's all basically controlled by particle physics and your consciousness is epiphenomena for some reason,
but could not be causal because what is consciousness that is not physics that could causally affect physics, right?
If physics is causally closed, okay.
So you don't really have choice.
So your experience of yourself as a choosing agent is ultimately an adaptive illusion,
which is also a problematic argument because why would it be adaptive to have that thing
if that thing doesn't affect causality at all?
And what is the weird metaphysics of how first person pops out of third person?
David Chalmers speaks about it in a very interesting good way when he says,
okay, so you've got a bunch of atoms, they're non-experiential,
and you arrange them in a particular way and experience pops out of it, right?
They've got position and momentum and mass and shit like that.
And now we've got like feeling and emotion and it's like a different type of stuff.
But if I have tools to study third person, I'm going to find third person because that's my tools, right?
So if my epistemology is measure shit and then do math across the stuff that I measured,
I'm going to come to a belief in that reality is measurable shit.
If I went the Buddhism direction and my epistemology was inquiring to the nature of my own experience,
I can do the exact opposite.
I can say, I actually don't know that there are any particles here.
I might be dreaming.
I might be a brain in a vat being stimulated by electrodes.
I might be a crazy person.
I might be who the fuck knows.
I can't know any of that.
All I can know.
So the Buddhists in Descartes did the same thing.
What can I know for sure?
Primaphasia, I'm experiencing something.
But the thing that I think is I and the thing that I think something might not be what they are.
But what I know for sure is experience.
So let me explore the nature of my experience.
And so then of course the Buddhists typically do the opposite reductive move.
What's real is consciousness and the physical universe is either not real or an epiphenomena.
And because if my epistemology is to inquire into the nature of experience,
what I'm going to come up as real is experience.
And so what you end up having is epistemologies that bias ontologies and are self referential.
But on the physics side and the physicalism interpretation of physics,
you get dreadful nihilism or incongruencies.
And other than that, you get weird religious shit.
And we're just not happy with any of this.
So we just like try not to think about this too much.
And we actually can't because we have to actually address what do we want?
Why do we want it? What is worth wanting?
The addict wants stuff that makes their life suck.
And the little kid who grows up in front of the screen with a bunch of flashing lights wants it again
because they were programmed to want it.
Their sovereignty is hijacked.
What is worth wanting?
What actually creates a good life?
What does good mean?
And the fact that we didn't like the bad religious answers for these doesn't mean that we get to throw these questions out.
Completely because you end up getting the existential risk of where we're at right now,
which is we don't ask those.
We just build the tech based on who pays us.
Great.
So we all get to go extinct in a world where we have no theory of choice,
but we are choosing based on shitty theory of choice,
prima facie, game theoretic theory of choice.
And we have a theory of causation.
So we're extending our shitty choices through exponential tech.
And so another way of saying what we have to actually get right
is individual and collective choice making that doesn't suck.
And another way of saying doesn't suck is individual and collective choice making that is
omnipositive, or at least vectoring towards omnipositive.
It is omniconsiderate in terms of considering all that will affect,
realizing that it's interconnected with all these things,
and that if I act in a way to beat the other guy,
I'm engendering his enmity in my own insecurity in the future.
If I'm harming, if I'm polluting the air, I breathe the air.
And this is where we have to shift all the way down to an identity level,
which is the idea that I'm a separate thing.
You're a separate thing over there.
I can advantage myself, independent of you or even at your expense,
is just actually ontologically not a well-formed idea.
Ontologically, when I say I, I might think I,
and I've got some idea of what that means,
a set of atoms contained in a particular boundary that looks like
this guy called me, it's on my Facebook picture,
and a set of memories, whatever.
But when I think of I, I usually don't think of all the plants
on the biosphere without which I would not exist,
because there would be no atmosphere and I would be dead.
And I usually don't think about all of the soil bacteria
without which the plants wouldn't exist,
and all of the pollinators.
But I don't exist without all those.
So if I think of I without those, it's a ill-formed concept.
But when I think of that ill-formed concept,
and I think that it's a good concept, it's a real thing,
I can think about advantaging that I,
at the expense of the things that I depend on.
And that is a kind of insanity,
but it is a kind of ubiquitous insanity currently.
And so for me to make a choice for me,
I have to know what the fuck I am.
And I am not a separate thing in game theater competition with everything else.
I am ultimately interdependent with and dependent on so much other than me.
So then I say, all right, to not debase the world upon which I exist,
the foundation upon which I exist,
I start to get I am an emergent property of this whole thing.
If there weren't the bacteria and the plants and the pollinators,
there weren't the people that came before that made up the ideas that I believe
and the words that I think in and the aesthetics that I perceive through.
If there wasn't gravity and electromagnetism making the whole fucking thing possible.
If it wasn't for so much stuff that I consider other than me, I don't exist.
And so then I get, I am an emergent property of the whole
and I am both interconnected with everything and totally unique within it.
And so are you.
And as soon as I get that, I get a couple things.
Because we're totally interconnected,
I cannot advantage myself at your expense in a real way.
I only can if I haven't factored loop closure everywhere.
But the way that I harm you is going to end up being an open loop
that is going to harm the substrate of what I care about.
And when I factor all the closed loops,
we get to David bombs, the evolution of wholeness,
this whole thing, the evolution of wholeness,
the Schrodinger equation of the whole thing, right?
Evolving in its complexity.
So I can't advantage myself at your expense in a meaningful way.
And I also can't understand myself without understanding myself
through my interactions with you
and your feedback and your reflection.
So choice.
We have to have a theory of choice
that comes from a philosophy that can actually relate choice and causation
and can have a theory of causation and a theory of choice
that are intercommentable with each other, right?
It's not a made up kind of theory of choice.
And it can serve as a basis for how to make good choices
in the presence of all of the technology we have
and something we said last time when we were together is
the technology is, because it's a causal extension of our choice,
has us extending our power to be like the power of gods.
The ability to create new life forms of genetic engineering
to destroy whole life forms and species to blow things up
like nuclear bombs are bigger than Zeus's lightning bolt was depicted, right?
Like the power of gods.
If you do not get the love and wisdom and consideration of gods
as a choice making basis for that power,
you'll use that power in stupid ways that will end up causing self-destruction
on a very tiny playing field.
And if I want to make choices that are good for me
and I'm interconnected with the whole,
I have to make choices that are good for the whole.
And that means I have to understand that I'm not a separate thing
and I have to be able to progressively consider
my relationship with everything,
the impacts of my choices and be on everything
and be able to internalize the things
that would have been externalities into my choice making process
and not just at an individual level, but at a group level.
We're doing group process and this becomes the future of design
as opposed to the open loop harm externalizing method that we've had
is how do we have progressively more omniconsiderate design
that is more omnipositive,
that is a safe vessel for the level of effect that it has.
The idea of us being separate from the world,
in many ways it's modern,
because if you look at how some of the what we call primitive tribes
looked at themselves and considered themselves, it was not the same.
They considered themselves a lot more connected with everything
or just a small part of the whole.
But like you said, those tribes, a lot of them got killed
by the other tribes who considered themselves as more separate.
And so how we place our sense of agency
and where we place our sense of what is us and what is not us
actually has a very significant effect on the outcomes.
And I think that in the modern world,
a lot of people don't even realize that this sense of separation
between self and world is a construct.
But because we have certain definitions,
scientific definitions or whatever of where a body ends
and the environment begins.
But if you start deconstructing that
and you can do it through any number of things,
like just intellectually deconstructing it or in deep meditation,
looking at where your sense of self ends
and sense of the world begins, it just breaks down.
That sense of self is very strong.
And maybe it is because capacity to affect the environment
is so much higher now with technologies and all these tools
that kind of have become the extension of ourselves.
Maybe that's part of the reason why people are so attached
to their sense of self,
because they actually see the effects so strongly.
Whereas in the past, people would see the effects of nature
a lot more strongly than the effects of themselves.
I think it's strong because of the dynamic of experience and time.
You know, you don't see the direct results of your actions
coming back to you after a long time
until they become external and separate.
And then when they come back at someone else
or something else affecting you
in a way you don't want to be affected,
but you don't loop it back to your original actions.
This is something I've been thinking about quite a lot
as you've been talking, Daniel,
is the sort of natural experience that people have
and how these theories might contradict those natural experiences
even though the natural experience is incorrect.
How do we communicate these concepts in a way that
even though it can conflict with someone's day-to-day
minute-by-minute experience,
but that could cause them to think differently
and expand their identity?
But people's experience is also dictated by their concepts.
You know, like those two things affect each other
because when people have an idea about how a certain thing is,
they tend to experience it in a way that is consistent with that idea.
True.
So if we look at the tribes that had more of a,
there's a web of life and we're stranded in a view
that maybe you were mentioning a moment ago,
they had an informal theory of choice.
They might have had some moral or ethical principles.
This was not like a formal system of formalized ethics,
but they had some theory.
And they had a very weak theory of causation.
They thought diseases happened by, like, ghosts
and obviously didn't know how to work with tech all that well and etc.
So they didn't make it through with other people that understood causation better.
And so the causation leads to physical adaptive advantage
and there's a theory of choice.
It's just a theory of choice called win out and lose games, right?
Game theory is the only theory of choice.
That theory is tapping out, right?
That theory itself leads to its own self-termination
because as the power keeps getting larger,
it becomes more than you can keep winning at all.
We said this before that win, lose eventually becomes lose, lose,
omnilose lose when you have levels of war that nobody can win.
When you have tragedy of the commons that are completely ruined commons,
when you have those types of dynamics,
when you have an information ecology that's so broken from the incentive
and form that nobody has any idea what the fuck is true.
So you either figure out how to create omniline win-win
as a new solution against win-lose other than win-lose,
or you get omnilose lose as the inevitable byproduct of trying to keep win-lose.
And it's funny because there's this very mythopoetic way of thinking
about what we're talking about that is otherwise a very kind of just technically clear thing.
We said scaling to get the power of gods,
we have to have love and wisdom of gods.
Similarly, we say we either create omniline win-win or we get omnilose lose.
It sounds a lot like heaven or hell on the other side of purgatory
and we got some hard choices to make.
And we might even ask if maybe those stories were metaphors for seeing,
hey, we're making choices that we kept getting more powerful that this would be problematic.
And not just the heaven and hell and purgatory story,
but that we're in the Kali Yuga and Sat Yuga is the next phase.
There's a lot of these stories that in a Joseph Campbell-like way,
we can say like, oh, that's actually a very interesting way to think about where we're at right now.
But we can't depend on Jesus to come back and solve it
or aliens to come fix it for us or a fifth dimensional light ray from the galactic center
or whatever it is.
We have to actually become that Jesus for those aliens.
We have to actually become a being that has the right capacity to make the right choices
and do the right sense making and the right choice making.
So it is true that a being that's a shit ton more effective at good choice making
needs to come to all these things and the types of beings we've been.
It's just we have to become them.
And with regard to Mike, your question on what experiences are natural,
obviously it's natural for me to think in English, not Mandarin.
But if I grew up in China, I would think in Mandarin and that would be natural.
And because I think in English, I have certain constructs of thought linguistically
that are related to the syntax of English that if I thought in Mandarin would be different.
My aesthetic would be different.
And if I grew up in the plains with the Sioux Indians,
what would be natural to me would be different.
And my identity, ethics, aesthetics, et cetera.
So this is very conditioned.
Now in the modern world where we experience ubiquitously feeling separate,
we also experience ubiquitously feeling alone and lonely, not just alone, but lonely.
And we can see that loneliness as a major source of depression, anxiety,
and ultimately even suicidal impulse is pretty much ubiquitous in the developed world.
We say, is that a natural experience?
Well, it's certainly an ubiquitously conditioned one.
Was there ever a person in an indigenous tribe throughout the history,
two and a half million years of hominid history or 250,000 years of homo sapien history that felt lonely?
Not that much.
You live in a tribe with 150 people that know every fucking thing about you
and that you've known forever and that you can fully depend on.
You know everything about them and you've got no secrets.
And lonely is not really a thing, right?
And separate from them is not really a thing.
And so we say natural, I think what you mean is conditioned ubiquitously.
So then we have to say, all right, so humans are more susceptible to being conditioned by their environment than most creatures.
Obviously, a dog that grows up in the wild or captivity is going to be different,
but we're even more susceptible because, and this is a really important thing to understand about sapiens,
the gorilla or the chimp that's close to us can grab onto its mom's fur in the first five minutes while she moves around.
And we can't even move our head for three months.
And a horse is going to get up and walk in 20 minutes and it takes us a year.
And just to have a real sense, like do the calculation,
how many 20 minutes fit into a year to get how many multiples it takes us to be adaptive in the most simple way?
And we're like, wow, we are embryos for an extraordinarily long period of time.
We are helpless for an extraordinarily long period of time.
Why is that?
Well, because the horse comes kind of pre-programmed how to be a horse.
It's going to be a horse pretty much the same from generation to generation in the wild.
And so it coming pre-programmed works because it adapted to fit its environment.
And so then it can hold the code of how to be adaptive in relationship to its environment.
But it used to be really adaptive to throw spears and probably none of us are all that good at throwing spears
and we're pretty adaptive because we like text and podcasts and shit that they didn't do
because what's adaptive for us changes pretty rapidly.
Because unlike the other creatures that are the product of their environment,
we are, as tool-evolvers, creatures that not only go and exploit every niche,
grillas didn't leave and go find islands and be in the water and go to the Arctic and shit, right?
Like they adapted to a niche.
We went and found every niche.
And then we made new niches, right?
We made cities and tree houses and all kinds of shit.
As a result, we had to learn how to adapt to the new world that we were in
and we were going to keep creating new worlds, finding and creating new worlds.
So as babies were born pretty much open to just start imprinting what world am I in
and how do I not have a genetic program to do this,
but how do I connectomically program to be able to do this?
But it's not just in childhood because we can change stuff so fast
that the whole tribe might get up and leave and go somewhere else
where we went from gathering to farming or to hunting, right?
Like something super different.
This is why we need adult neuroplasticity
to be able to change our kind of basal orientation even later in life.
And so we are radically affected by our environments and it's part of our adaptive advantage
and we are mostly affected by our social environments
by what the other people around us are paying attention to and doing
and what the nature of the relationships are like.
You'll notice that mostly now people live in nuclear family homes on their own,
don't interact with other people all that much and then they spend all their time
addictively looking at other people on screens.
They watch TV and they watch people and then they go to Facebook and they look at people
and they read news articles about people.
They're fucking fascinated by people but then we're conditioned currently
to both suck at interacting with people.
Capitalism has largely been a way of not needing each other directly
and being able to indirectly intermediate needing each other through money, right?
So the money can just buy whatever I need so I don't actually have to have friends.
I'm on neighbors or give a shit about anybody or have anyone give a shit about me.
And that seems really convenient and everybody is in a crisis of loneliness at home
looking at people hoping that they're getting likes which are not real relationships.
This is basically sugar rather than nutrients.
This is porn rather than a real relationship.
This is a hypernormal stimuli having co-opted real stimuli
but also having desensitized us to real stimuli while being comprehensively bad for us.
So when you ask how could people have a different experience
realizing that we are conditioned by our environment to think in words of a certain language
to experience in a certain way, I could say lots of things that won't matter.
I could say go spend more time in nature but nobody will do it.
We both know that people listen to podcasts, nobody will do it maybe one time
and then their life is busy and they will be the product of their environment again very largely.
And because the relationship with us here was a tiny part of all of the relationships that we're influencing.
And so I could say go spend time in nature and I could say have some good psychedelic journeys
and do this kind of breath work and contemplate that the atoms that make up your body were plants not that long ago.
But there would just be nice words to say that wouldn't end up affecting people that much.
If you want to ask how could people really change their experience
only thing I can say that is statistically really going to work is immerse yourself in an environment
that makes that likely around people that are doing that.
So if you went and lived as a tribe for a while on the Amazon, you go live with the Ochoa or something
you will experience the world differently before a couple weeks passes.
You will start hearing things in a different way, experiencing yourself in a different way,
feeling a connection to people in a different way.
And if you even change the group of people you're hanging out around and what they're motivated by
and what they think about and you change the quality of your relationships with them,
you will end up changing the basis of what seems like natural experience to you.
All right, that was part two of our interview with Daniel Schmockenberger
on the generator function of existential risks.
To listen to part one and part three and get all the links and show notes,
go to futurethinkers.org slash 57.
And to check out our sponsor, Qualia, go to futurethinkers.org slash brain hack.
If you're new to the show and you like a list of our top episodes and resources,
go to futurethinkers.org slash start.
If you want to sponsor our show, go to futurethinkers.org slash sponsor.
If you like our podcast, leave us a rating and a review on iTunes and elsewhere.
It helps others find the show and we really appreciate it.
Thanks for listening and we'll see you in the next episode. Bye!
