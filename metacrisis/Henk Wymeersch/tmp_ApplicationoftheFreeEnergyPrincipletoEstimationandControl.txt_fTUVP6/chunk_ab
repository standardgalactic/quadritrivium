The reason is that the control objective Ct is not encoded anywhere in this factor graph
so there is no way to determine reasonable controls.
The way this problem has been addressed in the literature is by encoding the control
objective directly in the factor graph.
So here again is our control stage cost which is quadratic in the state and quadratic in
the control.
So our paper number one proposes is to create an artificial prior on future controls and
this prior is proportional to an exponential negative the control cost.
So what this means for instance when h is very large is that our generative model tells
us that we will end up in states with low control.
Similarly to deal with the stage cost related to the state we introduce artificial known
observations and an associated likelihood function which is proportional to exponential
negative state cost.
So again when r is very large this tells us that our generative model brings us to states
where x is small.
We also remove future observations because those are not known.
This then brings us to the following factorization.
Of the states from time 0 to time capital T observations up to time little t because
future observations are removed the controls from time 1 to time capital T and then the
artificial future observations from time t plus 1 to time capital T.
We express this using the standard Markov model as a part related to the past with the prior
the state dynamics and the observation model and then a part related to the future where
we again have the state dynamics we have the prior over future controls and the likelihood
function related to future states.
Here we show again the factor graph from the previous slide where the green part relates
to the past, the red part relates to the future and x of t connects the past with the future.
Recall again in this graph the future controls and the future observations are unknown.
We now replace this part of the future with this factor graph on the right corresponding
to the factorization above.
So what we did is for each control we added the prior under control of the form described
above.
For each state in the future we add a likelihood function with an associated artificial observation
equal to 1.
Now we also removed all the future observations y t.
So now x and u are unknown and z is known.
But this is sufficient for us to run the belief propagation algorithm.
So we can do forward message passing where we start from the updated belief of x t given
all observations up to time little t and all controls up to time t minus 1.
We pass messages to the future.
We pass messages from the future to the current time ut.
Now we have two messages on this edge ut and we can compute the belief.
So following the belief propagation rule the belief ut provides us the belief over the
next control action.
We can take the mode of this belief and implement that action and then go through the entire
process again.
Now it was shown that by doing this we find modified Riccati equations for the LQG problem.
So when we specialize this scenario above to the linear state dynamics, linear observation
model and quadratic control we find something that's close to LQG but not quite.
A similar idea was proposed in reference to for state dependent costs.
It was proposed to add additional factors in the factor graph related to the state cost
with a regularizer lambda and this lambda could be tuned to obtain desired behavior.
We are now prepared to discuss the relation between active inference and LQG.
Recall that in active inference we are always trying to minimize free energy and this leads
us to our main idea.
Similar to the previous papers we will consider the future controls as hidden and their states
to be inferred.
Finally we will merge the control cost in the free energy objective.
Different from before but without any loss of generality we will use a rolling horizon
where at time t, time little t we will have a horizon from the current time up to capital
t time steps ahead.
So now we introduce our notation.
Why under bar are the observations up to time little t?
Why over bar are the observations over horizon of size capital t?
So from time t plus 1 to time t plus capital t.
Similarly the states under bar, x under bar represents the states up to time little t,
x over bar represents the future states, u under bar represents the controls up to time
t minus 1, u over bar represents the future controls including the control ut that we
want to infer.
We will then define a new generative model at time t denoted by pt.
This model depends on all the observations, all the hidden states, all the future controls
by this condition on the past controls.
We factorize this distribution into three parts, a normalization constant, the system
model which was the old generative model from early on in this presentation in which everything
was conditioned on all of the controls times a goal prior and this goal prior depends on
the future states, the future observations and the future controls.
Note that some variables such as xt appear in the goal prior and in the system model.
This is fine because the normalization constant z will make sure that the entire distribution
is well defined and normalized.
Let us look at the shape of the proposed goal prior.
Similar to the previous papers we consider to be an exponential minus lambda times the
cost over the horizon.
This cost as you may recall is quadratic in the state and in the control.
The parameter minus lambda will create different behaviors.
If lambda is very large this means that our goal prior is very concentrated around zero,
zero in the control and zero in the state.
If lambda is very small meaning close to zero this means that the goal prior is very flat
in the state and control space.
With this generative model we can now define a free energy objective at time t.
This free energy objective involves a cool black library divergence where p of t is a
generative model which is given and q is a distribution which we would like to determine.
By plugging in the definition of the cool black library divergence as well as the definition
of the generative model we can break down this free energy into two parts.
One part v of t which depends on the past and one part g of t which depends on the future.
The part g of t also contains the goal priors which relate to the future.
We show in the paper that the first time is minimized by a calmant filter and the second
term is minimized by a message passing backward in the factor graph.
The beliefs can then be found by running message passing on the corresponding factor graph
with the goal priors and these beliefs found by message passing will then also minimize
the free energy objective.
The principle outline here is called generalized free energy.
There is also a concept called expected free energy but this is different and will lead
to different controllers.
The entire factor graph is shown here.
In green again we have the past, in red and orange we have the future and what connects
them is the state at time t.
The factor graph corresponding to the past is exactly the same as it was before and it
is conditioned on all past controls.
The factor graph has its variables y and x under bar so there are the past observations
and the past states.
The graph on the right corresponds to the future.
We see that there are these orange vertices in the graph and they correspond to the factorized
goal priors.
We call that the goal priors is the exponential of a sum so this becomes a product of the
exponentials.
For each future state we have a goal prior, for each future control we have a goal prior.
We also have goal priors for future observations but these are set to uniform distributions.
We can now run message passing on this factor graph.
The messages on the factor graph are visualized here.
Let us focus on some of these messages.
The message a from x t going from left to right is the Kalman filter output after the
update so this is just a standard Kalman filter running from left to right.
The messages b and c regarding future states they are backward messages in the factor graph
which accounts for future goals so this depends on the goal priors in future times.
Now the message is e and d, the message e downward accounts for the goal prior of the
control at time t and the message d upward accounts both for the output of the Kalman
filter a and the future goals c and b.
We can then multiply the messages e and d and this provides us the belief of the control
at time t which we are interested in.
We can then apply the mode of this belief to the system and shift our time horizon with
one step and apply the same principle again.
The question now arises what can we prove about the relationship between active inference
as proposed in the previous slide and LQG.
In the paper we proved the following two statements.
Under a deterministic generative model active inference solves a stochastic optimal control
problem.
So here the state dynamics and the observation model are fully deterministic.
Secondly even when the generative model is not deterministic but lambda becomes very
small but positive then also active inference solves the underlying stochastic optimal control
problem.
To make this specific for LQG we find the following expressions for the controller with
the gain of the controller as well as the additional matrices that are also used in
the Riccati equations.
Different from the standard Riccati equations we have lambda in many of these expressions.
But again it is possible to show that when either condition one or two holds then the
lambda cancels out in all these expressions and we recover exactly the LQG Riccati equations.
So it is easy to verify that under condition one or two the controller becomes exactly
the LQG controller.
Let us show some results to verify our claims.
This slide shows the performance of the LQG and active inference controller for a time
window of 10 steps.
Each of the four figures has on the x-axis time and on the y-axis different quantities
of interest.
On the bottom left we have the control magnitude which we would like to be small, we have the
state magnitude which we would also like to be small, on the bottom right we have the
accumulated control cost and on the top right we have a measure of the free energy.
In each of the figures we show three curves, the LQG is in blue, in red is the active inference
controller with lambda equal to 1 and in green is the active inference controller with
lambda equal to 0.1.
We observe that the LQG controller applies large controls in the beginning to bring
the magnitude of the state close to 0 and then apply small corrections.
In contrast the active inference controller with lambda equal to 1 is very sluggish and
applies only very small controls leading to a slow decrease of the state magnitude as
a function of time.
The active inference controller with a small lambda equal to 0.1 behaves very close to
the LQG controller.
This can also be seen in the bottom right showing the accumulated cost, where LQG and
the active inference with lambda equal to 0.1 achieve similar accumulated costs while
the active inference controller with a large lambda leads to a large accumulated cost.
So these results confirm that for small lambda the active inference controller recovers LQG.
So it is interesting to see that larger lambda leads to less aggressive control with lower
free energy but higher cost and this result is somewhat counterintuitive.
One would expect that a larger lambda would lead to more aggressive control bringing the
state and the control magnitude close to 0 quickly.
Note that with a large lambda we will have a very concentrated goal prior around the
target value of 0.
The interpretation of this is as follows.
If your model tells you that you will anyway end up where you want, so this means with
large lambda your model tells you that you will have a control and a state close to 0
then there is no need to work hard to get there.
So there is no need to apply large controls to reach your goal because you think you will
get there anyway.
So this was a counterintuitive but interesting result.
This now leads us finally to our conclusions.
Active inference is a flexible and general framework that can be applied to stochastic
control and it has classical LQG as a special case provided we define the free energy objective
appropriately.
Active inference automatically deals for different kinds of uncertainty including in the generative
model itself.
The variational formulation is also good because it allows us to include various kinds of objectives
and constraints.
Also it does not presume any separation principle as in LQG it naturally emerges.
Overall active inference is a rich and emerging area with lots of open research problems for
the control signal processing and statistics community.
With this I would like to thank you for your time and your attention and your interest
in our research.
