Welcome to this presentation on the application of the free energy principle to estimation
and control.
My name is Henk Reimers and I'm with Chalmers University of Technology.
This is joint work with Thais van de Laar from TU Andhoven and Aitja Ocele-Kale from
Uppsala University.
The motivation for this work lies in the area of stochastic optimal control.
The picture on the right shows an agent which can act on its environment, infers the state
of the environment through sensors and observations, and collects rewards.
Using a control theoretic notation, we have the following ingredients.
The state X, which can contain the state of the agent and the world.
The control or action U, the observations Y, and finally the goals.
A classic example is LQG, which stands for Linear Quadratic Gaussian Control.
Here the agent has linear dynamics and a quadratic cost, quadratic in both the state
and the control.
LQG features a so-called separation principle, where the control assumes a point estimate
of the state and ignores any uncertainty of the state.
And on the other hand, the inference is done by a Kalman filter, which assumes known past
controls.
This decouples the control and inference problem.
In more general contexts, inference can be solved through the belief propagation algorithm,
of which the Kalman filter is a specific instance.
In addition to this classic example, there exist more modern approaches, such as model
predictive control and reinforcement learning.
In model predictive control, we assume to have knowledge of a generative model of the
agent dynamics, as well as the observation model.
The goals are encoded with a loss or objective function, and based on this, the model predictive
controller solves an optimization problem to determine the current control action.
On the other hand, reinforcement learning typically is model free, so it does not have
a generative model, and it builds value functions or policies.
The goals of the agent are encoded via a reward signal.
In this work, we will use a third way, called active inference, based on the reference below.
Roughly speaking, active inference tries to minimize free energy or entropy.
Before taking the quote from this paper, it acts to minimize surprise.
Avoiding surprise is a fundamental for survival and speaks to the basic need of organisms
to maintain equilibrium with their environment.
The main question that we want to answer in this work is the following.
Can we express stochastic optimal control in general, and LQG control specifically as
special cases of active inference?
With this motivation in mind, this brings me to the outline of this presentation.
First, I will provide some brief background on LQG and belief propagation.
Then I will elaborate on different ways how belief propagation has been used for control
problems.
Then we move on to the main topic, which is the use of active inference for control and
relation to LQG.
I will then show one example result and wrap up with conclusions.
So let's first start with LQG and belief propagation.
Let us start with LQG, linear quadratic Gaussian control.
I have taken this information on this slide from Wikipedia.
The first box shows the evolution of the state as well as the observation model.
The state at time i plus 1 is a linear function of the state at time i and the control perturbed
by Gaussian noise.
The observation at time i is a linear function of the state at time i plus independent Gaussian
noise.
So this is the linear part of the LQG.
The quadratic part refers to the cost.
The cost to be minimized is a quadratic function in the state and in the control.
There's an expectation over all random variables.
So basically we want our agent to stay close to zero and do this with as low control as
possible.
It turns out that this system can be solved in closed form and the discrete time LQG controller
is given by these equations.
The first equation gives us the evolution of the state given the control.
And this is basically the equation from the Kalman filter.
The estimate of the state at time i, so here the hat refers to an estimate, depends on
the previous estimate, the control and the correction factor which depends on the current
observation.
Given an estimate of the state we can then find the control via linear feedback.
We have a linear function that depends on the state estimate.
Now in these expressions there are several matrices and these can be found from the so
called Riccati equations.
Solvener Riccati equations provides the matrices needed to find the discrete time LQG controller.
The figure on the right visualizes the LQG controller.
So we have a Kalman filter which takes into account the control input, the observation
generates a state estimate, then there is a multiplication with minus k which yields
the control.
The control is applied to the plant or the system and then this generates via a sensor
a new observation which again is fed to the Kalman filter.
We now move on to belief propagation.
In belief propagation we consider distributions over multiple variables such as here p of
x1, x2 and x3.
We assume that this distribution can be factorized as follows.
So there's a normalization constant 1 over z and then three factors fA, fB and fC where
each factor contains a subset of the arguments of the original distribution.
On this factorization we can draw a factor graph.
In this factor graph each vertex or each node corresponds to a factor and each edge
or line corresponds to a variable.
So for instance a corresponds to fA, x1 corresponds to the variable x1.
We see that a only connects to x1 so this means that fA has only one variable namely
x1 but x1 connects to A and B. This is because x1 appears in fA and fB.
With this factor graph we can run the belief propagation algorithm.
This is a message passing algorithm over the factor graph.
The messages we pass are functions of the associated variables.
So again we here have the factor graph and we will now pass messages from the edges of
the factor graph inward.
So we first compute a message from A to x1 which we denote by mu from fA to x1 evaluated
in the value x1.
This message or this function is just given by the original function fA of x1.
At the same time we can compute the message from x3 to c so mu from x3 to fc evaluated
in the value x3 and we just set this to 1.
Now given these two messages we can compute messages from fB to x2 and fc to x2.
Both are functions of the variable x2.
The message from fB to x2 evaluated in the value x2 is given by the function fB multiplied
by the incoming messages so that's the message from A to x1 and then we integrate out all
the variables except x2.
Similarly the message from fc to x2 is the function fc which depends on x2 and x3 multiplied
by the incoming message which we've computed in the previous time step and then we integrate
out over everything except x3.
Finally we can compute these messages from fB to x1 and from fc to x3 following the same
rules.
To this third time step on each edge we have two messages one from left to right and one
from right to left.
Given these messages we can compute so-called beliefs.
The belief of x1, the belief of x2 and the belief of x3 and these are simply the marginals
of the original distribution so pfx1 is obtained by taking the original distribution and integrating
out everything except x1.
This belief which is equal to the marginal is given by the product of the two messages
the message from left to right from A to x1 and the message from right to left from B
to x1 the same holds for x2 and x3 and this provides a systematic and efficient way to
infer hidden variables.
It can be shown that these beliefs are exact when the graph has no cycles, when the factor
graph has cycles or loops the beliefs are only approximations.
Belief propagation has many applications including the Kalman filter, the particle filter, decoding
of error correcting codes and image processing to name but a few.
We now focus on the relation between belief propagation and free energy minimization because
in active inference we also minimize a type of free energy.
To reveal this connection let's express the joint distribution pfx in a slightly different
way.
So here x refers to the vector of all the variables so x can be a very high dimensional
variable.
We write pfx as 1 over z and z is a normalization constant and then a product over functions
so product A over possibly many functions fA where each fA takes as argument xA where
xA is a subset of the original x.
We now introduce a new notation e of x defined as the sum of the logarithm of the individual
fAs.
With this new notation e of x we can express p of x as 1 over z times the exponential of
minus e of x.
Here e of x can be interpreted as a kind of energy so e of x tells us the energy of a
certain configuration x.
The distribution p of x will have most of its mass in low energy configurations hence
free energy minimization.
The normalization constant z can in principle be found by solving this integral.
Now with this in mind let us now try to formulate a new optimization problem.
We want to minimize with respect to q so here q is the set of distributions.
The divergence between q and p and p is the original distribution.
By plugging in the definition of the cool black library divergence we can express the
divergence as an expected energy minus an entropy minus log of z.
So the cool black library divergence has an average energy, an entropy and then finally
log of z which is called the log partition function.
The first two terms are called the beta free energy.
Since we want to minimize the divergence we try to find a q that minimizes the average
energy and at the same time has high entropy so it's as broad as possible while minimizing
the average energy.
Now this is an optimization problem so we can in principle formulate the Lagrangian
and solve this optimization problem and it turns out and this was shown in reference
to that solving for q of x given some structure on q of x leads to the belief propagation
message passing rules.
So this has a consequence that when you're doing belief propagation you're actually
solving a better free energy minimization problem namely this one.
With this background in mind we can now move on to the relation between belief propagation
and control.
Let us assume that we have a generative model and we want to infer hidden states.
So what we can do is we can determine the joint distribution of the states and the observations
given the controls.
Due to the Markov assumption this distribution factorizes as the prior times the dynamic
model and the observation likelihood.
Let's further assume that our control objective is quadratic in the state and quadratic in
the control.
Now at time t our factor graph will look something like this.
We have a part of the factor graph that relates to the past where we have the past controls
which are known because we've already decided them and also the observations up to time
t which are also known because our center has observed them.
And then we have the future.
So this here contains the future controls which we do not know and the future observations
which have not been observed yet.
The factor graph related to the past contains the states up to time t.
The factor graph of the future contains the states from time t onward.
What connects them here is the state at time t.
Now on the factor graph related to the past, the one in green, we can run belief propagation
and based on this belief propagation we can find the marginals or beliefs.
These are of the form for instance of the distribution of the state at some time t prior
to the current time t and including time t given the observations up to time t and all
the past controls.
This can be found by just simply running belief propagation on this left side.
So to be a little bit more specific, in the past we have messages with known controls
and known observations.
So here are some arrows that represent these messages and let us focus on some of these
at time t.
So the message here labeled A turns out to be the prediction.
So this is the distribution of the state at time t given the observations in the past
so before time t and also the past controls.
So this is what we know about the state at time t before observing the measurement yt.
The message B is the likelihood so this depends on the observation and then the message C
which connects the past and the future is the update.
So this is the distribution of the state at time t given all the measurements including
yt and the past controls and this update of course following the message passing rules
is the product of the likelihood function and the prediction.
And these messages may seem very familiar to you because they are just the standard
equations used in the Kalman filter and the particle filter.
Now for the future even though we have this factor graph it is meaningless to do belief
propagation.
The reason is that the control objective Ct is not encoded anywhere in this factor graph
so there is no way to determine reasonable controls.
The way this problem has been addressed in the literature is by encoding the control
objective directly in the factor graph.
So here again is our control stage cost which is quadratic in the state and quadratic in
the control.
So our paper number one proposes is to create an artificial prior on future controls and
this prior is proportional to an exponential negative the control cost.
So what this means for instance when h is very large is that our generative model tells
us that we will end up in states with low control.
Similarly to deal with the stage cost related to the state we introduce artificial known
observations and an associated likelihood function which is proportional to exponential
negative state cost.
So again when r is very large this tells us that our generative model brings us to states
where x is small.
We also remove future observations because those are not known.
This then brings us to the following factorization.
Of the states from time 0 to time capital T observations up to time little t because
future observations are removed the controls from time 1 to time capital T and then the
artificial future observations from time t plus 1 to time capital T.
We express this using the standard Markov model as a part related to the past with the prior
the state dynamics and the observation model and then a part related to the future where
we again have the state dynamics we have the prior over future controls and the likelihood
function related to future states.
Here we show again the factor graph from the previous slide where the green part relates
to the past, the red part relates to the future and x of t connects the past with the future.
Recall again in this graph the future controls and the future observations are unknown.
We now replace this part of the future with this factor graph on the right corresponding
to the factorization above.
So what we did is for each control we added the prior under control of the form described
above.
For each state in the future we add a likelihood function with an associated artificial observation
equal to 1.
Now we also removed all the future observations y t.
So now x and u are unknown and z is known.
But this is sufficient for us to run the belief propagation algorithm.
So we can do forward message passing where we start from the updated belief of x t given
all observations up to time little t and all controls up to time t minus 1.
We pass messages to the future.
We pass messages from the future to the current time ut.
Now we have two messages on this edge ut and we can compute the belief.
So following the belief propagation rule the belief ut provides us the belief over the
next control action.
We can take the mode of this belief and implement that action and then go through the entire
process again.
Now it was shown that by doing this we find modified Riccati equations for the LQG problem.
So when we specialize this scenario above to the linear state dynamics, linear observation
model and quadratic control we find something that's close to LQG but not quite.
A similar idea was proposed in reference to for state dependent costs.
It was proposed to add additional factors in the factor graph related to the state cost
with a regularizer lambda and this lambda could be tuned to obtain desired behavior.
We are now prepared to discuss the relation between active inference and LQG.
Recall that in active inference we are always trying to minimize free energy and this leads
us to our main idea.
Similar to the previous papers we will consider the future controls as hidden and their states
to be inferred.
Finally we will merge the control cost in the free energy objective.
Different from before but without any loss of generality we will use a rolling horizon
where at time t, time little t we will have a horizon from the current time up to capital
t time steps ahead.
So now we introduce our notation.
Why under bar are the observations up to time little t?
Why over bar are the observations over horizon of size capital t?
So from time t plus 1 to time t plus capital t.
Similarly the states under bar, x under bar represents the states up to time little t,
x over bar represents the future states, u under bar represents the controls up to time
t minus 1, u over bar represents the future controls including the control ut that we
want to infer.
We will then define a new generative model at time t denoted by pt.
This model depends on all the observations, all the hidden states, all the future controls
by this condition on the past controls.
We factorize this distribution into three parts, a normalization constant, the system
model which was the old generative model from early on in this presentation in which everything
was conditioned on all of the controls times a goal prior and this goal prior depends on
the future states, the future observations and the future controls.
Note that some variables such as xt appear in the goal prior and in the system model.
This is fine because the normalization constant z will make sure that the entire distribution
is well defined and normalized.
Let us look at the shape of the proposed goal prior.
Similar to the previous papers we consider to be an exponential minus lambda times the
cost over the horizon.
This cost as you may recall is quadratic in the state and in the control.
The parameter minus lambda will create different behaviors.
If lambda is very large this means that our goal prior is very concentrated around zero,
zero in the control and zero in the state.
If lambda is very small meaning close to zero this means that the goal prior is very flat
in the state and control space.
With this generative model we can now define a free energy objective at time t.
This free energy objective involves a cool black library divergence where p of t is a
generative model which is given and q is a distribution which we would like to determine.
By plugging in the definition of the cool black library divergence as well as the definition
of the generative model we can break down this free energy into two parts.
One part v of t which depends on the past and one part g of t which depends on the future.
The part g of t also contains the goal priors which relate to the future.
We show in the paper that the first time is minimized by a calmant filter and the second
term is minimized by a message passing backward in the factor graph.
The beliefs can then be found by running message passing on the corresponding factor graph
with the goal priors and these beliefs found by message passing will then also minimize
the free energy objective.
The principle outline here is called generalized free energy.
There is also a concept called expected free energy but this is different and will lead
to different controllers.
The entire factor graph is shown here.
In green again we have the past, in red and orange we have the future and what connects
them is the state at time t.
The factor graph corresponding to the past is exactly the same as it was before and it
is conditioned on all past controls.
The factor graph has its variables y and x under bar so there are the past observations
and the past states.
The graph on the right corresponds to the future.
We see that there are these orange vertices in the graph and they correspond to the factorized
goal priors.
We call that the goal priors is the exponential of a sum so this becomes a product of the
exponentials.
For each future state we have a goal prior, for each future control we have a goal prior.
We also have goal priors for future observations but these are set to uniform distributions.
We can now run message passing on this factor graph.
The messages on the factor graph are visualized here.
Let us focus on some of these messages.
The message a from x t going from left to right is the Kalman filter output after the
update so this is just a standard Kalman filter running from left to right.
The messages b and c regarding future states they are backward messages in the factor graph
which accounts for future goals so this depends on the goal priors in future times.
Now the message is e and d, the message e downward accounts for the goal prior of the
control at time t and the message d upward accounts both for the output of the Kalman
filter a and the future goals c and b.
We can then multiply the messages e and d and this provides us the belief of the control
at time t which we are interested in.
We can then apply the mode of this belief to the system and shift our time horizon with
one step and apply the same principle again.
The question now arises what can we prove about the relationship between active inference
as proposed in the previous slide and LQG.
In the paper we proved the following two statements.
Under a deterministic generative model active inference solves a stochastic optimal control
problem.
So here the state dynamics and the observation model are fully deterministic.
Secondly even when the generative model is not deterministic but lambda becomes very
small but positive then also active inference solves the underlying stochastic optimal control
problem.
To make this specific for LQG we find the following expressions for the controller with
the gain of the controller as well as the additional matrices that are also used in
the Riccati equations.
Different from the standard Riccati equations we have lambda in many of these expressions.
But again it is possible to show that when either condition one or two holds then the
lambda cancels out in all these expressions and we recover exactly the LQG Riccati equations.
So it is easy to verify that under condition one or two the controller becomes exactly
the LQG controller.
Let us show some results to verify our claims.
This slide shows the performance of the LQG and active inference controller for a time
window of 10 steps.
Each of the four figures has on the x-axis time and on the y-axis different quantities
of interest.
On the bottom left we have the control magnitude which we would like to be small, we have the
state magnitude which we would also like to be small, on the bottom right we have the
accumulated control cost and on the top right we have a measure of the free energy.
In each of the figures we show three curves, the LQG is in blue, in red is the active inference
controller with lambda equal to 1 and in green is the active inference controller with
lambda equal to 0.1.
We observe that the LQG controller applies large controls in the beginning to bring
the magnitude of the state close to 0 and then apply small corrections.
In contrast the active inference controller with lambda equal to 1 is very sluggish and
applies only very small controls leading to a slow decrease of the state magnitude as
a function of time.
The active inference controller with a small lambda equal to 0.1 behaves very close to
the LQG controller.
This can also be seen in the bottom right showing the accumulated cost, where LQG and
the active inference with lambda equal to 0.1 achieve similar accumulated costs while
the active inference controller with a large lambda leads to a large accumulated cost.
So these results confirm that for small lambda the active inference controller recovers LQG.
So it is interesting to see that larger lambda leads to less aggressive control with lower
free energy but higher cost and this result is somewhat counterintuitive.
One would expect that a larger lambda would lead to more aggressive control bringing the
state and the control magnitude close to 0 quickly.
Note that with a large lambda we will have a very concentrated goal prior around the
target value of 0.
The interpretation of this is as follows.
If your model tells you that you will anyway end up where you want, so this means with
large lambda your model tells you that you will have a control and a state close to 0
then there is no need to work hard to get there.
So there is no need to apply large controls to reach your goal because you think you will
get there anyway.
So this was a counterintuitive but interesting result.
This now leads us finally to our conclusions.
Active inference is a flexible and general framework that can be applied to stochastic
control and it has classical LQG as a special case provided we define the free energy objective
appropriately.
Active inference automatically deals for different kinds of uncertainty including in the generative
model itself.
The variational formulation is also good because it allows us to include various kinds of objectives
and constraints.
Also it does not presume any separation principle as in LQG it naturally emerges.
Overall active inference is a rich and emerging area with lots of open research problems for
the control signal processing and statistics community.
With this I would like to thank you for your time and your attention and your interest
in our research.
