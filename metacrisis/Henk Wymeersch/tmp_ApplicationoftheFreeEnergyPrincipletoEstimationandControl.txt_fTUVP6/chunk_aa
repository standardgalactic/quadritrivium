Welcome to this presentation on the application of the free energy principle to estimation
and control.
My name is Henk Reimers and I'm with Chalmers University of Technology.
This is joint work with Thais van de Laar from TU Andhoven and Aitja Ocele-Kale from
Uppsala University.
The motivation for this work lies in the area of stochastic optimal control.
The picture on the right shows an agent which can act on its environment, infers the state
of the environment through sensors and observations, and collects rewards.
Using a control theoretic notation, we have the following ingredients.
The state X, which can contain the state of the agent and the world.
The control or action U, the observations Y, and finally the goals.
A classic example is LQG, which stands for Linear Quadratic Gaussian Control.
Here the agent has linear dynamics and a quadratic cost, quadratic in both the state
and the control.
LQG features a so-called separation principle, where the control assumes a point estimate
of the state and ignores any uncertainty of the state.
And on the other hand, the inference is done by a Kalman filter, which assumes known past
controls.
This decouples the control and inference problem.
In more general contexts, inference can be solved through the belief propagation algorithm,
of which the Kalman filter is a specific instance.
In addition to this classic example, there exist more modern approaches, such as model
predictive control and reinforcement learning.
In model predictive control, we assume to have knowledge of a generative model of the
agent dynamics, as well as the observation model.
The goals are encoded with a loss or objective function, and based on this, the model predictive
controller solves an optimization problem to determine the current control action.
On the other hand, reinforcement learning typically is model free, so it does not have
a generative model, and it builds value functions or policies.
The goals of the agent are encoded via a reward signal.
In this work, we will use a third way, called active inference, based on the reference below.
Roughly speaking, active inference tries to minimize free energy or entropy.
Before taking the quote from this paper, it acts to minimize surprise.
Avoiding surprise is a fundamental for survival and speaks to the basic need of organisms
to maintain equilibrium with their environment.
The main question that we want to answer in this work is the following.
Can we express stochastic optimal control in general, and LQG control specifically as
special cases of active inference?
With this motivation in mind, this brings me to the outline of this presentation.
First, I will provide some brief background on LQG and belief propagation.
Then I will elaborate on different ways how belief propagation has been used for control
problems.
Then we move on to the main topic, which is the use of active inference for control and
relation to LQG.
I will then show one example result and wrap up with conclusions.
So let's first start with LQG and belief propagation.
Let us start with LQG, linear quadratic Gaussian control.
I have taken this information on this slide from Wikipedia.
The first box shows the evolution of the state as well as the observation model.
The state at time i plus 1 is a linear function of the state at time i and the control perturbed
by Gaussian noise.
The observation at time i is a linear function of the state at time i plus independent Gaussian
noise.
So this is the linear part of the LQG.
The quadratic part refers to the cost.
The cost to be minimized is a quadratic function in the state and in the control.
There's an expectation over all random variables.
So basically we want our agent to stay close to zero and do this with as low control as
possible.
It turns out that this system can be solved in closed form and the discrete time LQG controller
is given by these equations.
The first equation gives us the evolution of the state given the control.
And this is basically the equation from the Kalman filter.
The estimate of the state at time i, so here the hat refers to an estimate, depends on
the previous estimate, the control and the correction factor which depends on the current
observation.
Given an estimate of the state we can then find the control via linear feedback.
We have a linear function that depends on the state estimate.
Now in these expressions there are several matrices and these can be found from the so
called Riccati equations.
Solvener Riccati equations provides the matrices needed to find the discrete time LQG controller.
The figure on the right visualizes the LQG controller.
So we have a Kalman filter which takes into account the control input, the observation
generates a state estimate, then there is a multiplication with minus k which yields
the control.
The control is applied to the plant or the system and then this generates via a sensor
a new observation which again is fed to the Kalman filter.
We now move on to belief propagation.
In belief propagation we consider distributions over multiple variables such as here p of
x1, x2 and x3.
We assume that this distribution can be factorized as follows.
So there's a normalization constant 1 over z and then three factors fA, fB and fC where
each factor contains a subset of the arguments of the original distribution.
On this factorization we can draw a factor graph.
In this factor graph each vertex or each node corresponds to a factor and each edge
or line corresponds to a variable.
So for instance a corresponds to fA, x1 corresponds to the variable x1.
We see that a only connects to x1 so this means that fA has only one variable namely
x1 but x1 connects to A and B. This is because x1 appears in fA and fB.
With this factor graph we can run the belief propagation algorithm.
This is a message passing algorithm over the factor graph.
The messages we pass are functions of the associated variables.
So again we here have the factor graph and we will now pass messages from the edges of
the factor graph inward.
So we first compute a message from A to x1 which we denote by mu from fA to x1 evaluated
in the value x1.
This message or this function is just given by the original function fA of x1.
At the same time we can compute the message from x3 to c so mu from x3 to fc evaluated
in the value x3 and we just set this to 1.
Now given these two messages we can compute messages from fB to x2 and fc to x2.
Both are functions of the variable x2.
The message from fB to x2 evaluated in the value x2 is given by the function fB multiplied
by the incoming messages so that's the message from A to x1 and then we integrate out all
the variables except x2.
Similarly the message from fc to x2 is the function fc which depends on x2 and x3 multiplied
by the incoming message which we've computed in the previous time step and then we integrate
out over everything except x3.
Finally we can compute these messages from fB to x1 and from fc to x3 following the same
rules.
To this third time step on each edge we have two messages one from left to right and one
from right to left.
Given these messages we can compute so-called beliefs.
The belief of x1, the belief of x2 and the belief of x3 and these are simply the marginals
of the original distribution so pfx1 is obtained by taking the original distribution and integrating
out everything except x1.
This belief which is equal to the marginal is given by the product of the two messages
the message from left to right from A to x1 and the message from right to left from B
to x1 the same holds for x2 and x3 and this provides a systematic and efficient way to
infer hidden variables.
It can be shown that these beliefs are exact when the graph has no cycles, when the factor
graph has cycles or loops the beliefs are only approximations.
Belief propagation has many applications including the Kalman filter, the particle filter, decoding
of error correcting codes and image processing to name but a few.
We now focus on the relation between belief propagation and free energy minimization because
in active inference we also minimize a type of free energy.
To reveal this connection let's express the joint distribution pfx in a slightly different
way.
So here x refers to the vector of all the variables so x can be a very high dimensional
variable.
We write pfx as 1 over z and z is a normalization constant and then a product over functions
so product A over possibly many functions fA where each fA takes as argument xA where
xA is a subset of the original x.
We now introduce a new notation e of x defined as the sum of the logarithm of the individual
fAs.
With this new notation e of x we can express p of x as 1 over z times the exponential of
minus e of x.
Here e of x can be interpreted as a kind of energy so e of x tells us the energy of a
certain configuration x.
The distribution p of x will have most of its mass in low energy configurations hence
free energy minimization.
The normalization constant z can in principle be found by solving this integral.
Now with this in mind let us now try to formulate a new optimization problem.
We want to minimize with respect to q so here q is the set of distributions.
The divergence between q and p and p is the original distribution.
By plugging in the definition of the cool black library divergence we can express the
divergence as an expected energy minus an entropy minus log of z.
So the cool black library divergence has an average energy, an entropy and then finally
log of z which is called the log partition function.
The first two terms are called the beta free energy.
Since we want to minimize the divergence we try to find a q that minimizes the average
energy and at the same time has high entropy so it's as broad as possible while minimizing
the average energy.
Now this is an optimization problem so we can in principle formulate the Lagrangian
and solve this optimization problem and it turns out and this was shown in reference
to that solving for q of x given some structure on q of x leads to the belief propagation
message passing rules.
So this has a consequence that when you're doing belief propagation you're actually
solving a better free energy minimization problem namely this one.
With this background in mind we can now move on to the relation between belief propagation
and control.
Let us assume that we have a generative model and we want to infer hidden states.
So what we can do is we can determine the joint distribution of the states and the observations
given the controls.
Due to the Markov assumption this distribution factorizes as the prior times the dynamic
model and the observation likelihood.
Let's further assume that our control objective is quadratic in the state and quadratic in
the control.
Now at time t our factor graph will look something like this.
We have a part of the factor graph that relates to the past where we have the past controls
which are known because we've already decided them and also the observations up to time
t which are also known because our center has observed them.
And then we have the future.
So this here contains the future controls which we do not know and the future observations
which have not been observed yet.
The factor graph related to the past contains the states up to time t.
The factor graph of the future contains the states from time t onward.
What connects them here is the state at time t.
Now on the factor graph related to the past, the one in green, we can run belief propagation
and based on this belief propagation we can find the marginals or beliefs.
These are of the form for instance of the distribution of the state at some time t prior
to the current time t and including time t given the observations up to time t and all
the past controls.
This can be found by just simply running belief propagation on this left side.
So to be a little bit more specific, in the past we have messages with known controls
and known observations.
So here are some arrows that represent these messages and let us focus on some of these
at time t.
So the message here labeled A turns out to be the prediction.
So this is the distribution of the state at time t given the observations in the past
so before time t and also the past controls.
So this is what we know about the state at time t before observing the measurement yt.
The message B is the likelihood so this depends on the observation and then the message C
which connects the past and the future is the update.
So this is the distribution of the state at time t given all the measurements including
yt and the past controls and this update of course following the message passing rules
is the product of the likelihood function and the prediction.
And these messages may seem very familiar to you because they are just the standard
equations used in the Kalman filter and the particle filter.
Now for the future even though we have this factor graph it is meaningless to do belief
propagation.
