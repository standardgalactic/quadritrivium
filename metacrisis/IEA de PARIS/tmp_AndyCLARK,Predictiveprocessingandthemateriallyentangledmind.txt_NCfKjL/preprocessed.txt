Hello, good morning, welcome back to those who were here yesterday, welcome for the first
time to a few people, welcome to this beautiful space, again thank you to the Institute for
Advanced Study and all of the staff including those here for making this possible.
So yes, I'm John Sutton, this is day two of our workshop on Memory, Place and Material
Culture. We had a wonderful afternoon yesterday, thank you to the speakers, the audiences and
the conversations flowed into the evening and I hope you continue today. Before morning tea,
after our first two talks, I'll just talk through a little bit of the practical logistics for the
rest of the day today, but I want to get us going straight away. We'll have two talks before
morning tea in the courtyard and again as yesterday, we'll have quite short question
times after the talks. Let me encourage you all to contribute and pitch in with your requests
for more and criticisms and complaints and comments and we'll see how the day takes us.
So first up, it's with great pleasure again, I've had the chance to introduce Andy a couple
of times over the years and thank you so much for coming over in the middle of a kind of book
tour that Andy's doing for his latest book, The Experience Machine. Andy, as you know,
is a professor of philosophy and informatics right at the University of Sussex. His work is
known to all of us, one of the great contributors to embodied extended distributed cognition and
many other fields in philosophy of mind and cognitive science and more recently in the work
on predictive processing. Today we're going to hear from Andy about predictive processing and
the materially entangled mind. That's great, thanks for having me, it's a great pleasure to be here.
You're right, the conversation did flow on into the night a bit, so forgive any fuzziness here.
So this talk is indeed about predictive processing, but mostly about material entanglements. It's part
of a project that I got involved in called Escape. It's a synergy grant, a European synergy grant
and it's well outside my comfort zone, you know, philosophy, informatics and suddenly bits of
archaeology, which is a surprise. So yeah, it's the intersection of all these things. It's a new
venture and we're just going to talk about some of the things that we're doing in the Sussex wing
of it. So it's a very big project with many moving parts. It started in 2018 when Felipe
Criado Boado contacted me out of the blue, someone I didn't know before, and along with a vision
scientist Luis Martinez, he had this piece accepted for scientific reports. It was an interesting
piece. The title alone will tell you something. The title was Co-evolution of visual behavior,
the material world and social complexity depicted by the eye tracking of archaeological objects in
humans. So it was an unusual paper with a very long title, which basically tells you what they're
doing. But I've long sort of pondered the kind of idea that our interactions with a built environment
might be doing something very fundamental, something maybe more fundamental than cognitive
science has yet managed to properly model or fully appreciate. So that was sort of, you know,
what they seem to be showing there was something that is very much like the sort of theme of natural
born cyborgs and other stuff that I've done, that we kind of get to merge with our own technologies,
creating hybrid forms, extended minds, that sort of stuff. That's why I was resonating to it.
But it didn't turn out really to be an extended mind project. I think it's something quite different
to that, but in an interesting way. So if you don't know what the extended mind is,
you don't need to know what the extended mind is for the purposes of today's talk.
What they were looking at was pots, basically, pottery from many different periods and the way
different pottery styles seem to be capturing different patterns of eye movement. So, you know,
basically, lots of pots from different eras and eye tracking of people from the present era,
obviously, looking at those pots. So they were particularly, they were full three-dimensional
replicas of the pots. They were representative of five different material styles belonging to five
distinct chronocultural periods, and those are the periods. So it's not surprising that if you've
got a pot that's decorated in a different way, you look at it differently, because, you know,
you've got nothing else to do in this task. You're just showing a pot and kind of, yeah,
there it is. So you've got to do something with the pot. You look at it, you look at it in different
ways according to what the decorations are on it. This is probably a pretty low-level thing.
At least it looks like the same gaze patterns probably emerged in the humans that first looked
at those pots as they're emerging in current humans looking at those pots. So it's pretty low-level
capture of attention. However, what they were talking about in the paper mostly was what they
saw as an interesting correlation between social complexity and the different patterns of visual
examination, the different styles encouraged. So I will just go through that briefly. It just sets
a scene for what I'm really going to talk about. So what they found was that societies had more
hierarchical nesting in their civic organizations. So not so much a flat organization, a flat social
organization. There's one in which you've got kind of, you know, a committee up here and a subcommittee
down there and a subcommittee there and more subcommittees. So if you had more nesting like
that in your civic organization, you had pottery styles that had kind of more up and down stuff
going on and your eyes would move around the pots differently. In particular, they were looking at
the vertical index which is just roughly the percentage of times your eyes are going up rather
than sideways. So a particular index, what they found is higher vertical index correlates with
more hierarchical nesting of governance and control. So, you know, there's a question, you know,
what does that show? My gut reaction at that time, actually it's still my gut reaction now,
is it doesn't really tell us very much. It just tells us that different pots have different styles
on them and that complex pots from societies that have that sort of nesting do encourage this up
and down style. So this could just be a case where complex societies are going new ways of making
stuff and that results in distinctive designs that have a higher vertical index. That's I think
the deflationary hypothesis. In this case, I think it might well be the right hypothesis.
However, there was a question, could this actually be evidence of something else,
some kind of subtle circular loop in influence between visual encounters with pottery with a
high vertical index and the ongoing construction of social orders with increasingly nested complexity?
In other words, could it be the case that if you lived in a society in which you had pots that
had that complex structuring in, that sort of encouraged you to generate even more complex
nested structures of governance and control? It's an interesting hypothesis. Could the
pottery mediated scan path complexity and the social complexity be each feeding of the other?
So that was the question that they brought along. As I say, my gut reaction deflationary
and still deflationary, to be honest, I'm still not convinced that the pottery stuff tells us
that much yet. But the question that I became interested in is this other question. So
how could you decide between the sort of the more radical hypothesis there and the less radical
hypothesis? So I think we're trying here to decide between radical entanglement, the various complex
worlds we build and live in fundamentally and non-trivially, all to the way our biological
brains think and reason. And what you could think of as simple causation. No, the changes aren't that
deep. It's just body of knowledge changes, social practices change. There are small changes in
brains, but there's not an interesting reciprocal exchange going on here. Not even during lifetime
learning, if you see what I mean. So we're not talking genetic change here. I think no one's
really expecting much of that. So there are people that have asserted some kind of radical
entanglement. Dan Dennett, Mithun, Stephen Mithun. Sorry, I always forget who's who there. It's
Merlyn Donald and Stephen Mithun, Deacon and many others. So people say this radical entanglement
might happen. I've said it myself. But I don't think we have any general model of how it happens
or what might be involved. The only exception to this that I know about is Stanislaus de
Hahn's work on the cultural evolution of reading. That's really interesting stuff and that works.
But there's no bigger theory available. This is not a question about the extended mind,
I think. Mines could become extended even if radical entanglement is false. So if you're
interested in the extended mind, I think it's a kind of footnote at best to these kinds of debates.
Yeah. So we sort of said that. Fuzzy lie. Don't worry about that. So we got the grant
somewhat to my surprise. In fact, what shall I say? An awful lot to my surprise. We landed this
big grant to spend six years trying to sort this problem out, clarifying, test these ideas about
radical intact. They're planning 41 different worldwide case studies conducted both within
their original settings and across settings. So there's an awful lot of field work going on.
There's kind of picture of the stuff that's supposed to be going on in different places.
But the Sussex team aren't really doing any of that. We're just talking with them.
The danger, as we see it, is a danger of mistake in simple correlations for evidence
of two-way causation within a single dynamical hole. That's the way that we would pitch the
question. How can we tell whether this is just sort of, I don't know, just correlation
or something much more interesting? So what we're trying to do is to plot a conceptual
landscape here. What are the, what's the space of non-trivial influence of the built world on
cognitive processing? What are the principal components of that space, if you like? What
could we go looking for? And what kind of evidence might already exist? And also, trying to run a
pipeline of simulation studies, which we think can prove the possibility of radical entanglement.
I mean, you know, you can't prove that radical entanglement actually happened in the cases
that they're looking at, but at least we could, we could kind of show that it could happen,
which would be interesting. So there's a framework. It's going to be predictive processing,
sometimes called active inference. Jacob Hooey wrote a book about it called The Predictive Mind
back in 2013. I wrote a book about it called Surfing Uncertainty. Hooey's book is rather
internalist in its sort of perspective, and mine is kind of externalist, takes the interactions
with body and world very, very seriously, but it's the same underlying story. So the core idea
is that brains are embodied prediction machines. That perception, cognition, and action are jointly
determined by the brain's ongoing attempts to minimize long-term prediction error. So prediction
error over whatever your temporal horizon happens to be, whatever the temporal horizon of your brain
happens to be. If you're a very, very simple creature, you may have effectively no temporal
horizon, then you're just predicting the present. If you're a more complex creature, you've got a
longer temporal horizon. And there is a possibility that we could go back to in discussion. I've been
thinking about a bit, which is that some of our practices might be extending the temporal horizon
of our internal processing, and that would be actually quite another fundamental change that
predictive processing could explore. Because basically there's only two things that determine
your kind of punch and power as a predictive processing machine, and that's the hierarchical
depth of your generative model and the temporal sort of width of your generative model. So anything
that improves either of those is doing something fundamentally interesting. So on the face of it,
this all sounds a bit neurocentric, because it's cognitions all about what brains do or don't do.
But I think if we dig a little bit deeper, we saw a machine almost like this come in here today.
What we discover is a model of what brains do that makes creating and exploiting structure
in the world perfectly continuous with the prediction error minimizing project of the brain.
So there's a companion talk to this with a whole thing that I'm not going through today, which is
the way that predictive brains or predictive systems of any kind, if they've got a temporal horizon,
select epistemic actions on the same basis as practical ones. So they will
use the environment to store and retrieve information for just the same reasons as they
use their own brains to store and retrieve information. It's very interesting to me from the
older extended mind perspective. However, the bit that I want to look at today is the attention
side. So one feature of these accounts is that they make attention really important,
but it's not just the sort of woolly notion of attention that is kind of like, yeah,
you know, whatever William James said about it. We kind of go word, we don't quite know what it
means. And it's not a sort of simple spotlight notion. It's not like you just you've got attention
and you just stick it here. Instead, it's something that is being estimated in every neural
population and at every level of processing. And it's an estimation of the inverse variance of
the prediction error signal. So basically, the way that this pans out is that you're looking,
you're assessing, but at every level in every area of processing, the reliability of your beliefs,
your prior expectations about the world, and whatever sensory evidence is being processed
by that area at that time. So it's this sort of, this sort of weight in game, as you know,
but it's not just one scale. It's like lots and lots and lots and lots and lots of scales.
And what's being weighted is generative model based expectations against
current waves of sensory evidence. So yeah, in essence, what's going on with this variable
so-called precision weighting is it determines how you use the knowledge that you've got. It
determines what you do with whatever the current model knows about the world. It determines which
bits of what you know about the world will be most influential here and now in bringing about
more processing or further action. So it's a brain sort of, where do I put my cognitive bets kind of
thing? And you know, if you're another wing of this stuff that is mostly what I look at in the
book, the experience machine is how that bears on computational psychiatry really, because if you
get that balance wrong, then your brain doesn't know what to take seriously and whatnot. And you
have the kind of neural equivalent of fake news and everything goes to hell in the bread basket
at times. So I think it's a really promising way of doing some of that investigation, but again,
not the project right now. The main thing to note here is that attention, this assignment of precision
to every neural population or every area, isn't just a little top-down tweak to a basically bottom-up
processing mode. It's a heart and soul of fluid intelligence. The reason that you can use the
body of knowledge you've got for different plans and projects and to do different things at different
times is that you can do variable precision weighting. And that's just part of the generative
model that is acquired through experience. You have to learn what the knowledge is, but you're
also learning how to apply it and how to vary the way you apply it in different contexts. Okay.
So it's a kind of constantly running meta model. It's a sort of, it's a model of how you should use
the knowledge that's in your first-order model. So if predictive processing is on track, altered
patterns of attention, this precision estimation would amount to maybe the deepest possible
alteration to the general profile of thinking that an organism could get by interaction with its
environment. So what do we got? We got one very preliminary result which shows that training a
predictive processing engine on actually the different pottery styles does induce different
attentional templates, which we called culturally patterned attention styles, and that that actually
can influence processing on new tasks and domains. So you will then go about new tasks differently
because your prediction engine has got a sort of template for how to assign attention. And
if you apply the same template to a new task, you're performing that task in a different way
to the way you otherwise have performed it. So that's all we've shown in that. It's a very
minor proof of principle. It's a way of looking at transfer learning, as it's sometimes called,
through the lens of predictive processing. So we needed some extra formal apparatus to do that.
So it's not simply transfer of knowledge. It's transfer of patterns of attention. It's transfer
of the meta-model, if you want to think of it that way. So that's one thing that we continue to
explore that. I think that architectures, materiality, and spaces train and prime patterns of
attention that in turn influence patterns of thought and reason. And we can all say that,
but now we've got a little bit of apparatus to run simulations where you can kind of see
just how that's going on and what it looks like. So one hypothesis is that it entangling attention
is one of the main ways that the built environment transforms the mind.
But there are others as well. So that's part of the story. But what else might be going on? And in
the remaining time, which is, I know, 15, 20 minutes maybe, I'm going to flag some more dimensions
and then stop and see where that leaves us. Right. So the first one is, so this precision
weighting thing. It's obviously a big internal trick that the brain has, the meta-model that
is running of its own first-order model. But it also looks as if we build worlds that externalize
precision estimations. And that's another way that you can start to apply some of these ways of
thinking. Recall that attention is this variable precision weighting. So if I want to find a
dropped pin in a bed of hay, my brain ups the precision on the aspects of visual information
that would specify pinhood. Maybe I know it's got a red head or something. So now I up the
precision weighting on everything red coming in through the sensory organs. That's why,
you know, if you're looking for your lost car keys, it really helps to know what color the fob is.
What's going on here is you're up in precision weighting, enabling that information to jump out.
But if we think about our built world, it's a sort of big reservoir of precision,
is doing an awful lot of this work for us. So we've offloaded precision into the world
in ways that enable predicted brains to do more with less. So we do things like painting white
lines along the edge of a winding cliff top road. That's, you know, we add structure to the wider
environment. But now the brain can solve the problem of keeping the car on the road by minimizing
prediction error with respect to a few simple optical cues are now given high precision weighting.
And so there you are. Now you're going to know you've got a much simpler job to do in order to
stay on the road. So we've shifted some of the task of the precision stuff out of the brain there
and into the environment. Okay. And, you know, yeah, sort of, yeah, watch out for frogs and
things like that. It's the same kind of trick. So these otherwise arbitrary structures,
because, you know, we have to learn about these are these sort of red triangles mean watch out
for these things and so on. They're acting as local proxies for precision. And this is something
being explored by people, including Ed Hutchins, who's probably well known in this group for a
little while now. Urgent fonts, food packaging, priestly robes, these are all shortcuts for
precision estimating brains. They say and take this or that especially seriously. If you squint
a bit, which of course I now do, most of the human built world, including all of our pattern
social practices like stopping at red traffic lights and so on, is a sort of bag of tricks
for managing the precision weighting stuff that the brain is busy trying to do in order to get
our things done, whatever they happen to be driving around. And as we behave in the world,
we gradually alter it kind of automatically, uploading precision estimations even without
trying to into the world. So desire parts are maybe the most obvious example of that. You know,
people walk on the grass where they want to go and suddenly a path starts to emerge and more
people will follow that path. So that's uploaded precision into the world or offloaded precision
into the world, just as an automatic byproduct of our own moving around in the world.
Yeah. And we've actually got a paper on this somewhere. Yeah, there it is.
And I wanted to show these, I don't know what's going on with some of those desire parts, but
seems very peculiar to me, but you know, just goes to show that we don't actually know what we want
until we say ourselves doing it. So there's a sub-project. Can we think about structuring
the environment as quite literally, not metaphorically, a kind of offloading of precision?
I sort of, I don't know what the word is, worried about this metaphor, but I'm going to
offer it anyway. Just as we can quantify and study embodied carbon, the carbon emissions involved
in creating some built product, we can start to quantify and study what you could think of as
embodied precision, the amount and nature of cognitive precision estimation work being offloaded
onto some particular bit of the built environment. I think, you know, we have a paper where we're
trying to do this currently under consideration. So the idea is to formally estimate just how
much work that used to be done by the brain is now being done by some particular bit of the
environment. You can measure that with things like how many cycles of processing does it take
to settle into a solution to which way am I going to go? So intelligent buildings and cities
will provide new opportunities for all of this, obviously. You know, intelligent buildings and
cities can tailor what they offer to our individual cognitive and emotional trails. That's offloading
an awful lot of this kind of work. David Charmer's book, Reality Plus, is a very nice
exploration of this. This is David's picture of me, apparently. Not that I recognize myself
there at all. Moving around in a world in which all my favorite things are signposted, you know,
yeah, tofu and comics and things like that. So, you know, that's a world in which a lot of the
work of estimating what I should and shouldn't take seriously as I go about my tasks is being
offloaded onto the world in a way that we can now kind of quantify.
This is the thing I'm most excited about to talk about today, I think,
because it sort of intersects particularly with what Valeria was talking about yesterday.
This is the way that material culture can enable us to sort of break a generative,
an internal generative model we've already got and move beyond it. So, let's have a little look at
that. The thought is that by externalizing what we know, building mock-ups and models,
we create these new public objects that you can do things with that you can't do in your
internal generative model or else that are hard to do in your internal generative model.
There's a rather difficult boundary. But I think this might be the single most transformative
epistemic bonus that has been conferred on our species by its immersion in these sort of material,
often symbolically inflected worlds that we've built. So, think about designing a new running
shoe. If you're a predicted processing person, you're going to think of this as you're trying to
minimize error in ways that serve a preferred outcome, a predicted future state of having
built a better shoe. That's just kind of what this looks like through a predicted processing lens.
But suppose you can't see a clear route to that preferred outcome. Then what you can do is try to
reduce salient uncertainties, minimizing the quantity called expected future prediction error,
by just looping it out into the world and playing around with it in different ways.
So, build yourself a model of the training shoe and start poking and prodding that model
in order to see, like the physical model, in order to see what happens.
I think that material models are doing something really exciting here. They're letting us explore
possibilities that aren't easily available using the internal generative model that we've got.
A little bit like physically shuffling those scrabble tiles in front of you when you're
trying to come up with new words. Your brain's not all that good at shuffling those things around
in that way, but if you do it physically, then because your brain's a wonderfully
tuned perception action machine, you can prompt it in ways that will get new and interesting
things back. These are kind of obvious observations or almost trivial in a way.
And yet I think that these things have transformed humanities sort of abilities to think about
their worlds in ways that we still don't appreciate. Notice that for this to happen,
your beliefs and world models have to be more than an internal generative model.
They have to be more than a probabilistic trend in the way you navigate the world
on the basis of stored knowledge. They need to be something out there.
They need to be a concrete item, apt for sharing, attending to in different ways,
reorganizing, questioning, and I think actually apt for breaking. The breaking point is one
that I've become very interested in. We'll come a bit more to breaking in a second.
If you think about this through the lens of what's attention here, then materializing your thinking
in some sort of concrete object lets you deploy the attention that your brain can do in different
ways. And I think actually that symbolic stuff does that as well because we can then say something
like, you know, now think about the, think about, I don't know, all the corners of this table or
something. And I can't do it sort of in my visual field right now because it's too long.
But if you just say that, I can start to attend in that way. Basically, I think that this is
sort of epistemic gold. By offloading and materializing stuff, we create objects that let us
see beyond the current limits of the generative models that our brains are bringing to bear.
That's a very powerful thing. This is an example. I only stuck this in since last night,
since I talked about diagrams and so on yesterday. But it's an interesting example.
It worked from Chambers and Reisberg way back in the mid-80s. And basically what they were doing
was they were showing, they were getting a bunch of people and making sure that they understood
the notion of a bistable figure like the famous duck rabbit. And then they would show them a new
bistable figure, one that they'd not seen before. They'd only let them see the new one for five
seconds. And then they would say to them, now can you find the other interpretation in your
imagination? People couldn't, without exception, they couldn't in the original experiment.
But then they said, well, now just draw from your memory the thing that you saw and then look at it
and having drawn from their memory the very thing that they saw and then engaged it,
they were then all able to find the alternative interpretation really easily. So there's a case
where you can sort of see beyond the limits of your current internal generative model by creating
a concrete version of that generative model and engaging it using perception and action routines.
So I was really interested in it. It turns out that actually they weren't quite right there and that
you can sometimes find the reinterpretation in visual imagery. But nonetheless, it's often much
easier to find the interpretation if you offload it. And I think for some more complex problems,
it actually may be essential to offload it. But all these things sort of bootstrap with expertise.
So thinking about the Klein bottle from yesterday, maybe if you're mathematically
expert enough, you could have unfolded that in imagination. So near future collaborations
between human architects and AI systems will add further layers here because the AI's are using
their generative model to create concrete products that we can engage with our generative models.
So that will of course allow us to again break and see beyond the limits of our current generative
models. So we now have this sort of nested ecosystem of different agents all using generative models,
some of them creating concrete versions that the other ones can then try and deal with.
I think the art and design is in this model revealing model break in business too. It's a
sort of way of materializing and re-encountering very high level assumptions about ourselves
and our social and physical worlds. That's a whole other dimension here. I think Mike
Wheeler and I are thinking about this in some way. Five minutes. I'm going to go really quickly.
I'm not going to talk about Benes Ferrari. I'm not going to talk about science. We know it does that
kind of thing. All I'm going to say is that there is something out there called sophisticated
inference. And I think material culture has played a huge role in sophisticated inference.
The idea there is that, let me get one slide that does it. There you go. In sophisticated
inference, the space that you're thinking about includes your own future beliefs.
That turns out to be an enormously powerful leap in cognitive space. So you're not just
thinking about how to achieve a future goal by intervening in the world and offloading
information into the world. You're thinking about how to improve your own future beliefs by doing
that. It's only when your own future, your own beliefs actually emerges objects for you
that you can start to deliberately build better worlds to think in. You can come up with schooling
and education and lectures and all the things that we do. So the thought that I was going to
float at the end, I will jump past it so we get some discussion, is just that offloading stuff,
creating material culture, may actually have come before our beliefs appeared to us as objects
that then enabled us to build material cultures so that we could improve them deliberately,
if you like. So once I've materialized my belief in how much you paid for something,
because a lot of knots on a bit of string, for example, then I can start to have thoughts about
those thoughts, question those beliefs, see them as beliefs. So maybe changes in the material order
preceded the emergence of sophisticated inference, in which case something enormously
important happened at that point. And that's where I'm going to start. Thank you.
