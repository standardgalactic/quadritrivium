What's going on here is you're up in precision weighting, enabling that information to jump out.
But if we think about our built world, it's a sort of big reservoir of precision,
is doing an awful lot of this work for us. So we've offloaded precision into the world
in ways that enable predicted brains to do more with less. So we do things like painting white
lines along the edge of a winding cliff top road. That's, you know, we add structure to the wider
environment. But now the brain can solve the problem of keeping the car on the road by minimizing
prediction error with respect to a few simple optical cues are now given high precision weighting.
And so there you are. Now you're going to know you've got a much simpler job to do in order to
stay on the road. So we've shifted some of the task of the precision stuff out of the brain there
and into the environment. Okay. And, you know, yeah, sort of, yeah, watch out for frogs and
things like that. It's the same kind of trick. So these otherwise arbitrary structures,
because, you know, we have to learn about these are these sort of red triangles mean watch out
for these things and so on. They're acting as local proxies for precision. And this is something
being explored by people, including Ed Hutchins, who's probably well known in this group for a
little while now. Urgent fonts, food packaging, priestly robes, these are all shortcuts for
precision estimating brains. They say and take this or that especially seriously. If you squint
a bit, which of course I now do, most of the human built world, including all of our pattern
social practices like stopping at red traffic lights and so on, is a sort of bag of tricks
for managing the precision weighting stuff that the brain is busy trying to do in order to get
our things done, whatever they happen to be driving around. And as we behave in the world,
we gradually alter it kind of automatically, uploading precision estimations even without
trying to into the world. So desire parts are maybe the most obvious example of that. You know,
people walk on the grass where they want to go and suddenly a path starts to emerge and more
people will follow that path. So that's uploaded precision into the world or offloaded precision
into the world, just as an automatic byproduct of our own moving around in the world.
Yeah. And we've actually got a paper on this somewhere. Yeah, there it is.
And I wanted to show these, I don't know what's going on with some of those desire parts, but
seems very peculiar to me, but you know, just goes to show that we don't actually know what we want
until we say ourselves doing it. So there's a sub-project. Can we think about structuring
the environment as quite literally, not metaphorically, a kind of offloading of precision?
I sort of, I don't know what the word is, worried about this metaphor, but I'm going to
offer it anyway. Just as we can quantify and study embodied carbon, the carbon emissions involved
in creating some built product, we can start to quantify and study what you could think of as
embodied precision, the amount and nature of cognitive precision estimation work being offloaded
onto some particular bit of the built environment. I think, you know, we have a paper where we're
trying to do this currently under consideration. So the idea is to formally estimate just how
much work that used to be done by the brain is now being done by some particular bit of the
environment. You can measure that with things like how many cycles of processing does it take
to settle into a solution to which way am I going to go? So intelligent buildings and cities
will provide new opportunities for all of this, obviously. You know, intelligent buildings and
cities can tailor what they offer to our individual cognitive and emotional trails. That's offloading
an awful lot of this kind of work. David Charmer's book, Reality Plus, is a very nice
exploration of this. This is David's picture of me, apparently. Not that I recognize myself
there at all. Moving around in a world in which all my favorite things are signposted, you know,
yeah, tofu and comics and things like that. So, you know, that's a world in which a lot of the
work of estimating what I should and shouldn't take seriously as I go about my tasks is being
offloaded onto the world in a way that we can now kind of quantify.
This is the thing I'm most excited about to talk about today, I think,
because it sort of intersects particularly with what Valeria was talking about yesterday.
This is the way that material culture can enable us to sort of break a generative,
an internal generative model we've already got and move beyond it. So, let's have a little look at
that. The thought is that by externalizing what we know, building mock-ups and models,
we create these new public objects that you can do things with that you can't do in your
internal generative model or else that are hard to do in your internal generative model.
There's a rather difficult boundary. But I think this might be the single most transformative
epistemic bonus that has been conferred on our species by its immersion in these sort of material,
often symbolically inflected worlds that we've built. So, think about designing a new running
shoe. If you're a predicted processing person, you're going to think of this as you're trying to
minimize error in ways that serve a preferred outcome, a predicted future state of having
built a better shoe. That's just kind of what this looks like through a predicted processing lens.
But suppose you can't see a clear route to that preferred outcome. Then what you can do is try to
reduce salient uncertainties, minimizing the quantity called expected future prediction error,
by just looping it out into the world and playing around with it in different ways.
So, build yourself a model of the training shoe and start poking and prodding that model
in order to see, like the physical model, in order to see what happens.
I think that material models are doing something really exciting here. They're letting us explore
possibilities that aren't easily available using the internal generative model that we've got.
A little bit like physically shuffling those scrabble tiles in front of you when you're
trying to come up with new words. Your brain's not all that good at shuffling those things around
in that way, but if you do it physically, then because your brain's a wonderfully
tuned perception action machine, you can prompt it in ways that will get new and interesting
things back. These are kind of obvious observations or almost trivial in a way.
And yet I think that these things have transformed humanities sort of abilities to think about
their worlds in ways that we still don't appreciate. Notice that for this to happen,
your beliefs and world models have to be more than an internal generative model.
They have to be more than a probabilistic trend in the way you navigate the world
on the basis of stored knowledge. They need to be something out there.
They need to be a concrete item, apt for sharing, attending to in different ways,
reorganizing, questioning, and I think actually apt for breaking. The breaking point is one
that I've become very interested in. We'll come a bit more to breaking in a second.
If you think about this through the lens of what's attention here, then materializing your thinking
in some sort of concrete object lets you deploy the attention that your brain can do in different
ways. And I think actually that symbolic stuff does that as well because we can then say something
like, you know, now think about the, think about, I don't know, all the corners of this table or
something. And I can't do it sort of in my visual field right now because it's too long.
But if you just say that, I can start to attend in that way. Basically, I think that this is
sort of epistemic gold. By offloading and materializing stuff, we create objects that let us
see beyond the current limits of the generative models that our brains are bringing to bear.
That's a very powerful thing. This is an example. I only stuck this in since last night,
since I talked about diagrams and so on yesterday. But it's an interesting example.
It worked from Chambers and Reisberg way back in the mid-80s. And basically what they were doing
was they were showing, they were getting a bunch of people and making sure that they understood
the notion of a bistable figure like the famous duck rabbit. And then they would show them a new
bistable figure, one that they'd not seen before. They'd only let them see the new one for five
seconds. And then they would say to them, now can you find the other interpretation in your
imagination? People couldn't, without exception, they couldn't in the original experiment.
But then they said, well, now just draw from your memory the thing that you saw and then look at it
and having drawn from their memory the very thing that they saw and then engaged it,
they were then all able to find the alternative interpretation really easily. So there's a case
where you can sort of see beyond the limits of your current internal generative model by creating
a concrete version of that generative model and engaging it using perception and action routines.
So I was really interested in it. It turns out that actually they weren't quite right there and that
you can sometimes find the reinterpretation in visual imagery. But nonetheless, it's often much
easier to find the interpretation if you offload it. And I think for some more complex problems,
it actually may be essential to offload it. But all these things sort of bootstrap with expertise.
So thinking about the Klein bottle from yesterday, maybe if you're mathematically
expert enough, you could have unfolded that in imagination. So near future collaborations
between human architects and AI systems will add further layers here because the AI's are using
their generative model to create concrete products that we can engage with our generative models.
So that will of course allow us to again break and see beyond the limits of our current generative
models. So we now have this sort of nested ecosystem of different agents all using generative models,
some of them creating concrete versions that the other ones can then try and deal with.
I think the art and design is in this model revealing model break in business too. It's a
sort of way of materializing and re-encountering very high level assumptions about ourselves
and our social and physical worlds. That's a whole other dimension here. I think Mike
Wheeler and I are thinking about this in some way. Five minutes. I'm going to go really quickly.
I'm not going to talk about Benes Ferrari. I'm not going to talk about science. We know it does that
kind of thing. All I'm going to say is that there is something out there called sophisticated
inference. And I think material culture has played a huge role in sophisticated inference.
The idea there is that, let me get one slide that does it. There you go. In sophisticated
inference, the space that you're thinking about includes your own future beliefs.
That turns out to be an enormously powerful leap in cognitive space. So you're not just
thinking about how to achieve a future goal by intervening in the world and offloading
information into the world. You're thinking about how to improve your own future beliefs by doing
that. It's only when your own future, your own beliefs actually emerges objects for you
that you can start to deliberately build better worlds to think in. You can come up with schooling
and education and lectures and all the things that we do. So the thought that I was going to
float at the end, I will jump past it so we get some discussion, is just that offloading stuff,
creating material culture, may actually have come before our beliefs appeared to us as objects
that then enabled us to build material cultures so that we could improve them deliberately,
if you like. So once I've materialized my belief in how much you paid for something,
because a lot of knots on a bit of string, for example, then I can start to have thoughts about
those thoughts, question those beliefs, see them as beliefs. So maybe changes in the material order
preceded the emergence of sophisticated inference, in which case something enormously
important happened at that point. And that's where I'm going to start. Thank you.
