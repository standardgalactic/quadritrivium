Moving on to Chapter 3, the High Road to Active Inference.
And so, recall that the High Road is like the why.
So, Ali, please begin. Just go as long as you want on through the High Road to Active Inference.
Okay, so the High Road to Active Inference began with the question about the conditions for the persistence of quote-unquote things.
I mean, what would we expect for a thing to behave if it persists through time?
And that's why sometimes FEP is referred to as the theory of every space thing.
And it's not absolutely everything, but think just in this sense of, I mean, persisting through time.
And the notion that allows for this thing to act as it persists through time is Markiplanket.
So Markiplanket is one of the most essential ingredients of the High Road approach to active inference.
And that's why this section opens up with describing how Markiplanket allows to describe the situation of interest through this conceptual and mathematical tool.
But one thing that I always point out in almost all our textbook cohorts is, in this section, it doesn't, I mean, exactly describe Markiplankets rigorously enough in terms of its carving up the, I mean, state space and so on.
So one thing I think can be clarifying in following all the discussions around Markiplanket is to keep in mind that it's just a boundary in the state space.
So it's not necessarily a spatiotemporal boundary, although in many cases it manifests itself as spatiotemporal boundaries such as the, I don't know, cell membranes or organelle membranes and so on.
But it's not necessarily the case.
So I think it can be helpful to keep that in mind when we, everywhere we see Markiplanket.
But in a very, very simply Markiplanket is what allows for the agent or the system to be statistically separated from its environment, as is shown here in Figure 3.1.
So basically Markiplanket mathematically is just the, I mean, the sensory and active states together.
It consists of both of those states.
And it is statistically separates what happened externally or internally from each other.
So in other words, internal states cannot observe or infer anything about the external states directly, but only through the Markiplanket.
And that's why it's essential to describe the systems in this way, because obviously in any sparse coupled systems, there isn't a possibility to observe the external states directly, but only through those sensory and active states.
And one other thing that I also believe can be a bit confusing in this picture is that, yes, typo in the active states and sensory states, because it's important to observe that active state only, or the flow in the active states more precisely, only depends on internal and Markiplanket states.
And on the other hand, in sensory states, sensory states only depend on the external and blanket states.
So mu and x in those two equations should be exchanged.
Yes.
Yeah.
Let me just unpack this.
Before we continue to add, before we continue.
Yes.
So this is what's known as the particular partition.
And that terminology was brought to the fore by Carl Friston's famous monograph in 2019.
And it's a little bit of a pun.
It's a particular partition because this is just one way to partition agent from environment, figure from ground.
And we call what it partitions out the blankets and internal states as the particle.
So we can think about the most inert least cognitive particle is like a speck of dust doing brownie and diffusion.
But also there are more sophisticated kinds of cognitive particles that can include world models, counterfactuals, what would happen if I did this, all of those kinds of sophisticated cognitive operations that we can explore in active inference.
So note the small typo that Ali mentioned.
Internal states and also these edges reflect causal possibilities amongst internal, external and blanket states.
Blanket constituting the active you and sensory why states.
So these are map, not territory.
This is not a spatio temporal articulation.
These are like causal maps of the world that may have to do with spatial temporal boundaries, but are not simply that.
And sensory data or sensory states flow on to internal states.
Internal states are involved in action selection resulting in active states, which have some causal consequence in the world, the generative process, the external states, which results in different sensory states coming in again.
And so the Markov blanket is what makes the internal and the external states conditionally independent.
And that can be thought of as just a no telekinesis, no telepathy clause.
The only way the information comes across internal and external states, which are symmetrical with each other, is through the boundary or the holographic screen or the Markov blanket.
You can continue on Ali.
Okay, so the next section is about surprise minimization and self evidencing.
So again, this term self evidencing is a common term in active inference literature.
First proposed by Jacob Howie.
So basically, it refers to how a system or agent or in other words, a particular state can gather the evidence for itself persistence or self existence through time.
In other words, by inferring the state of the environment and comparing that inference with its internal states or in other words, with its generative model.
It's basically a way, I mean, an interpretation of that.
That kind of inference is on the one hand, there is this dynamical interaction between the internal and external state, but we can somehow interpret that the physical dynamics as engaging in the active in the active inference or as model for
the persistence of the agent through time.
So that's basically what refers to as self evidencing.
And then we go through the subsection 3.3.1, which is which sets the stage for the recent formulation of active inference.
As in mechanics, which is to somehow to interpret the the active surprise minimization as minimizing the action through the Hamiltonian principle of least action, which is a variational principle.
And by variational principle is what what is meant here is just a computational or mathematical tool that allows for the computation or the derivative, the, I mean, specific derivations to happen.
So it's not identical to scientific theories or scientific, I don't know, facts or observations.
It's just a tool, a principle, mathematical tool.
So, again, one of the main misconceptions about free energy principle is that it is all falsifiable.
Obviously, if we see it in this way, it doesn't make sense to say that a mathematical tool or principle is on falsifiable because it doesn't say anything about the empirical evidence of the phenomena we're talking about.
So that's why here it's important to understand how the surprise minimization can be seen as this kind of variational principle.
And then equation equation 3.1 draws the parallel between the surprising as defined in section in chapter two, sorry, and this chapter.
So it kinds of ties up all the arguments provided in the previous chapter with this one and how everything comes together in a single formulation of active inference that they're just the two distinct approaches to arrive at same destination.
And then we go also, sorry, the other important equation and again, another key equation here is equation 3.2, which is a kind of, sorry, yes, equation.
I meant to say equation 3.2, not equation 3.1, sorry, my bad.
So yes, the other section is about the relations between inference cognition and stochastic dynamics as, I mean, all the previous discussions around how the active inference can be seen as a kind of self evidencing through the
variational principle such as FEP comes together to reframe the previous discussions we saw in chapter two about perception as inference and action as inference.
And to see the concepts of variational free energy and expected free energy through the lens of variational free energy, variational principle of least action.
So it unifies nicely all the material from chapters two and three into something that I mean, not necessarily distinct from each other, but just two sides of.
Awesome. Yeah, I find table three one to be very exciting. It draws together statistical physics, Bayesian information, information theory, and cognitive interpretations.
It's kind of like learn one thing, learn many things. Statistical physics has been talking about minimization of variational free energy for a long time and such methods are absolutely every day and professionalized in Bayesian statistics.
Active inference is using exactly just that to describe perception and action and so on. So that's very exciting.
Box 3.2 describes free energy in statistical physics and active inference. I sometimes joke that we have a few kinds of free energy. We have Tesla like electrical power should be available to everybody for no cost.
We're not talking about that right now. There's Gibbs free energy, which is the quantity that makes chemical or thermochemical reactions irreversible, like the hydrolysis of ATP.
And when we're talking about variational free energy, we're talking about information geometric space with similar dynamics, similar kinetics and thermodynamics, but rather than describing the reaction coordinates of a chemical reaction, we're thinking about it in terms of Bayesian updating.
And this is all called the Bayesian mechanics.
3.41 continues on with variational free energy.
3.42 goes into expected free energy. So we see this a lot. F, variational free energy. That's the real time unfolding sensory flow. And then G, expected free energy and the policy planning as inference.
Section 3.5 concludes with a novel foundation active inference to understand behavior and cognition. And it describes a few features of active inference, including its distinguishing features from a few other ways that people have looked at behavior and cybernetics.
Section 3.6 goes into a bit more detail on models, policies and trajectories having to do with agency and policy selection.
3.7 reconciliation of inactive cybernetic and predictive theories under active inference. Again, the exact kind of thing that there's a whole literature on and it's always amazing to hear everybody's perspective on in the textbook groups.
3.8 active inference from the emergence of life to agency and 3.9 summary. Do you want to just give any other thoughts that you have on chapter 3?
Again, it's one of the probably most fundamental chapters. And I mean, the topics covered in this chapter is essential or absolutely essential to understand everything active inference related.
Specifically, the discussions in section 3.6 onward. Again, I believe is really important to understand the broader context of active inference and how it relates to all the other theories.
But specifically, section 3.8 provides a nice view of how we can use the same modeling to the same theoretical tools to model both non-living dynamical systems or sparse couple dynamical stochastic systems and also to the systems that has agency
or sentience. So it's a much broader theoretical framework that doesn't restrict itself to only particular kinds of systems or agents. And we'll see much more elaboration on that in the later.
Wonderful. Alright, that concludes our overview on chapter 3.
