Chapter four is called the generative models of active inference and it begins with a quotation,
everything should be made as simple as possible but not simpler by Albert Einstein.
Ali, what is your overview thought or warning for chapter four?
Okay, so after the preliminary materials in chapters two and three,
which was largely based on providing some conceptual framework for developing the further
theory, chapter four delves into much more detail in terms of mathematical formulation and it
unpacks a lot more the way that the central equations of active inference is derived and
how to construct the important elements of active inference models, so say matrices A, B, C and D
and also how to put together generative models in different situations. So it basically lays
out the foundation for constructing active inference models both for discrete time situations
and continuous time ones, which will be used later in chapters seven and eight, but this is
probably one of the most challenging and at least mathematically dense
chapters in the book. So I would personally suggest reading through this chapter really slowly
and even if we don't get to understand every single detail of the chapter,
obviously we can return per required as we go through the textbook.
Thank you, Ali. Yes, so let's look through the sections. Just to add on though, chapter four is
one of the larger and more equation dense chapters because it is the common kernel or basis that's
then going to get applied in chapter five in the neurobiological case. There's a recipe for making
chapter four in chapter six, that's the recipe for active inference modeling,
chapter seven and eight are about the discrete and the continuous time variant or subtype or motif
of these kinds of things called generative models. So this is the real common root and
we'll just look at what the sections are. This chapter complements the preceding
chapter's conceptual treatment of active inference with a more formal treatment.
Section 4.2 from Bayesian inference to free energy. What would you say about this section, Ali?
Okay, so as we know, the free energy principle is inspired by previous work on Bayesian inference.
I mean, all the way back to Helmholtz's theory about unconscious inference or something to that
effect. I can't remember the exact term, but here, I mean, it provides in a bit more detail
how we can derive free energy principle formalism using the established Bayesian inference
formulation and particularly one of the key movements or at least one of the key decisions
through the derivation of free energy principle formulation is using Jane's inequality principle
to derive an upper bound instead of just using the exact values to compute
or to achieve required parameters. So that's basically, in my opinion, the key premise of
section 4.2 and to see in a bit more detail how we can achieve those upper bounds using
Jane's inequality directly by using, I mean, manipulations of Bayesian inference,
Bayesian statistical formalism.
Thanks, I'll just add one point from this section. Broadly, these are the problems of
inferring states of the world perception and inferring a course of action planning. So this
is again, referring to the perception and action and everything that happens in between is the
internal or the cognitive part of the inference, but this is like the blanket state cybernetic
input output. And then let's look at the first equation or how much equations overall
or what equations do you think we should highlight?
Okay, so maybe we can, I mean, just as a general comment about these different equations,
well, each of these equations provide a distinct step toward deriving the ultimate whole picture.
So even if we don't quite understand how we can derive from each step to the other one,
it's good to know that it's only required to understand how we get to that ultimate whole
picture. But ultimately, what we would need in order to develop for develop active inference
models is the ultimate equation or ultimate whole picture. So this is just a way to elucidate
the steps toward developing that whole picture. But again, it's not an essential requirement
to understand the materials of the rest of the book. But if we go from,
I mean, equations 4.1 toward the 4.4, or in other words, a variation of free energy,
well, equation 4.1 is just a basic definition of some properties and probabilities
in terms of conditional probability and so on. So equation 4.2 provides the
central Jane's inequality principle and how it relates to, I mean, conditional probabilities
and of course, joint probabilities. And then by using those two properties or those two
equations, we ultimately get to 4.4, which is the definition of variation free energy parameter
which is the parameter of interest that needs to be optimized in order to inference to happen
or at least perceptual inference to happen in active inference models.
Thanks. Only thing I'll add is F is the letter used for variational free energy. Think of it
like a computer program and the arguments that it takes in or the variables that it takes in
are q, which is the distribution that's under the statisticians control and y,
which are the data which are outside of the statisticians control. And
do you want to describe more about anything in this equation or carry on?
Just one thing that can probably be helpful is to somehow compare these steps with
the initial picture we had from chapter 2 because variation free energy was first introduced
in chapter 2. So it can be helpful to go back and forth between chapters 2 and 4 and try to
connect the dots between the related points there.
Section 4.3, generative models. All right, I'll read the first sentence,
then you can give it some thoughts. To calculate the free energy, we need three things,
data, a family of variational distributions and a generative model comprising a prior and a likelihood.
In this section, we outlined two very general sorts of generative model used for active inference
and the form the free energy takes in relation to each.
Okay, so as mentioned earlier, this chapter deals both with discrete time and continuous time
situations. So clearly, we would need two different types of generative models for each situation.
And obviously, the generative models or the way to construct generative models for
discrete time situations would vary quite a bit from the one for continuous time situations.
But the general principle underlying those generative models are basically the same,
which is, I mean, to somehow construct a model of the environment, I mean, either be it
for the situation that is sequential in time or for the situations that need to be
somehow, each moment of the situation needs to be accommodated in terms of a continuous time situation.
So figure 4.2 provides some examples of both. So for, yes, let me see.
Yes, so we have some examples of different kinds of generative models and case studies,
if you like, and it provides various ways to show how the dependencies between variables
between variables can be modeled using these kinds of graphical probabilistic models. So
one common way to represent generative models is to use these kinds of graphical probabilistic models
in active inference literature, which is at least in this case, the circles would represent
the random variables and the squares would represent the distributions, which would describe
the dependencies between those random variables. So we can see the clear
relationships between those parameters here, which is basically what this whole graph,
what constitutes the generative model that needs to be used for different situations.
And then in figure 4.3, we can compare the two different types of generative models based on
whether it's discrete time or continuous time situations. So the upper picture is a generative
model for the discrete time situation and the lower picture is the parallel continuous time
version of it. And as we can see, the general topology of these models are the same. The only
things that differ is the use of parameters for policies or I mean discrete time policies or
the continuous time ones. And we can obviously compare the different elements for both priors
states and external states, internal states, and so on by comparing these two models here.
Yeah, we often return to figure 4.3. It's kind of the Rosetta Stone
of generative modeling for the context of this book, because it's then going to develop out into
chapter seven and eight. And it represents a really fundamental decision made in modeling.
And in the later chapters, it's also shown how it can be made into a hierarchical model
that combines aspects of both. But within each level of modeling, still these are the
kinds of decisions that modelers are presented with when it comes to statistical modeling overall.
So section 4.4 goes into essentially the top half of figure 4.3, discrete time.
What would you say about discrete time?
Okay, so the discrete time situation is obviously the archetype discrete time situation,
which is the POMDP models. So at this point, I would very much like to recommend
following the material from set by step paper, because in that paper, the way to construct
POMDP models is described in a bit more detail. So if anyone feels like they should learn a bit
more about the gaps in the details, I would very much like to recommend that particular paper.
So yeah, I don't know how much detail we should go into, because it's, I mean, although it's not
maybe detailed enough for some tastes, but it goes in a quite extensive detail about how we can
construct these models using the concepts we've learned in previous chapters. So
ultimately, we reach equations 4.13 and 4.14, which are basically the culmination of
POMDP formulation using the vector notations and gradients and so on. So
that's great. Then we go to continuous time. Yeah, great.
A few things intervene in the continuous time
chapter. Once that we'll just mention here, because they're kind of boxed or
partitioned from the continuous time part, but they're following pages versus Markov blankets.
We won't go into it here, but kind of footnote that or look at some other places where we
talk about it outside of this chapter overview. Figure 4.4, basically message passing.
Again, a big topic. Let's kind of just go past it now. Back to the regularly scheduled
continuous time generative model discussion. And then another box to the generalized coordinates
of motion. So taking position plus derivatives of position. And that has some beneficial
properties that are described and unpacked also elsewhere.
Do you want to say anything about 4.5.2?
Well, the only thing that comes to mind is although, as I said before,
all the formulations here may look more, I mean, a bit too dense to understand at the first pass.
But some of the key, maybe components here could be obviously the material from box 4.2 and 4.3.
I think are quite essential to understand the underlying principle behind
deriving the continuous time situation. Because without Laplace approximation,
what we would have in terms of free energy minimization is, would look very much like
the Gibbs free energy. So I mean, the key distinction between
the free energy principle as described in active inference literature, as opposed to
Gibbs free energy, is this distinction, is the Laplace approximation. So this is what enables
us to go from Gibbs free energy to, I mean, the variation of free energy. So yeah, that's,
I mean, quite essential to make this, to be familiar with this essential approximation.
And obviously, the concept of generalized coordinates of motion will come time and time
again throughout the whole book, particularly in chapters eight and nine. So yeah, those two
concepts, I believe, needs a bit more attention. So yeah, sounds good. Box 4.3 Laplace approximation
equations, another message passing representation, and a summary. The key message to take away is
that approximate Bayesian inference may be framed as minimizing a quantity known as
variational free energy. This depends on a generative model that expresses our belief about
how data are generated. Anything else you want to add?
Nothing comes to mind at the moment, because as I said, this is, I mean, we're still
in the stage that we want to develop our essential tools to be used in the rest of the book. So
here, up until now, I believe, by the end of chapter four, we have acquired all the essential
necessary mathematical tools. And the next chapter, chapter five, is kind of acts like an interlude.
And I don't think it's the direct, I mean, continuation of chapters one through four. So
I believe the first section, or the first part of the book,
conceptually and mathematically ends here. So yeah, that's it.
Yes, it's a little bit like the pragmatic modeling part gets foreshadowed or explored in five,
now that we're all built up with four. All right, that's the end of the overview for four.
