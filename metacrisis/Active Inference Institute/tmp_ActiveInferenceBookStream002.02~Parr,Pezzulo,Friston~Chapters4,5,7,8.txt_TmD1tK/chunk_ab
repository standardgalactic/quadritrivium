The last paragraph of the first section describes that they're going to look at the
three different neural systems. Okay, section 5.2, microcircuits and messages.
What do you think, Oli? All right. So I mean, this chapter begins from how
message passing happens in neurobiological terms and compare it to the way active inference
frames this message passing mechanism. And specifically, if we look at figure 5.1 and
compare this figure to the ones we've seen before in chapters one through four, I think it was in
chapter four, we can see some clear parallels between how this kind of cortical message
passing happens in the brain versus how it is framed in active inference literature. And as we
can see, it's clearly inspired by the neurobiology of the brain. But then it's important to keep
in mind that it's not a direct one-to-one mapping between these two models. This is just
a kind of, I don't know, an interesting or illuminating, if you like, parallel to keep in mind
to somehow be a bit more confident about the viability of the theory we want to use for
message passing and active inference, which is to say it's not some haphazard theory that's just
been developed for practical reasons. It has some basis in neurobiology, although
it's not necessarily fully congruent with every detail of neurobiology.
Great. The specific example is going to involve this one region of mammalian cortex tissue
that has these six layers. And there's a ton of neurobiology.
The big takeaway for figure 5.1 is that it's possible to graphically lay out
nodes and variables and find some empirical correspondences. Again, some unique explanations
and predictions in certain cases. And that's one kind of modeling where it's really trying to
understand and improve the ability to do correlation and intervention and counterfactual
causal type analysis with the real system of interest. Or in a more pedagogical setting or
a research setting or an industrial setting, you might sweep across large families of structures
of models and there's no need to be grounded to any biological structure at all. So this is just
describing the specific neuroanatomical research that really arose out of the imaging work at UCL
and the SPM package. So that's where a lot of this comes from. 5.2? Yeah, good.
And sorry, just as a side note, I think watching one of Thomas Parr's lectures on neurobiology
of active inference, which is available on YouTube, would really help to understand the materials of
this chapter better. So I highly recommend watching that one. Thanks. Figure 5.2 gives a
re-rendering of a kind of classical view of a hierarchical predictive coding system works.
So here, abstracting a layer from the tissue six layer to just two layers here, computational layers
now, and then showing how there's hierarchical communication within a layer, but also others,
they're signaling within a layer and there's a hierarchy in Bayesian modeling with variables
that are higher order predictions about other variables. And that's the basis of the predictive
coding architecture. So 5.2 looks at some ways that the something that resonates with the cerebral
cortical architecture enables what might computationally look like or have some really strong and
explanatory values in actually relating to computationally a hierarchical Bayesian model,
which could do various general tasks. All right, 5.3 is motor commands,
leaving the prefrontal cortex going down to the butterfly looking cross section here. What is 5.3?
Okay, so 5.3 moves to the other half of active inference framework, which is,
I mean, how it can model the decision making and ultimately the movement of the agent in order to
minimize the expected free energy as opposed to variational free energy that we saw in perceptual
half of active inference. So it again provides a kind of correlation or analogy between the
structural neural anatomy, particularly related to, I mean, the motor commands and
how it can relate to active inference, particularly the continuous time active inference. So
we can see that for, I mean, for the external event or, I'm sorry, for the external state,
we can take, for example, the proprioceptive afferent, and then this proprioceptive afferent
acts as a kind of Y for the continuous time active inference, which needs to be,
I mean, processed in a way to optimize the expected free energy and how it relates to
both attention and precision. We'll see a bit more detail about those terms and the relation
between them in chapter eight, but I think here section 5.3 provides a good summary about the
general paths through the motor command systems of neurobiology.
Great. I'd say while the previous case study focused on how the connectivity within and between
the cortical columns could have a computational relationship with a Bayesian
hierarchical predictive coding architecture, the argument of the second case study is that a
continuous input, continuous output, kind of set point seeking reflexive motor behavior
with a moving set point with a descending moving set point enabling motion by changing ultimately
the set point and enabling a variation in the strategies to reach that set point through
different mechanisms. This is also describable in a compatible way.
That's a shorter section. Now, section 5.4, subcortical structures.
What would you say about this section?
Okay. So, subcortical structures are very important in the decision making and, I mean,
of the agents. So, obviously, here we need another kind of analogy between the way
that these plannings and decision makings happen neuroanatomically with the way that
that it's framed in active inference. But again, we can see it's clearly based on,
I mean, at least some of the important elements we've seen from the previous chapters.
So, for example, we saw how policy is described or how it relates to outcomes and preference and so
on. We can see those elements are directly inspired by neuroanatomical structures. So,
I guess that's, at least in my opinion, this section here 5.4 seems a bit more
sketchy in the meaning that it doesn't go into quite the extensive details about how
those structures can be compared. But for anyone who wants to further investigate these topics,
there are some useful references put on here on pages 93 and 94. So, yeah.
Thanks. Yeah, it's really abbreviated and over viewed. But we get an interlude from table 5.1
with putative roles of neurotransmitters. So, same perspective that we took before on neuroanatomical
functionalism here directly translates to neurotransmitter reductionism or essentialism
or something like that. So, certainly all neurotransmitters and molecules that play
variable roles in different settings. And this is the neat and scruffy
manifold all over again. One person might say, well, we need a theory for every acetylcholine
molecule in the world. They're all in a unique context. And someone else says,
all neurotransmitters are described by one parameter in this model. I'm getting value
from it. So, to me, that's an account. And somewhere in between is the work in this space,
which is making an attempt to have a principled and falsifiable approach to
model the computational aspects of specific regions and contexts and settings. And so,
acetylcholine, noradrenaline, dopamine and serotonin are given a little mini review here.
And so, it's not an exhaustive or an exclusive claim. It's kind of a provocation from computational
and molecular neuroscience. And people can look into the papers and also ones that probably
have been published since. 5.6 goes to continuous and discrete hierarchies,
which is graphically overviewed in figure 5.5. So, what would you say about this?
Yeah, one interesting thing about this section is the observation that
our lower-level engagement with the environment can be most successfully
characterized with continuous time formulations. But as we go up on the level of
cognitive concepts or at the level of cognitive hierarchies, and we come to concepts such as,
I don't know, decisions or even beliefs and so on, we can reach the area that the discrete time
situations would probably be more efficient to characterize the behavior of the agent. So, this
multi-scale structure of active inference modeling is quite evident in the way that
our message passing happens in our brain in terms of our lower-level data processing,
often to consolidating the higher-level cognitive concepts and ontologies.
Awesome. Thank you. To me, figure 5.5 demonstrates the kind of whole-of-body approach
that you could imagine. There's so many organs and systems and phenomena for which
there aren't specific generative models, so little can be said about situations where no
generative model has been articulated. And here's one where it has, so it gives you also,
it's kind of like reading a Drosophila melanogaster review paper relatively. It's like,
this is how much work it takes to get to this state of knowledge in an insect. So then in
another insect, do we know less about that insect empirically and genetically? So consider this to
be what's known to be a lot, however, also about one of the most sophisticated or specific cognitive
systems, at least we know. So there's that additional kind of like self-reflexive aspect
to this chapter that is not a cornerstone of active inference, but here it's just presented
in a synthetic case study. Anything else you want to say about 5?
Nothing particular comes to mind. Thank you.
All right.
Okay. Chapter seven is called active inference and discrete time.
Chapter seven is the first in a pair of chapters with chapter eight on discrete and continuous
time. So they're kind of like two forks of a river that we discussed in chapter four and before
and described the recipe in chapter six. Now seven and eight are kind of like one level deeper,
going from the kind of all of this group of animals to one level deeper into its classification scheme
on the way to the specific generative model for which it's actually given in its totality.
But everything prior to that is about the learning about its principles and this is kind of on the
trunk of the path to discrete time modeling, just like chapter eight will be about continuous
time modeling. What would you add in? Okay. So I think chapters seven and eight
really helps to understand in a more practical way how the materials from particularly chapters one
through five applies in real-time situations. So even if we somehow didn't get to understand
every details of chapters one through four, when we come to chapters seven and eight,
I think some of those uncertainties about our understandings can be clarified
at least in a practical sense. So I believe these two chapters are really helpful
in order to consolidate our understandings from the previous chapters.
Awesome. Well said. So it's going to involve specifying some discrete time models.
Seven point two goes into perceptual processing and the general structure of the chapter is going
to walk through a series of examples that build in complexity where they first start with perception
in seven point two, introduce decision making and then describe a few more types of motifs or
cognitive structure or patterns and also check out step by step and model stream one where it's
built up to in a different way. So the first example is I'll let you describe it since it's musical.
Okay. So yeah, the first example is the situation in which we try to describe the performance
of an amateur musician in terms of how we listen to the performance of an amateur musicians
in terms of the predictions we get from our anticipation of the following notes as opposed to
the actual notes that's being played. So these kinds of anticipatory reaction, listening reaction to
the musician can be successfully formalized using discrete time active inference by
putting together the matrices A for the states and matrix B for the transition between the states
or the transition probabilities which in this case describes the probability from going from
one note to the other and obviously the actual sequence that's been played which can be described
with the matrix D. So and another point I wanted to point I wanted to mention is
for anyone who has downloaded this chapter before, I don't know, I think about June or something,
I recommend re-downloading it from MIT's website because they have corrected some of the typos
that was previously present in this chapter, particularly in figure 7.2.
Cool. So this graphical model where a person is listening, this is a general perceptual
Bayesian framing, it's specified. Just like with any other equations, there's a lot to look into,
but A indicates the probability of an outcome given a state. This is saying if it were all
on the diagonal identity matrix, this is kind of a common motif, then states kind of map to
themself. So in the context of, in the context of this model, A represents the mapping between the
observed note and the underlying hidden true note. FNB describes the transition matrix of how those
change to time D is the prior. They're specified. Figure 7.2, do you want to describe it?
All right, so in figure 7.2, or at least the incomplete version of figure 7.2 we see here,
well, at the upper left part of the picture, we see, I mean, the beliefs about each note
at each step, at each time step. And at upper right, we somehow translate those beliefs into
specific numerical values. So instead of just assigning some continuous values, we
simplified the situation by assigning some discrete numerical values for each note.
And then the lower left is supposed to show the free energy gradients over time or in other terms,
the prediction errors we get from, I mean, comparing our predictions with the actual outcomes.
So lastly, the lower right picture shows, in parallel to the upper right picture,
determines the values of these errors. So we can see both the continuous, the initial,
at least initial continuous assignment and values, and then the further
discretizing of the values in order to get the discrete time situation or the more tractable
discrete time situations. Okay, so it's a general passive inference task where there's
priors about how states are going to change through time, and then there's real data coming in.
So that's the kind of classical predictive coding, video compression, Kalman filter,
Bayesian setting. 7.3 introduces a key motif, which is decision making and planning as inference.
So this is the idea of having a Bayes graph where the variables can relate to different
things. There's high composability. And here the idea is that a variable is going to be proposed
that we can do inference about that describes the process of decision making or policy selection.
So what would you say about 7.3? Okay, so 7.3 is obviously similar to what we
saw in chapter four. And if I'm not mistaken, even the topology is exactly the same with that
picture we saw previously. So this is the initial setup or which acts also as a review
about how these different components upon DP generative models
need to be described in such situations. But ultimately, the specific case study
we come across in this section is the attempt to model the behavior of the mouse in a teammate,
so the rat in a teammate. So especially teammates containing an aversive stimulus in one arm and
an attractive stimulus on the other. So this is this can act as a kind of toy example to use
this kind of probabilistic modeling to describe these situations.
Thanks. So that leads us right to figure 7.4. Here's a visualization of the situation
with the rat in this case, where there's a pleasant and aversive stimuli on each end of a
decision point. And there's also a epistemic opportunity to receive some information
about the context that the animal is in. And so that setting is described for both the case with
white on the left, black on the right, and black on the left, white on the right. And those are shown
in terms of their differences in the matrices, the explicit specification of the generative model.
Visualizations show some of the slices of the B variable, which reflect different transition
probabilities. C represents the preferences, which are expressed over the observable states.
D reflects the priors on the different states that need priors. 7.4. What would you say about this?
Okay, so in 7.4, it builds up on the previous section and adds other elements that we previously
saw in chapters 3 and sorry, 2 and 4, which is how the exact formulation for expected free energy
can be used, sorry, variation free energy can be used to formulate the tradeoff between the
I mean, information seeking and or at least between the epistemic value and information
seeking. So here, it uses, again, that rad example in a bit more, more extended and elaborate form
to formulate the epistemic value of observing Q in a given location. And figure 7.7 is a
representation of this situation. But another situation that's been, let me see, yeah, in 7.9,
another case study discussed here is the situation of the psychotic eye movements.
And because it is something that can be quite successfully described or characterized
in terms of information seeking versus the epistemic value. And the situation here is,
let me see, yeah, shown visually in figure 7.9, which clearly shows how our visual
psychotic eye movements can be described in such a way as to kind of trace the trajectory
of our eye movement among different regions of the visual space. And how the information we gather
from a given region can affect the, I mean, the subsequent trajectories of our psychotic eye movement.
