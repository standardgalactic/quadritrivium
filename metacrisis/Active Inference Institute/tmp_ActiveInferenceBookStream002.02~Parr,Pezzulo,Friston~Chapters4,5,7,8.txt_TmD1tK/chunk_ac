So, yeah, that's basically the main premise of this section, I guess.
Nice, great. 7.5? What would you say about it?
Okay, so 7.5, again, adds another dimension to the previous formulations. And this time,
we get to update the generative models by learning. And so the generative models for
this situation is a bit more complicated than the previous ones, because it now needs to
account for a mechanism or a way to update the matrices we had before. So in the previous
situations, we didn't account for learning, per se. But here, we directly update our general,
sorry, the word update can be confusing here. We get to somehow improve our generative models to
accommodate for these updating accounts. And yeah, so the situation here, or the case study
here, which somehow elucidates the way that the learning can be accounted for with these models.
Is again, a toy example of a creature in a simple world of black and white tiles, which kind of
tries to find a path to reach a given destination, a certain destination. So it is more complicated
than the situation we had for the rat example, because it only had, I mean, simple trajectories
that needed to traverse. But here, the creature or the agent, in this case, needs to do lots
of lots more learning and information seeking and so on. So all the previous elements
is kind of combined in this example. And it's a really good example to see how the different
components of active inference can be connected to each other. Nice. And 76 hierarchical or deep
inference burst a box 7.3 interlude on structure learning boxed off topic and a lot to say.
But structure learning broadly refers to learning the structure about a model,
using the same types of methods that you might to do inference on, for example, a more observable
sensor data reading, something like that. This section works towards the idea of nested inference
or multi scale modeling. What would you say about figure seven 12?
Okay, so again, this situation is, I think, the most complex situations of this chapter,
which builds up from the previous sections. And this time, it adds another layer to accommodate
for the inferences that happen in different time steps. So in this case, we have a multi
time or multi scale inference and learning happening, both at the levels of learning
and at the levels of information seeking. So this, this is represented in
figure seven point 12, which represents how kind of this fractal generative model
can be seen as a component in this multi scale, a bigger generative or as a kind of leaf in
this bigger, bigger generative model. So it can be seen as a lower level inference happening at the
leaf level, going up to the hierarchy and influencing, sorry, collaborating on the whole
process of learning and inference at the higher level. So yeah, I guess that's somehow summarizes
this figure. So if you have anything to add. That's, that's great. It's an example of the
composability of generative models, what we've talked about and had Toby Sinclair Smith describe as
as the compositional cognitive cartography, and just what kinds of connectors can and can't you do?
And how can that motif that the discrete time model introduces? And then the rest of these
features, including action and learning and so on get layered in on top. What can you do with that?
713 gives another example. Do you want to say anything about it or maybe continue on?
Yeah, so the case study here is the example of linguistic, I mean, language learning through
reading. So not language learning. Maybe it's just what happens in reading. Yeah, in comprehension.
So what happens when reading and in an anticipatory way, the words that that comes
each after the other. So why this kind of situation can be most successfully characterized
with this kind of modeling, because it involves different scales of learning and comprehension,
both at the level of, I mean, reading at the level of somehow observing the letters and then
going on to the words and then word groups and so on. So yeah, that's a really interesting way to,
again, combine all of those elements into a single unified model to see how those different
timescales, slow and fast timescales operate together to build this more encompassing model,
more encompassing generative model of the situation.
Great. Any closing thoughts on 7?
Nothing particular, not bad. Thanks.
All right. Next chapter is chapter eight, which is going to go into the continuous time.
All right. Chapter eight is called active inference and continuous time begins with that
timeless quote, everything flows, nothing stands still. So what would you say about chapter eight?
All right. So this chapter probably is my most favorite chapter in the book,
because of my own personal interest in, I don't know, the process materials and so on.
But yeah, so chapter seven acts as a really good starting point for anyone who wants to
develop the discrete time situations, to model discrete time situations
within active inference framework. But in chapter eight, we kind of get to
model a bit more interesting or, let's say, more involving situations.
And they're not necessarily toy examples we saw at least at the beginning of chapter seven.
So obviously, as the title suggests, this chapter deals with the continuous time situation.
So in that case, we'll need to, maybe at this point, refresh our memory about
what continuous time situation involves by reading the relevant parts,
reading or reviewing relevant parts of chapter four. So yeah, in chapter four,
we saw that the generative model for continuous time situation derives from the
it is a stochastic calculus in terms of putting the whole process
into two elements, two stochastic equations, one of which is the actual
state, the condition of actual states or the behavior of the actual states. And the other one
is the randomness that we need to account for in each real time continuous time situations.
So that's what we get here in equation 8.1. And then, building up from that equation,
we, it generalizes that equation to involve, I mean, the functionals of G and F instead of just
the single valued functions of G and F. So then we get to
put that into the situation that can be used for describing the behavior of dynamical systems,
which is a very well known situation to use these kinds of stochastic equations.
And it's widely studied how those, those kinds of dynamics can be characterized, especially
in recent Bayesian mechanics paper by Dalton, Saktiv Atevel and others. So, and then it gets to
some more specific examples such as Lothgabal-Terra dynamics and synchronicity and so on, in order
to show how these kinds of dynamics can be elaborated upon and can be generalized to,
and enables them to characterize more complex situations. So,
yeah, that's a really short and brief overview of the whole chapter. Maybe
we can talk about a bit more details as we go through it.
Great. Well said. Well, I'm sure for another day, the philosophical implications of eight,
seven and eight, and high road and low road, and all these other parts of the textbook, great topics.
I agree. I would see chapter eight as demonstrating continuity with some classical
continuous time modeling motifs from a few different areas of dynamical system science,
which is applied in like many, many, many fields, but these are some classic examples.
So, figure eight point one goes a little bit more into depth, or at least into more formalism
detail about exactly what we saw in chapter five with the spinal reflex arc with the proprioceptive
data coming in, and then a differential being calculated with the set point,
which reflects a descending prediction from a decision making layer. And that can be viewed as
this kind of mechanics that plays out in a phase space in continuous time, like a spring moving
around with someone making a certain path with an attractor, and a spring being dragged around
something in that area. Box eight point one goes into a very fascinating topic. Do you want to
describe it? Well, it's maybe one of the most thought provoking pages of the whole book. And
if I remember correctly, in all of the cohorts, this particular box
I mean gives always gives rise to lots of questions, because of some of the interesting
and at least initially counterintuitive claims here. But I don't want to spoil it. So
but as a kind of spoiler alert, it kind of gets to really interesting, but alas, very brief
discussion about the comparing these terms precision, attention, and sensory attenuation,
and the relation and similarities and difference between these two, these three terms,
and how each understanding each of them is essential to understanding the other ones.
But as I said, it's a really interesting topic, which gives rise to lots of discussions.
And I believe it's one of those topics that that's worth looking a bit more
looking into in some other literature as well.
Great. Well said. What a cliffhanger. Next, they go to a classic model family called Laka Volterra.
These dynamics inherit from characterizations of predator prey dynamics in ecology. So it's kind
of a classical ecology model shown in figure 8.2. On the top, it's actually the ecosystem model. Plants,
herbivores and carnivores, which follow different kinds of oscillatory trends in continuous time.
And so that also has enabled it to be applied for other so-called winnerless competitions.
And that relates to topics like neural Darwinism and also neural dynamics, where things have
kind of oscillatory relationships with each other, which are being modeled as a continuous time
underlying process with a lot of measurement noise and discretization through space and time.
Those are the kinds of algorithms that SPM explores more. And there's Laka Volterra and a
lot of other dynamical systems theory in SPM. So active inference kind of adds action and more
to what was laid out from a pure dynamical systems theory in SPM. Here, it really is just
showing the ecology example and how you can project. If you have three different species,
you can think about that motion in a cube or tetrahedron. And then you could project onto
kind of like looking at a lower dimensional manifold relating just two of the three species.
And that evinces this kind of oscillatory but also moving behavior.
That gets connected in figure 8.3 to neurobiology. What would you say about this?
Okay, so here in figure 8.3, we see some applications of Laka Volterra dynamics.
So the left column here represents what happens in, I mean, in eye blinking, eye blink conditioning.
So, of course, here we need to account for, I mean, the expected states of the sequences of events
that happens in the eye blinking. So the upper left figure shows the expectations
in terms of time. And then the parallel right hand side equation, sorry, right hand side figures,
shows the Laka Volterra system that is applied in the handwriting situation. So as we can see,
although the, I mean, mathematical technology is the same or at least the modeling technology is
the same, the outcome of each situation varies drastically in two distinct neurobiological
behavior, not neurobiological, but biological behavior. So, yeah, we can see how the same
modeling framework can give rise to different outcomes based on what parameters needs to be
optimized, what parameters are selected for modeling and so on. So I believe it's a quite
interesting example to compare handwriting and the blinking together and how those can be compared
to each other using the Laka Volterra dynamics. Great, thank you. Box 8.2 gives a variant on the
learning here presented with the formalism for continuous models, kind of a technical aside.
Section 8.4 is about generalized synchrony. So figure 8.4 is going to visualize one of the
classic dynamical systems, which is the Lorenz attractor. So what would you say about this
figure? Okay, so this section is truly interesting because when one thinks of active inference,
probably the first situations that comes to mind is the situations in which we have quite well
defined probability distributions for different parameters. But as we can see here in section
8.4, actually some of the formalism of active inference can be successfully used to characterize
even chaotic systems and in particular the way in which two chaotic systems can be synchronized
with each other. So this is a classic example of a chaotic Lorenz system, and it draws upon
from some of Professor Pristin's earlier work on birdsong synchrony. And as a side note, any
literature before 2016 is considered earlier history in active inference literature because
it evolves quite rapidly. So yeah, this kind of synchrony between two chaotic systems can be
interpreted as providing evidence or even, let's say, a way to model a kind of primitive theory of
mind in the sense that how exactly can we understand or can two agents can trace each
other's trajectories without, I mean, engaging in any direct exchange of observations between
their internal and external states. So yeah, that's a really good example and I believe one of the
most interesting examples of how active inference can even account for these kinds of behavior.
So the rest of the section goes into the details of how this kind of synchrony between
multi-scale Lorenz systems can happen and how can we formulate it mathematically in terms of
continuous time active inference. Awesome. And there's been more recent work on Mark
Alblanket since stochastic chaos, but the bird example is a classic. 8.5 goes into hybrid discrete
and continuous models. So this could be kind of like an in-between chapter of seven and eight,
but now that we've been introduced to the pure form of discrete and the pure form of continuous
models, here's shown that that composability extends to so-called hybrid models, where here
the lower level visually is using the continuous time formalism and the higher level is describing
the little line added here, the discrete time formalism. And this was the similar structure
described by the authors of the paper, active inference does not contradict folk psychology,
where they describe this lower level as motor active inference, which was closely
allied with the spinal arc reflex shown above. And then this higher level, they call decision
active inference, because in that case, it was referring to a discrete decision.
And so they used that kind of basic motif of continuous activity or continuous time modeling
at the more peripheral aspects of a cognitive entity. And like Ali said, more discretization
and hybridization as well at higher levels of the cognitive modeling.
And that type of an architecture here, instead of describing who wants the ice cream cone,
I believe, here, it's going to be a mixed or hybrid model that is going to call back the
isocade system, where there's a fixed point that is able to be moved as a set point. And then
there's a continuous time isocade that pursues the new fixed point. And so that's analogous to a new
set point or fixed point being specified from the top down muscle command about a new location
for a muscle, followed by movement towards it. This is a muscular activity that is realizing
that but but not in the elbow coming away from the hot stove. This is about the ice caking to an
epistemic foraging location specified by top down hierarchical systems. 8.3 describes little
technical aside on mixture of Gaussian Gaussian mixture models, kind of a technical modeling
note. And 8.6 closes. It says it's a huge topic and much has been left out. And so they list in
table 8.1 key advances in continuous time models. And those areas are synthetic bird song,
ocular motor delays, conditioned reflexes, smooth pursuit, eye movement, psychosis, illusions,
saccades, action observation, attention, hybrid models and self organization. And that's chapter 8.
What else would you say? And also what would you kind of lead someone to in the philosophical
implications of 8 because it sounds kind of cool? Okay, so well, the case of continuous time active
inference. I think it leads to really interesting questions, both in terms of philosophical questions
and also more practical modeling questions about what parameters needs to be accounted for and so
on. And as I said, I believe it's a more more interesting way of if not interesting, but but
at least more involved way of doing active inference modeling. But one thing that one of the
philosophical questions that Mao and I have explored in our paper is how the processes of I mean,
ontological processes can philosophically described using FPP assertions in terms of their
interaction with the environment in which they co constitute themselves. And we don't necessarily
distinguish between between the internal and the external states. So one obvious example of this
is that generalized synchrony example that we saw in this chapter, in which we don't necessarily
distinguish between which of the birds act as the agents and which one is the environment or the
vice versa. So these kinds of co constitution of the environment and the agent, which gives rise
to the partitioning of state space through a Markov blanket is one of the interesting
philosophical points that I think needs to be elaborated a bit more using
some of the recent advances in philosophy, such as the tools that's been developed in new materialism
school or some other philosophical approach approaches. But yeah, these kinds of
what exactly gives rise gives rise to emergence, what is the ontological status of emergent
properties and so on, are some of the burning questions for many philosophers today. And I
believe active inference and particularly continuous time active inference provides a clear,
precise mathematical formalism. Even if not to answer these questions, but at least
