So the upshot to this I think is that you end up with a view of agents as deeply coupled with their environments.
So under active inference, it's just nonsensical to try and think about agents as decoupled from their environment.
The generative models themselves are of the dynamics between the agent and the environment.
So these two things can't be decoupled.
Perception action loops.
So remember I said that perceptual and active inference are essentially the same thing.
They're the part of this circular causality.
And they see the agent constantly producing their own worlds.
They occupy the states that they're expecting to through action.
And we might think based on the literature that I've gone through that there's some role here for representations.
So we're not committing ourselves.
The active inference agent is not one that fits with very radical interpretations of inactivism.
And inactive inference agents are porous.
They're open to recruiting external material features of the environment to reduce complexity.
And therefore minimize the metabolic costs of realizing those expected states.
Okay.
So that was a bit of active inference set against the backdrop of some older questions in philosophy of mind, theoretical cognitive science.
But I'm going to jump straight into agency and motivation now because there's a lot to get through here.
I'm going to start with a dark room problem.
This is a canonical philosophical worry in active inference.
So a lot of people I think will be familiar with this.
And in really brief terms, it's really easy to state.
The challenge here is to say, if it's all about minimizing prediction error,
why don't we just find the most predictable environments possible and stay there and live a nice error free life.
So Andy Clark paints a picture of a dark room and you can just be strapped up to a intravenous drip feeding you the nutrients you need.
And you can just be really happy.
But clearly we don't do that.
Active inference, sorry, agents in the world, us, we generally typically sometimes we do enjoy dark rooms,
but most of the time we don't for any extended period of time.
And so there's a challenge here about how active inference can account for what appear to be motivated,
agential actions that necessarily mean we're confronted with some degree of uncertainty.
And I'm going to set active inference up in a kind of dialectic with folk psychology here,
because folk psychology faces no such concerns.
So to be very clear, folk psychology is just the kind of propositional linguistic framework that the folk use to talk about and describe motivated action.
So it's a language we typically use every day to describe the reasons people are doing things.
So folk psychology has this belief desire intention schema where if I want to explain my friend going to grab a beer,
I would say my friend believes there's beer in the fridge.
My friend desires beer.
And so those are combined to form the intention to go and get beer.
So folk psychology doesn't have these kinds of problems.
So just a little bit more on the dark room problem and another whale.
So the dark room problem invites more than one answer.
We're going to see more than one answer to it as we go along.
One response is to talk about what drives curiosity and play and exploration.
And we are going to see more of that.
But the first response offered by Friston, Thornton and Clark is to highlight the fact that organisms have basic needs to eat,
drink and reproduce what you might call evolutionarily ingrained needs.
And that those needs are going to necessitate some form of exploratory behavior.
So staying in the dark room will inevitably under most circumstances,
going to lead to physical dissipation.
So the question is, are these kinds of appeals enough?
And I wanted to pull up a paper by Colin Klein 2018 that I think is like a really nice articulation of these challenges.
Apparently I think he's wrong and it falls flat, but I think it's a strong articulation.
So Klein argues that predictions alone simply can't account for the motivational character of these needs.
So Klein says, look, it's fine that you've got these needs.
Nobody's going to deny that, but you just have this one primitive to describe how those needs motivate you and predictions just not going to do it.
So prediction alone as your sole primitive is not going to be able.
It's like predicting is not the same thing as being motivated by something.
And Klein is really not happy with appeals to these phenotypic needs.
And he construes these in the relevance of the evolutionary history of the organism and goes into a little bit of detail.
So he says, look, these expectations are either going to be about states, possible states.
So the state of being fed or clothed or warm, or they're going to be about state transitions about what the appropriate action is in one state to move to another state.
And he says expectations about states themselves aren't going to work because you have this problem of accounting for novelty.
So it might be the case that in my evolutionary past, all of my ancestors have had the same expectations about which states are going to occupy.
But there are lots of states in my life that I could be motivated to occupy that there's simply no evolutionary history to account for.
And he says there's a very similar or a similarly difficult problem with state transitions where essentially what we're talking about with state transitions, the most appropriate actions.
So I have expectations about the actions that are going to fill that I need to take under certain circumstances that are going to get me to a particular expected state.
And Klein's worry is that there's just going to be too many possible actions in any given situation.
So there's just a kind of a kind of explosion that's just going to be unaccountable for.
There's just going to be too many different ways of getting to an expected state that won't be, we won't be able to account for in our past.
So it's fairly robust, thorough challenge, built on the dark room problem.
And I think actually there are, it's a worry with two faces.
So I think the first is about how active inference can describe agential behavior with its austere purely doxastic landscape.
So if you imagine somebody looking at their house burning down, it seems like you need more than beliefs to explain any course of action they take.
So there has to be desire in there somewhere.
So this is an example Andy Clark uses in one of his papers and he says, you know, the desire to run in and rescue some kind of some item that you want from the house or the desire to claim on the insurance is the thing that's going to dictate how you explain the person's behavior.
So that's the first problem.
We need to look at how folks psychology tessellates with prediction.
And the other problem is maybe a little bit, maybe a little bit deeper and that's to ask how does active inference actually account for agential behavior.
So can we even tell some story using just predictions and prediction error about what really moves agents to act.
So firstly thinking about folks psychology, there's a couple of ways to go here and I think ultimately we can answer this worry.
So Ryan Smith and colleagues in 2022.
In their paper active inference models do not contradict folks psychology.
They argue that the formalisms of active inference can accommodate a belief desire distinction.
Now I'll admit that it's a very math heavy paper and it's a little bit beyond my capabilities but the claim here is simply that there's no contradiction.
Actually the worry that the important thing we have in folks psychology is desire and we're not getting that in active inference.
This is just an unfounded worry.
But there's another strategy as well.
So Joe Dewhurst in his 2017 paper.
He employs a different strategy and he says, look, what are we even doing with folks psychology?
Let's not kind of be too hasty.
And we need to think about what are we doing when we talk about these propositional kind of states.
So he says the first option is to say, well, we're just being realists here.
So when I say that X desires Y, I'm actually positing the real existence of a fine grained mental state in X's head.
And he thinks that this is actually the wrong way to think about things.
He thinks that folks psychology, actually it's an instrumentalist tool.
It's a kind of framework that we use just for picking out coarse grained behavioral patterns in some useful way.
So if we agree with Dewhurst here and we just become instrumentalists about folks psychology, again, it looks as though there's different.
We're not forced to kind of worry that there's some kind of empirical problem here where the mechanisms of active inference are, in some sense,
incommensurable with real properties that we're positing the existence of because we're just going to say that those things don't really exist in the way that we've typically assumed that they might.
So either way, whichever route you go down here, it looks like active inference agents have beliefs and that we're going to be fine in describing motivated action.
So what about the second problem?
So Andy Clark in 2020 authored a direct response decline.
So Clark wants to argue that actually using predictions and prediction errors and precision waiting, which I'll explain in a second.
We can get a picture of a gentle action.
So the first thing Clark does is to outline the fact that under active inference, we have a very nice well worn account of embodied action.
So in other words, what it takes to actually move the body to perform different actions is perfectly folded into active inference under the same kind of computational flow of prediction error minimization.
Just in a nutshell for me to move my arm, what I'm doing is making a prediction that my arm is in some place and then minimizing the prediction error.
So once that's on the table, Clark says, OK, that's fine.
We have this account of action, but that's not really what Klein means.
So the move Clark makes is to appeal to the hierarchical nested structure of the generative model and to say that higher level beliefs, which are temporal about temporally extended future states.
These are going to perform a really important role as a kind of controller for lower level actions.
So the idea is that if I have a high level temporally extended expectation to attain a PhD, what happens is that high level belief becomes unpacked at lower levels in the hierarchy.
And what you get are more fine grained faster action policies that the system expects to help support that higher level expectation.
And there's a mechanism in play here called precision waiting, which applies to prediction error signals, because obviously not all of the incoming, not all of the predictions we make about the environment are going to be equally important.
We're not going to have equal confidence in all of our actions to bring about an expected state.
And so what precision waiting does is essentially communicate how much confidence we have in a particular action policy to move us into that expected state.
Under perceptual inference, precision waiting just signals how newsworthy prediction errors are.
So how important they are for the system.
So precision waiting drives attention, for example, when we're talking about predictive processing.
But in this case, Clark's claim is that when you have this kind of nested hierarchy of expectations, so you have temporally extended expectations at the high level, unpacked into a kind of web of lower level action policies.
And precision waiting is kind of modulating the system's expectations about which of those policies are most likely to bring about the expected states.
You get something that looks a lot like motivated directed action.
And agency is kind of inherent in this model, because the agent has to the system rather has to model itself as a cause of those expected states.
So if I if I have these high level expectations, I in a very inherent way have to model my own cause or efficacy to bring about those states.
So on a very low level, the system has to model some form of agent because I can be the cause of change to my own sensory states so I can cover my own eyes and so on and so forth.
So kind of on this view agency is a latent variable in these systems when it's making these predictions.
So it just kind of turns up and Clark argues that you essentially end up with something that looks very much like agents and the folk psychology.
He thinks this is a better view because we end up with a quote different and arguably much more unified internal architecture trading in a single currency.
So Clark thinks look, OK, Klein's worry is we have this one primitive we have just prediction.
And the concern is that we're not going to be able to explain a gentle action.
But actually, there are ways that we can appeal to the hierarchical structure of the model and precision waiting.
And it turns out that this is a much more elegant picture as well.
But Klein might press the worry and anybody listening might kind of think, well, we've kind of done some work there, but why do we expect the things that we expect in the first place?
So why is my long term expectation to attain a PhD and not to join the French foreign agent, for example?
What means that I expect one thing rather than another?
And here, Clark just bites a bullet and says, nobody can explain that.
We've kind of shifted to talking about questions about, you know, spookier questions about free will.
And Clark says, look, if this is what you want, if this is what you're demanding, then you have to recognize that simply appealing to folk psychology doesn't get you that either.
Okay, so Klein or, you know, people with these kinds of worries need to be careful in how much they demand from the from the active inference agent.
And I think the real answer to that question, actually, this is more speculative, but the real answer to the deeper answer to these kinds of questions will lie in the kinds of things that we're going to look at in subsequent weeks.
So for example, the reason I want I have the expectation to attain a PhD rather than join the French foreign Legion is because I exist in a particular kind of socio cultural landscape with specific norms.
And a lot of my actions and beliefs are tempered and shaped by the expectations of people around me.
So this is just an overview.
We went over the dark room problem.
We do anything at all if it's all about uncertain if it's all about certainty and prediction error minimization.
This worry, I said you can kind of highlight two aspects of this worry. One is an actual account of agency under active inference, which we have.
We have this Andy Clark model of agency to do with hierarchical nested unfolding action policies over time and different precision mappings.
And then we have this worry about the relationship with folk psychology. So how do we describe.
Agential behavior and motivated behavior. How do we describe the behavior of agents if we only have recourse to prediction.
And as we saw, there's one argument here is that actually active inference the formalisms can accommodate this belief decide distinction.
And the other move that I really like is to just say, look, folk psychology is just this instrumental tool.
It is in effect a kind of it's part of the socio cultural landscape that we use to shape each other's expectations by putting these labels to explain our actions.
It's kind of form of active inference itself.
So there's this kind of philosophically interesting debate about folk psychology.
But we turns out that everything's okay and active inference agents are in fact agents with beliefs, desires and hopes and dreams and so on.
Okay, that's agency moving. We're making good time.
Okay, so in this section, I'm going to introduce quite a lot of stuff, but it's going to be all tied together hopefully rather neatly in the case study at the end.
I'm going to talk about allostasis and emotion. I'm going to say very, very briefly a little bit about the self as well.
Here's some examples of some of the core readings I've used again. These are all going to be up in the document as well, which will be made available.
And I really advise you to just have a browse through these. There's some really interesting stuff there.
I'm not going to talk about psychedelics or meditation or depersonalization, but there is some really, really cool active inference literature on those topics as well, particularly papers on psychedelics and ego dissolution by George Dean.
A new paper just came out by, you know, Cipollito, Jonas Mago and Robert Carhart-Harris and I think Fernando Rosas as well.
I think I might be mistaken on the authors there, but it's a very new paper.
Okay, so the thing that is going to underpin a lot of what I'm now going to go on to say is going to be this notion of allostasis.
And this plays a major role in a lot of work on emotion and the self in active inference.
Lisa Feldman Barrett, whose theory of emotion we're going to look at in a second, has argued that the brain's fundamental purpose, what it involves to do through that imperative to minimize prediction error is to manage the body's metabolic budget.
So the first prior is kind of our most expected state.
The first prior in the sense of like our strongest prior probability or highest expectation is to have good allostatic control.
But I'm getting ahead of myself there.
All that means is that we're balancing our body's metabolic budget or like energy budget in an effective way.
So error signals, like I said before, that are highly precise are those that are weighted very highly.
So prediction error signals that have high precision weighting are the synaptic gain on those error signals is going to be increased.
And that's going to drive learning, which means in future our expectations around those kinds of error signals is going to be we're going to pay more attention to those.
And the most highly weighted error signals that we're going to get are to do with our own homeostatic set points within our own body.
So if my temperature started to rise rapidly, my body temperature started to rise or drop rapidly, that prediction error signal, the violation of those expectations is really going to grab my attention and I'm going to do something about it.
And one way that we maintain these homeostatic states is through the body's own autonomic reactions.
So were I to start getting very cold, I would probably start to shiver.
And so the body is trying to kind of rectify those deviations from homeostatic set points.
But another thing that organisms engage in and humans do this to really remarkable degrees.
And again, this is going to be something that I suspect is going to come out in the subsequent weeks is that we engage in quite complex and sophisticated anticipatory actions to maintain homeostasis.
And this is something called allostasis.
So allostatic control, which I mentioned earlier as this first prior is essentially the regulation of homeostasis through action.
So it's actions that I can take to ensure that I stay within those important bounds.
So shivering is good, but living in a house with central heating is better.
Having a warm bed and living in a community where we've had these people of kind of engineers have worked to create a kind of material environment that acts as a kind of background support to a lot of these needs.
Something that's another important concept here is allostatic load.
So this is essentially the result of not having good allostatic control.
So again, allostatic control is just engaging effectively in actions that are maintaining those set points.
Whereas allostatic load is the physiological costs to the agent of losing that control.
So there's so many studies here that have just shown the immense physiological costs of operating beyond those set points and not having good control.
So allostatic load has been linked with reduced cardiac health, sleep problems, gut issues, mental health issues, depression, basically everything is really, really, really bad.
And so good allostatic control is very important.
And as we're going to see, it underpins quite a few very important things.
So really, we could dedicate a whole hour to looking at emotion, but I'm going to blast through it very, very, very quickly right now.
And this slide, I should say, one of the core readings on the document and the reading that I'm basing this on is Lisa Feldman Barrett's 2017 paper, The Theory of Constructed Emotion.
The main idea here is this is one of those theories that on the face of it, it seems quite simple, but actually there's quite a lot of subtlety and nuance to it.
Allostatic control means inferring the causes of changes to the body's internal state.
So in other words, if I want to keep good allostatic control, I need to know what's not just what's going on out there, but I need to know what's going on inside me as well.
Okay, so if there's some crucial change happening, I need to be on top of it.
