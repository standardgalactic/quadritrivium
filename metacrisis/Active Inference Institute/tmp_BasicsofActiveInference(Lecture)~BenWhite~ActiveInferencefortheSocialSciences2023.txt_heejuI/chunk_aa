All right. Hello and welcome, everyone. It is July 11th, 2023. We are in active inference for the social sciences. And today is going to be a lecture by Ben White on basics of active inference, the active inference agent. So Ben, thank you for the lecture off to you. And we're looking forward to it.
Thank you very much. Yeah, me too. I'm very happy to be a part of this, to be kicking things off with the first module on this course. I'm going to be talking about the active inference agent and trying to cover some of the basics.
I won't take too long introducing myself. Avel gave a very thorough introduction in his talk recently, but I'm a second year PhD student in philosophy at the University of Sussex.
And I work with Andy Clark and with Avel and some other people using active inference to try and find things out about our relationship with technology and within the context of wellbeing and mental health.
The aims for today, I'm going to try and provide quite a wide ranging overview of how active inference has connected with areas of philosophical interest and particularly how they relate to the individual agent and the experience of the agent and how the agent finds themselves in the world.
So I've split this up to look at several different defined topics. So I'm going to move quite quickly through mind, agency, emotion and phenomenology.
And I'm going to say a little bit about the self as well. And then there's going to be a case study at the end that I'm going to move through if we have time.
So one of the other aims that I have is to lay out very clearly some of the core concepts and core mechanisms like precision weighting and prediction error and generative models.
I'm not going to go into any technical details whatsoever. This is all going to be fairly abstract. My background and my interest in these frameworks is philosophical.
So it's quite abstract in terms of how everything fits together.
And I'm going to try and bring some of these threads together to build up a layered picture of what the active inference agent is like because this course is aimed towards seeing how active inference applies to larger scales, collective behavior, social norms, socio-cultural landscapes.
And I think in order to do justice to those things, we need to first have a good idea of the individual in active inference.
So we've got our work cut out for us. So I'll get started.
So before I dive in, I wanted to just say a little bit about how these frameworks hang together.
So active inference is based on Karl Friston's free energy principle. I think most of us are familiar with that.
But the free energy principle just states that in order to persist through time, biological organisms must occupy only those states that it would expect to occupy given the type of thing that it is.
So free energy is essentially a measure of the disattunement between a system and its environment.
And I'm going to say a little bit more about attunement and how that gets cashed out in various ways as we go on.
Active inference is a process theory that essentially explains how embodied organisms actually go about remaining in those expected states.
So how we actually go about minimizing free energy.
And the idea is that all adaptive behavior is explained by agents garnering evidence to confirm their own expectations.
And predictive processing, for the purposes of the lecture today, I'm going to treat these as synonymous because most of the work that I kind of came up through as I was learning about this was predictive processing.
And all of the papers that we're using today that refer to predictive processing refer to a predictive processing that I take to be more or less synonymous with active inference.
So that's a very embodied inactive flavor of predictive processing.
I think generally the difference is that predictive processing can be a much broader term and predictive processing can also apply to passive models of perception, but we're not going to concern ourselves with that.
Okay.
So some key concepts of active inference slash predictive processing then.
So the framework says that agents embody or agents have.
We won't concern ourselves with the nuance there, but they have a generative model, which is essentially a kind of understanding or a model of the regularities that underpin the dynamics between the agent and the environment.
So another way to think about this in very rough terms is a kind of mental model of the regularities in the world and how the agent exists in the world.
And using that model, the agent can generate predictions about its own sensory states.
So given the model, what kinds of sensory states would it expect to find itself in in a particular set of circumstances.
And so where those predictions don't actually match the sensory inputs.
So where there's a discrepancy between the actual incoming sense data and the prediction prediction errors are generated and prediction errors are going to flow upwards through the hierarchy.
So the generative model is said to be a hierarchical model and predictions flow downwards and prediction errors flow upwards.
And the sole imperative of the brain body system according to this framework is to minimize those prediction errors.
So a good way to kind of get a little handle on this is to think about just the example of perception.
So perception traditionally, so I'm talking about visual perception here has traditionally been understood as a bottom up process.
And what that means is that the brain waits for incoming sensory signals and then it processes them as they come in and it combines them in kind of increasingly abstract ways.
And what we see is some combination of those sensory signals that have come in and been processed and combined.
So if if you think about me leaving my apartment in the morning and stepping out onto the street and seeing some cars, what's happening there is some sensory information is hitting my retina.
And it's being processed first with very basic features like light shade edges and so on and so forth.
And then it's being combined with my understanding of what car is or what a bus stop is.
And that gives me my kind of visual field.
Predictive processing, famously at this point flips this picture upside down on its head.
So visual perception under predictive processing is a top down affair.
So the generative model is going to encode multi level expectations about the likely scene.
So just to be very clear, when we talk about multi level and hierarchical higher up is the higher up levels are going to be tracking longer time scales and higher levels of abstraction.
And the lower levels are going to be much faster, much more concrete.
So a changeable scene.
So there's going to be some variation in what I see when I leave my apartment every day is going to mean that there's always going to be some error in that picture.
So it's never going to be the same two days in a row or it's extremely unlikely to be.
So no matter how well I can predict what I'm going to see when I leave my house, it's always going to be cleaned up by some prediction errors.
OK, so essentially when I step outside and I step into the street, my brain is already anticipating what it's going to see out there.
OK, so I know where particular cars are going to be parked.
I know where the bus stop is so on and so forth.
And the perceptual experience that we have is a construction.
It's we actually experienced the expectation that has been kind of cleaned up post prediction error.
There's a little bit of debate there about exactly what it is that we experience.
But for our purposes today, we can say that visual perception is essentially a construction.
So on the face of it, prediction errors can be minimized using one of two strategies.
And this is going to be really, really important.
So we have what I just described there, which is perceptual inference.
That's where we revise our predictions to better fit the evidence.
So essentially, we update our model to better reflect the regularities in the environment.
Or we can engage in active inference, which is where the framework gets its name from,
which is where we essentially make the evidence fit the model.
So that's where we, in some sense, update the world so it conforms to the original prediction.
And active inference agents bring about those expected states by sampling the world in a bias way.
So they have certain expectations, and then we sample the world to confirm the predictions.
And I put perceptual inference there in scare quotes because perceptual inference on kind of readings of most readings of active inference
just becomes a kind of action readiness or as Ramster and colleagues put it, a state estimation.
So it's just perception and action are just wrapped up in this continuous loop.
But it's really all about the action. It's all really all about bringing about those expected states and perception is just kind of in service of that process.
And so Burenberg and colleagues refer to the active inference agent as a crooked scientist, which is something I really like.
I think that's a very charming image.
So we are scientists that are engaging in some very dodgy behavior because we're not really interested in having an accurate model.
We're more interested in confirming our original predictions.
Okay, so that was a very, very rapid overview of some of the core concepts and mechanisms, but they're going to crop up continuously.
So I think we're going to get a much better grasp on them as we go on.
In this section, I'm going to start to expand on some of those things that we've just put on the table.
And I'm going to begin to flesh them out in terms of thinking about how active inference agents actually exist in the world.
So I should say I put this here to remind me.
So those of you who are taking part in the course, I'm going to make available a document with all of the readings that I have.
Firstly, all of the readings that I've used to make these slides and then a bunch of other readings as well that I think are going to be very relevant.
And obviously they're going to be organized in terms of the structure of this presentation as well.
So this is just, I put a little sample up on these slides, but there's going to be much more in the document.
So I think it's always good to start with some guiding questions.
So I've thrown up three here.
The first is how does active inference characterize cognition?
So we've just looked at predictions and prediction errors, but we're going to expand on that a little bit.
And we're going to ask what kinds of processes does cognition involve?
How is cognition related to the material environment?
This is going to be something, how cognition relates to the environment is going to be very important in subsequent weeks.
And we're going to ask quite briefly what kinds of vehicles might realize cognitive processing.
Okay, so there's a couple of whales.
Whales are going to be recurring theme in this lecture, but I wanted to take a step back and ask about attunement.
Because this is one of those concepts that gets thrown around a lot.
And I think once it's grasped, it becomes very, very clear and very easy to use.
But I think prior to it being grasped, it can be a little bit opaque and a bit slippery.
So on the left, we have a back whale that was found dead in the Amazon rainforest, allegedly.
I don't know if this was true.
And on the right, we have a whale in its natural environment.
And I think clearly the intuition here is that in one case, the whale is very well attuned to its environment.
And in the other case, it's not.
And I think that we can start by asking, what is it about the ocean that allows the whale to be well attuned?
And what I want to say here is that the particular dynamics between the ocean and the whale allow the whale to do certain things that it can't do when it's in the Amazon.
So in other words, we might say that the ocean affords certain things to the whale, specifically filter feeding,
being able to move its vast weights around with ease, things that unfortunately a whale in the Amazon can't do.
So affordances are opportunities for action.
They are things roughly speaking in the environment that the agent can act upon.
And there's some debate about the proper way to really characterize affordances,
but I like to think of them as relational properties emerging from an environmental feature and some skilled embodied ability of the agent.
So the same thing in the environment will not afford the same thing to different organisms.
So for example, soil to a human agent might afford walking on or laying down on,
but to an earthworm it affords very different things.
And also, even amongst the same organisms with humans,
especially different surfaces and features of the environment will afford different things depending on your particular skill set.
And agents are going to go about minimizing their uncertainty.
So minimizing prediction errors, staying attuned to their environment through an engagement with affordances.
And this is going to be something that is going to drift away slightly as we move through some sections and then it's going to come back very heavily at the end.
And I should say that the idea of affordances, to my knowledge, it first emerges in the work of James Gibson,
the ecological approach to visual perception.
I think it's chapter eight in that book that really lays out a theory of affordances.
So this idea that systems or organisms maintain their organization through ongoing action
means that active inference has been labeled as quintessentially inactive by Carl Friston.
And I think that this is a nice, inactivism is an older framework from theoretical cognitive science.
And I think it's a nice way to understand some of the key features of active inference,
but it does come with a little bit of historical philosophical baggage and I'm going to try and unpack that really quickly.
Because I want to unpack exactly what it is that we're committing ourselves to when we think about cognition in activist terms.
And so inactivism comes in various strands very, very quickly and roughly you have auto poetic inactivism,
which commits itself to what's known as the mind life continuity thesis.
This is essentially that the structures that give a ground to life itself are the same structures that instantiate cognitive processes.
Sensing on this view is teleological because it's oriented around supporting vital systems.
So things in the environment have intrinsic meaning to the organism because they support the vital functioning of the organism in some way.
Sensory motor inactivism emphasizes how mobile and embodied agents essentially enact the grounds and conditions for their own sensory engagements with the world.
So the world is not a kind of neutral static given that we're parachuted into.
Actually the conditions through which we sense the world are the conditions that we can move and kind of interact with the world in various ways.
And then there's radical inactivism and this is the view that cognition is purely grounded in these agent environment dynamics.
It's a purely dynamicist account of cognition and there's no role whatsoever for internal representations.
So certainly in certain brands of sensory motor inactivism there remains a role for internal manipulation of representations.
And mental representations are just mental states that are about something in the world.
So some mental state is reconstructing some feature of the world and this all harks back to kind of old debates in philosophy of cognitive science
between internalist notions of cognition and more embodied, embedded, extended and we'll take a more of a look at that in a second.
But the question here is what are we committing ourselves to with active inference and I need to be a little bit careful
because I think that people are going to disagree with this to a certain extent.
But I think we're certainly committing ourselves to sensory motor inactivism.
I think that's fairly uncontroversial and I would say as well that the underpinning of active inference by the free energy principle by my lights also commits us
or it certainly chimes very deeply with the core tenants of auto poetic inactivism.
But I think what's really interesting in trying to understand cognition in the active inference agent is to understand what role, if any, there are for representations.
So if you have any familiarity with the literature on active inference, you'll know that active inference people are always banging on about models, prediction errors,
various other computationally sounding things.
And so the worry here is at least for why it's not necessarily a worry, but the thought the thought would be, are we committing ourselves to a form of cognitivism.
And again, cognitivism is this is generally considered to be a old fashioned notion of cognition as like computation over symbol symbolic representations in the brain according to some syntactic set of rules or something.
And typically, over the last couple of decades in cognitive science people have been moving away from that view.
So it would be interesting if we were committing ourselves to something that we've already moved away from.
And some people think that we we are making that commitment.
So Jacob Howie, for example, has argued in various places that essentially predictive processing or active inference commits us to these very rich reconstructive notions of representation.
And even Andy Clark, Jacob Howie's foil in many, many places has said himself that he thinks that representations play a role here as well.
So in a talk in 2016, Clark said, it's internal representation.
I think it is these are representation using stories, but it's not internal representation in quite the way that we originally thought about it.
So there's some differences, perhaps major differences between Howie and Clark there, but others have pushed back.
So there's a paper by Maxwell Ramstead and colleagues from 2020.
I think it's a tale of two densities paper where they say that a proper understanding of the generative models in active inference suggests that there's no role for representations.
So this is the kind of anti how we view.
So I really like this paper from Axel Constan and Andy Clark and Carl Friston, which essentially tries to broker some kind of piece in a debate that in some form or another has been raging for decades.
So it'd be quite nice to finally put this to bed.
So they argue that we can find in the formalisms of active inference, we can find grounds for an armistice here.
So they argue that the formalisms of active inference show that, quote, representational and non representational cognitive processes can be implemented by the brain under active inference.
And they do this by showing that active inference accommodates a dual architecture.
So they show that on the one hand, in some sense, the brain has to represent because under active inference action selection policies are happening over extended time scales.
And so the agent has to engage in some kind of counterfactual thinking.
So if I do X, what happens if I do Y, what happens and so on.
And the argument very roughly speaking is that that is going to entail some kind of representation or some use of representations.
But on the other hand, they argue that what they call deontic actions are processed through different mappings in the generative model and they're directly triggered by sensory input.
So deontic action is an action that is ingrained by our sense of the expectations of others.
So the example that the authors use is stopping at some red traffic lights in the middle of the night when there's nobody else around.
So in a sense, the claim is that we, through a kind of social conditioning, we know what other people would expect us to do in those circumstances.
And that allows for this kind of dynamic, reflective action policy setting.
And in those cases, there's no room for represent or there's no need for representations there.
I would definitely suggest people interested in this go and read this paper because you might already have thought of some potential objections here to the claim that representations aren't needed for deontic action.
And they do go into some detail in talking about potential objections there.
So that's the kind of status of the role of representations.
It's a live debate. Not everybody agrees.
But it certainly looks as though active inference has the formal tools to accommodate both sides of the debate.
And it might turn out that that's a good thing because we can make everybody happy or it might turn out to be a bad thing because you might think, well, we've not really settled anything one way or another.
I wanted to take some time, though, to mention extended active inference.
So I've been talking about inactivism and I've mentioned this debate between cognitivism and newer ways of thinking about cognition as embodied or embedded.
And these are part of what's known as the 4e paradigm.
And clearly, I think we are talking about inactive cognition and you get embodied and embedded kind of for free with this.
So the only one left on the table is the extended mind.
And this is just the claim that under some conditions, the system that's realising cognitive processing can literally extend outwards from the mind.
So I'll put some nice readings for this up if anybody's unfamiliar and interested, but it's usually the most controversial of the fouries.
And Andy Clark, who is one of the original authors of the extended mind paper, really thinks that active inference accommodates the extended mind theory.
And it's not just that it accommodates it, but it actually solves some old problems in the extended mind about knowing how and why the brain knows when and where to recruit certain extra neural resources.
And part of the argument for this, what really sits at the core is Clark develops this argument based on the fact that the generative models in active inference are always making a trade off between accuracy and complexity.
So you want your model of the world to be accurate enough that you can act effectively on the world.
But you also want to minimise the metabolic costs of having models that are overly complex.
And so Clark utilises this insight to make the case that this explains that in some cases we as agents, we make use of these extra neural kind of props and tools and things.
And he says that the imperative to reduce uncertainty provides a location neutral cost function that's always been missing from the older arguments around extended mind.
