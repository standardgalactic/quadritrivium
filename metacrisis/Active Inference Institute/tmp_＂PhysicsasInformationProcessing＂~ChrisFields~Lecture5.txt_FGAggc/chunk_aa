Hello and welcome, everyone. It is September 14, 2023. And we are in the course, Physics
as Information Processing. It's lecture five. Looking forward to this lecture, and then
a little conversation, Chris, off to you. Thank you.
Okay. Thank you, Daniel. And welcome to session five of Physics of Information Processing.
And today we're going to talk about space time. And particularly about the idea that
space time is emergent from information processing. And this has now become a very mainstream idea
in the quantum information and quantum gravity communities. And there's still, of course,
some people holding out for the idea that space time is fundamental. But I would say that
at least in these communities, I expect it's become now the dominant idea that space time
is not fundamental, that space time is not part of the basic ontology of reality or something
like that. And that, instead, space time is a construct. So that's what today's session is going
to be about. So for those of you who are just joining or who would like a little review, we
started this course talking about quantum information theory. And the idea that quantum
information theory is actually a new science about systems that communicate across a boundary.
And the systems are always called Alice and Bob for kind of historical reasons. And emphasize
that this new science is topological. It's about connectivity between systems. It's not
geometric. So it doesn't assume a space time embedding. So it leaves open the possibility
that space time is actually a construct. It doesn't assume space time up front. And in
the second session of the course, I think we talked about how this boundary could always
be represented as an array of qubits and that what the systems on each side of the boundary
do to communicate is prepare and then measure the states of these qubits. So they communicate
by exchanging classical information via a quantum channel. And this introduces a particular kind
of quantum noise whenever they don't share the reference frames that they use to prepare the
bits and measure the bits. So it's not necessarily a perfect communication channel from a classical
fidelity point of view. But all of the noise is quantum noise. It's not classical. And it always
derives from differences in the reference frames that are used by the two agents. We
reformulated the free energy principle in this setting. And the free energy principle turns out
to be very simple. It turns out to be a classical limit of the principle of uniterity, which is just
the principle that information is conserved. And we saw that the approach to perfect alignment
between the two agents or perfect prediction on the part of the two agents becomes more
challenging, not surprisingly, as system complexity increases. So the more complex the
communication, the harder it is to predict, as one would expect. In the last session, we talked
about how constraints on thermodynamic free energy induce compartmentalization in agents. So
whenever agents interact with their environments in fairly complex ways, then they end up being
compartmentalized into compartments that have to communicate classically. And this is solely
a result of quantum information theory. You don't have to assume any biology to get this
compartmentalization. But clearly, this is something that we see in biological systems. Not
only do we see compartments, we also see hierarchical control, which is likewise a sort of
automatic outcome of the theory for complex systems. And finally, last time, we talked a
little bit about how these compartments communicate and introduce the idea of communication
protocols that involve local operations on a shared quantum resource of some kind, and
classical communication. And we talked to Fermat about how classical communication is a
little bit problematic to define in quantum theory because, of course, you're introducing
classicality. And so you have to make some very specific assumptions about what's classical. And
we'll see later on today what the key assumption that gives you classicality is as we talk about
spacetime. But before I go into the topic of spacetime, I want to spend a little time...
OK, so this, sorry, this is where we're going. We're talking about spacetime, and we'll talk
about applications to biology next time. But before we get going on this, I want to spend
a little bit of time talking about explanation. And this is motivated by a question that came
up, a very long question that touched on many different topics that came up in the interactive
Q&A last time. And a fair amount of that question had to do with, directly or indirectly, the
question of what scale-free explanation looks like and how it compares to scientific explanation
in general, most of which is reductive. So I want to make a short detour into the question
of how reductionist theories, which all of you are probably very familiar with, relate to
scale-free theories. And particularly, the free energy principle is a scale-free theory.
So it's a theory that's unlike most of the theories that most of you probably learned about
in college. So we're all familiar with reductionist theories and the reductionist idea of scientific
explanation, which is really an outcome of philosophy of science in the 20s and 30s. People
like Rudolph Carnap were among the first to actually make precise what this idea of reductive
explanation is. But the basic idea is that the laws or the dynamics or the formalism that
describes behavior at some micro scale, the behavior of little things, actually explains
the behavior of big things. So the basic reductionist idea is that what the little things are doing
is what's important. And what big things are doing are either emergent phenomena or they're
just epiphenomenal. They're essentially just appearances. And this defines a fundamental
scale. So the real assumption in any reductionist theory is that there's some fundamental scale
at which the real dynamics takes place and everything else is just an appearance or some sort
of emergent phenomenon. And I try to be very careful when I use the word emergence because
it's one of the most ambiguous words in the philosophy of science and people use it to mean
all sorts of different things. But whenever you say emergent, it always sort of implies the idea
that it's emergent from something. And so the idea of emergence is essentially a reductionist idea
because it at least implicitly assumes that there's some micro scale from which the macro scale is
emergent. And so the discussions of emergent phenomena are offered very consistent with
reductionism even if they claim not to be. And we're all taught that science is reductionist.
And there are certainly people who publicly claim that science is reductionist. Richard Dawkins
is certainly a shining example of this. But I suspect that a lot of scientists just kind of pay
play lip service to reductionism without really believing in it because they don't really believe
that the macroscopic phenomena that they are studying, life, for example, is completely determined
by what's going on at the micro scale or what's going on at the micro scale plus some stochastic
noise or something like that. So I want to explicitly contrast this with a scale free theory.
In a scale free theory is not simply a holistic theory. It's much more precise. In a scale free
theory, little things and big things obey the same laws or they have the same dynamics where
they're described by the same formalism. So it's not that the macro scale is emergent from the
micro scale. The macro scale and the micro scale actually obey the same laws. And the
relationship between macro and micro is probably best stated as what implementation of macro
scale things have micro scale components and those micro scale components are doing something
or other and whatever they're doing implements the macro scale system and its behavior. But this
implementation is not an explanation or at least it's not an explanation all by itself. And the
bumper sticker version of scale free theories is just as above so below. Whatever laws you have at the
macro scale are the ones the same ones you have at the micro scale and vice versa. So scale free
theories in a sense are simple. It's the same theory at every level of description. So what is this
idea of scale? And how does it relate to what we've talked about in this course? Well, what we've
talked about in this course all has to do with what an agent can observe on its boundary. So scale
has to do with how information is encoded on the boundary. And we can describe that scale in many
different ways. Here's a commonplace way to describe it in physics by a relationship between
energy and distance or size. And you've probably all heard of the Planck scale. The Planck scale is
the smallest scale at which current physics makes any sense. And in energetic terms, it's defined by
energies on the order of 10 to the 19th GEV, which is gigo electron volts. The LHC, the Large Hadron
Collider in Switzerland, gets to about 10 to the 4th GEV. So many, many orders of magnitude less than the Planck
scale. So the Planck scale is extremely energetic compared to what we can probe experimentally. The Planck
scale energy of 10 to the 9th GEV corresponds to a distance of about 10 to the minus 35 meters, which is
incredibly tiny. An intermediate scale on this diagram is about the scale of a nucleon. So, for example, a
proton or a neutron, they have a rest mass of about one GEV and a size of about one femtometer. So this
intermediate scale is roughly the scale of nuclear physics. And then if you go to our scale, the scale of
things that are about a meter in size, human beings, tables and chairs, lots of animals, the things we're most
familiar with, that corresponds to an incredibly tiny energy. 10 to the minus 5 electron volts is about the energy of a
photon of radiation. And a meter is about the wavelength of long-wave radio. So if you think about visible light, it has a
wavelength of about 10 to the minus 5 meters and energy of about an electron volt. So it's not surprising that visible light
feels energetic to us because it's orders of magnitude above our natural scale of energy of this sort of thermal
environment in which we exist. So it's the encoding scale on the boundary that is going to relate theories of things that we
see to each other. And this ends up giving a kind of complicated picture. If you think of Alice interacting with Bob and measuring
things on the boundary. Sorry, I'm a little bit ahead of myself. I just wanted to show you a picture of a reductive theory.
This is the Big Bang. You're all familiar with this. Something happens at the Planck scale and the entire universe results from it by
just a scale change. So this is a reductive theory. A scale-free theory is much more complicated. So here's what a scale-free
theory looks like. Alice interacts with Bob and she's looking at her boundary. And her boundary states, the boundary states she sees could be
encoded at different scales. So there may be a small scale, scale one, and a bigger scale, scale two. And what Alice is looking for is some
reductive theory that relates the first scale, the micro-scale, to the macro-scale. And so it's not that the micro-scale came first, like it
did in the Big Bang, and the macro-scale came later. These are encodings that can be happening simultaneously. And the theoretical
task is to find theories that relate between these scales. And these are implementation theories, or you might call them embedding theories from one
scale into another. And what we want as a consistency criterion is for these theories to be constant in time. So as Bob evolves in
time, and so what Alice sees are these encodings on her boundary, she wants the relationship between encodings at different scales to stay
constant. Because if it doesn't, she'll never be able to figure anything out. She'll just have this chaotic relationship between things happening at
different scales, and nothing will make sense. So you have this complicated picture in scale-free theories. And if you look at this, and it seems vaguely
familiar, or even vaguely, even very familiar, then it should. Because this is exactly the kind of explanatory structure or semantic structure that you
have in computer science. If you're studying the behavior of a computer, which is just some piece of hardware, some physical
system, then you can describe what that piece of hardware is doing at different scales. And in fact, what we call the hardware level of
description, when we talk about, you know, our laptops, for example, is typically the level of description of circuits of transistors inside
microprocessors. We think of that as the hardware level. But of course, that's just a description, too, that's layered on top of lots of much smaller scale
dynamics, all the way down to the scale of atoms and eventually nuclei and elementary particles and all that other stuff. So there's some quantum stuff that's going on that we
describe as having a classical hardware level of bit exchange. And then we describe the behavior of programs on top of that. So what we want, what makes a
computer useful is that we can assign semantics to programs. And we can talk about semantic relationships between programs, for example, the relationship between the internet
protocols, stuff that zoom is doing, and what you actually see on your user interface and here on your user interface, that needs to stay fixed over time, or zoom becomes incomprehensible and not programmable and in fact
not even well defined as an algorithmic system. And the same is true for the relationship between the operating system and the hardware description, or between the operating system and zoom is a program. So computer
science works in exactly this scale free way. And so in a sense, scale free explanation is very familiar. It's exactly the kind of explanation that we have in computer science. And that's why I refer to the relationship
between scales as one of implementation, because that's a term that we're familiar with from thinking about computers implementing programs. So what are these theories, and if we think in biological terms, we can think of pathways
embedded in a cell, or cells being embedded in tissues, all the way up to niches of various kinds, which may be social niches, being embedded in ecosystems and ecosystems being embedded in the biosphere. And so we have all of these essentially semantic
theories that talk about how one scale embeds in another scale. And if we have these sorts of scale transitions, we can think of representing the relationship between transitions at different scales, in terms of the activity of some kind of
renormalization group, which is transforming entities and processes at one scale into entities and processes at another scale. And I think in biology, we can at least put forward the hypothesis that in all the way from pathways
embedding in cells to ecosystems embedding in the biosphere, we could put forward the hypothesis at least that the renormalization group flow across all of these scales is trivial. That is that the embedding theories all have the same formal structure.
The structure that describes how pathways embed into cells is the same structure that describes how ecosystems embed into the biosphere. Now I certainly can't prove that. I believe that it's likely.
So I think it's an interesting hypothesis to investigate. And this is an hypothesis that I want to leave you with.
As a result of this class is something to think about.
To what extent, in thinking about biological systems or even social systems are the embedding theories the same.
So in thinking this way that the physics of the situation is has the same formal structure as computer science has in terms of explanation.
Oh, we're of course faced with a problem, which is that we're not just reconstructing physics.
We're talking about the physics of of interacting observers. We're talking about the physics of communication.
So we're modeling how we do physics.
And that makes our theory self referential.
And so we have to base the music from Gertl's theorem, which tells us essentially that powerful self referential theories can't be both consistent and complete.
And by powerful, I just mean powerful enough to express arithmetic. So not terribly powerful.
So this becomes an issue that we have to live with.
We will always be faced with this issue of having to construct metatheories, which tell us how our lower level theories work.
But we never get to the end of that process.
We never can get to theories that are both consistent and complete because our tower of theories becomes progressively actually more powerful as it goes up.
So Gertl, in a sense, poses a problem.
But in another sense, just tells us what life is like as a scientist that you can't have everything all the time doing science.
But we probably all already knew that.
So let's now talk about space, end of digression, and see how these ideas apply to a system or a tower of systems that can construct a spatial embedding in its world, in its observed world.
So let's start out thinking about a system that can see its environment, but what it sees as a completely uniform environment where nothing is happening.
So if you have a uniform environment where nothing is happening, you can't distinguish one part of the environment from another.
So you have no sense of space.
So what I want to do is build up from scratch what's needed to have a sense of space.
And then we'll talk a little bit about what organisms are doing.
And I think a very interesting question is, if you look at phylogeny, at what points in phylogeny, what lineages in phylogeny actually need to have a sense of space?
And what do they have to have in terms of computational power to have that sense of space?
So if you just have a uniform environment, you don't have a sense of space.
But the first thing we can do with an environment is to divide it up into some number of parts that are somehow distinguishable.
So suppose we can segment the environment into two different parts.
And if you think about E. coli, for example, E. coli has a bunch of sensors on the front and a bunch of effectors on the back.
So it can sense much more about the front part of the environment than the back part of the environment.
So already with E. coli, you have this kind of segmentation of the environment into parts.
But so far, nothing has happened in this environment.
We just have segments that can be distinguished.
So let's consider an environment like this that has distinguishable segments.
What one next wants to know about these segments is whether they're next to each other.
And if we just have a bunch of segments, we just have a set.
And a set doesn't have any order or any relationship information.
So I've drawn this environment as a rectangle of rectangles, but that's just a representation.
And it looks like there are relationships between these rectangles.
But if they're just a set, then there are no relationships.
If we want to add relationships, then we have to add them explicitly.
So let's add some connections between these different segments of the environment to turn that set into a graph
that says explicitly, for example, that the light green segment is attached to the light blue segment.
And it's also attached to the dark green segment.
And the dark blue segment is attached to the light blue segment and attached to the dark green segment.
So now we have something that's much more mathematically complicated than a set.
We have a graph that has some connection information.
So we have a connection to apology.
Now, adding this topology does something very important.
It breaks the exchange symmetry that is there if you just have a set.
So if we just have a set, it doesn't make any difference.
If I exchange, for example, the positions of the upper right light blue segment with the upper left light green segment.
But if we have this connection to apology, then it does make a difference.
I've twisted the graph around, and if I have to break connections to exchange things, then I've actually changed the topology.
So adding a topology, adding connections can disrupt a symmetry that's there if there is no topology.
So now let's think about, now we have the structure that's required to think about something happening.
So suppose that at some time, as measured in the bit counter time reference frame of the observing system, something happens.
So a dot appears in the light green segment.
And we can express this actually in the notation of quantum field theory.
We can think of the light green segment as a field, and the dot appearing is the occurrence of an excitation in that field.
And so we can write this notation that says that the green field was excited to produce a dot, which is up arrow green dot.
