and conformal field theory, quantum field theory on the boundary
of the black hole.
So things you can see, for example, going on on the horizon
of a black hole or dual to the geometry in the interior
by this mathematical relationship.
You can use that mathematical relationship
to construct a quantum error correcting code
by using the symmetries of the field theory
as a resource for redundancy that allows you
to represent the same thing in different places.
And as soon as you can do that,
you can build up a network of elements
that look basically like elements of a lattice.
And a lattice is a graph to which you can assign
some metric relationships and you get a metric space.
And in physics, typically the goal is to get Einstein's equations
out of this to start with some information,
theoretic sorts of constructs, and get the equations
of general relativity.
But what are the equations of general relativity?
I mean, these are things that Einstein derived
by doing fairly simple thought experiments
using the invariances of ordinary Euclidean space
combined with ideas from special relativity
about the speed of light being constant.
And he added the principle of equivalence,
which is the idea that essentially mass
and acceleration are the same thing,
or that gravity and acceleration are the same,
the weight and acceleration are the same thing.
So it's at least reasonable to expect
that this inferential process that's going on
in theoretical physics is generating the same thing,
although in a very formalized way,
that we can generate by thinking about how organisms deal
with their environments.
And so I want to leave this with you
as the second hypothesis here.
The first one was the hypothesis about a trivial
renormalization group flow across scales and biology.
The second hypothesis is that these two approaches
to emergent spacetime are actually giving us
the same information and maybe starting out
with the same information, in fact,
that we may be able to think about these fundamental
building blocks of information flow and entanglement
in terms of the sorts of perceptual relationships
that we've been talking about in this class.
So there's our second hypothesis to think about going forward.
And I just want to leave you with the idea that organisms,
at least, manage this very interesting process
that we can think of starting with the construct of memory.
Organisms have memory at multiple scales,
and it's memory that allows them to formulate the idea
that there are objects.
And as soon as you have objects that persist,
you have a source of redundancy.
So you can do error correction.
And if you can do error correction,
if you can move the same thing from one place to another
and have it encode the same information,
then you can build spacetime.
And as soon as you have spacetime,
you can use the spacetime as more memory.
You can use the spacetime itself as an error corrected code
because spacetime has the nice ability to hold lots of copies
of some informative structure.
So you can build libraries or you can build computers
or you can build the internet if you have spacetime,
which gives you more memory,
which gives you more object persistence,
which gives you more error correction,
and the circle just spins around.
So organisms do this,
and the question is how does it get off the ground?
And next time we can at least speculate
about that kind of origin of life type of question
in the context of the FEP
and its realization in simple systems of communicating observers.
So as always, I encourage you to use the interactive Q&A.
The fifth discussion session with Andrew,
who's unfortunately not here today,
will be the last day of September,
Saturday at five as usual,
five o'clock European time,
eight o'clock California time.
And then the last session of the class
will be the 12th of October.
So thank you very much.
And I look forward to your questions
in the interactive Q&A.
Thank you, Chris.
Can I ask you some short questions?
Sure.
All right.
So first from Lana wrote,
is the periodicity the time lengths between
and during the excitations built in?
So this was when we had the four cells
and there was the inklings of a clock coming into play.
I think the simplest answer is that
if nothing else is happening in the environment
except this periodic behavior,
then it's not actually meaningful to talk yet
about the time between the events.
So if I have a periodic, a system that's quasi periodic,
for example, and nothing else is going on in my environment,
then I can't distinguish that from a perfectly periodic system
because I would have to have a perfectly periodic reference
to know that the other system was a periodic.
And if you think about the internal clock
being driven by information flow into the system,
then something has to be happening in the environment
for information to be flowing into the system
and being counted.
So again, as we talked about a few sessions ago,
the internal memory only gets updated when something happens.
So internal and external get tied together.
And this is why we can use external clocks.
But it's also why we depend on external clocks
because our intuitive sense of time is very malleable.
And so we actually, as humans, use external clocks
to correct for the malleability of our own intuitive sense of time.
Wow.
And there's probably a lot to be said for that procrustian nature
of having the conometer, the clock on the wall,
and the tick, tick, tick as an evolutionarily novel ruler
of time playing a logistical role in time-bound society.
Yeah.
And think of earlier clocks that earlier cultures have used
and that animals use.
I mean, we have the solar clock, the lunar clock, et cetera.
And the orbit of Venus clock that was so important to the Mayans
and so on.
So there are lots of available clocks in the environment
if you have the ability to notice them
and enough memory to realize that they're clocks.
Awesome.
All right, here's a question from Upcycle Club.
They wrote, how does representing the network structure
and dynamics with quantum embeddings affect the semantic interpretation
of the network?
The embedding theory is the simplest semantic interpretation
of the network.
So...
And again, a good way to think of this is through the lens
of computer science.
As soon as you have a programming language that allows variable binding,
then you've built a very simple kind of semantics into the language.
If you have X as a variable and you have variable binding,
then at different times in the execution of the program,
X can have different causal consequences
for downstream computation
because it's been bound to a different value.
So it has different semantics.
So the semantics of X, again, go back to Gregory Bateson.
The basis of semantics is differences that make a difference,
make a difference to behavior,
make a difference to actionability.
So variable binding gives you differences that actually make a difference
to what the program does.
So what is that?
That is the semantics that's assigned by your embedding theory
between what the programming language is doing
and what the operating system is doing and so forth.
And I think it's no surprise that the semantics of programming languages
end up being represented with these very general mathematical constructs
like category theory,
which are essentially tools for representing arbitrary relations
between systems.
And that's what embedding theories are.
They're theories that state relationships between systems.
Awesome.
And very provocative to explore on the biological and life
and on the spacetime and matter sides,
which are, of course, seamlessly integrated through,
if only our own experiences.
Yeah, this is another place where it's useful to go back to this
difference between reductive theories and scale-free theories.
And scale-free theories are all about semantics
because they represent the existence of stuff happening at different scales
and so they naturally raise the question of what are these relationships?
What are these embedding theories?
And in reductive theories, there's a very deep sense in which semantics doesn't matter.
If you think that all that's going on that's important
is the stuff that's going on at the Planck scale,
then you don't need any semantics, right?
And so you see this in late 60s, early 70s, old-fashioned AI,
where people talk about computation as purely syntactic.
And this is a very reductionist view of computation
without the semantics.
And without the semantics, the computation is useless.
It doesn't tell us anything at all unless we can map it to something.
So we have this philosophy of science tradition
that tells us that semantics is not important.
And we end up with tools like the FEP
that essentially tell us that semantics is all important.
And from a biological point of view,
the most natural response is, of course, we knew that all the time.
That's why we're doing biology.
