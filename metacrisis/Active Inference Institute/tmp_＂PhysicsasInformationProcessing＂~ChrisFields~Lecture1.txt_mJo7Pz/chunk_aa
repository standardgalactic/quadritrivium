Hello and welcome everyone to the Active Inference Institute.
This is session one of the course, Physics as Information Processing with Chris Fields.
First we'll have Anderra Ghire and Chris Fields introduce themselves and then we'll
carry on with the first lecture here.
Check out the video description for a link to the course overview website where you can
ask questions that will be answered asynchronously, register to participate in the discussions
which happen about two weeks after each of the six lecture sessions and just learn more
about this area.
So thank you both so much for joining into this adventure.
We are starting now and first please Anderra Ghire introduce yourself and then Chris introduction
and lecture.
Thank you.
Hello, so I'll be the course assistant and so I'm a post-doc in math specializing in
probability and I have a deep interest in the physics of information and I've been familiar
with Chris's papers for a while so yeah, just here to learn myself.
Thank you Anderra and thank you Daniel.
I'm Chris Fields and I'll be presenting this course in six sessions and Anderra will be
organizing discussion sessions after each of those and all of this is explained on the
course website.
So let's start.
This is a course on physics is information processing and this first session will be a
historical perspective on the idea that physics is is or is about information processing.
And I'll just start with a few quotations that span the middle of the 20th century from
Wittgenstein in 1920 saying the world is all that is the case so defining the world in
terms of facts, not objects, Landauer in the early 60s proclaiming that information is
physical and then John Archibald Wheeler who in many ways is the grandfather of this
era recently stating it is it from bit so things come from information i.e. bit strings.
And if nothing else this shows that the formulations of this idea get pithier as the 20th
century rolls on but the history goes back further clearly than the 20th century.
But I'm only going to really talk about a piece of it.
And the timeline that I'll actually discuss today the most relevant history of this idea
goes back to the mid 19th century.
And the first specific thing I'll talk about is Clausius definition of entropy.
But with the beginning of the understanding of thermodynamics and the role of information
in thermodynamics you get this very interesting multidisciplinary progression of ideas that
incorporates the beginning of quantum theory and the beginning of quantum theory can kind of be
dated to the the first Solve conference in 1928 and the famous debate between Bohr and
Einstein over whether quantum theory is about knowledge information or objects things.
But it incorporates a lot of work in computer science and logic and mathematics.
So interestingly computer science was born effectively in the mid 30s with the work of
church and Turing which very rapidly converged with the work in physics.
So today we'll be talking about both computer science and physics.
And then in the second half of the 20th century this just exploded into a huge area.
And in consequence of that mid 20th century development we're beginning to see a new idea
about physics which is roughly encapsulated in quantum information theory.
And the new idea is this.
It's that what physics is actually about is information transfer across boundaries.
And the information we can represent the information transfer like this and this is a
convention I'll use a boundary is always a blue ellipse.
And the agents that are exchanging information across this boundary are conventionally called
Alice and Bob which is just a more polite way of saying A and B.
And when you think about this picture it becomes clear that what physics is really about is
communication.
And this is a wild re-description of the idea of what physics is compared to the ideas of
Newton or Laplace or even the 19th century ideas.
And it's very different from the idea that's been preserved in 20th century physics in
the lineage of Einstein and others who viewed classical physics as in a sense of fundamental
either as completely fundamental or as a fundamental adjunct to the idea of what physics
is.
So this way of thinking about physics is a very deeply quantum-theoretic way of thinking
about physics.
And where we're going in this course today is really how did this all happen?
It's the origin story.
And then in the next session I want to talk about the origin story.
And then in the next session I want to discuss quantum information theory explicitly and
in particular how quantum theory makes this conclusion that physics is about communication
very simple and obvious, much more obvious than it is in classical physics where it
takes work to formulate this idea.
Then in the next session we're going to talk about semantics and how observations become
meaningful to the agents who make them and hence how actions become meaningful to the
agents who make them.
Then in August we'll talk about communication theory a little bit more explicitly and talk
about how agents employ multiple communication channels when they're communicating.
And this is obvious when you think of people communicating.
They not only talk to each other, they look at the same things, they point to things, etc.
So this is what I mean by multiple communication channels.
Then in September we'll leverage that discussion to talk about how space-time actually emerges
from communication.
And this is one of the most important aspects I think upon in information theory.
It provides us with a way of viewing space-time as an emergent phenomenon that communication
is what is fundamental in some ontological sense and the box in which it happens, space-time
is not.
So the final session in October will talk about applications to biology via the free energy
principle and future directions both in physics and biology and elsewhere.
So it's going to be an interesting ride.
I'm keeping formalism to a minimum because we're directing this toward the broad array
of people who are interested in active inference and who are involved with the active inference
institute.
And I would ask you to hold questions because we have a lot to get through in an hour for
the interactive discussion and for the discussion forum.
So I hope I explained things well enough that all of the concepts will be understandable.
If not, Wikipedia is actually a wonderful resource in this area for just definitions of terms.
So if there's anything that just a term that is a trip up, try Wikipedia, it's probably
a very good source for what these terms mean.
So let's start.
Our story, as I said, begins in the 19th century.
And in the mid-19th century, lots of physicists were devoting their efforts to figuring out
how to make better steam engines.
And one question that arises when you're trying to design a steam engine is what happens
physically when you add heat to a system at constant temperature?
So if you're building a steam engine, you've got a boiler because you need to make steam.
And as you turn up the heat to your boiler, you get more steam, but the temperature doesn't
change.
So this is a mystery.
What is the heat actually adding to the boiler that is not increasing the temperature?
And Clausius responded to this question in a way that's sort of typical for a physicist
or an mathematician.
Since he didn't know what the answer was, he just invented a new name for whatever it
was and gave it a formal definition.
So he called it entropy, which is a made-up word that if it was translated from the Greek
would roughly mean transformation content in trophy.
And he represented it by a simple equation that the change in this new concept, entropy,
which is always called s, is just equal to the change in heat q at constant temperature
t.
So this, obviously this equation just reformulates the question in declarative form saying whatever
this stuff is, its changes in this stuff are just changes in heat at constant temperature.
Well, heat is energy, and this wasn't completely recognized in the mid-19th century, but the
way you'll see this equation in a current textbook is ds is the energy change in energy
at constant temperature.
So even more commonly you would see it written as the change in energy is equal to the temperature
times the change in entropy.
It's the most common sort of textbook way of seeing this.
The question, of course, is what is this quantity?
What is this entropy?
What does this concept mean?
And about 15 years after Clausius proposed it, Boltzmann had the key insight, which is
that entropy is a measurement of our uncertainty about the state the system is in.
And in particular, he, again, of course, went to formalism and said the entropy s is equal
to some constant times the number of states that the system can be in that look the same
to us.
And since that number of states is enormous, the way to make that manageable is to take
the log of the number of states.
The natural log is ln.
And this constant k is called Boltzmann's constant.
And Boltzmann was able to do this because he subscribed to a radical and very unpopular
theory that material things, including gases like air, were made of atoms.
And heat made the atoms move around.
And as you increase the amount of heat, the atoms can move in many different ways.
So the number of states that they can be in that look the same to us increases.
And that's what entropy is.
It's this increase in the number of states that the system can be in that all look the
same to us with the measurements that we can make.
And since they look the same to us, we're uncertain about exactly what state they're in.
So entropy is a measure of uncertainty.
This was really the beginning of modern physics because what it says now is that decreasing
uncertainty requires energy.
It links a measurement of uncertainty to a measurement of energy.
And if you think about the uncertainty principle and quantum theory, the core idea of the uncertainty
principle is you can't measure a system without disturbing it.
So to actually act on a system requires energy.
And that's what you have to do to get information.
So here's Boltzmann basically inventing quantum theory.
So we're going to fast forward by another 15 years to 1900.
And in 1900, Plank solved this problem called the Blackbody Radiation Problem, which was
basically how much heat does your hot boiler give off into the air?
And all of the measurements of the heat that hot boilers gave off to the air ran into problems
in classical physics and caused contradictions and quantities that went to infinity and all of that was bad.
So many people were trying to solve this problem.
And Plank solved it by making a simple postulate.
He said the energy of the radiation is proportional to its frequency.
So it's color in the case of light.
And if you go higher frequency, you end up in ultraviolet and x-rays and gamma rays.
If you've got a lower frequency, you go into microwaves and radio and all of that.
So this is a nice way of talking about radiation.
And it turned out that this solved the problem.
I mean, just assuming the simple proportionality relationship produced spectra for Blackbody Radiation
that worked, that matched what you saw experimentally.
Well, this means something very important.
It means because this number, h, the proportionality constant called Plank's constant, is a number.
It's finite.
It means that energy comes in discrete units of h.
You can have one h or two h or 10 million h, but you can't have half an h of energy.
So it's quantized.
And this is widely recognized as the birth of quantum theory.
But of course, we should have known this already if we just thought a little bit.
Right?
We know that changes of energy are proportional to changes in entropy by temperature.
And we know that entropy is a measure of the number of states.
And numbers of states are just numbers.
You can have one state, two states, three states, 10 million states, 100 billion states,
but they're all just a number, one up.
And it's not infinite.
There's not an infinite number of states unless you have an infinite amount of energy, which we don't have.
So we knew already that entropy could only take discrete values.
And since energy and entropy are basically the same thing,
we knew already that energy could only take discrete values.
So we could have realized in 1900 that energy is quantized because the number of states is quantized.
So it shouldn't have really been a mystery why energy was quantized, but it was a mystery.
And it's still a mystery.
People still debate the meaning of quantum theory.
But another thing we could have known in 1900 was something very important.
And it's that this quantum of action, Planck's constant, which is units of action, which is energy times time,
is intimately related to Boltzmann's constant.
And Boltzmann's constant has units of energy over temperature.
But this wasn't actually understood until the 1950s.
No one really figured this out.
There was this relation until the 1950s.
And when it was figured out, it was figured out by a guy named Jean-Carlo Wick.
And he introduced this notion of the Wick rotation by realizing that if you have an equation of classical physics
and in it there's the term 1 over kT, you can always replace that 1 over kT with this other expression,
