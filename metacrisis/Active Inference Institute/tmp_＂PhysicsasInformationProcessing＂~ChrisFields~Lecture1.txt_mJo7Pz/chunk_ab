it over h bar.
h bar is just h divided by 2 pi.
And you'll get an equation that's valid in quantum theory.
And this is typically described in textbooks as a trick.
And whenever something in physics is described as a trick, what that really means is it's something we don't understand.
And lots of papers have been written about the meaning of the Wick rotation.
But to start to understand the Wick rotation, I want to look at this equation a little bit.
1 over kT is 1 over in energy.
And it over h bar is 1 over in energy since h bar is units of energy times time.
We have time in this equation.
And then we have this factor i, which is typically thought of as just an imaginary number.
So it's the square root of minus 1.
And what's this i doing in this equation?
And in fact, you see factors of iT over h bar if you're familiar with quantum theory everywhere in quantum theory.
So what's the meaning of this imaginary number?
And if you just think of iT or of i as this arbitrary imaginary number that somehow renders the equation mysterious,
then the whole of quantum theory is mysterious, but the Wick rotation is very mysterious.
But what i is actually is an operator.
And if you think of the real numbers as an axis, which is always drawn horizontally, so here it is.
Here's zero.
And the axis is pointing that way.
I'm sorry, I can't get the camera really far enough away to see my arm here.
So what is i?
i is actually an operator that rotates the whole real axis by 90 degrees.
So if you if you see a plot of complex numbers, then the real numbers go this way and the quote imaginary numbers go that way.
So what multiplying by i is done is rotate by 90 degrees.
And of course, if you do i squared, you rotate twice by 90 degrees by arm would do this, but you end up pointing that way.
And those are the negative numbers.
So i is an operator that rotates something by 90 degrees.
And if you rotate four times by 90 degrees, you're back to the identity.
So i to the fourth is one.
So this tells us something very interesting, which is that what the Wick rotation is really talking about is a rotation.
It's a geometrical equation.
And in July, we'll come back to this and really probe what this Wick rotation means physically.
But as I said, this wasn't understood in the 1950s.
And by the 1950s, a lot had happened.
So quantum theory had been developed.
Einstein had had their debate.
Particle accelerators had been built.
The atomic bomb had been built.
Nuclear physics was well in its way.
So an enormous amount of practical physics had been done.
Quantum theory was highly developed.
People were starting to think about quantum field theory before they made this this simple realization that's formulated in the Wick rotation.
So this is a harboring of things to come.
Let me continue in physics.
We need to backtrack in time a little bit and look at what the mathematicians were doing.
So across the hall in the math department, one year after the Solve conference in 1929, Kurt Gertl proved his famous first in completeness theorem.
And the theorem states that no formal system that contains arithmetic can be both consistent and complete.
And that means that either there are true statements that aren't provable in the formal system, or there are false statements that are provable, or of course, both.
And Gertl's proof is actually extremely simple.
Almost all of the work in the proof is setting up all of the notation and procedures and so forth to formulate within arithmetic the sentence, this sentence is not provable.
And once you have that sentence formulated within arithmetic, then the conclusion of the proof is obvious.
You know, if you can prove the sentence this sentence is not provable, then you prove something that's false.
And if you can't prove it, then there's in their sentences that you can't prove an arithmetic.
So this was incredibly bad news for mathematicians who thought that finite discrete operations, which is what proofs are, and also what computations are, can exhaustively enumerate the facts.
And this was the assumption behind Wittgenstein's claim that the world is all that is the case, the world is a collection of facts.
And optimistically, he thought that first order logic would allow us to enumerate all those facts and we'd be done.
Dreams of a final theory again.
So, so Gerdl's theorem means that no system with finite capabilities, no system that can just do finite discrete operations can fully describe its environment.
It will always be in an environment where there are true things that aren't provable or false things that are provable.
But I think more relevant to a discussion of agents is that it means that no agent can describe itself.
Any agents theory of itself will either contain true statements that it can't derive, or false statements, or it will either miss true statements that it can't derive, or it will end up deriving things that are false about itself.
Of course, we see this in psychology all the time.
So, an immediate consequence of Gerdl's theorem was an intense investigation of what computation actually is, what it meant to talk about finite discrete operations.
And two leaders of this were, of course, Church and Turing, and here's a picture of a Turing machine, which is just a little device of a couple of tapes, and a tape reader and a simple logic unit that either writes a one or a zero if it sees a one or a zero.
And they defined a computation as a process that can be implemented in finite time by such a machine, or by Church's lambda calculus, or by any of the now hundreds of other methods that are provably equivalent to a Turing machine.
So what does this mean?
It means that computation is a physical process that can be mechanized, and it turns out mechanized in any one of a huge array of ways.
And it means that many different implementations of any computation are possible.
So I can do it on a Turing machine, I can do it on my laptop, I can do it on my head, etc.
The most important things it means is that there are questions with no computable answer.
This is the revenge of Gerdl's theorem.
And two of the most famous questions of this kind are, given some arbitrary program, will it halt?
Will it get to an answer in finite time?
The answer to that question is, this is undecidable.
No procedure can figure this out.
And the other undecidable question is, given some arbitrary program, what function does it compute?
And you'd think that would be simple, that you can read a program and figure out what function it computes.
But it turns out that is undecidable.
That cannot be done by any finite process.
So this was another body blow to the goal of understanding everything with finite discrete processes.
But it also set the stage for something new.
It set the stage for thinking about an agent who interacts with a computational process by giving it an input and then looking at its output some time later.
And this, of course, will look familiar because I've included the blue ellipse, which is the boundary, which these days we call a user interface.
And the user interface just allows some finite action on the system and then the ability to observe some finite response by the system.
So we can now ask, what can Alice determine by acting in some finite way and then making some finite number of observations,
i.e. receiving some finite number of outputs from the system that she's acting on?
And the first 20 years of this produced a large number of answers, all of them negative.
So to go back to Turing, he proved that Alice can't tell what's implementing the function that she sees being implemented.
She can't tell whether a given input will lead to an output. That's the halting problem.
Shannon showed that Alice can't tell what the inputs mean to the system.
His whole theory of communication is completely independent of semantics.
And his theory of communication actually accurately describes what Alice can observe.
Rice is the one who proved that you can't determine what program is generating the outputs,
and then Moore proved a very similar result in a completely different formal setting of general cybernetics
that you can't tell what process is generating the outputs by finite observation.
But what Alice can do is build a predictive model of what generates the output she sees in response to her inputs
and test it by designing new inputs.
And this is, of course, as Carl Popper told us, the process that we call science.
So Alice can do science even though she can't answer any of these fundamental questions.
Now this, of course, has a huge technological consequence.
Since this theory of computation tells us that processes are effectively virtual,
we don't know what they are and we can't determine what they are except in theory by making a theory.
Technologically, it means we're free to use virtualization everywhere because we have to deal with it anyway.
And this allows us to build multi-level architectures.
It means that we can architect computers where no layer of the computation has any idea of what's going on below or above it and doesn't need to.
And that's what makes practical programming possible.
So from these no-go theorems that tell you what you can't do, you actually get an enormous boost into it and use to probe the world and develop theories and on and on and on.
So in a sense, Gertl birthed not only computer science but practical computing by showing us that virtualization is just the way the world works.
So now let's go back to physics where these ideas were replicated, basically reintroduced, reinvented by Feynman in developing his pathological formulation of quantum theory.
And basically what Feynman realized was that in any physical process of the observer, Alice, prepares some state that she's interested in.
She prepares some input to an experiment.
Then she lets something happen and then she sees what the result of the experiment is.
And the canonical experiment in physics is scattering.
You fire two protons at each other and they intersect someplace and stuff comes out and you measure the stuff that comes out.
And what you can measure is momentum and spin, energy, things like that, position.
So these processes conserve the total values of the things you can measure.
So in particular, they preserve momentum and angular momentum and they preserve other things that are harder to measure and are only approximately conserved anyway like lepton number.
But Feynman's contribution to this way of thinking about experiments was to say, look, if you want to understand the output, you have to sum over all of the possible processes that could have produced the output from the input, no matter how improbable they are.
So the famous idea from Feynman diagrams is if you have an electron that's scattering off an atom, you measure the initial state of the electron that you've generated with an accelerator or something, and you measure the final state which involves momentum and spin and so forth.
And for all you knew, partway through the process, the electron disappeared and an entire universe appeared and then annihilated with a copy of an entire anti-universe and the electron came back out.
And you have to include processes like that if you really want to understand and correctly predict the outcomes of your measurements.
And Murray-Gelman lifted the bumper-sicker slogan from T.H. White and the Once and Future King, Everything Not Explicitly Forbidden as Mandatory, that sums up Feynman's idea.
And this is called the Totalitarian Principle, since it's what's written in T.H. White's book on the Kingdom of the Ants, for which everything not explicitly forbidden is mandatory.
So let's think about a real example that's a bit, actually, it's the same as scattering, the ultimate scattering experiment in physics is a black hole.
Stuff goes into the horizon, stars, whole galaxies, whatever. Something happens and stuff comes out. And what comes out is Hawking radiation.
And information is conserved if the information in the Hawking radiation is actually the same as the information in the stuff that went in.
And conservation is not qualitative, it's quantitative, it's only the quantity of information that's supposed to be preserved.
And you can ask how much is the information that's being emitted by a black hole that's emitting Hawking radiation.
And Beckenstein was the one who figured this out using an incredibly simple argument, but I'll just give the answer here.
The answer is that the total entropy of the black hole is its area divided by four.
And the area in this equation has to be computed in Planck units, which are units where Planck's constant and the speed of light and Boltzmann's constant and other interesting things are also equal to one.
So one Planck area is the Planck length squared, which turns out to be about 10 to the minus 70th meter squared.
So this says that black holes are the most entropic entities we know of.
So a black hole about this big with a radius of about a meter has an entropy of about 10 to the 70th, which is of course an astonishingly big number.
And a black hole with the area of the sun, so a moderate sized cosmological black hole, a real thing that we can observe with a gravity wave telescope or something, has an entropy of about 10 to the 79th.
And really big black holes, which are bigger than the entire solar system, have entropies into the 10 to the 80th, you know, 10 to the 85th or something.
So these are enormously entropic entities.
But Beckenstein didn't just tell us that.
He told us something about the structure of the interface, the horizon of a black hole.
And this is what he told us.
You can compute entropy in bits just by using Logspace 2 instead of Logspace E, natural logs.
And that's just multiplying the natural log by about 1.4.
So we can write the entropy of black hole in bits, and it's about A over 6.
So what does this mean to say that we can think of the entropy in terms of bits?
What it says is that we can think of the interface in terms of a bit array.
And we can think of all of these bits as encoded on this interface at a density of one bit every roughly 6 plank lengths squared, 6 plank areas.
So this is an incredibly dense encoding in a black hole.
And it's an interesting idea about black holes, but what gets really interesting is what happens when you generalize it.
And of course, physicists are drawn to generalization, and that's what happened next.
Gerard Tohoft almost immediately thereafter on the basis of Beckenstein's work formulated the holographic principle.
And what the holographic principle says is we can think of any system as approximately a black hole.
And the only approximation is the encoding density.
The boundary of any system encodes the information that we can get about the system at some density, and the density is less than the density for a black hole because we're not black holes.
And this rules out a lot by limiting the amount of information that you can extract from a system to the amount of information that you can actually write on its boundary.
And one of the things that it rules out is knowing the geometry on the inside of the system.
And Tohoft put it this way, which I think is a brilliant thought experiment, he says, look, the metric inside this system can be so curved, effectively the space time inside the system can be so curved that you could stick an entire universe inside.
And we never know because it wouldn't change the amount of entropy, number of bits that are encoded on the boundary.
So the inside geometry can be anything and you can't find out by looking at the outside of the system.
Now the holographic principle almost follows from classical physics.
There's Euler's theorem, which, which tells you about one over R squared forces.
And you can think of the forces penetrating the boundary at some density and the difference between Euler's theorem and the holographic principle is just that the density in classical physics can go to infinity.
And in the holographic principle, it can only go to this maximal density that's achieved by a black hole.
Why? Because if you try to get any higher and more information, the information gets sucked into the black hole and you can't get it out.
So the holographic principle becomes a guiding principle for thinking about any physical system and what it means to extract information from a physical system.
And in fact, we'll talk about this next time.
You can say exactly what the information encoded on the boundary of any system is.
And what I'll go through next time is seeing that the information that's encoded on the boundary is a specification of the energy that's being exchanged by the by the interaction, which of course is linear in the number of bits.
It's just counting the number of bits.
Okay, so with the holographic principle, we now have a complete new science about systems that are exchanging of finite discrete information across a boundary by encoding that information on the boundary and then reading the information off the boundary.
So here's a new way of thinking about physical interaction.
Alice writes a message on her boundary with Bob and Bob reads the message off the boundary and then writes his own new message, which Alice then reads.
And of course, that's what's happening right now.
I'm writing information by speaking on an interface, which is effectively the Internet, and you're reading that information off the Internet by listening to it.
And when we get time for questions, you'll be writing information on the Internet, which I'll be reading.
Now, the key thing about this new science is that it's topological.
It's about connectivity across some communication channel, which we represent as a boundary.
It's not geometric.
So it doesn't assume anything about space time.
So it allows us to build a model of space in particular as an emergent phenomenon.
And it allows us to see time as not some absolute external abstraction, but something that the communicating agents measure for themselves.
And so they each act in their own time.
And we get a very natural measure of time in terms of how many bits I see coming across my boundary.
And again, we'll talk about that in the next couple of sessions.
And then talk about it more thoroughly in September when we talk about emergent space time.
So quantum information theory looks very different from the physics that came before.
Not because it's adopted a new formalism, the tools of quantum theory.
It's because it's entirely changed the thinking about what physics is and what it's about.
And replace this idea of forces and balls banging into each other and all of that with the idea of communication between agents.
And of course, that's familiar from an active inference perspective.
So we can now back up a little bit to see what they were doing in classical physics during this period.
Well, let me let me go on a little bit. Sorry.
This is this is just a slide quoting Wheeler, who of course is the most radical in terms of in pithiest in terms of formulating these ideas.
But here's his characterization of this new physics.
It's not reductive.
All of the reasoning is circular.
You don't have smaller things and then yet smaller things and then yet smaller things forever.
There are no laws.
So what you see on the interface is a message.
