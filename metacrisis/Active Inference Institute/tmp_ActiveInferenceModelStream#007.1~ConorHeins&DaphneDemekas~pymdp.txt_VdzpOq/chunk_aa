Hello and welcome, everyone.
This is the Active Inference Institute.
It's November 15, 2022, and we're in Model Stream 7.1.
We're going to be discussing PyMDP, a Python package for Active Inference in discrete state spaces.
We will all say hello, then we'll pass to Connor for a presentation.
Following the presentation, we'll have some discussion, take a look over PyMDP scripts,
take any questions that are coming up in the live chat.
Thanks to the authors for joining today and also to Jakub.
We'll just start by saying hello.
So I'm Daniel, I'm a researcher in California, and I'm really excited to learn a little bit more about PyMDP and see how Active Inference gets applied.
And I'll pass to Jakub.
Hi, everyone. I'm Jakub. I'm a student in the UK and also very excited to hear more about PyMDP and discuss the recent developments and plans for future development.
I'll pass it to Daphne.
Hi, I'm Daphne.
I'm working here in London. I used PyMDP a lot for work that I did with Connor on my master's thesis and also just work that we've been doing since then.
So I definitely think it's a really, really great package and I'm happy that people are going to start using it more.
Excellent. All right, Connor. Thanks a lot for joining. Take it away.
Thank you. Thanks for the invite. I'm glad that we arranged this. It's like a nice opportunity to go on the live stream.
I've been on a few times now, so it's always nice to come back.
So I don't have a slide introducing myself, so I'll just say a quick sentence just about who I am.
So I'm Connor. I'm a PhD student in biology at the Mott Plunk Institute for Animal Behavior in Constance in Germany.
And most of my work is about applying active inference and kind of the Bayesian lens on cognitive science and complex systems, applying that to collective behavior, like collective animal behavior.
But today I'll be talking about kind of one of my side PhD projects, which has been developing this PyMDP package, which was very much a collaborative group effort with a bunch of people from the active inference community.
So yeah, let's dive in. So the package has actually been out for a while, and the paper came out earlier this year.
And I want to begin by emphasizing that PyMDP is very much a work in progress.
Even though we released a paper and it's definitely like a usable standalone package, there's always a ton to keep developing.
And towards the end of the presentation, I'll talk about some of those ongoing developments, which are really exciting in my opinion, and we'll really open up the usage and extendability of the package.
So basically, I mean, this is an active inference institute podcast or live stream, so I don't have to spend too much time motivating.
I don't think active inference, but in short, the PyMDP package is a Python package for simulating and running active inference processes in discrete state spaces.
And the discrete state space case is a very well studied case and very well characterized and it's been very popular in like models of decision making, like discrete decision making and planning.
And has seen a lot of application in the neurosciences as models of like discrete decision making behavior in, for instance, humans or other animals.
So just I'll give a little outline of the presentation.
So first I'll just introduce the team of people who have worked on PyMDP and were authors on the paper, but the actual effective team goes much larger than that because there's a lot of people who are developing or using it and contributing in their own ways that weren't actually coauthors on the paper.
And then I'll discuss the motivation for the package.
So a brief overview of active inference in discrete state spaces, but as I mentioned, I think this is a really nice, like, venue to have this discussion about PyMDP because I don't think I need to go too deep into discussing what active inference is.
So that'll save us some time to like get more into the depth of PyMDP.
And then we'll talk about the existing approaches to simulating active inference agents, like namely in MATLAB using SPM and just kind of compare and contrast PyMDP with SPM and help us understand like kind of where PyMDP is coming from in terms of its origins in SPM really.
And then we'll talk about some of the features of PyMDP and its general package structure.
And then we'll dive into a few usage examples where I'll show some like outputs of simulated agent behavior and then the code that accompanies that just to kind of demonstrate what the general flow of PyMDP looks like.
And then at the end, I'll just discuss some of the future directions and ongoing active branches, I guess, of PyMDP.
There are ongoing development efforts that I think will make PyMDP super exciting and extendable to all kinds of new use cases.
So I'm really excited about that.
And as Daniel was saying, if at any point someone wants to butt in or has questions for clarification, just feel free to let me know and we can kind of dwell on some points for longer.
Okay, so I'll start by introducing the team.
So the co-authors in the paper in addition to me were Barron, Millage, Daphne, Demakas, who is here today, Brennan Klein, Carl Friston, Ian Cousin, and Alexander Chance.
So Carl and Ian are my co-PhD supervisors.
And Carl is the original progenitor of the MDP or Discrete State Space Formulation of Active Inference in MATLAB.
So it's nice to kind of have his stamp of approval on our work here and his kind of co-sign on that.
And then just, I think it's important to emphasize how critical everyone here is to the package and it wasn't really just, I did do, I guess, most of the actual software development.
But the early stages of PyMDP were really conversations between me, Brennan, and Alec back in 2019 or maybe even earlier, just about the need for a Python package that does active inference.
And I think probably other people were having similar conversations around that time.
But it was because we all kind of came together that we're able to build this thing in not too long of a time.
I mean, more than two years, and it could have probably been done faster if I was working full time on it.
But it was really fun to kind of watch the progression of this.
And then as Daphne said, she's actually one of the first people who really used active PyMDP in her own master's thesis.
And not only that, but in a very ambitious application, which is like multi-agent collective behavior, having a bunch of PyMDP agents interacting with each other to simulate the kind of opinion dynamics and echo chambers and social networks, which is really exciting.
So that was really gratifying to work with her on that.
And yeah, so it's really nice also that Daphne is part of this because it's like a very good example of a pioneer in the PyMDP active inference community.
And then Barron also, me, Alec and Barron did a lot of work on developing some of the more sophisticated message passing techniques in active inference in PyMDP.
And Barron was also really critical in helping me write the paper.
And he's just great at writing and conceptualization and just was also used actually PyMDP and some of his own work on successor representations and active inference, which is kind of cool.
Okay, so motivation.
One of the biggest things which everyone here is I'm well aware of is there's just much more popularity and interest in active inference these days in the past 10 years and especially the last five years.
So it's kind of obvious that we need some more general user friendly frameworks for actually letting people learn about active inference.
First of all, from a pedagogical perspective as well as applying it in their own research or industrial applications or whatever they want to actually do with active inference.
And there's a lot, especially of interest in active inference from certain communities, not just neuroscience anymore, but things like machine learning, data science, engineering, network science, even software development, software engineers.
And a lot of those interested fields, the most dominant language is Python.
So when a lot of times what I've heard is when people come to learn about active inference and they're have a background in Python or R or something and they figure out that it's all in MATLAB they have a there's kind of a barrier to entry, because they either don't know MATLAB, or they have a hard time parsing MATLAB code,
especially if they're from like non array programming frameworks like maybe they're a front end web developer who uses JavaScript or something and they're not actually familiar with like big multi dimensional array programming.
So that was another motivation for making specifically a Python package that does active inference.
And then finally, we, because it's in Python, that means that we're now creating an ecosystem that can talk to other ecosystems.
So ideally, PyMDP isn't going to be just used in a script that only uses PyMDP.
It's going to be used with other software packages, some of them having to do with artificial intelligence or network science or all kinds of frameworks that are relevant.
And so you can now kind of plug and play with PyMDP agents and put them in environments like open AI gym for reinforcement learning, for instance.
And, and now you can work with active inference in diverse applications.
And that's really great for Python because Python by design and by its community driven development just has so many different packages that were built very well for some specific thing.
So now that it's in Python, you can kind of use it with in tandem with all those other Python packages.
Okay, so now a brief intro to active inference.
So the fundamental paradigm of active inference is that you consider an agent embedded in its environment.
And unlike kind of traditional, more passive approaches to perception and behavior where you kind of consider the environment gives you information, you do some sensory motor transformation and then you perform an action.
Active inference very much emphasizes the fact that inference or the problem of dealing with uncertainty characterizes both perception or what we call like state estimation or belief updating, as well as action, which is where the active inference part comes into play.
So agents are not only updating their beliefs about the states of the world, the hidden states out there, by minimizing this bound on surprise called free energy.
And that's where these kind of Hamholtzian ideas of perception as inference come from.
But also you're minimizing surprise or a bound on surprise also to choose your actions.
So inferring actions becomes just another sort of inference problem.
So policies or sequences of actions are considered latent variables or hidden states, and then you also do inference about those.
And by casting both sides of the perception action coin as an example of surprise minimization, what you events are agents that kind of display purposeful and curious behavior.
And there's very like there's nothing about active inference that means you have to use discrete state spaces or pi and EP. That's just one particular type of or class of generative models for active inference.
But the the palm DP discrete state space generative models are really easy to work with with active entrance because a lot of these quantities that I'm showing here are very easy to compute when you're dealing with palm DPs or partially observed markup decision processes.
So we'll get into all the mathematics and in a little bit, but that's just a basic paradigm of active inference agent and environment trying to minimize surprise, both doing perception and action by minimizing surprise.
And just for a more in-depth mathematical review of active inference in discrete state spaces, namely using these partially observed markup decision processes, I would recommend reading this paper, which is excellent in the Journal of Medical Psychology by Lance, Thomas,
Nor Sebastian, Victorita and Carl. It's just a really great description from a formal basis of how we get to the update equations for active inference starting from the most like formal treatment of categorical and Dirichlet distributions.
And also the time DP paper, the version we have on archive also has a bunch of appendices that do a lot of the similar sort of math. So I would also refer people to that.
Okay, so now let's get into the generative models that form the bread and butter of the agent's brains in prime DP, essentially. So central to active inference is writing out down a generative model, which is just a specification of how an agent believes its world works.
It's environment. How does it believe its environment influences itself, like the dynamics of the world progress and how does that those hidden state dynamics also give rise to observations. That's all encoded in what we call a generative model, or a world model, some people also call.
So in pine DP, we only deal with a very specific sort of generative model, which are called these partially observed Markov decision processes. So these are a classic model of sequential decision making and planning under uncertainty. They're not unique to active inference people use palm DP models for classical reinforcement
learning and all kinds of decision making problems. They're called Markovian Markov decision process because the state at the current time only depends on the action in the state at the previous time. So that's the definition of a Markov process, they kind of have this shallow temporal dependence.
That for instance, a non Markovian process doesn't have like a process that has longer term or deeper temporal dependencies that stretch further back in the past.
Palm DPs are often but not always formulated discrete state space and discrete time. There's nothing about the word palm DP that means they have to be these multinomial or categorical distributions. But just when we talk about palm DPs and active inference, we're almost always talking about these discrete ones.
That have to do with basically, you can only be in one of K discrete states at a time and you only progress to one of K discrete states at the next time. So everything is discrete, but there's nothing intrinsically about these Markov decision process that has to be discrete. I think it's just worth mentioning that because that's an important kind of conflation that people often make when they see the word palm DP.
And the reason we decided to use this discrete palm DP generative model for active inference is not just because of the applications to sequential decision making and planning.
But also there's just a massive pre existing active inference literature since 2010 2011 on using palm DPs as generative models for decision making tests. So all the mathematics for doing active inference with these models is already done.
So we didn't have to invent any new math or theory to actually code this up because a lot of it has already been written in in papers for like 10 years. So that made our life easier when developing it.
So now let's dive into just the main components of these palm DPs.
I'm only listing the four major ones here, but there's other components that we can discuss if anyone's interested.
But essentially this this line at the top is a description of the generative model in terms of a joint distribution over hidden states s and observations o into the future.
So the because of the Markovian nature of this generative model, you can write the joint distribution with this factorized basically product of priors and likelihoods that factorizes across time.
So that's why there are those products over time in the top. But it doesn't matter understanding the math right now.
But the main components which will map on to a schematic in a second are the agents beliefs about how the hidden states cause observations, which we encode in something called the observation model or the likelihood mapping, often called the a matrix or the a array.
So this is a probabilistic representation of how do hidden states at the current time affect or give rise to observations at the current time.
Second, we have the transition or dynamics model, which is another sort of likelihood that encodes the agents beliefs about how hidden states at one time relate to hidden states at the next time.
So this maps. This is what the agent uses to make forward predictions about how will the world evolve if this happens or if that happens, as well as to carry kind of messages or empirical priors from the past, given where I was yesterday, where must I be now, given my beliefs about how the world evolves sometimes.
That's all encoded in the B array or the B matrix. And then these final two are kind of priors. The first of which is very important, which is called the C array or the C vector, which encodes the agents prior beliefs about what observations it's likely to encounter.
And as we said in the beginning, active inference is all about casting both action and perception as an inference problem.
So active inference as a framework kind of turns the classic paradigm of reward functions and reinforcement learning on its head by saying, instead of a reward function, just equip the agent with a kind of optimistic belief that in the future, I will see this kind of data.
And then by performing inference with respect to a generative model that has that prior belief in it, the agent will look like it's searching for observations that conform with its priors.
So this goes in line with this idea that active inference is a process of self-fulfilling prophecy. The agent believes I'm more or less likely to see certain observations.
And then by doing inference about policies with such a bias to generative model, the agent will kind of bring itself to actually realize its prior preferences or these prior beliefs about observations.
So that's all encoded in the C vector. And the C vector you can basically think of as the Bayesian translation of the reward function.
And you can actually exactly relate the entries of the C vector to rewards and reinforcement learning, but we won't, we don't have to get into that now, but I can share papers if anyone's interested.
And then finally, there's the prior over hidden states. And this is simply the agent's belief about what is the prior likelihood of each hidden state at the first time step of the simulation.
So this isn't really a necessary thing in active inference, but oftentimes when we're simulating agent, we use a finite temporal horizon with a start time and an end time.
So if you have a, like a finite temporal horizon, it means that you have to basically plug in a prior belief about what the world looks like at time step one.
And that prior belief is encoded in this D vector.
And we can just intuitively sketch out the POMDP generative model as a Bayesian graph where nodes are connected by edges by these arrows if they depend on each other.
So these red nodes represent the hidden states transitioning to each other over time.
And the blue nodes, the O nodes represent the agent's beliefs about how observations are generated from those hidden states.
So that's just a graphical representation of the POMDP.
And the A matrix encodes, as we said, the agent's beliefs about how those red nodes give rise to the blue nodes at any time.
So usually we assume that this A matrix is time invariant.
So they believe that this observation mapping doesn't itself depend on time.
That's at least that the assumption made in most POMDPs.
And then there's also the agent's time invariant beliefs about how the transition dynamics of the world bring states at time t minus one to states at time t.
And then policy selection or action comes into play in inactive inference by framing actions as controlled transitions.
So the B matrix does not just say how does it how should the world look now given how it was yesterday, but how should it look now given how it was yesterday and the fact that I took this action at time t minus one.
So the B matrix is not only conditioned on past states, but also on previous actions.
So policies pie come into play as a belief or distribution over those actions.
So the actions directly influence the transitions and then the transitions affect the states.
So that's how kind of we think about action in a lot of POMDP scenarios.
And actions are or policies pie or sequences of actions or collections of actions.
And then we have this critical objective function called the expected free energy that determines which actions are more likely than others.
So by minimizing expected free energy, we optimize a belief about policies.
And then the expected free energy itself is a function of your generative model, your beliefs about the world, which includes this biased prior belief about what observations you expect yourself to see.
So the kind of reward function enters policy selection via this expected free energy.
And that's why we're able to still kind of call all of this a form of an inference problem because we're minimizing this expected bound on surprise.
So and in doing so, we kind of infer distribution over policies, which we then sample from to actually generate actions and change the world.
And then as I said, the D vector is basically just a prior on that initial hidden state distribution.
So yeah, in summary, to build a POMDP active inference model, you have to write down or encode these A, B, Cs and Ds.
And there's some other priors too that we can talk about like a prior over policies, which is called an E vector.
But for now, you can just think of most of the heavy lifting in active inference as consisting in writing these things down actually encoding what your agent believes about the world that it exists in.
And these things in the case of these categorical discrete distributions, they end up looking like matrices and vectors.
Since everything is categorical distributions, you're not dealing with continuous infinite dimensional spaces.
You're dealing with things that have a discrete number of entries like a four by four matrix or five by five matrix.
Yeah, I think before we, yeah, so the next part I'm going to talk about like the MATLAB Python dialectic.
Before we move on to that, though, should we maybe pause if there's any questions?
Yeah, great. Thank you. Daphne or Yaka, do you want to provide any thoughts or reflections?
There's one thing that I always been a little bit confused about which might be nice to resolve now, which is in the parameterization of the prior preferences.
Can you explain like why exactly it is that when you initialize them, you just do them with like integers greater than one greater than or equal to one.
But then you like softmax them later on when you actually use them in inference, like why do we not define our prior preferences as probabilities?
Yeah, that's a good point and a good thing to mention.
So usually in both the MATLAB and the Python implementation, so when building PyMDP, we made the decision to do the same thing.
When you're encoding that C vector, instead of encoding it as probabilities, which it ultimately should be, what you really write down is the log of the C vector.
So you write it down in terms of relative log probabilities.
And the reason that's potentially a more, more intuitive parameterization rather than writing it down directly in terms of probabilities is because the log of the probability is more like the actual reward.
So if we go back to this generative model, the expected free energy being a free energy is actually and then a free energy is sort of a kind of KL divergence.
It's encoded in terms of bits, which is naturally in a logarithmic space.
So if you're writing down like a prior in terms of in log space, you know, kind of if I change the number of log units in my prior preferences, I have it, it's more linearly related to the change in the expected free energy for a policy that leads to that observation.
Whereas if you're writing down the priors in terms of probability space, that the resulting change in the expected free energy from changing something in probability space is not going to be linear.
It's going to be a nonlinear change.
So if we think of rewards as log probabilities, something that's like one extra log unit or one natural log, one net or one bit more valuable than another thing, that will actually reflect itself in terms of the expected free energy difference between seeing thing one versus thing two.
Whereas if we encode those two things in terms of probabilities, that expected free energy difference won't be as intuitive simply because it won't be linear.
So that's like a quick reason for why we decide to encode things in log space.
But there's nothing mathematically necessary by any means you could easily just write the priors directly in terms of probabilities.
It's more of a, it's like a, it's just a way to make it more of a word like really.
Yeah, that makes a lot of sense.
Thanks.
Thanks, Jacob.
Yeah, I'm wondering is there a hard requirement on the and the matrices or tensors to be actually encoded as categorical matrices or because thinking about like the function of the matrix and the way the agent uses it to infer its hidden state from an observation.
I'm thinking whether it would be possible to encode high dimensional state spaces in something like an neural network that can approximate these probabilistic representations as well and learn the kind of the relationship between observations and hidden states.
Yeah, definitely.
Sorry.
There might be a, there might be a like, but yeah, basically whether you think that would be possible and because I feel like the Python implementation, which I'm sure you'll get into the advantages of Python over Matlab also affords the interoperability with other libraries and within the science.
Totally. Yeah. That's a good, great point. So I kind of mentioned towards the beginning when we talked about POMDPs, there's nothing about POMDPs that means all these things have to be matrices.
The main thing that defines a POMDP is that it's partially observable so that the agent only gets to see the blue nodes and that it's a Markovian process so that there's like a temporal shallowness in the memory of the hidden states.
But you could replace A and B with any kind of parameterized, it has to be a proper likelihood function so it has to be have that like the properties of being a likelihood.
There's nothing that means that it can't be a multivariate Gaussian or a Cauchy distribution or Bernoulli distribution or yeah, any kind of exponential family or some like parameterized neural network.
Like that people use oftentimes in reinforcement learning like they'll parameterize a dynamics model just with a bunch of neural networks that say take time, state at time t and move them to state at time t plus one.
The reason there's difficulties with that is simply because it's unclear how to compute things like the expected free energy once you start moving into these not as nicely behaved distributions.
So a lot of the reason that this framework was developed in the discrete categorical state version is not only because for modeling low dimensional tasks like someone playing a slot machine or deciding to go left or right in a Y maze.
It's easy to use these discrete distributions to kind of give a good description of that behavior. That's one thing, but also is literally a mathematical tractability reason.
There aren't always closed form solutions for the expected free energy depending on what those those distributions look like.
So there's some work on doing this. For instance, Magnus Kudall has a paper in entropy where they compute the expected free energy in linear dynamical systems basically where all these A's and B's are represented by Gaussians.
So they call this like a linear controllable dynamical system.
And they also have to make some assumptions about how the actions affect the B matrix, which really isn't a matrix anymore. It's more like a continuous Gaussian.
And they find that the normal terms you get with the expected free energy like the information gain.
Those a lot of the interesting things that we get with when we use categorical distributions, those terms actually disappear when we use Gaussian.
So you actually don't get the nice like information gain seeking or information seeking terms that you would normally get by using categoricals.
But there's other ways to get around that like if you use neural networks, you can still use things like sampling approaches to compute expected free energies.
Where you basically like sample a bunch of possible trajectories and then you can use the samples like kind of Monte Carlo style to compute expected free energies.
And a lot of the groups that have done like scaling active inference to deep neural networks, they've used that kind of approach.
