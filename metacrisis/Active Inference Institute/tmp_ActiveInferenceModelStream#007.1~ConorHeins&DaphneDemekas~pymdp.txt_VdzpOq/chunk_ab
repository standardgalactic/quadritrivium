So like Alec Chance's paper scouting scaling active inference does stuff like that.
A lot of the stuff out of like Tim Verbalin's group.
I don't want to leave anyone.
I think like in nor has done obviously a lot of nor sagittas done a lot of active inference with deep neural networks.
As a virus found us like a lot of these people were actually trying to apply active inference to deep neural nets.
They have to basically come up with ways to compute that expected free energy that are different than the way it's done in pine VP.
Because in pine VP, you can exactly compute that expected free energy, which ends up just being a bunch of matrix vector products and then a summation.
So it just becomes more difficult when you use more complex distributions, but it's by no means impossible.
You just have to you have to come up with some kind of approximation.
But there are ways I think you can get around that, which involve what people have been calling hybrid models where you might have deep neural networks.
You might have to kind of feed into a POMDP at like some higher layer.
And then in POMDP space, like a discrete categorical space, you can still compute expected free energies and free energies.
But you can still take advantage of at lower levels, like high dimensional neural networks to project your data into this low dimensional space.
So you do all the active inference in this lower dimensional POMDP space where everything's exact.
But you can still take advantage of the nice dimensionality reduction and feature extraction properties of neural networks to first like pre process the observations.
So that's something I'll talk about at the end, because we've made some progress on the first steps towards making that possible, which involves basically making all of pine VP auto differentiable so you can like train neural networks that are attached to POMDP models.
Yeah, it's a great question.
Awesome. I see this as like the body plan of generative models.
And maybe what's a leg today is an antenna tomorrow or like gets longer or thicker.
There's a lot of different ways to swap out and compose different parts of the model.
So thanks carry on.
Cool. Yeah.
Cool. Yeah.
So yeah, now let's talk a little bit about the traditional software for active inference research, which I'm sure everyone on the call right now is familiar with the original way it was done was basically Carl Friston and a few others wrote a bunch of MATLAB scripts part of the SPM package.
Which is originally developed for like neuro imaging data analysis and statistical testing.
And there's a sub package called DEM or dynamical expectation maximization, which is used for not only active inference, including continuous active inference, but also just like generalized filtering and and fitting nonlinear state space models from empirical data.
So within that we have all the this kind of discrete active inference toolbox.
Most of the functions of which are prefixed by SPM underscore empty MDP.
So the main function that basically does everything that pine VP does for the most part is is containing one function called SPM MDP VBX.
And kind of in a tongue in cheek way, we often joked around that pine DP is just a package that implements that one function because that one function essentially does active inference and learning.
And all the message passing just in in one call, which in many ways is very nice because you just have to pass in a pine MDP to it.
And then it in a kind of black box way gives you all the belief updating and history of actions and everything that that you need.
They are despite its elegance and it's like usefulness and how robust it actually is because you can pass in any agent with any generative model and will pretty much work.
The issue is because it's just a single function, it's not very modular.
So it's very hard to flexibly compose different sub computations of an active inference process.
So if you want to do some bespoke active inference application where like, oh, before I do hidden state inference, I want to update the parameters of the matrix and I want to do it in this particular way.
It's very hard to do that in SPM MDP VBX.
So what often I see happening and I did this myself in my master's research is I end up having like five different versions of this function that have some little like specialist prefix for the specific thing that I was doing.
And then that just ends up being like kind of not efficient because you're creating a bunch of boilerplate code and you have one function that does the particular little version of active inference that you want to explore.
So just by modularizing this one function, even within MATLAB, you could do this.
You would basically make the thing a lot more flexible and save a lot of copying and pasting of code.
Another issue is that inference and policy selection are fixed so you can't kind of compare and contrast different message passing approaches and action selection routine.
So there's just one way it's done in SPM MDP VBX, which is called marginal message passing.
And so it's hard to like compare that.
Like what if you change the message passing algorithm?
Whereas in SPM MDP, we have it right now to message passing algorithms that you can kind of side by side compare.
I mean, we hope to add more in the future, which is very easy because it's just a modular thing.
And then because of what we were talking about in the beginning with it, just by virtue of this being in MATLAB, it's harder to synthesize with other frameworks like in reinforcement learning, open AI, gym.
And deep neural networks like in TensorFlow or PyTorch or Jax.
So just the fact that it's in MATLAB already limits it.
Of course, people have found ways to do like cross language, cross platform code, like moving from Julia to MATLAB to Python and back.
But it's just a lot more heavy lifting involved in that.
And then I'll just lay out some kind of pros and cons of MATLAB and Python.
So one of the things I do like about MATLAB is it's very easy to get started with array programming, has a nice editor.
There's a lot of use in history and computational neuroscience.
Like even when I gave this tutorial at a course on computational psychiatry this past year, I think the majority of the tutorials were still using MATLAB.
So there's still a good reason to use MATLAB just because of the history and the number of packages that are well suited for doing neuroscience and psychophysics stuff.
So there's that advantage, just kind of like a almost momentum or legacy advantage.
But of course, there's issues like it's proprietary.
I think that's the biggest one you need to pay one way or another to use MATLAB.
And there's not as much community driven development.
I mean, there is some like the file exchange, but it's not the same level as something like Python.
And then Python kind of can compete with MATLAB because it has array programming in the form of NumPy.
Of course, it's open source.
It's been widespread adopted across tons of fields, not just academic ones, but lots of commercial application.
And there's a lot of community driven development in the form of packages like Pi and VP.
But one disadvantage is for me, at least it wasn't as easy to get started with MATLAB as it was with MATLAB
because you often have to like install a bunch of things and learn about virtual environments.
And you have to like learn about a lot of more programming stuff before getting started with using Python.
So I think that is one reason that MATLAB is actually still useful as a pedagogical tool.
Like the time between getting MATLAB and actually doing programming is pretty short, which is nice.
So yeah, the package is called within this in for actively GitHub organization is called Pi and VP.
And you can just pip install it like in a virtual environment or just in your base installation of Python.
And then once you have a Pi and VP installed, you can kind of import and use in a flexible modular way,
all the different sub packages or sub modules like the agent module, which basically just implements the agent class.
And then there's different modules like inference and control and learning that are independent of actually an active inference agent.
And you can just use them to do message passing for hidden state inference or to compute expected free energies on possible policies
or to compute updates to parameters.
So all those things can be composed and flexibly kind of, you can make kind of Frankenstein active inference agents
by composing all these things in the bespoke way that you desire.
And a lot of it, the main workflow of Pi and VP comes down to specifying the generative model in the forms of these discrete arrays
and then plugging them into the brain of the agent and instantiating an agent.
So that is pretty much encapsulated by these two lines at the bottom.
Import the agent from Pi and VP, and then you just build one by plugging in the ABCs and these.
And then you have this agent object that you can use to do hidden state inference through methods like infer states,
infer policies, which is the agent internally computes the expected free energy of its policies.
And then you finally can sample actions.
So these three lines are the kind of the main players of any active inference loop, state inference, policy inference,
and then action selection.
And you just kind of tie those in a loop over time to instantiate an active inference process.
And the kind of the circularity of the action perception loop comes into play with the inputs to the agent,
which are its observations and the outputs of its action selection, which are its actions.
And of course, you need to use the last action to then get a new observation and that you do by plugging the action into some environment.
This is also in the active inference literature often referred to as the generative process.
So the actual world out there that generates your data.
So this is the kind of classic action perception loop you'll see in any active inference agent.
And this is also not even unique to active inference.
This is just how reinforcement learning problems are generically framed.
So like open IGM uses a very similar control flow or kind of like, yeah, sensory motor loop.
So here's an example of just doing that, like import the agent, set up the generative model, and that's the hardest part.
So I kind of conveniently put ellipses after the dot dot dot, but that's actually where most of the code is going to happen is building the generative model.
You build the agent, you build some environment, which you can either import as one of the stored piloted P environments, or you could get it from open AI gym, or you could just create your own environment.
So this would be your code that actually describes how the world works out there, the world that the agent is interacting with.
And then you can implement a time step of active inference with those few lines of code right there.
And that would all be kind of those last lines like eight through 15, those would kind of be wrapped within a loop over time.
Yeah, this is just more examples of like this would be a quick way to in this example, we're not even running active inference, but we're just using one of the message passing algorithms from the Algos sub module to do hidden state inference.
So in this case, I just created a random a matrix, I created a random observation, and I gave, and I made up a random prior and then I can just do one like fictive update of what an agent might be doing during hidden state inference and optimize q s, or the beliefs about
hidden states. So this is an example of how you could actually use time to be to just do generic inference on hidden Markov models, you don't even need to use it within an active inference loop, you could just use it to do like statistical inference on a hidden Markov model with
the particular algorithms that we've provided.
And then of course, one of the advantages I talked about over the SPM is that you can create customized active inference processes.
There's a lot of like extra arguments to the agent class where you can kind of turn on and off different parts of the reward function or the expected free energy for an agent.
So for instance, in this agent, the agent does not incorporate expected utility, which is the kind of reward C vector driven component of action selection.
But the agent does use state info gain and parameter information game which are two other components of the expected free energy.
So there's a lot of ways that you can kind of create a bespoke active inference agent that doesn't that will behave in different ways depending on these kind of keyword arguments that you provide.
Yeah, here are just more examples like in this case, we're using active inference agent just to do hidden state inference and no actions at all.
So the agent is just inferring hidden states.
It's updating its beliefs about the a matrix and it's updating its beliefs about the D vector or the initial hidden state.
And so you can, you know, just flexibly put together all these lines and create an agent that does whatever you want doesn't even need to be acting in the world technically.
Okay, so now I'm going to show a few examples of time to pee in action.
So this is one that's a classic active inference paper.
It's one of my favorite papers.
It was, I think it's called scene construction and active inference words describing a task where an agent has to, and this was used to actually model human data in a psychophysicist.
An agent has to gaze contingently uncover two out of four quadrants that that have particular images in them.
And there's four quadrants total and two of them have the images of interest in them.
And if the two images in this case are a bird and a cat image, and that's an example of a flee scene.
So the agent basically or the human has to categorize the latent scene, which is simply defined by the combination of two images, you know, and then categorize the scene.
So it's basically a categorization categorization task, but the agent needs to gaze contingently uncover a sequence of cues before it knows what that category is or what that scene is.
So it combines the epistemic components that we all know and love about active inference trying to uncover the hidden state of the world by actively sampling it.
So in this case, they're sampling the world by moving their eyes to different quadrants to uncover what's what's behind them.
And then actually choosing what the true category is based on what it learned about the world.
And that's where it's trying to maximize utility because there's some reward associated with categorizing correctly.
So this is an example of that which was originally done in MATLAB and I just re-implemented in PyMVP.
And this is another example where now the scene is the feed scene.
So the cues are in the lower two quadrants in this example.
So the agent has to look around the different quadrants.
It finally sees that there's a bird in the lower right, seeds in the lower left.
So this must be the feed scene.
And just to show an example of what does that actually look like in active inference?
Like what does the code look like for that?
So the first thing you would do for this one is you would set up your agent, which again, you just throw in the A, B, Cs and Ds.
In this case, I used a particular message passing algorithm called marginal message passing, which is the same one used in MATLAB.
You set up a policy depth and an inference horizon, which is kind of like a memory, like how much of the past observations you take into account.
And then you set up a environment.
This is like the external world that the agent will be interacting with.
In this case, I'm calling it the scene construction environment, which just tells me once the agent moves its eyes to a certain place,
how does that action then determine what the agent sees next, which will be, you know, whatever is behind the quadrant that it decided to look at.
So those are the main two things.
Those are the two sides of the action perception loop, the agent and the environment.
And then what it's often common to do is to get in an initial observation by resetting the environment, which is a convention barred from OpenAI Gym.
You basically do environment.reset.
It's like a method of the environment that spits out the initial observations.
And then it's often useful in these things to create like lists or dictionaries that have a mapping between the observation indices, which are like integers between zero and however many observations there are.
And then what those actually correspond to semantically.
So this is just a very common way to like, because all the POMDP and the environment will be spitting out are like ones and twos and zeros and all these like discrete indices.
But it's useful to have these lists that you can use to kind of semantically map particular indices to things that are meaningful, like seeing the bird image or choosing the category one versus category two.
And then once you've done that, you just write a loop over time where you're basically performing active inference, which consists in hidden state estimation and policy inference.
You sample an action, which then gets fed back into the environment to produce another observation and then that happens over time.
So that's the whole action perception.
So this is, yeah, so that's, it's deceptively simple how like short it looks, but I'm kind of glossing over something that I'll talk about later, which is I mentioned earlier, which is as easy as this looks the hardest part is actually done way before any of this happens,
which is writing down the ABCs and Ds. That's by far the most time intensive and like complex part of active inference is actually writing down the generative model.
Once you have the generative model written down, then the rest basically is like clockwork.
You just have to link the agent to the environment and then just run like five or six lines just to actually implement the thing.
But the hardest part is writing down those ABCs and Ds in the beginning.
I'll show another example, which I kind of call teammates on steroids.
So in the classic teammates task that I forgot what the original paper was, but it's something that's been very popular in the active inference literature for a while.
That I think Carl came up with maybe in 2015 or earlier even is you have an agent, a mouse that has to visit two potential sources of either reward or punishment in its environment, and it doesn't know which arm of this teammates contains the reward.
So it has to visit a queue first before it knows which arm has the reward and which one has either no reward or like a shock, like a negative stimulus.
So I just kind of spatially extended the teammates so that agent has to now visit a sequence of queues, each of which reveals the location of the next queue in order to figure out the final queue, which is just the location of the cheese versus the shock.
So this is another example where the agent first goes to queue one, then it knows where queue two is and then it knows where the cheese is.
So I call this epistemic chaining because the agent doesn't actually have to plan its route all the way to the final location of the cheese.
All it has to do is get to the next queue, which then reveals where the next queue is, which finally reveals where the hidden location of the reward is.
So you're kind of using epistemic value or curiosity to allow an otherwise temporally shallow animal to plan its way to a distal reward or something that it can't plan to get to a priori.
And again, this is just an example of what that would look like in time to be where it basically looks exactly the same. It's just that the environment and the generative model are different.
But the general flow of the code is always has this this kind of classic pipeline.
Okay, so now I'll get to the kind of the most important part. And I think the biggest source of confusion with active inference is that the hardest part is the generative model.
All the complexity comes into encoding the agent's beliefs about the world. So how do I write down the A, Bs and Cs and Bs?
In in paradigms like deep neural networks or like unsupervised learning, you don't have to write down the model, the neural network learns the model by just observing loads and loads of data.
So it's less sample efficient, but you don't have to encode as much to begin with. So this is kind of the kind of coincides with a larger divide between model three and model based approaches with deep neural networks.
You're effectively issuing the sample, the statistical complexity of having to write down the model by just sticking together a bunch of nonlinear function approximators and then just learning the beliefs that the agent has about the world by just bombarding it with data.
The same thing goes for deep reinforcement learning like deep Q learning and active inference. The agents are much more sample efficient in the sense that they don't need to train on like billions of data vectors.
But on the other hand, there's more investment on your end as the modeler because you have to write down explicitly what the agents beliefs about the world are.
You don't just equip it with something as generic as a convolutional layer and some values and stuff and then let it learn. You actually have to hand code that.
So I think this is one of the biggest differences between model based reinforcement learning where you actually encode a Bayesian generative model of the world and kind of more model three or data driven approaches.
But there is it's not such a dichotomy. There are ways to kind of combine the two.
But just to kind of show that very specifically, for instance, in this scene construction demo that I showed like a few slides ago, if you just look in terms of sheer lines of code, which one took more code.
You can kind of use the amount of lines of code as a proxy for statistical complexity or how much information is contained.
So the simulation itself running the active inference loop was like 15 lines of code just like and that code itself is already very generic and not specific to the scene construction demo.
Writing the generative model itself. That's where all the heavy lifting is done. That's where all the information that's specific to that task gets encoded.
So for instance, I just look at how I created the a matrix, the beliefs about the observation mapping for the scene construction demo that already is like way more code than just running the entire active inference simulation.
So just like by the sheer amount of code, you can already tell. Oh, yeah, there's a lot of assumptions and an information that's being baked into the generative model and that's where most of the heavy lifting of active inference actually comes from.
Yeah, so I just think it's important to remark on that because that's like a really key thing that I think anyone who wants to start working with active inference models and in discrete state spaces should kind of wrap their head around is that the model does most of the work for you.
The expected free energy. Yes, is a very interesting objective function that has many advantages, but most of the power of active inference comes into writing down what your agents beliefs about the world are.
And then once you have that, then all of the rest kind of just does does the work for you because the pine TV code is very generic.
What's not generic is how you encode the beliefs about the world.
Okay, so now I'm kind of finishing up it maybe we should dwell just on that bit for a second that does anyone have thoughts or comments or questions about this.
If not, it's I can just proceed and finish off.
Daphne or yuck up or I'll ask one.
I don't have any questions.
All right, well you've emphasized the specification of the agent generative model.
And how about the other side of the coin. How do we specify a generative process. How do we specify the environment for the agents.
