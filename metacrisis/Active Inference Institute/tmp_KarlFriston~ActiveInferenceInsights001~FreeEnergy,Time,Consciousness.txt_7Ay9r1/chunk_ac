unless we're pan-psychists, but we won't go just down that route just yet.
That's where you were taking us, which is a fun place to be. How does one elude pan-psychism in
the free energy principle? Well, I think you've just answered your own question there,
perhaps we'll just unpack that a little bit. What's the difference between a stone
and a thermostat and me? I think what you're now asking is, are there,
what I will call natural kinds, but I wasn't allowed to because that has a lot of philosophical
baggage, but for the purpose of this conversation I'll just say, are there natural kinds of things
that display different kinds of behavior that can be equipped with different kinds of
teleology, and I would say absolutely, and you've just rehearsed some of the key things.
So, as you noted, to be animate is a big thing, in the sense that the stone is not animate,
in the sense that it moves itself. You know, stones started wandering uphill or flying around,
that would be much more interesting, but they don't have that animacy that is characteristic
of biotic self-organization. So, what does that mean in a deflationary sense of the free energy
principle? It just means that the active states are an empty set, and usually for stones you might
also argue that the internal states are empty. We don't have to. The stone can be making sense
of its world in terms of its latent states, or acquire that interpretation. Perhaps that's a
nice example of what we were trying to drill down on earlier on in terms of, is there a generative
model? I mean, one could argue that a stone has a generative model of external milieu in terms of
the temperature, and that there's a latent variable, which is a hidden state of a latent state, you know,
as well known in statistical thermodynamics, which is a sufficient system called the distribution
of things. And the temperature could well be inferred by the internal, the interior of a stone,
and therefore it is making sense of the pattern of its sensory exchanges with the world on the
surface of the stone, simply because it's warming and cooling, and therefore tracking the ambient
environmental temperature. So, that's perfectly free energy principle consistent. It's a very
boring kind of sense making, but, you know, and it is that boring kind of sense making that I had
in mind when talking about, if you preclude the active part of active inference, and just think
about sort of perceptual inference, then you're sort of missing the point, really. But I think a
stone is a good example of that. Now, would you say the stone has a generative model? You could
argue that, but, you know, it's you trying to explain the behavior of a stone, but the stone
doesn't have very much behavior. But you could start to think, well, now if you test a hypothesis,
it is actually registering the temperature on the inside. And again, just to rehearse the argument,
the fundamental argument we were talking about before, you will never know, though, until you
break the stone, because once you break the stone, it's no longer a stone, it's just a broken stone.
So you're exactly the same problems or issues confront you, even with the stone. But you're
focusing on that sort of big move from things with and without active states. I think then you're
talking about things that are animate, things that have animacy and active on the world.
Is that sufficient to get the kind of behavior that we've been talking about, which is, you know,
the kernel of active inference in terms of what active inference brings to the table in terms of
information seeking and having a variety of different affordances, particularly epistemic
affordances, as an explanation for behavior. You could argue, no, a thermostat can act upon the
world. It can switch heating elements on or off and it senses things by its thermo receptors.
You could say like very much like a Watts governor. It now has active states, it has inputs and
outputs, and it has internal dynamics. And in one sense, the Watts governor is a good regulator
and therefore conforms to the cybernetic view of a gerative model. And it's acting in the right
kind of way. So does this kind of system have the curiosity that would be associated with
information seeking and resolving uncertainty? And you would argue, well, no, it doesn't.
So now you've got another kind of natural kind, which does. So what's the difference between
a Watts governor or a thermostat or possibly a virus and me? Well, I think the key difference is
a particular kind of self modeling that just comes from being very big and a bit structurally
complicated in the sense that there are certain parts inside my body and inside my brain that are
so distant from the active states that are actually moving my arms, my actuators or are
engaging my autonomic reflexes that I cannot see them directly. And if that is the case,
the only way that I now can sense the consequences of my action is to carelessly through the sensory
states. Now this introduces a really interesting distinction between me and a virus. It means
that I now will start to treat my own action as a hidden or latent cause of my sensations.
And that brings to the table. Okay, well, if now my own action is being inferred as a random
variable, then now there's a separation between what I think I'm doing or what my active inference
would allow me to talk about. It tells what I'm planning to do or what I'm inferring to do
in the spirit of planning as inference and what I'm actually doing. So this brings you to an even more
a different kind of thing, which would be something like me, that now has beliefs about
its own action, which is distinct from the actual real variables that constitute the active states
of the Markov blanket. And as soon as I have beliefs about my own action, I have to ask myself,
well, as a physicist, I would ask, well, what are the probability distributions over my
active states, those states that actually change the causes or the external states generating my
sensations? And when you write that down, what you get is exactly what we're talking about before,
which is this mixture of expected information gain, basically expected free energy that has
can be decomposed into this information seeking part, and the constraints afforded by your prior
the prior part of your journal model, the things that you find very surprising.
Plus it is to unpack that intuitively. Let's take the predictive processing surprise minimization
view of self organization and an active inference. And that means that I now have the
the kind of creature that exists. Therefore, I must be minimizing my surprise. Furthermore,
I now have beliefs about my actions. I have prior beliefs. I must have prior beliefs about my actions.
Of course, this is all sub personal. So what kind of prize would I have about my own actions?
Well, I'm going to because I exist and I'm a free energy surprise minimizing kind of thing because
I exist, then I must choose those actions that minimize the surprise I expect following that
action. So that means I'm going to minimize my expected surprise. Now there are two ways in
which I can do that. And we've already spoken explicitly about the two ways. One way is just
to notice that the average surprise, the expected surprise is just uncertainty. So I can reduce
my expected surprise or my anticipated or my average surprise by getting to grips and knowing
what would happen if I did that with greater precision. So this is information seeking.
This is the sort of the curiosity. It's the novelty seeking part. It is the exactly the part
that underwrites the the basic principles of optimum Bayesian design, getting the data that
minimize my uncertainty because uncertainty just is expected surprise. There's another way of doing
minimizing expected surprise. If I know what is very surprising, if you know, by being
at very, very low temperatures or being very poor or being snubbed socially, everything that is,
if you like, not characteristic of me given that I exist in a particular way,
then any deviations of that basically can be thought of as surprising. So if I stray beyond
very much in the spirit of homeostasis, although now we're talking about allostasis because we're
talking about the future, then I'm going to choose those actions. Then don't just do the
information seeking responding to the epistemic affordances, but also will elude those kinds of
surprising states that are very uncharacteristic of me being very poor, missing out on that opportunity,
being embarrassed, being in pain. Although pain is probably a bad example. We're talking about
the sensations that we want to want to avoid. So that would speak to the prior cost or the
prior surprise, the negative which was the preferences you mentioned before. So you can
look at these constraints afforded by the priors, the pragmatic part, if you like, of these affordances.
Some people call them instrumental. So you've got epistemic and instrumental affordances that
just are the expected information gain and the negative expected cost, which would be the
manifestation of the constraints. You can look at the complement of constraints in terms of
where I don't go in my sensory state space as where I do go, which are my preferences,
my preferred and characteristic attracting sets. So just to remind myself and you and
people who are listening still at this stage, why are we going through all this? Well, because this
is what comes out of a consideration of beliefs, prior beliefs, about the way active state should
unfold. And that becomes very pertinent when now I have to actually embody those beliefs
in my gerative model. So suddenly now I become something that is very distinct from
a thermostat or a virus, because now I have a gerative model of the consequences of my action.
And that gerative model is very simple. It just says a priori. I will a priori think that the
action that maximizes information gain and maximizes this instrumental value
are going to be the most likely policies. And this could be construed as a high end
kind of planning as inference. It's not as trivial or simple as a sort of KL control,
because we've got this epistemic part of it, but it still has a spirit of planning of inference.
So what I'm saying quite simply is there are certain things that plan and there are certain
kinds of things that don't plan. A virus doesn't plan. The weather doesn't plan. Evolution doesn't
plan. The stone doesn't plan. And you could argue that many smaller insects don't plan.
But as you get bigger things, then they start to plan. And I think you move from non-planning to
planning at which point, by definition, you are ascribing to these bigger things, like you and
me, ascribing a teleology to our behavior that rests upon a gerative model that includes the
consequences of its own action, which crucially equips it with a temporal depth because the
consequences are in the future. So unlike the thermostat, that doesn't need to look very far
into the future. Things that plan that are so big that they lose contact with their actual actuators.
Now, look as if they have this sort of temporal depth, this temporal thickness
to their gerative models, specifically enabling them to plan their actions into the future.
So that's how I would get out of the panpsychism argument.
Okay, I've never heard that argument before about the distance from the actuators. That's very
interesting because it seems in that sense that the temporal depth is downstream on size, in a sense.
Is that a correct reading? And is there a reading of your argument that you've just laid out there
in which you can have deep temporal modeling, planning as inference, and allostasis as well as
a sense of retrospective inference, without being a large thing? Or is this notion of being
far from your actuators so that you have to start disambiguating what is your action and what is the
action of the world or other people? Is that fundamental? Yeah, I mean, this is a really
interesting notion because I've actually never heard it really be elucidated like that. So I'd
love to know whether there is a way that a virus could be doing deep temporal modeling despite
its minimal size. Right. I mean, to be honest, I don't know. But a sort of an intuitive answer
would be no size really does matter. It really does matter. So by size, I'm implicitly talking
about the sparse coupling that underwrites the conditional dependencies in mathematical
systems that give rise to Markov blankets and blankets and blankets. So I'm not literally
talking about how wide something is in terms of millimeters. I'm talking about the sort of
the hierarchical depth of conditional dependencies and being secluded behind Markov blanket after
Markov blanket after Markov blanket. So if you're comfortable associating physical size with that
hierarchical depth in terms of what a thing is in terms of having this internal hierarchical
structure internal Markov blankets, then I think the answer is very clear. No, you have to have,
you have to be sufficiently big. So I'd be extremely surprised. So for example, I haven't
said this before, but I would imagine very, very small insects can't plan, whereas
something the size of a bee might be able to plan. So I'd imagine that a Drosophila couldn't plan,
but I would imagine a bee might just be able to get there. And it starts,
first of all, that immediately speaks to a philosophical vagueness in terms of that
temporal thickness, which is interesting. I mean, is it categorical? Can I say this kind of artifact
can be explained in terms of a generative model that does not cover its own action because it has
direct access to its actuators because it's sufficiently simple and small. There are no
Markov blankets that intervene between the effectively the sense making part and the,
for example, sending predictions and the predictive processing spirit down to the
actuators. Whereas this thing is so big that there are inevitably, with probability one,
so many Markov blankets intervene, there is no now direct access to action upon the world.
And so that might be a bright line. It might be a qualitative distinction between things that do
and do not have a representation of their own actions. On the other hand, the counterargument,
I think, would be this more vaguenation that even a thermostat could be construed as having
a little glimpse of the future simply because it could be cast as, say, a PID controller, which
basically invokes now rates of change. And as soon as you evoke rates of change, you've got sort of,
you know, a mathematical image of a little trajectory into the future. So it could be a great,
it could be a great thing. I've lost, I've lost the importance of your question. I'll ask you a
question again. It was probably unimportant. But this is really, sorry, I'll go again. This is
very interesting line that I'd love to dive down into. So my interpretation of this, my reading of
this, is that in a sense, having bundles of Markov blankets embedded in Markov blankets
leads to the inevitable inference that there is something like me which is doing something and
there is something that's not like me, which is also causing sensory data that I'm receiving.
That makes sense. Now, let's tie in temporality to this. Does it, does this,
does this mean that in a sense, temporality is a corollary of selfhood in the sense that
the selfhood is almost more fundamental than the temporality because it comes from
the dimensionality of the things that are Markov blankets upon Markov blankets.
And how can we integrate temporality and temporal thickness or depth into this picture?
Right. That's absolutely right. You could tell the story in a number of different ways. If you're
talking to, for example, Chris Fields or Maxwell Ramstead, they might start talking about irreducible
Markov blankets that necessarily require the notion of nested Markov blankets and that there is some
one or more core irreducible Markov blankets that would have this aspect of looking at one's own
inference processes and acting internally. And they get the notion of mental action,
you can then start to work towards sort of qualitative experiences in terms of
self-modeling and what that might look like. The other way that you could tell this story
is to know that to invoke the renormalization group if you were a physicist and just know that
when you're talking about any hierarchical generative model, there is inevitably a
coarse graining that includes time in that depth, which means very simply that the deeper parts of
my generative model and indeed viruses are on a free energy principle view.
Encoding beliefs about things that change more and more slowly as you go deeper and deeper. So
there's a separation temporal scales. So a common sensical view of this is that the neuronal
representations down near my primary auditory cortex or indeed in the brainstem are representing very,
very fast fluctuations in pressure and frequencies and sort of scenographic-like
representations that are changing your possibly even infractions of a millisecond if you're an owl.
But certainly over 100 milliseconds that I move up to sort of your primary auditory cortex and I may
be in the realm of hundreds of milliseconds and sort of frequency lines that define phonemes and
I move up to secondary auditory cortices and then through the auditory hierarchy getting to the level
of words right up to say, you know, some parts of prefrontal cortex where we've got entire semantics
and syntax and narratives and stories start to emerge and as every time we go deeper into the
hierarchy we slow down or we extend the temporal compass thereby providing a context for the faster
influences and fluctuations at the level below. So again we come to this, you know, notion of
size entailing a certain kind of depth and this is a hierarchical depth in the generative model
that as you say necessarily goes in hand in hand with a temporal depth and a separation of temporal
scales. So I think you're absolutely right there has to be a temporal aspect to deep generative
models and by deep I just mean that there are nested Markov blankets. So, you know, to define
a hierarchy is only defined in terms of the conditional independences and implicitly the
Markov blankets of one level in the hierarchy that defines it as a level in the hierarchy without
the Markov blanket the hierarchy would not be there. So that's another example of this sort of
being open but being closed and being able to individuate something from something else
