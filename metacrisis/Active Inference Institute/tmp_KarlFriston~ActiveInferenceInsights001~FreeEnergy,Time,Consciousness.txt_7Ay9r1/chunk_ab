in mathematics that would take you down the path of information geometries and statistical manifolds
and belief updateings as movements in these geometries and try to understand the metrics
in these spaces and how that relates to uncertainty and precision. Oh, that's a lovely story.
We don't need to really pursue that at the moment. The key point being made here,
the gerative model is not physically instantiated other than in terms of some sufficient statistics
in any setting. However, in the free energy principle, the gerative model is in or of itself
never actually instantiated in terms or realized in terms of its sufficient statistics. And that's
because of something that we were talking about before, which is the dynamics. So the dynamics
is a gradient flow, which means that all we need are the gradients of the free energy.
So we don't need the gerative model. All we need is the gradients of this free energy function,
or technically a function or a function of a function of the gerative model. So
if I was building an artifact, I could certainly prescribe the right dynamics by writing down
the gerative model, and then I could evaluate the gradients of that free energy function of the
gerative model. And then those gradients would actually drive the dynamics. I could simulate
self-organization, I could simulate active inference, I could simulate belief sharing,
whatever I wanted to do. But in that simulation, all I really needed were the gradients of the
free energy. And we only need the gerative model because the free energy is a function of the
gerative model. So in that sense, the gerative model is really a theological description that
you would bring to the table to understand your sense-making of another thing. The thing in and
of itself just needs the free energy gradients. It just needs to self-evidence by moving in the
direction of maximizing the, on average, the log evidence for some abstraction, which is a
gerative model. But he doesn't need to know that. Just so we don't confuse people. So moving up
those log evidence gradients very much like the paradox of moving up concentration gradients
to clump together and to keep in this attracting set and to resist the dispersion of the random
fluctuations inherent in these random dynamical systems is just the same as moving down the
negative log evidence, which is the free energy itself. It's the same thing.
So what does that mean? Well, what it means is the gerative model is something that you bring
to the table to endow the behavior of this thing with an explainable teleology of the kind. It
looks as if this thing is acting in a way to maximize the evidence or gather the evidence
for its gerative model. And this is what I think its gerative model is. But notice the
thing in and of itself doesn't go through this process. It could, of course. So if I and now
people, things like you and me, we do have a model. We do have and we communicated and we talk about
it. But most things would not possess that deep structure. So you can invoke homuncular,
you can invoke self-modeling, you can invoke the kind of not homunculus in the sense that you
would normally end up with an infinite regress, but the kind of metacognitive aspect that comes
from any deep or deeply structured hierarchical gerative model. You can get that sort of homunculus
like detachment and metacognitive perspective on things. But strictly speaking, the gerative model
is only entailed by the dynamics. And I should say that this is probably best to think about
applications of the free energy principle as being limited to simulating and modeling and
understanding other things as opposed to oneself, unless one's doing or self-modeling.
And so in that sense, you are ascribing a teleology, a purpose
that would explain the behavior of something that you're observing. But notice, because you're
observing it from its point of view, you are an external state. And from your point of view,
all you can see is the Markov blanket. You cannot see what's going on on the inside. So it is,
it would be mathematically impossible for you to ever know whether what's going on in the inside
is indeed describable as even a gradient flow on a free energy functional, let alone did it have its
own internal model, did it have its own beliefs. It just looks like that from your point of view.
Because you will never, ever know by definition. If you knew, then you would, by definition,
have breached the Markov blanket, and that thing would cease to exist as the kind of thing that it
was before. It may sound obvious or it may sound trivial, but I don't think it is if you just look
at people like me, who spend our entire lives trying to peer through the Markov blanket using
brain imaging, for example, and, you know, or people doing comparative anatomy by dissecting
dead things, or by inventing new techniques to try to try and get, you can never get beneath
the Markov blanket, but you can certainly sort of have some perspective on the Markov blanket
that are closer and closer to the internal states, but you can never actually get in there.
I imagine a whole philosophy on this, that you couldn't see your own visual processing.
You couldn't hear your own cochlear dynamics. You can't ever have access to the inside very
much in the spirit that internalists would say you can never have direct access to the outside.
So I think that also applies to these stories about generative models and the Bayesian mechanic
interpretation of that comes along with the with the free ownership principle. Does that help in
some way? It's a little bit subtle, and I can just imagine a lot of my friends tearing their hair
out because they have very particular views about this. You know, people think it's a model of a
model. Some people think it's, you know, take it very internally, some people externalist,
but mathematically, you know, it's not the model which drives as, you know, it is the dynamics
which drives us, and that dynamics can be interpreted in relation to a generative model,
should you want to. Excellent. Well, I'm going to follow that philosophical line of thinking,
and what I want to ask is, this is an interesting point, the notion that we can get to a closer
approximation of what the internal states of ourselves are, for example, but it's always
going to be an approximation because piercing the mark of blanket would constitute self-disintegration
under the under the rules of all the principle, the free energy principle per se. How in a sense,
therefore, do we know or how how how much can we even make the claim therefore that there is such
a thing as an external dynamic that goes beyond anything, goes beyond our sensory inputs? It may
seem like a deeply non-scientific question, but the way you were speaking, I couldn't help but
think of people like George Barkley and even Kant and this phenomenal, numinal distinction.
If we don't ever have direct access to so-called external dynamics, because that is
part and parcel of the physical game that we're in, why can't we just say that there are no hidden
states and all we get are probability distributions over sensory appearances, but we don't need
something like a likelihood mapping to something that's latent? Well, you could say that and a lot
of my friends do say that and I think in the sense that the notion of direct access means that you
will never know in some sort of heuristic sense what the external states are if they exist at all.
And on the other hand, you could also argue that there is profound access to the
and existentially meaningful access to the external states. It's just organized in such a way
that it has to be transacted by the Markov blanket. So, I haven't had this conversation
for several years now. I used to have lots of pat answers, which depending on who I was talking to,
but the way that you phrase a question, which was very clever, just made me think, well,
what is the internalist argument? Does it mean that because I don't have direct access, that the
external states don't directly influence me, that means that they don't exist?
Is that the argument that an internalist would make? Well, might it be the case that all we have,
we don't need to invoke a kind of further realm, a further causal realm, an underlying ontological
realm. If all we have are just the streams of sensory input and seemingly some regularities,
some patterned regularities in them. Why, at least in terms of doing the Bayesian modelling,
do we need to invoke a hidden state or an underlying cause? For example, if we just take
a look at the low road tax of inference, we talk about how cognitive creatures like ourselves
predict the world, let's say at a very simple level. A question that I've always had is why
why does the claims of predictive processing or predictive coding more generally?
Why does it make claims about hidden states? Why doesn't it make claims just about regularities
within streams of appearances? That's the way that I would pose that question.
Right. I'm just mindful you've introduced predictive coding and predictive processing.
We have the energy cost. Just for those people who may not know that the intimate
relationship between all these things. Another way of looking at free energy is the amount of
prediction error. More specifically, if you're talking to some people, it would be the total
amount of precision weighted prediction error, which means that there's another reading of
minimising free energy, which is not the maximising the model evidence, but minimising
surprise where surprise is the implausibility of some sensory data given your model of that data.
In exactly the same way that if I go around gathering data that I find continuously surprising,
then that's basically evidence I haven't got the right kind of model to be able to explain,
i.e. predict those data. Again, we have one of these completely equivalent descriptions,
but they have very different flavours that to maximise model evidence is to minimise surprise,
which is to minimise a prediction error, which is to maximise predictability, which is to minimise
variational free energy and so on and so forth. They're all the same. They're all the same thing.
So when you're talking about predictive coding as a particular algorithm for minimising free energy,
technically a variational scheme, which is known in engineering as a Kalman filter,
what you're saying is I'm making certain assumptions about my generative model
that mean I can write down these free energy gradients as prediction errors. So literally
the prediction error, my apologies, the precision way to prediction error
literally is mathematically exactly the gradient that we were talking about before.
So just join the dots. So thank you for your question. Why is it that I need to invoke latent
states in order to make sense of data? I would argue that the sense making is just explaining
data in terms of latent states. And those latent states are often referred to in the technical
literature as hidden states. And what do you mean by that? Well, it just means they're unobservable
from the point of view of the free energy principle, they are hidden behind the Markov blanket.
So I would say that detecting patterns simply is invoking latent states that organise the
structure of those patterns. So I don't think there's anything magical about latent states.
So the latent states that are, if you like, entertained by your internal dynamics and your
sense making are not the external states. They are descriptions that are only relevant to the
generative model that you need in order to define the prediction errors or the free energy gradients
in order to simulate or describe your sense making in terms of your neuronal dynamics or
your internal dynamics or the operations, say a thermostat. But there's a twist here. There's
a reason it's called active inference because it's got a big active at the front. So it's not
good enough just to say, oh, I can understand psychology just as making lots of sense of my
sensory impressions. That's not good enough. Because you are coupled to the world
in reciprocally, which means that you actually are not only are you exposed to the world,
but the world is exposed to you in an exactly symmetrical way. Then how could one describe
that? Then we come back to where we started with this partition of blanket states into sensory and
active states into the inputs and the outputs, which means that sense making is quintessentially
active. So one way of phrasing that is that, yes, I'm making sense of all this sensory data
and all the patterns, but at the same time, I'm actually in charge of actively soliciting those
data, causing those patterns. So I think once you bring that sort of inactive aspect to the table,
so I notice that you treated predictive coding as a sort of generalization,
predictive processing. It might be easier if you do it the other way around because then
you have the grace to accommodate action in predictive processing its most general sense.
If you don't, then predictive processing I think is an incomplete story. You have to have,
as part of the processing, the active solicitation, the garnering and the structuring
and querying the world in a way that reciprocates with the right kind of information that allows you
to do the good sense making in terms of the latent states that we're talking about.
I mean, formally, this takes us into another part of the story which we didn't previously unpack
about active inference. So somebody might ask, well, okay, so we've got this
application of a free entry principle that a physicist might be quite comfortable with
to behavior that entails some kind of sense making, cheekily, I like to call this sentient
behavior, then I get told off for that. So how does this differ from reinforcement learning?
How are behavioral psychology? It differs in a quite fundamental way because the big thing
that active inference brings to the table is that, first of all, it says you are in charge,
you are actively sensing. So this is vision, active perception, active inference in this
most general sense, which means now what are the imperatives for generating your own data?
Now you could if you were doing behavioral psychology of a behaviorist sort in the 20th
century, you could say, well, I just want to generate those sensations that I find rewarding.
And so I've got some privileged sensory channels that I'm going to label as reward,
and I'm just going to act in a way that maximizes the reward that comes in.
From the point of view of the free energy principle, that's not what happens. What happens is
I want to find those data that resolve the expected surprise or the expected free energy
consequent on acting, soliciting those data. So what's expected surprise? Well, technically,
it's an entropy, it's just a methodical measure of uncertainty. So what that means is that the
description of things that persist or exist over some particular period of time
reduces now to a kind of Bayesian mechanics in which the behavior will look as if it is
information seeking and uncertainty resolving. It will look as if it is responding to these
epistemic affordances under constraints. And it is those constraints that, if you like,
would be the homologue of the reward. They are literally the constraints endowed by the prize
on the generative model, the kind of sensory states that constitute this attracting set,
to which I am attracted. But the vast majority of the drives for good behavior or behavior
that things that exist would evince is this resolution of uncertainty, this information
seeking. And indeed, that leads you into considerations. Well, how would you describe
that kind of information, that optimal information seeking behavior if you were a scientist or
if you were an engineer? And if you were a scientist, you'd be looking at the principle
of optimal Bayesian design, articulated by people like Dennis Lindley in the middle of
the last century, where you measure the quality of an action in terms of the information gain
afforded by the data that you secure by acting in this way. Exactly the same idea emerged in
the context of active learning in machine learning. This is the problem. If getting data is costly,
what data point would I need to resolve the greatest uncertainty about my beliefs about the
latent causes of those data? So active learning, again, notice your active and explicit part of
this process. So in answer to your question about would it be sufficient just to understand
sentience in its most basic or elemental form? As discovering patterns in data, I would say no,
because that denies the openness of any given system immersed in her world in two directions,
the world influencing you and you influencing the world. As soon as you put that into the mix,
I think it's very difficult to conceive of any construct, possibility in philosophy,
but certainly any construct in computer science or physics that would
that would permit a complete description of sentient behavior just in terms of sensing patterns.
Does that make sense? Yeah, that makes sense. That makes sense. Makes sense to me.
Yes, I want to stay on action for a second, because I think the notion of the minimization of free
energy or the maximization of model evidence could make some intuitive sense for an agent like
ourselves in the sense that we have our sensory inputs, which are divergent from our priors,
and we act to change the world to make those sensory outcomes more in line with our expectations,
our preferences. However, at a more fundamental level, one definition that we spoke about with
the free energy principle is that the internal and external states look like they're tracking
one another across a Markov blanket. Now, a question that comes to my mind when I see a
definition like that, which is that makes sense for me for a sentient, conscious human,
but what would a stone be tracking or what would a drop of oil be tracking to use an example that
you've used before? I think going to something a bit more less animate, and I presumably answer lies
in animacy, I think would be a really interesting avenue to go down, because it gets these fundamental
questions of how does something as minimal as a particle retain its particle-ness,
and what does that really look like when we presume that they're not conscious,
