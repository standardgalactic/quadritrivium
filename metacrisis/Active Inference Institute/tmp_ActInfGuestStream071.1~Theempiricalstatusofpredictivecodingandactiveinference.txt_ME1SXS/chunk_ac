stuff tractable um is is i think part of the question yeah look the scalability issue with
active inference is not an active inference issue it's just a Bayesian learning and decision making
issue yeah um it's there's nothing inherent to active inference that is that slows things down
um any any technique that can be used in any other sort of Bayesian decision making
method can be used in active inference um so yeah this is a the grand question of how the brain
is so efficiently able to do this um where there's yeah like Ryan said i mean you run these very
basic tasks and because of this the exploding state space of uh Bayesian decision making right
where just these is exploding state space of probability probabilities um you get a massively
expanding search tree ultimately and yeah nothing to do with active inference obviously of course
active inference is um the Bayesian framework and so it's it has to sort of use this um but
that's just a i mean if we can solve this then uh that's uh yeah it would solve a lot of things
that we could solve how how we can perform Bayesian inference and especially Bayesian
inference in the service of decision making how we can do that very efficiently um that would solve
a lot of things um but as of yet yeah if we want to try speed active inference up we have to speed
the general field of Bayesian decision making up it's part of it yeah yeah oh it's sorry right
okay i was just gonna say it's part of it perhaps from a strategy standpoint
not trying to generate the world's biggest plan but actually going back to that rules
business and saying can we start there instead of a plan that because of its just its size
makes things intractable can we could we swap something else in for something we know won't
work which is world's biggest plan um i mean i mean look there's lots of different you know
things that you know might you might consider like like one one thing you know for these like
exploding decision tree um sorts of problems is you know finding some way to um chunk things
together right to make like the you know so for instance instead of you know so i'm gonna go i
decide i'm gonna go walk to the store or something right like in a certain sense i've got a ton of
different options about you know where to put my feet at each step and you know like what door to
go out of and things like that um but it's not really clear that when i'm planning like to go
to the store that i'm really explicitly considering all of those details right like i can chunk it
into you know i'm gonna walk out to the street you know i'm gonna walk you know 10 you know i'm
gonna walk a mile to the store and then i'm gonna walk into the store right in which case if i've
chunked that to just kind of three steps right then my decision tree is is already much more
tractable and then you just need to tell some story about you know how you know when we get to
certain points and that really abstract chunked um plan when we get into those kind of chunk states
how we make these kind of more local decisions about what to do when we're at the at smaller
scales like when we're in those states so you know there's lots of kind of hierarchical
chunking-ish sorts of things that you know you might think that the brain could be doing to make
these things tractable but and there's lots of different kind of options on the table and things
that people might try but all things have certainly all these things have not been kind of you know
tested against behavior yet to see you know which ones are are the most plausible but there's
certainly there's certainly different possibilities for what might be going on um you know another
example you know this is something that we test empirically in my lab right now is um the way
people might do something called like a verse of pruning um and so so what you're kind of doing
there is you're just and this is kind of similar to what i said before is you know when you start
kind of planning down some tree if there's some early outcome where you're imagining that it's
going to lead to some really negative outcome right like on like if like on step one or step two
and a possible plan i think oh there's going to be some big negative thing then i will just
no longer simulate down the rest of that branch so it's a way of kind of reducing the number of
branches in a tree i have to search and and so that's it's often called again aversive aversive
decision tree pruning and that um you know obviously you need to do something like that right to keep
it tractable but at the same time it can cause problems right because sometimes the best plan
might go through something negative in the short term but lead to the best thing in the long term
right and i mean so that's the kind of thing we we test in the lab is whether or not different
psychiatric conditions involve kind of doing pruning too much right or too little um and
how that could lead to so optimal behavior in the long term um the other i think the other
aspect of the the previous question you asked i also had to do with you know you read part of
the thing talking about um the uh the the neural basis of of active inference and how that should
kind of remain tentative and um just to kind of clarify that a little bit i mean the i mean the
main point is is just that again this kind of involves being clear about the separation between
algorithm and implementation right so the the algorithm is basically just the mathematics
right that we um you know that we kind of lay out but then the implementation question is you know
what are the different possible ways you can kind of set up the brain to to carry out those equations
and it's a separate question what the you know how the brain um how the brain is doing that even
if you think that there's good reason to believe that the brain is doing the doing something that's
well characterized by the mathematics um and um unlike predictive coding which has been around a
long time and you know there's been much more opportunity to test um at least um qualitative
predictions right you know things like whether or not there's evidence for like omission responses
in the brain right like when there's the lack of a stimulus when it was expected leads to
an oral response um which is you know one of the stronger pieces of evidence that the brain is doing
something predictive like predictive coding um you know for for active inference uh there really
has been like one imaging study and it was done like in 2015 and it was like an older version of
active inference it wasn't the current for based on the current formalism um you know so there's
this just hasn't been done right and the and even the um you know the little kind of column
structure things like what's in the um like what's in the review in our in our in this paper um
is super just kind of like promissory heuristic sort of things like here's a bunch here's a
bunch of little balls that will pretend they're like neurons and here's how you could connect
them together roughly you know to um do some sort of you know some message passing algorithm but
you know your thinking is you know your imagining or hypothesizing is that is the one that the brain
is doing for this kind of approximate inference process um and um you know even when it comes to
um the current mathematical proposal um for how the brain might be doing something like this um
there's different hypotheses right so like initially most people when they were doing
simulations with active inference models were assuming um something called variational message
passing which is just a particular way of kind of repeatedly doing local approximate
Bayesian inference on different kind of nodes in a graph um and they you do this kind of over and
over again and then kind of converges to a good guess about what the posterior should be over
states at each time point right but but you know then after that a little bit um more recently
you know Thomas Parr and Carl um and uh maybe other people were on the paper I can't remember
they proposed a kind of updated version of that called marginal message passing
which is a little better um it does a little it's slightly more it can get at better
approximate posteriors um you know on average um but then there are others right there's belief
propagation is another sort of message passing algorithm that's you know been considered so
there's a bunch of these right and each of those even if they're doing active inference right like
they will also predict different neural dynamics um so so those are also even just active inference
under what message passing algorithm um separates into a bunch of different competing hypotheses
about what you would measure in the brain awesome I'll read a question from live chat
Andrew P writes curious for Dr. Smith where might he place his previous work like
simulating the computational mechanisms of cognitive and behavioral psychotherapeutic
interventions insights from active inference in relation here would this kind of work be more
on the side of theoretical exploration like the cognitive affective behavioral interactions
construct or is there a direction for empirical testing I mean as is absolutely it was a theoretical
simulation work sort of paper right I mean we weren't fitting that model to any sort of empirical
data um it does I mean that well that paper in particular does lend itself to at least certain
that that model is a little too general like the patterns of behavior that it predicts are
really not specific enough to like test in a you know some sort of task um you could you could
make it you could modify it in a way that might make it specific to some sort of task that would
involve um you know specific sorts of explore exploit decision-making choices under kind of
expected negative outcomes things like that um but uh it does make a kind of sort of qualitative
prediction um that could be that could be tested um you know so one one one parameter that you might
fit right based on that model that I think would be super interesting if you could kind of turn it
into a task is this um parameter that reflects like the degree to which your cognitive beliefs
influence your automatic affective responses or expected affective responses um the um so I mean
it would be really interesting to to try to use that to figure out whether or not there are we
can measure individual differences in this kind of what what often gets called like cognitive
penetrability right so whether or not I you know because I might explicitly believe I'm safe right
in a certain situation but I might see something that still makes me feel really this automatic
kind of fear response in my body or something even though I explicitly believe it's safe right my
affective responses don't always have to match my explicit cognitive thoughts that's often a thing
you see right in people with with um with affective disorders so this kind of individual difference
that you could estimate about the degree to which these two um essentially these two uh
different uh hidden state factors you know in the in the model the degree to which those things
interact effectively um but the the other the other um prediction that it makes um again it's a
little more qualitative but certainly something that could be tested or made more precise is um
that you know in that in that model um one of the the interesting things that came out of the
simulations is that um you actually it's probably actually not a good idea to make people think
that they're explicitly believe that they're in a safe context um before you do something like
exposure therapy um because if you do that then basically what you're learning what you the beliefs
that you're updating in your your likelihood right about like what actions are going to lead to what
outcomes those will be updated under the belief of safe context right so that means all you have to do
is switch back to believing you're in the dangerous context and then you're just right back to the
problematic avoidance behavior again right um and there's a bunch of other there's recent empirical
work that's um very consistent with this idea actually about you know explaining using this
kind of latent causal inference framework not not an active inference in particular but just
latent causal inference more generally that um you know to really get exposure therapy to work
long term um you need to kind of prevent that people are inferring that there's a new latent
cause in operation when people are doing exposure because otherwise they're they're just learning
safety under under a context where they can really easily get the get their um maladaptive
avoidance to come back just spontaneously um just by kind of being uncertain about the context
they're in later right like they're not they're not actually unlearning the problematic belief
they're just inferring there's a new cause where under that new cause it's safe um so so it was much
better um the the prediction of that of that paper would would be that could be tested right
and again like I said there is now some evidence consistent with this um is that uh it's kind of
better to do things to keep people uncertain or keep people believing that they're they're still in
the same context so that when they go through this exposure therapy and get unexpected outcomes
they're actually kind of overwriting their previous beliefs as opposed to just inferring
they're in a new context awesome all right I'll I'll ask a kind of general question that I'd love
to hear everyone's perspective on we have active inference or predictive coding or some other type
of formal framework that does interface with empirical data like we're discussing so on one
hand we have the empirical system or the measurements coming from it existing data sets data sets we
create and so on and then on the other side of this kind of statistical interface is like numbers
free energy principle category theory stuff that is really not having its validity or truth
um or essence described by any particular system so how do you just approach this usefully as a
graduate student researcher or as like a broader clinical research program and like think about
what system are you choosing what statistical interface and how detailed do you go there
and how do you think about like what's on the other side of the statistical interface
because a lot of times the discussion around like the validity of active inference and free
energy principle is a super philosophical question as if a philosophical discussion would resolve
uncertainty about the empirical status of active inference which is actually the direction that you
all took it from the interface back to the system rather than interface as outcome from something
theoretical to debate um i feel like i have been kind of hogging the show here a little so i'll let
um other people uh answer that first at least um okay sorry that was quite a long question so i'm
sorry if i missed some of it um and just correct me if i'm not answering right uh i think i will
talk on um this idea between yeah on the one side uh sort of mathematical formulations representing
something like uh brain processes and then you have sort of mathematical formulations
brain process and behavior and um how these are sort of scenes as unified i think um as ryan said
about uh talking about neural implementation um there's many ways in which um we could set up
neural like a neural neural implementation to achieve um something like predictive coding
or active inference uh the same goes for algorithm where um there's many different
sort of forms of algorithms could achieve uh or statistical windows as you said if you want to go
more broad um could achieve the same um neural implementation so uh and then ultimately uh
behavior so uh in terms of and please stop me if i'm not answering correctly um but yeah in terms
of sort of as a graduate student choosing what to focus on um i think it's sort of a evolving
process where you constantly just try read and come up with new ideas about how one can think
about how these can connect and how one can represent the other um and and for me it's been
particularly interesting just actually understanding what active inference is in its um mathematical
essence right we've got this first principal account of it um debated from concepts of homeostasis
and yeah it's it's can sometimes feel a bit strange when you look at um quite simple mathematical
implementations in an algorithm and see that well this isn't so different from something like
Bayesian reinforcement learning with specific directed exploration um so they are these there's
a sense of of that sort of first principal account um that comes all the way from Markov blankets
where's that captured in these algorithms and um that's something i'm still trying to figure out
to be honest um it's i think it's it's in in general it's a problem right too and i think that's why
um when ryan mentioned sort of looking at more complex tasks i think more complex tasks would
allow us to explore more complex representations of active inference um that can that can perhaps
capture um some of the foundations that friston laid out um so does that answer the question
sort of yeah that's awesome yeah i mean the other i mean the other things i would say i mean
i think i understood at least part of the question a little differently um was that um you know there's
sort of what i by that i don't mean that you misunderstood raw and you just answered your
different i know that's all good in my answer part part of what i took your question to be
was something about you know to what degree answers about the you know quote unquote validity of
active inference or the free energy principle can be established through kind of like philosophical
argument versus what can be um or what needs to be answered um through kind of direct empirical
testing um and um and i think um you know here i think it depends a little bit right so i mean
i think what you know what philosophy is really good at is actually more finding ways that something
couldn't be true right i mean i take you know i mean i have a philosophy background um as well and
i think i think philosophy is really good at like conceptual analysis right so trying to find you
know where some particular conceptual framework um you know where there's tensions right or where
there's things that don't sort of at first at first need sort of appearance seem like
their contradictory have some sort of internal tension um but when you really drill down you
find certain contradictions right that you know rule out something or make it really implausible
um either through sort of direct logical um um argumentation or um through kind of like
um just proposing sort of thought experiments that they're really kind of pump intuitions
in a way that that make a difference so so i think you can rule out with philosophy
