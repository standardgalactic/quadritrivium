as inference
so it's casting action
as a process of inference
so you actually
if you subscribe
to this formulation
the choice
on the selection of what to do next
is an active inference
it's inferring this is the most likely thing
something like equal to
in this belief state
so I'm going to be trying to involve that kind
of uncertainty over the work towards those
more preferences
and you can write all of these
particular generalizations
and as prize
in this project of logic
if you
then just
apply something
this is called a new field approximation
this prioritizes beliefs
with
normal forms of reverse distributions
you can then just go
and get some of the shelf
mathematics
to describe the belief argument
and the message passing
that having that
sort of general model
even assuming you as a brain
from my perspective
even if you're not a mathematician
that must be from your point of view
possibly not from your perspective
but from my perspective
the results are remarkably simple
and more importantly
not very very similar
to the dynamics
of belief update
in simplified versions
of pregnancy
so remember before I said we will just
three things we don't know in this model
state of the world
policies apply
and the position of confidence
placed with those policies
and it turns out that the
solutions
that optimize that free energy
functional
can be expressed
here as a non-linear function
of linear mixtures
of beliefs about the past
of the future and observations
now and this starts
very much like a very simple neural network
model
a sigmoin firing activation function
operating upon linear mixtures
of activities elsewhere
in the brain
a very very simple expression
that now starts to
provide a metaphor
for neural responses
and belief updating
as encoded by neural fireworks
here an expectation
about being in a state
one state or another state
notice also this implies
expectations
about the past and the future
so written into this
gender model
is an elemental form of memory
and perspective
post-diction and prediction
of the future
so there's a sense of time
and progression implicit
in this update
if we look at
policy selection
it's simply a softmax function
of this expected free energy
of goodness for policy
and this is a passive softmax
response rule
used in economics
and much of reinforcement learning
interesting with the confidence
here
looks as though it's updated according
to something that's very similar to a reward prediction
which takes us off
in a very different direction
in the direction of dopamine and the relationship
of reward predictions
but I want to move on to
optimising the parameters of this model itself
and again
it's very nice because
with the update rules and solutions
that optimise this free energy function
nothing exactly like
heavy learning
so you have these functions
that have associated term here
can't see this here
because this is the deep one
but if we were looking at the A1 and CB
I would come from the states
carrying together as a product
a decay term here
and which is accumulated by building
a connection sense
and then decay again as a function of time
and then finally we have
our action selection here
with very simple rules
you can start to engineer
or propose a very
crude
or coarse function of that
observations come in say
visual cortex
they are used to update
beliefs about states of the world
so they have a countless of time
and these beliefs are then used
to evaluate the goodness
of a policy in terms of expected free energy
and risk and ambiguity
and you can code it again
the front part of the race
of basal ganglia
cortoformatic
looms
where the confidence in these policies
may be mediated by safety
in the ventric technical area
and then they generate the next action
that changes the world
gives you the next observation
and so the cycle continues
the very crude
but relatively simple understanding
of the potential massively that you get to
so here's
I'll close down with two examples
one a very simple example of origin
in a maze
and then we come to
a proof of certain many of our advances
in deep
generative models of this sort
that have been used to construct things like
language comprehension and reading
so this is an example
I don't need to
go through it in detail
in brief
all we have is a little rat
in our house
in a teammate
and it lights rewards
and it's got two moons
it can make two moons
and it doesn't know whether the reward is on the right
or on the left
what it also knows though
is that there is an instructional
key at the bottom of the maze
that tells it
whether the reward is on the left
or the right
so it presents an interesting choice for us
it can go
to one arm
and once it goes to one of the two rewarding arms
it has to stay there
which means that it can go there
and get two rewards
on 50% of the time
or it can go there and get nothing
on 50% of the time
or it can go down here
to find out where the reward is
and then get
50% probability
the reward will be a half the time
so the expected value
