is exactly the same
but
by going to the instructional key
the epistemic key
it can immediately reduce
the epistemic part
which means that
if this maze
was minimising
it was optimising
it's expected
so it should go
if we simulate those equations
on the previous slide
when the journal took off
which I've written down here
for this parallel
it should go
and get the epistemic key
and then go and get it to reward
and indeed that's what it does
so it starts off
what I'm showing here
is behaviour
of over 32 trials
where the reward was
on the left or the right
and the policy of the terms
the outcome
the amount of reward it got
and beliefs about
whether the reward is on the right or the left
in terms of the addition states
what I've done here
is after switching the reward
after the first couple
of presentations
I then left the reward
on the left hand side
I want to see what's going to happen
so initially
as we might anticipate
the mouse goes
and finds the epistemic key
there's always this uncertainty about what to do
and then indulges
in its risk
of those behaviour
so then chooses the pragmatic
preferred option
by going straight to get the reward
and as happy as it could be
however
as time goes on
it now learns that
in fact the reward is over there
all the time
so now the epistemic value
of that instructional key
gets less and less and less
it's resulting in less and less uncertainty
because it's increasing in certain
that the reward
is on the left hand side
changing on this experience
with head on learning
learning about these initial states
that are heavy and sparse
so at what point
it changes its preferred policy
and chants to a pragmatic
exploitative policy
so it's a natural progression
from
exploration
to exploitation
but it's purely
a reflection
of the fact that we are using the belief
based function
because the goodness
of the thing to choose
depends upon my beliefs
and my uncertainty
whether I need to get right back to certainty
or as a search
it depends upon the need of a search
and of course
they are very familiar with the fact
that there is no need to search
as they are strained for
that expected value
and to engage in exploitative behaviour
this slide just summarises
at the same point that I've been making
so
basically learning
underwrites confident
policy selection
and that confidence is reflected
in this precision parameter
which is said to not like don't really
add it in deeply and stimulate
time-to-time updates
of this parameter here
as long as it's exactly like don't really
we can also look at simulations
of need-to-peak updates
of the experiment in the trial
so it's seen that it sees this
making value
because since it sees anything
it has to iterate these equations
in order to
find the days optimal
solution
and that looks a lot like
an adventure-related potential
in that journal of physiological research
and interestingly what happens is
it was less belief updating
when it's more familiar
and confident about the environment
to get an attenuation
of these responses
but an increase in the confidence
because it knows exactly what's going to happen
and
what it expects to happen
it does indeed happen
and how it goes and it stronts
its knowledge about where the reward
is
so using that different example
you can tell all sorts of stories
you can tell a story
about the representation of the future
and the past
so this is the beginning of the trial
the first one, the second move
and of course as these beliefs are updated
what are beliefs about
what future consequences of action
now become memories
of our past
so there's an interesting shift
of time and table frames of reference
that means that things are
what's a prediction
that becomes very interesting
accumulating beliefs from trial to trial
it also allows you
to think about
the approach responses
to the things that you would ultimately choose
as opposed to things that you are
not going to choose
and there's a nice
literature in the empirical
papers
showing exactly this form of saltation
divergence as time progresses
in terms of selective responses
shown by these
expectation encoded
simulated
neural populations
that mirror
or reflects exactly
the empirical results
we can even plot
these responses as a function
of where the house is
and what emerges from this
kind of architecture
are place levels
that sometimes are very
unartiguous
for example, the two rewarding locations
sometimes are a bit more
unartiguous
we can also perform simulated
on-board experiments
especially negativity experiments
so
these are the same results that I showed you before
but I'll now tell you
a different story about them using
a different language as if I were
an electric physiologist
doing on-board paradigms
so what I can do
is to believe updating
when this is from our scenes
the same stimulus
when it's familiar with it
and when it's not familiar with it
it does the same
response, selects the same policy
so the only thing that's different
