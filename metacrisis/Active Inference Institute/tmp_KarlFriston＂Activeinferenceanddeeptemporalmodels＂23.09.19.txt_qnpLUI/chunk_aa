I spent the morning at Kremlin and its museums, and I've never seen so many of them.
I've never seen so many beautiful things in my life in one day before.
So to thank you, I'm going to show you my most beautiful formula, the Mathematical Equation.
I'm going to spend the next two hours deep on stratigraphy.
I'm joking. I have been told that not everybody here is a mathematician.
So I will be showing you my favourite equations, but I'm using them as pictures to remind me
what to say and just to rehearse the formal structure of the arguments.
This lecture is really an invitation for us to think about what the brain is doing,
and I'm going to take the perspective that the brain is optimising something.
The question is, what is it optimising?
So I'm going to ask you to stand back from psychology, or she learning, or education, or artificial intelligence,
and just ask the simple questions.
What is it that we want to achieve? What quantity do we want to optimise?
And then you have to offer an answer, which is indeed borrowed from statistical physics,
and it has a very different meaning and interpretation for us.
And so what I want to do is to walk through the interpretations and the meanings
from a psychological cognitive and sentiment perspective.
Further to that, if we understand the mathematical principles of higher grade function,
and indeed just the sections and action, then it should be possible to shine a new light
on message passing in the brain, on the neuronal dynamics, on the anatomy of the brain,
the function of the anatomy, or the computation of the anatomy.
So much of my lecture will be trying to rehearse the general idea in some abstract terms,
illustrating how far one can take the formal principles in understanding the empirical brain responses
that a lot of us measure.
Say, for example, if we achieve an electromagnetic brain response,
it's not indeed just our choices and our decisions and our behaviour.
So the agenda is ambitious, so I'm going to address a very deep and bigger question.
What do our brains do?
And provide a potential answer, but use the answer to illustrate to you
how far one can get in simulating the understanding, quite high level or sophisticated behaviour.
So I want to eventually end with an example of reading and language understanding
and its brain response currents.
So that's where we're going to try and go.
If I take too long, you're going to stop me, and then we'll have a conversation
which is usually the best part of these lectures.
There's a question for that.
So I'm not going to talk for more than an hour, thankfully,
and then I look forward to having a conversation about some of these discussions.
Now, have you all read this?
I will be asking questions.
This is what I'm going to be talking about.
I go to introduce the question at hand
and then provide this answer in terms of something called active inference,
closely related to things like active learning, machine learning,
or active vision in the visual sciences.
This is putting perception in an inactive context
that we are actively perceiving and we are in charge
of the way that we palpate or sample the world
in order to optimise our perceptual synthesis.
I'm going to show you that mathematically that's just the same
as gathering evidence for one's own existence.
And I'll explain that connection in about five or six months.
All of this self-evidencing, this active perception, this active inference
rests upon predicting worlds actively,
having a model of how the world works
and how the world supplies our sensory organs,
the sensations, the situation of cerebral outcomes.
And that's known as a generative model,
a model that generates sensory consequences
from the causes out there which we want to understand.
So I'm going to spend some time illustrating the kinds of generative models
that we use to simulate behaviours,
general studies, or in economic games,
and the resulting or ensuing belief-outdating
that we can then look at as if we were
electro-physiologists of red ladders.
And that's from the principles to the process theory.
So making a distinction between the principles of optimisation
that we're talking about, active inference and self-evidencing.
And the neuronal processes that are occurring in your head at the moment
that we can measure as neuroscientists and cognitive neuroscientists.
I'll briefly rehearse some empirical predictions.
I'll go through this quite quickly,
because I want to get to the end with this,
some of the most advanced applications of these ideas
to understand the functional architectures of the brain.
And that seems some deep generative models
that have a deep hierarchical structure,
both in terms of abstraction, but also in terms of time.
Exactly the sort of models you would need to generate language.
And I'm going to turn that on its head and say,
these are the models that we use to understand language.
And I'm going to present some very simple simulations of reading
and show that they produce the same electrophysical responses
that we use in classical paradigms,
like we just match negativity or P300 paradigm.
So let me just start very abstractly and very simply.
Imagine you are an owl.
You are a bird of prey and you're hungry.
So what are you going to do?
Boris, what are you going to do?
Search, perfect.
In that one simple answer to search,
there is a whole range of hindricks about what we architect at times.
So here's the owl searching and there's the unhappy prey
who is about to be eaten by the searching owl.
So clearly, if I'm hungry, the first thing I'm going to do
is to resolve my uncertainty about where the prey exists,
where here the mouse exists.
And it's that notion of searching,
resolving uncertainty that underwrites everything
that I'm going to talk about for the next 50 minutes.
It's important because there are two ways you can write down,
and here are the first and the foremost,
but I repeat, please do not worry about the maths,
they're just here to remind me about different ways of thinking
about things that you can write down in computer code
or in deep mathematics things.
There are two ways you can write down the objective for living,
for the owl, for you, or for me.
First of all, we could write down an expression
for things that we do, controlled variables in engineering,
which we'll call a view of this point in time,
as maximising the value of some states of the world
if I did that thing.
And then what I would be able to do
is to create a policy pie that for any given current state,
if I apply this action, I will move to the next state,
and then I will maximise the value of being in that next state.
So this is the notion of a value function of states
giving rise to a state action policy.
It requires you to believe in and commit to it
and believe in the idea that for every state
there is a label which tells you how valuable that state is
and then all you have to do is to choose the action
that takes you from this state to the most valuable state.
But that's not going to work to explain searching.
For the simple reason that searching is all about
reducing uncertainty, then we know immediately
because uncertainty is an attribute of a belief,
it's not an attribute of a thing,
it's an attribute of a belief about something,
then the function that we need to optimise
has to be a function of a belief.
And if the belief is about something,
then that's a function of a function of something.
And mathematically that's called a functional.
So what we actually need of the alternative way of doing this
is that our best action at this point in time
maximises a functional of beliefs about states
and their ends, states in the world,
if I did this action here.
And I'm going to describe this belief in terms of probability
and posterior probability distribution over different states.
Again, don't worry about the maths, just remember
that Q is a probability belief
and that gives a very different sort of optimisation state.
So we've moved from a value function of states of the world
to a function of beliefs about things of the world.
Furthermore, the notion of searching tells you
that something else is very important.
It means that it matters whether I search for my frame
and then I eat it, or I can't eat it and then search it.
So time and order really does matter,
which means that you can't just write down an object of function.
As a functional of beliefs, you have to define a policy
which is a sequence of actions in a particular order.
So now what we have is a notion that there is a best policy
that entails or prescribes a sequence of actions
in a particular order.
In machine learning or control theatres,
there's sequential conceptization.
For us, all it means is we know that the best kind of policy
of the system, pie and star again,
maximises the sum of this functional,
and I'll say that this is a free-action functional,
of beliefs to give them that particular policy,
and that if I don't write policy,
I can select the right action.
So these two contrasting ways of writing down formally
what things, what living things do,
emerge in many different guises, in many different contexts.
So the notion that you can explain behaviour
in terms of optimality value function
rests on optimality principle,
and from this you'll find lots of examples,
optimal control theory, dynamic programming,
deep reinforcement learning,
utility theory in economics,
apples induction, and so on and so forth.
Some of these you may have heard of,
some of which you've done,
but they're all having caught this commitment
to a value function that can describe
everything we do in terms of optimisation.
The other approach, which is the approach
that we are going to pursue,
is based upon a principalities action.
So action here is just basically a time integral,
or a time average of an energy,
and I've just said F cos a free energy.
So this path is an action,
and we're going to maximise that action
so that we choose the optimum action.
And this is a free energy principle,
also known as active inference.
From this we will hopefully get along
artificial curiosity and intrinsic motivation
in robotics emerges.
We can also cast this in terms of basic decision theory
