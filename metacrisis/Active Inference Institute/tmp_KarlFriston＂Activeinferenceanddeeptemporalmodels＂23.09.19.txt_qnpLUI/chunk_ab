that I mentioned before.
This is an aspect of sequential policy optimisation.
Now I'm deliberately sort of
contrasting classical value function
with free energy functions.
So how do I find these approaches
that are belief based, and these are not?
But I'll actually show that they come out together again.
So this becomes this
when we remove those certainties.
I'll try to start working back to expected details
of theory in a few slides.
But for the moment, let's just focus
on what this quantity is.
And I'm going to give you one answer,
and I'm not going to motivate you.
There is a deep backstory here
from statistical physics and basic mechanics.
But I'm not going to worry about that.
I'm just going to tell you what it is,
and then hopefully convince you
it is a super objective function
by a series of examples
that will come up in this lecture.
So here's the basic idea.
This quantity is known as a variational...
...quantity.
It's the information theoretic quantity.
Quite closely related to something invented here
in complexity or multilinear complexity.
It's also known as an attendance low boundary machine learning.
In statistics, it's known as the log model evidence.
It's also known as partial likelihood.
You can forget about all these different names.
The reason I'm listing that's all we can know
is it's a very, very important quantity
which you see in many of every field.
The fact that it's called basic model evidence
has given me a license
to describe this optimization self evidence.
I will see why that works in a moment.
From a statistician's point of view,
this native physical free energy here,
this native evidence or log evidence
or evidence low bound elbow machine learning,
is just the probability
of getting these observations own.
At this point in time,
given a model of how these outcomes were generated,
and I am going to leave that long.
From a statistician's point of view,
you can always write this quantity,
this evidence as complexity,
this native evidence as complexity minus accuracy
or log evidence as accuracy minus complexity.
What that means is,
we're going to consider the brain
as a statistical organ,
an organ that's trying to make inferences
just in exactly the same way that you, the scientists,
try to make inferences about
differences between one group and another group
using a statistical analysis of covariance.
The brain is doing exactly the same thing
with its sentient data.
It's trying to test different hypotheses,
different beliefs about how those sentient data
were caused,
and it's doing so by maximizing
the native model of evidence,
which means that it is trying to find
the simplest, minimally complex explanation
that provides an accurate account
of the sentient data.
And that's going to be very important
in the contextualization, it's going to be very important.
So it's not just finding an accurate account of data,
it has to be parsimonious and simple
in the sense of a computer laser.
So this variation of the energy
is just the mathematical expression
of this mixture of complexity and accuracy,
and we imagine that the brain just organizes,
learns, infers, passes the message,
it's all in the service of minimizing
this complexity minus accuracy
or maximizing accuracy, minimizing complexity.
And if that were the end,
then that would be perfectly synchronized
and form an accounted perception.
But what we're interested in here
is how the brain covers back
and actively samples the data
that it could use to infer
a critical structure in the outside world.
And that's the reason we're still here.
So we've already said that we have to
define the problem in terms of
sequences of actions or policies.
And what we're going to say
is that we're going to select those policies
that maximize the expected free energy
after performing that sequence of behaviors.
So what that means is we're going to effectively
choose policies that minimize complexity,
expected following an action,
and minimize accuracy,
sorry, maximize accuracy following an action.
But notice now the outcomes
are now random variables.
They haven't yet occurred.
They are in the future.
So now we have to take an average over that,
things that could happen in the future.
So now we're talking about the average complexity,
and that turns out to be risk.
Risk, and this is where the accuracy
reminds me to provide the formal definition.
So risk is really the needs about
what will happen if I pursue this policy,
compared to what a priori I prefer not to have.
So I'll say that again.
Risk is the divergence or the difference
between what I think will happen if I do this
and what I prefer a priori to having.
So here my client believes about
the sorts of outcomes that I encounter
define the sorts of outcomes
that I expect to experience.
I've been rich, happy, warm,
having my temperature within the physiological range.
These are the things that make me
and they're good to me and they're happy to me.
They are my a priori,
my prior beliefs about the outcomes
that I will attain if I pursue this policy.
And the goodness of the policy
corresponds to the minimal reducing the difference
between what I think is going to happen
and my priori preferences.
That must be that sort of risk.
Let's see another instance of that for me.
Economic perspective at the moment.
At the same time,
I'm going to maximize my expected accuracy.
So what would that look like?
What does the expected accuracy look like
if I haven't actually got the observations that happened?
What it means is
I am going to deliberately choose policies
that make the sensory data as unambiguous as possible.
So if it's like I walk into a dark room,
I'm going to turn the light off
because that's a policy
which means that I can unambiguously see what's going on out there
so to reduce the uncertainty about what it calls
if there are sensory pressures.
There's a table in front of me
that's a light over there
that I will not be able to see in an ambiguous sensory context
if the lights don't work off.
So this is a little bit like the joke
about the man who was drunk
and he's searching for the keys
and he's searching for his keys out at the lamphouse
or the streetlight.
So that's the thing.
What do you do?
I am searching for my keys.
Did you drop them there?
No, I dropped them over there.
So why are you searching here?
Because I can't see over there.
That's a clinically based optimal response.
And it reflects about that we were part of the drive
for our good policies
and those which minimise our rigidity
or maximize expected agency.
And then once we found the good policy
or some of the action from that
the actual change in states in the world
down there beyond our sensory market blanket
and that will supply new observations
of what we do, our perceptual synthesis again
and finally, simplest academic explanation of what's going on
use our beliefs about states of the world
to a rollout simulator
another future of another policy
to set the policy that minimises the risk of non-rigidity
to set the action and so the perception action
or the action perception cycle continues
on and on and on
all in the service of minimising risk and rigidity
minimising expected surprise
or negative free energy
which is just uncertainty
in physicists also called empathy
but you can remember this as putting these two things together
it's just minimising uncertainty about the future
where that uncertainty includes preferences
