between the perception and the actual
is its beliefs
that it has accumulated
through experience
and if I associate this with a standard
stimulus and this with a normal
stimulus
we can comparably update and take
the difference and indeed we can
reproduce the phenomenology
of the next batch of negativity
do the same thing
with those simulated
main responses and show
a classical phenomena
in single unit electric physiology
in dopamine
cells, namely a transfer
of phasing responses
from the rewarding
queue, the ad-condition
stimulus to the structural
epistemic queue which you can think of
here as the conditioned students
so again
nothing's changed here
other than I've told a slightly different
story about the results
that emerge from this little simulated
right and all of that lend
a degree of constant validity
as overall thesis
that everything is in
the service of self-evidencing
maximizing evidence
and I don't model the world
and selecting actions
that minimise uncertainty
namely this phenomena of negativity
so I'm going to finish now
with a very quick run-through
of exactly the same
technology and ideas that
apply to slightly more sophisticated
generated models
of a sort that people might use
to understand language
and generate language
in this graphic
not because it's educational
but because
we spent a long time drawing it
and again that was a joke
it's a nice graphic because
once you bring
down formally
what you think
is a thriving message
passing belief updating and behaviour
you can now
extend that formalism
by generalising it
to
hierarchical structures
and when you do that
you start to see
lots of emerging behaviours
that now look a lot more like
the kind of behaviours that psychologists
study in human beings
so what we've done here
is taken our standard
little MDP model
space-kicking program
time-on, time-to-generating
metrics paid
and our can here
have the transitions
carried by the B-metrics
the 10 problems of policies pie
that are informed
by the expected theology
gene equivalence of those policies
what we've done is put another one of these
on top
crucially
it operated
at a slower time scale
so
when it comes from
the process
of the higher hierarchical level
now
cause things that don't change
on a faster time scale
there are lots of things we could have chosen
we could have chosen
the likelihood of major seats
or we could have chosen
the likelihood of a particular policy
we've actually chosen here just the initial set
but it means that
the outcomes from the genetic models
are in-plane
for the duration
of the same transitions at the lower level
and you can imagine
putting a faster level below this
a faster and a faster one
so you're writing in
you're baking in
to your genetic model
not only a high model
depth or abstraction
but also
a deep diachronic
one time depth
or abstraction
of temporal scalars
over time
and of course that's what we need to understand language
I will have a representation
of a sentence or a phrase
at one level
and that's the same sentence of phrase
from the beginning of the first phony
to the end of the last phony
it's the same object
but on a faster time scale
that this current word
will change
the word from the beginning
of the words
first time frequency died
phony possibly to the last one
and as we pinker
lower and lower and lower
we now generate faster and faster dynamics
using this kind of model
so you may be asking
and that's all that this equation says
they're just a hierarchy generalization
of the first copy
used in the rat
into this deep diachronic structure
this is exactly the same model
and the reason I show this
and the reason I like this model
is you can generate this
graph
automatically from this graph
and this graph is known as a factor graph
they may not mean very much
to psychologists
but if you're a computer scientist
and you want to
design the message passing in the most efficient way
this is the design
so
what we're saying here
or what this figure
says
if you can write down the form
of your geratin model
you have automatically
written down
the message passing graph
and at some level
a brain must be using
in terms of connections
and passing messages over
these connections
for those of you who are interested
factor graphs are interesting
because they place the variables
on the edges
and the probability distributions
at the nodes
that's why we call factor graphs
so the probability distributions are
the factors are quite active on the margins
you can forget that
it's interesting to
remember
is you can generate these things automatically
and once you've done that
you can start to make little brains
that could actually be
and a lot of systems
or very large scale
silicon integration chips
for example or classical computers
Turing computers
so
once you've written down the factor graph
and you've learned the architecture
of the message passing
you can actually go to neuroanatomy
and as we're on the isomorphisms
in terms of the structure
