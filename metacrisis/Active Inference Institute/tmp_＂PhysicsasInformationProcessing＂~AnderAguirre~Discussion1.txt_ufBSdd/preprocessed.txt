Hello, and welcome everyone. It's June 3rd, 2023. We're in the first discussion section
for physics as information processing course with Chris Fields. So welcome to all who are
here on and off camera. Really exciting to see so many people joining for this participatory
discussion section. So in this first interval, we're going to go to anyone who raises their hand,
and it would be awesome to hear from everyone who would like to share
what excites them about what they heard in the first lecture, how has their background and journey
led to them joining? What are they hoping to learn and do in this course? So
first hand, we'll get to go. Maybe, Ander, you can actually start first, just speaking as a
normal participant. And then meanwhile, anyone is welcome to raise their hand.
That sounds good. So I am a mathematician by training, but I've always been interested in the
more physics side of things. And in particular, I followed Chris's work,
worked pretty closely since probably 2019. And I'm just very interested in the whole
ontology of quantum measurement, what it means. I feel like very often when it's taught in physics
classes, it's hand waved away. And even though I myself subscribed to the philosophy of shut
up and compute, sometimes not everything, you need to do a little bit of philosophy maybe.
And this may be one of the things where you need to do it. And you need to address it from
first principles. And I think Chris is doing that. So I'm just here to learn more. And yeah,
that's basically it. Awesome. I'll just go from the top left because I'm not necessarily watching
like what order, but Ross first. Hi. I'm interested in the kind of overlap between cognitive science
and physics and seeing if there are any panpsychist implications of the observation that
physical processes seem to follow the same kind of principle that we see apply to
brains and cognitive systems.
Awesome. Thank you, Aval.
Hi, I'm Aval Gennacher-RÃ¼sch. I have mixed backgrounds,
mostly physics and cognitive science. Right now I could be qualified as a philosopher of
physics and cognitive science. And I'm interested in
musically formalizing participatory realism, the idea that
observer participates in creating reality. I did some work in formalizing that in the case of
social norms and things like language. And I met a block based on my lack of knowledge of quantum
physics and mathematical tools that I used for that. So I'm here in a course to get those formal
skills. Awesome. Thank you, Aval. Blue?
I think Alex was first. Okay. Alexi, go for it. Oh, sorry.
Hi, I'm Alexi. I'm a clinical psychologist in Maryland in the States. I work with patients
do psychotherapy mostly. My interest came through Mark Sohns, who is, you know, the creator and
leader of neuropsychoanalysis. And he has been collaborating with Friston on a hard problem
of consciousness. And he's now working on AI with emotions. And so he, you know, I blame him. I say
it with respect and love that for introducing mathematics into our field, which was mostly
devoid of mathematics. And people started looking at these partial differential equations and Mark
Blankets. So I wrote a paper about a possible extension of their model to include chaos theory.
And I just want to understand deeper concepts involved. I have some understanding of them,
but perhaps not enough. And I'm enjoying this course a great deal. I mean, I think Chris,
among other things, is a fantastic teacher. And he is able to express things very clearly.
And it just, I have, oh, I have goosebumps. So I'm very grateful to everybody. And thank you so much.
Cool. Blue, then Francesco. Hi, everyone. I'm Blue Knight. I'm with the Active Inference
Institute. So I've been tracking this work of Chris's, you know, connecting quantum information
and active inference for, I don't know, a couple of couple years now through that thread. I'm super
interested actually in the aspect of space time arising from communication. I think that that's
fascinating to dive into and how it connects to the, like, Buddhist and maybe more Eastern
religion concept of dependent arising, like how the interdependence of all things leads to their
existence. And so from that aspect, I think that that's what I'm most interested in exploring.
My background is neuroscience, but I've also been studying practicing Buddhism for like 25 years.
So, yeah, that's it. Thanks. Awesome. Francesco, then Corbi.
Thanks, Daniel. Ciao, everyone. Super happy to be here. I also have a mixed background because
I studied cognitive anthropology and philosophy of science. And currently, a few months ago,
I started a PhD in artificial intelligence in an AIDS application in the educational field.
And yes, I'm actually super excited about all the science-based implications
that Chris Fields mentioned in the first lecture, the connection. I have no background in physics,
but the connection between the first principles of physics and free energy principle and scientific
cognition in general is what is really getting me excited in these days. Thank you.
Cool. Corbi, then anyone else who raises their hand?
Hey, yeah, great to be here. My background is in quantum engineering. I've always been motivated
by figuring out what the next big thing in computing was in high school. Moore's law was
sort of coming to an end, and it seemed like a really big opportunity to figure out what the next
computing architecture might be. So I did academic research in no more computing,
quantum computing, and quantum sensors, and I felt like I was a little bit too early for
all of those things. So I went into cloud and cloud gaming and streaming AI services,
but I'm still extremely motivated to figure out what the next big thing is in the holographic
principle seems like a good foundation to explore that.
Awesome. I'll raise my own hand. Then anyone else is welcome to go. So I'm Daniel. I'm a researcher
at the Institute and have known Ander for some years and learned about all kinds of stochastic
matrices and how the math and the quantum were linked, and we had talked and joked about
learning from Chris Fields for a long time. So when the opportunity arose to enact it
and extend it and open it, we were very excited, and also don't have much of a formal background
in physics outside of working with Jacob and Ali on live stream 49.
So it's just going to be a great time to learn and see where it all goes.
Okay. Anyone else want to say hello in this kind of opening section?
Okay. Well, continuing forward, of course, please just feel free to raise your hand
anytime you'd like to jump into the stack under for several minutes or however you see,
could you reflect on lecture one? Where did Chris enter? How did he proceed and where did
the lecture take us? Thank you. Yeah, so you zoom in a little bit more on the PDF.
Can you hear me? Thank you. Go for it. Let's take a look at the slides from last time.
So I think if we were to summarize this whole lecture, basically what Chris was trying to say
is that there are many hands coming from different directions pointing at the same thing, right?
Pointing at the fact that information will have a central aspect
in physics in the future, but also possibly biology, right? So the earliest hints came from
statistical mechanics, if you wish, but also quantum mechanics, right? And here we need to make
like an important subtle distinction between what it is, the uncertainty principle, right? Because,
you know, we think of that as the informational part, right? Like a limit on the resolution
on information. And what is the observer effect? Just to make that distinction clear, I think
Chris is talking about the observer effect. The uncertainty principle is an artifact of the
wave nature of quantum mechanics, right? You can get it from properties of the Fourier transform,
Cauchy Schwarz inequality, but at the end of the day, it's what I was saying earlier, right?
What is the basic picture of measurement, right? And that's sort of what Chris with
Glaceburg and Marciano is trying to address. And the first two hints came from quantum mechanics
around 1900 and statistical mechanics a few decades earlier. Then there's, you know,
more stuff, right? Like, obviously, physicists resisted to inertia of hundreds of years of a
totally objective, you know, observer independent universe, but the hints continue to pile on.
Some stuff coming from math, but perhaps the most interesting is again, you know, how
all those paradoxes from statistical mechanics were made sense of with land hours
proposal. And then as physics got more fancy past, you know, QFT and stuff, you know,
people started thinking about quantum information more seriously. So we had
Bell's theorem and all these proposals by Deutsch and Wheeler and so on. And I should say that
this is not a fringe idea in physics. The paradigm of it from QFT is very much mainstream. I mean,
Google it, a lot of physicists think about it, you know, I don't know how many of them, you know,
talk about it openly, but, you know, it's been a widespread folk intuition, so to speak, that,
you know, information will have this sort of central role. And the last sort of hints pointing
in that direction of information having a central arc came on the one side from high energy stuff.
And the holographic principle is it originally arose in the context of black hole thermodynamics,
but then, you know, it was extended to, to, you know, what's called the ADS-CFT correspondence.
And the very most recent stuff was, you know, it's basically the first reason for having a central
arc came on the one side from high energy stuff. Can you speak a little more loudly?
It's all good. Continue on it.
Yeah, I thought there was a question, but someone was just unmuted. It's all good.
Okay. Yeah, so that's basically it, right? So let's, there's hints from many directions,
and most recently, you know, the work of Friston and Levin, coming from biology,
all pointing in the same direction, right, that we might have to make sense of some sort of
potentially scale-free information processing paradigm for what it means to be an observer,
you know, sort of all, all of these things are pointing in that direction. And I think
Chris's work is, is addressing that head-on, right? As I said earlier, you know, sometimes,
you know, it's, it's good to the computations, but sometimes you need to take a step back and think
more philosophically about what you're doing and what the bigger picture is.
And then maybe after people talk a little bit, I can, you know,
we can warm up to the second lecture. We can build our windows that I have open here.
Some of them are the papers that are to come.
Yeah. Anyone, please feel free to raise your hand on this, Alexi, and then anyone else.
So I had one thought about this thing, you know, when you've mentioned that there's various
perspectives on the same concept, and I'm trying to be cautious. On the one hand, I think, again,
this amazing clarity and precision with which Chris expressed thoughts is, is fantastic and
enjoyable. On the other hand, it requires simplification and generalization. And what I
observe in my field is people take a term like entropy and run with it. And then they get very
quickly from thermodynamic entropy to Shannon to, you know, whatever entropy, and they talk about
sort of, and that's not the case. I mean, if we go back to Clausius and all the others, there was a
system of postulates and axioms that must be maintained. And if you step outside of that,
so like us organisms, we're open systems, we're not closed systems. So we should not apply directly
second law of thermodynamics. And, you know, I deal a little more with Kolmogorov's and I entropy,
and which is not the same thing. And I guess when we very quickly move from sort of thermodynamics to,
you know, to statistical mechanics with Boltzmann to, you know, all the way around,
that we assume it is the same thing, I think we may need to be careful because we're just
ignoring the differences in assumptions. You know, Shannon talked about telegraph,
he talked about a receiver getting a message from a sender, right? And Clausius talked about,
you know, gas. So, but, but I do think that's a balance in that. And there's no way to not
have simplification and generalization. I just wanted to say somewhere, somehow we need to kind
of, you know, go back there and just say, you know, entropy is not entropy is not entropy.
And all these things are kind of, you know, long capers, right?
Yes, Jacob, then Ali, then anyone else?
I just wanted to ask for, I guess, perhaps, slightly longer comment from Alexei on
that what you see as the fundamental differences in different descriptions of entropy and whether
there is some, I guess, general concept that could be extracted from all of them such as
level of information or uncertainty about a particular system, even though these specific
formulations of entropy they talk about, they were derived from different specific systems and
they have different equations describing them and they mean slightly different things.
Do you think it would be wrong to say that there is some commonality between them?
Did I answer Daniel or? I don't know, I just think we need to, at some point,
formalize it and be careful with these generalizations. For example, when we talk about
active inference and Friston, right, you know, when we say living organisms must minimize,
I'm quoting Friston, by the way, these are not my thoughts, the entropy of sensory states,
and then people take that idea sometimes in generalize and they say that that's what life is,
minimization of entropy, right? But if you actually look into human brain and measure entropy,
then it goes all over the place, up, down, and in between, and so I don't think that's an
accurate description and the reason for that is we're shifted away from the assumptions and,
again, so Kolmogorovs and I-entropy talks about the phase space and the divergence of
trajectories in phase space and sort of the volume in phase space, while when you talk about this
Friston's entropy of sensory states, these are probability distributions and Shannon talked
about the uncertainty of the receiver about the message sent by the sender. So if you want to
extract the same thing, I suppose you can, but it's important not to lose at least the difference
between what kind of object are we applying to? Are we looking at probability distributions?
Are we looking at phase space? Are we looking at ideal gas? Where are we at? And we're just
not to say entropy is entropy is entropy. That's not the case. I mean, it's just a formula,
but depending on the context assumptions and things, it can be very different.
Thanks, Ali and Ava.
Yes, in regard to Alex's comment, I just wanted to mention the research of people like Brutal
Fanel and Stefan ThÃ¼rner, because for many years, they've been exploring this idea of different
conceptions of entropy and they've come up with at least a dozen different meanings for entropy
and they formulated it in a kind of generalized framework. But at the same time, they've also
devised a way to somehow classify them into equivalence classes and to see whether one or
more of those conceptions of entropy can be theoretically derived from some others or
they're totally distinct and different conceptions of entropy. So yes, I agree that in some
situations we need to be more specific about what exactly we mean by entropy and information
I mean, based on the question or the situation of interest.
Thanks, Ava.
So just to vote in the entropy discussion, I want to emphasize that entropy is PNAB,
so it's a function of probability distribution, which is, I think, I'm not sure what the term
is in English, but it's a measure of how big something is in a space. And to get that, you
have to define a space. And in many cases, it's pretty arbitrary to decide what the space you
look at is. For example, you talk, I don't know if it was, let's say, someone talked about
entropy of fMRI. Well, those are aggregate measurements of neural activity. This is not
the space of all possible configuration. And even if I add something like the space of all,
let's say functional configuration, the state in your own that reflect activity, this would be
another thing than the space of, I don't know, atomic configuration, where the specific atom are.
So, yeah, you don't have a warranty that's when you define entropy, you define it on
a meaningful space that's not given. So let's, I kind of think there is one entropy and that's
it and it's an infoconstruct. But also we have to, if we agree to that, then it entails that we
have to find it, that it's not just given out there in the data.
Well, certainly being precise in the formalism and in natural language for what we mean by
information and entropy and all these terms is pretty important. I mean, the course we're in
is physics as information processing, but without an understanding of how information is being meant
here, or what measurements and transformations were performing, what informational operations, then
it is a non sequitur. So how do we go about clarifying or understanding what kinds of theoretical
results in a given domain or like what is shown on the timeline here might apply to some other area?
Just one thought. Anyone else can raise their hands with a thought or a question?
Otherwise, Andre, you can maybe flip over to select a few of the submitted questions.
Cool. Could you zoom in with control plus a fair amount?
Thanks. So everyone, we really appreciate the submitted questions. We've been able to get
13 answered by Chris so far. And as with the transcripts of the discussions and lectures,
the questions and answers are going to be part of what is published by the active journal.
So we really encourage basic questions, advanced questions. Just there's no
question that doesn't make sense to submit. So under maybe just pick one question
that you think's interesting to begin with and summarize Chris's response. And then everyone
else is welcome to kind of add some other thoughts on this. Yeah. So I'm going to scroll up and down
just to sort of sample. Let me start with the first two. Can you hear me well? Yep. Since
perhaps I think you submitted this as a trial, right? Is that right? Yeah. So what is information?
What is time? And here's Chris's response. I guess everyone has a chance to look at it.
Now, the one if I am to summarize this, the one thing I've heard that, you know, personally,
the one I like the most answer at least as an adage to summarize it is that information
are differences that make a difference. This I might have heard from Chris. I might have heard
from someone else. I don't remember. But that's one thing. Now onto the question of what is time.
And I guess this also addresses Alex's question of what is entropy. I
so I'll make a bigger point now. What is time? This is partially addressed. And I guess the main
thing I'll be doing here is rather than look back, look forward into what's to come in the course.
And in particular, I'm thinking about
these two papers. And in particular,
particular. And I've had a chat with Chris about this. This picture over here. Okay.
Excuse me a little bit more. Yeah. I'll get back to it in a bit. But basically, this
is sort of like kills two birds with a stone, right? This picture and especially the text
around it and the title of this paper is the physical meaning of the holographic principle.
Okay, you guys can see here archive 2020, 2210. Anyways, this addresses the notion of, you know,
the time on entropy because the point that's the claim that's made in this paper is that these
are local observer relative definitions. And we can look into more detail around it. But basically,
the coordinate TA time is an entropic measure. So it is non decreasing. And it goes up with
the observer measuring entropy in the system. Because of the symmetry of the definition,
the second law applies to both. So both observer and environment see entropy increasing
in their environment and vice versa. Anyways,
from this picture, one gets that you have this internal circle time QRF. So this just
is a necessary artifact of recording information into a memory. That's the claim made in the paper.
And that essentially induces the passage of time, at least from the perspective of the observer.
It also, I think, Alexi was talking about, which I totally understand and agree that
this different notions of entropy. Now, I suppose as the, you know, I would be the one to
like definitions and so on. But I do think it's good to have a certain amount of flexibility.
So I see the same as energy, right? Like there are different kinds of energy, you know,
there's energy mass equivalences. And, you know, if you push it into supersymmetry,
you can think of it as equivalent to force also. But I do think there is some value in being
sloppy or intuitive about the notion. So the fact that, you know, entropy, yes,
there are different definitions. I don't see it quite as a problem. You know, one can explore
here around this page. There is a definition of, you know, local entropic time. And,
you know, one can think of it as, you know, space base or we can think of it as, you know,
coding theory, but it is at the end of the day, I do think it is true that both of them are hinting
at the same direction, just like, you know, you may have different kinds of energy, but, you know,
they're not unrelated concepts. And, yeah, I hope one day we get very precise definitions
out everything. But for now, I think it's good to speculate. Okay, there's a question. Yeah,
Corby, and then anyone else on this? Yeah, quick question. So I haven't read that specific paper,
but is there any work on the frequency or the rate at which one could read and write information
on the boundary? Yes. Well, I don't, I don't, I can't think of the paper exactly, but this
is lower bounded. I think Chris talked about it. If not in the last lecture, I think I've seen a
talk of him on YouTube, it's lower bounded by the uncertainty principle, right? So, so for instance,
plans, Planck's constant is in units of action, which is energy times time, right? So Landau's
principle says you need a certain amount of energy to erase information. And, and you have time,
right? And then you have an energy time uncertainty principle, putting those things together, right,
you have any minimum amount of action, which is, you know, literally the minimum,
the action is measuring what I said earlier, right? Like differences that make a difference,
the minimum difference that you can make. So the minimum difference to, you know, erase a bit
or rewrite it on the boundary, so to speak, right? So yeah, that would be given by the
uncertain relation, energy times time, I don't know exactly what the figure is, but it's obviously
very small compared to everyday life. Got it. And with, is there work on the upper bound,
like for example, would it be the number of degrees of freedom on boundary?
I don't think it's bounded from above, but, but obviously, you know, then we do want to
bond it from above for technological reasons, if you wish, but I think that is a, an engineering
problem, but not one of, you know, physics.
Andre, a few, few questions on this figure. What are Y and E? B, the blue oval, as Chris has
pointed out, is going to consistently be the screen, the blanket. And so we often associate
that with the, at least some blankets include these information gathering and utilizing systems.
So what is happening internal to the agent here? What kind of computation is happening?
And is this something that is actually proposed to be occurring?
Or is this a schematic or a map of how we could think about something occurring?
So I think this question is going to be better addressed by Chris himself, probably in the next
lecture. For now, what I can say is I can reiterate what I was trying to say earlier, which is that
what these addresses and, you know, this basic measurement picture, so to speak,
is the observer effect, right? Like this notion that you cannot observe something without,
without disturbing the environment. So if you then try to make sense of this by constraining,
you know, from first principles, land hours principle and so on, basic requirements on
information processing from the point of view of energy, then you get that this boundary
is constrained in some ways, right? This Y is what's called the memory sector, meaning that
you don't just act on the immediate information that you perceive. You may also act on,
you know, staff that's distorted, that you have perceived before.
And then sure, E, the red part is what you're perceiving right now, the environment.
But actually, I don't know if I have it in here.
But this is further broken up into other parts. So not schematically here. Let me try to find it.
Now, I don't know. Actually, bear with me for a second. I think you might be here.
That's the same picture.
Anyways, I'm not, I don't want to waste time by trying to find it. But back to the original picture.
Yeah, sure. E is what you're measuring. And again, Chris will talk about this in more detail.
But from first principle assumptions, they deduce that you're going to have to, you know,
deploy your finite resources for gathering information in clever ways on the boundary,
right? So you're going to have to deploy some resources for memory, some resources as a free
energy, as a heat sink, right? So you're going to be processing information. So you're going to be
dumping, you know, so-called low quality stuff into the environment, you're going to be dumping heat.
And then older than the memory, you're going to be focusing on what's going on right now.
And this environment, then what I wanted to say is that it can be further broken up into
what's called pointer and reference sectors. So you can imagine this
E sector here has to be broken into further sectors. Because then there's the other first
principle notion, okay, so what does it mean to observe something? The differences that make
a difference, right? So you can only, there's only information with context, right? There is no
information per se. It's all semantics, right? So you only observe things in context. So some
things are going to have to stay the same, namely, you know, reference states here, and some things
are going to be, right? So you can think of this as the gauge in your car, right? Obviously, the
circle stays the same, right? And that's your context. So to speak, that's with the, you know,
numbers in it, they never go away. They stay the same. But as you speed up around, the gauge is
going to change. And that's how you know only in that context, you can tell your own speed
when you're driving the car, right? So anyways, this is just a teaser for things to come.
Now you can go back to more questions. Then I'll, I'll pick maybe two or three more over here.
Yeah, pick another question or anyone raising their hand, or if anyone in the live chat wants
to write something, I'll read it. How about the wick rotation? Could you summarize what the wick
rotation is, and then state this question about it? So that's a great question. I knew the answer
better. I wish I knew the answer better. But, conformally speaking, the wick rotation is just,
so it's Chris said, you know, you take a quantity multiplied by i square root of negative one,
that amounts to a 90 degree rotation, a complex plane. Fine. Now that appears to be a trick.
And what it does effectively is take you from quantum mechanics to statistical mechanics,
roughly speaking. Now, why it works, that's probably, I mean, I have no clue. I don't know
the answer myself. But now when things appear to be a trick and they get you something, maybe
there's more to it than what's, you know, apparent at first. But I myself don't know the answer.
It's a very good question. But I do think you may have to do with the fact that
on the one hand quantum and statistical mechanics, both are talking about
constraints on information processing, maybe from different perspectives. But again, that's
just a speculation on my end. So it's maybe a way of relating the two.
Do we just review, just read Chris's full answer to this, read the question and then Chris's full
answer? So as the wick rotation, so as the matter of energy, somehow it's just 90 degrees per
perpendicular to three dimensional space. And then I'll just read Chris's answer. Then anyone,
please raise your hand if you want to like add something more on this. He wrote, in a sense,
yes, time is perpendicular to our three dimensions of space, matter and energy, we ourselves exist
in time. Wick is pointing to the intimate relationship between how we measure time and how
we determine or judge that something maintains its identity through time, i.e. stays the same
thing through time. To tell time by a clock, I have to be sure I'm looking at the same clock,
set the same way, etc. We'll get to this in July and August.
Could you connect this on to some of the semantic information flows
discussed in live stream 17 or any of the other cone, co-cone diagrams from the papers that you
showed earlier? Sure, again, I have to practice it by saying that I am working on understanding
it myself. But just from, you know, small email exchange that I had with Chris and
what's that in this paper, one can sort of see that it's related to the picture I was showing
earlier, namely this one, right? But let's scroll down a little bit to page
to page 23, 24, surely talk about the rotation again.
So I think it has to do with, at least from what I heard from Chris,
the email has to do with this notion of local time, the definition of local time
in this observation picture. And, you know, the fact that you rotate 90 degrees twice,
you get the opposite sign basically, right? So the local time, effectively that's going to
ensure that the picture is symmetric and that, you know, the flow is going to be opposite in
the direction towards the observer and towards the environment.
Okay, Alexei, then anyone else raising their hands?
I am struggling to understand and part of my difficulty bear with me for being so kind of,
but I remember Sean Carroll's lecture on time in different versions of physics and it took
a whole hour. So are we talking Newtonian time, which is universally the same? Are we talking
relative time? You know, philosophically, are we talking about people who believe that only
present here and now is real while future and past do not exist? You know, so when we say time is
perpendicular to, my sense is that just like Chris mentioned in the lecture, we're reinventing the
time. We're creating a new definition by calling it the same word we're all familiar with and that
creates some dissonance where I just think that maybe we need to kind of say that this is something new
and if it's not something new, then which version of the time are we talking about Einsteinian,
Newtonian or whatever, you know? It's a simple question if we, before we say it's perpendicular,
do people agree that future is real or not, you know?
I'll give a thought on this, Alexi. Again, speculation, neither here nor then.
If we take Chris seriously, that communication is an ontological primary and that rather than
putting communications protocols in some sort of time, Newtonian time, Einsteinian time, these
are these different time concepts, these different scaffolds that then communication gets inscribed
within or stretched within, but if we start with a topological understanding of communication
rather than a geometric understanding of spacetime, so that's apology of the blanket and the observer
in their sensorium, then different kinds of cognitive agents ranging from what we would
call non-living to what we could call living, they, depending on their cognitive sophistication,
are able to create or project different time concepts. Time concepts that are not useful
will lead to their own material disillusion, the failure to maintain that kind of cognitive
sophistication, which requires not just information gathering and utilization, but also energy
gathering and utilization. And so when you have multiple agents who are, whether you conceptualize
them as within a message passing relationship and having no surprise on the clock on the wall,
then they're like in a local time synchrony, but the primary case is one entity
engaging with its sensorium and its self-modeling time in terms of progression.
And so these previous time concepts and these different attempts to kind of survey the
structure of spacetime reflect different cognitive models that cognitive agents have proposed
for time, and that communication actually like takes ontological primacy over all of these ramified,
potentially overlapping, potentially non-overlapping time concepts,
but they're downstream of agents' cognitive models, not vice versa. I'm not sure, it's how I
interpreted what Chris said about communication being an ontological primary though.
So if we take one instance of a COVID-19 virus, a single kind of instance, it's got no memory
and no ability to plan. Is there a local time for that virus versus humanity observing how the
virus spreads across the globe? And we have agents with memory and planning. So
is there a time relative to a single photon? My hunch is that there's a possibility that we're
simply inventing something brand new and defining it brand new, and I just, the drawing parallel back
to all these other times, I understand the effort, but it's just, is that a brand new time? Is that
some other kind of time? I don't know. Daniel, can I say a few words? Yeah. So I understand Alex's
question, and I think it's sort of related to, at least morally speaking, to your concern about
different kinds of entropy, right? And what I can say is that also they have Glarey's group,
Marciano, and so on. They have this recent paper on quantum error correction.
So I have yet to read it and take a closer look, but surely talk about space time, right?
And my punch to address Alex's question is that if you could sort of recover the more familiar
notions of time in a correspondence principle sort of way, by taking some Newtonian limit,
or h bar goes to zero, or speed of light goes to infinity, and so on, that would be very
satisfactory. Sure, they may be doing a new thing, but regarding all stuff and the new thing
is a good sanity check. And so long as we can see that being worked out, I think I'd be very happy.
And just to the virus point, I would argue that viruses through their physical engagement, they
do have memory, but they don't necessarily have the awareness or a metacognition on that memory,
except as granted by intergenerational evolutionary type pressure. But they do have past events
modifying them within some set of possibilities. So they're just being them.
And their internal time is on, there isn't a cognitive observer necessarily,
in terms of like a narrative I, I am watching the clock, but they do change.
But it's certainly very provocative if this is not or we could also look to Kronos and Kairos,
and about the relationship of time and timeliness and agentic time.
So I think there's, and then at this point, are we stretching just this one four-letter word too far,
just like we brought up with information entropy? I mean, can we think about these as kind of like
continents? And then just if someone said I visited this continent, you know that you need to specify
a lot more? Or what granularity are these topics? Corby and then anyone else raising their hands?
Yeah, on this topic, for a Markov blanket, you have a transition matrix. My question is, how do you,
how are you allowed to increment that transition matrix to calculate a future state? Like, what
does that mean in terms of time? Are you only allowed to increment it by like the lower bound time,
which is h over ln kbt? Does that question make sense?
I'll give a first pass on this. We have discrete time and continuous time models in active inference.
For a discrete time model where you have the B transition matrix, then the choice of delta t
is a modeller's decision. And we talked about this a lot in live stream 52 on accelerated
optimization. There's some delta t that's like going to get you the most accelerated optimization.
If you have too small of delta t, then you're either oversampling or you're maybe even physically
unrealistic, like you pointed to, or if you have some long delta t, you're also not making an
informative simulation in the discrete time case where you're explicitly making time steps.
Contrast with the continuous time generative model, where we're doing more of a Taylor series
expansion on the generalized coordinates at which point delta t is not required,
you're merely just taking an extended approximation from a given snapshot. And so delta t doesn't
even come into play. Got it. Okay, that makes sense. I don't know how it ties into time, but
it helps clarify a little bit. Okay, anyone else can raise their hand or under kind of
maybe one more question or one more topic from a paper, just what's one more puzzle piece that
you think gives us a first coat of paint from questions submitted or from one of these papers
that you have up? What's something that helps us connect the dots a little bit?
So I don't think I have a good answer. Let me look at the questions again.
So regarding time, I'll just reiterate what I said as I look over these questions, which is that I
believe Chris will address it in this bit. I don't know if you can see what I'm sharing on the
screen, but the paper who styled this communication protocols, blah, blah, blah, from March of this
year, does talk a lot of space time. It's a quantum error correction code. And to me personally,
it'd be nice to see that can be more familiar notions of time, such as special relativity
can be recovered as limits in correspondence principles. Yeah, do you want me to say anything else?
If anyone could even just summarize like what would these what are these codes referring to?
Or how are these codes one and the same as communication protocols?
Or are they related to the blanket and the the heat that was being output as waste?
What will what will these codes allow us to do better if we understood them more?
I don't know that I can answer that right now. I don't I don't think they understand the question.
Um, yeah, I just don't know. Sorry.
Terry, and then anyone else raising their hands?
Um, I'm not a physicist. I don't understand maths at all.
Um, but what attracted me to this, um, idea of physics is information processing is that,
um, it feels like at a base level, the whatever the X on noble external state is,
it could be perceived as made up of information. You know, we kind of think of ourselves living in
a particular universe, but, you know, at a base level, there are these and stop me if I'm just
talking 14 year old nonsense here, but, you know, this notion of quantum fields, they feel like
informational fields. And, um, that the concept of physics is purely information that we then
interpret as a physical universe, which we inhabit because what we can't see beyond our
Markov blanket. And I find this a really exciting idea that as a that whenever you start to think
of the, um, surprise all as the, um, variational free energy between, you know, sort of your,
your prior and your posterior, if you start to think of that variational free energy, not as
energy, but as an information gap. So that if you see the energy as actual, you know, because
energy, when you think of energy and physics terms, it's kind of this magical thing that does,
that allows work to happen or, you know, but, but, but what is it, you know, we've got a term for it,
and we've got, we've got, we've got a, you know, is it jewels, you know, or whatever, you know,
but, but, but what does it mean? It has this magical property. But if you think of it as an
information, um, then it starts to make sense. So variational free energy starts to be seen
as a difference in information between what you, uh, your, your prior and your posterior.
So whenever you start to think about time, time is this conceptual metric that we use
almost because we have memory. And, um, I wonder if we are getting, uh, trapped in this semantics.
And all I have is English because I don't understand the maths really. But, um, we're, we,
we, you know, you guys come in here and you do understand the maths and then you convert it
into English, which I think I understand. And then I can think of it like a kid in a
Swedish shop or a physics class at the 14 years of age. I don't really know what I'm saying. I
don't know what I'm saying, but at a fundamental level, everything that we're talking about is
in for, I, I, I can understand this as information and I'm excited to see how things develop. What
are your thoughts on this craziness?
So many, uh, great pieces in there. Physics as information processing. What is it that actually
allows the steam engine to run and the ball to move that kind of physics? And then as the timeline
laid out, also occurring during a period of, of immense technology and, and apparatus
development and, and digitization and all of this and unconventional computing as well,
it's almost like this, uh, materialist dualism that instead of with, with mind and body, that
there was some kind of dualism between the types of forces and energies and ways of thinking about
what made the steam engine work and then what makes a computer work. And so you'd understand
how the computer could fall off a cliff, but then you wouldn't necessarily be able to use that exact
kind of mechanics and physics and dynamics to study like how the computer operated. But you
could say it weighed 11 pounds. And so I think the, the provocation and then, which is as you
hinted towards, um, first off would be awesome to have a natural language, math and graphical
synthesis, the triple play, but also then to use a common approach to understanding this and what
wasn't working or is not proposed to work is trying to fit information into that non cognitive
universe. But rather if we take the cognitive primacy of communication, then all of these forces
as inferred by the sensorium, which is all we're ever going to have totally the most valid first
principles constraint to make, maybe then we'll be able to have a lot more synthesis about even
how steam engines work when we understand that we're bound in that inference by our sensorium
and all that comes along with that and the variational free energy bounding surprise on our
generative model. Dean? I'm going to, I'm going to come at this from the position of a 14 year
old lens because I don't think there's anything unique about that. I think on most basic level,
what I took away from Chris's lecture was that there are, that there's information processing
of at least two types. One is figuring it out as in figure five or any of these other figures
that are up on the screen right now or anything that stabilizes the world around us. I think
examples have come up in this conversation already of topology or geometry. There's a second type of
information processing, which is outing the figure type of information, which is searching.
And I think Alexi used the expression, which time are we talking about? So these two types
of processes are quite different. If you do a comparative analysis of the two, you have information
practicing and you have to have both of the processes to really get a sense conceptually of
what it is that the communication is really talking about. If you are information practicing
your wayfinding until you collapse to something we might describe in a verb tense as wayfound.
It's in the quantizing of that invariant state, that vast ocean, if you want to think of it
metaphorically, out to all horizons on the manifold into a variant or a nuanced or a discretized
or an orientation type of state where all of a sudden things like currents and temperature ships
and even things that we've applied labels to like entropy and a transition matrix suddenly appear
out of the seeming nothingness or the invariance. So that's not the math, but that's kind of stepping
back and saying what is the math trying to say? I think it boils down to the comparative analysis
and then I think I'm hoping that in the coming lectures Chris starts opening that up and doing
kind of the back and forth between the two types of figuring it out and outing the figures because
I think when both are part of the conversation, that's when we start to get a sense of what's
really going on. Thank you. In this sort of closing sessions, especially if anyone hasn't
added anything, please feel free to give any last reflections or questions arising.
So first, as many people as would like can reflect that way. And then after every hand has been
addressed, then, Andrew, if you could just give us a kind of look ahead. So Avel first, then anyone
else with just where does this discussion take us and what do you want to continue to learn more
about, Avel? So, two things. First, if I walk out of here without understanding what is a
corn cocoon, I will be disappointed. And I don't actually expect it to happen in like
12 hours. That seems like it's something. But more seriously, as a colleague, I can agree
without difficulty that there is a unity between my physical structure and communication protocols.
And that communication protocols or quantum reference frame kind of individuate physical
states, individuate meaningful measurement outcomes that are verified as physical possibilities.
But the question is then where do communication protocols slash quantum reference frame come
from? Like what are the physical determinants that bring about those things? And that seems
like a question that could be addressed at a conceptual level of this course. And I would like,
I saw the livestream 17, so that did not help. Anyway, I would like to have more grip into that
if we can translate it into the foreign courses. Thank you. Thank you.
Anyone else, especially if they haven't added anything yet?
I want to give kind of a closing thought.
Yeah, there's a lot to learn. And Chris brings up a lot in his lecture, even though we've only
had one, it's one of the denser hours of epistemic resourcing that I've seen. And there's also all
of these accessory and cutting edge works that that under showing. So I'm really looking forward to
how we can continue to carry and improve the questions that we have.
So that the uncertainties in our generative model that we have, how would this be useful?
What does this mean? Why is this variable this way? What is the implication for that?
Those are all the great questions. And let's just get them inscribed on our classical documents
so that we can have other quantum cognitive agents reduce their uncertainty
by looking at the responses. So with that, under if you could just look ahead to lecture two
and just like point towards where we're going to be going.
Yes. So so lecture two will I think from what I heard from Chris is going to start to address
these basic ontological aspects of the quantum measurement. So the two sources to look at would be
two papers, mainly this one physical main of the holographic principle. This one's by far the
most pedagogical one. So if I had to suggest that folks look at one, it would be this one.
It's very easy to read, I think. But maybe for a bit more detail, there's the slightly older one
and they talk at length about cone, coco and diagrams here.
Free energy principle for generic quantum systems. So I think that's what's going to come
in these two lectures, basically the main ideas behind these two papers.
Anya, that's all I have to say.
All right. Well, thank you for all who joined this first discussion. Hope that it was
fun for you and for the audience and check out the course site for the readings to get ahead
of the second lecture. We'll all watch the second lecture and come back
in about a month for the second discussion. Everyone's welcome to join and the website
has the registration information. And so now that we've seen a little bit of one way it can be,
we may do something similar or we may do something different for the second discussion.
So thanks again, everybody. See you later.
Thank you.
