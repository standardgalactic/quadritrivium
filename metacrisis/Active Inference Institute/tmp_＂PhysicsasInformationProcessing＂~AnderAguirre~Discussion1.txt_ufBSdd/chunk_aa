Hello, and welcome everyone. It's June 3rd, 2023. We're in the first discussion section
for physics as information processing course with Chris Fields. So welcome to all who are
here on and off camera. Really exciting to see so many people joining for this participatory
discussion section. So in this first interval, we're going to go to anyone who raises their hand,
and it would be awesome to hear from everyone who would like to share
what excites them about what they heard in the first lecture, how has their background and journey
led to them joining? What are they hoping to learn and do in this course? So
first hand, we'll get to go. Maybe, Ander, you can actually start first, just speaking as a
normal participant. And then meanwhile, anyone is welcome to raise their hand.
That sounds good. So I am a mathematician by training, but I've always been interested in the
more physics side of things. And in particular, I followed Chris's work,
worked pretty closely since probably 2019. And I'm just very interested in the whole
ontology of quantum measurement, what it means. I feel like very often when it's taught in physics
classes, it's hand waved away. And even though I myself subscribed to the philosophy of shut
up and compute, sometimes not everything, you need to do a little bit of philosophy maybe.
And this may be one of the things where you need to do it. And you need to address it from
first principles. And I think Chris is doing that. So I'm just here to learn more. And yeah,
that's basically it. Awesome. I'll just go from the top left because I'm not necessarily watching
like what order, but Ross first. Hi. I'm interested in the kind of overlap between cognitive science
and physics and seeing if there are any panpsychist implications of the observation that
physical processes seem to follow the same kind of principle that we see apply to
brains and cognitive systems.
Awesome. Thank you, Aval.
Hi, I'm Aval Gennacher-RÃ¼sch. I have mixed backgrounds,
mostly physics and cognitive science. Right now I could be qualified as a philosopher of
physics and cognitive science. And I'm interested in
musically formalizing participatory realism, the idea that
observer participates in creating reality. I did some work in formalizing that in the case of
social norms and things like language. And I met a block based on my lack of knowledge of quantum
physics and mathematical tools that I used for that. So I'm here in a course to get those formal
skills. Awesome. Thank you, Aval. Blue?
I think Alex was first. Okay. Alexi, go for it. Oh, sorry.
Hi, I'm Alexi. I'm a clinical psychologist in Maryland in the States. I work with patients
do psychotherapy mostly. My interest came through Mark Sohns, who is, you know, the creator and
leader of neuropsychoanalysis. And he has been collaborating with Friston on a hard problem
of consciousness. And he's now working on AI with emotions. And so he, you know, I blame him. I say
it with respect and love that for introducing mathematics into our field, which was mostly
devoid of mathematics. And people started looking at these partial differential equations and Mark
Blankets. So I wrote a paper about a possible extension of their model to include chaos theory.
And I just want to understand deeper concepts involved. I have some understanding of them,
but perhaps not enough. And I'm enjoying this course a great deal. I mean, I think Chris,
among other things, is a fantastic teacher. And he is able to express things very clearly.
And it just, I have, oh, I have goosebumps. So I'm very grateful to everybody. And thank you so much.
Cool. Blue, then Francesco. Hi, everyone. I'm Blue Knight. I'm with the Active Inference
Institute. So I've been tracking this work of Chris's, you know, connecting quantum information
and active inference for, I don't know, a couple of couple years now through that thread. I'm super
interested actually in the aspect of space time arising from communication. I think that that's
fascinating to dive into and how it connects to the, like, Buddhist and maybe more Eastern
religion concept of dependent arising, like how the interdependence of all things leads to their
existence. And so from that aspect, I think that that's what I'm most interested in exploring.
My background is neuroscience, but I've also been studying practicing Buddhism for like 25 years.
So, yeah, that's it. Thanks. Awesome. Francesco, then Corbi.
Thanks, Daniel. Ciao, everyone. Super happy to be here. I also have a mixed background because
I studied cognitive anthropology and philosophy of science. And currently, a few months ago,
I started a PhD in artificial intelligence in an AIDS application in the educational field.
And yes, I'm actually super excited about all the science-based implications
that Chris Fields mentioned in the first lecture, the connection. I have no background in physics,
but the connection between the first principles of physics and free energy principle and scientific
cognition in general is what is really getting me excited in these days. Thank you.
Cool. Corbi, then anyone else who raises their hand?
Hey, yeah, great to be here. My background is in quantum engineering. I've always been motivated
by figuring out what the next big thing in computing was in high school. Moore's law was
sort of coming to an end, and it seemed like a really big opportunity to figure out what the next
computing architecture might be. So I did academic research in no more computing,
quantum computing, and quantum sensors, and I felt like I was a little bit too early for
all of those things. So I went into cloud and cloud gaming and streaming AI services,
but I'm still extremely motivated to figure out what the next big thing is in the holographic
principle seems like a good foundation to explore that.
Awesome. I'll raise my own hand. Then anyone else is welcome to go. So I'm Daniel. I'm a researcher
at the Institute and have known Ander for some years and learned about all kinds of stochastic
matrices and how the math and the quantum were linked, and we had talked and joked about
learning from Chris Fields for a long time. So when the opportunity arose to enact it
and extend it and open it, we were very excited, and also don't have much of a formal background
in physics outside of working with Jacob and Ali on live stream 49.
So it's just going to be a great time to learn and see where it all goes.
Okay. Anyone else want to say hello in this kind of opening section?
Okay. Well, continuing forward, of course, please just feel free to raise your hand
anytime you'd like to jump into the stack under for several minutes or however you see,
could you reflect on lecture one? Where did Chris enter? How did he proceed and where did
the lecture take us? Thank you. Yeah, so you zoom in a little bit more on the PDF.
Can you hear me? Thank you. Go for it. Let's take a look at the slides from last time.
So I think if we were to summarize this whole lecture, basically what Chris was trying to say
is that there are many hands coming from different directions pointing at the same thing, right?
Pointing at the fact that information will have a central aspect
in physics in the future, but also possibly biology, right? So the earliest hints came from
statistical mechanics, if you wish, but also quantum mechanics, right? And here we need to make
like an important subtle distinction between what it is, the uncertainty principle, right? Because,
you know, we think of that as the informational part, right? Like a limit on the resolution
on information. And what is the observer effect? Just to make that distinction clear, I think
Chris is talking about the observer effect. The uncertainty principle is an artifact of the
wave nature of quantum mechanics, right? You can get it from properties of the Fourier transform,
Cauchy Schwarz inequality, but at the end of the day, it's what I was saying earlier, right?
What is the basic picture of measurement, right? And that's sort of what Chris with
Glaceburg and Marciano is trying to address. And the first two hints came from quantum mechanics
around 1900 and statistical mechanics a few decades earlier. Then there's, you know,
more stuff, right? Like, obviously, physicists resisted to inertia of hundreds of years of a
totally objective, you know, observer independent universe, but the hints continue to pile on.
Some stuff coming from math, but perhaps the most interesting is again, you know, how
all those paradoxes from statistical mechanics were made sense of with land hours
proposal. And then as physics got more fancy past, you know, QFT and stuff, you know,
people started thinking about quantum information more seriously. So we had
Bell's theorem and all these proposals by Deutsch and Wheeler and so on. And I should say that
this is not a fringe idea in physics. The paradigm of it from QFT is very much mainstream. I mean,
Google it, a lot of physicists think about it, you know, I don't know how many of them, you know,
talk about it openly, but, you know, it's been a widespread folk intuition, so to speak, that,
you know, information will have this sort of central role. And the last sort of hints pointing
in that direction of information having a central arc came on the one side from high energy stuff.
And the holographic principle is it originally arose in the context of black hole thermodynamics,
but then, you know, it was extended to, to, you know, what's called the ADS-CFT correspondence.
And the very most recent stuff was, you know, it's basically the first reason for having a central
arc came on the one side from high energy stuff. Can you speak a little more loudly?
It's all good. Continue on it.
Yeah, I thought there was a question, but someone was just unmuted. It's all good.
Okay. Yeah, so that's basically it, right? So let's, there's hints from many directions,
and most recently, you know, the work of Friston and Levin, coming from biology,
all pointing in the same direction, right, that we might have to make sense of some sort of
potentially scale-free information processing paradigm for what it means to be an observer,
you know, sort of all, all of these things are pointing in that direction. And I think
Chris's work is, is addressing that head-on, right? As I said earlier, you know, sometimes,
you know, it's, it's good to the computations, but sometimes you need to take a step back and think
more philosophically about what you're doing and what the bigger picture is.
And then maybe after people talk a little bit, I can, you know,
we can warm up to the second lecture. We can build our windows that I have open here.
Some of them are the papers that are to come.
Yeah. Anyone, please feel free to raise your hand on this, Alexi, and then anyone else.
So I had one thought about this thing, you know, when you've mentioned that there's various
perspectives on the same concept, and I'm trying to be cautious. On the one hand, I think, again,
this amazing clarity and precision with which Chris expressed thoughts is, is fantastic and
enjoyable. On the other hand, it requires simplification and generalization. And what I
observe in my field is people take a term like entropy and run with it. And then they get very
quickly from thermodynamic entropy to Shannon to, you know, whatever entropy, and they talk about
sort of, and that's not the case. I mean, if we go back to Clausius and all the others, there was a
system of postulates and axioms that must be maintained. And if you step outside of that,
so like us organisms, we're open systems, we're not closed systems. So we should not apply directly
second law of thermodynamics. And, you know, I deal a little more with Kolmogorov's and I entropy,
and which is not the same thing. And I guess when we very quickly move from sort of thermodynamics to,
you know, to statistical mechanics with Boltzmann to, you know, all the way around,
that we assume it is the same thing, I think we may need to be careful because we're just
ignoring the differences in assumptions. You know, Shannon talked about telegraph,
he talked about a receiver getting a message from a sender, right? And Clausius talked about,
you know, gas. So, but, but I do think that's a balance in that. And there's no way to not
have simplification and generalization. I just wanted to say somewhere, somehow we need to kind
of, you know, go back there and just say, you know, entropy is not entropy is not entropy.
And all these things are kind of, you know, long capers, right?
Yes, Jacob, then Ali, then anyone else?
I just wanted to ask for, I guess, perhaps, slightly longer comment from Alexei on
that what you see as the fundamental differences in different descriptions of entropy and whether
there is some, I guess, general concept that could be extracted from all of them such as
level of information or uncertainty about a particular system, even though these specific
formulations of entropy they talk about, they were derived from different specific systems and
they have different equations describing them and they mean slightly different things.
Do you think it would be wrong to say that there is some commonality between them?
Did I answer Daniel or? I don't know, I just think we need to, at some point,
formalize it and be careful with these generalizations. For example, when we talk about
active inference and Friston, right, you know, when we say living organisms must minimize,
I'm quoting Friston, by the way, these are not my thoughts, the entropy of sensory states,
and then people take that idea sometimes in generalize and they say that that's what life is,
minimization of entropy, right? But if you actually look into human brain and measure entropy,
then it goes all over the place, up, down, and in between, and so I don't think that's an
accurate description and the reason for that is we're shifted away from the assumptions and,
again, so Kolmogorovs and I-entropy talks about the phase space and the divergence of
trajectories in phase space and sort of the volume in phase space, while when you talk about this
Friston's entropy of sensory states, these are probability distributions and Shannon talked
about the uncertainty of the receiver about the message sent by the sender. So if you want to
extract the same thing, I suppose you can, but it's important not to lose at least the difference
between what kind of object are we applying to? Are we looking at probability distributions?
Are we looking at phase space? Are we looking at ideal gas? Where are we at? And we're just
not to say entropy is entropy is entropy. That's not the case. I mean, it's just a formula,
but depending on the context assumptions and things, it can be very different.
Thanks, Ali and Ava.
Yes, in regard to Alex's comment, I just wanted to mention the research of people like Brutal
Fanel and Stefan ThÃ¼rner, because for many years, they've been exploring this idea of different
conceptions of entropy and they've come up with at least a dozen different meanings for entropy
and they formulated it in a kind of generalized framework. But at the same time, they've also
devised a way to somehow classify them into equivalence classes and to see whether one or
more of those conceptions of entropy can be theoretically derived from some others or
they're totally distinct and different conceptions of entropy. So yes, I agree that in some
situations we need to be more specific about what exactly we mean by entropy and information
I mean, based on the question or the situation of interest.
Thanks, Ava.
So just to vote in the entropy discussion, I want to emphasize that entropy is PNAB,
so it's a function of probability distribution, which is, I think, I'm not sure what the term
is in English, but it's a measure of how big something is in a space. And to get that, you
have to define a space. And in many cases, it's pretty arbitrary to decide what the space you
look at is. For example, you talk, I don't know if it was, let's say, someone talked about
entropy of fMRI. Well, those are aggregate measurements of neural activity. This is not
the space of all possible configuration. And even if I add something like the space of all,
let's say functional configuration, the state in your own that reflect activity, this would be
another thing than the space of, I don't know, atomic configuration, where the specific atom are.
So, yeah, you don't have a warranty that's when you define entropy, you define it on
a meaningful space that's not given. So let's, I kind of think there is one entropy and that's
it and it's an infoconstruct. But also we have to, if we agree to that, then it entails that we
have to find it, that it's not just given out there in the data.
Well, certainly being precise in the formalism and in natural language for what we mean by
information and entropy and all these terms is pretty important. I mean, the course we're in
is physics as information processing, but without an understanding of how information is being meant
here, or what measurements and transformations were performing, what informational operations, then
it is a non sequitur. So how do we go about clarifying or understanding what kinds of theoretical
results in a given domain or like what is shown on the timeline here might apply to some other area?
Just one thought. Anyone else can raise their hands with a thought or a question?
