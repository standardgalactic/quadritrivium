Hello, and welcome. It's February 20th, 7th, 2023. We're here in Active Inference Bookstream,
number 2.1 with Thomas Parr, as well as Terry and Ali. So thank you all for joining. This
should be a very fun discussion, and let's go first by way of introduction and beginning to
Thomas. Welcome, and please, how did you come to write this textbook in collaboration with
Pazulo and Friston? Well, again, thank you very much for having me here. How did we come to
write this textbook? That's a good question. I think Giovanni was the person who really had
the idea and pushed for this, but I think Carl had already previously been approached by the
editor who ended up managing the publication of the book. And when Giovanni had the idea that
this would be a really useful contribution, he and I had a number of discussions about it,
both in London and in Rome, where he's based, and sort of put out a plan and sort of came up
with something that we hoped would be useful for the community. I suppose part of the reason why
we thought it'd be a useful thing to do is that Active Inference is a very multidisciplinary
type field that draws a lot from lots of different domains. And I think one of the struggles we all
had when we started out in it, and I think certainly what students and people who are interested in
the topics say to us is that it's often quite difficult trying to find all of these different
topics, trying to find the right background when they don't all come from one field. So putting
it all together in one book with a consistent notation, consistent language, and trying to make
it complete, we hoped would just make the field a little bit more accessible to others who wanted
to get involved.
How did this dovetail or integrate with your own education and research
directions? Like what phases or stages were you moving through while this discussion was happening?
Well, I think so when we initially started discussing it, it must have been probably the
last year of my PhD. And so by that stage, I'd spent two to three years sort of getting
to grips with Active Inference, being relatively recent in terms of learning some of the key
material myself. I think it was quite a good stage to be at in terms of thinking about what
I would have wanted to know a few years previously. The actual writing of the book then carried on
over the next year or two. And so by that stage, I was back in clinical training as well,
which I think is also a useful perspective in terms of some of the later chapters and thinking
about application of Active Inference type models in the context of clinical conditions and
psychiatry, neurology and those sorts of areas.
That's actually a really interesting note. How did the clinical experiences
give a perspective on the book? Or how did the book or your learning in these
abstract and theoretical domains give a different insight in the clinical setting?
It's a good question. And I think you're right to highlight the bi-directionality of it,
both are useful in different ways. I think, for me, the clinical setting is quite useful in terms
of thinking, first of all, interacting with different sort of audience. So people who
may be much more skilled on the clinical side of things maybe had less experience on the
mathematical, computational side of things, but may still find concepts very useful.
So from that side of things, it's quite a useful exercise in being able to communicate
these things. And then there's sort of the interaction on the patient side of things and
trying to, you know, I think it makes a big difference. First of all, interacting with patients
has an impact on how you might think about the interesting questions and the things you might
want to use them to inference for. But also having an understanding of principles behind
it helps you, I think, formulate your questions about how individual patients might present
or the reasons for that. I think it's a useful way of thinking about, I mean, I suppose on two
levels. So one, one in terms of understanding, understanding particular sorts of pathology
in terms of the inferences that might have gone wrong. But also in terms of understanding your
own active inferential processes when you're trying to work out what's going on yourself,
asking the right sort of questions to gather the right sort of information and test those
internal hypotheses. I think that's a very useful way of thinking about being a doctor.
Awesome. Yeah, a lot to say about that. The Bayesian optimal differential diagnosis.
Ali or Terry on any of these thoughts, do you have any questions or comments you want to
raise there or any follow ups? Well, yeah, actually, I just wanted to ask,
what are the intended audience of the book? And what do you think are the minimum prerequisites
for going through the whole book? Because I'm sure you'll agree that even today,
the intersection between the people who are well versed in mathematics and the people who have
a solid background in say neurobiology are quite narrow. So what's the solution here? And what
did you think about about the organizational structure of the book that could accommodate
various audiences? So again, another really good question. And I think you're right to
highlight that that intersection is relatively narrow. And there are lots of lots of people who I
think struggle to feel involved in those kinds of discussions. And you're absolutely right to
say that. And I think one of the solutions is the sorts of things that you guys are doing in
terms of setting up these reading groups, trying to open that up, trying to provide
educational resources. I think that's really very important. In terms of the specific
questions about the book itself, who's the audience? Well, and, you know, what are the
prerequisites? I think it sort of depends what you want to get out of it and what your purpose
is in reading it. So some aspects of the book, I suppose, will appeal more to some audiences and
others to other kinds of audience. I would hope that the readership or the audience is
relatively broad, because we've tried to provide as much background as we can on the
mathematical and the neurobiological side of things to try and complement the skill set of
whoever's not in that intersection who comes to it, who may need a bit more sort of neurobiological
grounding or who may need a bit more mathematical and technical grounding. But I think ultimately,
like with most textbooks, you don't necessarily need to come to it thinking, I'm going to understand
everything in this book when I come away from it. Some people may want to, but other people,
depending upon what they want to get out of it, they might say, actually, what I really want to
get out of this is an understanding of some of the kind of message passing as it manifests in
the brain. Whereas other people might come to it and say, actually, I already understand a bit
about that. And what I want to get out of this is to understand a bit more of the technical detail.
You know, if your roboticist is coming to this, it probably doesn't matter too much about how things
are implemented in the brain. If you're a neurobiologist coming to this, you may not need to
know the details of the implementation, as long as you can understand it conceptually. And I hope
the organization, particularly putting things like appendices with detailed information at the back,
tries to keep most of the text a bit more accessible, the early chapters being relatively
light on the technical material and building up the concepts before then moving into the more
technical aspects.
About the chapters, let's actually just take a quick look at the chapters. We're in the
Coda document that we use. So perhaps you could just say one or a few sentences on each chapter.
What's the short of each chapter? What is it doing in this 10 chapter work?
We're testing my memory now. The titles are there. The cues are there.
Epistemic cues are there.
Okay. Well, the overview is what it says on the tin. It's a description of what's to come in
the subsequent chapters to try and orientate people. And the low-road and high-road, I think,
again, Giovanni has to take credit for this idea of saying, actually, it's often difficult to know
where to begin when you're trying to understand active enterprise and the free energy principle
and the Bayesian brain and all those sorts of things, because it seems like there are so many
potential starting points, all of which ultimately, I think, get at the same thing, but knowing where
you begin is often really quite difficult. And so these two chapters were designed to explicitly
acknowledge that difficulty. So the idea of the low-road is to say, well, let's take a pragmatic
perspective where we take ideas that are developed in neuroscience based upon our understanding of
the brain and of psychology and models that have been developed to explain those things.
And let's take those to their conclusion, which is, in a sense, the free energy principle,
which is active inference, the idea of using our internal models both to drive behavior and to
understand what's going on in the world around us. The high-road perspective was designed to
take a different approach to that. And instead of starting from what's already been developed and
the sort of natural trajectory in the neurosciences, the high-road chapter was more saying,
let's start from those principles as much as we can. And here was a tricky balance to try and
make sure that we didn't go too far in depth, that we put people off by the third chapter, but
enough that people might get a bit of a conceptual sense of where you might go with this. And here
we sort of started from the kind of physics of self-organization, the ideas that come
underneath that, and how those ideas can then feed back into this same idea of active inference,
that the neurosciences and psychology seem to have arrived at relatively naturally.
So chapter four is when things start getting a bit more technical and starts dealing with the
specifics that I think most people would want to know if they were implementing these models
themselves. So trying to understand exactly how you construct an internal model, how you specify
it, write it down mathematically, how you perform inference with those, how you understand the
message-passing schemes. Chapter five then takes a step back and thinks, well, in the context of
the brain, in the context of neurobiology, what do those things actually look like? How do we
understand those inferential processes with different kinds of generative models in terms of
what we know about message-passing and synaptic communication? And those five chapters together
really are the first part of the book, and the subsequent five chapters really then take you
through the idea of actually applying those ideas in particular domains. So chapter six is sort of
the overview chapter looking at the rest of it, and it's really setting out if you wanted to sit
down and design a model, either conceptually or computationally. These are the steps you would
have to go through to construct that. And then chapters seven and eight take you through that
sort of recipe for several examples, building up different concepts, both for models designed in
discrete time where you're formulating things as a sequence of events and models formulated in
continuous time where you work with continuous dynamical systems and differential equations.
Chapter nine, as I mentioned before, you know, one of my interests is in the clinical domain
and trying to understand what sort of models people use to form inference, how those can go
wrong. And part of the research on that is thinking, well, how do we then fit these models and use
them to actually draw inferences about what's going on, particular individuals or groups of
individuals. And then chapter 10 takes the broadest perspective and looks at how that relates to
a range of other theories and ideas, both in the neuroscience and beyond. And then I'm sure the
appendices speak for themselves. Awesome. Great overview. Let's come back to our questions. So
you laid out that the first half of the book is about learning and the second half of the book
is about applying and it begins with that recipe. So if chapter six is the recipe, as a chef and
sous chef yourself, what else have you found was important in the restaurant? What is your
full stack active inference modeling look like? And maybe summarizing in six or going beyond
what was in six, like what have you found to be important as part of the modeling journey,
especially with teams where people might have like different levels of domain specific knowledge
or active knowledge? I like the restaurant analogy, actually. I was thinking, I suppose the
first thing that anybody needs to do if they're setting up a restaurant or a model is thinking
about who their customers are and what they might want. And so in the scientific domain,
you're thinking, well, what sort of journal am I planning on submitting this to? What sort of
questions are the communities who read those journal articles interested in?
And I think that's a really important first step so that you know what I'm actually trying
to do with this model. What's not such a good idea often, although you can find interesting
things by doing it, is to say, well, I'm going to develop a model and work out what it's for later
on. You can end up sort of tying yourself up a little bit by doing that. Although there is value
in exploring us, I think we would all agree. Then I suppose in terms of ingredients is relatively
straightforward and deliberately so. I suppose the idea of these sorts of models is that they
should be relatively accessible, relatively easy to construct without having to make use of large
supercomputers or anything like that. A lot of the models that I developed can be run on a laptop
fairly straightforwardly. So obviously just picking your computing language and how you're
going to construct that, there are now a couple of different options out there. Matlab being the one
that I think is most well developed, but now lots of people developing Python-based implementations
of excellent print schemes and particularly in the Markov decision process realm.
So that gives you, I suppose, some of the key ingredients. But then it's really
sitting down and thinking, well, how would I now actually construct this model? The key question,
I think, often is, if you have a particular phenomena or a particular task in mind that
you want the model to perform, it's thinking, well, how would I go about actually constructing the
data that's presented to a participant performing that task or presented to an agent that's trying
to perform this task? And if you can explain how those data are generated, then you effectively
have the generative model that implicitly your agent or your participant or whoever else should
be using to be able to perform that task. So really, all you need to be able to do is to work
out how to create the experimental stimuli. And once you've done that, you've almost already
created your model. And the rest comes from predefined, pre-existing inversion schemes
that actually minimize the free energy for that model. So at that stage, it becomes relatively
straightforward. I say relatively because as anybody who's as experienced in coding knows,
a lot of time is spent debugging and throwing up errors and spending a while trying to work out why
a little bit of Googling to work out your solution around that. I think those are all
important parts of the process. Awesome. Ollie or Terry on that recipe and modelling question.
Any thoughts on that?
Yeah, actually, I also wanted to ask if you have a plan for extending the
codes or supplementary materials to include other languages such as
IMDP packages or any other packages that might come up later and would be widely used in active
inference. Because as far as I know, there isn't any specific GitHub repo or something
dedicated for this book so far. So do you have any plans in developing such a supplementary
material for the book? We don't at the moment. It might be an interesting thing. I mean,
it might be a useful thing to be done by the active inference community as well as a way of
sort of being engaged and involved in it. I suppose one of the difficulties of doing that for a book
is that often these things move on so quickly by the time it comes out in the book that those
packages have moved on. So I think it was useful for us to give a simple example of
Matlab implementation to demonstrate the principles in one of the packages that we've
been involved in developing. So it's part of the SPM package where we've been involved in
developing the active inference routines as part of that. And I think it is certainly a good idea
thinking about having other repositories out there with all sorts of demonstrations. So the
SPM package does have a range of different demonstrations for different sorts of implementations
as well for different kinds of models, which is a really helpful resource. And I often suggest
to people that one of the first ways of getting a sense of how to build these models is to look at
one that's already been built and do their best to break it and change things and make it behave in
different ways. And so I think you're absolutely right to say examples of the code and how it's
implemented are vital in bringing people into this. But yes, I think you're absolutely right
to say that it would be really helpful to have both Python and Julia implementations and all
sorts of different ways of doing it and examples so that people can pick those up completely.
And of course, one of the advantages of Python and Julia is that they're open source and you
don't necessarily need an academic license to use it, which again, I think increases the number of
people who can access it. Giving an overview to someone who might not have been following in the
blow by blow of our little literature corner over the last few years. How would you say
active inference modeling has evolved during your time in this space? Where are we at?
Just generally speaking with applications, what applications are developed or possible today
that when you were starting your PhD seemed like they were just implausible?
So I think if we go back a bit, I suppose some of the earlier implementations were
almost all in the continuous domain. So they're all based upon
things like these generalized filtering type algorithms that
were initially developed as sort of filtering methods to perform time series inference.
Those things then equipped with an active component then became some of the earlier
active inference type simulations. And a lot of that focused on things in the movement domain,
whether that be eye movements or control of limbs to the extent that you can sort of
develop handwriting and those sorts of things. One of the interesting things that started to
happen based upon that was to think about how you then get something that looks more sequential
emerged from those continuous dynamics. And handwriting was actually a really good example
of that, where there were some simulations developed in which the continuous model
effectively had something a bit like a Lotka-Volterra dynamic in it. The idea there being that you
can sort of... So Lotka-Volterra dynamics are a kind of dynamical system often used for modeling
things like predator-prey interruptions. And the idea is that you get a peak in terms of a predator
population, sorry, prey population, followed by a peak in the prey, the predators as they eat the
prey causing drop in the prey population. But then as the predators die out, the prey come back and
you get this sort of sequence of peaks from predator-prey, predator-prey. You can generalize
that to then have multiple different populations, each preying on one another. So you get this
sequence of peaks over time. And that sort of emergence of sequences from continuous dynamics
I think was one of the key moves early on in terms of thinking about things through a bit more
cognitive in terms of how do I do this and then this and this and start to plan and generate
much more purposeful type of behavior. So that went so far until things moved much more to a
an explicitly discrete or sequential style of modeling. That's when things like Markov decision
processes started to the employed in the active inference domain or partially observed Markov
decision processes. So those are models that say, I assume my world is a sequence of states over
time one after another, each of which generates some observations. And from those observations,
I can make inferences about the states. You can then say, well, if I can act upon the world,
I can change the sequence of states as they evolve over time. And by selecting different
actions or different ways I can change that sequence, I'm effectively planning and making
decisions. So that's again where you get into a much more cognitive sort of domain.
Some of the key advances sort of early on while I was doing my PhD were the emergence of things
like deep hierarchical models of things like Markov decision processes. So here the idea was
that in the world around us, there are lots of things that evolve over different sorts of time
scales. And so one of the early examples of this was reading the idea that when you move your eyes
around the page, you're sort of looking at an individual word or the letters within that word,
but that word is itself part of a sentence. And the time it takes you to infer what the word is,
is going to be much shorter than the time it takes you to work out what's going on in terms of the
sentence and the sentence and the paragraph and the paragraph on the page and the page and the
book and etc. And so there's a whole hierarchy of different time scales in the world around us.
And one way of accounting for that relatively simply in these sorts of models is to simply
stack them on top of one another. So the outcomes you're now predicting from one of the Markov
decision process type models I was describing now reflects a sequence of very fast states.
And each of those states, yes, exactly is the graphic of this. And then each of those
outcomes predicts another sequence of very fast states. So what you're left with is at the very
top level, a very slow progression of state after state after state. But each of those states
which might be the sentence, you then have the words in that sentence beneath it. And then for
each of those words, you might have each of the letters in that sentence, oh sorry, each of the
letters in that word. And so that then sort of expanded the range of things that we can do in
terms of how we give some sort of deep temporal structure into the kinds of models we develop.
Sometimes people refer to that as kind of breaking the Markov property. So for those who
aren't familiar with this, the Markov property is the idea that each step in time depends only
upon the step immediately previous to it. That all you need to know about the next time step is
what's happening in the current time step. Nothing else in the past is useful once you know that.
And people often suggest that means that these processes are effectively memoryless because
it's only where I am now that matters. It's not what was happening well into the past.
By having a hierarchical structure where you have a higher level state that is more slowly
changing, that's contextualizing what's going on at the lower level, it means that the fast
sequence of changes is actually dependent upon something much slower that's happening. And that
itself is dependent upon something much slower and that gives you a sort of memory. It allows for
the Markov property to be broken. So like I said, we're sort of starting off with continuous,
the emergence of discrete from continuous, the explicit modeling of continuous models,
decision making, and the sort of exploration, exploitation type trade-offs that you get once
you treat planning as being in this process. The idea that you can then stack these things
hierarchically and have a range of different timescales that break the Markov process allow
us to think about things like memory and working memory. And I suppose one of the next stages was
the combination or the reintroduction of the continuous models as the lowest level of one
of these hierarchies. The idea that actually it's all very well being in to make decisions,
but at some stage you need to implement those decisions in a world that's made up of physics and
muscle lengths and continuous sensory inputs. And so putting these sort of hybrid models together
where you can use your decisions to then predict short sequences and trajectories
to find in a continuous domain. And I suppose since then there have been a range of further
developments in active inference in that setting and a range of developments
in a number of other directions and sort of in parallel. So some of those developments have been
thinking about implementation in other languages. Some of those developments have been thinking
about combination with methods from deep learning and the introduction of variational autoencoders
and function approximators and amortized inference, which a lot of people in the robotics community
have been pioneering particularly. And so that would be one direction that things
have been developed in. And I suppose the other direction that's seen a lot of development recently
has been the sort of underlying principles, the physics that underwrites active inference,
the principles and how you justify the sort of dynamics and where free energy comes from as a
concept and why it's useful in understanding how systems behave. So I hope that wasn't too much
all at once, but there was a sort of run through of some of the developments that spring to mind
over the last few years. Awesome. Perry or Ali, any comments on that? Quick historical review.
Again, yeah, I had another question regarding. Actually, it's a question out of mere curiosity.
Did you have any specific reason for adopting this particular mathematical notation throughout
the book? Because it seems like it somewhat diverges from the notations used in some of the
well-known papers in the literature. And I know active inference notation is not
consistently used across the literature, but even in the papers that came up after the publication
of this book, written by Professor Friston or others, I don't see this specific notation used
a lot in those papers. So I just wanted to ask about the reasons behind this choice.
Okay, I mean, I think you're absolutely right to say that it's not always been used consistently.
And it's worth saying that in some, there are a couple of different tensions that affect this.
So one is the idea that in a lot of papers that are published, because it's only dealing with an
aspect of active inference and not trying to try to put it all together, they can sort of use a
notation that may be the same as a notation you might use in another field without worry that
people will get conflicted and misinterpret one variable for another because everything's just
defined within that one domain. Another aspect to it is that in, depending upon the paper in
question and the audience is aimed at, sometimes the notation is designed to match what might be
used in that broader field. Whereas we've tried to try to make it as consistent as possible across
different fields. Are there any particular notations that you were thinking of or
would like to help clarify? Yes, so for example, as just a very simple example,
using eta for external states instead of x that's been used or s and a for active states and
sensory states or other kinds of uses of lowercase matrices and uppercase matrices,
which is used differently in a paper such as a step-by-step tutorial on active inference
and these kinds of rotational divergences. Okay, so yes, I mean there's some quite nice
illustrations. So things like a and s, which as you say are often used in some of the more
physics-orientated papers. Unfortunately, if you use those in a Markov decision process paper,
people will interpret those as being states and parameters of a likelihood matrix.
So unfortunately, we couldn't use those same letters without causing more confusion. So
what we try to do there is to focus more on the sort of practical inference side of things of,
you know, the actual implementations we tend to use of active inference in particular problem
settings. And there when you're constructing a generative model, particularly if it's in the
continuous domain, we'll often use x as being the states that I have to infer, which from the
perspective of the physics type papers are the external states, the things that are having an
effect on my sensory states. I can't remember whether we used y or o for the sensory states
or the data in the book. I think it's y. I think it's y, okay. And that sort of fits with the idea
of using it in the continuous domain, where in a lot of settings when you're trying to predict
sensory data, often y will end up being the variable that we end up using.
So I appreciate that it can be confusing, sort of jumping between different resources.
But what we strove for in this one was internal consistency as much as we could,
and trying not to overload specific variables or symbols. Thank you. Thanks.
Bigger 4.3, we bound ourself coming back to again and again. It for sages chapters seven and eight.
And it lays out discrete time on top and continuous time on the bottom in terms of their
structural similarities, but also they have some really important differences.
So could you just, as you see it, how do the discrete time and the continuous time models
have similarities and differences such that you can lay them out in a graphically or visually
similar way? Yet there's also some key differences, for example, in what each of the nodes mean. So
how are they similar and different? And what did you hope to illustrate by laying out figure 4.3
this way? So the reason that they can be seen as similar is that effectively what you're trying
to do in both cases is to represent a trajectory. We're trying to represent how something evolves
in time. And that something may be the states s in the discrete time, or they may be the
trajectory of some hidden states x in the continuous domain. And when you're trying to say
how do I actually put together a set of numbers that tell me how something's evolved or evolving,
there are a couple of different ways you can do that. One is to say I'm going to tell you where
it is at time one, and then I'm going to tell you where it is at time two, and then at time three,
et cetera, et cetera. And that's a perfectly legitimate and perfectly reasonable way of
writing down a trajectory. It obviously misses out all the times between time one and time two,
which may or may not be significant, depending upon what the problem is you're dealing with.
The second way that we approach it is to say, well, you can use what's known as a Taylor series
approximation or a Taylor series or a polynomial expression for that trajectory. So essentially
what we're saying there is that you just construct a function that follows the shape of that trajectory
of the time. And you can construct that function by saying, well, let's look at where I am now
at this point in time and my position. How am I changing over time? If you just took those two
variables together, you just have a straight line telling you where you are and where you're going
to be. If you then take account of your acceleration, you may even have a curve line which says, I'm
here now, I'm going to be there then. And as you add in more and more elements to this, so the rate
of change of the velocity and the rate of change of the rate of change of the velocity, et cetera,
and just add in more and more rates of change, you effectively get a more and more precise
function telling you, at least locally around where you are now, where you're going to be
at various other times. So that difference, either using Taylor series coefficients, which are just
successive rates of change, or by using a sequence of states over time, are what's illustrated in
the lower and upper graphics here, respectively. Each of those things has, as you said, they have a
lot in common in terms of their structure, in terms of the number of numbers you need to be able to
describe them. But each has their advantages and disadvantages, depending upon what you're trying
to actually model. And I think, for me, I tend to think of the sequence of categorical states over
time as being quite useful in thinking about anything to do with decision making, deciding
between alternatives, thinking about sequences like language and words, and
even sequences in terms of the eye movement you make, looking in one place, then the next,
then the next, rather than continuously moving your eyes around space in a smooth fashion.
Whereas the process of interpreting sensory data that's coming in from your retinor or from your
stretcher structures or whatever else, or in terms of actually driving action that changes
continuous variables in the world, there, I think, is much more useful to be able to describe
things in terms of a continuous number and those sorts of continuous trajectories that you can
get through using these Taylor series type approximations, which are often referred to in
this context as generalized coordinates of motion. Awesome. Cool. Well, another figure
that we come back to again and again, tucked away in chapter nine, but giving a lot of context,
is a figure 9.1 on what is termed meta Bayesian inference. So could you describe what
is happening in this figure? And is this exactly as every empirical ethologist is going to agree
with? Is this controversial? Is this novel? So what is it doing for active inference when we
conceptualize our empirical task structurally like this? And do you think that that is exactly how
ethologists have long seen this topic? Or is this something contentious?
I think so. I'll talk through the figure, but then you can tell me if that's how ethologists
traditionally look at it and whether it's contentious or interested. So the idea behind
this figure is to highlight that when dealing in psychology or neuroscience or fields where you're
trying to understand what a particular system is behaving or how it's essentially what's going
on in the mind or the brain or the nervous system of some creature that is behaving in some way.
There are two levels at which you can look at it. One is saying, well, from my perspective as
the experimenter, what are the data I'm measuring? And that may be behavioral measurements from that
creature. It may be taking electrophysiological measurements. It may be asking it a question if
it's a patient in a clinical setting. And I'm essentially now trying to optimize my model of
this system. Now, in the neurosciences and in the settings I outlined above, the kind of system we're
interested in is the sort of system that has its own model of other sorts of system. And so the
kinds of questions we're interested in, the kind of parameters we're interested in, are things like
the prior belief that this creature has about this thing or how precise it thinks its data are.
And so the parameters I'm now asking a question about are the parameters of the beliefs of another
system. So the outside box here, the one that the parameters are going into, the experimental
stimuli are going into and the observed behavior are coming out of, is essentially my model of how
this other system behaves. Now, the inside box here, the one in the dotted line or the dash line,
that now represents the model that that system has to understand the world around it.
And so when we construct our model, that contains another model inside it.
And by saying, if this creature believed this, this is how they would make inferences about
their environment and generate behavior, I can now predict the behavior I would observe if this
creature believed this thing. And so the subjective objective idea is that the objective model is
is the experimenter and its beliefs and the experimenter's beliefs and the observations
they make by measuring what the participant or experimental subject does. And the subjective
model is the participant's model of the world around them. So that I think would be my summary
of how this figure works and the idea it's trying to get across. Now, you can tell me how
the logists may disagree with that or agree with it or what they'd find controversial.
I'll give a thought and Ali or Terry, feel free to give a follow up or a thought.
I don't think in principle, it's contentious or dissentious. However, there might be an explicit
or implicit belief, of course, ironically, that we can simply take some of these parameters coming
into our experimental setup, and even the selection of experimental stimuli, we can take those as
simply given, which is a very frequentist perspective. And then on the observed behavior,
we can simply use descriptive statistics. We'll characterize the predefined groups according
to the one experiment we ran, we'll do the t test, we'll do the ANOVA, we'll come up with a p value,
it's going to be frequentism all the way forward. So even if we're doing frequentism about a
generative model, there's a sense that if we just clamp down on our experiment and treat it as
handed down without error, instead of being actually drawn from distributions of possible
experiments we could have done, if we clamp down upstream of the experiment, we'll just be able to
describe and ultimately publish without needing to look at what else could have happened.
And I think this diagram and also the very subtle dashed line around even the bigger box
suggests that just like the active clinician engaging with a conversation or the active
experimenter also engaging iteratively over their system of interest, that itself is an adaptive
epistemic process where, yes, you could simplify it by treating the parameters and the stimuli
sequences as simply given, but then you don't have access to the generative model that actually
is the generative process for the subjective model. So summarize in principle,
uncontentious, however, sometimes discordant with the ways that behavioral experiments are
conceptualized, planned, and analyzed.
One reason why it's helpful, because during the book we try to stay on the road, on the high road
and the low road, but when we're in discussions, it's almost like there's a whole road map and
people have their own backgrounds and questions, and it kind of comes together with all these
different topics that people are curious about ultimately that bring them to active inference.
So going to this metabasian figure helps in some way situate how the authors are seeing this entire
process of experimental design, observation, and cognitive modeling, and that was unpacked also
in the recent map territory fallacy fallacy work.
Ali or Terry, any thoughts or questions in our last 10 minutes?
Is there a figure you want to look at, or we can go to some other places?
Maybe we can ask the missing figure in, I think it was in figure 7.2 in the lower left.
So then this was actually corrected by, I don't know quite how it ended up
vanishing, but it was corrected in one of the later print versions. I'll see if it's in the
version I've got in front of me now, and I can show you what it would have looked like,
but apologies for that. Which figure is it? 7.2 on page 128.
Unfortunately I picked up a copy of the book, it doesn't have it.
So there have already been one and a half fractal dimensions of printing?
Yeah, I think unfortunately I don't think that one's corrected yet in the online
copy. Essentially what would have appeared in that figure, or what did in some of the later
printing versions, is the errors that are driving the updates of the states in the plot above.
So the errors here are effectively the free energy gradients, or the negative free energy
gradients. So essentially as you go down the free energy gradient, you're effectively minimizing
your error. So what that means in practice is that the errors would effectively be the rates of
change of the states in the plot above. So if you take the states in the plot above and you imagine
what the rates of change of each of those lines are, that's what you would get in the plot below.
Well, on this same kind of topic, in Chapter 7 we're in the discrete time generative models,
and a little bit less so here, but in the latter models here in 7.13, and in what we just looked
at in 7.2, where are these continuous interpolations coming from if it's a discrete time model?
That's a good point. So I suppose there's an important distinction to make here. There's the
distinction between the things that are being inferred, which are happening over discrete steps
in time, and then there's the inference process itself, which deals in probabilities, which are
themselves continuous. They can vary in this case anywhere between 0 and 1, and the time over which
that inference happens. So to give you an example, if you imagine that the way you're
analyzing this figure at the moment is by making a series of saccadic eye movements,
you're not sort of continuously moving your eyes from one plot to the next. You're making a series
of rapid ballistic movements that then take you from one to the next, and effectively every time
your eyes stand still, you have a new data point that you're now trying to analyze.
You can imagine that between that or every time your eyes stay still, everything else in your
brain doesn't just stop. So once you've got that new data point, your brain now has some time to
assimilate that data point to draw inferences about what it's seeing there, and those inferences
happen through the continuous dynamics of what's happening in your neurons and various networks
across your brain. So you can think of the continuous changes that are happening for each
state as not being changes in the world that we're trying to infer, not being changes in the
data point I'm currently looking at, but the changes in my beliefs about it before I then
sample the next data point. Interesting. I have a few more figure-based quick questions, but
Terry, do you want to bring up anything, or just give any remarks? No worries if not.
I am very much just trying to understand these concepts, so I don't make any pretence
at having any deep understanding, but the key thing that interests me is how you jump from
this sense of how we update our predictions into the clinical realm. I'm interested in pain medicine,
and I see neuroplastic pain as very much a mistaken prediction based on often
misinterpretation by medical professionals and the information that they give to patients,
and I'm really intrigued by the potential for
using these concepts to give patients an architecture in which they can start to understand
that they have some agency in altering their future predictions and really changing their
experience of their pain. And as your clinician, I'd be really intrigued to get your sense of
where this is going in that type of clinical realm. I think that's a really nice example as
well, thinking about attitudes towards pain and particular kinds of pain, particularly when there
isn't a sort of obvious tissue damage that can be identified that's causing that pain,
but that still is obviously a very real sensation to the people who are experiencing it.
And I think it generalizes to a number of other conditions, and probably there's an overlap with
things like functional neurological disorders, probably a number of medically unexplained symptoms
and things where there is a huge amount of overlap in these sorts of processes where you
can't necessarily directly identify a big structural lesion of any sort, but clearly the
way the nervous system is working something isn't quite right. Pain I think is an interesting
example particularly because pain is never a real thing out there in the world, is it? I mean it's
always an inference, and so I think that's one of the first things to say to anybody, you know,
regardless of whether you've got pain because you've lost a leg or whatever else, there's no
such thing as pain as an external state, it is just your inference, your explanation for
these signals that are coming in from your body. Now one of the important concepts that I think
comes into all of these sorts of syndromes, and probably many others as well, is the idea of
precision, the idea that the data I predict, sensory data coming in, I can in addition to
estimating what those data are going to look like, I also have to estimate how confident I am
in what they're going to look like. If I'm not very confident or I think I'm dealing in something
that's very noisy, then I can actually discount a lot of that signal, whereas if I believe that
it's all extremely reliable, very clean data that's coming in, then I want to put a lot of
weight on that. And another way of saying that is that there are some things I pay attention to,
and some things I don't. And you know, just saying it's reliable data is just a matter of saying I'm
going to attempt it. Now it might be that in a number of these different syndromes, what's going
on is that our bodies have overestimated the precision, overestimated how clean the data
are coming in from a particular body part. And that's why we then overinterpret any signals
that are coming in as if they are painful. And the inferences we draw from those signals are just
too confident and too driven by something that they shouldn't be. An example I often like to use
in clinical discussions is that I told you to pay attention now to the feeling that your left
sock was having on your left big toe, you can probably feel it, but you were completely unaware
of it before. But now if I say continue to pay attention to that and carry on paying attention
to that, and you're unable to pay attention away from it anymore, if you felt that all the time,
it would feel unusual, it would feel wrong, and maybe a misestimation of precision in that way
may be what's going on in a lot of these sorts of conditions. Partly, I think it's often interesting
that in some of these people there was at some stage an injury or something that affected that
body part. And you can imagine that if you went through a period of illness where you did have
tissue damage and were in pain for that reason, it might direct your attention to that and you may
have a learned deployment of attention towards that area that might persist even after the
insult and the damage was resolved. And again, maybe it's that sort of persistent
mislearning of how precise things are that leads to the continued experience of pain,
even when there's nothing sort of externally causing it anymore. So I think just having a sense
of inference and of the idea that all our perceptions of the world around us are ultimately
inferences says that if we change our model in some way, if we're able to adapt and change and
if our plasticity in our brains and spinal cords can be exploited, then we can also correct those
mislearned models and hopefully resolve those sorts of symptoms and syndromes.
Awesome. Thank you. Well, we are at the very close. Really appreciative, Thomas,
of how much we were able to cover. What are your closing words or thoughts, potentially to someone
who has listened to this one hour discussion as their appetizer before joining the part at all
restaurant service? Well, now I've enjoyed the discussion. Thank you for having me here.
I'll reiterate what I said earlier. I think one of the key things in making this field more
accessible to more people are these sort of regroups, the ideas of trying to create these
educational resources and to unpack and discuss these things. And I'm impressed by all the
infrastructure you've put together and by everything you've managed to achieve by doing this.
So I'd certainly encourage anyone who's interested to join up to your reading groups
and I hope they get something out of it and out of the book. Thanks, Ali.
Well, yes. Actually, since the beginning of the first cohort, this book has practically become
part of our life that we love and cherish. And it's amazing to see how with each iteration
of our reading group, it gives rise to a whole different set of questions and discussions.
And actually, I'm a firm believer in this famous quote, I don't know by whom, which says that
the sign of a great book is not how many answers it provides, but rather how many
thought provoking questions it engenders. So based on that criterion and judging by the
variety of questions brought up during our live discussions as a kind of empirical evidence for
that, I think it's probably safe to say that this is truly a great book. So congratulations on that
and thank you. Thank you very much. Great closing thoughts. So Thomas, thank you again.
You're welcome back anytime. Thanks Ali and Terry for all the great participation. And until next time.
Bye.
you
you
