in a lot of different ways.
So you can form classification tasks in which you provide an image and you run the energy
minimization and get the label.
But you can also, for example, perform generation tasks in which you give the label, run the
energy minimization and get the image.
You can perform, for example, image completion, which should give half the image and let the
model converge to the second half and so forth and so on.
So it's basically a model that learns the statistics of the dataset in its entirety
without being focused on classification or generation in general.
So this flexibility is great.
The problem is that because of this, every single task doesn't work well.
So you can do a lot of different things, but none of them is done well.
And here I want to show how using interventional queries instead of standard correlation queries
or conditional queries slightly improves their results of those classification tasks.
So what are the conjecture reasons of this test accuracy on those tasks not being so
high?
The first, the two reasons are that the model is distracted in correcting every single error.
So basically you present an image and you would like to get a label, but the model is
actually updating itself to also predict the error in the images.
And the second reason, which is the one I said, is that the structure is far too complex.
So again, from an Occam razor argumentation, this is the worst model you can have.
So every time you have a model that fits a dataset, that model is going to be less complex
than this one that is going to be preferred.
But in general, just to start it, the idea is can querying this model be interventions
be used to improve the performance of those fully connected models?
Well, the answer is yes.
So here is how I perform interventional queries.
So I present an image to the network.
I fix the error of the pixels to be equal to zero.
So this error doesn't get propagated in the network.
And then I compute the label.
And as you can see, the accuracy improves, for example, from 89 using the standard query
method of pretty difficult in networks to 92, which is the accuracy after the intervention
and the same happens for fashion means.
And I think that a very legit critic that probably everyone would think when seeing
those plots is that, OK, you improve on means from 89 to 92, it still sucks, basically.
And yeah, it's true.
And I'm actually in the later slides, I'm going to show how to act on the structure
of this fully connected model will improve the results even more until the point they
reach a performance that is not even close to state of the art performance, of course.
But it's still up to a level that becomes basically acceptable and worth investigating.
So yes, so this is the part about causal inference using predictive coding.
And I guess to summarize, I can say that the interesting part of the results I just showed
is that I showed that predictive coding is able to perform interventions in a very easy
and intuitive way because you don't have to act on the structure of the old graph anymore.
Sometimes those functions are not available, so forth and so on.
But you simply have to intervene on a single neuron, set its prediction error to zero and
perform an energy minimization process.
And these extended allowed us to define predictive coding based structural causal models.
Now we move to the second part of the work, which is about structure learning.
So structure learning, as I said, deals with the problem of learning the causal structure
of the model from observational data.
This is actually no problem that has been around for decades and has always been, until
a couple of years ago, tackled using combinatorial search methods.
The problem with those combinatorial search methods is that their complexity grows double
exponentially.
So as soon as the data becomes multidimensional and the Bayesian graph that you want to learn
grows in size, learning it, it's incredibly slow.
The new solution that came out actually a couple of years ago in a newspaper from 2018
showed that it's possible to actually learn this structure, not using a combinatorial
search method, but by using a gradient-based method.
And this was basically this killed the problem in general because now you can simply apply
your on the parameters, which is the prior proposed that I'm going to define a little
bit better in this slide, around gradient descent.
And even if you have a model that is double, triple the size, the algorithm is still incredibly
fast.
And for this reason, this paper is, yeah, I think it's kind of new and I think already
has around 600 citations or things like that.
And every paper that I'm seeing now about causal inference and learning causal structure
of the graph uses their method.
It just changes a little bit, they find faster or slightly better inference methods, but
still they all use the prior, this paper defined, and I do as well, and we do as well.
So here we define a new quantity, which is the agency matrix.
The agency matrix is simply a matrix that encodes the connections of the model.
So it's a binary matrix, and in general, it's a binary matrix.
Then of course, when you do gradient-based optimization, you make it continuous and then
you have some threshold at some point that basically kills an edge or set it to one.
The entry ij is equal to one if the Bayesian graph has an edge from vertex i to vertex j
or zero otherwise.
So for example, this agency matrix here represents the connectivity structure of this Bayesian
network.
And basically this method tackles two problems that we want about learning the structure
of the Bayesian network.
The idea is that we start from a fully connected model, which conceptually is similar, actually
is equivalent to the predictive coding network I defined earlier, which is fully connected.
So you have a lot of vertices and every pair of vertices is connected by two different
edges, and you simply want to prune the ones that are not needed.
So it can be seen as a method that performs model reduction.
You start from a big model and you want to make it small.
So what's the first ingredient to reduce models?
Well, it's of course sparse city.
And what's the prior that everyone uses to make a model more sparse is the Laplace prior,
which in machine learning is simply known as the L1 norm, which is defined here.
The solution that this paper that I mentioned earlier proposed is to add a second prior
on top, which enforces what's probably the biggest characteristic of Bayesian networks
on which you want to perform causal inference, is that you want them to be acyclic.
And basically they show that acyclicity can be imposed on an agency matrix as a prior,
and it has this shape here.
So it's the trace of the matrix that is the exponential of A times A, where A is the
agency matrix again.
And basically this quantity here is equal to zero if and only if the Bayesian network
or whatever graph you're considering is acyclic.
So I'm going to use these in some experiments, so force those two priors on different kinds
of Bayesian networks, and I'm trying to merge them with the techniques we proposed earlier
about performing causal inference via predictive coding.
So I'm going to present two different experiments.
So one is a proof of concept, which is the standard experiments showed in all the structural
learning tasks, which is the inference of the correct Bayesian network from data.
And then I'm going to build on top of the classification experiments I showed earlier,
and show how actually those priors allow us to improve the classification accuracy, the
test accuracy of fully connected predictive coding models.
So let's move to the first experiment, which is to infer the structure of the graph.
And the experiments, they all follow basically the same pipeline in all the papers in the
field.
The first step is to generate a Bayesian network from random graph.
So basically normally the two random graphs that everyone tests are Erdos-Renis graphs
and scale-free graphs.
So you generate those big graphs that normally have 20, 40, 80, 80 different nodes and some
edges that you sample randomly.
And you use this graph to generate a data set.
So you sample, for example, N, big N data points.
And what you do is that you take the graph that you have generated earlier and you throw
it away.
You only keep the data set.
And the task you want to solve now is to have a training algorithm that basically allows
you to retrieve the structure of the graph you have thrown away.
So the way we do it here is that we train a fully connected predictive coding model on
this data set D, using both the sparse and the acyclic priors we have defined earlier.
You can see whether actually the graph that we converge to, after pruning away the entries
of the agency matrix that are smaller than a certain threshold, is similar to that of
the initial graph.
And the results show that this is actually the case.
So this is an example and I show many different parametrizations and dimensions and things
like that in the paper.
But I think those two are the most representative examples with an air nosher in a graph and
a free scale graph with 20 nodes.
And here on the left, you can see the ground truth graph, which is the one sampled randomly.
And on the right, you can see the graph, the predictive coding model as learned from the
data set.
And as you can see, they are quite similar.
It's still not perfect.
So there are some errors, but in general, the structures, they work quite well.
We also have some quantitative experiments that I don't show here, because they're just
huge tables with a lot of numbers and I thought it was maybe a little bit too much for the
presentation.
But there is also that they perform similarly to contemporary methods.
Also because I have to say most of the quality comes from the acyclic priors that was introduced
in 2018.
The second class of experiments are classification experiments, which as I said, are the extensions
of the one I shared earlier.
And the idea is to use structure learning to improve the classification results on the
means and fashion means data set, starting from a fully connected graph.
So what I did is that I divided the fully connected graph in clusters of neurons.
So 1B cluster is the one related to the input.
And then we have some specific number of hidden clusters.
And then we have the label cluster, which is the cluster of neurons that are supposed
to give me the label predictions.
And I've trained them using the first time, the sparse prior only.
So the idea is, what if I prune the connections I don't need from a model and learn a sparser
model?
Does this work?
Well, the answer is no.
It doesn't work.
And the reason why is that at the end, the graph that you converge with is actually degenerate.
So basically, the model learns to predict the label based on the label itself.
So it discards all the information from the input and only keeps the label.
And as you can see here, the label y predicts itself.
Or in other experiments, when you change the parameters, you have that y predicts at zero,
that predicts x1, that predicts y again.
So what's the solution to this problem?
Well, the solution to this problem is that we have to converge to an acyclic graph.
And so we have to add something that prevents acyclicity.
And what is that?
One is, of course, the one I already proposed.
And then I show a second technique.
So the first one uses the acyclic prior defined earlier.
And the second one is a novel technique that actually makes use of negative examples.
So a negative example in this case is simply a data point in which you have an image, but
the label is wrong.
So here, for example, you have an image of a 7, but the label that I'm giving the model
is a 2.
And the idea is very simple and has been used in a lot of works already.
So every time the model sees a positive example, it has to minimize the variational free energy.
And every time it sees a negative example, it has to increase it.
So we will want this quantity to be minimized.
And actually, with a lot of experiments and a lot of experimentations, we saw that the
two techniques basically first lead to the same results and second lead to the same graph
as well.
So here are the new results on means and fashion means using the two techniques that I just
proposed.
And now we move to some which are still not great, but definitely more reasonable test
accuracies.
So here we have a test error of 3.17 for means and a test error of 13.98 for fashion means.
Actually, those results can be much improved by learning the structure of the graph on
means and then fixing the structure of the graph and do some form of fine tuning.
So if you fine tune the model on the correct hierarchical structure, at some point you
reach the test accuracy, which is the one you would expect from a hierarchical model.
But those ones are simply the one, the fully connected model as naturally converged to.
So for example, from a test error of 18.32 of the fully connected model train on fashion
means by simply performing correlations or conditional queries, which is the standard
