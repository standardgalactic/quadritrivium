a big role in this, but the education level does.
If the income level doesn't change much, it means that maybe there's a hidden variable,
in this case, the intelligence that determines the income level of a person.
The third quantity important in causal inference is that of counterfactuals.
So for example, a counterfactual answers the question, what would be, had we changed C
to a different value in the past?
So for example, we can see that the difference between interventions and counterfactuals is
that interventions act in the future.
So I'm interviewing in the world now to observe a change in the future.
Well, counterfactual allow us to go back in time and change a variable back in time and
see how the change would have influenced the world we live in now.
And those are defined by Judea Perle as the three levels of causal inference.
Correlation is the first level, intervention is the second level, and counterfactual is
the third level.
What are interventions?
I'm going to define them more formally now, how that I gave an intuitive definition.
And I'm using this notation here, which is the same actually throughout all the presentation.
So X is always going to be a latent variable, SI is always going to be a data point or an
observation, and VI is always going to be a vertex.
So every time you see VI, we're only interested in the structure of the graph, for example.
So let's assume we have a Bayesian model, which has the same structure as the Bayesian
model we saw in the previous slide.
Given that X3 is equal to S3, this is the observation we make, statistics allows us to compute the
probability or the expectation of X4, which is the latent variable related to this vertex,
given that X3 is equal to S3.
To perform an intervention, we need a new kind of notation, which is called the do operation.
So in this case, X4, we want to compute the probability of X4, given the fact that we
intervene in the world and change X3 to S3.
And how do we do this?
To perform an intervention, Judea Perl tells us that we have to have an intermediate step
before computing a correlation, is that first we have to remove all the incoming edges to
V3.
So we have to study not this Bayesian network, but this second one.
And then at this point, we are allowed to compute a correlation, as we normally do.
And this is an intervention.
A counterfactual is a generalization of this that, as I said, lived in the past, and they
are computing using structural causal models.
A structural causal model is a tuple, which is conceptually similar to a Bayesian network.
But basically, we have this new class of variables on top, which are the unobservable variables
they use.
So we have the Bayesian network that we had before, X1, X2, X3, S4.
But we also have those unobservable or variables that depend on the environment.
You cannot control them, you can infer them, but they are there.
And F is a set of functions that depends on all the, basically, F of X3 depends on X1,
because you have an arrow, on X2, because you have an arrow, and on the unobservable
variable that also influences X3.
So yes, intuitively, you can think of a structural causal model as a Bayesian network with those
unobservable variables on top, and each unobservable variable only influences its own, its own
related variable X.
So, for example, IU will never touch X1 as well.
U3 will only touch U3, U1 will only influence X1, and so forth, and so on.
So performing counterfactual inference answers the following question.
So what would X4 be at X3 being equal to another variable in a past situation, U?
And computing this counterfactual requires three different steps.
So abduction is the computation of all the background variables.
So in this step, we want to go back in time and understand how the environment, the unobservable
environment, was in that specific moment in time.
And we do this by fixing all the latent variables X to some specific data that we already have,
and performing this inference on the use.
Then we're going to use the U to keep the U that we have learned, and perform an intervention.
So a counterfactual can also be seen as an intervention back in time, in which we know
the environment variables U1, U2, and U4 in that specific moment.
And what's the missing step?
So what would X4 be at X3 being equal to another data point in that specific situation?
Now we can compute a correlation.
And the correlation, we do it on the graph in which we have already performed an intervention
using the environment variables that we have learned in the abduction step.
And this is a counterfactual inference.
This is the last slide of the causal inference introduction, and it's about structure learning.
Basically, everything I've said so far relies on the fact that we know the causal dependencies
among the data points.
So we know the structure of the graph, we know which variable influences which one,
we know the arrows in general.
But in practice, this is actually not always possible.
So we don't have access to the causal graph most of the times.
And actually learning the best causal graph from data is still an open problem.
We are improving in this.
We are getting better.
But how to perform this task exactly is still an open problem.
So as I said, basically, the goal is to infer causal relationships from observational data.
So given a data set, we want to infer the directed acyclic graph that describes the
connectivity between the system and the variables of the data set.
So for example here, we have an example that I guess we are all familiar with thanks because
of the pandemic.
So we have those four variables, age, vaccine, hospitalization, and CT.
And we want to infer the causal dependencies among those variables.
So for example, we want to learn directly from data that the probability of a person
being hospitalized depends on its age and on the fact whether it's vaccinated or not,
and so forth and so on.
So this is the end of the long introduction, but I hope it was clear enough and I hope
that I gave the basics to understand basically the results of the paper.
And now we can go to the research questions.
So the research questions are the following.
First I want to see whether creative coding can be used to perform causal inference.
So creative coding so far has only been used to perform two compute correlations in Bayesian
networks.
And the big question is, can we go beyond correlation and model intervention and counterfactual
in a biological, plausible way?
So in a way that it's, for example, simple, intuitive, and allow us to only play with
the neurons and not touch, for example, the huge structure of the graph.
And more in practice, more specifically, the question becomes, can we define a creative
coding-based structural causal model to perform interventions and counterfactuals?
The second question is, as I said, that having a structural causal model assumes that we
know the structure of the Bayesian network.
So it assumes that we have the arrows.
Can we go beyond this and use creative coding networks to learn the causal structure of
the graph?
Basically, giving positive answers to both those questions would allow us to use creative
coding as an end-to-end causal inference method, which basically takes a data set and allow
us to test interventions and counterfactual predictions directly from this data set.
So let's tackle the first problem, so causal inference via creative coding, which is also
the section that gives the title to the paper, basically.
And here I will show how to perform correlations with creative coding, which is already known,
and how to perform interventional queries, which I think is the real question of the paper.
So here is a causal graph, which is the usual graph that we had.
And here is the corresponding creative coding model.
So the axes are the latent variables and correspond to the neurons in a neural network
model.
And the black arrow passes prediction information from one neuron to the one down the hierarchy.
And every vertex also has this error neuron, which passes information up the hierarchy.
So the information of every error goes to the value node in the up the hierarchy and
basically tells it to correct itself to change the prediction.
So to perform a correlation using creative coding, what you have to do is that you take
an observation and you simply fix the value of a specific neuron.
So if you want to compute the probability of X4 given X3 equal to S3, we simply have
to take X3 and fix it to S3 in a way that it doesn't change anymore and run an energy
minimization.
And this model, by minimizing, by updating the axes via a minimization of the variational
free energy, allows the model to converge to a solution to this question.
So the probability or the expected value of X4 given X3 equals 3.
But how do I perform an intervention now without acting on the structure of the graph?
Well, this is basically the first idea of the paper.
This is still how to perform a correlation.
So fix S3 equal to X3 is the first step in the algorithm.
And the second one is to update the axes by minimizing the variational free energy.
An intervention, which in theory corresponds in removing those arrows and answers to the
question, the probability of X4 by performing an intervention, so do X3 equal S3?
This coding can be performed as follows.
So I'm going to write the algorithm here.
So first, as in a correlation, you fix X3 equal to the observation that you get.
Then this is the important step.
You have to intervene not on the graph anymore, but on the prediction error and fix it equal
to zero.
Assuming a prediction error equal to zero basically makes sense, meaning less information
up the hierarchy or actually sends no information up the hierarchy because it basically tells
you that the prediction is always correct.
And the third step is to, as we did before, to update the axes, the unconstrained axis
or X1, X2, X4 by minimizing the variational free energy.
As I will show now experimentally, by simply doing this little trick of setting a prediction
error to be equal to zero, it prevents us to actually act on the structure of the graph
as the theory of Duke-Alculus does and to infer the variables after an intervention
by simply performing a variational free energy minimization.
What about counterfactual inference?
Counterfactual inference is actually easy once we have defined how to do an intervention.
And this is because, as we saw earlier, performing a counterfactual is similar to performing an
intervention in a past situation after you have inferred the unobservable variables.
So as you can see in the plot I showed earlier about the abduction action and prediction
steps, the action and prediction steps, they did not have those two arrows.
They were removed.
Pretty coding allows us to keep the arrows in the graph and perform counterfactuals by
simply performing an abduction step, as it was done earlier, an action step in which we
simply perform an intervention on the single node.
So we fix the value node and we set the error to zero and run the energy minimization, so
minimizing the variational free energy to compute the prediction.
So I think this is like an easy and elegant method to perform interventions and counterfactuals.
And yeah, so I think the thing we have to show now is whether it works in practice or not.
And we have a couple of experiments.
And I'm going to show you now two different experiments.
The first one is merely proof of concept experiment that shows that the predictive coding is able
to perform intervention and counterfactuals.
And the second one actually shows a simple application in how interventional queries
can be used to improve the performance of classification tasks on a specific kind of
predictive coding networks, which is that of a fully connected model.
Let's start from the first one.
So how do we do this task?
So given a structural causal model, we generate training data and we use it to learn the weights,
so to learn the functions of the structural causal models.
And then we generate test data for both interventional and counterfactual queries.
And we show whether we are able to converge to the correct test data using predictive
coding.
And for example here, those two plots represent the interventional and counterfactual queries
of this specific graph, which is the butterfly bias graph, which is a graph that is often
used in testing whether causal inference, whether interventional and counterfactual
techniques work is as simple as that.
But in the paper, you can find a lot of different graphs.
But in general, those two plots show that the method works, show that the mean absolute
error between the interventional and counterfactual quantities we compute and the interventional
and counterfactual quantities from the original graph are close to each other.
So the error is quite small.
The second experiment is basically an extension of an experiment I proposed in an earlier
paper, which is the learning on arbitrary graph topologies that I wrote last year.
In that paper, I basically proposed this kind of network as a proof of concept, which is
a fully connected network, which is in general the worst neural network you can have to perform
machine learning experiments, because given a fixed set of neurons, basically every pair
of neuron is connected by two different synapses.
So it's the model with the highest complexity possible in general.
The good thing is that since you have a lot of cycles, the model is extremely flexible
in the sense that you can train it, for example, on a minst image and on a data point and on
its label.
But then the way you can query it, thanks to the information going back, is you can query
