way of querying operative coding model, adding interventions and the acyclic prior together
makes this test error much lower.
And we can observe it for means as well.
I'm now going a little bit into details on this last experiment and on how the acyclic
prior acts on the structure of the graph.
So I perform an experiment on a new dataset, which is, I mean, calling it a new dataset,
it may be too much, is the, I called it a two means dataset in which you have the input
point is formed of two different images and the label only depends on the second image.
On the first image story.
So the idea here is, is the structure of the model, the acyclic, the acyclicity prior and
things like that able to recognize that the second half of the image is actually meaningless
in, in performing, in learning the in performing classification.
How does training behave in general?
Like, for example, we have this input, input node, output node, and only the nodes are
fully connected and the model converge to a hierarchical structure, which is the one
that we know performs the best on, on classification tasks.
Well, here is a, is an example of a training method of a training run.
So that's C zero, which is the beginning of training.
We have this model here.
So as zero corresponds to the, to the seven, so to the first image as one corresponds to
the second image, again, we have the label Y and all the latent variables X zero X one
X two, and the model is fully connected.
So the agency matrix is, is full of ones.
There are, there are no zeros.
We have self loops and things like that.
We train them at the model for a couple of epochs until, and what we note immediately is
that, for example, the, the model immediately understands that the four is not needed to
perform classification.
So it doesn't.
So every outgoing node from the, from the second input cluster is removed.
And something we didn't understand is that this is, this cluster is the one related to
the output.
So we have a, we have a linear map from S zero to Y directly, which is this part here.
But we know that actually a linear map is not the best map for, for performing classification
on means.
So we, we need some hierarchy.
We need some depth to, to improve the results.
And as you can see, this line here is the, is the accuracy, which up to this point, so
up to C2 is similar to a, so it's 91%, which is slightly, slightly better than linear
classification.
But once you go on with the training, the model understands that it needs some hierarchy
to better fit the data.
So you, you see that this arrow starts getting stronger and stronger over time until it, it
understands that the linear map is not actually really needed and it removes it.
And so the, so the model you converge with is a model that starts from a zero, goes to
a hidden node and then goes to the, to the label with a very weak linear map, which actually
gets removed if you, if you set that threshold of, if you set that threshold of, for example,
0.1, 0.2, at some point, the linear map gets forgotten.
And everything you end up with is with a, is with a hierarchical network.
That is, that is, so it has learned the correct structure to, to perform classification tasks,
which is a hierarchy.
And it has also learned that the second image didn't play any role in defining the,
the test accuracy.
And this is all, this is all performed.
So all those jobs are simply performed by, performed by one free energy minimization
process.
So you initialize the model, you define the free energy, you define the priors.
So the, the sparse and the cyclic prior, you run the, the energy minimization and you converge
to hierarchical, to a hierarchical model, which is well able to perform classification on minced.
And then if you then perform some fine tuning, you reach very competitive results as you do in
feed forward networks with the, with back propagation.
But I think that's not the interesting bit.
The interesting bit is that you, like all this process, this process altogether
of intervention and the acyclicity allows you to take a fully connected network
and converge to a hierarchical one that is, that is able to perform classification with good results.
And yeah, that's basically it.
I'm now, oh yeah, wow, I've talked a lot.
And I'm, this is the conclusion of the talk, which is, I'm basically doing a small summary.
And I think the, the important takeaway if I have to give even one sentence of this paper
is that predictive coding is a belief updating method that is able to perform end to end causal
learning. So it's able to perform interventions to learn a structure from data and then perform
interventions and counterfactuals. So causal inference in natural and efficiency model
interventions by simply setting the prediction error to zero.
So it's a, it's a very easy technique to perform interventions.
And you simply only have to touch one neuron, you don't have to act on the structure of the graph.
You can, you can use it to perform, to, to create structure causal models that are biologically
plausible. It is able to learn the structure for, from data, as I said, maybe a lot of times already.
And, and a couple of sentences about future works is that
something that would be nice to do is to improve the performance of the model we,
we have defined, because I think it performs reasonably well on a lot of tasks.
So it performs reasonably well on structure learning on, for me, intervention and counterfactuals.
But actually, if you look at state of the art model, there's always like a very specific method
that performs better in a, in the single task. So it would be interesting to see if we can
reach those level of performance in, in specific tasks by, by adding some tricks on, or some,
or some new optimization methods, and to generalize it to, to dynamical systems,
which are actually much more interesting, the static systems. So such as dynamical causal models
and, or other techniques that allow you to perform causal inference in systems that move.
So an action taken in a specific time step influences another node in a later time step,
which is basically Granger causality. Yeah, that's it. And thank you very much.
Thank you. Awesome. And very comprehensive presentation. That was really
muted. Sorry, muted on zoom. But yes, thanks for the awesome and very comprehensive
presentation. There was really a lot there. And there was also a lot of great questions
in the live chat. So maybe to warm into the questions, how did you come to study this
topic? Were you studying causality and found predictive coding to be useful or vice versa?
Or how did you come out this intersection? I actually have to say that the first person
that came out with this idea was, was better. So, so like, like, I think a year and a half ago,
even more, he wrote like a page with this idea. And then he got forgotten, and no one picked it up.
And, and last summer, I started getting curious about causality and
I read, for example, the book of why as I listen into podcasts, I don't know the
standard way in which you get interested in a topic. And, and I remember this,
this idea from Baron and proposed it to him. And I was like, why don't we expand it and,
and actually make it a paper. So I, I involve some people to work with experiments and,
and this is the final result at the end. Awesome. Cool. Yeah.
Um, a lot to say. I'm just going to go to the live chat first and address a bunch of different
questions. And if anybody else wants to add more, I'm going to turn the light on first,
because I'm, I think I'm getting in the dark more and more. Yes.
Who said active inference can't solve the dark room issue? Oh, yes, here we are.
So would you say the light switch caused it to be lighter?
Yeah, I think so. No issues here. Um, okay. ML Don wrote since in predictive coding,
all distributions are usually Gaussian, the bottom up messages are precision weighted
prediction errors where precision is the inverse of the Gaussian covariance.
What if non Gaussian distributions are used?
Is, um, basically the general method stays, the different, the main difference is that you,
you don't have prediction errors, which, uh, as was correctly pointed out is the,
basically the derivative of the variational free energy. If you have Gaussian assumptions,
yeah, you don't have that single quantity to set to zero.
And you probably will have to act on the structure of the graph to perform interventions.
And also you, uh, and colleagues had a paper in 2022 predictive coding beyond Gaussian
distributions that, that looked at some of these issues, right?
Yes, yes, exactly. So that paper was a little bit, the idea behind that paper is, uh,
and we model transformers. That's the biggest motivation using predictive coding.
And the answer is, uh, is no, because the, the attention mechanism as a softmax at the end,
and softmax calls to, uh, like not to Gaussian distribution, but to,
yeah, to softmax distribution, the, I don't get the name now, but yes.
And, uh, so yes, that's a generalization. It's a little bit
tricky to call it. Once you remove the Gaston assumption is a little bit
still tricky to call it predictive coding. So it's a,
so for, for example, like talking to, uh, to Carl Freestone,
like predictive coding is only if you, if you have only Gaussian, Gaussian assumptions.
But yes, that's more a philosophical debate than, uh,
Interesting. And another, I think topic that, that's definitely of, of great interest is
similarities and differences between the attention apparatus in transformers
and the way that attention is described from a neurocognitive perspective and from a predictive
processing precision waiting angle. What do you, what do you think about that?
Well, the idea is that, um, yeah, I think about it is that in from a pretty processing
and, uh, and also operational inference perspective, attention can be seen as a,
as a kind of structure learning problem. There's a, I think there's a recent paper from,
from Chris Buckley's group that shows that there should be, there should be a reprint on archive
in which basically they show that the attention mechanism is simply
learning the, the precision on the, on the weight parameters specific to out to a data point.
So this precision is not a, is not a, is not a parameter that is in the structure of the model.
So it's not a model specific parameter. It is a fast changing parameter like the value nodes
that gets updated while minimizing the version of free energy.
And once they, once you've minimized it and compute it, then you throw it away.
And from the next data point, you have to really compute it from scratch.
So yes, I think the, the analogy computation wise is, uh, the attention mechanism can be seen as
a kind of structure learning, but a structure learning that is data point specific and not
model specific. And I think if you want to generalize a little bit and go from,
from the attention mechanism in transformers to the attention mechanism cognitive science,
I feel they're probably too different to, like to draw similarities and, uh,
I think the structure learning analogy and the, how important one connection in is
with respect to another one probably does job much better.
Cool. Great answer. Okay. ML Don asks, in counterfactuals, what is the difference
between hidden variables X and unobserved variables U?
The difference is that you can, uh, I think the main one is that you cannot observe the,
the use. You can use them because you can, you can compute them and fix them,
but you cannot, the idea is that you have no control over them. So the use,
the use should be seen as a environment specific variables that they are there. They,
they influence your process. Okay. Because the, for example, when you go back in time,
the environment is different. So the idea is for example, if you,
like going back to the, to the example before of the, of the expected income of a person with
a specific intelligence of education, uh, uh, education degree, the idea is that if I want to,
to see how much I will learn today with a, with a, with a, I don't know, with a master degree,
is different with respect to how much I would earn 20 years ago with a master degree is different.
For example, here in Italy with respect to other countries and all those variables that are not
under your control, you can not model them using your vision network, but they are there.
Okay. So you, you cannot ignore them when you, when you want to draw conclusions.
So it's, yeah, it's basically everything that you cannot control. You can infer them. So you
can, you can perform a counter counterfactual inference back in time and say, Oh, 20 years
ago, I would have earned this much if I, if I was disintelligent at this degree on average,
of course. And, but it's not that I can change the government policies towards jobs or the,
or things like that. It's a deeper counterfactual. Yes, exactly. So yeah, those are the use.
Awesome. All right. Have you implemented generalized coordinates in predictive coding?
No, I've, no, I've never done it. I've, uh, yeah, I've studied it, but I've, I've never
implemented it. I know they tend to be unstable and, uh, and it's very hard to make them stable.
I think that's the, that's the takeaway that I got from talking to people that have implemented them.
But, but yeah, yeah, I'm aware of some papers that came out actually recently about them that,
that tested on some threshold encoder style. Actually, I think still from Baron,
there's a, there's a paper out there that came out last summer, but no, I've never played them with
them myself. Cool. From Bert, does adding more levels in the hierarchy reduce the distraction
problem of predicting input? Adding more level in, uh, in which sense, because the
destruction problem is given by cycles. So basically you provide an image and the fact that you have
a, so edges going out of the image, going in the, in the neurons, and then other edges going back,
the, this basically creates the fact that you have a, that the error of, that those basically,
these ingoing edges to the pixels of the image, they create some prediction errors. So you have
some prediction errors that get spread inside the model. And that's, yeah, and this problem,
I think is general of cycles. And it's probably not related to hierarchy in general.
So it's, it's, it's the two incoming edges to the pixels. If you don't have incoming edges,
you have no, uh, no distraction problem anymore. Cool. And, and the specification of the acyclic
network through the trace operator, that's a very interesting technique. And when was that
brought into play?
As far as I know, I think it came out with a paper I, I cited in 2018. I, I don't know,
at least in the causal inference literature, I'm, I'm not aware of any previous methods.
I would say no, because that, I mean, that's the highly cited paper. So I would say they came out
with that idea. Wow. Yeah. That's, that's quite nice that you can do gradient descent and learn
the structure. I think that's a, that's a very powerful technique. Yeah. Sometimes it's like
when you look at when different features of Bayesian inference and causal inference became
available, it's really remarkable. Like why, why, why hasn't this been done under a Bayesian causal
modeling framework? It's like, because there's only been like five to 25 years of this happening.
And so that's very, very short. And also it's relatively technical. So there's relatively
