Hello and welcome.
It's active inference gas stream number 51.1 on July 28th, 2023.
We are here with Tomaso Salvatore and we will be having a presentation and a discussion
on the recent work, causal inference via predictive coding.
So thanks so much for joining.
For those who are watching live, feel free to write questions in the live chat and off
to you.
Thank you.
Thank you very much, Daniel, for inviting me.
It's been a big fan of the channel and I've been watching a lot of videos, so I'm quite
excited to be here and be the one speaking this time.
So I'm going to talk about this recent preprint that I put out, which has been the work of
the last couple of months.
And it's a collaboration with Luca Vincetti, Amin Makarak, Bernmille and Thomas Lukasiiewicz.
It's basically a joint work between Versus, which is the company I work for, the University
of Oxford and Theo Vien.
So during this talk, I will, this is basically the outline of the talk, I will start talking
about what predictive coding is and give an introduction of what it is, a brief historical
introduction, why I think it's important to study predictive coding, even for example
for the machine learning perspective.
I will then provide a small intro to what causal inference is.
And once we have all those informations together, I will then discuss why I wrote this paper,
what was basically the research question that inspired me and the other collaborators.
And present the main results, which are how to perform inference, so intervention and
counterfactual inference, and how to learn the causal structures from a given data set
using predictive coding.
And then I will of course conclude with a small summary and some discussion on why I
believe this work can be impactful in some future directions.
So what is predictive coding?
Predictive coding is in general famous for being a neuroscience inspired learning method,
so a theory of how information processing in the brain works.
And brain formally speaking, the theory of predictive coding can be described as basically
having a hierarchical structure of neurons in the brain and you have two different families
of neurons in the brain.
The first family is the one in charge of sending prediction information, so neurons in a specific
level of the hierarchy send information and predict the activity of the level below.
And the second family of neurons is that of error neurons.
And the error neurons, they send prediction error information up the hierarchy.
So one level predicts the activity of the level below.
This activity has some, this prediction has some mismatch, which we were actually going
on in the level below.
And the information about the prediction error gets sent up the hierarchy.
However, predictive coding was actually not burned as a neuroscience, as a theory from
neurosciences, but it was actually initially developed as a method for signal processing
and compression back in the 50s.
So the work of Oliver, Elias, which are actually contemporary of Shannon, they realized that
once we have a predictor, a model that works that is well in predicting data, sending messages
about the error in those predictions is actually much cheaper than sending the entire message
every time.
And this is how predictive coding was born, so as a signal processing and compression
mechanism in information theory back in the 50s.
He was actually in the 80s, that he became that exactly the same model was used in neuroscience.
And so with the work from Mumford or other works, for example, explain how the rate enough
processing formation, so we get prediction signals from the outside world, and we need
to compress these representation and have this internal representation in our neurons.
And the method is very similar, if not equivalent to the one that was developed by Elias and
Oliver in the 50s.
Maybe what's the biggest paradigm shift, happening in 1999, thanks to the work of Raoul
and Ballard, in which they introduced this concept that I mentioned earlier about hierarchical
structures in the brain, where prediction information is top down and error information
is bottom up.
And something that they did that wasn't done before is that they explain and develop this
theory about not only inference, but also about how learning works in the brain.
So it's also a theory of how our synapses get updated.
And the last big breakthrough that I'm going to talk about in this brief historical introduction
is from 2003, but then it kept going in the years after, thanks to Carfriston, in which
basically he took the theory of Raoul and Ballard, and he extended it and generalized
it to the theory of generative models.
So basically the main claim that Carfriston did is that predictive coding is an evidence-maximization
scheme of a specific kind of generative model, which I'm going to introduce later as well.
So to make a brief summary, the first two kinds of predictive coding that I described,
so signal processing and compression and the information processing in the retina and in
the brain in general, they are inference methods.
And the biggest change, the biggest revolution that we had in 1999, so let's say in the 21st
century, is that predictive coding was seen as a learning algorithm.
So we can first compress information and then update all the synapses or all the latent
variables that we have in our generative model to improve our generative model itself.
So let's give some definitions that are a little bit more formal.
So predictive coding can be seen as a hierarchical Gaussian generative model.
So here is a very simple figure in which we have this hierarchical structure, which can
be as deep as we want.
And prediction signals go from one latent variable, Xn, to the following one, and it
gets transformed every time via function gn, or gi.
And this is a generative model, as I said, and what's the marginal probability of this
generative model?
Well, it's simply the probability of the last, can you see my cursor?
Yes, right?
Yes, perfect.
So it's the generative model of the last vertex, is the distribution of the last vertex, times
the probability distribution of every other vertex, conditioned on the activity of the
vertex before, or the latent variable before.
I earlier said that it's a Gaussian generative model, which means that those probabilities
they are in Gaussian form, and those function g, in general, and especially since, for
example, in a round baller paper, and in all the papers that came afterwards, also because
of the deep learning revolution, those functions are simply linear maps, or nonlinear maps with
activation functions, or nonlinear maps with activation function and an additive bias.
So we can give a formal definition of predictive coding, and we can say that predictive coding
is an inversion scheme for such a generative model, where its model evidence is maximized
by minimizing a quantity that is called the variational free energy.
In general, the goal of every generative model is to maximize model evidence, but this quantity
is always intractable, and we have some techniques that allow us to approximate the solution.
And the one that we use in predictive coding is minimizing a variational free energy, which
is a lower bound of the model evidence.
In this work, and actually in a lot of other ones, so is the standard way of doing it,
this minimization is performed via gradient descent, and there are actually other methods
such as expectation maximization, which is often equivalent, or you can use some other
message-passing algorithms such as belief propagation, for example.
And going a little bit back in time, so we're getting a little bit about the statistical
generative models, we can see predictive coding, as I said already a couple of times, as a
hierarchical model with neural activities, so with neurons, latent variables that represent
neural activities, they send their signal down the hierarchy, and with error nodes or
error neurons, they send their signal up the hierarchy, so they send the error information
back.
What's the variational free energy of these class-operated coding models?
It's simply the sum of the mean square error of all the error neurons, so it's the sum
of the total error squared.
And this representation is going to be useful in the later slides, and I'm going to explain
how to use predictive coding to model causal inference, for example.
What do you think predictive coding is important and is a nice algorithm to study?
Well, first of all, as I said earlier, it optimizes the correct objective, which is
the model evidence or marginal likelihood, and then it does so by optimizing a lower bound,
which is called the variational free energy, as I said, and the variational free energy
is interesting because it can be written as a sum of two different terms, which are
and each of those terms optimizing it as important impacts, for example, in machine learning
tasks or in general in learning tasks.
So one of those terms forces memorization.
So the second term basically tells forces the model to fit a specific data set.
And the first term forces the model to minimize the complexity.
And as we know, for example, from the Occam's razor theory, if we have two different models
that perform similarly on a specific training set, the one that we have to get and the one
that is expected to generalize the most is the less complex one.
So updating generative model via variational free energy allows us to basically converge
to the optimal Occam razor model, which both memorizes a data set, but is also able to
generalize very well on unseen data points.
A second reason why predictive coding is important is that it actually doesn't have
to be defined on a hierarchical structure, but it can be modeled on more complex and
flexible architectures such as directed graphical model with any shape or generalized even more
to networks with a lot of cycles that resemble brain region.
And the underlying reason is that you're not learning and predicting with a forward
pass and then back propagating the error, but you're minimizing an energy function.
And this allows basically every kind of hierarchy to be, allows to go behind hierarchies and
allow to learn cycles.
And this is actually quite important because the brain is full of cycles as we have some
information from some recent papers that may manage to map completely the brain of some
animals such as fruit fly.
The brain is full of cycles.
So it makes sense to train our machine learning models or our models in general with an algorithm
that allows us to train using cyclic structures.
The third reason why predictive coding is interesting is that it has been formally proven
that it is more robust than standard neural networks trained with back propagation.
So if you have a neural network and you want to perform classification tasks, you, predictive
coding is more robust.
And this is interesting in tasks such as online learning, training on small datasets
or continuous learning tasks.
And the theory basically comes from the fact that imperative coding has been proved to
approximate implicit gradient descent, which is a different version of the explicit gradient
descent, which is the standard gradient descent used in the, in every single model basically.
And it's a variation that is more robust.
I think, okay, I did a quite a long intro to predictive coding.
I think I'm now moving to the second topic, which is causal inference and what's causal
inference?
Causal inference is a theory, is a very general theory that has been formalized the most by
Judea Perl.
He's definitely the most important person in the field of causal inference.
He wrote some very nice books.
For example, the book of why is highly recommended if you want to learn more about this topic.
And it basically tackles the following problem.
So let's assume we have a joint probability distribution, which is associated with a Bayesian
network.
This is going to be a little bit the running example through all the paper, especially with
your net with Bayesian networks of this shape.
Those Bayesian networks, the variables inside, they can represent different quantities.
So for example, a Bayesian network with this shape can represent the quantities on the
right.
So a socio-economical statue of an individual, its education level, its intelligence, and
its income level.
Something the classical statistics is very good at, and it's a while most used application,
is to model observations or correlations.
A correlation basically answered the question, what is the, if we observe another variable
C?
So for example, in this case, what is, what's the income level, the expected income level
of an individual, if I observe his education level?
And of course, if that person has a higher degree of education, for example, a master
or a PhD, I'm expecting general that person to have a higher income level.
And this is a correlation.
However, sometimes there are things that are very hard to observe, but they play a huge
role in determining those quantities.
So for example, it could be that the income level is much, much more defined by the intelligence
of a specific person.
And maybe that the intelligence, so if a person is intelligent, he's also most likely to have
a higher education level.
But still the real reason why the income is high is because of the IQ.
This cannot be studied by simple correlations and has to be studied by a more advanced technique,
which is called an intervention.
An intervention basically answers the question, what is the, if we change C to a specific
value?
So for example, we can take an individual and check his income level, and then change
its education level, so intervene on this word, and change his education level without
touching his intelligence, and see how much his income changes.
For example, if the income changes a lot, it means that the intelligence doesn't play
