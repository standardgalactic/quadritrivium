Or maybe there's another interpretation too. So I was just I don't know one way to look at it.
This question about the relationship between the internal and the external states after you've
not denoised them but decorrelated them in a specific way is
this topic of the synchronization map. So they go from just pointing out that okay we've
decorrelated internal and external states with these blanket states. Now we're going to ask how
these internal states might encode information about expected external states. So after we've
decorrelated systems how do they have a memory or a model of one another. For this we would need to
characterize a synchronization function sigma mapping the expected internal state to the expected
external state given the blanket state. So sigma mapping function of internal states of
blanket states is going to relate to the external states at a condition of blanket states.
So there's still the dependence of both internal and external states on the blanket state b
and now there's a sigma mapping that is going to connect internal and external states.
And lemma 2.1 goes through some details of that specification. So the first is proposing that
there is a function that maps internal and external states for any blanket state.
I don't know how generally it holds but or this would be cool to talk to the authors about
but it's sort of proposing this sigma function and maybe in certain cases it's a zero line maybe
in other cases there's some meaningful relationship but for biological systems it does seem like there's
a meaningful relationship so those are the systems that we want to explore modeling.
They then write in how to construct this synchronization map the key idea is to map an
expected internal state mu of b to an expected external state eta of b and to do this the
two points they provide are one to find a blanket state that maps to this expected internal state
i.e. by inverting mu and two from this blanket state find the corresponding expected external state
i.e. by applying eta. We now proceed to solving this problem. Given an internal state mu we study
the blanket state b such that the internal states of b equals mu. They provide some details
maybe that is meant to be external states i'm not sure. Here
they're framing that inference that mu is doing the internal states in terms of covariance matrices
which is in this equation 2.4 and then frames that matrix of unknowns as a system of linear
equations or those are kind of similar representations and then solves for in the vector space using this
equation 2.5. They use this matrix negative function which is the more penrose pseudo inverse
it would be helpful to have someone explain in a little more detail what matrix inverse is or
what degrees of freedom there are in choosing different kinds of matrix inverses but i just
found it interesting from wikipedia that a common use of the pseudo inverse is to compute a best fit
or least squares solution to a system of linear equation that locks a solution
another use is to find the minimum norm solution to a system of linear equations with multiple
solutions so this means that there might be multiple solutions that are equally well fit
and this just allows you to pick one fast maybe
and then they write that the key for the existence of a function sigma mapping expected internal
states to expected external states given blanket states is that for any two blanket states associated
with the same expected internal state these be associated with the same expected external state
this non degeneracy means that the internal states e.g. patterns of activity in the visual
cortex have enough capacity to represent all possible expected external states e.g. 3d scenes
of the environment that's formalized in this lemma so first was the point discussed before
that we're interested in positing the existence of this sigma function
and then point two so these are all points that are going to be equivalent to each other
point two is that for any two blanket states one and two that the if the two external states
that are equal I'm sorry if the two internal states are equal to mu's of those two b's then
it's going to map on to equal points in external world so the blanket is a is a map that conserves
certain properties of the relationship between internal and external states
then points three and four of the lemma 2.1 use this kernel and image notation
so here was an image that was kind of interesting about how spaces were mapped to each other so
so someone could definitely help explain what some of these mappings mean or what the implications
are and then this sideways u shape is the proper subset symbol so that's meaning that every element
of the first one is in the second one but b is a bigger subset so certain mappings between
the internal and blanket covariance and the external and blanket covariance
and then similar for the pi
figure two provides a visualization of some simulations drawn that support just from a
simulation perspective the existence of a useful synchronization map function
so figure two on the left side shows
the external states given blanket states so those are the orange
yeah external states up these are the flipped
sorry about that so the blue are the internal states the sigma mapping corresponding
to the internal states conditioned on the external states and they're doing a very accurate job
of mapping on to the external states in orange and then in b is a so-called non example
and i'd be curious what changed or what was different about this second example or is there
more than one way to break the model what are the cases where it does or doesn't have a mapping
then in section three they go from that sort of static estimate of variables that are just
having a specific relationship to one another to basing mechanics operating through time
so they write in order to study the time evolution of systems with a marco blanket
we introduce dynamics into the external blanket and internal states
henceforth we assume a synchronization map under the conditions of the lemma 2.1 so that was
what was discussed up here and now in contrast with figure one which was just the variables
by themselves now we're putting the t subscript and looking at how those variables are changing
through time we use a bacillus to depict an intuitive example of a marco blanket that persists
over time here the blanket state between the membrane and actin filaments of the cytoskeleton
which mediate all interactions between the internal states and the external medium external
states in example three one they write there are many examples of stochastic processes at a
gaussian steady state p to name a few stationary diffusion processes with initial conditions
x0 tilde p the time evolution is given by an ito stochastic differential equation appendix b
next slide that will be on the next slide and then also example three one more generally
any marco process at steady state p such as the zigzag process any mean zero gaussian process
at steady state and any random dynamical system at steady state p so questions for people who know
more or the authors would be what what kinds of systems fall under this category of stochastic
processes and what kind of real world systems would be modeled well or not by this formalism
and then going a little deeper into that first point in the examples appendix b points to this
helmholtz decomposition and that is conditioned on this ito stochastic differential equation
and again just questions for someone who knows more or for the authors what does this ito
calculation entail and what does or doesn't fit under this ito framework that would or wouldn't
work using some other definition uh just not sure about that but in this helmholtz supplement
there is a very nice figure 10 that is looking at in this kind of hill climbing perspective
so in an algorithm that's looking in its local neighborhood and doing a calculation to sort of
make local decision making and hill climb that action of a stochastic but hill climbing
algorithm up this gradient or down depending on which way you set it up is going to have
two components to its motion this straight up the mountain component like putting a ruler
on the mountain and the maximum slope and then another component that's at the same likelihood
like the same altitude same energy that's going at an isocontor just like you can always do for a
mountain so with that idea in mind it's possible to take the full dynamic of a stochastic trace
on the top left this is like this particle that is going different colors through time
that just like converging to the top of this mountain diffusing like to the bottom of the
bowl like a marble going to the bottom of the bowl and then that trajectory can be separated
into this spiral or I mean this oval the time irreversible component and then a time reversible
component that has a much more direct trajectory and that was in the section with remark 3 1
which is again related to the Edo stochastic differential equation which I just I'm not too
sure about but they write when the dynamics are given by this Edo stochastic differential
equation a Markov blanket of the steady state density does not necessarily imply that internal
and external states cannot influence each other so that's I think kind of the interesting
tension in question is how do certain mathematical definitions of conditional
independence of blanket states separating internal external states how does that partition
sometimes result in subsets or systems that don't influence each other versus systems that do
so in the case of systems that don't influence each other I was thinking of like an empty bottle
of air in a room of air like they're separated systems but they interact through the bottle
so they could transfer heat for example through the hot bottle could transfer heat to the room
but the systems would only interact conditioned on the temperature of the interface versus
a person in a room could be planning or thinking about the external states so just how do we think
about what kinds of systems are still having influence or a long-term implication for each other
even after conditioned on the interfacing a certain way and then also just how does this
example provided kind of show that that'd be good to walk through and the absence of reciprocal
influences between two states in the drift sometimes but not always implies conditional
independence okay going back to the main pecs and figures in figure four there's now looking at this
synchronization map and transition probabilities for processes at the Gaussian steady state
so on the left is we plot the synchronization map as in figure two only here the samples are
drawn from trajectories of a diffusion process with a marco blanket so now it's a temporal process
on the right panel there are the transition probabilities of that same diffusion process
for the blanket stay at two different times and this is called a joint distribution because there's
two distributions that are being looked at jointly and if you kind of went to one side and
looked through the cornfield one way you'd see this Gaussian p of b sub s and if you looked at
it the other way you'd see p of b through t and then they write which is kind of interesting
this shows that in general processes at a Gaussian steady state are not Gaussian processes
so that was just like very curious about what that meant in fact the ornstein-olbeck process
is the only stationary diffusion process that is a Gaussian process so the transition probabilities
of non-linear diffusion processes are never multivariate Gaussians not sure what that means but
sounds kind of interesting all right so then uh from this sort of mapping between internal and
external states through time the synchronization mapping sigma they write in the section on
predictive processing we can go further and associate to each internal state mu a probability
distribution over external states such that each internal state encodes beliefs about external states
so now rather than just framing these as two dynamical processes that track one another
like oh there's this synchronization map between this random variable internally and this random
variable externally they're going to frame this q distribution so q is going to be a probability
distribution inside of mu that's over external states such that each of those states encode
beliefs about external states so q of internal states is about eta and that's going to be
a normal distribution with a mean of the actual external state it's like mean and variance
um so it's eta and then sigma of mu the mapping function comma variant so here these two are both
what it's being uh is being inferred is the the external state and the mapping and then there's
the variance of the precision on external states and then um 3.4 is just giving a little bit more
details about how that q is like this internal function where the states of it are reflecting
beliefs about external states um and then there's a relationship to how the internal states are
conditioned on blanket states this was a um interesting remark that'd be cool to have people
discuss would be uh when they wrote note a potential connection with epistemic accounts of quantum
mechanics namely a world governed by classical mechanics sigma um conditioned at zero or i don't
know if there's another way to read that symbol with three lines in 3.2 um not sure because in 3.2
i'm not sure if there was a sigma in which each agent encodes gaussian beliefs about external
states the could appear to the agents as reproducing many features of quantum mechanics so this was
just cool like what is the relationship between the sigma internal external mapping function
and then classical versus quantum mechanics so maybe classical systems are like the ones where
there's a clean separation between the internally external states quantum states being ones that
are more uh with residual influence between internal and external states
so returning to the um predictive processing they write that under that 3.4 so under this
implication of the q internal distribution expected internal states are the unique minimizer of a
colbach libeler divergence so that mu through time internal states through time are the minimization
the argument over the mu states internal states of the d k l the k l divergence of
the internal belief distribution q about external states q of eta double line
divergence between the objective distribution of external states conditioned on blanket states
so if that q distribution can converge to the p then that is being minimized effectively
this measures the discrepancy between beliefs about the external world q sub mu of eta and the
posterior distribution over external variables computing the k l divergence they're going to
obtain this 3.5 and um that's argument so it's minimizing the differential between sigma's prediction
so that's sigma of mu minus the actual external states through time sigma mapping function of
internal states minus the actual state multiplied by so pi of the precision matrix of eta relating
to how wrong that differential is so that's kind of an interesting framing um and then they write
that the right hand side of 3.5 that equation is commonly known as the squared precision weighted
prediction error which is the discrepancy between the prediction and the expected state of the
environment and it's weighted with a precision matrix that derives from the steady state density
this equation is formally similar to that found in predictive coding formulations of biological
function which stipulate that organisms minimize prediction errors and in doing so optimize their
beliefs to match the distribution of external states so that's pretty cool and we can talk
more about it but it kind of reminds me of this uncertainty reduction imperative rather than a
reward maximization imperative it's kind of like saying if the organisms are minimizing
their prediction errors not maximizing the reward they will optimize their beliefs to
match the distribution of external states so maybe that's related to that variance reduction
imperative so we had the predictive processing section and then there's one more layer that
gets added on so from the Markov blanket to making it a Bayesian Markov blanket happening through time
to introducing this Q distribution that's a special type of internal state
they write in the next section we can go further and associate expected internal states to the
solution to the classical variational inference problem from statistical machine learning
and theoretical neurobiology so it's going to be that Q function is going to be a very specific
kind of Q function so not all systems are going to be doing this subtype of variational inference
or maybe they are um that is something we could discuss but we can finesse that mapping function
between the internal and external states and what exactly the internal states are because if you can
co-design both the internal states and the mapping then you have a lot of degrees of freedom in
mapping on to external states even as they change so now we're going to do a really
subspecial phrasing maybe it's a more general one but just one specific phrasing of how the
internal states could map onto the external states and that's going to be as expected internal states
as the unique minimizers of a free energy functional i.e. an evidence bound so now there's a
f function over blanket and internal states through time and that f function is going to
consist of two elements which is like a divergence term that we saw before with the divergence between
Q's internal estimate of external states divergence and the actual external states conditional
blanket states minus the uh the evidence term and then that can be equivalent to this energy
minus entropy term so that last line expresses the free energy as a difference between energy
and entropy pretty cool energy or accuracy measures to what extent predicted external
states are close to the true external states while entropy penalizes beliefs that are overly precise
so that's cool like energy is locking in on the good solution that's like maybe the component
