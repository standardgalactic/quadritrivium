Hello and welcome everyone. This is the Active Inference Lab. It is Active Inference Lab
live stream number 26.0 on August 6, 2021. We're going to be discussing the paper,
Bayesian Mechanics for Stationary Processes. Welcome to Active Inference Lab, everyone.
We are a participatory online lab that is communicating, learning, and practicing applied
Active Inference. You can find us at the links on this slide. This is a recorded in an archived
live stream, so please provide us with feedback so that we can improve on our work.
All backgrounds and perspectives are welcome here as well. Today, as you can find out at this
short link, we're going to be having the context video for the discussions on August 10th and
17th in number 26.1 and 26.2, which are going to be group discussions with some of the authors
joining. So if you want to join for any of these talks or discussions, then please just let us know.
And also, we have a few weeks where we don't have a paper decided. So if you have a suggestion
and want to be in those discussions, let us know. All right. Today in Active Stream number 26.0,
the goal is going to be to learn and discuss this paper, Bayesian Mechanics for Stationary Processes
by Dacosta, Friston, Heinz, and Pavliotis, which was posted in June 2021. This video,
just like all the dot zeros, is just an introduction to some of the ideas and a walk
through the paper one way. It's not a review or a final word. We're going to give some overview
to the paper, then address a few of the keywords from a big picture perspective,
then go through the figures in the formalism.
I'm Daniel, and I'm a postdoc researcher in Davis, California.
So in this paper, the aims and the claims are laid out as followed. They wrote,
in this paper, we considered the consequences of a boundary mediating interactions between
states internal and external to a system. So it's about interfaces and interactions.
That's the first point. On unpacking this notion, we found that the states internal to a Markov
blanket look as if they perform variational Bayesian inference, optimizing beliefs about their
external counterparts. So when the boundaries set up in a certain way for which kinds of systems,
that's what we'll be asking. How do those systems look from the outside? Or what does it look as
if they're doing? And then three, when subdividing the blanket into sensory and active states,
we found that autonomous states perform active inference in various forms of stochastic control,
i.e. generalizations of PID control. So a lot of these terms like variational Bayesian,
Markov blanket, PID control, we're going to be talking about more. But at the overview level,
that's the aims and claims as the authors write them. The abstract states that the paper develops
a Bayesian mechanics for adaptive systems. And then there's only these three claims again,
which are again related to the blanket and adaptiveness of
systems, and then this partitioning of that interface into sense and action. So first,
we modeled the interface between a system and its environment with a Markov blanket.
This affords conditions under which states internal to the blanket encode information
about external states. Second, we introduce dynamics and represent adaptive systems as
Markov blankets at steady state. This allows us to identify a wide class of systems whose
internal states appear to infer external states consistent with variational inference
in Bayesian statistics and theoretical neuroscience. Finally, oops, we partitioned the blanket into
sensory and active states. It follows that active states can be seen as performing active inference
and well known forms of stochastic control, such as PID control, which are prominent formulations
of adaptive behavior in theoretical biology and engineering. So this is going to be
relating to math and physics with the Markov blankets and the far from equilibrium
thermodynamics component and information theory. And then it's also going to be developed into
some domain specific cases or shown to be a generalization of domain specific cases,
like theoretical biology and engineering. These are the sections of the paper. So first,
there's an introduction which has some of the general concepts. Then there's a section on
Markov blankets, which is going to be a key framework and a key formalism to track because
it's kind of what the paper is about. So first technical sections about the Markov blankets,
followed by a discussion of how that relates to Bayesian systems or certain types of Bayesian
systems. And then this general Bayesian framework is then sort of specified towards an active
inference model, specifically with an eye towards stochastic control. And then there's a discussion
and conclusion. And then there's three supplemental sections related to the synchronization map,
which is about the synchronization of internal and external states and how that's related to some
math results. There's the Helmholtz decomposition, which is related to vector fields and splitting
them into different components. And then there's the free energy computation. And some more details
related to what exactly that formalism is used as here. The keywords of the paper were variational
Bayesian inference, non-equilibrium steady state, Markov blanket, free energy principle,
predictive processing, and active inference. So first non-equilibrium steady state.
Here's a figure from this paper, stable, unstable and metastable states of equilibrium
definitions and applications to human movement. So there's going to be some mathematical definitions
and uses of non-equilibrium steady state. But this is a domain specific usage that gets at some
of the nuances. There's not just one single way to frame equilibrium, especially for dynamical
systems. And this is shown as with an example of a person in movement. So there might be a total
thermodynamic equilibrium, like death. But even within the non-death regime, there are still many
types of equilibrium. There's lying down, there's sitting in a chair, there's a walking steady
state, there's this metastable state, and then there's unstable states of equilibrium. So a few
just questions would be like, what does it mean to have equilibrium of a measurement or a system at
a given level? And what's the difference between that static, just something on the ground versus
a dynamic equilibrium, like something that maintains its position and uses energy to do that?
So in those cases, where does the energy come from? How does this relate to dissipative systems?
So something where there's energy to be expanded like a battery or a tornado. And how does that
relate to adaptive systems and predictive systems? Which ones are related to active inference? One
or both? And what about when there are multi-level systems of equilibria? So some that are higher
frequency or shorter time scale or smaller spatial scale, and others that are broader? And also,
what if we want to have action oriented models of those systems? So we want to have the description
that's optimal for some other set of requirements other than just accurate measurement. And that's
where this paper is going to be kind of headed is thinking about dynamical systems in general
and looking at equations that relate to solving them. Another keyword was variational Bayesian
inference. And there's a lot of good literature on this topic from a lot of different perspectives,
but this article had a good way of phrasing it, which is that variational inference methods
consist in finding the best approximation of a distribution among a parameterized family.
And in contrary to sampling approaches, a model is assumed, which is the parameterized family,
implying a bias, but also a lower variance. In general, variational inference methods are
less accurate than sampling based like MCMC ones, but produce results much faster. These methods
are better adapted to big scale, very statistical problems. So again, to just kind of give a
qualitative example, it's like the real world is giving you this data, which is the blurry
image of a cat. One way to get at what was the generative underlying cat that generated this
blurry image would be to sample and find the most likely cat generating this image or do various
other sampling schemes. And if it's set up properly, it can fit arbitrary distributions.
However, it can be computationally expensive and really hard to know if you're solving the problem
at a functional enough rate. In contrast, the variational inference approach is related to
fitting the data based upon a pre specified family of distributions, which can still be really
general. And it can be fast, but it can sometimes lead to fitting the optimal support for a given
distribution. But having it just be categorically wrong on some other dimension, like estimated
through a deep time or something like that. So variational Bayesian inference is this lower
cat, where there's a known cat, and then you stretch and distort the cat, for example,
reparameterize the cat to find an underlying likely cat. Another
keyword was Markov blanket. And there was good discussion of this with Mel Andrews in act
M stream 14. And we kind of had this continuum from Markov on the left here, which was studying
systems from a mathematical perspective, didn't have a computer. So I was looking at matrices,
and which elements of matrices were connected to each other or not. And then that was developed
later by Pearl and others in this broader Bayesian statistical framework. That's where the Markov
blanket terminology starts to come from. And it's also oriented towards empirical data and
discovering dependencies in data. And it's very computational. And then part of the literature
that we're discussing is where Friston and others have built on or use this Pearl blanket slash
Markov blanket theme, and done a few things like separating the blanket states into sensory and
action, incoming and outgoing nodes, introducing the cybernetic imperative to be a goal seeking
or a multi scale good regulator to maintain that non equilibrium steady state, which we'll talk about
and we did. And then also, what's being explored from a formalism perspective in the paper we're
discussing is how is it or under what conditions do internal states act as if they're a model of
their non local dependencies. So causal dependencies in the world would be reflected by how the brain
or some other computer system works. So that entails this generative model of the action
in the niche, and also brings in the element of including the blanket in internal external states
as part of this broader inactive embodied in culture slash pragmatic term.
Also, in active stream 14, we had this sort of progression emerging from the structuring of the
paper, where there's a sort of vector field meets diffusion thermodynamics branch on the bottom with
Focker plank and Helmholtz. And that's related to non equilibrium steady state densities.
And then the free energy principle, subsuming all these sort of pieces like the Markov blanket
being maintained through time necessitates the generative model of action is what the internal
states have to be inferring. So the useful components of the external world. And then
under free energy principle, there's all these sort of related areas like active inference is
that implementation of systems under the free energy principle, how is this related to predictive
processing, Bayesian brain. So we talked more about that. But today, we're going to look at how
it's used more by the authors in this paper. So the first thing is that the code is available on
Connors GitHub. And we'll also be talking with Connor, hopefully. So let's maybe work through it
or if anyone has the chance to run it, tell us about how it went or what they learned from it,
but I didn't get to run it. So we can start with figure one, which is laying out the big
graphical intuition of the whole paper. And everything is going to be specifying
constraints or some details or implications of a way of interpreting this intervention of the
blanket states between internal and external states. So we have mu, the internal states,
be the blanket states, and eta, which are looks like a curly n. And this is going to allow us to
separate any set of nodes, mu that we're specifying, and ask, what are the blanket states that insulate
from a, some statistical codependency sense, which will be exploring insulates these two
partitions. And the captain says the Markov blanket is depicted graphically as an undirected
graphical model also known as a Markov random field. The circles represent random variables.
The lines represent conditional dependencies between random variables. The Markov blanket
condition means that there is no line between mu, internal states, and eta, the external states.
This means that mu and eta are conditionally independent given b. So conditioned on the blanket,
these internal external states are separated. In other words, knowing the internal state mu
does not afford additional information about the external state eta when the blanket state b is
known. Thus, blanket states act as an informational boundary between internal and external states.
So that's what the blanket states do. And this paper is going to explore some of the consequences
of that. And again, anyone who has more expertise in this area or some piece of knowledge that I
totally missed or got wrong, my apologies, but just want to try to represent it as I saw it,
because a lot of the details I was just wanting to learn more about and go over with the authors.
So if anyone has comments in the chat, they can also just like mention it. Okay, so let's jump
into how they present some of the key pieces of the formalism, because that's the details of free
energy principle slash some of the grounding of active inference. So even if this is new
formalism for you, then also hopefully it's also interesting. So keeping that picture in mind
to figure one, another way of writing out that graphical model with a formalism is this eta and
then upside down t. So that's the external states and then mu conditioned on b. So that is blanket
states are Markov blanket separating internal external states. So the top third of the slide,
these are like writing out in prose, the same thing as this graphical model.
And then we can take that tuple that set of those three kinds of nodes. So we have measurements
from all these different things or generative models of each of these different categories of
nodes. And then that set. So all the measurements slash generative processes that x is existing
in a bigger state space. So the little x, which consists of like the real measurements
are part of the bigger x, which is kind of like the capital versions of each of these.
And those spaces, the the e, b and I are taken to be Euclidean spaces for simplicity. So I don't
know if it holds for other kinds of spaces. So that's just defining the total space, the possible
states of each of these nodes, and then also the ones that you actually observe.
They start with this case of a Gaussian distribution P encoding the distribution of states of the
world. So P is a random variable that has drawn from n. That's like a normal distribution symbol
with a mean and a variance. And the mean is zero. And the variance is this
pi to the negative one. And this has an associated precision covariance matrix. So mean zero,
that's the convention and means the normal distribution. And so it's zero mean, that's
the variance pi. And the precision matrix is pi. So then the variance is this pi inverse.
And sometimes the equations are formulated in terms of precision. Other times they're
discussed in terms of variance. But those are kind of like two sides of the same coin.
Because the same, it's just like describing it, whether it's how uncertain you are is like
looking from the mean out, and then your precision might be kind of like is like focusing down.
Uh, unpacking in terms of Gaussian densities. So we find that a Markov blanket is equivalent
to sparsity in the precision matrix. So the Markov blanket condition is entailing that the
covariance, the pi between external states and internal states, that's the first
one here is equivalent to between external states and internal states, the mu and the etsa, and
that both of those equal zero. So there's no covariance. And they're the same as zero.
So they're conditionally independent. That's the Bayesian covariance matrix framing of the Markov
blanket sparsity. Blanket states act as an information boundary between external and internal
states. And this allows the breakdown of instead of the all by all consideration of correlations
for data fitting or generative modeling, you can reduce the model space to a much better
uh formulated subset of equations, which are related to inferring distributions relating to
like external states conditioning on the blanket states and internal states conditioning on the
blanket states. This enables us to associate to any blanket state its corresponding expected
external and expected internal states. So then this equation said drew us a set of equivalencies
which were that the internal states as a function of blanket states so mu of b
were equal to the expectation fancy e of mu conditioned on b. So that's the expectation
of external states conditioned up on of internal states.
Internal states conditioned on blanket. I think that's corrected is right. Then three,
that that expectation is the same as the distribution that's being calculated involving
the objective distribution p and that also that that's related to
the variance in covariance like this sigma variable of internal and blanket states
mu and b mapping to the space of all the possible internal states which was i from here.
And then they wrote pursuing the example of the nervous system, each sensory impression on the
retina and ocular motor orientation blanket state is associated with an expected scene that
caused sensory input, which is the expected external state and does an expected pattern
and an expected pattern of neural activity and the visual cortex expected internal state.
So it's a way that that objective distribution of p gets integrated into the internal models
output given blanket states.
