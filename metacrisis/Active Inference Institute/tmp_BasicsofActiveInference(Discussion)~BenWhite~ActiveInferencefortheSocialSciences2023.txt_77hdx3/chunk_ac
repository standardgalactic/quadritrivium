shamelessly plug a preprint that Mark and I have released recently where
we one thing. So one thing that I'm really interested in that I can kind of speak with
some confidence on is the fact that I do think there's a point at which the theory of affordances
or the language of affordances starts to break down.
So the preprint that I'll share around is looking at ambient technology specifically. So this is
technology that you the whole kind of impetus behind its design and conceptualization is that
the user doesn't have to actually do anything. So it proceeds the user pragmatically and
epistemically. It knows what kinds of things you want it to do and it just does them in the
background and it works by shifting kind of it suddenly shifts the material environment such
that it impacts your field of affordances in real time. And the argument that we make in that paper
is essentially that the affordances approach doesn't really work for this and that we need to kind
of think again. So yeah I mean I think that that might be a nice but like how to conceptualize
generative AI for example under the affordances framework. I'm not I don't know anybody that's
done any work on that but it would be it would certainly make a really cool project.
You are making it right now.
You are making it right now I know because you set me the paper to review.
Oh yeah. Thank you. Thank you. We were looking at we so we are
early stages of a paper on the role of generative AI in classrooms. So from the perspective of
thinking about what kinds of affordances generative AI represent in a learning environment
particularly with we're kind of applying active inference to classroom design and looking at it
from the different angles of different educational and pedagogical theories.
But it's very early to date with that stuff. Yeah this is I suppose it probably depends on
your view of generative AI in general I mean it depends whether you would consider it a real
cognitive agent or not. I think I think some people are more inclined to think of it as you
know this thing this is a thing that has agency it has real understanding it has like
you know and other people are maybe less inclined to think of it in those terms and I think that
might be that might bear significantly on on how you think of it. Yeah that's super cool that
basically the last project you mentioned is very very in line with my PhD project right now because
my main PhD project is our European Farmings and it's about the AI for personalized education
and I'm trying to follow it through the the active inference framework from a multi-level
perspective. So yeah very cool yeah we should we should probably talk more about that.
Okay Darius.
Yeah I wanted to ask I mean this may be directed more at Mark because I know this is kind of his
work in terms of slopes of uncertainty and about doing better than expected at reducing prediction
error over time. I was wondering kind of how the architecture of that is built into the into the
particle into the into the generative model because we have the kind of for me there's I don't know
if it's an out and out sort of conflict but we have these kind of implicit priors that we're going to
fulfill certain expectations or minimize prediction error regarding certain things right so
homeostatic priors or happiness well-being whatever it is but then your claim is that we
have the higher order beliefs the higher order predictions that over and above that I also need
to become I also need to do well better than expected at reducing prediction error over time.
So is that I it's still quite fuzzy in my mind but is there a kind of potential conflict there
between the general priors that the system has and then going actually in a sense violating
those expectations by going over and above them which itself constitutes an expectation how do
you kind of resolve that tension? Yeah that's interesting but this is very much in Ben's
wheelhouse too. Ben and I have been working on slopey stuff for almost as long as I've been
on
this original paper called we'll get it we'll get it up they do a really good job of showing
technically where this sits within a deep parametric model. Lara Sanved Smith's paper on
metacognition also does this by showing you have these you have these depths of modeling
where models above are modeling models below okay and optimizing over those models below.
So we have some good computational backbone for thinking for not only thinking that this is the
case but beginning to express how it does the work it does so let's let's leave that for digging
into it though technically on sort of on our own let me say sort of at a higher more abstract level
still hopefully it's useful. It's not weird to think that this kind of anticipatory system is not
only making predictions about the world but it's also part of what it's predicting is how fast or
slow how efficient it is in particular contexts at resolving certain kinds of errors that's a
perfectly fine thing to think that we're also predicting slopes of engagement and then and then
all we're saying then is system also pays attention to when those expectations are breached and it's
learning from those breaches that's that should just be the bread and butter for what the system
does anyway so precision is a second order is a second order process in much the same way
precision is about how well how how reliable are lower level predictions and then using that
that amount to toggle how impactful either errors or predictions are so we've already
got baked into the system right from early days this idea that the system is not only making
predictions but monitoring its own predictive processing regimes and then toggling based on
how reliable those those substreams are all we're adding here is you know when we first when we
first thought about those mechanisms one thing that can happen this is just good for everybody
who's interested in active inference because I bump into this with my students all the time
when we say something like precision waiting a tendency to think of this thing
so we go to define this precision way like where's the biological instead or like this precision
waiting when we actually get into a bio system and we look for these things the truth is precision
is going to be weighted in lots of different ways I mean that's the real frontier of this
research is to actually find how these things are instantiated and the answer is going to be
multifarious I mean it's going to be you're going to have precision adjustments happening throughout
the system in lots of ways it could be synchrony and asynchronous and desynchronies between systems
it could be neuromodulatory chemicals it could be structural structural shapes within the brain
I mean precision is going to be set in in lots of different ways so
all all that we're pointing out here is is that one of the ways that the precision is being set
one of the ways that the system is tracking how efficient it is and then upping or lowering
the amount of impact error signals or predictions have is that it's happening in an embodied way
so we've looked over to the affective search and and lo and behold there are all these signatures
that we were looking for for this kind of for this part of the machinery so yeah so not weird
that the system is tracking its own regularities and adjusting that was baked in right from the
beginning how it does that that's one of the frontiers it's going to happen in lots of different
ways lo and behold affective dynamics the shoe fits they look like they do that stuff and then
you bring it to the lab and you go back to like reward prediction error research and sure enough
neuromodulatory chemicals reward systems are tuning affective dynamics relative to better
than and worse than slopes of uncertainty management it's exactly what we would expect
given the computational model so then it's an easy it was an easy next step to start saying well
look there's one of the way is that precision there's one of the ways that affective system
and i just noticed there if the only decision is that and not the only thing the affective
system is doing we never want to say we never sort of we're careful not to be reductionist
to say oh affect is always error dynamics i don't think that's right i think is that error
dynamics are expressed in part affectively and they have this impact on the system that we can
that we've known about for a long time even from just the reward prediction error
literature does that help or was that a bit is that okay yeah uh yeah just to just to add to
two things really quickly there because i know time is kind of catching up with us i would
emphasize as well like just as a kind of general point the one of the things i really like about
the work on aerodynamics and affect is it has this nice kind of broad capture of the kinds of
affectivity that agents can experience so we're not just when we talk about affectivity we're not
just talking about full-blooded emotions or even just moods but um we're talking about what one of
the things i kind of drew attention to in the lecture was matthew wrackliffe's work on existential
feelings which i think is just such a um that the connection between active inference and phenomenology
that you find in some of this work is just insanely powerful and it's one of the things
that really drew me into the framework um the second thing just building
one thing that i don't think i mentioned in lecture is active inference as it has been
described as a quintessentially metacognitive framework so you have kind of built into the
architecture you have expectations over expectations so you have kind of predictions
about predictions and this has proved to be um just you know again just phenomenally useful for
thinking about certain aspects of phenomenology as well so um yeah i think that these are these
are like real strengths of the framework okay um i don't know how we're doing for time are we are we
strictly limited on or can we we are not as far as i know Daniela
well um i can probably do i definitely do another 15 um if anybody has any more questions
and people i should say as well we're not that doesn't mean that we're all held captive
to my time frame so if people if people do need to leave within the next five or ten minutes that's
absolutely fine but um i'm certainly happy to to carry on if anybody has any more questions
or comments
um if no one else has got any i would quite like to go back to
earlier in the discussion you were talking about the value of uncertainty
um because uh in organizations in which i'm working with we often have conversations about
uncertainty and what we find is over time there tends to be a drift towards risk aversion
so we're working with a large uh international construction company at the minute and that they
used to have a culture in which innovation um uh was uh was quite well embedded and and over time
is kind of drifted towards very very risk averse culture where actually rather than
learning through the process and being able to tolerate uncertainty you know around opportunities
and you know what can be accomplished and that kind of the valence that comes with
the intrinsic reward i suppose of being able to reduce uncertainty about capabilities to
a situation where they're trying to anticipate all of the risks up front before they even get
involved in the project so you know there's there's some kind of valence switch market and you
mentioned um david blaine earlier you know and something about the context you know there was
a switch in valence from yeah this is this is a kind of a play this is a safe thing to do to
actually know this is you know this has real consequences so i'm just kind of i'd like to
explore that a little bit more actually if anyone's got kind of any insights or papers or comments
yeah so i think that's a really interesting question actually and i i think um so it seems like
one of the things one of the things we've been talking about is this connection between uncertainty
prediction error minimization and affectivity in individual agents and it sounds like what you're
asking is like how does that translate to uh like collectives so one of the things about active
inference that we're going to see in subsequent weeks is how it scales up to kind of larger systems
like companies or you know groups that are trying to achieve some kind of shared goal
um how do you how does the value of uncertainty translate onto like how is it scalable in that
sense right is that would that be a fair kind of yeah i i guess so um i i guess it's um i mean
the uncertainty often when it's talked about uncertainty is talked about as risk which is
you know i suppose is you know predictions or anticipation of outcomes that we don't want
right whereas there there's also positive uncertainty right you know which is you know i
suppose you know simply stated opportunities you know through curiosity you know what might we
be able to achieve you know this kind of novelty seeking i guess that you know new opportunities
that we haven't exploited um and they're just you know in the kind of organizational systems
you know i i think there are various pressures that cause it but but over time you see these
cultural shifts towards uh you know a very very kind of risk averse you know where you know where
the valence is obviously quite negative you know you know quite a non-pleasant feeling for people
and yeah yeah this is yeah this is good this this is right at the heart of my current work um i'm
really interested so you know we just had a big stint where active inference models were starting
to be used in computational psychiatry especially for pathological disorders so addiction depression
disassociative disorders OCD PTSD um it's a really it's quite a it's a quite um a sexy framework
for thinking about some of the ways that the cognitive system breaks down and the sort of
new move right now i mean we have a collection coming out right now in neuroscience of consciousness
is to think about these things in terms of okay if we are predictive systems and we have a
sufficiently rich model of how that system works in a particular niche what sorts of what sorts of
what sorts of ways can we intervene on that system in order to have positive outcomes
rather than just modeling what the negative outcomes are and i hear that a lot in what
you're saying now so i'm just going to drop one link for you there this is kasper hasp again
wrote a little a very small paper with a nice little model called sophisticated affective
inference he has a little bot that tends towards catastrophe catastrophizing the future if it's
given if it's given like fun medium fun medium fun dangerous over time it tends to it tends to
expect the dangerous one um uh like after 10 000 iterations it basically lives in the worst possible
scenario which is a nice little song this is my wife i'm going to give this is this is most people
today you know so the question is um i want to know given the model why does that happen
and what can we be doing to intervene and part of the answer is going to be
we need to become more tolerant of uncertainty that's one of the things that the system can
be better or worse at so this just takes the computational modeling and then looks to things
that we already know about emotional regulation that that's that's part of the story um so um
we've done a little bit of work on this with our predictive dynamics of happiness and well-being
and ryan smith is definitely doing work on this with active inference and well-being um so
definitely check that out if you're if you're interested here um but i just flag one interesting
thing here that relates just to what we were just talking about about layers of modeling
one way that let's say two ways there's two ways that a system and it's probably gonna be lots but
the two that come to mind that a system can become more tolerant uncertainty is one exposure
so this is why exposure therapy might be useful for our kind of a system
you want to expose the system to volatility at the lower and middle levels of the hierarchy
and have it turn out okay so that this one of the things that the system can come to predict
is not only you know particular outcomes but it can also predict how much error is involved
in particular outcomes so for instance um when i used to give um a pro talk i was always really
nervous um and i don't know if that ever really went away but i've had so much exposure to the
anxiety of giving a professional talk that basically i don't notice it anymore but if you
were to ask me at the beginning of my talk mark right now what's your phenomenology
and i sort of meditatively looked i'd probably say yeah i'm nervous but if you hadn't have said
that and you were just like hey what's up i've been like oh yeah i'm great like it's all good i
don't even notice it anymore that's because the system knows that i have errors
but as soon as i start talking they drop away and because i know the arc that even error in the
system it becomes non-newsworthy it's no longer interesting volatility to be tracking that's
just from exposure okay so what's happening is you're having errors at a lower or middle level
that a higher level is now modeling saying when we come here we should expect this arc of error
same thing when you work out like real gem rats they can feel good
well you're having your jet your tiller skeletal system right but at the higher level it's now
learned not only that that has a natural arc but that that's a good sign at a higher level
so now you're getting a positive you're getting positive prediction error slope high
and negative prediction error slope low okay so one yes exposure to you can model your own
responses this is something ben and i work on with horror movies you can you can you can
you can take an active role in mindfully observing your own reactions to volatility
and um this comes up in our paper on horror we've already dropped the link today if you
