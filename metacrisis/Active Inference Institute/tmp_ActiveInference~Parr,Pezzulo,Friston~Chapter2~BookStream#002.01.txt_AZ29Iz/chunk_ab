Dalton's Activate of L. Maxwell, Rammstein and others have shown that
those two concepts of optimality are actually congruent with each other.
So that's one of the reasons that the duality between FEP,
free energy principle and constrained maximum entropy principle can be...
I mean, it's one of the justifications for providing that dual formalism between those two.
So, but another point I wanted to mention here is because I've seen that using the word hidden state
can be a bit confusing for some people because when we observe something as an observation,
obviously it is not quote-unquote hidden, right?
So what a hidden state here refers to is actually the hidden cause of that observation.
So it's not that the observation itself is hidden from the observational or it's unobserved.
So that might be a bit confusing if we don't take into consideration
the exact meaning of the hidden state or latent state here and in the rest of the literature.
So continuing from section 2.6, here we see one of the two central equations of active inference,
which is equation 2.5 for variational free energy.
So it's, as I said, I mean understanding this equation and how each line of its formulation
represents in terms of the trade-off between energy entropy or complexity in accuracy
or divergence and evidence is key to understanding almost everything in the rest of the book
and in many other literature on active inference.
So this is the perceptual part of active inference.
So variational free energy is a parameter that is a notion that parameterizes
the surprise of our perceptual information we have about the external states.
And then we'll see in the next section the related and almost symmetrical formulation
to variational free energy, namely expected free energy, which is basically the action part of the active inference.
So we can see how those two can somehow be seen as a kind of unified formalism
but described in the alternate expressions.
So, but another thing about equation 2.5 is it may be a bit, I don't know, daunting to see
all the relations between those three lines of the equation and how we can get from one to the other.
So there are some supplementary materials that we have developed in the past weeks
which I think can help in clarifying how the derivations of these three lines of equation be done so.
I hope they would be clarifying in, I mean, and help to understand how those three lines relate to each other.
So, but the key point here is to understand that variational free energy is not something absolute
but it's just an upper bound for the minimization.
So as Daniel just mentioned, it's untractable to have an absolute amount for the surprise to be minimized.
So we need to have an upper bound in order to make that more tractable
because by Jane's inequality, as we saw in chapter 4, I think,
we can see that how the upper bound of a surprize necessarily provides a condition for minimizing
the precise or the exact free energy.
And that's the key insight of equation 2.5 or the notion of variational free energy,
which is to provide this upper bound instead of the exact amount of surprize or to be minimized.
Yeah, what I'll add there is if you knew exactly how well you should be surprised by a given data point Y,
then you would have had the optimal model.
However, that is not tractable.
And so by making a quantity that's always higher or an upper bound, the variational free energy F,
which is a function of broadly Q, our beliefs or variational beliefs,
which are built in a way that makes them very compositional, very optimizable, very interpretable and data.
And we can reduce the divergence here, the KL divergence with a double line between Q, our beliefs and P,
the kind of actuality of it.
And if we can reduce this divergence, in other words, minimize the free energy,
then we will come closer and closer to the true surprise function
and do that in a tractable, incrementally optimizable way.
So equation 2.5 is going to be the variational free energy,
different ways that it can be represented as it takes in data and beliefs about the world Q.
And then this is going to be expanded into the future to include action with G, the expected free energy.
Now, there's a lot more that we can say about this.
There's a lot of technicalities to go into.
But broadly, notice that G, the expected free energy, is a functional of policy, Pi,
because it's only being evaluated to select amongst different action outcomes, Pi.
And another important difference is that it's going to be describing sensory outcomes that haven't yet happened.
A sort of, what would I perceive if I did A or what if I did B?
And it's that kind of comparison that allows the expected free energy functional here
to be used in action selection or policy selection as inference, planning as inference.
That section is expanded upon.
And in figure 2.6, we see a very nice representation of the expected free energy equation
and then how when certain aspects of this equation or situation are zeroed out,
we get certain other familiar cases.
For example, where there's no epistemic value, there's no information to learn,
then you get ruthless expected utility theory.
And conversely, where there is no pragmatic value to extract,
so all outcomes are equally valid or preferable,
then you get things like infomax principle and optimal Bayesian design.
And then everything in between is the space that we're interested in.
And so this figure 2.6 shows that the expected free energy functional
can be seen as like a generalization of a lot of other settings
related to perception and action and planning amidst uncertainty.
And section 2.9 closes the low road.
They took us all the way to active inference from Bayes theorem
through the generative model on to active inference
and clarifies these two notions of variational free energy.
That's the real-time perceptual unfolding, evidence lower bound on suprisal,
tractable, optimizable, and so on.
And F and G, the expected free energy,
which is able to do planning as inference or policy selection as inference.
Expected free energy is fundamentally prospective
and that enables counterfactual cognition.
Section 2.10 summarizes active inference is the theory of how living artifacts
underwrite their existence by minimizing surprise or tractable proxy
to surprise variational free energy via perception and action.
And they motivate that from a first principles Bayes theorem starting place.
Any closing thoughts on chapter 2, Ali?
I would just humbly suggest for the people who want to go through this chapter
to try their best to really understand specifically
what equation 2.5 and 2.6 represents and how...
I mean, how to use those equations to describe different situations
with some missing elements as well.
Because I believe those sections and particularly those equations
are absolutely essential for understanding everything active inference related
both in the rest of this book and in almost all.
Thanks. And just the last thought I'll give on chapter 2
is this is exactly the work that we do in the active inference textbook group.
We really welcome all backgrounds.
Every single question and uncertainty you have is beautiful.
We have a lot of resources that Ali and others make
to make the math approachable and rigorous
and natural language descriptions of the equations and so on.
So yes, it's really important to understand the equations
because after all, that's like the skeleton that gives meaning
to our usage and fluency of the active inference ontology
which Ali and I are speaking right now.
We're not just saying surprise is related to this because we felt it that way.
There is an underpinning and it is a really interesting life's work to explore it
but we're finding ways to communicate it and learn and teach it better and better every time.
So that's chapter 2. That's the low road.
