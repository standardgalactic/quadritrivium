it externalize harm to do the yellow teaming up front and then see where there are harm
externalities how you can change the design to internalize those and make sure that and this
comes that question that you said Jeff Goldblum asked should we do it there is there's a way to go
about inquiring into the should we do it and for some things you'll say no for other things you'll
say well we could change the design in these ways right so obviously if social media had not had an
advertisement model for its revenue model that monetized people's attention where it made more
money as they spent more time on site and as they clicked on engaged in more things
they would have that same underlying technology of curating all the world's information harvesting
that about people and curating the world's information to them if say you had paid for the
service and you could set the settings of how you wanted your mind to be affected you could say
actually I want to be exposed to ideas that are very different than the ideas I currently have
I want not things that appeal to limbic hijacks I want things that have longer
deeper analyses rather than short attention span types of things and so you could develop those
technologies in very different ways if you thought through what the externalities might be and then
said how do we do the design differently there's all this conversation about you know should we
censor hate speech on social media and it's mostly a misnomer because it's not that someone says some
nasty thing that's a problem it's that that thing gets amplified to millions of people in a way that
in a town square would have never happened what is the algorithm that determines if five people
see your thing or 500 million people see see your thing the algorithm actually selects for the things
that cause the most division right because if you get people fighting in a thread the activity of
that thread up regulates the thing so your that algorithm is going to polarize the population
it's going to create more enmity now could you design the algorithm to say we're not going to
maximize for engagement in which case fighting can be engagement we're because we're not talking
about freedom of speech now we're talking about an unprecedented thing in the history of the world
which is the ability to have someone's idea put in front of the whole world and we're differentially
we're the ones the algorithm is getting to choose how much attention this thing gets it's not like
there's some natural selection where if people really like it it's that the algorithm is picking
certain things that it's optimizing how many people's newsfeed it puts it in and what position of
primacy so it's like that has nothing to do with freedom of speech that's a totally different weird
thing and so if you say well we can amplify anybody's ideas what is the basis of the should
the ought associated with the is of which ideas are really worth amplifying right now it's mostly
the worst ones the ones that are intrinsically most divisive and limbic hijacking and attention
capturing and so what if you said okay we're going to look at ideas that or content that gets
generally positive reaction across political lines and across ideological divides meaning
you're finding things that are more shared views than divisive views and that those people can still
post a divisive ones but the ones the algorithm is going to upregulate or those ones you'd have a
very different world you'd have a world in which people started to recognize more and more things
that they share in common that they could then build on and continue to increase rather than like
the weirdest strawman versions of each other that isn't you know even true and you'd have an
incentive for content creators to write things that were actually going to appeal across ideological
divides rather than appeal to one while polarizing the other which actually drives the arms race of
both sides so if you do that kind of anticipate the harms that might occur you might just not do
the thing but you might also do the thing very differently right you might also change the
design in very fundamental ways so if we optimize for very narrow goals don't pay attention to all
the other effects but continue to have exponentially more powerful technologies that scale to the world
faster and have effects in the world much faster the cumulative externalities of that plus the
cumulative rivalry that drives will be extinctionary and this is this kind of comes like from a
mythopoetic perspective it's something like the total amount of technological power that
humanity stewards is not like any other animal right even stone tools were not like any other animal
but the ability to make new species to genetic modification
is really really different the ability to split the nucleus of atoms and make you know
nuclear explosions and nuclear winter the ability to like clear cut whole ecosystems and make cities
the size of tokyo and um and artificial intelligence that can beat all humans combined at defined types
of tasks this is much more like the power of gods of antiquity but without the wisdom of it
and that self-destructs so we have to be we have developed the kind of mind that analyzes parts well
that analyzes a very tiny domain of is called the objective parts interaction that we can define
kind of causal relationships with that we can apply to technology they get selected for if it
grows profits wins and war can convince more people to believe a particular thing
and we have not correspondingly grown the wisdom of what types of tech should be built
and basically what i'm saying is we either have to do that or this process doesn't continue very long
that's really well said my my concern is how is this going to be regulated like you're going to
need a third party to decide hey this second order effect is going to cause a lot of harm
you can't do that you know it instantly makes me think let's say i'm trying to upload a youtube
video and somebody says hey this is going to lead a lot of viewers to go check out some conspiracy
theories that might lead them to oppose the government you can't post this video so i'm
generally i've been butting heads quite a bit with these like policy makers of the algorithm
the regulators because you know as a content creator i'm always fighting them over what i can
and can't post you know i'm always getting things taken down or shadow band and that's very frustrating
so it's yeah it's it's kind of it's a very hard thing to regulate yeah yeah and so
there are twin failure modes that the world faces right now and throughout history there's always been
failure modes that come from whatever the system of social coordination is
government institutions market religion whatever the systems of social coordination are
are not effective and you get increasing chaos right under critical things aren't manufactured
or made that need to be the people are warring with each other there's you know things like that
or uh the failure mode on the other side is that the social coordination system has enough power
to check those things and it itself becomes corrupt and you get tyranny and so there's this
kind of chaos on one side and tyranny on the other twin failure modes and we don't want either of those
and in the same way that we said we're for the first time in history even though empires have
always fallen we're at the place where the thing that you could consider the unit of civilization
is really the entire planet it's these six continents supply chains that make the stuff
that all of the civilizations depend upon and the you know following globalization
we are at a place where for the first time you could have chaos global catastrophic risk
for the whole planet we're also though at the place where you could have
autocratic dystopia for the whole planet right before industrial tech you could not destroy the
ecosystem like you just couldn't fish out all the fish with fishing rods and um but when you have
mile long drift nets where you can pull up whole schools of fish all day long yeah you can overfish
the whole ocean right but it's the scale of the tech before nuclear weapons we couldn't destroy
everything quickly even if we wanted to there were people who really tried and so the scope of the
catastrophic potential is related to our technological capacity but the same technology can also make
control dystopias right so and this is you can see the intuitions of people focus on one of these
problems or the other there are people who are just terrified of all regulation and typically tend
to think that it's almost always corrupt or whatever and there are other people who are terrified of
what um especially exponentiating tech and market forces due to the world uh without anything
checking them and those are both right they're both right and so there is a place where to think
about this correctly you have to embrace the rock and the hard place and not just focus on one and not
the other because the solution to one ends up being the other one so um we can see that
multipolar traps between major nations lead to probably destruction of everything right you the
nations in uh in multipolar traps with each other are all extracting more fossil fuels every year
are all externalizing all their pollution costs because nobody wants to internalize the pollution
costs themselves if everyone else doesn't and then have less margins to compete everybody is putting
more energy into militaries etc so you're like okay the competitive nature between the countries will
destroy everything so then of course one proposed solution is we need a one world government so we
don't have that situation and yet of course we can see that where power has become very concentrated
it has become corrupt in proportion to its concentration everywhere and so the idea of a
one world government obviously that in the time of the roman empire or whatever you couldn't really
see what everybody was doing you couldn't enforce rule of law because of the technological limitations
but in an age where we have real-time imaging of every spot on the surface of the earth every day
through satellites and increasingly all of the sensor data all of the satellite data it can all
be fed to ai's big enough to process all of that you could have a terrifying global dystopic control
state and if that and when we talk about checks and balances on power if let's say we recognize how
destructive uh decentralized artificial intelligence the way it is developing right now could be
because can you use it to uh make better chemical weapons to make better bio weapons to figure out
better infrastructure attacks to make better fake news to make better propaganda yeah you
can use it for all those things and even the things that you say are good things are good in
the definition in the in the market that does the narrow success thing so you just speed up the
narrow success in the total externalities so uh we'd say no the ai's going to save the environment
because it's going to give us all these efficiencies we have continued to make more efficient tech
every year while using more energy every year because efficiency doesn't mean you use less it
means the cost of the industrial input goes down so new market sectors open up that weren't
profitable before that's called the jevons paradox and it's one of the really critical
things to understand here is that if if in all kind of environmental type dynamics is when the
cost of energy goes down we use more energy not less when the cost of compute goes down we use
more compute not less when the cost of material acquisition goes down use more not less because
of certain like let's say the cost of energy goes down now there's this whole area that i can mine
that i couldn't afford to mine before because the you know cost to extract it wasn't enough of the
market value but now that my energy costs are lower now i can mine a lot more areas and put more
mining pollution and industrial tailings into the water so in the market type dynamics that
we have now where almost every goal happens at the expense of lots of externalities even the
quote unquote good things that you use ai to optimize end up driving the rate of the externalities
faster let alone all of the weaponized kinds of applications so okay so let's say we're like man
that's really bad and everybody's racing to do it and we're doing we're doing the exact opposite of
like thoughtful forecasting and testing and seeing what the same thing would be we're doing the here's
chat gpt let's roll it out to hundreds of millions of people instantly let's get governments to start
using it let's get companies based on it where then once you find out how bad it is you can't
roll it back because the entire economy depends upon it um the speed of deployment scale of ai
right now compared to say of oil when you know you uh u.s uh standard oil was developing is so
mind-blowingly fast um like the speed to a hundred million users in a week with threads or something
like that um and that's the exact opposite of how you make sure that you are not causing a lot of
harm is you do a lot of protracted thinking and then when you implement it you do a lot of observation
of the implementation at a smaller scale so you have a safe to fail probe and then you make changes
on it because of the things you couldn't have anticipated without doing some experiments
not rapid speed to scale okay so you say well what if we right now we're not succeeding on
regulating it in any good way it is proceeding and the applications of it are uh are really terrifying
but let's say we could and we said all right we're we're going to stop all of the kind of market
application of that thing until we have and the burden of proof has to be on proving that it's
adequately safe not that it can destroy stuff like lead and dbt and then later after does all
the destruction then we regulate it at this level of power by the time we realize the destruction
that it's caused it will be too late and irreversible okay well if we did regulate it and say we're
not going to allow that who is the we right that would actually because there's so much market
investment that would take government intervention using monopoly of violence rule of law backed by
monopoly of violence to shut it down do you know that military contractors and um classified
projects are not still developing the same things like it does that get shut down in the same regulation
pretty hard to know that and if it's not then you're still developing that tech just with no
checks and balances right such that um the monopoly of power now is nearly an infinite monopoly of
power this is the hardest issue humanity is facing right now is that the market forces
without very strong regulation will drive us past planetary boundaries and
lots of other global catastrophic risks incentives alone are not enough to fix it it needs deterrence
and yet they're the regulatory systems don't have the visibility the integrity the uh checks and
balances they need to be trusted with the power needed to regulate all of that so we both have to
basically restructure governance such that the transparency accountability etc necessary to
regulate things is there with actually the wisdom at the basis is there and have it do that very
quickly and obviously the chances of that don't sound good um it doesn't fundamentally require
violating any laws of physics to do it it's just a hard thing to accomplish but it's important to hold
that the control dystopia and the decentralized catastrophe that the solutions to one usually
make the other one worse and the actual solutions we're interested in are neither and this is where
we need to actually deepen our understanding of the failure modes hold them together and hold
designing away from both as the design constraints
yeah because right now with AI it's advancing so fast it seems like each week there's some new
giant update when you look at AI from a year ago it's it's a joke compared to what crazy deep fake
things they're coming out with today and they're talking about regulating it but it seems like
it's just advancing so fast by the time they try to figure out some regulating laws it's going to be
like 20 20 000 years more advanced and correct yeah the tech is evolving at the intersection of
multiple exponential curves and I mean formally exponential so you know we started to understand
exponential curves in tech with like Moore's law where you would get this progressive doubling of
compute speed you know per cost every couple years
right now just in the hardware in the AI space right the GPU cluster hardware it is 10xing
in six months and the rate of that is increasing
so you have an exponential curve in hardware you have an exponential curve in the amount of data
available you have an exponential curve in the amount of capital being invested and
people working in the space an exponential curve in the software development of the you
know cognitive architectures involved and all of those are intersecting and law is not only
still working as slowly as it did in the founding of this country it's working slower because of
increasing polarization of the population that is elected increasingly polarized representatives
that disagree on everything and so the nature of our so and that's important to say the social
media is actually an early AI right it's using an early form of AI to be able to curate all of the
world's content to maximize its stickiness for you and behavioral dynamics for you based on
harvesting behavioral data about you so deep fakes now are creating content that first one
was curating content the curating content is very powerful because that creates the incentive
for every other content producer in the world to produce stuff that the algorithms want
including mainstream news and everything else that increasingly is successful only in so far as
it's successful there so that technology has simultaneously decreased the capacity of government
to govern has decreased the clarity of thinking and collective intelligence of the population
increase their enmity and increase their certainty and wrongness at the same time
which is a terrible combination right increasing certainty with decreasing scope of understanding
on both sides or all sides of most issues while increasing the rate of the tech development
itself and increasing the amount of capital allocated to it so obviously that regulatory system is dead
and the you the regulatory system has to be able to move at a speed of the technology
that is being developed or it has no chance at all that requires some very fundamental
innovations and how we think of governance and of course if the founding fathers were alive today
and not doing the thing in 1776 and they were looking at the tech stack today and the issues now
