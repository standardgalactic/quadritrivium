your top down predictions. But now you have a really simple and a very plausible account of
active sensing, active vision, more generally active inference. There are two ways in which
you can resolve prediction errors. You can either change your mind or change the thing you're predicting
and thereby supplying the predicted outcomes. So this is a very simple account of action and
perception, and also speaks to the circular causality. You know, these are two processes working
hand in hand, your perception is causing action and action is causing perception. And this is how
we sense make. So that that sort of active inference perspective, which then I think
became sort of the dominant paradigm, the cognitive neurosciences under the root because
something called predictive processing, which was a sort of generalization of predictive coding to
accommodate this inactive aspect of perception. So the free energy principle on one reading
really is just a mathematical specification of this kind of active inference. The trick
here is to cast things in a most general and generic way as possible. And by things,
I just mean the resolution of prediction errors. And the way that you want does that is by appealing
to the maths of Bayesian inference, and noting that exact Bayesian inference is not physically
realizable. And you then have to turn to how can you physically realize
inference of this sort. And the solution was in fact
on offer from the 1950s from people like Richard Feynman in western
and physical sciences, but also hints at exactly the same solution in the early Russian literature
in terms of algorithmic complexity and comore graph complexity. Those two perspectives converge
upon exactly the same mathematical quantity, which is a variational free energy. This is
not a thermodynamic free energy. This is just a statistical way of measuring the thing that
you want to optimize in any active inference, whether it's your brain doing inference about
its sensations, or whether Richard Feynman, you're trying to infer the probability distributions
under the paths of small particles in quantum electrodynamics, whether you're trying to infer
the message that was sent in cryptography, for example, all of these kinds of problems
could be subsumed under one optimization process, which is the maximization of Bayesian model
evidence. It's literally the likelihood of these data being generated by the model that you're
currently considering. So that's in Bayes' statistics known as Bayesian model evidence,
and the variational free energy provides a tractable bound or approximation to that
marginal likelihood or model evidence, which means that in its most general form, the problem of
inference, and indeed the process of inference, if you don't want to consider a problem,
can be written down mathematically as any process that either maximizes marginal
likelihood, model evidence, or if you're in machine learning, maximizes free energy as an
evidence lower bound to also known as an elbow ELBO. There's a slight wrinkle here.
In machine learning, they use the negative of free energy that Feynman used. So if you
haven't talked to physicists, you would want to minimize your free energy. If I'm talking to
machine learning people, you want to maximize it. It doesn't matter from our point of view.
The key thing is that you're just writing down the functional, the mathematical form
of the quantity that underwrites the process of inference, that is a variational free energy
in optimizing, either minimizing or maximizing this, either by perception or through action,
then you are effectively self-evidencing. And that's a term which I like from Jacob Owen,
that comes from philosophy, that if we understand ourselves as making sense of the world,
through a process of inference, that implicitly what we are saying is that we are all in the game
of garnering sensory evidence for our models of that world. So we are literally self-evidencing,
and you can just write that down as the process of maximizing Bayesian model evidence for your model
entailed by your brain and your body, or you can write down as a physicist in terms of
minimizing a variational free energy, and that's the free energy principle. And that's roughly
where we stand at the moment, although there are lots of unanswered questions and construction.
You laid out very, very nicely. It's very easy to follow there. I do have many follow-ups here
about a few things. I'll try and keep them very general and reframe some of what you said.
So starting with Hemholz, you were talking about how there's this inside-out
and outside-in, that there's almost a dichotomy in how we understand the world. Do we understand it
from within ourselves and then based on what input, outputs, inputs, inputs, whatever way you
want to describe it? And maybe that's just how it's fallen out. Is it always kind of in these two
camps or categories? Or is there a third or fourth way? Is there another way where we could see
a kind of mixture where it's not exclusively outputs from the environment, or excuse me,
inputs from the environment, and then we output? Is there some way in which we can understand that
there's a third way or a fourth way or other ways in which these are combined and where it isn't just
input output, right? It's not just a kind of zeros and ones kind of thing. Is there another way in
which there's an integration of them, or potentially some way of the phenomenology of
how we're experiencing things? Is it always this kind of way and how it lays out? Because it seems
throughout time, as you were saying with Hemholz, there's this kind of dichotomy where people will
kind of look at the economy here and then all the way to the machine learning stuff in the 90s,
it kind of does this oscillating back and forth between where do we put the emphasis? Where do
we put it first? So I guess that's one component. The second piece here when you were talking about,
if I understand this, is the free energy principle from what you're saying sounds like it's the kind
of quantitative and statistical ways of understanding the Bayesian model of the brain.
And so they're kind of seen in tandem along with the active interference, which you explained.
Connected with that, there's this emphasis on perception, especially with humans.
So I wonder with the predictive types of elements that you were discussing,
where do we, this might be a little bit further out, and maybe some of this may be hard to know,
all of this is very human. I wonder if we understand some of these things for non-human
brains as well, such as with chimps or bonobos or even other types of, it might not even be
with primates, it could be what do we understand about how potentially
these workings of the brain could be for other brains that are non-human. So trying to look at
a kind of evolutionary structure and model. And then finally, curious about if you have any thoughts
about, you mentioned philosophy and as you were saying that there was this kind of in the 21st
century now, not just an over emphasis on the brain, but how the brain and how it works
is distributed through the body and how the body is a necessary component,
which automatically always makes me think of philosophy of Merleau-Ponty, who also
thought a lot about perception. And in many ways had very technical ways of thinking about
perception. He talked a lot about eyes, talked a lot about sensations. And I wonder
his estimation, he was looking at the phenomenology of things, of how do we understand
experiences through the body, that the body is a necessary or as a prerequisite of sorts as a
type of bridge for understanding our various perceptions. So those are my follow-ups. I
don't want to throw it all at you at once, but any thoughts about any of that, I guess.
Right. You've just thrown all three massive issues at once. All wonderful issues. So we have to
say at least something about each one of them. So in order then. So is there a third way that
does not commit to an inside-out information extraction or an inside-out construction versus
a constructivist approach versus an outside-in-extractionist approach? In a sense, that just
is the Bayesian brain. That just is the free energy principle, an active inference that is
entailed by the free energy principle, because that is explicitly saying you have to have this
two-way traffic. And this emerges in many different levels. So for example, we've already talked about
this sort of recurrent two-way traffic of message passing in the brain from the
the sensory parts of the brain to the deeper hierarchically organized levels of a
the brain's generative model. This whole belief updating, this whole Bayesian mechanics depends
upon this counter-stream of top-down predictions and bottom-up prediction errors on a predictive
coding architecture view. So you've got this recurrence, you've got this circular causality
for free, but even more generally, as you intermitted in the third issue, you brought to the table
here, the very dependency of perception on action and action on perception. So put simply,
I have to perceive and infer in order to plan what to do next. So action is predicated on my
perceptual synthesis and making sense of the current state of my body and the world and what I
want to query the world about, what kinds of questions, what kind of information do I want
to get by deploying my sense organs. So that is all the self-evident, but in the same way that
action depends upon perception. Perception also depends upon action. What I see depends upon
where I look, how I place my skin in relation to an object that I'm trying to palpate.
So there's this unbreakable circular causality and two-way street between action and perception.
So I would submit that your quest for the third way has been delivered
for you by the inactivist term, by the inactivist application of too predictive coding
to the brain hypothesis at the turn of the century. I think that is the third way.
You can't tease these two things upon, really. So that should be comforting and it should resolve
any angst that might be seen. It's very satisfying, yes. Good. Now the second issue,
what was the second issue? The second one was about the non-human brains, the evolutionary construct.
Yeah, the way that you asked that question makes me realize I've made a fundamental mistake here
in terms of telling the story and unpacking the narrative of
actually the inference in its 21st century version. So most of what I've just
talked about in terms of the mechanics of inference and optimal action
could be applied to a thermostat or a virus. So I think I would actually put the question
the other way around. Perhaps that's how you actually meant the question. I'm sure that we'll
come to this later on. What is special about the human brain that has some unique and definitive
properties that you would not find in a thermostat that you would not find in a virus,
you would not find in a mouse. You possibly wouldn't even find it in your most blood pet
or some of our closer companions and say monkeys in the evolutionary view.
So I think that that's the direction the question should be approached. It's less,
what's special about mice? It's more how do you make the move from mice to men under this kind of
architect. So most of the work that we do is at a much lower level. It's just trying to understand
the fundamentals of active self-organization through the lens of sense-making and inference
that can be written down as a process that usually is actually an emergent property of
any self-organizing system that preserves a distinction between itself and the environment
that doesn't decay, dissolve, dissipate, die. So looking really, trying to explain how on earth
does a virus survive and what must it do in terms of this circular third-way exchange between the
inside and the outside to preserve its very boundaries and demarketing from its external
milieu without worrying too much about what it is to be human. So we haven't got anywhere near a
sort of Marx-Semes-like set of questions yet. We are not talking about Bayesian beliefs as
propositional beliefs that you and I might hold and be able to articulate. We are not talking
about qualitative experience. We are just talking about the mechanics of belief updating on
neuronal circuits, neural circuits, chemical circuits, electrochemical circuits that we see
in every kind of biotic and possibly even nonbiotic self-organization. So that's a deep question.
And the answers to that question take you through a series of natural kinds of things
that all are doing some inference of possibly a weak kind and can all be, if you like,
described in terms of an ontology of generative models. If you remember before we're talking about
the underlying importance of a generative model in supplying the architecture under which you
generate your hypothesis, your top-down predictions, and you use basic, this is the thing that you are
securing evidence for, which means that if there is a difference between mice and men,
then there has to be something quintessentially different between the generative models that you
and I employ to navigate actively our world and those that mice and flies use. And once one starts
to think about it, one might ask what is the difference between a, let's take viruses, flies
are a little bit too sophisticated. What's the difference between a mouse and a mouse?
And what's the difference between a mouse and me or you? And normally the story goes that
it's all to do with the ability to plan. It may sound very, very simple, but once you've heard the
argument, you know, it is actually a very simple argument, but it's also quite a compelling argument
and has a degree of explanatory scope in the sense that the thing that distinguishes between a
thermostat or a virus or some single that cell organism succumbing to nutrition gradient through
chemotaxis relative to something like a mouse that does stuff in anticipation of consequences
is the ability of the mouse to plan. The mouse can think about and select those actions that
have consequences and that's absolutely profound when it comes to writing down or thinking about
the generative model that that mouse is using. It tells you immediately that the mouse has a model
of the consequences of its action. If it has a model of the consequences of its action,
given that the consequences occur after the action, then it also has a model of the future.
So it now has a model that has a temporal depth and this can be contrasted with the thermostat
or a virus. It does not, the thermostat doesn't have a model of the future. It doesn't have an
explicit hypothesis or question in its internal machinery of the kind. Or what would happen if
I switch this heat making device on or not? It just responds reflexly in the moment. But the mouse
will have a much more elaborate model, sometimes described as a model with temporal depth that
frees you from the moment, but also crucially underwrites ability to plan and thereby select
from one plan or another plan one course of action relative to another course of action.
That endows that middle agent with agency because now it is selecting amongst a number of different
courses of action that are entertained and can be supported by its generative model. So now together
you've got this sort of move from a thermostat to a mouse that entails the notion of agency and
temporal depth simply because this generative model has the ability to model the consequences of
action. So that's one move. Does that give you qualitative experience? Does that give you propositional
beliefs? Do we have mouse philosophers who spend their time puzzling about why they see red and
does one mouse see the same red as another mouse? No, absolutely not. So we still haven't answered
the question. What's the difference between a mouse and a man? Sorry, just on the mouse thing,
it sounds like what you're describing in the ability to have some planning sounds like there's
at least a low level type of what we would call executive functioning, where there's
abilities to plan, organize sequence, which we normally see in the prefrontal cortex for humans,
but it seems like it's a very low level ability, whereas as we get to humans, there's a much
deeper level of abstraction that we understand. Is this about right? No, that's exactly an
absolutely right. So all I'm talking about here is exactly the prefrontal cortex and all the attendant
executive machinery right down through some supplementary motor area, pre-motor cortex,
motor cortex down to the brainstem and all the executive components that are responsible for
executing the plans that are elaborated through selecting the right course of action in the
prefrontal cortex. So that's exactly right, and that's what mice can do. We don't have quite as
well developed a prefrontal cortex as we do, but they have the equivalent capacity to execute plans
simply because they need to have that because they have plans in their head, but having plans in
their head is a remarkable capacity. A thermostat, I repeat, does not need to have a plan. It just
responses a reflex as do many forms of life or biotech self-organization.
So you also used a hierarchically deeper levels of abstraction. I think that's the absolute key
here. And then this question would be, well, what are you hierarchically abstracting? And I think
most people would answer, well, you are now creating models of your executive process. So now a mouse
has the ability to plan and to execute those plans by selecting the plans that are most plausible
in the sense that they will resolve surprise or prediction error or free energy in the future,
we call that expected free energy technically, which has some interesting aspects to it.
So the mouse then has this ability. So what ability do we have because of our more evolved
prefrontal cortices with these deeper levels of abstraction? Well, it could simply be to
entertain the hypothesis that I am a person who is now selecting these plans, and it is me who is
actually executing these plans. So to a genitive model that now entertains the hypothesis, the
belief, the representation, that it's me doing these things. It is me who is embodied. It is me
who is the agent. So an agent doesn't need to know that it is an agent to actually plan and
