is equipped with something called a statistical manifold uh by by which I simply mean that every
point on this manifold that corresponds to a pattern of brain activity is equipped with a belief
structure a belief distribution a Bayesian belief about something else about the outside
you the causes of my sensations and that means that I can now talk about an information geometry
of a different sort which is now about the outside an extrinsic information geometry
and that's quite a nice picture because this notion of belief updating we've been talking
about Bayesian belief updating um is um corresponds to a movement on this statistical manifold so
you imagine that every pattern of activity in your brain corresponds to a point on this abstract
manifold this surface in a very high dimensional space and every time the pattern of activity changes
in virtue of your neural dynamics you make a movement on that space and that movement is
belief updating it is exactly the same as moving from the prior to the post year
that we were talking about before so sensations come in and they move the point on the
statistical manifold to produce the belief updating so you are literally now
describing mathematically in with an information geometry belief updating and the process of
inference and the key link here is that the the measures the the metrics that define this
information geometry or belief updating are something called a Fisher information metrics
and mathematically they correspond exactly to the precision we were talking about before
so technically it's the curvature of that space as measured by the uncertainty or the precision
of your belief updating so there's an intimate relationship now between the precision that
we were talking about in relation to um say mark zones and just to slip in here that the cells of
origin of those neurotransmitter systems that control the post-synaptic game that mediate the
precision are exactly in the mid-brain structures that mark things as the seat of consciousness
so this is all entirely internally consistent but it's also consistent with the Freudian notions
of drives because you can think about this movement on the statistical manifold this belief updating
has been driven by forces what are these forces they are the prediction errors they are the
the surprises they are the effect of the likelihood on the priors that force it or move
it to produce the posterior so now we have this notion very elemental notion of a drive
um probably uh is sort of oversimplified from a Freudian perspective but I think the mechanics
sort of holds true here the drive is literally the thing that the uh rule of the prediction
error is in driving belief updating from prior to posterior beliefs but crucially we now understand
this force this drive as being gated or nuanced or contextualized by the precision which brings
us again back to the fundamental importance of getting this sort of metacognitive beliefs about
beliefs or predictions of precision right in terms of enabling those particular forces and drives to
be manifest in the way that we both organize our neural activity but also explore our belief space
by moving around on the statistical manifold so you can write this down very precisely in terms
of information geometries there's lots of really interesting maths that gets right into the world
of gauge theories and they're like all telling the same story but the the met the the potency of
these drives depends upon the curvature and the curvature depends upon the precision or our
beliefs about the uncertainty the point here though from a from a sort of consciousness
point of view is that there are two descriptions of movement on this statistical manifold there's
a description from the extrinsic perspective in terms of the probability distributions encoded by
a point on the manifold and there's another intrinsic description which is the simpler one
just about the probability of being having this pattern of neural activity so there are people
I'm thinking about colleagues such as Vanille Weiss who together have put this forward
as a way of repairing dualism that you've basically got a a dialectic for free mathematically in the
information geometry that jointly ascribes a particular physical instantiation in terms of
neural activity and patterns of neural uh neural firing to any point on on the statistical manifold
that is accompanied by beliefs about something else which would be the sort of the more mindful
aspect of a cut cut easy divide so you've got this sort of a monistic account that has these
dual aspects of mindful belief updating and the physical embodiment of that and I think
would be a potentially useful way to talk about sort of dualism versus monism when it comes to
you know to some some possible to be a historic but to my limited understanding sort of you know
foundational issues in in consciousness research and that that would be that would be sort of
one if you like opportunity for the the Bayesian mechanics afforded by active inference and the
free energy principle to produce some quite clear predictions about the relationship between
mindful stuff and brainful stuff where there has to be if you like a convergence within this
within this information geometry the other thing it plays to the table is that it is the
movement on this manifold it is moving through a belief space that is the essence of inference
so if you remember the free energy principle is just trying to describe the dynamics of a system
that has a non-equilibrium steady-state solution for a certain amount of time
inherent in the use of the word dynamics is you're looking at a process that is unfolding in time
what that tells you is that if you want to talk about consciousness then it may be best to think
of consciousness as this process of belief updating in exactly the same spirit that we talk about
evolution as an evolutionary process we don't talk about the state of evolution at any particular
time that's divorced from the dynamics the paths and the process itself the the process of evolutionary
selection as opposed to say selecting a plan or a particular belief or you know you know
are updating my beliefs in terms of the process of inference in the under the Bayesian brain
hypothesis you know we talk about the process as unfolding in time so what that would mean
if you just translate that perspective which is inherently dynamical to consciousness research
is that questions about what is to be consciousness have to be recast in terms of what processes must
you evince in order to qualify as having a conscious process so states of consciousness
and levels of consciousness are only meaningful when they apply to something that is changing in
time essentially moving on this on the on the statistical manifold so that's from my point of
view a couple of things that are brought to the table by having a formal mechanics or calculus
underneath belief updating to have a calculus of sentience of physics of sentience
that tells you immediately you're talking about processes and of course if you think of you know
in terms of any optimization or casting things in terms of any optimization then you are talking
about the process of optimization and indeed even from the point of view the the semantics of things
like variational principles of least action you know the leastness the minimization aspect of it
talks again to a you know a process that is you know playing out over time so that that would
certainly be if you like one's constraint on offer for anybody who talks about consciousness
you know are they committed to a way of thinking about consciousness that just doesn't have any
any meaning in the sense that it's about a particular state of being
or can they recast or formulate their hypothesis in terms of of a process
the third level of the third sort of thing on offer with active inference brings us back again
to the geratin model so remember previously we were talking about differences between
thermostats flies mice and men and I was very comfortable talking about those differences
from a unique perspective which was simply what are the differences in the geratin models so if
you can boil everything down to the evidence for which you are trying to secure which is the
geratin model then you can start to talk about different kinds of consciousness in a graded way
in terms of different kinds of geratin models and we've already had this conversation in the
sense that you know I can have a geratin model for a thermostat which has which is temporarily
very thin it has just you know very very limited temporal horizon and then we graduate to particles
and creatures that have a more allostatic as opposed to a homeostatic
way of engaging actively with their environment that acquire a certain temporal depth and then
we move on to the opportunities when you now have a geratin model in the future for different
outcomes different counterfactuals and then we move into Anil Seth's notion of counterfactual
richness but noting the counterfactual richness as one way of understanding consciousness or a
conscious process is only allowable when your geratin model has a the capacity to model counterfactual
outcomes in the future and then we get to the remarkable counterfactual outcomes that underwrite
things like you know the philosophical zombie and brain of that and then we start to talk about
you know we can go from mice to men to philosophers and then start to engage
engage with that kind of conversation so that's another you know sort of structural
architectural formal thing or way of dealing with these questions that you get from the
physics and the mathematics which wouldn't otherwise be there if you're just using narrative
arguments historical arguments i was going to say you were talking about this convergence and as
you were describing that i was thinking about how there is a kind of multi-dimensionality
or a continuum aspect of this having a being a process of things as opposed to just a static
kind of state which i think um based on many of the things that are right on consciousness that
makes a lot of intuitive sense and i think the way that you're talking about this underlying
calculus to try and describe a kind of process makes a lot of sense in terms of artificial
intelligence so maybe we can make the the leap a little even further right mice to men to philosophers
to ai right how to obviously there's there's it's sort of connected with the consciousness
question although you could make artificial intelligence that doesn't need to be conscious
i don't think it's a prerequisite although some people may argue about this but
in terms of again the this underlying calculus that is there with the free energy principle what
what is it's i guess specifically or
something that gives it this uniqueness that can give us information about
artificial intelligence obviously artificial intelligence is a very broad
uh categorization so i mean my you know dishwasher and microwave is a form of artificial
intelligence right at a very very low level um and then there's artificial intelligence that we
see in science fiction stories such as you know humanoid robots that are sentient and they maybe
have feelings and you know things like that over time so there's the whole gamut here and of course
you could make the the the offshoot of well first you have to define intelligence which is a
very long uh footnotes which we don't have to do although if you want to i would
totally be fine with you explain that as well but just how do we understand the
elements of free energy principle for artificial intelligence now and how it could have potential
utility as we continue to grow and expand our mechanics or computational aspects of
artificial intelligence part of your question was your how how
does a free energy principle give you the opportunity to get more information about
the uh the nature of intelligence and as you uh innovate sort of artificial intelligence
interestingly that that that that getting information about is actually at the heart
of the answer i'm going to give you so the answer i'm going to give you um i'm going to
foreground though um one key thing which is um in response to a previous question here what's
a free energy principle for um uh very much like the theory of natural selection it doesn't really
do anything we're afraid it's it's it's very pretty and you can have philosophical debates
about it which i'm sure we'll come to um but you know why is it useful um and the answer is um
if you read the free energy principle as a um a mechanics that is apt to describe
systems that have solutions to the non-equilibrium study states in terms of an inference process
then the answer is it's pretty you know it is not useful however once you know the functional form
of the equations of motion the dynamics the updating
that have to be in play in order for the system to be intelligent or to be to show
a self-maintaining or auto poetic like behavior what you can do is um write into an artifact
the generative model and all the prior preferences entailed by that generative model
to simulate an artifact that has the same properties of self-maintaining and maintaining its
Markov uh blanket why would you want to do that well i think there are two cardinal reasons why
you might want to do that why would you like to build a Markov blanket maintaining building
machine that's doing inference that it basically is realizing the principle of least action that
prescribes the differential equations that you would write into a computed simulation or indeed
a robot or a your um an artifact that was supposedly showing artificial intelligence
well i think there are two reasons to do that one you might want to build um an active inference
agent an artifact that's actually doing active inference how would that differ from artificial
intelligence as we currently know it that's a really cool question there are very clear answers
and i'll and i'll give you the answers in a second but just to um conclude this part of the answer
with the second motivation that you might want to build synthetic agents with synthetic sentience
under the variational principle of least action that is the pre-energy principle
is you might want to make inferences about somebody else and i'm now thinking from the
point of view of a psychiatrist you might now want to try and simulate choice behavior and active
inference under a particular model and then adjust the preferences and the price of that
judging model until you reproduce the kind of observable behaviors that this patient has or this
particular cohort shows so what we're talking about here is essentially using the simulation
models as observation models of real world people you know and other creatures uh in order to try
and understand their internal dynamics and their internal mechanics there's something quite fundamental
about that particular take because remember before we we talked about the markoff blanket in
shrouding the internal states of something from the external states so that there is if you like an
impenetrability there's a fundamental statistical divorce between um the internal states and the
external states meaning that the external states are hidden from the internal states
but the converse is also true which means that from the outside the internal states of any creature
or person or loved one are forever hidden from you from the outside so you can only ever infer
the machinations and the dynamics and the belief updating on the inside which is interesting because
that means you know much of my day job in neuroimaging is trying to peek behind the markoff
blanket to see what's going on but you you can't do it properly otherwise you'd have to break that
you'd have to breach the markoff blanket to become invasive or study dead things in order to
try to infer what was going on so that means that you need to you need to use inference just to
understand the internal dynamics of the brain of a person which means that the only way you can do
it is to have a generative model of a generative model which means that now you've got another
use case for active inference in the free energy principle which is building models of people and
patients and then adjusting them until you use the adjusted model that renders the the behavior of
the model very similar to the behavior of the patient to infer this is how the patient works or
at least this is as if you're at as if explanation or counter mispatient so that there's my two
principle motivations you know to use these as a principled model of choice behavior or
neuronal responses that I could measure empirically to answer my hypotheses about how the brain works
there is another agenda which we've touched upon which is your more the fun of creating
artificial general intelligence or general intelligence or AGI and the distinction between
artificial intelligence and AGI or the next wave of you know artificially or intelligent artifacts
I personally do think is staring you in the face
in terms of what the difference is between active inference
and under more sophisticated generative models that have the ability to model the
consequences of action so if the end point of this argument is that if it is the case
that I have to minimize my prediction errors or free energy or at least look as if I am
minimizing my prediction errors or free energy just to exist if it is an existential imperative
for existence then a priority I must also believe that every action that I choose every choice that
I make must be that which is the most likely to minimize prediction errors or free energy
after I've made that action but the consequences of my action are now scored
in terms of their ability to reduce the free energy I expect following an action
so then what you can do mathematically is just look at the expected free energy
and ask how could I interpret that what does it look like what are the qualities how
what are the the first principal accounts of a good choice or a good action and when one does that
it looks like as you might expect there are a number of different interpretations of this
expected free energy which is a big thing that active inference takes the table so this is an
