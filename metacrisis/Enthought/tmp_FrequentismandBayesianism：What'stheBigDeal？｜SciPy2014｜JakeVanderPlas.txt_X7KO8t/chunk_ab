and some p value, and you integrate over all possible p values.
So this is what Bayesians are doing when they're marginalizing.
They're just getting rid of parameters they don't care about.
So you do some algebraic manipulation,
and I'm not going to go into the details of how to solve this.
That's in the paper, but you basically find that the odds are 10 to 1
against Bob winning after the marginalization.
So here's what you got.
You have the frequentist says 18 to 1 odds,
the Bayesian says 10 to 1 odds,
and the question is, who's right?
I'm not going to tell you who's right,
but I'm going to tell you what's different.
The main difference is that the Bayesian approach allows this nuisance parameter to vary,
and the frequentist approach keeps the nuisance parameter fixed.
There's probably some people thinking in here that I'm full of it
because the frequentist can allow nuisance parameters to vary in certain ways,
and you can explore the sampling distribution,
or you can transform the problem,
you can do various things like that.
But what I would say is I think the Bayesian approach
offers a much more natural way to allow these nuisance parameters to vary.
So essentially the difference is that the frequentist
is taking a slice of this joint probability
and gets a very narrow posterior.
It's not really a posterior, but they get a very narrow result.
The Bayesian is taking the whole probability
and kind of squishing it horizontally there,
and in the process of that gets a very, very wide result.
So this is the difference, taking a little slice
versus taking the whole thing and integrating or squishing it together.
So second example, this is one that I think is the most important difference
between frequentism and Bayesianism,
and this is the handling of uncertainties.
So when someone gives you an answer, you say the flux is 999 plus or minus 4.
What does that plus or minus mean?
And it turns out that because of the philosophical differences between these approaches,
it's a very, very, it's a very different thing.
It's subtly different, and it's different in a way that a lot of people miss.
So for frequentists, they talk about something called the confidence interval.
They say if this experiment is repeated many times,
95% of those cases, the computed interval will contain the true value.
So this is a 95% confidence interval.
Bayesian says given our observed data, there's a 95% probability
that the value lies within the credible region.
So these seem really similar, but notice the difference.
The things that are varying and the things that are fixed are the opposite.
So the frequentist keeps the model parameter fixed
and says that the confidence interval itself is varying.
The confidence interval is derived from the data,
which is a random quantity derived from the model, right?
The Bayesian keeps the credible region fixed
and varies the value of the model parameter.
So the model parameter is, this is our belief about the model parameter
that can kind of move throughout space.
So this is the way that they're a little bit opposite.
And this is why for a long time,
Bayesians were called, the Bayesian problem was the inverse probability problem
because you're basically taking the frequentist problem
and you're turning it on its head, right?
And this ends up having some interesting consequences for certain problems.
And I want to illustrate it with this truncated exponential example.
So this is something that James, he was a physicist back in the mid-20th century.
He wrote about in 1976.
Basically, we're considering a model where you have a certain amount of time
given by theta, and after that certain amount of time
you have a probability of something happening.
So for the example that James used,
this was a chemical inhibitor that kept a device from failing.
And then say after 10 minutes, that chemical inhibitor runs out
and the devices start failing with this exponential decrease.
And I put this in a blog post recently and I thought it was pretty cool.
I got an email from a guy in the Institute of Health Metrics at UW,
and he said, this is the exact model that we use for mosquito nets in Africa.
So it turns out that this model actually is applicable.
People are using this sort of thing.
So the question is, you observe some failure times of this device
or you observe when the mosquitoes get through the nets.
And you want to estimate from that how good these mosquito nets are,
how long they'll last before they fail on average.
So we have a couple ways of doing this.
We want to know the 95% bounce on this parameter theta,
how long the nets last before they fail.
Now for the common sense approach, we can look at this and we can say,
well, it's impossible for these to fail while the inhibitor is still there.
It's impossible that the failure starts after a certain time.
So each point that we observe has to be greater than that inhibitor time.
So the smallest observed point here is 10,
so we can immediately say that theta has to be less than 10.
So this is just common sense.
We're not applying any statistics.
Oh really?
I'm running over.
I'm going to finish this real quick.
So a frequentist approach, this is the unbiased approach.
You create an expectation value.
You create an unbiased estimator.
I won't go into the details,
but I can assure you this is the correct unbiased estimator.
You compute the sampling distribution
and what you find is that the 95% confidence interval is between 10.2 and 12.2.
So we've said by common sense it should be less than 10,
but the frequentism tells us, well, it's between 10.2 and 12.2.
So let's see what the Bayesian does.
The Bayesian, again, we use Bayes' theorem.
We compute the likelihood.
With the flat prior, we get a posterior that looks like that.
We draw the limits and Bayes says 9.0 to 10.0.
So immediately right now you're thinking,
frequentism is wrong.
But actually, frequentism isn't wrong.
What we're seeing here is that the frequentism
is answering a different question than what we expect.
And to visualize that,
so basically the Bayesianism is making a probabilistic statement
about the model parameter, that theta that we're interested in,
given this fixed region we've computed.
And on the other side, frequentism is giving a probabilistic statement
about the recipe for computing this bound
given a fixed model parameter.
And, you know, I plan to do way too much in here.
So I'm going to end with just this visual description of what this is
because I think this will kind of hit at home.
So the Bayesian-Credible region looks like this.
We create an interval and we say,
given all these parameters that fit within our belief structure,
95% of those will fit within this interval.
The frequentist version says,
well, we have one parameter and we're going to construct
a whole bunch of credible regions
and 95% of those credible regions will contain this parameter.
But the problem is right now we happen to choose this one, right?
So we can't say that this value has a 95% chance
of sitting in this particular credible region.
But the frequentist result is still correct
in the sense of the long-term limiting frequency
of this recipe for constructing things.
So you've got to remember this.
In general, when someone gives you a frequentist confidence interval,
it's not 95% likely to contain the true value.
And this is a Bayesian interpretation of a frequentist construct
and it happens all the time.
You have to be really, really careful about this.
So I always imagine this sort of conversation happening.
And the statistician says,
95% of such confidence intervals and repeated experiments
will contain the true value.
The scientist says, ah, yeah, so there's a 95% chance
of the values in the interval, right?
And the statistician says, no, you see,
parameters by definition can't vary,
so referring to chance in that context is meaningless.
The 95% refers to the interval itself.
And the scientist says, oh, so there's a 95% chance
of the parameters in the interval.
And the statistician says, no, it's like this.
A long-term, limiting frequency of the procedure
for constructing this interval ensures that 95%
of the resulting ensemble of intervals contains the value.
The scientist says, ah, so there's a 95% chance
that it's in the interval.
And the statistician says, no, it's just,
write down what I said in the paper and the scientist says,
okay, value is 95% likely to be in the interval.
So this is what we have to be really careful about
as scientists, that we don't misconstrue
those frequentist constructs.
I have some other slides, but I'm going to just skip down.
I obviously planned way too much.
These sorts of things happen, you know.
But basically, the conclusions,
so frequentism and masianism fundamentally differ
in their definition of probability.
And in simple problems,
the results are basically the same.
You don't have to worry too much
if you're computing the mean of a distribution.
But what I think I've argued for here,
and people might disagree with me,
is that Bayesianism provides a more natural handling
of nuisance parameters.
So these things that you have to fit into your analysis
but don't really affect the results
except for the intermediate calculations.
And I think Bayesianism provides
a more natural interpretation of errors too.
You know, we don't, with our scientific results,
to phrase it in terms of this 95% of an ensemble
of potentially calculated confidence intervals.
We want to show an interval
and say the parameter is 95% probable to be in there, right?
So I would say that Bayesianism is more natural
for communicating our scientific results to the public.
There's some philosophical issues with priors in there,
and to give a fair treatment,
I probably would have gone over those a little more.
But what I can say is that both paradigms are useful
in the right situation.
And I think it's on us as scientists
to learn the situations where they are useful
