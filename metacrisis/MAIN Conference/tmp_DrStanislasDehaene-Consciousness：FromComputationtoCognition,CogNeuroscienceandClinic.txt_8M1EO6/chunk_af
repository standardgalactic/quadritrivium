It's all done in the laboratory of Lionel Nakash.
And in this paper,
putting together a lot of data from different groups,
we show that using random forest algorithm,
we can get robust EEG decoding.
And what's important is that it is cross-site now.
So from different clinical sites,
maybe using different EEG machines,
we can begin to have a common classifier.
In the last part of my talk,
and it will have to be necessarily a bit short,
I would like to move to this theme
that I mentioned at the beginning.
Do we all have the same contents of consciousness?
I hope I've shown you that from macaque to human,
we have similar mechanisms of access to consciousness.
When we perceive a line or when we perceive motion
or perhaps a face,
the mechanisms are very, very similar.
And we have this nonlinear access to consciousness.
But my hypothesis is that there is something different
in the human when it comes to higher level states.
We have richer states.
We are able to represent information in deeper manner.
In this review, in the journal Neuron,
we discussed the representation of sequences
with this idea coming from this local global paradigm
that sequence knowledge is already present in the monkey.
And of course, the monkey already knows
about the transitions between the items
and knows that there's one which can be expected.
I've shown you this.
Other experiments show that the monkey
can group the items together into charms.
The monkeys also know about the order of items.
This is the first.
This is the second.
This is the third.
I've shown you that.
And the monkeys may even be able to learn algebraic patterns.
So the first two are identical, for instance, AAB.
AAB.
And then there is perhaps a violation of this pattern.
All of this is available to monkeys
and probably many other levels as well.
But there might be a level which is unique to humans.
This is the level of nested symbolic structures.
In order to describe the structures of language,
such as this very simple phrase here,
those gifted car factory workers,
you need three structures.
You need parenthesis.
You need to be able to say it's a car factory.
And then there are workers in this factory.
It's not just car factory workers as a sequence of words.
It's a tree.
And the same applies to mathematical patterns
or mathematical equations, to musical patterns.
Or to a theory of mind, we need recursive structures
that have nested inside each other.
So our hypothesis is that this may be unique to the human brain
and that the human brain uses these three structures
to further compress the information
to discover some of the laws, symbolic laws, perhaps,
that Joshua Benjio was talking about.
It uses nested three structures
to further compress the information
into extremely compact expressions.
I don't think I have time to show you
all of the power of these ideas,
so I'll refer you to this book, How We Learn.
But I think this is a key idea
for how humans grow concepts.
And I'm very influenced by the work of George Tannenbaum
and in his paper by Kemp and Tannenbaum,
they show how to build an algorithm
that can build new concepts out of the grammar,
the generated grammar, or smaller ones,
and build concepts such as the tree of species.
Through this idea of a sort of language of thought,
a grammar of thought.
So we are starting to investigate
whether this actually occurs in humans
and not in the monkey,
using extremely simple paradigms.
I will show you one.
You'll be surprised by the simplicity.
So I hope you can see the movie on the screen.
You can see that there are eight locations here,
and this is a test for young children.
We tell them there's a fish,
which is hiding in this pond,
and try to guess where he's going to go.
Okay, so this is one trial learning.
I hope you guessed where he's going to go next.
Everybody says he's going to go here,
and then here, and then here.
You guessed the pattern even before it occurs.
Try to do that with neural networks.
It's not so easy.
This is very fast learning.
And what we've shown
is that in order to understand this sort of learning,
you need to assume some kind of language of geometry.
You need to assume primitives.
For instance, you need to assume
that the subjects understand that all of these diagonals,
they're all symmetries.
They're on the fixed axis.
They're all similar in a certain sense.
And so you can anticipate that this is the next one,
and then there will be this other segment here.
And you need a sort of formula
that expresses these repetitions.
This formula is a bit complex,
but really what it says is that
there is a repetition of four segments.
There's a first segment, second segment,
a third segment, and a fourth segment here.
And you can build other shapes,
such as two rectangles.
As long as there is regularity,
you can find an expression which is compact
and captures the regularities that human captures.
Well, we've done a lot of work with this.
I won't bore you with all of the results,
but just to say that our memory is driven
not by the length of the sequence.
In fact, all of the sequences we use were always eight items,
but it's driven by the minimal description length,
which we call Kolmogorov's complexity.
So the length of the shortest expression
that can compress the sequence.
And if you don't have a language,
you don't have the correct expression
for these minimal description lengths.
It's not just slots.
There's this idea of working memory as slots.
But I think in humans, it's not just slots.
It's really compressing the information.
Well, we have a lot of ongoing work at the moment
to try to see if this is true in the monkey.
As you can see, we did experiments with adults,
we did experiments with preschoolers,
so it doesn't have much to do with being trained in mathematics.
We even tested adults from the Amazon
that have very little access to education
and again found that you make more errors
sentences for phrases, sequences that are more complex.
That is to say, they are less compressible.
They require a longer expression in order to be described.
Well, this relationship we don't find in the monkey at all.
In the current work, which is done by Lippin-Wong
at the Institute of Neuroscience in Shanghai,
after staying in my lab for many years,
we see that monkeys do not seem to care at all
about the temporal or geometrical regularities.
They just store the locations in working memory.
They don't seem to care for the structure of the transitions.
In order to understand this language,
you need to care very much about the nature of the transitions,
much more than the particular locations.
You need to be able to understand that it's four segments,
regardless of how they are oriented, for instance.
Monkeys don't seem to be there.
In very recent work, which has just been accepted
in plus computational biology,
we extend this approach to auditory sequences.
Let me test your memory for sequences.
Do you think you could remember this sequence?
Okay, this is eight, I'm sorry, 16 tones long,
16 tones that can be A or B, high or low.
Let me have you listen to it again.
Okay, I think you will agree
that it's almost impossible to remember.
At least you will need many trials before you can remember it.
Now, let's listen to this one.
You think you could repeat it?
The evidence is that you can.
Maybe you'll need two trials, but not more,
because this is highly regular, and maybe there's another one.
Your capacity to remember these sequences
is again proportional to their regularity.
We've tested this in two different ways.
One is just to ask subjects for subjective complexity rating.
So just rate the complexity as you feel it.
But the other is to ask for an objective measure of complexity.
Can you detect whether there is a violation?
One of these tones is wrong.
Can you detect it?
And this is the percentage of detection,
and combine also with the time that it takes you to detect.
So remarkably, the very same measure of complexity
that we used for the geometric sequences
can be used for the auditory sequences.
It's a cross-modal notion of regularity.
The only thing that you need to capture what's happening here
is a notion of nested repetition.
There is repetition of repetition of repetition,
and you can compress the sequence
