cycle can continue. So in short policy selection can be cast as maximization of an expected free
energy whereas perceptual inference can be thought of as maximizing this evidence lower bound.
So all I've said is essentially we have at hand a variational principle of this action
that subsumes both the epistemic information part of optimal behavior and that which can
be ascribed to the extrinsic or the expected value under some prior preferences and together
they simply mean that we are choosing those actions inferring the plans the policies that
maximize expected free energy. And this is very close of course to things notions
like planning and control as inference but in a way that has this dual aspect of
of both trying to be information seeking at the same time being reward seeking or
goal seeking in our epistemic responses. So let me briefly illustrate what implications
that formulation that sort of least action formulation has for belief updating in the
brain and subsequent behavior. I've written this down formally in a way that I thought might be
revealing for people in computer science after designing message passing schemes. What this slide
is meant to show is that if I had a generative model of how hidden states, unobservable latent
states out there generate outcomes then I can always write it down as a probabilistic graphical
model. So for example we could assume a discrete state space model a hidden mark of model or a
mark of decision process partially observed of the following form where the world is in different
states one of a number of different states and they move from one state to the next point in time
in accord with a probability transition matrix that itself depends upon the particular sequence
of actions that I prosecute the plan that itself depends upon the expected free energy
and once I know the states of the world I can then generate through a likelihood mapping here
denoted by the A matrix some observable outcomes. So this would be a generative model for a partially
observed mark off decision process that is inherently inactive in the sense that the state
transitions depend upon my action which itself depends upon the expected free energy and you
know please ignore these equations here they're just articulating this example of a generic or
universal example of discrete state space models of the way that my observable outcomes my sensory
information could be generated. This is nice because if I can write down a probabilistic
graphical model there will always exist a factor graph and in this instance a
phony or normal style factor graph that is the cousin or the conjugate of this graphical model
here where the nodes become edges and the edges become nodes we don't need to worry about that
although we need to know is if we can write this down we can always write this down what
does writing this down mean well it entails a very precise a concrete and crisp description
of the message passing that would be needed in order to infer all the unknowns here basically
states the world that I don't know and what I am doing the policies. I can infer all of these by
passing messages around over these edges that now correspond to the run the unknowns the random
variables given the parameterised generative model in terms of the priors the probability
transition matrices and the likelihoods named in the likelihood mappings between causes and
consequences and I've just written down this message passing here in terms of belief updating
belief updating of things that we don't know states of the world under a particular policy and the
policy which is just a softmax function of the expected free energy here and the expressions
that inherit from the the form the functional forms of the previous slides written down in
terms of linear algebra and the matrices and tensors of the generative model here.
Those updates which are just off the shelf a variational message passing updates on the previous
slide are quite remarkable not just in terms of their simplicity but the isomorphism with the
kind of message passing we actually see in the brain so I've just written them out again here
just to highlight how similar they are to things that we already use to simulate neuronal dynamics
and message passing on neuronal networks so for example here the belief updates for hidden states
the world under a particular policy any particular point in time are just a non-linear function of
a linear mixture of outcomes through the likelihood matrix and beliefs or expectations about past
states and future states that are mixed together in the right kind of way to give me the best
expectation or guess about the state of the world at any particular time. Policy selection is this
standard softmax function softmax response function of this expected free energy I've
equipped this model with a precision parameter which we don't need to worry about too much for
the point of view of this talk it's interesting in neurobiology because it seems to behave very
much like a reward prediction error of the kind that people who studied opening responses in the
brain like to think about. Learning just becomes effectively optimizing the parameters of the model
the D here are the initial states at the beginning of a plan or a policy but the same
functional form applies to all the parameters and it looks exactly like Hebbin or associated
plasticity evidence accumulation in this simplest example here and then action selection is just
selecting the most likely action from my inferred or favorite plan. This is a slide that just
describes a simple setup a toy example I'm going to use to illustrate the generic kinds of behavior
that ensue when we solve those belief update equations or that variational message passing
on the previous slide. So this is a simple an example as we could find that has both the sort
of pragmatic and epistemic bits that speak to the intrinsic and the extrinsic value
entailed by the expected free energy. The idea here is that there is a little mouse
it can make two moves in this T maze and it wants to find a red reward that can either be in the
left or the right upper arm of the maze but in addition it has the option of going to the lower
arm where there's an instructional cue that will tell it where the reward is so you can't see where
the reward is from its starting point in the center so it's got a choice it can either take a gamble
and 50 percent of the time be correct and 50 percent of the time be wrong but I should say that once
it goes into one of the upper arms it has to stay there these are absorbing states so there's a 50-50
chance that it will secure its reward for two moves or not or it can choose to go and solicit
the information, garner the information from the instructional cue but it's wasting one move
but it then knows with a fair degree of certainty where to go to the baited arm to get its reward.
So from the point of view of an expected utility theory or a Bellman optimality principle these
two plans have the same expected utility of 50 percent but from the point of view of something
that is self-evidencing that has this epistemic part to the objective function the belief-based
part then clearly it is going to be better or it's the animal will plan and infer that it will
be pursuing an epistemic policy responding to the epistemic affordance of the instructional
cue and indeed when we build this paradigm into a Markov decision process write down the
factor graph iterate and solve the belief update equations that's exactly this kind of behavior
which we see so I'm just summarizing that here with the following graphic the top panel just
shows you the side in which the reward was placed this image representation here is of the plans that
the synthetic mouse committed to with a degree of uncertainty at certain times these are the outcomes
this is if you like the reward or negative loss function here and these reflect learning as there
updates to the initial state at the beginning of the plan namely the context is the reward on the
left or the right and what we see is at the beginning as we'd anticipate the the mouse goes and gets its
a response to his epistemic affordance and then goes to secure its reward and and then what we've
done is leave after the first couple of trials the reward on this side so as time goes on it learns
that the context is almost is all is increasingly likely to be reward on the left hand side
and this has the effect of reducing the epistemic or the intrinsic value the information gain the
epistemic affordance and relative to the extrinsic or more pragmatic part of the expected free energy
in other words the the the information gain afforded by the instructional condition conditions
conditions stimulus starts to decline and at a particular threshold falls below
the pragmatic or the extrinsic value at which point the most likely behavior is to go straight
to where the little mouse knows the reward is sitting and that's exactly what we see at a
Brown trial 20 here and the synthetic mouse goes straight to her reward on all occasions
so that's a basic illustration of the deterministic and systematic move from exploratory behavior to
exploitative behavior simply because there is this dual imperative to seek information
and then seek reward or to maintain certain goal states but of course as you become more
familiar with the environment after exploring it the novelty or the salience of that environment
starts to decrease and the more pragmatic imperatives start to emerge and what I'll do now
is to show you how one can generalize this kind of formulation to reproduce
quite sophisticated epistemic foraging of the kind that we might engage in and I'm going to use
reading as an example or a very simplified form of reading just to emphasize the interesting
sequelae of formulating optimal behavior where you're putting this sort of imperative to minimize
uncertainty into the objective function and how this uncertainty changes with time and with the
things that you decide to do to produce a particular pattern and scheduling of the way that we go and
gather information from the world so everything that follows I'm now taking rewards away so that
we can just reveal the epistemic nature of sentient behavior and I'm going to start with
exactly the same kind of Markov decision process as a generative model of outcomes
so this is a reproduction of an earlier slide with our Markov decision process here what I'm
going to do now is just make it slightly more interesting I'm going to add a temporal depth
I'm going to now put a Markov decision process on top of my first Markov decision process
and crucially the higher level Markov decision process is going to unfold more slowly in time
so this higher level process on the point of view of a generative process or model
provides now a context that sets the initial states for a lower process that unfolds more
quickly in time and then is reset for the next context in the next context so from the point
of view of the generative model we're now if you like placing priors on the kinds of behaviors
that would generate the most likely outcomes at the lower level from the point of view of the equivalent
normal style factor graph we can conversely regard the lower level as supplying evidence for which
context we are in so this dual exchange is implicit in the belief updating which is just a
solution to the belief updating equations on the variational message passing that emerges under
these deep diachronic generative models so the particular model that I want to use to illustrate
the behaviors at hand is a model of reading but a very simplified model where we're using
sort of iconic letters as opposed to a script so the lowest level of the model
is sketched out here and basically what this little agent can see is either a particular
icon a particular letter depending upon where she looks at different quadrants in this word here
and these outcomes the what and the where aspects of the sensed or observed outcomes
are generated by a number of hidden states first of all the hidden state what word is she looking at
is it the word flea which is composed of these two letters there's a little bird next to a cat
or is it the word feed there's a little bird texture some seed the bird could eat
or is it the word weight there's nothing next to the little bird so this describes the context
generating the particular outcomes at the first level so we need to know what was the letter
flea feed or weight we'd also need to know where she was looking at this quadrant or that quadrant
or this or that quadrant here and just to make things interesting I've also put a flip in here
which you can regard as a font change just to make it more realistic so that there's a degree
of generalization so given these states I can always generate a particular set of outcomes under
a particular set of moves moving from one to two or one to three or three to four and and that is
sufficient to write down a factor graph which is sufficient to define the message passing
and the self-evidencing what about the second level right well clearly the sequence of words
this you know the sentence that I can be generating is it's is itself something that I have to
specify from the point of view of the geometry model so I have to specify well what kinds of
sentences could possibly be generated and I've taken three sentences here there's also a what
where aspect to the the structure here so it's what sentence is in play that is generating
the words at the lower level or providing a contextual prior for the words at the lower level
and you know where am I in terms of the word and as part of the sequence as opposed to the
letter as a part of the word and I can write down these hidden states that I can generate
everything I need to know to tell me what the current word is and where I am looking in terms of
wandering through the sentence so I've got two levels of action I can look at the next word or
I can look around this current word various letters and accumulate information as I progress
through the letters and the words and thereby simulate reading and this is basically what it
looks like so on the top is a description of or a graphic illustrating the sequence of moves that
this little agent made whilst reading this four word sentence flee wait feed wait here
the interesting thing about it is that she starts off looking around
and accumulates evidence summarized here in terms of the expectations about what's happening
at the lower level and what's happening at the higher level so this is the second level expectation
about what centres she's reading and these are the first level expectations about the particular
word in plain and the interesting thing about this simulation is that as soon as she's certain
about the word because if there's a cat on the on the upper row then it has to be flee
she jumps to the next word and after a couple of sufficient samples to reassure her or render her
sufficiently certain that the next word is wait that she then jumps to the next word so what this
is evincing is exactly the kind of sparse sampling that we actually use not just in reading but in
terms of visually palpating the world in fact palpating the world with all different kinds of
sense organs just to get the information that we need in order to resolve our uncertainty about
where to look next to build confident certain beliefs about states of affairs out there
and this is basically what we wanted to show with this with this simulation. I just wanted to
conclude by coming back to the you know the really interesting issues about the temporal scheduling
of this kind of diachronic belief updating and foraging in this instance foraging for information
but if we put rewards in then it would be you know a mixture a blend of foraging for rewards and
goals and information. So what I've done here is to show the results of the simulated belief
updating in a way that an electrophysiologist might measure them in a real brain. So at the top
we have the six sentences and I just if you like color coded the weight or the expectation
that we are looking at the sentence one or sentence four here as time progresses during
the planned reading of this sentence. Similarly at the lower level here are the three alternative
words that could be seen at you know at each sorry the three alternative words that could be
being looked at as a root as as the agent is foraging each of the letters in that word
and the final panel here are essentially just the smoothed versions of these
rasters of if this was simulated electrophysiology neural activity that can be read as local field
potentials that we can measure empirically and notice that the speed at which we accumulate
information about the sentence is by construction of this diachronic deep generative model much
slower than the the accumulation of evidence for the word flea feed or weight. So we see a very
fast accumulation evidence so we're definitely looking at flea here as soon as we start and
then we quickly make up our mind about the subsequent words and then move on to the next word
and slowly accumulate evidence for the different sentences and there's an ambiguity here because
we have to wait until the last word to disambiguate between sentences one and four and indeed when
we get to the last word this little agent can indeed confidently believe that she has been
reading the sentence flea weight at feed and weight. I've shown the results like this because
this is exactly the kind of neuronal dynamics that one sees empirically when looking at this kind of
paradigm in monkey electrophysiology. So for example here these data or this simulation
very similar to the the data acquired from used electrode recordings in this instance
pre-secadic delay productivity in the prefrontal cortex that is very reminiscent of the activity
in this simulation. Similarly recording fluctuations in electrochemical potentials that are one way
of measuring the fluctuations in neural activity show a remarkable similarity between the synthetic
and real measurements of periscadic field potentials during active vision
in this instance in I think macaque monkey looking at higher and lower sorry lower and
higher areas of the visual brain. So I'm going to finish there because it provides a nice
empirical endorsement that the brain may actually be using this sort of message passing
in the service of sampling the world in the best way possible in in the sense of optimal
Bayesian design which has been understood and formalized for since the 1950s in terms of
