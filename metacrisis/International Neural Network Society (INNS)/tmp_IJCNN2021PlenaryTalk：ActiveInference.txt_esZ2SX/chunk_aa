Yeah, there. Hi, nice to meet you. Very, very nice. So I might start now. So I have the extreme
honor to introduce Professor Karl Friston, who is Professor for Neuroduty at University College
London, and he is Scientific Director of the Wellcome Test Center for Neuroimaging in London.
He is one of the leading experts for brain imaging. And in particular, he invented the key
technology, SPM, which is cited as a single paper more often than the publications or the
citations of others in their whole lifetime. So he is regarded as one of the top 10 most influential
neuroscientists. And I think, personally, that this research is a superb example of how formal
mathematical modeling and machine learning can help to understand key questions of mankind.
So how the brain works. Professor Friston is a recipient of the first Young Investigators Award
in Human Brain Wapping, Fellow of Real Society, Fellow of Academy of Medical Sciences, recipient of
Minerva Golden Brain Award, and actually the awards are so many that I would eat up your time.
Therefore, rather than elaborating on your CV, please allow me to give the word to you
and to learn more about active inference. I'm pleased to attend your talk and everyone else.
Thank you. Thank you.
It's a great pleasure to be here and I very much welcome the opportunity to speak to you today.
I'm going to talk about active inference and ask a question. What is optimal about behavior
and in particular, are there different kinds of optimality, specifically Bayesian optimality in
terms of those things that we aspire to, and the information gain consequent upon our sentient
behavior? My talk's going to have two parts to it. I'm going to, first of all, set up the framework
in terms of self-evidencing, namely action and perception in the service of maximizing the
evidence for our generative models of the world. I'm going to motivate that in terms of normative
accounts of sentient behavior and relate it specifically to things like artificial intelligence
and machine learning. The second part of the talk will use simulations to illustrate technically
the kind of message passing entailed by the first principal account of the first half of the talk.
I want to conclude with a special focus on epistemic foraging, responding to affordances that
denote information gain following a particular course of action or policy.
Self-evidencing active inference is predicated on the notion that there's one underlying
objective function such that we can read perception and action as a process of optimization.
So what are we optimizing? Effectively, we're optimizing the log probability of some outcomes
given a generative model, or more simply me, who would anticipate those outcomes. And clearly,
that's almost a tautological specification of the kinds of outcomes I expect to encounter.
So we can read this as those outcomes that have value. And we can, from that reading,
motivate a description in terms of reinforcement learning and engineering optimal control theory.
And if we were an economist, this would be something like expected utility theory.
What I will go on to unpack is an approximation, a bound approximation to this kind of value
that I'm going to refer to as a variational free energy or more simply free energy.
Interestingly, the negative of this free energy or value is self-information or
surprise or more simply surprise and information theory. So this provides a normative account
of sentient behavior in terms of things like the principle of maximum efficiency or the principle
of minimum redundancy, the informatics principle, and indeed the free energy principle itself.
That in turn is interesting because the long-term average of
surprise or surprise or self-information is entropy. So if we imagine that action and
perception are always trying to maximize value or minimize surprise, then on our average it will
look as if action and perception are also trying to minimize entropy. And that, of course, is a
holy grail of many accounts of behavior in terms of self-organization, particularly synergetics.
And of course, if I were a physiologist, this would just be a statement of homeostasis,
keeping physiological states within viable bounds, resisting the second law where those
physiological states would be dispersed and I would dissipate, dissolve and possibly die.
There's a final perspective on this quantity, which I'm going to leverage,
and that's a much more statistical perspective, namely the probability of outcomes given me as a
model. And in Bayesian statistics that will be known as Bayesian model evidence, and from that
we can link to accounts of brain function in terms of the Bayesian brain hypothesis,
evidence accumulation, perceptions, hypothesis testing, predictive coding, and so on. So those
are, if you like, some of the normative theories that can all be subsumed by putting in play
a particular objective function. And the form of the objective function that I'm going to consider
is a free energy that comprises a negative energy and entropy here. For those of you not
familiar with this, you can see this as essentially a statement of James's maximum entropy principle,
so that we want to effectively maximize the entropy of our beliefs about the states of the world
that are generating outcomes under certain constraints. And those constraints we will see
are described by the likelihood of those outcomes given those states, those usually hidden or latent
states of the world, and our prior beliefs about the states of affairs generating our observable
outcomes. So in machine learning, many people will be familiar with this as an evidence lower bound,
so I can unpack this free energy functional, its functional form, in terms of an expected log
evidence, simply the probability of some outcomes under a given model, plus a
Kullback-Niebler divergence that can never be less than zero, and that constitutes
a bound on the log evidence, giving me this notion of an elbow or an evidence lower bound.
That's, I think, an important perspective. Because I'm defining the evidence in relation
to a generative model, I have now, at the start, an explainable and interpretable
account of how any data that I'm trying to classify or use to infer the causes
generates those data. So basically, I should have for free explainable and interpretable
artificial intelligence. What we will see is that we'll also, for free, from this Bayesian
perspective, leverage a design optimality in terms of optimal experimental design,
which we can understand as augmenting an abductive inference, and indeed, if we take
that abduction through from inference to learning, auto-poetics, self-constructing,
self-teaching kind of inference and learning, which in principle would lead to a first
principal account of data foraging and epistemics, and possibly generalized AI or AGI.
I want to look at this objective function from a different perspective, simply by rearranging
the terms. So all I've done here is just move the terms around and expressed it in a way that
would be more familiar to statisticians. So this same objective function can be written
down as a mixture of accuracy and complexity, where the accuracy is simply the expected log
likelihood of some observable data, given their causes, minus the complexity, that is simply
the KL divergence between my posterior beliefs, Q, and some prior beliefs about the states
generating those data. So that's an intuitive view of this objective functional. The complexity
term plays a key role here. Many people will recognize this in terms of Occam's principle
in the brain, this translates into things like functional specialization and redundancy minimization,
efficient coding. From an engineering perspective, this complexity also can be understood in terms
of minimizing the computational cost, literally the degree to which I have to change my mind
in going from my prior beliefs to my posterior beliefs in order to provide an accurate account
of the data at hand. Implicit in this prize, of course, are parsimonious explanations and beliefs,
possibly heuristics, about the generation of the data that I'm trying to explain.
From the point of view of thermodynamics, that computational complexity translates directly
into a thermodynamic or in biology, a metabolic cost via things like the Czarzynski equality
and Landauers principle, which put it very simply means that if I'm doing it in the right kind of
way, I'm doing it very quickly, and I'm doing it with the minimum use of energy.
What I really want to do, though, is to take that sort of partition of the right kinds of
inference and actions in the moment and think about the free energy in the future
following a particular act upon the world. What we're going to see when we consider
self-evidence in the future is the emergence of, again, this dual perspective on an objective
function in terms of optimal Bayesian design that we will see effectively maximizes the accuracy
I expect following a move or a sampling of some data, while the expected complexity corresponds
to optimal Bayesian decision-making in relation to some prior preferences. The two together
constitute this self-evidencing so that the Bayes optimal design perspective and
Bayes optimal decisions are two aspects of the same thing, two sides of the same coin,
which is this underlying imperative to maximize this evidence lower bound.
To make that more heuristic, let me just pose a question to the audience. Imagine you're an owl
and you're hungry. If we were in person, I would actually ask someone in the audience to say,
what are you going to do next? Almost invariably, somebody correctly replies,
well, I'm going to look for my food. I'm going to search for my prey and then predate that prey.
That simple answer has a lot of implications for the choice of the account of sentient behavior.
If it were the case that I were simply choosing my actions in order to maximize some value function
of the states that would ensue, say s of t plus one, following a particular action,
I could formulate optimal behavior in terms of a state action policy. So I would be choosing
those behaviors or those control variables that maximized my value function, my state
action value function of the consequences of behaving and thereby elaborate an optimal policy.
However, the simple answer I'm going to search for food means that this may not be the best way
of formulating optimal behavior. So why is that? Well, to search is to resolve uncertainty about
where my food is, but uncertainty is an attribute of a belief. Therefore, that tells us that perhaps
the better kind of objective function or certainly its functional form is a functional in the sense
that it's a function of a function where that function is a belief. So put that simply,
optimal action may depend upon states of the world or on beliefs about states of the world.
So I'm, if you like, formalizing that in terms of actions that maximize a functional g of beliefs
about states of the world given a particular behavior. Furthermore, thinking about the searching
for food example, it matters the order in which I do things. So I can either try to eat my prey and
then search for it or search for it, identify where it is, and then try to eat it. And that
ordered aspect leads us into the notion of sequential policy optimization. And technically,
that has implications for the functional form of a normative account based upon an objective
functional. And I've formulated that here in terms of maximizing a path interval of a functional
of the beliefs about states of the world that would ensue under a particular policy,
where a policy here now is read as a sequence of actions or controls, which is distinct from
the state action policy here. And the reason that I'm trying to emphasize the difference
between these two things is to deliberately introduce a dialectic between accounts of
sentient behavior based on Bellman's optimality principle versus this path interval of an energy
functional, which in physics would be known as an action. So what we're talking about basically
is a principle of least action that accounts for optimal action in terms of beliefs about states
versus an optimality principle that does not. And many of you will recognize the different
themes and schemes that arise under these two complementary approaches to optimizing behavior.
So for example, under Bellman's optimality principle, we'd have optimal control theory,
dynamic processing, deep RL, Bayesian RL, Bayesian decision theory, state action policy
iterations, and so on. A slightly different perspective emerges under a principle of least action,
including active inference, and probably more interestingly, artificial curiosity and in
robotics as we know as intrinsic motivation. I'm going to cast that in terms of optimal Bayesian
design, that for free becomes part of sequential policy optimization and the treatment of partially
observed Markov decision processes. So what is this free energy and it's associated functional
that underwrites a principle of least action for a physics of sentience, a Bayesian mechanics that
explains our best accounts for optimal behavior. Well, here I've just rewritten the free energy
functional function, my apologies, in terms of accuracy and complexity. So this is the KL
divergence we've already spoken about, and the accuracy, the log likelihood of outcomes given
states. And here just by moving these terms around, we can see immediately how this also
translates into an evidence bound in the sense that this is another non negative KL divergence.
And this is the the evidence associated with particular outcomes. The reason I've written
this out is to disclose, to show what I think is quite a beautiful mapping between this quantity
and the same quantity that would be expected, given I have committed to a particular act.
Now, the reason that the expected gets into the game, and we move from F to G, is that before I
have acted, I don't have the sensory outcomes available. So now I have to take an expectation
of the free energy under my posterior predicted beliefs about what I would observe if I made that
move. And what one sees is that the expected complexity translates into something called
risk, the expected accuracy becomes something called a negative expected accuracy becomes
ambiguity, while the evidence bound and log evidence can be read as an intrinsic and extrinsic
value respectively. So let me just try and relate that to well established normative accounts of
behavior. First of all, let's just focus on the intrinsic value and pretend we had no
prior beliefs about the outcomes that characterize me, sometimes referred to as prior preferences.
So let's ignore the extrinsic value, the prior preferences, and see what is left. And what is
left is effectively an expected divergence between beliefs about states of the world
before and after observing their consequences. And this is just the degree to which my uncertainty
has been resolved by committing to this particular policy. Another way of expressing that is the
information gain or the epistemic affordance due to this policy. In the visual search literature,
that's known as Bayesian surprise, mathematically, it's just the mutual information between the
states of the world causes and their consequences, their observable consequences in the future
conditioned upon a particular policy. What I'm going to do now is take away a particular
kind of uncertainty and remove it from the table. And the uncertainty I'm talking about is the
ambiguity. So if I remove ambiguity and pretend that I can see directly all these hidden states,
these latent states generating data, then I can ignore this and we just are left with the risk
term. So what's the risk term? Well, that's another KL divergence. And it just scores the
difference between beliefs about states in the future, given that I have committed to this policy
and my prior preferences. So it's just the difference between what I anticipate will happen
if I do this versus the kinds of outcomes that I would prefer or expect myself to encounter.
And in engineering, this is just KL control in economics, it's risk sensitive because we are
no longer dealing with ambiguity. So let me make a final move here and take away the risk
part in the sense that there is no relative uncertainty about the outcomes of my behavior.
And what are we left with? Well, we're left with this term here, which is back to this
value term in the first review of normative theories, the extrinsic value that reflects
my prior preferences about the kinds of observations I expect to solicit irrespective
of how I behave. And from this we can interpret optimal behavior in terms of an expected value,
for example, in economics expected utility theory. So there's an interesting relationship between
this account of optimal behavior and the Bellman optimality account of optimal behavior
that arises when I successively remove different kinds of uncertainty from the problem. And in fact,
we can now repair that dialectic between the Bellman optimality principle and the principle
of least action simply by saying that the latter is a special case of the principle of least action
when there's no uncertainty to worry about. The ensuing computational architecture that
we now use to simulate and understand sensitive behavior in the neurosciences and in our simulations
of active inference is shown on this slide. So in brief we can think of outcomes being
gathered by our sensory organs being taken into the brain, for example, to be assimilated,
to build posterior or Bayesian beliefs that maximize this evidence lower bound about hidden
states, external states beyond technically what is a Markov blanket separating the external states
from internal brain states to optimize our beliefs to get the best approximation or best guess about
the states generating those outcomes. And then we use our beliefs about the external latent states
to evaluate well what would happen if I did that in terms of the expected tree energy and we've seen
that we can unpack that or carve it into risk and ambiguity terms and then form beliefs about
the most likely policies that we are pursuing and then we can suffer an action from those policies
generate a new observation from the world out there and the cycle the action perception
