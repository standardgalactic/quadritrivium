Yeah, there. Hi, nice to meet you. Very, very nice. So I might start now. So I have the extreme
honor to introduce Professor Karl Friston, who is Professor for Neuroduty at University College
London, and he is Scientific Director of the Wellcome Test Center for Neuroimaging in London.
He is one of the leading experts for brain imaging. And in particular, he invented the key
technology, SPM, which is cited as a single paper more often than the publications or the
citations of others in their whole lifetime. So he is regarded as one of the top 10 most influential
neuroscientists. And I think, personally, that this research is a superb example of how formal
mathematical modeling and machine learning can help to understand key questions of mankind.
So how the brain works. Professor Friston is a recipient of the first Young Investigators Award
in Human Brain Wapping, Fellow of Real Society, Fellow of Academy of Medical Sciences, recipient of
Minerva Golden Brain Award, and actually the awards are so many that I would eat up your time.
Therefore, rather than elaborating on your CV, please allow me to give the word to you
and to learn more about active inference. I'm pleased to attend your talk and everyone else.
Thank you. Thank you.
It's a great pleasure to be here and I very much welcome the opportunity to speak to you today.
I'm going to talk about active inference and ask a question. What is optimal about behavior
and in particular, are there different kinds of optimality, specifically Bayesian optimality in
terms of those things that we aspire to, and the information gain consequent upon our sentient
behavior? My talk's going to have two parts to it. I'm going to, first of all, set up the framework
in terms of self-evidencing, namely action and perception in the service of maximizing the
evidence for our generative models of the world. I'm going to motivate that in terms of normative
accounts of sentient behavior and relate it specifically to things like artificial intelligence
and machine learning. The second part of the talk will use simulations to illustrate technically
the kind of message passing entailed by the first principal account of the first half of the talk.
I want to conclude with a special focus on epistemic foraging, responding to affordances that
denote information gain following a particular course of action or policy.
Self-evidencing active inference is predicated on the notion that there's one underlying
objective function such that we can read perception and action as a process of optimization.
So what are we optimizing? Effectively, we're optimizing the log probability of some outcomes
given a generative model, or more simply me, who would anticipate those outcomes. And clearly,
that's almost a tautological specification of the kinds of outcomes I expect to encounter.
So we can read this as those outcomes that have value. And we can, from that reading,
motivate a description in terms of reinforcement learning and engineering optimal control theory.
And if we were an economist, this would be something like expected utility theory.
What I will go on to unpack is an approximation, a bound approximation to this kind of value
that I'm going to refer to as a variational free energy or more simply free energy.
Interestingly, the negative of this free energy or value is self-information or
surprise or more simply surprise and information theory. So this provides a normative account
of sentient behavior in terms of things like the principle of maximum efficiency or the principle
of minimum redundancy, the informatics principle, and indeed the free energy principle itself.
That in turn is interesting because the long-term average of
surprise or surprise or self-information is entropy. So if we imagine that action and
perception are always trying to maximize value or minimize surprise, then on our average it will
look as if action and perception are also trying to minimize entropy. And that, of course, is a
holy grail of many accounts of behavior in terms of self-organization, particularly synergetics.
And of course, if I were a physiologist, this would just be a statement of homeostasis,
keeping physiological states within viable bounds, resisting the second law where those
physiological states would be dispersed and I would dissipate, dissolve and possibly die.
There's a final perspective on this quantity, which I'm going to leverage,
and that's a much more statistical perspective, namely the probability of outcomes given me as a
model. And in Bayesian statistics that will be known as Bayesian model evidence, and from that
we can link to accounts of brain function in terms of the Bayesian brain hypothesis,
evidence accumulation, perceptions, hypothesis testing, predictive coding, and so on. So those
are, if you like, some of the normative theories that can all be subsumed by putting in play
a particular objective function. And the form of the objective function that I'm going to consider
is a free energy that comprises a negative energy and entropy here. For those of you not
familiar with this, you can see this as essentially a statement of James's maximum entropy principle,
so that we want to effectively maximize the entropy of our beliefs about the states of the world
that are generating outcomes under certain constraints. And those constraints we will see
are described by the likelihood of those outcomes given those states, those usually hidden or latent
states of the world, and our prior beliefs about the states of affairs generating our observable
outcomes. So in machine learning, many people will be familiar with this as an evidence lower bound,
so I can unpack this free energy functional, its functional form, in terms of an expected log
evidence, simply the probability of some outcomes under a given model, plus a
Kullback-Niebler divergence that can never be less than zero, and that constitutes
a bound on the log evidence, giving me this notion of an elbow or an evidence lower bound.
That's, I think, an important perspective. Because I'm defining the evidence in relation
to a generative model, I have now, at the start, an explainable and interpretable
account of how any data that I'm trying to classify or use to infer the causes
generates those data. So basically, I should have for free explainable and interpretable
artificial intelligence. What we will see is that we'll also, for free, from this Bayesian
perspective, leverage a design optimality in terms of optimal experimental design,
which we can understand as augmenting an abductive inference, and indeed, if we take
that abduction through from inference to learning, auto-poetics, self-constructing,
self-teaching kind of inference and learning, which in principle would lead to a first
principal account of data foraging and epistemics, and possibly generalized AI or AGI.
I want to look at this objective function from a different perspective, simply by rearranging
the terms. So all I've done here is just move the terms around and expressed it in a way that
would be more familiar to statisticians. So this same objective function can be written
down as a mixture of accuracy and complexity, where the accuracy is simply the expected log
likelihood of some observable data, given their causes, minus the complexity, that is simply
the KL divergence between my posterior beliefs, Q, and some prior beliefs about the states
generating those data. So that's an intuitive view of this objective functional. The complexity
term plays a key role here. Many people will recognize this in terms of Occam's principle
in the brain, this translates into things like functional specialization and redundancy minimization,
efficient coding. From an engineering perspective, this complexity also can be understood in terms
of minimizing the computational cost, literally the degree to which I have to change my mind
in going from my prior beliefs to my posterior beliefs in order to provide an accurate account
of the data at hand. Implicit in this prize, of course, are parsimonious explanations and beliefs,
possibly heuristics, about the generation of the data that I'm trying to explain.
From the point of view of thermodynamics, that computational complexity translates directly
into a thermodynamic or in biology, a metabolic cost via things like the Czarzynski equality
and Landauers principle, which put it very simply means that if I'm doing it in the right kind of
way, I'm doing it very quickly, and I'm doing it with the minimum use of energy.
What I really want to do, though, is to take that sort of partition of the right kinds of
inference and actions in the moment and think about the free energy in the future
following a particular act upon the world. What we're going to see when we consider
self-evidence in the future is the emergence of, again, this dual perspective on an objective
function in terms of optimal Bayesian design that we will see effectively maximizes the accuracy
I expect following a move or a sampling of some data, while the expected complexity corresponds
to optimal Bayesian decision-making in relation to some prior preferences. The two together
constitute this self-evidencing so that the Bayes optimal design perspective and
Bayes optimal decisions are two aspects of the same thing, two sides of the same coin,
which is this underlying imperative to maximize this evidence lower bound.
To make that more heuristic, let me just pose a question to the audience. Imagine you're an owl
and you're hungry. If we were in person, I would actually ask someone in the audience to say,
what are you going to do next? Almost invariably, somebody correctly replies,
well, I'm going to look for my food. I'm going to search for my prey and then predate that prey.
That simple answer has a lot of implications for the choice of the account of sentient behavior.
If it were the case that I were simply choosing my actions in order to maximize some value function
of the states that would ensue, say s of t plus one, following a particular action,
I could formulate optimal behavior in terms of a state action policy. So I would be choosing
those behaviors or those control variables that maximized my value function, my state
action value function of the consequences of behaving and thereby elaborate an optimal policy.
However, the simple answer I'm going to search for food means that this may not be the best way
of formulating optimal behavior. So why is that? Well, to search is to resolve uncertainty about
where my food is, but uncertainty is an attribute of a belief. Therefore, that tells us that perhaps
the better kind of objective function or certainly its functional form is a functional in the sense
that it's a function of a function where that function is a belief. So put that simply,
optimal action may depend upon states of the world or on beliefs about states of the world.
So I'm, if you like, formalizing that in terms of actions that maximize a functional g of beliefs
about states of the world given a particular behavior. Furthermore, thinking about the searching
for food example, it matters the order in which I do things. So I can either try to eat my prey and
then search for it or search for it, identify where it is, and then try to eat it. And that
ordered aspect leads us into the notion of sequential policy optimization. And technically,
that has implications for the functional form of a normative account based upon an objective
functional. And I've formulated that here in terms of maximizing a path interval of a functional
of the beliefs about states of the world that would ensue under a particular policy,
where a policy here now is read as a sequence of actions or controls, which is distinct from
the state action policy here. And the reason that I'm trying to emphasize the difference
between these two things is to deliberately introduce a dialectic between accounts of
sentient behavior based on Bellman's optimality principle versus this path interval of an energy
functional, which in physics would be known as an action. So what we're talking about basically
is a principle of least action that accounts for optimal action in terms of beliefs about states
versus an optimality principle that does not. And many of you will recognize the different
themes and schemes that arise under these two complementary approaches to optimizing behavior.
So for example, under Bellman's optimality principle, we'd have optimal control theory,
dynamic processing, deep RL, Bayesian RL, Bayesian decision theory, state action policy
iterations, and so on. A slightly different perspective emerges under a principle of least action,
including active inference, and probably more interestingly, artificial curiosity and in
robotics as we know as intrinsic motivation. I'm going to cast that in terms of optimal Bayesian
design, that for free becomes part of sequential policy optimization and the treatment of partially
observed Markov decision processes. So what is this free energy and it's associated functional
that underwrites a principle of least action for a physics of sentience, a Bayesian mechanics that
explains our best accounts for optimal behavior. Well, here I've just rewritten the free energy
functional function, my apologies, in terms of accuracy and complexity. So this is the KL
divergence we've already spoken about, and the accuracy, the log likelihood of outcomes given
states. And here just by moving these terms around, we can see immediately how this also
translates into an evidence bound in the sense that this is another non negative KL divergence.
And this is the the evidence associated with particular outcomes. The reason I've written
this out is to disclose, to show what I think is quite a beautiful mapping between this quantity
and the same quantity that would be expected, given I have committed to a particular act.
Now, the reason that the expected gets into the game, and we move from F to G, is that before I
have acted, I don't have the sensory outcomes available. So now I have to take an expectation
of the free energy under my posterior predicted beliefs about what I would observe if I made that
move. And what one sees is that the expected complexity translates into something called
risk, the expected accuracy becomes something called a negative expected accuracy becomes
ambiguity, while the evidence bound and log evidence can be read as an intrinsic and extrinsic
value respectively. So let me just try and relate that to well established normative accounts of
behavior. First of all, let's just focus on the intrinsic value and pretend we had no
prior beliefs about the outcomes that characterize me, sometimes referred to as prior preferences.
So let's ignore the extrinsic value, the prior preferences, and see what is left. And what is
left is effectively an expected divergence between beliefs about states of the world
before and after observing their consequences. And this is just the degree to which my uncertainty
has been resolved by committing to this particular policy. Another way of expressing that is the
information gain or the epistemic affordance due to this policy. In the visual search literature,
that's known as Bayesian surprise, mathematically, it's just the mutual information between the
states of the world causes and their consequences, their observable consequences in the future
conditioned upon a particular policy. What I'm going to do now is take away a particular
kind of uncertainty and remove it from the table. And the uncertainty I'm talking about is the
ambiguity. So if I remove ambiguity and pretend that I can see directly all these hidden states,
these latent states generating data, then I can ignore this and we just are left with the risk
term. So what's the risk term? Well, that's another KL divergence. And it just scores the
difference between beliefs about states in the future, given that I have committed to this policy
and my prior preferences. So it's just the difference between what I anticipate will happen
if I do this versus the kinds of outcomes that I would prefer or expect myself to encounter.
And in engineering, this is just KL control in economics, it's risk sensitive because we are
no longer dealing with ambiguity. So let me make a final move here and take away the risk
part in the sense that there is no relative uncertainty about the outcomes of my behavior.
And what are we left with? Well, we're left with this term here, which is back to this
value term in the first review of normative theories, the extrinsic value that reflects
my prior preferences about the kinds of observations I expect to solicit irrespective
of how I behave. And from this we can interpret optimal behavior in terms of an expected value,
for example, in economics expected utility theory. So there's an interesting relationship between
this account of optimal behavior and the Bellman optimality account of optimal behavior
that arises when I successively remove different kinds of uncertainty from the problem. And in fact,
we can now repair that dialectic between the Bellman optimality principle and the principle
of least action simply by saying that the latter is a special case of the principle of least action
when there's no uncertainty to worry about. The ensuing computational architecture that
we now use to simulate and understand sensitive behavior in the neurosciences and in our simulations
of active inference is shown on this slide. So in brief we can think of outcomes being
gathered by our sensory organs being taken into the brain, for example, to be assimilated,
to build posterior or Bayesian beliefs that maximize this evidence lower bound about hidden
states, external states beyond technically what is a Markov blanket separating the external states
from internal brain states to optimize our beliefs to get the best approximation or best guess about
the states generating those outcomes. And then we use our beliefs about the external latent states
to evaluate well what would happen if I did that in terms of the expected tree energy and we've seen
that we can unpack that or carve it into risk and ambiguity terms and then form beliefs about
the most likely policies that we are pursuing and then we can suffer an action from those policies
generate a new observation from the world out there and the cycle the action perception
cycle can continue. So in short policy selection can be cast as maximization of an expected free
energy whereas perceptual inference can be thought of as maximizing this evidence lower bound.
So all I've said is essentially we have at hand a variational principle of this action
that subsumes both the epistemic information part of optimal behavior and that which can
be ascribed to the extrinsic or the expected value under some prior preferences and together
they simply mean that we are choosing those actions inferring the plans the policies that
maximize expected free energy. And this is very close of course to things notions
like planning and control as inference but in a way that has this dual aspect of
of both trying to be information seeking at the same time being reward seeking or
goal seeking in our epistemic responses. So let me briefly illustrate what implications
that formulation that sort of least action formulation has for belief updating in the
brain and subsequent behavior. I've written this down formally in a way that I thought might be
revealing for people in computer science after designing message passing schemes. What this slide
is meant to show is that if I had a generative model of how hidden states, unobservable latent
states out there generate outcomes then I can always write it down as a probabilistic graphical
model. So for example we could assume a discrete state space model a hidden mark of model or a
mark of decision process partially observed of the following form where the world is in different
states one of a number of different states and they move from one state to the next point in time
in accord with a probability transition matrix that itself depends upon the particular sequence
of actions that I prosecute the plan that itself depends upon the expected free energy
and once I know the states of the world I can then generate through a likelihood mapping here
denoted by the A matrix some observable outcomes. So this would be a generative model for a partially
observed mark off decision process that is inherently inactive in the sense that the state
transitions depend upon my action which itself depends upon the expected free energy and you
know please ignore these equations here they're just articulating this example of a generic or
universal example of discrete state space models of the way that my observable outcomes my sensory
information could be generated. This is nice because if I can write down a probabilistic
graphical model there will always exist a factor graph and in this instance a
phony or normal style factor graph that is the cousin or the conjugate of this graphical model
here where the nodes become edges and the edges become nodes we don't need to worry about that
although we need to know is if we can write this down we can always write this down what
does writing this down mean well it entails a very precise a concrete and crisp description
of the message passing that would be needed in order to infer all the unknowns here basically
states the world that I don't know and what I am doing the policies. I can infer all of these by
passing messages around over these edges that now correspond to the run the unknowns the random
variables given the parameterised generative model in terms of the priors the probability
transition matrices and the likelihoods named in the likelihood mappings between causes and
consequences and I've just written down this message passing here in terms of belief updating
belief updating of things that we don't know states of the world under a particular policy and the
policy which is just a softmax function of the expected free energy here and the expressions
that inherit from the the form the functional forms of the previous slides written down in
terms of linear algebra and the matrices and tensors of the generative model here.
Those updates which are just off the shelf a variational message passing updates on the previous
slide are quite remarkable not just in terms of their simplicity but the isomorphism with the
kind of message passing we actually see in the brain so I've just written them out again here
just to highlight how similar they are to things that we already use to simulate neuronal dynamics
and message passing on neuronal networks so for example here the belief updates for hidden states
the world under a particular policy any particular point in time are just a non-linear function of
a linear mixture of outcomes through the likelihood matrix and beliefs or expectations about past
states and future states that are mixed together in the right kind of way to give me the best
expectation or guess about the state of the world at any particular time. Policy selection is this
standard softmax function softmax response function of this expected free energy I've
equipped this model with a precision parameter which we don't need to worry about too much for
the point of view of this talk it's interesting in neurobiology because it seems to behave very
much like a reward prediction error of the kind that people who studied opening responses in the
brain like to think about. Learning just becomes effectively optimizing the parameters of the model
the D here are the initial states at the beginning of a plan or a policy but the same
functional form applies to all the parameters and it looks exactly like Hebbin or associated
plasticity evidence accumulation in this simplest example here and then action selection is just
selecting the most likely action from my inferred or favorite plan. This is a slide that just
describes a simple setup a toy example I'm going to use to illustrate the generic kinds of behavior
that ensue when we solve those belief update equations or that variational message passing
on the previous slide. So this is a simple an example as we could find that has both the sort
of pragmatic and epistemic bits that speak to the intrinsic and the extrinsic value
entailed by the expected free energy. The idea here is that there is a little mouse
it can make two moves in this T maze and it wants to find a red reward that can either be in the
left or the right upper arm of the maze but in addition it has the option of going to the lower
arm where there's an instructional cue that will tell it where the reward is so you can't see where
the reward is from its starting point in the center so it's got a choice it can either take a gamble
and 50 percent of the time be correct and 50 percent of the time be wrong but I should say that once
it goes into one of the upper arms it has to stay there these are absorbing states so there's a 50-50
chance that it will secure its reward for two moves or not or it can choose to go and solicit
the information, garner the information from the instructional cue but it's wasting one move
but it then knows with a fair degree of certainty where to go to the baited arm to get its reward.
So from the point of view of an expected utility theory or a Bellman optimality principle these
two plans have the same expected utility of 50 percent but from the point of view of something
that is self-evidencing that has this epistemic part to the objective function the belief-based
part then clearly it is going to be better or it's the animal will plan and infer that it will
be pursuing an epistemic policy responding to the epistemic affordance of the instructional
cue and indeed when we build this paradigm into a Markov decision process write down the
factor graph iterate and solve the belief update equations that's exactly this kind of behavior
which we see so I'm just summarizing that here with the following graphic the top panel just
shows you the side in which the reward was placed this image representation here is of the plans that
the synthetic mouse committed to with a degree of uncertainty at certain times these are the outcomes
this is if you like the reward or negative loss function here and these reflect learning as there
updates to the initial state at the beginning of the plan namely the context is the reward on the
left or the right and what we see is at the beginning as we'd anticipate the the mouse goes and gets its
a response to his epistemic affordance and then goes to secure its reward and and then what we've
done is leave after the first couple of trials the reward on this side so as time goes on it learns
that the context is almost is all is increasingly likely to be reward on the left hand side
and this has the effect of reducing the epistemic or the intrinsic value the information gain the
epistemic affordance and relative to the extrinsic or more pragmatic part of the expected free energy
in other words the the the information gain afforded by the instructional condition conditions
conditions stimulus starts to decline and at a particular threshold falls below
the pragmatic or the extrinsic value at which point the most likely behavior is to go straight
to where the little mouse knows the reward is sitting and that's exactly what we see at a
Brown trial 20 here and the synthetic mouse goes straight to her reward on all occasions
so that's a basic illustration of the deterministic and systematic move from exploratory behavior to
exploitative behavior simply because there is this dual imperative to seek information
and then seek reward or to maintain certain goal states but of course as you become more
familiar with the environment after exploring it the novelty or the salience of that environment
starts to decrease and the more pragmatic imperatives start to emerge and what I'll do now
is to show you how one can generalize this kind of formulation to reproduce
quite sophisticated epistemic foraging of the kind that we might engage in and I'm going to use
reading as an example or a very simplified form of reading just to emphasize the interesting
sequelae of formulating optimal behavior where you're putting this sort of imperative to minimize
uncertainty into the objective function and how this uncertainty changes with time and with the
things that you decide to do to produce a particular pattern and scheduling of the way that we go and
gather information from the world so everything that follows I'm now taking rewards away so that
we can just reveal the epistemic nature of sentient behavior and I'm going to start with
exactly the same kind of Markov decision process as a generative model of outcomes
so this is a reproduction of an earlier slide with our Markov decision process here what I'm
going to do now is just make it slightly more interesting I'm going to add a temporal depth
I'm going to now put a Markov decision process on top of my first Markov decision process
and crucially the higher level Markov decision process is going to unfold more slowly in time
so this higher level process on the point of view of a generative process or model
provides now a context that sets the initial states for a lower process that unfolds more
quickly in time and then is reset for the next context in the next context so from the point
of view of the generative model we're now if you like placing priors on the kinds of behaviors
that would generate the most likely outcomes at the lower level from the point of view of the equivalent
normal style factor graph we can conversely regard the lower level as supplying evidence for which
context we are in so this dual exchange is implicit in the belief updating which is just a
solution to the belief updating equations on the variational message passing that emerges under
these deep diachronic generative models so the particular model that I want to use to illustrate
the behaviors at hand is a model of reading but a very simplified model where we're using
sort of iconic letters as opposed to a script so the lowest level of the model
is sketched out here and basically what this little agent can see is either a particular
icon a particular letter depending upon where she looks at different quadrants in this word here
and these outcomes the what and the where aspects of the sensed or observed outcomes
are generated by a number of hidden states first of all the hidden state what word is she looking at
is it the word flea which is composed of these two letters there's a little bird next to a cat
or is it the word feed there's a little bird texture some seed the bird could eat
or is it the word weight there's nothing next to the little bird so this describes the context
generating the particular outcomes at the first level so we need to know what was the letter
flea feed or weight we'd also need to know where she was looking at this quadrant or that quadrant
or this or that quadrant here and just to make things interesting I've also put a flip in here
which you can regard as a font change just to make it more realistic so that there's a degree
of generalization so given these states I can always generate a particular set of outcomes under
a particular set of moves moving from one to two or one to three or three to four and and that is
sufficient to write down a factor graph which is sufficient to define the message passing
and the self-evidencing what about the second level right well clearly the sequence of words
this you know the sentence that I can be generating is it's is itself something that I have to
specify from the point of view of the geometry model so I have to specify well what kinds of
sentences could possibly be generated and I've taken three sentences here there's also a what
where aspect to the the structure here so it's what sentence is in play that is generating
the words at the lower level or providing a contextual prior for the words at the lower level
and you know where am I in terms of the word and as part of the sequence as opposed to the
letter as a part of the word and I can write down these hidden states that I can generate
everything I need to know to tell me what the current word is and where I am looking in terms of
wandering through the sentence so I've got two levels of action I can look at the next word or
I can look around this current word various letters and accumulate information as I progress
through the letters and the words and thereby simulate reading and this is basically what it
looks like so on the top is a description of or a graphic illustrating the sequence of moves that
this little agent made whilst reading this four word sentence flee wait feed wait here
the interesting thing about it is that she starts off looking around
and accumulates evidence summarized here in terms of the expectations about what's happening
at the lower level and what's happening at the higher level so this is the second level expectation
about what centres she's reading and these are the first level expectations about the particular
word in plain and the interesting thing about this simulation is that as soon as she's certain
about the word because if there's a cat on the on the upper row then it has to be flee
she jumps to the next word and after a couple of sufficient samples to reassure her or render her
sufficiently certain that the next word is wait that she then jumps to the next word so what this
is evincing is exactly the kind of sparse sampling that we actually use not just in reading but in
terms of visually palpating the world in fact palpating the world with all different kinds of
sense organs just to get the information that we need in order to resolve our uncertainty about
where to look next to build confident certain beliefs about states of affairs out there
and this is basically what we wanted to show with this with this simulation. I just wanted to
conclude by coming back to the you know the really interesting issues about the temporal scheduling
of this kind of diachronic belief updating and foraging in this instance foraging for information
but if we put rewards in then it would be you know a mixture a blend of foraging for rewards and
goals and information. So what I've done here is to show the results of the simulated belief
updating in a way that an electrophysiologist might measure them in a real brain. So at the top
we have the six sentences and I just if you like color coded the weight or the expectation
that we are looking at the sentence one or sentence four here as time progresses during
the planned reading of this sentence. Similarly at the lower level here are the three alternative
words that could be seen at you know at each sorry the three alternative words that could be
being looked at as a root as as the agent is foraging each of the letters in that word
and the final panel here are essentially just the smoothed versions of these
rasters of if this was simulated electrophysiology neural activity that can be read as local field
potentials that we can measure empirically and notice that the speed at which we accumulate
information about the sentence is by construction of this diachronic deep generative model much
slower than the the accumulation of evidence for the word flea feed or weight. So we see a very
fast accumulation evidence so we're definitely looking at flea here as soon as we start and
then we quickly make up our mind about the subsequent words and then move on to the next word
and slowly accumulate evidence for the different sentences and there's an ambiguity here because
we have to wait until the last word to disambiguate between sentences one and four and indeed when
we get to the last word this little agent can indeed confidently believe that she has been
reading the sentence flea weight at feed and weight. I've shown the results like this because
this is exactly the kind of neuronal dynamics that one sees empirically when looking at this kind of
paradigm in monkey electrophysiology. So for example here these data or this simulation
very similar to the the data acquired from used electrode recordings in this instance
pre-secadic delay productivity in the prefrontal cortex that is very reminiscent of the activity
in this simulation. Similarly recording fluctuations in electrochemical potentials that are one way
of measuring the fluctuations in neural activity show a remarkable similarity between the synthetic
and real measurements of periscadic field potentials during active vision
in this instance in I think macaque monkey looking at higher and lower sorry lower and
higher areas of the visual brain. So I'm going to finish there because it provides a nice
empirical endorsement that the brain may actually be using this sort of message passing
in the service of sampling the world in the best way possible in in the sense of optimal
Bayesian design which has been understood and formalized for since the 1950s in terms of
planning the best way to experiment to disclose those data that resolve the most uncertainty
about your hypothesis. So this this notion of Bayes optimality in terms of the principle of best
experimental design is one way of reading the epistemic or the intrinsic motivation supplied
by the expected free energy and in that sense you can regard perception and active sensing active
perception in exactly the same way that people like Richard Gregory conceive of active sensing
perception as hypothesis testing that really our behavior our sentient behavior is in the
in the game of getting the right kind of information that resolves our uncertainty about
states and affairs beyond our sensory organs or outside of our skull. I'm going to give the
final word to Einstein just to remember that all of this rests upon this simple understanding
and realization that the evidence for our models of the world can always be split into
negative complexity or simplicity and accuracy which just means not only in science but also
in our sentience everything should be made as simple as possible but not simpler and with that
all that remains is for me to thank all the people whose ideas I've been talking about and
to thank you for your attention and I look forward to discussing these these issues with you in the
near future. Thank you very much indeed. Thank you very much so now you have to imagine that
everyone is clapping it was a wonderful talk thank you very much. I would like to ask everyone who
has question to to put those in the chat and I take the opportunity to start with the first
question so you have this wonderful deep diachronic model would that also be a possibility to easily
model drift and changes in in in in the environment say if you have for example seasonal changes
and people or persons are very very very good at adapting to that so would you could you do that?
Yes that's a very interesting question in relation to modeling the current epidemic which is we've
actually applied these models to the coronavirus outbreak that's a very very pertinent question
yeah absolutely so I think what you're saying there is that there are there is a separation of
time scales at many many levels so it just keeps on going so what you would do in in this particular
example is just put another Markov decision process there was an order of magnitude slower on top of
the two levels that we already got and this would provide a much slower context so there will be
belief updating adaptation at a much much slower time scale. You know interesting aspect of your
question of course is what we were looking at there is the fast belief updating inferring latent
hidden states of the world the same expected free energy and free energy minimizing sorry
maximizing elbow maximizing dynamics and apply to the parameters of the model give you learning
so there's also inference and learning and the two of those go hand in hand as well
so that's another interesting aspect and then you can even take that further and say can you put
structure learning on top by using the time integral of the model evidence so now you can have a
picture of structure learning how many levels would you want that can be read as Bayesian model
selection or natural selection if you're in biology. Very interesting so we have one question in the
text from Pranav Mahayan I might summarize the first question so these are actually two first
would be whether you could also model something like Pavlovian fear and risk aversion principles
so how can this reward seeking and information gain seeking also account for risk aversion schemes.
I think it to articulate risk aversion and risk sensitivity within this framework is relatively
straightforward so all you do is you rewrite your cost function or your reward function as a log
probability so I was talking about sort of prior preferences so you just take the logarithm of that
and then you have a continuous variable that goes from plus to minus infinity that scores your
the value or the extrinsic value of a particular outcome once you've done that
then you get all these sort of natural behaviors that one would see in reinforcement learning
with the added benefit that now you can measure your reward or your utility in natural units because
now you've expressed your utility as a log probability so if you use natural logarithms you've
got natural units which interestingly have the same units as information so now you've got the
value of information you can either say well this bit of information is worth three dollars
or three dollars is worth this amount of information and the way that you balance the two
is effectively by ascribing a precision an inverse variance to to those belief distributions so
once you've written down your reward in terms of probabilistic preferences now you can make them
quantitatively bigger or smaller by making them more or less precise so if I'm very very confident
a very precise belief I like these things that I don't like those that will dominate the
pragmatic or the expected utility part of the expected free energy and that will
suppress possibly the risk aspect whereas if I have very very imprecise I don't have any
confident beliefs about what I find rewarding my subjective utility if you like then it's likely
that I might become more sensitized to the to you know to the the ambiguity term and I might
become more careful I want to resolve ambiguity and be less sensitive to the or act as if I was
less worried about the risks because I'm less confident about what I find rewarding so a
really interesting question you know in principle you should be able to fit the precisions in order
to make them match exactly with economic sort of forms of risk aversion and and the like
there's another question in the chat so this would be the question about your views on including
or how including emotional balance into the framework and so suggestion would be whether
first and second derivative of free energy would fit that or what would be your view on that
well that's a challenging question and certainly people have
have proposed for many years now that the sort of you know the rate of change of free energy
as a score in terms of how well you are engaging with your world is is a nice valenced measure
of that could predict your your emotional status and I think that's a useful heuristic
however I think that the you know the the more direct answer to this is
it's all in the uncertainty you know if you estimate and you predict that you are now in
a context of an environment in a situation where your uncertainty about what to do is irreducible
that's going to make you feel anxious and bad so you know the good feelings the aha moments the
realization oh yes I've won that lottery or it's it's it's dinner time or it's my favorite sitcom
all of these things when you feel good and happy all come along with this collapse of
uncertainty and the way forward becomes very very clear so in a sense that is in fact an
anticipation of a you're an improvement in the free energy but that has to be represented in the
generative model you know it has to be part of your belief structure in your belief system and
your representation of the world and it looks as if that is the closest thing in the generative
model to that kind of monitoring the fluctuations in how well I'm doing in terms of the free energy
seems to be the precision of beliefs about what I'm doing which is of course the dopamine
you know what we think it might be the thing that dopamine registers so the at a very simple
level of just valency or things being good and bad I would describe that to the precision
of beliefs about what I'm going to do so when things get better that basically means is I can
see the way ahead clearly thank you so I'm looking a bit at the time so I I I expect
although there would be more questions we have to stop here because the time is more or less
over and we expect the next speaker to be here in in like two minutes so thanks
from everyone attending the session for this excellent talk and discussion and we hope every
to be able to see everyone and you also in particular in the near future in person thank you
well thank you very much bye bye
