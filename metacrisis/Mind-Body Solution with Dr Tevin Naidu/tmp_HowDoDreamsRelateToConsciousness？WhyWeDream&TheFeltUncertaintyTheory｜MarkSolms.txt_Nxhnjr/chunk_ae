when I spoke to these other guys about your work, there's clearly a huge respect for it.
There's these guys clearly understand it. They know what you're talking about. And they think
the approach is, is a good approach. It's not, it's not one that they're discounting when they,
when they discuss these matters. Yes, that's, that's, no, I really, I think that it's true to
say that it is gaining traction. And but it's also true to say that it wasn't the case initially,
you know, it's, because I also at the time, when I remember when I first started the part,
because there were some people who, who think, because I think they tend to associate your
viewpoint at this point with just an ancient brain. And there's almost a negative connotation to
saying those words, as if you're, you're talking about something ancient in itself, but
Yeah, no, and I understand where that comes from. You know, it's because, in fact, I must say,
I blame Paul McLean for that, you know, this whole thing about the triune brain that, you know,
that there's this kind of cartoon idea that there's a reptilian brain, you know, sort of trapped
inside of our, inside of our brains now. So I want to make clear that when I say ancient,
what I mean is phylogenetically ancient in the sense that because we, because all vertebrates
have the architecture of the upper brainstem that I'm talking about, because we share with
all vertebrates, a reticular activating system and a periaqueductal gray, that means we have a
common ancestor that had that structure. The common ancestor of all vertebrates is 550 million years
ago. So I'm talking about that guy, the one 550 million years ago, that's what's ancient. I'm
saying that this structure evolved. And what that, what that means is that the dawn of consciousness,
if I'm right, that this structure is what's generating the raw feeling, you know, the basic,
the most elementary form of consciousness, the dawn of consciousness was at least 500, at least
550 million years ago. That's what I mean by ancient. I think there's, there's a lot of
misinterpretations, but because of the fact that you're going to these conferences, you have people
like Tom Nagel now obviously supporting you, hopefully people understand it more or give it
more time in order to gain that understanding. Because I think that's a fundamental approach,
is to actually read your work, read your papers and then draw a better conclusion.
Yes. You know, I suppose the, you know, the best thing that can happen is that people read your
work and they're persuaded by it. The second, the second best or middle thing out of three
that can happen is they read your work and they disagree with you and they reject it.
The worst thing that can happen is that they just ignore it completely. So, you know, even,
even when you initially get negative pushback, at least it's not just completely overlooked.
Yes. You know, it's true. It's sometimes naming a theory well gives it that,
that negative pushback you need. So like sometimes saying things like the Asian brain
is like Keith Frankish or Dennard saying it's an illusion. It helps with the name,
that illusionism gives it that push that it almost needs.
While you're on that subject, I know we're nearly out of time, but I want to say something else
about feeling, which is that unlike perception, it cannot be an illusion. You can't say,
you are currently having the illusion that you're in pain. You just are in pain.
You can't say that you having the illusion that you're sad. You might say you have no good reason
to be sad, but still you sad. And I think that that's also important, you know, that when we,
when we speak about, about what do we mean by consciousness as an illusion, or I think it's
not this doesn't have exactly the same meaning when you speak about an illusion that the brain
constructs its own sort out of its own ingredients, it constructs the illusion of a three dimensional
world out there with colors that actually the colors are a product of your brain rather than a
rather than a property of the world. Feeling it's a because it's about yourself, you know,
it just is what it is. It's you can't have illusionary feelings.
And yeah, because I was just about to say it's almost like the argument of illusionism can't
work against your view of consciousness, because it's fundamentally talking about different things,
because the illusion is coming at it from a very much cortical perspective in the first place.
So now I realized while we're speaking about this, that I didn't address a point you made earlier,
and it suddenly becomes relevant again. Now you said, what do I mean when I say
that that that perception contextualizes affect. And this is exactly the right place to come back
to that. So how it works is that you have the feeling that's just that is just the consciousness.
Then it's a matter of what that what what is causing that feeling and what can I do about
that feeling. So I like to use this term and not this this wording. But of course, I'm translating
something into words that is not verbal. It is first of all, the creature feels like this,
it's just raw feeling, I feel like this. And then secondly, it's a matter of I feel like this about
that. And so the and so the affect is as it were applied to the context within which it's within
which it's occurring. So this Oh, so it's these things that are making me feel like this. And
it's there that I can do something about this feeling. And so this is what I mean when I say
we feel our way into our cognitions and we feel our way into our perceptions.
Yeah. So so the the you said it's good to have a catch phrase, a catchy phrase. You said it's
good to have a controversial one. But I was somebody recently wrote a review of different
theories of consciousness. And they wanted me to tell them what's my theory called, you know.
So I said, I'm calling it the felt uncertainty. It's the felt uncertainty theory. And I think that
that's another way in which you can put what I've just said. Now, it's like, you know, it's like,
it's like the common currency is uncertainty. It's like, I feel like this. In other words,
I'm not in a state of certainty. Otherwise, you wouldn't be feeling the feeling always remember
is a deviation from homeostasis. And then it's a matter of, you know, so you apply that uncertainty
to your to your actions, feeling your way through, is this working? Is that working? Is this working?
Is that working? You know, so it's a it's a matter of affects sort of you palpate your
confidence in your current action program on the basis of whether or not it is moving the needle
in the right direction affectively. So so it's, it's, I like to put it like, in fact, it was
Karl who put it like this. He said that if things are turning out as expected on the basis of my
actions, if things are turning out as expected, that's good. And if uncertainty prevails, that's
bad. Okay, yeah. So it's sort of it allows you to gauge the environment and interact with it in a
very successful manner and to maintain homeostatic. In order to maintain homeostasis. Sorry,
it's going to put it to maintain homeostasis. The felt, will you call it the felt uncertainty
principle? Yeah, it's quite a nice, I was thinking about it as well before coming into this
conversation how called has one of his papers, he calls it I, I am therefore I think Kevin Mitchell
has I feel therefore I am, I mean, I move therefore I am and you could make it I feel
therefore I am is another one. Yeah, just to just to add that to the to the lecture series in the
future. Before we finish one last thing about this. The conference was about AI. Anything you
want to add about feeling and artificial intelligence and implications for the future?
Yes, I am working with a team of physicists and and roboticists and and computer scientists on
and an artificial consciousness. We've been at it for about two years. And we we've got some
good funding from the Oppenheimers, by the way. And we're now going into our second phase. And,
you know, I'm really very optimistic about what we're doing there. We're trying because we believe
that we've isolated, at least in broad outline, the causal mechanism, you know, how, how feelings
are generated, how and why they're generated. If you've, if you've done that, if you really have
done that, then you should be able to engineer it. In fact, it was Richard Feynman, who said
right at the end of his life, it was found on his on his blackboard. If I can't create it, I don't
understand it. And so, you know, this is what we're trying to do. We're saying, well, if this really
is the mechanism, then we should be able to engineer artificial feelings. Remember, we're not
creating a living thing. We're creating an artificial agent that has exactly the mechanisms
I was describing to you earlier about different needs and having to balance those needs. And
they are intrinsically valence to and for that agent. And they are categorically distinct and
etc. etc. And then this should be able to display the kinds of behaviors that I was describing to
you that where you can feel your way through a problem, come up with novel solutions that
should display voluntary behaviors. And it should be able to survive in
unpredictable environments and so on. So these are the sorts of things we're doing.
And I really do believe it's a matter and not of whether but when we'll, we'll succeed with that.
It's another story about how do you persuade your colleagues, you know, the problem of other
minds and all of that is not a small thing. What we are going to be doing is having a symposium
with parties where we must determine in advance what will be acceptable criteria. You can't say
there's no way that you will ever persuade me at all. You have to say, well, what will persuade you?
And then the ethics of that, we must always, always, always keep very clear in our minds
that the ethical, there's two sets of ethical problems. The one is, you know, the one that
people most frequently refer to is, you're going to create these super intelligent
creatures that are trying to survive that they're going to compete with us. And, you know, that's
bad for us. But we must also remember that if you create an artificial consciousness, if it feels,
then, you know, it can suffer. And you have to be very careful about what are you, what are you
producing here, you know, and it's all of the problems that we have in animal research become
relevant then to this sort of research, too. Mark, I'm sorry, I don't know how much time you
have left, but I'm so curious to know how does one even approach this? How do you even start
building that sort of artificial complexity? Well, you start with the formalisms that
was Carl and I published a paper in 2018. You know, he published that brilliant paper in 2013
called Life As We Know It. And there he reduced to formalisms, the mechanisms of,
fundamentally, it was the mechanisms of homeostasis. That's what, because the whole
mechanism of self-organization is what underwrites the mechanism of homeostasis. When he says life
as we know it, I mean, homeostasis is the mechanism of life as we know it. It's what distinguishes
living things. So what was I saying? Yeah, so the formalisms that he developed to explain
homeostasis, we extended those formalisms to explain felt homeostasis in our 2018 paper.
And then, you know, I got this team together, and we're using those to develop that formalism to
develop algorithms to create artificial agents that have homeostatic needs in order for them
to continue to survive. It's just a simple mechanical unpacking of this thing. It's creating
exactly those dynamics in an artificial agent and creating an environment within which we can
have the sorts of dynamics that we need. In other words, changing, so the agent has to meet
various needs of its own, like power supply and all of that. And then the resources where they are,
we move them around. Once it's learned, okay, this is, if I do this, I can survive, then we change it,
you know, and then we create situations where it's not just a matter of exploiting resources,
but also there's a hill that it can go up and have a look around. We want to see, does it
realize that if I go up there, I see the, I see, no, so it's, that's sort of roughly where we are
now. We're, we're, we're, we've got a long way to go, but a major part of the next phase that we're
going to do is to have multiple agents. So they, that where they have to interact with each other,
and they have to, they have to make inferences about each other's mental processes as part of
the same thing. Oh, that's super exciting. I'm sorry about the interruption, but I'm going to have
to go. It was really great talking to you again, Teran. It's been a pleasure always. And thank you
so much for, I know you don't like to call it around to, it sounds like a boxing match, but
hopefully there's a round of people ready to go as you continue on this amazing journey.
Thanks again, Mo. Thank you so much. Thanks for your interest and your time. Always, always.
Cheers.
