story they put together convincingly in the philosophical literature, but it may well be
a direction of travel over the next few years. The next few years. Okay. Can you discuss the
potential limitations or tendencies of your theory and area where further research is needed? So
criticize your own theory, Carl. Right. The first thing to do is to know that the free energy
principle is a principle. So it's not really there to be criticized or falsified. It's a little bit
like how much this principle of least action you can apply it or not depending upon what you'd like
to do. So it is in the application to various things that the challenges actually arise and
that's where the heavy lifting starts. And generally that reduces to specifying the world model,
the generative model that is necessary to define the free energy. And that's, I think,
where the challenges are. If you want to look at those challenges through the lens of, say,
a psychologist or a neuroscientist or an artificial intelligence researcher,
then you'll be asking what are the process theories that under this kind of generative model
would best account for the empirical evidence that best explains how this brain works or this
artificial intelligence works and what are alternative theories and what are alternative
message passing schemes that would emulate the dynamics that would ensue when applying
the free energy principle. So that largely is a very sort of
inelegant summary of my day job. It's basically trying to work out neuroanatomy,
functional architectures, physiology in a way that I can understand my brain and the brain of my
experimental subjects in terms of an application of the free energy principle to a generative model
of a particular sort. The question is, what is that generative model? And that's most of what
people are doing, brain imaging and psychology and neuropsychology and electrophysiology,
trying to understand how does the brain work by understanding its structure.
In terms of the outstanding challenges, if you wanted to build a brain or you wanted to build
artificial intelligence that was more like natural intelligence for sort of biomimetic
or generalized artificial intelligence, then I think the challenges here, first of all,
again identifying the right generative model and the attributes of those
generative models that speak to the scaling variant aspects that one would associate with
your brain. We've talked a lot about hierarchical generative models, but there are certain things
that are conserved as you move up the scales. Other things are not. So one characteristic
of these generative models is that as you move to hierarchical world models or generative models,
as you move to deeper levels, things slow down. So you get a separation of temple scales.
So that would be one challenge in terms of incorporating more and more temple scales
into generative modeling or into the generative models that you could then apply the free energy
principle to. So if you can write down either in computer software or just in narrative form,
if you can write down a generative model, everything else is sorted. The free energy
principle provides you with the tools and the methods to compute what would happen if you let
this generative model explain its world in exactly the same way that Hamilton's principle of least
action would allow you to compute the trajectory of a ball if you supplied the specification of
the ball. So if you can supply the specification of the generative model, job done, the problem is
getting the generative model right. And that is not easy. And one could indeed say that
this is a challenge faced not just by life scientists and particularly neuroscientists,
but it's also a challenge faced by machine learning and the artificial intelligence community.
A challenge that's made particularly more acute when you realize that the kind of
generative models that are requisite for surviving and modeling our current worlds would have to
acknowledge the fact that our worlds are largely comprised of other things like us. So it's not
just getting my generative model right, it's getting my generative model of your generative model right
and getting our generative model right and the communication between them. So taking again a
sort of bimemetic perspective, what we're talking about is not just finding the right kind of
generative models, but also understanding how they function, the implicit active inference and
learning processes that ensue when you put lots of these artifacts or systems together
and how that fits into sort of ecosystems of shared intelligence and shared federated inference
and federated learning. So that's an outstanding challenge at the moment. Another outstanding
challenge is being able to simulate what evolution has done when you view evolution
as another free energy minimizing process. And I mean that technically in the sense that
when you use free energy as a measure of the marginal likelihood, you are in a position then
to select certain hypotheses, models or phenotypes using Bayesian model selection. And if you look
at natural selection as basically nature's way of doing Bayesian model selection, then you can read
natural selection as the use of Bayesian model selection based upon variational approximations
to model evidence, namely the variational free energy to do structural learning. So by structural
learning, I mean selecting the right kinds of structures automatically. So that's still a largely
unsolved problem in statistics and in machine learning, and it's a really important problem.
So if you have the right structure, we now know the maths that would be required to run that structure
in active inference, for example, and deploy it on robots and also generalize the notion of free
energy minimization to the parameters so that you're learning the contingences and the connectivity.
But what we don't know is how to build the right structures. So the structure learning,
I think, is a really important issue there. Right. I do want to talk about machine learning and
artificial intelligence, but give me one second. I need to get the sun out of my face here. So give
me one second. This is better. All right. Yeah. Yeah. So I do want to talk about artificial
intelligence. You must know at least something about it, giving computational modeling and all
that, given the complexity of the brain and the free energy principle. Do you believe it's
feasible to create a truly conscious machine intelligence? And I'm putting conscious between
because available, whether you can even measure it, you can have a pragmatic answer or philosophical
answer. Or are there any unique aspects of biological systems that we can be replicated
in artificial constructs? I think in principle, provided one pursues this notion of
biomimetic artificial intelligence, then I see no reason why, ultimately, one could not
produce conscious artifacts of the kind that it would not be possible to differentiate.
We wouldn't know whether they were conscious or not. So I'm going to appeal to your option of
giving you a pragmatic answer. That's perfectly fine. So if consciousness is just like everything
else, a hypothesis that provides a simple explanation for all my sense making,
then one has to ask, where does it come from? On the one hand, and on the other hand,
acknowledge it is just a hypothesis that explains my sensations. So I'm just going to pursue both
lines because they both have implications in terms of answering your question. The first line
is this notion that selfhood is just a hypothesis. It's not necessary to live.
One can imagine viruses quite happily getting through, being very, very successful in their life
without self-awareness and self-consciousness. So why do we need, why do certain things
act as if they had a certain self-model and awareness? And one obvious answer, of course,
is that if our world is populated by things like me, then I need to be able to differentiate
who disambiguate the causes of certain things when both you and I can cause them, for example,
speaking. If we're really in a state of generalized synchrony and we're riffing together,
basically we're singing from the same hymn sheet. And if we're singing from the same hymn sheet,
I need to infer, did you say that or did I say that? And for the world, who's turn is it to talk
and who's turn is it to listen? And I can only do that if I actually have a notion, oh, it's me
as opposed to you. And indeed, if you think about the problems faced by a very small baby
just being born into the world, it's going to take some time for this baby to grow and learn
a world model or a generative model that confirms the hypothesis that mum is actually separate from
the baby, that mum is a thing in her own right. And it is plausible that having established that mum
is a creature in her own right and something that is separate from me, then I may have another
hypothesis that perhaps I am something that's separate from mum and the rest of the world.
So, you know, having a notion that others exist and then, oh, perhaps I am like that,
maybe one root to selfhood that inherits from really important dyadic interactions
that are purely encultured, but are a necessary part of this biomimetic kind of structural learning.
And of course, that argument goes through to all levels of communication and
in culturalism and the acquisition of language and shared narratives and so on and so forth,
all of which rest upon the notion that I share a belief, a world model, a set of norms, a narrative
frame of reference, a common ground with you. So that may be why certain things
have selfhood and other things don't in the sense that, you know, to be a virus, I don't need to talk
to other viruses, I just need to be able to operate within my milieu, which is usually inside of a
host cell that I'm affecting. So that may be one reason why certain things are conscious in the
sense of being self-aware. The second theme, though, is that I have generated this hypothesis
and I have confirmed it, but it remains just a hypothesis. So consciousness is just like anything
else, it's just a fantasy, which means that I will never know whether I'm conscious or, and
especially I will certainly never know whether you're conscious. So the whole point of having these
partitions, these Markov blankets that define things is, and to be able to read the Markov
blanket under the holographic principle, is that you could never get inside, you can only see what's
on the surface. So I will never know what's going on inside your head and I will never know whether
you are actually sense-making or whether the, or my description of your neuronal dynamics as
basal inference and active inference is actually true. I will just never know that. All I can do
is gather evidence from your behavior and from what you say. So if that's the case, then it
certainly is plausible that one could build machines that behave sufficiently similar to you for me to
imbue them with consciousness. So consciousness only lives in the head of the person making the
inference that you are conscious or I am conscious. It doesn't actually live in the head of the machine.
So on that pragmatic answer, I would say, yes, there's nothing to stop me building a machine
that would have all the characteristics of consciousness. To do that, I'm going to have to
be very much more biometric. Also just notice that there's a principle reason why it would have,
the machine would have to look like me and would have to have the same epistemic affordances
or respond to the same epistemic affordances that I do. In order to be conscious and in order for
me to imbue you as consciousness, you have to be sufficiently like me. And to be sufficiently like
me, I have to infer that you are sufficiently like me. I have to be curious about you to find out,
inferring from the way you dress, the way you talk, gathering evidence that you are sufficiently like
me before I can endow you with the note, with the attribute of being conscious like me.
That means that holds for an intelligent artifact. So the intelligent artifact will have to want to
be like you. And not only that, it will have to be curious about you. Otherwise, you will never
and it will never have the capacity for consciousness, if consciousness is an attribute
of sense-making in this sort of federated context. So I think it's quite important because that means
you can't build artificial intelligence that is conscious unless it looks and feels almost identical
to you. But on what level? There's one thing that I had an interview with Michael Levin.
He's a synthetic biologist. I'm not sure if you're familiar with him, but he said something
that reminds me, what he said reminds me of that. And it was, again, I'm paraphrasing,
that very interesting thing happens when you give a prompt to, for example, chat GPT.
And we think we are giving it prompts and we are asking it things, but it gives us a result back.
But you can also look at the result, you get back as a prompt to us. So could the result we ask
for actually be an inquiry? Because with these generative models, you give a prompt and then
you get a certain result and then you tweak it, you give some extra prompts until you get what
you want. But actually, what he's saying is it could be considered as kind of a dialogue if I'm
understanding him correctly. Yes, I know Mike very well. So he was a co-author on
that quantum information treatment of the Fieri principle. And yeah, that's again a sort of
brilliant thought experiment. I think now a lot of people are putting these large language models
together so they can talk to each other, which is exactly what we were talking about before
in terms of babies talking to their mothers or me talking to you.
So I think that thought experiment is very prescient and begs the question whether these
generative AIs, specifically large language models, and notice that it's not
generative models that create pictures or code that people ask, is this going to be
a conscious artifact or not? It's just those generative AI systems that generate a language
that invite these questions. And I think that's really important because then before I was trying
to paint a picture in which consciousness can only emerge in the context of living in the
universe comprising lots of conspecific or creatures like me, that I can assume have the
same kind of generative model or common ground that I do that enable communication. There can
be no communication without this common ground. And the specific thing about large language models
is that they do communicate in language. So AI, generative AI that chooses pictures,
does not communicate in a language. And you would not confuse a generative AI producing
very realistic photographs with a conscious artifact. You could easily confuse a large
language model generating a conversation and actually engaging a conversation with a large
language model. You could easily start to view it with consciousness simply because it's behaving
as if it was another agent that's very much like me using my language because that's what
it's designed to do. It's great all the data of communication in order to reproduce as if
it was having a conversation. It was communicating not in pixels, not in code, not in music,
but in words. And I think that's what gives it the potential to give the illusion of consciousness.
And I say illusion cautiously and advisedly because to actually have a generative model
under the hood would be for me a prerequisite to actually have a true inference process.
And there are certain arguments about whether you can look at large language models as possessing
a true geratin or world model or whether they're just detecting statistical regularities in the
sequences of words that have been produced and can therefore reproduce these particular
sequences. So it's a very vexed issue. But perhaps tangential to Mike's point that if
you've got something that can talk to you and sounds like you, even if it doesn't look and feel
like you, it certainly will sound and feel like you, then it now has the potential to become
something that you could infer was conscious. And on that definitely, that's as far as you
can ever go in my world anyway, simply because you'll never know, because you're always living
on the inside of your head. I get it. I get it. Harking back to what you said earlier about babies,
trying to realize or learn that their mother is separate from them makes me think about,
I don't know who said that, but basically, there is not really a way for a brain to tell
that the input that is coming from me, like your brain cannot actually tell that the input is
coming from me because the input that is coming from me to you goes through your neurons and
the neurons itself obviously cannot make a distinction. That's, anyway, that just popped
in my mind. Do you often work with genders with models yourself when you work lately,
because this whole large language thing has blown up and I find it hard to keep track of it,
because it's going so fast. It's so exciting and exhilarating. I was wondering, how much
do you keep up with it? I don't keep up with the literature or the developments, as you say,
in the past 100 or now, possibly 200 days, it's been absolutely amazing. Having said that,
I do work with generative models all the time at many levels, because we are all generative models,
so working with my students and when I used to see patients, patients I'm working with,
biological generative models, but certainly most of my academic theoretical work is basically
exploring and deploying generative models, usually in silico, to try and understand the
mechanics of the Bayesian mechanics and the fundamentals of active inference.
So, yeah, I work all the time with generative models. It's not quite the same as generative AI,
though. It's just interesting that generative AI has now come to be known as things like large
language models that generate stuff. When people talk about generative models in statistics and
in theoretical biology, they mean something much closer to the notion of a world model in machine
learning, that there's actually a structure under the hood that's gathering information, making
inferences about states of affairs, and then planning what to do next. So, most generative AI
doesn't do that, but the kind of generative models I work with, usually purely in academia,
