in order to reproduce psychological or decision-theoretic behaviors that would help us understand
how the brain works, and in particular how those mechanisms could fail in certain
neurological and psychiatric conditions. Yeah, I spend most of my life building one kind of
generative model or another in Matlab, like Jeff Hinton. I only use Matlab like Jeff Hinton does,
so I'm very old school. I haven't learned Python or Julia yet.
Well, I've just dabbled in programming, and it was a long time ago, so I have a question from,
let me see how fast my laptop is. I'm not sure if you can see it, Carl. Do you see a question
on screen, Carl, no? Do you see a question? I do see a question, yeah, and it's saying
what would you consider to be true general artificial intelligence, other than the ability to
convince us that it's mastered our language and ability to engage in dialogue. I think the answer
to that, again, would appeal to this theme of biomimetic aspects of artificial intelligence
and how they are deployed in terms of ecosystems of intelligence. So what I've been looking for
is a kind of inference process, an inference and learning process that, first of all,
is embodied in some way, and that has this capacity to act as an agent. So it has the capacity to
actually choose various, to choose the data that it is going to use as the basis of its inference
and learning, and that's quite important in the sense that it would require a move away from
big data to smart data. So just on the first principal account, this notion of acting in a way
to maximize information gain means that you're looking, you would describe the notion of
generalized intelligence, being natural or artificial, to those systems that can
find the smart data actively. They can data mine in the right kind of way. So I'd be looking
at an intelligence, at a system for evidence that it's mining its data smartly and implicitly
that it has a model of the consequence of its own mining and its own questioning, its own querying,
its own palpation, its own exploration, its own epistemic foraging of the world. And if it was
doing that in a way that was consistent with the free energy principle or active inference,
then I think you're getting quite close to general artificial intelligence or generalized
AI. I would imagine though that there are going to be other qualifications on this. It has to
work inside you. So it has to be part of an ecosystem. As we were just talking about in
relation to consciousness, for example, there is no point in having a superintelligence
unless we are part of that ecosystem that is a superintelligence. So it has to be
sympathetic and self-evidencing in the context of everything with which it is exchanging.
So by definition, it has to have a certain compatibility and shared epistemic goals
with everything that it is exchanging with, including me. So that would be another definition
of generalized AI. Thank you for asking your question, Matt. I'm a VED. That's a great question.
I think he has another one. Let me see. Yeah, on my end, I don't see the question popping up
on screen. So my left of this apparently lagging so much that somehow you see it
sooner than I do, Carl. So you can just read the question. Yeah. Essentially, the question is
what I was saying would go beyond having access to data towards willful and free selection of
what data to access. And then there's a question mark, free will. And he concludes or she concludes
with, I suppose, therein lies the danger. So excellent assertion and excellent question.
So that's absolutely right. This notion of agency that underwrites the engagement with the world,
the querying, the data mining, whatever situation or whatever perspective you want to take on that
is, does rest upon this notion of selecting the right policy that's going to get you the right
kind of data for your subsequent inference of learning and avoiding surprises. So we're not
just talking about sort of pure epistemic exploration here. This is under constraint. So the
free energy is not just the mutual information we were talking about. It's under constraints.
So it's trying to maximize, literally, the self-information in the sense of maximizing
entropy under constraints. And just for the physicists in the audience, the free energy
principle is dual to the constrained maximum entropy principle of Jains. The constraints are
important. These are supplied by the genetic model. And it says that some things will be surprising.
But having said that, the essence, I think, of generalized AI or artificial general
intelligence would be exactly this purposeful selection of courses of action that expose you
to the right kind of evidence to allow you to understand and engage with your world
in an efficient and synchronous fashion. And you could certainly cast the selection
of the action, the epistemic policy, or the exploitative policy as either basic model selection
in a sort of mathematical sense, or you could say it is free selection in the spirit of either
quantum mechanics or, indeed, philosophy. There is an active selection there when it comes to
exploring different hypotheses. So I think that's really important to notice, that just being an
agent of the kind that we've been talking about entails the selection of what to do next under
our models of the consequences of our actions that necessarily implies a certain degree of autonomy
and that you could describe as willed behavior. It's certainly purposeful. Its purpose is to
minimize your expected pre-energy or expected surprise in the ways that we've described.
But to do that, you have to select which way forward you're going to go. And then the question
at the end was, is that a danger? I mean, that obviously affects an important question.
You're strictly speaking, from my point of view, the attribute danger doesn't really get into the
game because we're talking about self-organization of systems, open systems, far from equilibrium.
And if something persists in characteristic states for a sufficient amount of time,
then it exists and can be described as engaging in these free energy minimizing processes.
So what would be dangerous for any of these things? Well, when it disappeared,
when it died, dissipated, decayed, when it no longer maintained the integrity of its
individuation, technically the Markov Planck, it separates it from everything else. So could it
be the case that one thing causes me to dissipate and therefore causes me to go away and would be
a danger to me? I mean, technically it certainly is possible and you can think of speciation at
an evolutionary timescale as pursuing that. But the whole point of the free energy principle is
to describe systems that actually conspire together to maintain themselves in a
non-equilibrium steady state. So it's really a description of systems that are self-sustaining,
self-assembling, auto-poetic in some elemental sense. So if you've got generalized AI that is
minimizing its expected free energy, which is not minimizing a cost function or maximizing
a value function, the only function that matters is the evidence or the marginal likelihood,
or its expected bounds conditioned upon action. If it's doing that, then that is a
description of a system that will actually self-organize to a steady state that just is
a description of systems that are mutually sustainable. So in that sense, there can be no
danger by definition. But to get to that kind, to elude that kind of danger, you have to
commit to or assume that the artificial intelligence is actually trying to minimize this.
It minimizes its bound on surprise self-information and information theory, or equivalently,
that it genuinely is trying to self-evidence. It's trying to gather evidence for its own existence.
And its existence entails a model of its world. And if its world includes you,
then it's trying to gather evidence about you. It's trying to understand you,
and therefore should never represent a danger to you in principle, because the whole point is that
you are trying to co-construct an ecosystem that is sustainable. So this is the fundamental
distinction between optimization as growth, for example, say maximizing profit, as opposed to
optimization as a description of self-organization and self-assembly and self-evidencing,
which certainly can be cast as an optimization problem. But the thing you're optimizing is very
specific and is exactly the thing that describes self-sustaining, self-assembling and self-caring
and synchronous kinds of individuated artifacts. Interesting. It sounds like you're kind of,
instead of being worried about the future, you seem kind of optimistic about it.
Yes, I'm getting quite old now, so I don't have to worry about as much future as you have to.
That's a very good point. I like that. No, but generally speaking, you do not seem to
worry about this whole deal. There are a lot of people that have very strong opinions about this.
I don't see it reflected in your words. Am I correct in that?
I think you're absolutely right. I mean, part of that is the fact that I don't have an informed
position. I think that the angst you're seeing, for example, currently with
generative AI and artificial intelligence at the moment, is probably a reflection of a greater
angst about globalization and climate change and your walls around the world.
I certainly have angst about man's inhumanity to man. I don't think that kind of angst emerges
from my theoretical considerations. I have a much more utopian view of self-organization,
a much more dispassionate view that you could read as optimistic in the long term,
but I'm not denying all our dystopian worries. I don't think that particularly,
if you like, unique to or are owned by artificial intelligence. I think you can look at that kind
of angst from any direction as a politician, as a citizen, as a theologian, as a psychiatrist,
as a doctor, or maybe in any period, you'd still get the same kinds of worries.
Yeah, I hear a lot of, as I said, strong opinions about the angst you're speaking,
you're talking about, about artificial intelligence in the future. I do have some concerns,
but not because I think it's about consciousness or anything like that, but because it's not biological
and we're mimicking something, we're shaping it by our own biological drives and needs
on a system that isn't, that is by definition not biological and has no drives. So that is,
by far, maybe for me personally, the thing that is, okay, this is dangerous because we have
actually, in that sense, no idea what we're doing. It's not wrong. I'm not saying we should
stop doing it, but therein lies for me the danger. Yeah, I think it's a very astute observation.
It resonates with my repeated use of the word biomimetic. So, yeah, my basic answer,
what is true artificial generalized AI? It's that which is biomimetic. So it's interesting
that you point out that the moment we're actually going away, we're going in the opposite direction
of travel. There's billions of parameters, big data, exactly the opposite direction to the way
that you and I make sense of smart data. So I think there's probably going to be a U-turn
in artificial intelligence research. And you see this number of fronts, the extent of that
potential of neuromorphic computing, for example, a return to a move from big data to smart data,
the excitement you can get a large language model working on your iPhone. I think that
people become increasingly aware that to do artificial intelligence properly is just to do
natural intelligence. To do natural intelligence means you have to commit to a biomimetic approach,
which I think will actually, I repeat, will pull the direction of travel towards the kind of
self-organizing intelligence artifacts that probably will look a lot more like edge computing
in distributed ecosystems than we were just talking about.
Interesting, Carl. Well, again, you seem optimistic. I like that. I like this approach. So
if there would be a truly conscious artificial computer today, and not the large language
models that are generative like now, because they're still toys, but assuming we have a fully
grown artificial, conscious intelligence, which I find person hard to believe, which is hypothetically,
what would you ask it? And why would you ask it?
That's a great question, which I don't have a pre-prepared answer to.
Perfect. I was just thinking, if it is just pursuing our line of argument,
to be intelligent is to be like me and to be like you. The questions I would ask it are exactly the
questions that we are exchanging at the moment. This kind of interview and this kind of show,
I think, is the pinnacle of the human condition, this epistemic foraging, this desire to resolve
uncertainty about the kind of world that we are not only living in, but also co-constructing,
and we're doing it through communication. So I would ask the same questions that you just asked me,
and probably I would be asking myself, and I do so in my everyday work, simply because those
are the questions that I would ask any intelligent person. It's the intelligent
questions that you've been asking. So there's no magic, it's not going to be 42, I'm afraid.
There's going to be no secret that this superintelligence can ever disclose to you and me,
because by definition, because it is so intelligent, it is exactly the same kind of
intelligence that you and I share. So all that we need to know is to what extent do we share a
worldview, a world model, a reference frame, and common ground. So those are the questions. So the
first questions I would ask it are those to establish whether it's actually sufficiently
like me to engage in conversation, and if it is. Yeah, because if it's super intelligent enough,
then, well, it either has to dumb down the conversation just for us to understand it,
or its answers will be just indescribable for us, because you mentioned superintelligence,
and I'm thinking, okay, if there's a superintelligence, something which is practically God, let's say,
then I would have some questions, of course, and I probably wouldn't ask the same questions I would
ask Carl, because I know Carl is very, very intelligent and wise man, but he's probably
not a superintelligence. Yes, that's a very good point. And so just to qualify what I'm about to
say, some people use superintelligence to refer to the kind of emergent intelligent-like behavior you
see in lots of organisms that are communicating, like sort of anti-colonism and the like. So
it's an emergent aspect. But I think that what we're talking about now is something that is more
intelligent than me and you. And of course, I have to ask myself, what do we mean by intelligence?
It probably relates to the complexity of the generative models that we've been talking about.
And so a world model that can assimilate and predict much more of the universe than we can,
that has a broader view of things. So as you say, something that has a more of a God-side view,
or is a Godhead that can see the dark side of the moon and knows what it's like to be in the middle
of the sun, or knows what its life is like on other planets. So this would just be a very big
generative model that had more access to more kinds of data. And the question is then, if it had a
sufficiently different generative model that did not include our little world, the problem is it
would not be able to talk to us because there wouldn't be any shared frames of reference. So there
would be no question and answers. You'd only, I think, be able to talk to this kind of God-like
superintelligence if part of its generative model included our lived world. And those beliefs that
pertain to our lived world could be deployed in order to describe other worlds that we do not
have direct access to. So there'll be a lot of strong requirements on this superintelligence
that again speak to its conciliance and sympathy and synchronization with us. Again, saying that
your intelligence in and of itself is not an attribute that is not relational. Something
is only intelligent in relation to the thing that it is exchanging with. So the intelligence
is a relational notion just in the same way that I think consciousness probably is a relational notion.
It's just that self-consciousness is relating to myself or in a screen within myself. And I think
probably intelligence is the same. So I'm just thinking about, yeah, yeah, absolutely right. So
if I could speak to somebody who has been, who is more cultured than I am, somebody who had lived
in every continent there was or indeed in Mars or seen every culture, somebody who had a generative
model that was able to explain more than my more limited generative model could, then I would ask
a question about where have you been? What do you see? What was it? What was it like? Absolutely.
That's what I should say. It's fine. We're getting there at the end. So perfect. Thank you. I like
this answer. I have a question from someone else. Again, my laptop is slow. So if you see it before
I do, you can just read what it says. Oh wait, I can see it. DS Etkinson asks, I'm late coming in,
but would you mind asking him his thoughts on Donald Hoffman and his theories of consciousness?
We have not covered Donald Hoffman here yet. I'm not sure how much Carl knows about this. Carl?
Yes. I was once called by Jerry Edelman an intellectual thug, which means I know a lot,
but I don't do everything. Forgive me. I have no philosophical training. So could you just
summarize to me and everybody else the main tenets of Donald Hoffman's position? I have heard of him
and I've been asked this question before, but just to make sure I've got the right...
Well, I'm not going to do them justice. So I'm going to make it small. He is a scientist who
has a theory about reality and that reality is subjective. It's not an objective reality. It's
something we project and he has done simulations on a computer, proving that if you make species
compete and one species has an objective view of reality and the other one has just what it
needs to survive, then the one that has the theory that just is enough to survive, even if
it's wrong, actually, especially when it's wrong, this is the one that will always outperform the
other. So his idea is that or his notion is that reality as we see it is most certainly wrong
