Welcome to Asking Anything, I'm Jack, I had a couple of technical difficulties with the
interview with Gal Friston yesterday, somehow the first half hour of the interview was not
recorded and there were some audio drops on my end.
The quality of my video isn't too great, I had some connectivity issues, my laptop was
running really slow, it was lagging, I think I was asking too much of this machine, but
I tried to salvage what I could, there are still two hours left of the interview.
What wasn't recorded was the introduction, Carl's explanation of his free energy principle
which if you are interested you can find on YouTube, I especially would recommend his
interview with Kurt Chai Mungle on the theories of everything channel, which I just keep recommending.
Having said that, if you're feeling particularly generous, consider donating to my Patreon at
patreon.com slash Asking Anything, while you're watching please like, subscribe, so let me
just introduce Dr. Carl Friston, a free energy farce here, neuroscientific nomad, Bayesian
brain, blacksmith, computational connoisseur, he is an entropotaming titan shaping our understanding
of the brain and cognition.
His work on the free energy principle offers a unified framework for deciphering the complex
dynamics of biological systems.
With a career spanning three decades, Carl has navigated the realms of theoretical neuroscience,
computational modeling, and systems biology, influencing our interpretation of the brain's
intricate workings.
He has authored over 800 scientific papers, marking transformative strides in his view.
Enjoy.
We talked a little bit about consciousness, and I am curious how your work relates to
there to other prominent theories of consciousness, such as, for example, integrated information
theory, global workspace theory, quantum consciousness theory, and higher order thought theory.
I'm not sure, I have a suspicion you know at least three or four of those, but how does
your work relate to those theories?
Again, a very interesting question.
In fact, with colleagues, specifically colleagues like Maxwell Ramstead and Adam Saffron, and
others, we've just written a series of papers looking at a minimum unifying model of consciousness
that speaks to a lot of those theories.
So I can answer your question quite expertly, because I've just read papers.
So they all have, to a greater or lesser extent, something in common with and conciliate with
the free energy principle.
And I say that because you have to remember, and I am always reminded of this by Jakob
Howing, that the free energy principle is not a principle that you can apply to believe
the theory of consciousness.
So it's not there to explain consciousness.
So it probably plays a useful role when applied to various theories of consciousness.
And when you do apply it, then generally you find, I repeat, a lot of conciliance.
So let's just take the global workspace theory.
So if you look at the mechanics of active inference that would be an application of
the energy principle to a brain, for example, and look at the various process theories such
as predictive coding and predictive processing, then you can see direct homologs or aspects
of global neural workspace theory and indeed global workspace theory emerge from first
principles.
So what are those first principles?
Well, we've just said that we can understand processing in the brain as a process of inference
under some kind of world model or gerative model that entails the space of hypotheses
that we bring to the table to explain our sensations.
What kind of model is it?
Well, we know two things about it in the spirit of the good regulator theorem or the notion
that in order to successfully explain our sensory input, our models of the world generating
inputs, our world models have to have a degree of anthropomorphism with the way that the
sensations are generated, which just means that the internal world models or gerative
models entailed by our brain have to have both a dynamic because of the importance of
time.
We're talking about processes here and hierarchical structure.
And as soon as you say that and then you work out the Bayesian mechanics or the free energy
minimizing dynamics that play out on these dynamic hierarchical gerative models, you get
to something that looks very, very much like a global neural workspace.
First of all, there is something deep in the centripetal hierarchy that is informed by
and predicts and tries to explain lower levels that are towards the surface of the centripetal
hierarchy.
Furthermore, not everything gets to the global workspace in the sense I have to be able to
attend to certain sensory inputs in order to grease the pathways to give those kinds
of prediction errors, for example, privileged access to deeper levels of processing.
So this is if you're a psychologist, you would recognize as attention.
If you were an engineer, you'd recognize it as trying to optimize the Kalman gain.
Mathematically, it's just a question of selecting those newsworthy prediction errors that convey
precise information and estimating the precision and thereby enabling those prediction errors
to ignite and induce Bayesian belief updating deep in the hierarchy.
So that would be, if you like, a mathematician's or a computational neuroscientist's description
of global neural workspace.
So just knowing that you have to have a dynamic hierarchical model would suggest that the
constructs that emerges under the neural workspace theories must almost be true and are emergent
from applying the free energy principle to those characteristic generative models that
you'd see in things like you and me.
The integrated information theory, that's slightly more distant in terms of its relationship
to the free energy principle because integrated information theory is really a description,
it's an axiomatic theory that is based upon a handful of axioms that must be true or are
assumed to be true and true of consciousness.
And then it tries to identify certain information, theoretic characteristics of systems that
possess these axiomatic characteristics.
So it's not really a simple principle in the sense of the free energy principle, it's more a
consequence of committing to certain axioms.
However, those commitments lead to something that looks very, very similar to the free energy
principle in terms of a set of partitions that are trying to, in some way, maximize
the mutual information between those partitions in a distributed set of states that are all
influencing each other.
And again, that sparsity of connectivity is exactly what underwrites the hierarchical
structures we were talking about previously.
So if you think about it, how do I make a graph or a network hierarchical?
I just remove any connections that jump over too many levels.
I just render it very, very sparse so that it has a set of partitions.
Technically, these are Markov blankets that define the thing that's, you know, if a hierarchical
level in my brain is a thing, then it has to have a Markov blanket.
If it has a Markov blanket, that induces a certain kind of partitioning and sparsity of
influence and connections.
Furthermore, the emphasis of a special kind of distributed mutual information that is inherent
in IIT can be read as an important aspect or part of variational free energy.
So in order to predict well, in order to maximize the evidence for my generative models
of a sensed world, I effectively have to maximize mutual information between my explanations
for what's going on out there and what is actually going on out there that actually entails
a maximization of the mutual information between my internal representations within my
generative model or my world model and the sensory fluctuations providing the evidence
that's helping me revise my beliefs about what's going on.
Those fluctuations in predictor coding would be the prediction errors, but that also holds true
in a hierarchical setting.
So not only am I trying to maximize mutual information between what's going on in my head
and what's going on in my world.
In fact, in this instance in your head, so that we can engage in this kind of generalized
synchronization, this shared narrative of conversation.
And I do that generally with my entire lived world.
But I'm also trying to maximize mutual information with my sensorium, but also all the inner
screens that constitute the different levels of my generative model.
So now we're right back to where IIT would would want you to be.
You're trying to maximize mutual information in a particular way over partitions that I
would read in terms of a heterarchical carving nature at its joints inside your generative model.
In terms of higher order thought theory, that almost emerges for free as soon as you talk
about hierarchical generative models.
You can't have any inference that is not, if you like, apt for descriptions under
higher order thought theories as soon as you commit to hierarchical predictive coding
of the hierarchical generative models.
So the whole point of having nested hierarchical levels in our generative model where each
level is trying to predict the level below right down to the sensorium is that you effectively
have inducing higher and higher order inference processes.
So that, you know, there may be some very deep one or more distributed systems that are trying
to predict the Markov blanket, which is another system inside the brain, very much like an
onion and so on all the way out to the sensory courtesies and ultimately our sensory epithelia.
So at some point you are going to encounter, especially in those kinds of creatures or
generative models or creatures that entail generative models that are this aspect of
matencing the capacity to plan into the future and you furthermore internalize action.
So internal action, mental action is basically selecting the data on the inside and that will
correspond to attention.
You've now got this deep link now between attention and conscious processing under higher
order thought theory that emerges simply because I have to have a model of the consequences
of my sense making, even if it's within my skull and within my hierarchical model.
And through that, you now start to tell a story about what am I modelling?
I'm modelling myself, modelling my data.
So you have a natural, it is quintessentially higher order just in virtue of having a hierarchical
world or generative model, which you would need in order to do your self-evidence or to
minimize your free energy.
Was there a fourth one, I can't remember?
Machine information, integrative mass theory, global workplace theory, quantum conscious theory
and higher order thought theory.
Those were the four most popular ones.
Quantum.
Yes, well very briefly, I would read the quantum
formulations very much along the lines that is emerging at the moment,
described by people like Chris Field and Jim Blaisebrook and Mike Levin.
So they've recently come up with this notion of an inner screen hypothesis,
which a lot of my colleagues are quite excited about and subscribe to, which is exactly...
Well, first of all, let me just qualify why this is a quantum approach.
It is not that consciousness emerges at the level of very small quantum physics.
It's the fact that you can understand anything from the point of view of quantum information
theory and the particular aspect that Chris and colleagues focus on is the notion of a
holographic screen that allows two systems to couple to each other.
So the bulk of one system reading and writing the holographic screen under the holographic
principle in tandem with and with shared quantum reference frames, another system.
And if you now apply that kind of quantum information theory to the hierarchical structures
we were talking about entailed by higher order thought theory, and one could imagine would
be endorsed by a minimum kind of partition under IIT, then what you have is this notion that there
are lots and lots of inner screens in our brains, which play the role of holographic screens
from a classical point of view, Markov blankets onto which you read and write classic information,
all with a shared or common frame of reference, quantum frame of reference.
And if you have that picture in mind, you can start to ask the question, well,
is there a special one right deep inside, and I mean literally deep, deep in terms of the
deep learning notion of a hierarchical world model you find in machine learning,
but also literally deep in the brain, where the cells of origin that do this attentional
modulation, this mental action that we were talking about, celebrated by people like Mark
Soames live. And then you arrive at a notion of an irreducible set of internal neuronal states,
whose Markov blanket or inner screen can never be, if you like, can't be reduced any further.
And it may be that this is a good model for the unique aspects of consciousness, the unitary
aspects of consciousness that are celebrated in the IIT axioms. And in a sense, this is, you know,
a resurrection of the Cartesian theater, it's a neo Cartesianism of a very bold sort that is
literally reintroducing the notion of screens, but in a very nuanced way that actually is very
consistent with the physics of self organization, because it all rests upon a free energy principle
or a quantum version of the free energy principle, where you're reading the holographic screens as
Markov blankets, but now on the inside of a hierarchical or a partitionable generative model.
Right. But is that the same thing as subjective experience?
Yeah, I'm not sure that you would be the people who are exploring this as a sort of minimal model
that gracefully accommodates things like higher order thought theory, global neural workspace,
and a number of other theories would be quite so adventurous to say at this stage, yes,
this is what this explains, qualitative experience of subjective experience, that I think they would
argue this would be a minimal requirement in the same spirit that I was arguing previously,
that to be to be a self is to be an agent and to be an agent is to act and to act is to have
a janitor model that entails and can generate the consequences of action. So I think that most
people would say that that sort of sentience of a non basic or trivial sort would certainly require
this kind of mental action that is ensues from having an inner screen that can never can only see
the consequences of its own action by acting on screens that are lower down or more superficial
in the hierarchy. And that's simply because there are no screens within side this minimal
mark off blanket or bulk. So a lot of this does rest upon action, mental action, which I repeat
you can read as attention if you like it's your mental action. And then if you read it along
those lines, you can then relate it to notions pursued by people like Thomas Metzinga and
Jakub Limonowski, where you can render things phenomenally transpire opaque by this kind of
mental action. So there is I think quite a nice story that could be built upon this notion of
an inner screen that is open to the rest of the brain and therefore is acting upon engaging with
the rest of the brain in a particular kind of way that looks like mental action and attention
that enables things to become phenomenally opaque. And this would be a prerequisite for
sentience of a certain sort. So I think there's a story to be told that I'm not sure that that
story they put together convincingly in the philosophical literature, but it may well be
a direction of travel over the next few years. The next few years. Okay. Can you discuss the
potential limitations or tendencies of your theory and area where further research is needed? So
criticize your own theory, Carl. Right. The first thing to do is to know that the free energy
principle is a principle. So it's not really there to be criticized or falsified. It's a little bit
like how much this principle of least action you can apply it or not depending upon what you'd like
to do. So it is in the application to various things that the challenges actually arise and
that's where the heavy lifting starts. And generally that reduces to specifying the world model,
the generative model that is necessary to define the free energy. And that's, I think,
where the challenges are. If you want to look at those challenges through the lens of, say,
a psychologist or a neuroscientist or an artificial intelligence researcher,
then you'll be asking what are the process theories that under this kind of generative model
would best account for the empirical evidence that best explains how this brain works or this
artificial intelligence works and what are alternative theories and what are alternative
message passing schemes that would emulate the dynamics that would ensue when applying
the free energy principle. So that largely is a very sort of
inelegant summary of my day job. It's basically trying to work out neuroanatomy,
functional architectures, physiology in a way that I can understand my brain and the brain of my
experimental subjects in terms of an application of the free energy principle to a generative model
of a particular sort. The question is, what is that generative model? And that's most of what
people are doing, brain imaging and psychology and neuropsychology and electrophysiology,
trying to understand how does the brain work by understanding its structure.
In terms of the outstanding challenges, if you wanted to build a brain or you wanted to build
artificial intelligence that was more like natural intelligence for sort of biomimetic
or generalized artificial intelligence, then I think the challenges here, first of all,
again identifying the right generative model and the attributes of those
generative models that speak to the scaling variant aspects that one would associate with
your brain. We've talked a lot about hierarchical generative models, but there are certain things
that are conserved as you move up the scales. Other things are not. So one characteristic
of these generative models is that as you move to hierarchical world models or generative models,
as you move to deeper levels, things slow down. So you get a separation of temple scales.
So that would be one challenge in terms of incorporating more and more temple scales
into generative modeling or into the generative models that you could then apply the free energy
principle to. So if you can write down either in computer software or just in narrative form,
if you can write down a generative model, everything else is sorted. The free energy
principle provides you with the tools and the methods to compute what would happen if you let
this generative model explain its world in exactly the same way that Hamilton's principle of least
action would allow you to compute the trajectory of a ball if you supplied the specification of
the ball. So if you can supply the specification of the generative model, job done, the problem is
getting the generative model right. And that is not easy. And one could indeed say that
this is a challenge faced not just by life scientists and particularly neuroscientists,
but it's also a challenge faced by machine learning and the artificial intelligence community.
A challenge that's made particularly more acute when you realize that the kind of
generative models that are requisite for surviving and modeling our current worlds would have to
acknowledge the fact that our worlds are largely comprised of other things like us. So it's not
just getting my generative model right, it's getting my generative model of your generative model right
and getting our generative model right and the communication between them. So taking again a
sort of bimemetic perspective, what we're talking about is not just finding the right kind of
generative models, but also understanding how they function, the implicit active inference and
learning processes that ensue when you put lots of these artifacts or systems together
and how that fits into sort of ecosystems of shared intelligence and shared federated inference
and federated learning. So that's an outstanding challenge at the moment. Another outstanding
challenge is being able to simulate what evolution has done when you view evolution
as another free energy minimizing process. And I mean that technically in the sense that
when you use free energy as a measure of the marginal likelihood, you are in a position then
to select certain hypotheses, models or phenotypes using Bayesian model selection. And if you look
at natural selection as basically nature's way of doing Bayesian model selection, then you can read
natural selection as the use of Bayesian model selection based upon variational approximations
to model evidence, namely the variational free energy to do structural learning. So by structural
learning, I mean selecting the right kinds of structures automatically. So that's still a largely
unsolved problem in statistics and in machine learning, and it's a really important problem.
So if you have the right structure, we now know the maths that would be required to run that structure
in active inference, for example, and deploy it on robots and also generalize the notion of free
energy minimization to the parameters so that you're learning the contingences and the connectivity.
But what we don't know is how to build the right structures. So the structure learning,
I think, is a really important issue there. Right. I do want to talk about machine learning and
artificial intelligence, but give me one second. I need to get the sun out of my face here. So give
me one second. This is better. All right. Yeah. Yeah. So I do want to talk about artificial
intelligence. You must know at least something about it, giving computational modeling and all
that, given the complexity of the brain and the free energy principle. Do you believe it's
feasible to create a truly conscious machine intelligence? And I'm putting conscious between
because available, whether you can even measure it, you can have a pragmatic answer or philosophical
answer. Or are there any unique aspects of biological systems that we can be replicated
in artificial constructs? I think in principle, provided one pursues this notion of
biomimetic artificial intelligence, then I see no reason why, ultimately, one could not
produce conscious artifacts of the kind that it would not be possible to differentiate.
We wouldn't know whether they were conscious or not. So I'm going to appeal to your option of
giving you a pragmatic answer. That's perfectly fine. So if consciousness is just like everything
else, a hypothesis that provides a simple explanation for all my sense making,
then one has to ask, where does it come from? On the one hand, and on the other hand,
acknowledge it is just a hypothesis that explains my sensations. So I'm just going to pursue both
lines because they both have implications in terms of answering your question. The first line
is this notion that selfhood is just a hypothesis. It's not necessary to live.
One can imagine viruses quite happily getting through, being very, very successful in their life
without self-awareness and self-consciousness. So why do we need, why do certain things
act as if they had a certain self-model and awareness? And one obvious answer, of course,
is that if our world is populated by things like me, then I need to be able to differentiate
who disambiguate the causes of certain things when both you and I can cause them, for example,
speaking. If we're really in a state of generalized synchrony and we're riffing together,
basically we're singing from the same hymn sheet. And if we're singing from the same hymn sheet,
I need to infer, did you say that or did I say that? And for the world, who's turn is it to talk
and who's turn is it to listen? And I can only do that if I actually have a notion, oh, it's me
as opposed to you. And indeed, if you think about the problems faced by a very small baby
just being born into the world, it's going to take some time for this baby to grow and learn
a world model or a generative model that confirms the hypothesis that mum is actually separate from
the baby, that mum is a thing in her own right. And it is plausible that having established that mum
is a creature in her own right and something that is separate from me, then I may have another
hypothesis that perhaps I am something that's separate from mum and the rest of the world.
So, you know, having a notion that others exist and then, oh, perhaps I am like that,
maybe one root to selfhood that inherits from really important dyadic interactions
that are purely encultured, but are a necessary part of this biomimetic kind of structural learning.
And of course, that argument goes through to all levels of communication and
in culturalism and the acquisition of language and shared narratives and so on and so forth,
all of which rest upon the notion that I share a belief, a world model, a set of norms, a narrative
frame of reference, a common ground with you. So that may be why certain things
have selfhood and other things don't in the sense that, you know, to be a virus, I don't need to talk
to other viruses, I just need to be able to operate within my milieu, which is usually inside of a
host cell that I'm affecting. So that may be one reason why certain things are conscious in the
sense of being self-aware. The second theme, though, is that I have generated this hypothesis
and I have confirmed it, but it remains just a hypothesis. So consciousness is just like anything
else, it's just a fantasy, which means that I will never know whether I'm conscious or, and
especially I will certainly never know whether you're conscious. So the whole point of having these
partitions, these Markov blankets that define things is, and to be able to read the Markov
blanket under the holographic principle, is that you could never get inside, you can only see what's
on the surface. So I will never know what's going on inside your head and I will never know whether
you are actually sense-making or whether the, or my description of your neuronal dynamics as
basal inference and active inference is actually true. I will just never know that. All I can do
is gather evidence from your behavior and from what you say. So if that's the case, then it
certainly is plausible that one could build machines that behave sufficiently similar to you for me to
imbue them with consciousness. So consciousness only lives in the head of the person making the
inference that you are conscious or I am conscious. It doesn't actually live in the head of the machine.
So on that pragmatic answer, I would say, yes, there's nothing to stop me building a machine
that would have all the characteristics of consciousness. To do that, I'm going to have to
be very much more biometric. Also just notice that there's a principle reason why it would have,
the machine would have to look like me and would have to have the same epistemic affordances
or respond to the same epistemic affordances that I do. In order to be conscious and in order for
me to imbue you as consciousness, you have to be sufficiently like me. And to be sufficiently like
me, I have to infer that you are sufficiently like me. I have to be curious about you to find out,
inferring from the way you dress, the way you talk, gathering evidence that you are sufficiently like
me before I can endow you with the note, with the attribute of being conscious like me.
That means that holds for an intelligent artifact. So the intelligent artifact will have to want to
be like you. And not only that, it will have to be curious about you. Otherwise, you will never
and it will never have the capacity for consciousness, if consciousness is an attribute
of sense-making in this sort of federated context. So I think it's quite important because that means
you can't build artificial intelligence that is conscious unless it looks and feels almost identical
to you. But on what level? There's one thing that I had an interview with Michael Levin.
He's a synthetic biologist. I'm not sure if you're familiar with him, but he said something
that reminds me, what he said reminds me of that. And it was, again, I'm paraphrasing,
that very interesting thing happens when you give a prompt to, for example, chat GPT.
And we think we are giving it prompts and we are asking it things, but it gives us a result back.
But you can also look at the result, you get back as a prompt to us. So could the result we ask
for actually be an inquiry? Because with these generative models, you give a prompt and then
you get a certain result and then you tweak it, you give some extra prompts until you get what
you want. But actually, what he's saying is it could be considered as kind of a dialogue if I'm
understanding him correctly. Yes, I know Mike very well. So he was a co-author on
that quantum information treatment of the Fieri principle. And yeah, that's again a sort of
brilliant thought experiment. I think now a lot of people are putting these large language models
together so they can talk to each other, which is exactly what we were talking about before
in terms of babies talking to their mothers or me talking to you.
So I think that thought experiment is very prescient and begs the question whether these
generative AIs, specifically large language models, and notice that it's not
generative models that create pictures or code that people ask, is this going to be
a conscious artifact or not? It's just those generative AI systems that generate a language
that invite these questions. And I think that's really important because then before I was trying
to paint a picture in which consciousness can only emerge in the context of living in the
universe comprising lots of conspecific or creatures like me, that I can assume have the
same kind of generative model or common ground that I do that enable communication. There can
be no communication without this common ground. And the specific thing about large language models
is that they do communicate in language. So AI, generative AI that chooses pictures,
does not communicate in a language. And you would not confuse a generative AI producing
very realistic photographs with a conscious artifact. You could easily confuse a large
language model generating a conversation and actually engaging a conversation with a large
language model. You could easily start to view it with consciousness simply because it's behaving
as if it was another agent that's very much like me using my language because that's what
it's designed to do. It's great all the data of communication in order to reproduce as if
it was having a conversation. It was communicating not in pixels, not in code, not in music,
but in words. And I think that's what gives it the potential to give the illusion of consciousness.
And I say illusion cautiously and advisedly because to actually have a generative model
under the hood would be for me a prerequisite to actually have a true inference process.
And there are certain arguments about whether you can look at large language models as possessing
a true geratin or world model or whether they're just detecting statistical regularities in the
sequences of words that have been produced and can therefore reproduce these particular
sequences. So it's a very vexed issue. But perhaps tangential to Mike's point that if
you've got something that can talk to you and sounds like you, even if it doesn't look and feel
like you, it certainly will sound and feel like you, then it now has the potential to become
something that you could infer was conscious. And on that definitely, that's as far as you
can ever go in my world anyway, simply because you'll never know, because you're always living
on the inside of your head. I get it. I get it. Harking back to what you said earlier about babies,
trying to realize or learn that their mother is separate from them makes me think about,
I don't know who said that, but basically, there is not really a way for a brain to tell
that the input that is coming from me, like your brain cannot actually tell that the input is
coming from me because the input that is coming from me to you goes through your neurons and
the neurons itself obviously cannot make a distinction. That's, anyway, that just popped
in my mind. Do you often work with genders with models yourself when you work lately,
because this whole large language thing has blown up and I find it hard to keep track of it,
because it's going so fast. It's so exciting and exhilarating. I was wondering, how much
do you keep up with it? I don't keep up with the literature or the developments, as you say,
in the past 100 or now, possibly 200 days, it's been absolutely amazing. Having said that,
I do work with generative models all the time at many levels, because we are all generative models,
so working with my students and when I used to see patients, patients I'm working with,
biological generative models, but certainly most of my academic theoretical work is basically
exploring and deploying generative models, usually in silico, to try and understand the
mechanics of the Bayesian mechanics and the fundamentals of active inference.
So, yeah, I work all the time with generative models. It's not quite the same as generative AI,
though. It's just interesting that generative AI has now come to be known as things like large
language models that generate stuff. When people talk about generative models in statistics and
in theoretical biology, they mean something much closer to the notion of a world model in machine
learning, that there's actually a structure under the hood that's gathering information, making
inferences about states of affairs, and then planning what to do next. So, most generative AI
doesn't do that, but the kind of generative models I work with, usually purely in academia,
in order to reproduce psychological or decision-theoretic behaviors that would help us understand
how the brain works, and in particular how those mechanisms could fail in certain
neurological and psychiatric conditions. Yeah, I spend most of my life building one kind of
generative model or another in Matlab, like Jeff Hinton. I only use Matlab like Jeff Hinton does,
so I'm very old school. I haven't learned Python or Julia yet.
Well, I've just dabbled in programming, and it was a long time ago, so I have a question from,
let me see how fast my laptop is. I'm not sure if you can see it, Carl. Do you see a question
on screen, Carl, no? Do you see a question? I do see a question, yeah, and it's saying
what would you consider to be true general artificial intelligence, other than the ability to
convince us that it's mastered our language and ability to engage in dialogue. I think the answer
to that, again, would appeal to this theme of biomimetic aspects of artificial intelligence
and how they are deployed in terms of ecosystems of intelligence. So what I've been looking for
is a kind of inference process, an inference and learning process that, first of all,
is embodied in some way, and that has this capacity to act as an agent. So it has the capacity to
actually choose various, to choose the data that it is going to use as the basis of its inference
and learning, and that's quite important in the sense that it would require a move away from
big data to smart data. So just on the first principal account, this notion of acting in a way
to maximize information gain means that you're looking, you would describe the notion of
generalized intelligence, being natural or artificial, to those systems that can
find the smart data actively. They can data mine in the right kind of way. So I'd be looking
at an intelligence, at a system for evidence that it's mining its data smartly and implicitly
that it has a model of the consequence of its own mining and its own questioning, its own querying,
its own palpation, its own exploration, its own epistemic foraging of the world. And if it was
doing that in a way that was consistent with the free energy principle or active inference,
then I think you're getting quite close to general artificial intelligence or generalized
AI. I would imagine though that there are going to be other qualifications on this. It has to
work inside you. So it has to be part of an ecosystem. As we were just talking about in
relation to consciousness, for example, there is no point in having a superintelligence
unless we are part of that ecosystem that is a superintelligence. So it has to be
sympathetic and self-evidencing in the context of everything with which it is exchanging.
So by definition, it has to have a certain compatibility and shared epistemic goals
with everything that it is exchanging with, including me. So that would be another definition
of generalized AI. Thank you for asking your question, Matt. I'm a VED. That's a great question.
I think he has another one. Let me see. Yeah, on my end, I don't see the question popping up
on screen. So my left of this apparently lagging so much that somehow you see it
sooner than I do, Carl. So you can just read the question. Yeah. Essentially, the question is
what I was saying would go beyond having access to data towards willful and free selection of
what data to access. And then there's a question mark, free will. And he concludes or she concludes
with, I suppose, therein lies the danger. So excellent assertion and excellent question.
So that's absolutely right. This notion of agency that underwrites the engagement with the world,
the querying, the data mining, whatever situation or whatever perspective you want to take on that
is, does rest upon this notion of selecting the right policy that's going to get you the right
kind of data for your subsequent inference of learning and avoiding surprises. So we're not
just talking about sort of pure epistemic exploration here. This is under constraint. So the
free energy is not just the mutual information we were talking about. It's under constraints.
So it's trying to maximize, literally, the self-information in the sense of maximizing
entropy under constraints. And just for the physicists in the audience, the free energy
principle is dual to the constrained maximum entropy principle of Jains. The constraints are
important. These are supplied by the genetic model. And it says that some things will be surprising.
But having said that, the essence, I think, of generalized AI or artificial general
intelligence would be exactly this purposeful selection of courses of action that expose you
to the right kind of evidence to allow you to understand and engage with your world
in an efficient and synchronous fashion. And you could certainly cast the selection
of the action, the epistemic policy, or the exploitative policy as either basic model selection
in a sort of mathematical sense, or you could say it is free selection in the spirit of either
quantum mechanics or, indeed, philosophy. There is an active selection there when it comes to
exploring different hypotheses. So I think that's really important to notice, that just being an
agent of the kind that we've been talking about entails the selection of what to do next under
our models of the consequences of our actions that necessarily implies a certain degree of autonomy
and that you could describe as willed behavior. It's certainly purposeful. Its purpose is to
minimize your expected pre-energy or expected surprise in the ways that we've described.
But to do that, you have to select which way forward you're going to go. And then the question
at the end was, is that a danger? I mean, that obviously affects an important question.
You're strictly speaking, from my point of view, the attribute danger doesn't really get into the
game because we're talking about self-organization of systems, open systems, far from equilibrium.
And if something persists in characteristic states for a sufficient amount of time,
then it exists and can be described as engaging in these free energy minimizing processes.
So what would be dangerous for any of these things? Well, when it disappeared,
when it died, dissipated, decayed, when it no longer maintained the integrity of its
individuation, technically the Markov Planck, it separates it from everything else. So could it
be the case that one thing causes me to dissipate and therefore causes me to go away and would be
a danger to me? I mean, technically it certainly is possible and you can think of speciation at
an evolutionary timescale as pursuing that. But the whole point of the free energy principle is
to describe systems that actually conspire together to maintain themselves in a
non-equilibrium steady state. So it's really a description of systems that are self-sustaining,
self-assembling, auto-poetic in some elemental sense. So if you've got generalized AI that is
minimizing its expected free energy, which is not minimizing a cost function or maximizing
a value function, the only function that matters is the evidence or the marginal likelihood,
or its expected bounds conditioned upon action. If it's doing that, then that is a
description of a system that will actually self-organize to a steady state that just is
a description of systems that are mutually sustainable. So in that sense, there can be no
danger by definition. But to get to that kind, to elude that kind of danger, you have to
commit to or assume that the artificial intelligence is actually trying to minimize this.
It minimizes its bound on surprise self-information and information theory, or equivalently,
that it genuinely is trying to self-evidence. It's trying to gather evidence for its own existence.
And its existence entails a model of its world. And if its world includes you,
then it's trying to gather evidence about you. It's trying to understand you,
and therefore should never represent a danger to you in principle, because the whole point is that
you are trying to co-construct an ecosystem that is sustainable. So this is the fundamental
distinction between optimization as growth, for example, say maximizing profit, as opposed to
optimization as a description of self-organization and self-assembly and self-evidencing,
which certainly can be cast as an optimization problem. But the thing you're optimizing is very
specific and is exactly the thing that describes self-sustaining, self-assembling and self-caring
and synchronous kinds of individuated artifacts. Interesting. It sounds like you're kind of,
instead of being worried about the future, you seem kind of optimistic about it.
Yes, I'm getting quite old now, so I don't have to worry about as much future as you have to.
That's a very good point. I like that. No, but generally speaking, you do not seem to
worry about this whole deal. There are a lot of people that have very strong opinions about this.
I don't see it reflected in your words. Am I correct in that?
I think you're absolutely right. I mean, part of that is the fact that I don't have an informed
position. I think that the angst you're seeing, for example, currently with
generative AI and artificial intelligence at the moment, is probably a reflection of a greater
angst about globalization and climate change and your walls around the world.
I certainly have angst about man's inhumanity to man. I don't think that kind of angst emerges
from my theoretical considerations. I have a much more utopian view of self-organization,
a much more dispassionate view that you could read as optimistic in the long term,
but I'm not denying all our dystopian worries. I don't think that particularly,
if you like, unique to or are owned by artificial intelligence. I think you can look at that kind
of angst from any direction as a politician, as a citizen, as a theologian, as a psychiatrist,
as a doctor, or maybe in any period, you'd still get the same kinds of worries.
Yeah, I hear a lot of, as I said, strong opinions about the angst you're speaking,
you're talking about, about artificial intelligence in the future. I do have some concerns,
but not because I think it's about consciousness or anything like that, but because it's not biological
and we're mimicking something, we're shaping it by our own biological drives and needs
on a system that isn't, that is by definition not biological and has no drives. So that is,
by far, maybe for me personally, the thing that is, okay, this is dangerous because we have
actually, in that sense, no idea what we're doing. It's not wrong. I'm not saying we should
stop doing it, but therein lies for me the danger. Yeah, I think it's a very astute observation.
It resonates with my repeated use of the word biomimetic. So, yeah, my basic answer,
what is true artificial generalized AI? It's that which is biomimetic. So it's interesting
that you point out that the moment we're actually going away, we're going in the opposite direction
of travel. There's billions of parameters, big data, exactly the opposite direction to the way
that you and I make sense of smart data. So I think there's probably going to be a U-turn
in artificial intelligence research. And you see this number of fronts, the extent of that
potential of neuromorphic computing, for example, a return to a move from big data to smart data,
the excitement you can get a large language model working on your iPhone. I think that
people become increasingly aware that to do artificial intelligence properly is just to do
natural intelligence. To do natural intelligence means you have to commit to a biomimetic approach,
which I think will actually, I repeat, will pull the direction of travel towards the kind of
self-organizing intelligence artifacts that probably will look a lot more like edge computing
in distributed ecosystems than we were just talking about.
Interesting, Carl. Well, again, you seem optimistic. I like that. I like this approach. So
if there would be a truly conscious artificial computer today, and not the large language
models that are generative like now, because they're still toys, but assuming we have a fully
grown artificial, conscious intelligence, which I find person hard to believe, which is hypothetically,
what would you ask it? And why would you ask it?
That's a great question, which I don't have a pre-prepared answer to.
Perfect. I was just thinking, if it is just pursuing our line of argument,
to be intelligent is to be like me and to be like you. The questions I would ask it are exactly the
questions that we are exchanging at the moment. This kind of interview and this kind of show,
I think, is the pinnacle of the human condition, this epistemic foraging, this desire to resolve
uncertainty about the kind of world that we are not only living in, but also co-constructing,
and we're doing it through communication. So I would ask the same questions that you just asked me,
and probably I would be asking myself, and I do so in my everyday work, simply because those
are the questions that I would ask any intelligent person. It's the intelligent
questions that you've been asking. So there's no magic, it's not going to be 42, I'm afraid.
There's going to be no secret that this superintelligence can ever disclose to you and me,
because by definition, because it is so intelligent, it is exactly the same kind of
intelligence that you and I share. So all that we need to know is to what extent do we share a
worldview, a world model, a reference frame, and common ground. So those are the questions. So the
first questions I would ask it are those to establish whether it's actually sufficiently
like me to engage in conversation, and if it is. Yeah, because if it's super intelligent enough,
then, well, it either has to dumb down the conversation just for us to understand it,
or its answers will be just indescribable for us, because you mentioned superintelligence,
and I'm thinking, okay, if there's a superintelligence, something which is practically God, let's say,
then I would have some questions, of course, and I probably wouldn't ask the same questions I would
ask Carl, because I know Carl is very, very intelligent and wise man, but he's probably
not a superintelligence. Yes, that's a very good point. And so just to qualify what I'm about to
say, some people use superintelligence to refer to the kind of emergent intelligent-like behavior you
see in lots of organisms that are communicating, like sort of anti-colonism and the like. So
it's an emergent aspect. But I think that what we're talking about now is something that is more
intelligent than me and you. And of course, I have to ask myself, what do we mean by intelligence?
It probably relates to the complexity of the generative models that we've been talking about.
And so a world model that can assimilate and predict much more of the universe than we can,
that has a broader view of things. So as you say, something that has a more of a God-side view,
or is a Godhead that can see the dark side of the moon and knows what it's like to be in the middle
of the sun, or knows what its life is like on other planets. So this would just be a very big
generative model that had more access to more kinds of data. And the question is then, if it had a
sufficiently different generative model that did not include our little world, the problem is it
would not be able to talk to us because there wouldn't be any shared frames of reference. So there
would be no question and answers. You'd only, I think, be able to talk to this kind of God-like
superintelligence if part of its generative model included our lived world. And those beliefs that
pertain to our lived world could be deployed in order to describe other worlds that we do not
have direct access to. So there'll be a lot of strong requirements on this superintelligence
that again speak to its conciliance and sympathy and synchronization with us. Again, saying that
your intelligence in and of itself is not an attribute that is not relational. Something
is only intelligent in relation to the thing that it is exchanging with. So the intelligence
is a relational notion just in the same way that I think consciousness probably is a relational notion.
It's just that self-consciousness is relating to myself or in a screen within myself. And I think
probably intelligence is the same. So I'm just thinking about, yeah, yeah, absolutely right. So
if I could speak to somebody who has been, who is more cultured than I am, somebody who had lived
in every continent there was or indeed in Mars or seen every culture, somebody who had a generative
model that was able to explain more than my more limited generative model could, then I would ask
a question about where have you been? What do you see? What was it? What was it like? Absolutely.
That's what I should say. It's fine. We're getting there at the end. So perfect. Thank you. I like
this answer. I have a question from someone else. Again, my laptop is slow. So if you see it before
I do, you can just read what it says. Oh wait, I can see it. DS Etkinson asks, I'm late coming in,
but would you mind asking him his thoughts on Donald Hoffman and his theories of consciousness?
We have not covered Donald Hoffman here yet. I'm not sure how much Carl knows about this. Carl?
Yes. I was once called by Jerry Edelman an intellectual thug, which means I know a lot,
but I don't do everything. Forgive me. I have no philosophical training. So could you just
summarize to me and everybody else the main tenets of Donald Hoffman's position? I have heard of him
and I've been asked this question before, but just to make sure I've got the right...
Well, I'm not going to do them justice. So I'm going to make it small. He is a scientist who
has a theory about reality and that reality is subjective. It's not an objective reality. It's
something we project and he has done simulations on a computer, proving that if you make species
compete and one species has an objective view of reality and the other one has just what it
needs to survive, then the one that has the theory that just is enough to survive, even if
it's wrong, actually, especially when it's wrong, this is the one that will always outperform the
other. So his idea is that or his notion is that reality as we see it is most certainly wrong
and his claim is that he can mathematically prove that and he's made a mathematical theory and it
is mathematically a pluralist theory in a sense that you need at least two conscious agents to
begin with, but philosophically speaking he admits that he is a monist and that says an idealist,
that consciousness is the source of existence and everything particles and energy and all that kind
of stuff is a projection from conscious agents like agreements between them. Donald, sorry,
I'm butchering your theory but this is as far as I can briefly explain it.
Well that was really useful and everything you said made perfect sense to me and I sort of
rang bells so somebody's clearly also tried to explain it briefly in previous conversation,
but everything that I think sounds perfectly consistent, you know, this notion of having
wrong models that are optimal in the sense of being sufficient to explain and predict our
exchanges with the lived world is to my mind absolutely right. It sounds very much like the
notion of satisfying, there's a whole theorizing where simplicity or minimization of complexity
is the objective function and this inherits directly from the sort of information,
theoretic perspective that you get from the free energy principle in all its carnations
and certainly would be the line that somebody like Jürgen Schmidt-Huber would pursue that to
compress is to have the best kind of model and that that compression can be scored by
the variational free energy. You will find it in economics and the sense of heuristics
and satisfying of the kind promoted by Goethe-Pigurenza and you will find it in machine learning
and data science in the form of compression. So again, you know, only having that stuff that you
need, everything else being redundant, so your models have to be as simple as possible.
As a statistician, I think this is a really fundamental point because, you know, if you just
write down the expression for model evidence, the marginal likelihood of your data given a particular
model and you write down or decompose the logarithm of the evidence, you could always express it as
accuracy minus complexity. That means to do good self-evidencing, to have a high adaptive
fitness for example, just means to provide an accurate account of your sensorium as simply
as possible by minimizing the complexity. So it doesn't surprise me in the slightest that when
you simulate agents who have the right kind of simple model, as Einstein said, you know,
keep everything as simple as possible but no simpler. So there's a golden loxary theme of
simplicity and damning down that will always outperform somebody who is overfitting their world
and having very, very accurate true models or certainly closer to the generative process.
And you find that, you know, basic phenomena emerging everywhere, not just in data compression,
but in overfitting, for example, in machine learning where they ran to things like sort of
sharp minima simply because they're overfitting the data. They need to simplify things by mini-batching
or blurring data, for example. So as a deep truism there, which means that all your models are wrong,
so he's absolutely right, but they're the best models, that's the important thing,
if they've maximized the evidence. You talked about the relational aspect and monism
and that physics is a construction. I think most physicists would agree with that, you know,
physics is measurement and measurement is inference and most of certainly sort of relational
quantum mechanics would certainly say that, you know, most of the things that we actually deal
with are, can only be described in terms of observation and measurement. I've just read
Carlo Revelli's book, Helga Land, which is beautiful, that convinced me even more that
everything is, as you put it, relational or certainly arising from diadic or dyadic
interactions and measurement. Yeah, I find that the theories that deal with information as a fundamental
fundamental thing of nature or even the most fundamental thing, I find them always hard to grasp
and it's silly because on the other hand, I have no problem considering that consciousness is
fundamental and that all exists is consciousness and interactions within consciousness and that
all particles and stuff like that are manifestations, imitation. I have no problem with that, but when
someone says, you know, reality is just information, somehow it just doesn't click. I see numbers and
I'm like, okay, how can reality be pure information? Yeah, but I mean, that's the message
throughout those and I mean, you know, Richard Feynman, you know, energy is information, you're
it from it, you know, wherever you look. Well, certainly as I get older and I did physics,
quantum physics and probability theory as a young man, and I thought that was reality. I thought
that's the description of reality. But as I get older, I start to realize that even
classical quantum physics, classical mechanics, statistical mechanics, quantum mechanics,
and even I think the Bayesian mechanics of the free energy principle, these are all just stories.
They're just stories that are part of my judging model that I've, some of my teachers have shared
with me, that you have as much, if you like, veracity as my fantasies I am me or that, you know,
or anything else that characterizes my beliefs and explanations for the world, for the,
of my world. So I know what you mean, but I'm pretty sure it's right that your notion of an atom
and a particle, have you ever seen an atom? No, no, I used to think, no, I used to, well, they
made beautiful pictures and drawings and animations, but apparently you kind of make a photograph
because they're smaller than the wavelength of light. So light passes over it. At least that's
a metaphor they gave me. So I'm at the stage that, okay, these are just useful metaphors,
nothing wrong with them, but they are metaphorical, just like almost anything we describe in reality.
But that just makes it easier for me to think about these things, that they're useful models.
Well, absolutely, I mean, making it easy to think about them is exactly the sort of, you know,
the free energy minimizing process that we were talking about, having those good, fluent,
simple models that make the most sense of everything that we encounter. But just think about what
you said, you find it very difficult to think about a universe that is composed just of information
and numbers, but much easier to think of it composed of atoms. And yet you've never actually
seen an atom. And even if you could, you wouldn't be able to see it. Even worse fantasies, at least
you can see a number. So, you know, isn't it strange the way our brains compel us to try
find these very simple, intuitive, good enough, sufficing stories to explain our world, even physics.
And physics changes every year, you know, there's no ground truth there. It's, you know, it's,
no, I have, well, I used to think that there was an objective truth, and then I didn't think
there was an objective truth. And now I'm just, I don't know, it's just I'm getting pretty agnostic
the older I get. So, but you're right, the stories we tell ourselves is, is one of the most
interesting things. It's also the way how our language works. It is metaphorical.
Almost everything is a story. A theory is a story from beginning to end, that this is how it seems
to work. There are certain archetypes, how these stories are built. And this is how we understand
it. And it's just, it blows my mind why this is apparently the only way that it works for us, or
maybe that is what language is. I don't know, I'm just, so someone else, oh wait, DSX had one other
question. It will be cool to ask him about his opinion about UFOs. But that's totally different
direction. But why not? This isn't, you can ask any question here. So, Carl, there we go. One
question to another question. No relationship. But what do you think of UFOs or UAPs as they are
called nowadays?
I guess the question is, well, I mean, certainly if, you know, if there are what is on the tin,
then they're just unidentified and surprised and unexplainable, so they will certainly
attract our attention under the free energy principle, or if we're doing the right kind
of active inference. If the implicit question is, you know, is there other kind of intelligent
life form elsewhere? I think that, you know, that is an interesting question, which again, I have
not, I don't have an informed view on. And my sort of back of an envelope, calculations, just
looking at the probability that we have co-evolved to a similar level of intelligence, and we allow,
you know, you mentioned it, communication. So the only way that we'd actually, you know,
see a UFO is one way of communicating. It's a deontic cue. It's a sign that there's something
roughly our size and, you know, it has the same kinds of physics of us that is around that can
to be in the same spatial location under my physics stories in my world. The probability
of that happening, I think, is fantastically small. So I would be extremely surprised if there were
if there was a lot, if there were aliens driving and identify flying objects around the earth,
very, very surprised, simply because there are so many constraints that would render
render the probability of that happening under my physics world model, my my intuitive physics.
Yeah, there seem to be a lot of reasons why aliens, well, the time it takes to travel,
the fact that you can go faster than light, the if it takes like, I don't know, 10,000 years to
get here, then by the time they get here, maybe their civilization has died out, and they even
forgot why they got here in the first place. Maybe alien life is so different than us that
simply are not able to see it. And then they're not able to see us in the list goes on. So
there are there are enough reasons. I kind of agree there. It seems like a slim chance, the
fact that they might exist sure, but no, why not? It seems awfully possible that they exist.
But so there's your answer, DS Adkinson. I have another question.
Let's see if that's the right one. MadMVD asks, makes me really wonder what happens to the energy
within us upon death to where does it dissipate? What form must it take? And the age old question,
where does conscience go after we die? Right, so again, sorry, we're just picking your brain
or all kinds of things, want to know how your brain works, how you think about things. So
right. So, so from a review of the sort of physics of self organization, I mean, death is
you're a really interesting phenomena in the sense it is exactly a failure of this
maintenance of this individuation of the thing from everything else. And, you know,
certainly using the word dissipate is I think very, very interesting. You could have used a word
decay or dissolve. All of these words basically reflect that there is a loss of the integrity
of the Markov blanket or the separation of you as a living thing from the rest of your world.
So I'm just picking up on the use of the word dissipate here. It is, I think, possibly
inevitable of all self organizing processes because this kind of process of self assembly
and sustainability into an attracting set cannot has to be, if you like,
reproduced over increasing time scales, which means that there is always a time scale
where things die very, very quickly from your point of view. And there's always a time scale
above where things last for an eternity from your point of view. So for example,
for me as a body, a lot of my cells actually die very, very frequently. And indeed,
whenever I cut my hair, I'm discarding some part of my body that has died and that will dissipate.
That would be looking at things at a smaller, faster time scale that indeed could actually
constitute me at my time scale. But also there's a time scale, let's say an evolutionary time scale
or cosmological time scale that is much longer. So if I am me and my conspecifics, I could endure
for thousands of, if not millions of generations. But at each level, there will be an inception
and a dissipation that is just part of this nested kind of self organization that can be
described as self, if you like self evidencing or free energy minimization, just in virtue of this
existence of solutions to certain states of being that is expressed again and again in this nested
way over different time scales. And where does the energy go? That speaks to the notion of
conservation of energy. But what I've just described is something that is not conserved when it dies.
So I prefer to think of this as from interpreting energy really purely in terms of
information and the processes, the information processing that is entailed by sense making and
by living. So in many respects, you don't have to worry about thermodynamic energy. It is the
energetic processes read as making sense as perception and active sensing that is the expression
of the, if you like, the informational aspects of the energy and just being and possibly even
being conscious is probably best thought of as a process that could be read as consuming energy.
And when that energy is no longer consumed, that is when you die. But of course, in your death,
death, there will also be other things will be reborn from your death, either at an evolutionary
time scale or indeed in terms of your what's happens to your atoms. I'm sort of wandering away
from the question now because I'm not sure what kind of energy that we would we were talking about.
And I would read energy really as I repeat a potential literally as like a voltage
where that potential is always read as a log probability, which brings us right back to the
notion of information, which means that, you know, in a very simple sense, energy and information
are the same thing. But things are processes. And that means it's the way that we handle energy
and handle information, which defines us. And in a sense, death is the cessation of that kind of
processing. Thank you. All right, another one, which is people are asking questions. Finally,
guys, this is what I this is why I'm using streaming art. So you guys can ask questions.
So a question from I'm not sure how to pronounce it see the cheat. I think you talk about using
free energy to explore future consequences. But what about reflecting on past actions? I see
self reflectivity as a key characteristic of general intelligence personally. Yep, I'd agree
entirely. So that notion of introspection and self reflection, and indeed reflecting on past
actions is speaks very much to what I was what we were talking about previously in terms of
the essential role of memory and and planning and memory in planning. And one aspect of
that planning is that one can improve one's world models, one's internal models or generative models
by effectively rehearsing what has happened to you and maximizing the efficiency of those models.
But interestingly, usually involves a not an increase in the accuracy, because whilst you're
reflecting or introspecting, there's no more data that you're trying to assimilate. So that you there
is no notion that you're trying to maintain the accuracy of your expressions of data because
there are no data. However, if you remember, we were just saying before the log evidence,
which the free energy approximates or bounds is equal to accuracy minus complexity. This means
you can still self evidence by minimizing the complexity of your models. And one way of doing
that is basically to resolve do house cleaning, if you like, and remove redundant aspects of your
world models during periods of interception. And some people also argue during sleep as well
during dreaming, for example, that these are excellent opportunities to continue self evidencing
in your head by looking for associations by resolving or eliminating redundant parameters
of your generative model that were otherwise rendered them too complicated and with a tendency
to overfit. So one picture of this is Julia to known as picture, for example, of synaptic
homeostasis that we spend our entire working day rushing through life, building up all sorts
of associations in terms of brain connections or synaptic connections in the brain. And then
we have quieter moments of interception of introspection, possibly
mindfulness and certainly sleep, during which we can revisit all those associations and remove
all the unnecessary and incidental associations in terms of synaptic connections in the brain.
So that we wake up the following morning or in a refreshed way with a slightly simple explanation
that has stood the test of time as it were. So I think that's absolutely crucial. And you see
that empirically in those sorts of context, you know, even little mice running around mazes
will emit signals from their brain suggesting that they're actually replaying and rehearsing
what they're going to do and what they have just done in order to consolidate and to
install the right simple kinds of generative models in the head that are apt for navigating
this particular situation. Thank you. Thank you, girl. What are your thoughts on NDE's
near-death experiences? I said nothing to do with your field. Still curious.
Near-death experiences? That's exactly right. I've never had one, but I had a friend who really
wanted to ask you that question. Right. I mean, these often associated with altered states of
consciousness and they, I think that they're terribly informative and presumably, if you've
ever experienced them, can be quite life-changing. And indeed, one might imagine that the experience
of these altered states of consciousness are sometimes the aspirational goal of certain
meditative practices or mindfulness practices and indeed some therapeutic applications
that engage the sense-making systems that we were talking about before in relation to mental
action and attention. So from my perspective, the typical near-death experience and outer body
experience, for example, that might be characterised by seeing yourself, you leave your body or you
might see a particular sort of photic stimulus, a bright light, for example.
So these are sort of characteristic experiences that are reported by people who have had near-death
experiences. And they all speak to, I think, a very important aspect of our sense-making. First
of all, they do reveal how fragile our fantasies and our hypotheses are. So if you take psychedelic
drugs, for example, that act upon exactly the same mechanisms that may be engaged during near-death
experiences, either physiologically or non-physiologically, depending on the trauma that was
inside of the near-death experience, then you will experience how fragile our sense-making can be
in terms of simple notions like time is flowing or sounds are actually seen as opposed to heard
or that my body is all joined up and doesn't fly around, for example, or that I live inside my body
usually just behind the eyes. All of these are as fantastic notions as Jack's notion of an atom
that is never actually seen and can be so quickly dissolved. I am me, I am a person. These are very,
very fragile hypotheses that can easily be, if you like, removed either in certain psychopathological
conditions or psychiatric conditions or by taking drugs or by trauma that is experienced,
unfortunately, by some people. Normally what they involve with is, involve is this
aberrant selection of evidence for sense-making at various levels in these hierarchical models
and, biologically, what that looks like is a failure of or an aberrant function of something
called neuromodulation. Now neuromodulation just means modulating the excitability of neurons,
brain cells that communicate messages that cause this belief updating and sense-making
passing messages from one level of the hierarchy to the next level of the hierarchy.
And it is very important which messages you pass, both upwards in terms of the prediction
errors and downwards to provide context and constraints in the form of predictions for
sense-making at the level below. And one way in which you select these, it is thought, is to
enable certain ascending messages that ascend deep into the hierarchy through these gain
mechanisms, either whether it is a potential gain or post-synaptic gain if you are an electrophysiologist
that are mediated by certain neurotransmitters. And interestingly, it is the transmitters that
are affected, the transmitters that are affected by psychedelics and by psychotropics and most
drugs using neurology and psychiatry. These drugs act exactly upon these neuromodulatory
mechanisms. So I would imagine that most near-death experiences fall into this class of altered
states of sense-making where there are no constraints on or you are dissolving certain
deep constraints by removing through a better neuromodulation,
moving the constraints on certain kinds of sense-making at particular levels in a hierarchical
model, which enable you now to experience the world in a very, very different way. I do repeat
though that this is sometimes a therapeutic objective. Imagine that you, for example, were
very depressed and had built a model of your world in which anything you did was met by
rejection or hostility or potential ego-distanic responses. And the best thing is just to stay
in bed and do nothing. And the problem with these kinds of world models is that they preclude you
going and searching evidence that challenges that model and that hypothesis. So the very fact
you're disengaging socially in terms of interpersonal or dynamic interactions means you can never test
the hypothesis that everyone is going to be nasty to me, which means that there's no challenge.
So these, if you like, self-assembly malignant hypotheses, sometimes you want to actually
challenge and resolve. And one way of doing that is to target these neuromodulatory mechanisms,
either by using drugs or by engaging in certain attentional practices. So you become skilled
at actually activating and deactivating these modulatory systems to allow yourself to explore
other ways of being, other hypotheses about your interaction with the world. And I'm thinking here
about meditation practices, internal attention states, mindfulness practices, some with or without
the use of psychedelics to augment that. And they can be very powerful, these modulatory
drugs. I don't know if that helps. I haven't had a near-death experience, so I can't speak to
No, no, no, it wasn't my, it was a question for someone else, so I can't speak for them,
but I hope it was helpful to them. He said something that interests me, though, that
there are some things that you find much more interesting, or at least as much as interesting,
like I'm in my body and there are a couple you mentioned. Anyway, one, I like to, I love to
read one book by Oliver Sacks, I think he's called S-A-C-K-S. And he had some stories about
patients. And I think one of those stories was about someone who was dislocated from their body,
like a couple of centimeters to the back and then a couple of centimeters up. So they were always
looking, looking somewhat down on their body. And so when they had to, you know, when they had to
grab something, they always missed. So they had to purposefully relocate their hand in a very
awkward way to magically somehow grab the cup they want to get. And there was also a case by
of a patient who had, who was stuck on the ceiling and was looking back upon themselves.
And while they tested it, obviously by writing something on a paper and putting in, putting it
with the letters upwards towards the ceiling and ask, ask the patient if he could read it. And
obviously he couldn't already made something up, but it was very convincing. That's the first thing
I think about when, when I think about near-death experience or no, no, out-of-body experience.
Yeah. But I would like to, maybe there will be a pill or a drug or something that I could try
to, for like 20 minutes, that would be so cool. Just for 20 minutes, just be, just be on the ceiling,
how that feels or not be in my body. It will be, I think it will be a very interesting experience.
So you should, you should invite Mark Sones to talk to you. I don't know if you've,
how do you spell his last name?
So S-O-L-M-S, Mark, his, his, his world expert on what's called neuropsychoanalysis,
which is basically a mixture of neuropsychology, brain lesion experiments and clinical treatment
in neurology and psychoanalysis. And he's also one of the world's experts on
Freud and Neofroidian thinking. But he, he has some really great ideas about consciousness,
but also has lots of patience of the kind that you've just described, for example.
So fascinating.
Yeah, it really is. You should, you should get him onto, you know, one of his, Amy Dolman is one of
his colleagues and she has a patient or not, but a patient just, you know, somebody that
she, she investigates, whose visual system is just reversed. So she sees everything.
Wow.
When you see her go from left to right, she sees it as going from right to left,
and she has to compensate. And then he realized this when she went to university and started
actually talking about the direction in which things moved or so. And really fascinating how,
how easily our geometric, our perspective taking models can go wrong with certain kinds of,
you know, aberrant neurophysiology or anatomy. And in light of that, if you also,
there's a gentleman called David Rudroff, Rudroff, our you, our AUF, who's currently,
he's been Grenoble, is currently relocated to France. And he's a great theoretician who has a
whole projective geometry theory of consciousness that really does put where you think you are
centre stage in building models of how we engage and act in our world. And of course, you know,
where we are physically and how we move our bodies around is a really important part of that.
Yeah, because you're not, you're not in your foot right now, you're somewhere here,
which is kind of super strange if you think about, I wonder if that's also one of these
things that are just efficient, maybe, or maybe there's some free energy principle going on there.
But I wonder, I actually did wonder about that somewhat earlier. It's like,
why am I not in my foot, why my hand? Yeah, it seems we all share this.
Apparently, we're about two centimetres just behind the midline of our eyebrows, apparently.
That's where most of us put ourselves. But it's a really interesting, why, why, why would that be
the vantage point? David's theory of projective geometry as a key architectural principle of
the generative of the world models that we have given our kind of bodies that we have to use in the
way that our eyes have evolved or our engineer, you know, that provides some really interesting
insights into, you know, the vantage points of where am I? Yes, being in a foot, I would have to think.
But thank you for the recommendation. I'll look him up. Thank you. Has you written any books,
Mark Soms? Well, Mark Soms has written a number of books. I think the most recent one was called
The Hidden Spring by Mark Soms. But he's quite a prolific and a very well known international
speaker. So you should be able to find, so he lives in Cape Town that travels the world,
you know, seeing academics and patients and the like. And the other gentleman was David Rudroff,
who was younger and hasn't written a book yet, to my knowledge. But he has certainly written some
very interesting high-end papers in this field with his mathematics. Yeah, and I'm a good at
reading papers, but luckily we live in the age of chat GPT, so I can ask for a summary, right?
So, Carl, were there any moments in your personal or experience in your personal life that you say
have significantly shaped or redirected your research in theoretical neuroscience and computational
modeling? I think most of the decision points were formative and absolutely crucial. Yeah.
So I don't think there were, you know, there were no specific events because very much,
as we were talking about before, in terms of having a narrative, having a willful narrative
about what's going to happen to me next and actively putting yourself in that position,
that's been very much where that I've designed my career. So every move I have made has been chosen
in order to align with this axis or direction of responding to epistemic affordances, to put
myself into a situation where I could ask questions and try to answer them doing theoretical
neuroscience. So formative ones were, I haven't got time to tell the story, but ending up doing
psychiatry, for example, that was very important and spending two years in an old Victorian style
lunatic asylum with 20 to 30 chronic schizophrenics and psychiatric nurses and
colleagues in a community setting for two years. That was quite interesting. Formative time of my life.
I can imagine. Yeah, there must be a lot of good stories there, maybe for another time, but
it's very interesting. And the other way around, how has your understanding of it
influenced your daily decisions and how you think about the world? Has it gone the other way around?
Yes, I think that's certainly true. I mean, a lot of the common sensical interpretations of active
inference are those that actually comply with the free-engine principle. They are accurate,
but really simple explanations for observed behavior. And of course, I observe my behavior
more than anybody else's. So a lot of this is, yes, that makes a lot, that's a simple explanation
for why I did this or why I perceived that or why I thought that or why I planned that.
So yes, there is a circular causality. Was there a moment where you were like, well,
if I hadn't done this work, then I wouldn't be this person? Or has Carl always been this person?
And he always knew what to do. And this work didn't change his inner perspective on life.
Oh, if it's so personal, you don't have to share. I'm just curious.
I'm just trying to think about your entertaining, but sensible answer. No, I think I just think
I was very lucky. I don't think I've actually used any theoretical insights to further my career.
Having said that, perhaps there is one interesting example, which is,
I came into science in the context of brain imaging and first contributed by writing
software packages for analyzing brain imaging data, specifically statistical parametric mapping
with colleagues. And subsequently, procedures that we mentioned right at the beginning of
the interview, such as dynamic causal modeling, having to solve the problem of how to analyze
scientific data exposed me in part to the principles of more generic modeling and sensemaking,
which meant that it was easy for me to see how one could apply exactly the same principles that
we were using to analyze our scientific data to the brain when it was trying to analyze its
sensory data. And very fortuitously at that time, people like Geoffrey Hinton and Peter Dian had
just moved next door to our brain imaging unit. And we're telling a very similar story from the
point of view of the importance of generative models in making sense of data in the context of
artificial intelligence and machine learning as it was in the early 90s. So I think the
confluence of those two things were very formative in terms of translating basic statistical
mathematical principles and methods into a story about biological self-organization,
and in particular, neuroscience, that then could be generalized to any kind of biotic
self-organization. We're nearing the two and a half hour mark, so I don't want to take up too
much of your time. I'm not sure how much energy you have. I don't want to be rude. We haven't
discussed this because we just went into the interview, but usually around 90 minutes,
two hours, I'm like, okay, how are you doing? Should we go on? Do you want to eat something?
Have a break or just end it? So here's that moment. Do you want to go on or like, no, Jack,
I'm done for today. I have to see my wife, whatever, hug my kids.
Let's do one more question, or if you have one. I have tons of questions. Yeah, okay. Well, then
it's a silly question, actually. Well, Carl, if you could have a conversation with any historical
figures, whether scientists, philosopher, religious leader, whatever, about our views,
who would it be and what would it be about? I'll be torn between Helmholtz and Richard Feynman,
I think. Oh, Feynman? Yeah, yeah. I mean, there are lots of great thinkers, but certainly for
me, for me or my heroes, apart from Sherlock Holmes, who's not a real person, are Helmholtz,
the other polymath and Richard Feynman, so it would be nice. I don't know that I'd have anything
sensible to ask them, but at least I could say I'd spoken to them and be very proud of that.
Well, I have been a fan of when I was younger. I listened to the audiobooks, the Feynman lectures
on physics, and oh boy, this man couldn't explain anything to you. It was so gorgeous to listen
to his stories, the way he explained it. He made everything simple, and he adds such a joy,
such a wonderful way of teaching these things, and it was really genuine. He wasn't even teaching,
he was just telling stories, but what would you have asked either of them? Because you would
like to speak with them if you could, but I'm curious, what would you have asked them?
Well, I would have, I'd like to have known where the contributions came from. In exactly the same
way the past two questions have been asking me, where how did my life story and career intersect
with and cause the academic contributions. I would also have the same kind of fascination for
these people's epistemic journals, at what point did they have these insights, and what
point were they articulated, what was a kernel for it, and do we understand them now in the spirit
in which they were originally invented? What I'd be particularly interested in is what would have
happened if Helmholtz had met Feynman? See, Helmholtz, I think, had all the right ideas that one could
even argue under right generative AI, and the past hundred days of great achievements in
machine learning and artificial intelligence research. He had all the right ideas, right from
the physics, right from sort of Helmholtz's decompositions and Helmholtz free energy,
right through to the psychology and his work on perception and optics. I think he knew an
enormous amount and tried to find a unifying principle for all of this intelligence and this,
he framed it unconscious inference, in terms of a neural energy or a neural energy that played
the same role as a Helmholtz energy or a Gibbs energy in free energy, in thermodynamics,
and really failed. And it's interesting that Freud at the time was coming up with his notions
shortly afterwards of energetic and bound energies and the like, and I would like to ask him to
extend Freud, was trying to take this narrative further, but just didn't have the right physics
grounding. But specifically, what would have happened if Helmholtz had met Feynman? Because
Feynman, I think, came up with the right kind of free energy that Helmholtz was looking for
in his work on the methodical formulation of quantum electrodynamics. That's the free energy
that is the free energy of the free energy principle. It's this thing that is used to
do good modeling, to work out the probability of things which normally you wouldn't be able to do,
because you couldn't solve this impossible marginalization or intractable marginalization
problem, but he worked out a way of doing it of the kind that could be used by physical systems,
like the brain. And had Feynman been able to talk to Helmholtz, I'm sure that we would have got
things like the free energy principle, not in the early 21st century, but in the early 20th
century. Yeah, you'll be out of a job then, so maybe it's for the best girl.
Girl, thank you for your time, really. I appreciate it so much, people here too.
I'm not sure how the weather is. It looks very nice behind you. It looks very sunny,
it's super sunny here, so I'm going to enjoy the last bits of sunshine too.
Yeah, just thank you for this conversation. I really enjoyed it.
Yes, well, I did as well. Thank you very much for having me. I can see the sunshine
fading behind you. I have very much enjoyed the questions. I didn't realize that so much time
had passed. You're absolutely right. So thank you. Okay, thank you. And I'll be speaking again.
If not, girl, good luck. Cheers.
