Welcome to Asking Anything, I'm Jack, I had a couple of technical difficulties with the
interview with Gal Friston yesterday, somehow the first half hour of the interview was not
recorded and there were some audio drops on my end.
The quality of my video isn't too great, I had some connectivity issues, my laptop was
running really slow, it was lagging, I think I was asking too much of this machine, but
I tried to salvage what I could, there are still two hours left of the interview.
What wasn't recorded was the introduction, Carl's explanation of his free energy principle
which if you are interested you can find on YouTube, I especially would recommend his
interview with Kurt Chai Mungle on the theories of everything channel, which I just keep recommending.
Having said that, if you're feeling particularly generous, consider donating to my Patreon at
patreon.com slash Asking Anything, while you're watching please like, subscribe, so let me
just introduce Dr. Carl Friston, a free energy farce here, neuroscientific nomad, Bayesian
brain, blacksmith, computational connoisseur, he is an entropotaming titan shaping our understanding
of the brain and cognition.
His work on the free energy principle offers a unified framework for deciphering the complex
dynamics of biological systems.
With a career spanning three decades, Carl has navigated the realms of theoretical neuroscience,
computational modeling, and systems biology, influencing our interpretation of the brain's
intricate workings.
He has authored over 800 scientific papers, marking transformative strides in his view.
Enjoy.
We talked a little bit about consciousness, and I am curious how your work relates to
there to other prominent theories of consciousness, such as, for example, integrated information
theory, global workspace theory, quantum consciousness theory, and higher order thought theory.
I'm not sure, I have a suspicion you know at least three or four of those, but how does
your work relate to those theories?
Again, a very interesting question.
In fact, with colleagues, specifically colleagues like Maxwell Ramstead and Adam Saffron, and
others, we've just written a series of papers looking at a minimum unifying model of consciousness
that speaks to a lot of those theories.
So I can answer your question quite expertly, because I've just read papers.
So they all have, to a greater or lesser extent, something in common with and conciliate with
the free energy principle.
And I say that because you have to remember, and I am always reminded of this by Jakob
Howing, that the free energy principle is not a principle that you can apply to believe
the theory of consciousness.
So it's not there to explain consciousness.
So it probably plays a useful role when applied to various theories of consciousness.
And when you do apply it, then generally you find, I repeat, a lot of conciliance.
So let's just take the global workspace theory.
So if you look at the mechanics of active inference that would be an application of
the energy principle to a brain, for example, and look at the various process theories such
as predictive coding and predictive processing, then you can see direct homologs or aspects
of global neural workspace theory and indeed global workspace theory emerge from first
principles.
So what are those first principles?
Well, we've just said that we can understand processing in the brain as a process of inference
under some kind of world model or gerative model that entails the space of hypotheses
that we bring to the table to explain our sensations.
What kind of model is it?
Well, we know two things about it in the spirit of the good regulator theorem or the notion
that in order to successfully explain our sensory input, our models of the world generating
inputs, our world models have to have a degree of anthropomorphism with the way that the
sensations are generated, which just means that the internal world models or gerative
models entailed by our brain have to have both a dynamic because of the importance of
time.
We're talking about processes here and hierarchical structure.
And as soon as you say that and then you work out the Bayesian mechanics or the free energy
minimizing dynamics that play out on these dynamic hierarchical gerative models, you get
to something that looks very, very much like a global neural workspace.
First of all, there is something deep in the centripetal hierarchy that is informed by
and predicts and tries to explain lower levels that are towards the surface of the centripetal
hierarchy.
Furthermore, not everything gets to the global workspace in the sense I have to be able to
attend to certain sensory inputs in order to grease the pathways to give those kinds
of prediction errors, for example, privileged access to deeper levels of processing.
So this is if you're a psychologist, you would recognize as attention.
If you were an engineer, you'd recognize it as trying to optimize the Kalman gain.
Mathematically, it's just a question of selecting those newsworthy prediction errors that convey
precise information and estimating the precision and thereby enabling those prediction errors
to ignite and induce Bayesian belief updating deep in the hierarchy.
So that would be, if you like, a mathematician's or a computational neuroscientist's description
of global neural workspace.
So just knowing that you have to have a dynamic hierarchical model would suggest that the
constructs that emerges under the neural workspace theories must almost be true and are emergent
from applying the free energy principle to those characteristic generative models that
you'd see in things like you and me.
The integrated information theory, that's slightly more distant in terms of its relationship
to the free energy principle because integrated information theory is really a description,
it's an axiomatic theory that is based upon a handful of axioms that must be true or are
assumed to be true and true of consciousness.
And then it tries to identify certain information, theoretic characteristics of systems that
possess these axiomatic characteristics.
So it's not really a simple principle in the sense of the free energy principle, it's more a
consequence of committing to certain axioms.
However, those commitments lead to something that looks very, very similar to the free energy
principle in terms of a set of partitions that are trying to, in some way, maximize
the mutual information between those partitions in a distributed set of states that are all
influencing each other.
And again, that sparsity of connectivity is exactly what underwrites the hierarchical
structures we were talking about previously.
So if you think about it, how do I make a graph or a network hierarchical?
I just remove any connections that jump over too many levels.
I just render it very, very sparse so that it has a set of partitions.
Technically, these are Markov blankets that define the thing that's, you know, if a hierarchical
level in my brain is a thing, then it has to have a Markov blanket.
If it has a Markov blanket, that induces a certain kind of partitioning and sparsity of
influence and connections.
Furthermore, the emphasis of a special kind of distributed mutual information that is inherent
in IIT can be read as an important aspect or part of variational free energy.
So in order to predict well, in order to maximize the evidence for my generative models
of a sensed world, I effectively have to maximize mutual information between my explanations
for what's going on out there and what is actually going on out there that actually entails
a maximization of the mutual information between my internal representations within my
generative model or my world model and the sensory fluctuations providing the evidence
that's helping me revise my beliefs about what's going on.
Those fluctuations in predictor coding would be the prediction errors, but that also holds true
in a hierarchical setting.
So not only am I trying to maximize mutual information between what's going on in my head
and what's going on in my world.
In fact, in this instance in your head, so that we can engage in this kind of generalized
synchronization, this shared narrative of conversation.
And I do that generally with my entire lived world.
But I'm also trying to maximize mutual information with my sensorium, but also all the inner
screens that constitute the different levels of my generative model.
So now we're right back to where IIT would would want you to be.
You're trying to maximize mutual information in a particular way over partitions that I
would read in terms of a heterarchical carving nature at its joints inside your generative model.
In terms of higher order thought theory, that almost emerges for free as soon as you talk
about hierarchical generative models.
You can't have any inference that is not, if you like, apt for descriptions under
higher order thought theories as soon as you commit to hierarchical predictive coding
of the hierarchical generative models.
So the whole point of having nested hierarchical levels in our generative model where each
level is trying to predict the level below right down to the sensorium is that you effectively
have inducing higher and higher order inference processes.
So that, you know, there may be some very deep one or more distributed systems that are trying
to predict the Markov blanket, which is another system inside the brain, very much like an
onion and so on all the way out to the sensory courtesies and ultimately our sensory epithelia.
So at some point you are going to encounter, especially in those kinds of creatures or
generative models or creatures that entail generative models that are this aspect of
matencing the capacity to plan into the future and you furthermore internalize action.
So internal action, mental action is basically selecting the data on the inside and that will
correspond to attention.
You've now got this deep link now between attention and conscious processing under higher
order thought theory that emerges simply because I have to have a model of the consequences
of my sense making, even if it's within my skull and within my hierarchical model.
And through that, you now start to tell a story about what am I modelling?
I'm modelling myself, modelling my data.
So you have a natural, it is quintessentially higher order just in virtue of having a hierarchical
world or generative model, which you would need in order to do your self-evidence or to
minimize your free energy.
Was there a fourth one, I can't remember?
Machine information, integrative mass theory, global workplace theory, quantum conscious theory
and higher order thought theory.
Those were the four most popular ones.
Quantum.
Yes, well very briefly, I would read the quantum
formulations very much along the lines that is emerging at the moment,
described by people like Chris Field and Jim Blaisebrook and Mike Levin.
So they've recently come up with this notion of an inner screen hypothesis,
which a lot of my colleagues are quite excited about and subscribe to, which is exactly...
Well, first of all, let me just qualify why this is a quantum approach.
It is not that consciousness emerges at the level of very small quantum physics.
It's the fact that you can understand anything from the point of view of quantum information
theory and the particular aspect that Chris and colleagues focus on is the notion of a
holographic screen that allows two systems to couple to each other.
So the bulk of one system reading and writing the holographic screen under the holographic
principle in tandem with and with shared quantum reference frames, another system.
And if you now apply that kind of quantum information theory to the hierarchical structures
we were talking about entailed by higher order thought theory, and one could imagine would
be endorsed by a minimum kind of partition under IIT, then what you have is this notion that there
are lots and lots of inner screens in our brains, which play the role of holographic screens
from a classical point of view, Markov blankets onto which you read and write classic information,
all with a shared or common frame of reference, quantum frame of reference.
And if you have that picture in mind, you can start to ask the question, well,
is there a special one right deep inside, and I mean literally deep, deep in terms of the
deep learning notion of a hierarchical world model you find in machine learning,
but also literally deep in the brain, where the cells of origin that do this attentional
modulation, this mental action that we were talking about, celebrated by people like Mark
Soames live. And then you arrive at a notion of an irreducible set of internal neuronal states,
whose Markov blanket or inner screen can never be, if you like, can't be reduced any further.
And it may be that this is a good model for the unique aspects of consciousness, the unitary
aspects of consciousness that are celebrated in the IIT axioms. And in a sense, this is, you know,
a resurrection of the Cartesian theater, it's a neo Cartesianism of a very bold sort that is
literally reintroducing the notion of screens, but in a very nuanced way that actually is very
consistent with the physics of self organization, because it all rests upon a free energy principle
or a quantum version of the free energy principle, where you're reading the holographic screens as
Markov blankets, but now on the inside of a hierarchical or a partitionable generative model.
Right. But is that the same thing as subjective experience?
Yeah, I'm not sure that you would be the people who are exploring this as a sort of minimal model
that gracefully accommodates things like higher order thought theory, global neural workspace,
and a number of other theories would be quite so adventurous to say at this stage, yes,
this is what this explains, qualitative experience of subjective experience, that I think they would
argue this would be a minimal requirement in the same spirit that I was arguing previously,
that to be to be a self is to be an agent and to be an agent is to act and to act is to have
a janitor model that entails and can generate the consequences of action. So I think that most
people would say that that sort of sentience of a non basic or trivial sort would certainly require
this kind of mental action that is ensues from having an inner screen that can never can only see
the consequences of its own action by acting on screens that are lower down or more superficial
in the hierarchy. And that's simply because there are no screens within side this minimal
mark off blanket or bulk. So a lot of this does rest upon action, mental action, which I repeat
you can read as attention if you like it's your mental action. And then if you read it along
those lines, you can then relate it to notions pursued by people like Thomas Metzinga and
Jakub Limonowski, where you can render things phenomenally transpire opaque by this kind of
mental action. So there is I think quite a nice story that could be built upon this notion of
an inner screen that is open to the rest of the brain and therefore is acting upon engaging with
the rest of the brain in a particular kind of way that looks like mental action and attention
that enables things to become phenomenally opaque. And this would be a prerequisite for
sentience of a certain sort. So I think there's a story to be told that I'm not sure that that
