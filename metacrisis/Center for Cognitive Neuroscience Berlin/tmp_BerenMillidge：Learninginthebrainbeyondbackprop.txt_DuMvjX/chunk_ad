And so what this means is effectively that the errors are in predicted
coding are redistributed throughout the network flexibly and sort of
dynamically, depending on what the other bits of the network are doing.
And so this is a nice visualization from our paper we have on this, which is
basically trying to give you a simple example, different to the one I gave
you about how interference works.
So another place you can get interference is if you have two outputs of the
network.
So the idea here is we have some, some bear in some like sitting at the
stream and you know, it's, it's sitting at the river and it's expecting to
smell some fish, which it likes to eat.
And it's also expecting to hear, you know, the water, you know, running down
the river, but for whatever reason, like it's, it's, I don't know, like it's,
it can't hear the water, like it's air is like damaged or something.
I don't know.
It can't hear the water.
And so obviously this produces a prediction in it's expecting to hear
the water and it can't.
And so what this means is if you have this double layer output.
So basically the input is like everything it sees here, and then it
expects both the, to see the fish and to hear the river.
And then basically if it doesn't hear one of them, then it's placed like this.
If it doesn't hear the river, then there's an error here.
And then back from the property, obviously back propagate this error all
the way backwards through the network, right?
And then update all the weights to make it think it's less likely to actually
hear the river.
And so what this obviously does is it changes the whole network, even though
this output was actually correct.
It was actually expecting to smell the fish and it was smelling the fish.
But this output basically as soon as this network changes, then this output
itself will change again.
And so basically this by having, by this prediction of it failing to hear the
river, it will end up actually in the future predicting it's less likely to
smell the fish as well, even though that prediction is actually correct.
And this is because basically backup updates all these weights without
taking into account essentially what the rest of the network is doing.
And so this obviously causes this interference effect.
And you can see this would actually be important.
Like for these kind of animal examples where you don't want this kind of weird
interference between different sensory modalities, say, well, like prediction
as a one-modality effect, how you think about other things, other modalities.
It's kind of a strange thing to happen.
And so predictive coding basically solves this because during the inference phase,
obviously there's an error here, but this error gets distributed back through
the network, but it also gets distributed to the correct output.
And essentially it's saying like the correct output is given an error because
the rest of the network has lots of errors, there's no error here.
And so it distributes the error up to here.
And then what this means is it updates the weights to this output in a way that
takes into account the weight updates being made back here.
And so this basically maintains this prediction, even though the rest of the
network has changed out from under it essentially.
And so this interference effect actually has quite a large effect on your
networks in practice.
And so one of the things we've been looking at is actually does this make a
difference?
And the answer is yes, it does.
So if we, so one thing to think about is that this happens much greater for
deep networks and it happens much greater for, there's more interference at
higher learning rates.
And so this kind of explains why back propagation, you essentially need
really low learning rates to do anything in backprop.
Essentially because with high learning rates, these interference effects become
very large in that you're basically always overshooting the output because
and high depths, the high learning rates are always overshooting the output.
And people don't normally do like multiple output neural networks, but
this is also something that would happen a lot.
So that's one issue with backprop.
Another issue with backprop, which in the inference learning IL does better is
that IL actually takes shorter paths through basically through the weight
space to, to reach the correct output than backprop.
And so the way you can think about this is backprop makes local updates.
So we just take like a step in the best immediate direction, whereas for
all the weights individually, but what predict coding IL does is IL can
actually take steps directly towards the correct output.
This is because it can sort of accumulate information from the rest of
the network during this inference phase.
And so one way to think about this is we can actually look at this measure
called target alignment, which is basically the angle between the direct,
most direct path from input to predicted output to the target and then
the output after you've done some learning.
So basically after a wait step.
And so this is basically showing the dynamics of backprop versus IL learning
some point.
The backprop basically this, we start out here and we want to go to here and
backprop takes this really long sort of secures path through the weight space.
Whereas IL basically takes a much faster space, much shorter path through
the weight space, and this basically will result in faster learning as well.
And so we can show basically that for deep networks, there's more interference
and target alignment, this, this measure gets much worse for backprop, but
stays the same for predict configuration.
Similarly, perspective configuration can actually and certain cases train
a lot faster than backprop because age making these shorter steps and B, it
has, it's got less interference.
And so each step kind of counts for more because that's not having to undo
the effect of other weights all the time.
So similarly, we find that IL outperforms backprop in various other situations.
And so perhaps the most interesting is this small batch online training.
So it's well known that basically backprop is really bad at online training.
Online training is where you essentially just get one example at a time.
So backprop always needs like large batch sizes.
And this increases with the size of the network that you have.
And basically this is thought to sort of average out various kind of noise
in the data set, but it also helps reduce interference as well by, by
basically averaging out the interference you get from the independence of weights.
And so it's kind of crucial to predict occurring IL, there's much better than
backprop on this, it makes sense why.
But also this makes perfect sense because the brain can only do online learning.
The brain can't like look at a whole randomly selected set mini batch of
like a hundred visual scenes at a time.
It has to only, you know, it's confronted with what it has to post
is what it's confronted with at the moment.
And so in these kinds of situations, IL does much better.
Similarly, in the small data limit, when there's not a whole lot of data,
basically IL does better and learns more from individual data points than backprop.
Again, this is something similar to what the brain needs to do and is what
is machine learning is currently quite bad at doing because it uses backprop.
Similarly, we can also show interestingly that IL outperforms backprop on
continual learning tasks.
So essentially what this is, is you have basically various distributions of data
tasks, you have various tasks you have to do, the distribution changes between
tasks and then, you know, you learn one task and then you have to learn the
next task.
And usually what happens if you do this, is you have this thing called
catastrophic forgetting where when learning task two, the network will
completely forget how to do task one at all.
And this is really bad, obviously.
And this is probably not something, I mean, it kind of happens in the
way you obviously do forget stuff, but you don't have this complete
catastrophic forgetting that happens with backprop.
And so we hear each other basically, it's not great.
So these are different tasks here and basically performance collapses whenever
we switch to a new task, which isn't particularly ideal.
But what we show is basically that IL actually does slightly better and
covers a lot faster than backprop whenever there is a switch between tasks.
So we know that particular IL is better than backprop and continue learning.
And similarly, we can also show that basically IL performs equivalently to
backprop on standard machine learning training patterns.
So like this thing is as good as backprop when in, where backprop does really
well, such as, you know, large batch sizes were at IID data sets, but also just
better than backprop, interestingly in many of the exact situations where we
expect the brain to do better.
And so this is one of the reasons I think IL is super exciting.
And it's kind of much more similar to how the brain must work than obviously
how backprop works, because the key thing about IL is that it is a
fundamentally parallel and local algorithm.
Like you have this inference phase where all the network is converging and
this can be done for every single neuron in the network in parallel and
independently of all the other neurons where the information is exchanged.
Basically all the time about how the local prediction has changed, but like
you never get a situation where like neurons have to wait for what other
neurons are doing to send them, you know, how to update themselves.
This never happens in IL.
And this is why like, I think it's much close to what the brain is doing.
And obviously it builds off predictive coding and the very large amount of
evidence that predictive coding or something very similar to that is
implemented in the cortex.
So IL also has some relatively nice theoretical properties.
So like we can actually analyze it in the linear case.
And so I have a paper which will be coming out like this week, discussed
doing a lot of these kinds of theoretical analyses.
And another very interesting property is we can show basically that this
IL algorithm essentially is interpolates between what back propagation does
and what target propagation does.
And where target propagation is obviously computing these local targets.
What we can think about the IL algorithm are these sort of the errors
in equilibrium, the activities equilibrium, effectively kind of like
local targets in that there, how the network should update itself should
evolve over time in order to better match the data.
And you can kind of think of this itself as a process of Bayesian influence.
Where essentially what we have is we have a prior, which is where we
start in the forward pass.
And then we have some data given to us, which is like the correct
input target association.
And then we want to compute the posterior of that activity, given the likelihood
obviously, which is the data and the prior, which is the feasible pass
configuration.
And then this actually obviously corresponds to the posterior for the layer,
which is exactly what the equilibrium activities of the network actually
represent.
So another way we can think about this in the same sort of setting is that
IL basically competes these local targets, but with priors to make them stay
close to the feed forward pass initialization, which is obviously helpful
basically so you don't completely over fake your network to every single
data point that you get.
You want to kind of stay relatively close to what you're already thinking is
good, but then obviously also gain some advantage from using this local target
update rule.
So other things that we can show is that basically we know that IL can actually
converge to a minimum of the loss function.
So this algorithm is actually a reasonable one.
It's not actually will converge the batch as well as back up and it has very
similar convergence properties to back up.
And essentially the idea is we can think about IL is basically from this
variational EM algorithm, which it's also, we can think about this as being
related to the target pop again, and if we can think about minimizing the free
energy as basically minimizing a constrained version of the loss function
minimized by back up where the loss function is anywhere.
The sort of additional constraints are basically the stay close to your
