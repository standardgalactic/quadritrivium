Okay, so I have a pleasure to introduce Dr. Baron Milic from the MRC Brain Networks Dynamic
Unit at the University of Oxford.
Baron received his Doctorate in Machine Learning and Computational Neuroscience from the University
in Edinburgh in 2021 and joined the group of Rafale Bogage at Oxford as a postdoctoral
researcher that same year.
Baron's work focuses on the intersection of machine learning and computational neuroscience
and in particular he's done a lot of work on several aspects of the free energy principle
and its process theories.
For example, he has critically assessed the standard form of the expected free energy
function and he has demonstrated how predictive coding approximates back propagation on a
large class of commonly used machine learning models, thereby showing that something close
to backprop might actually be more biologically plausible than generally thought.
And one thing that I want to stress in particular is that Baron is one of a number of younger
researchers who are putting a lot of effort into writing really good reviews and tutorials
and making the free energy principle much more accessible than it notoriously used to
be, which is a merit that can hardly be overestimated.
So Baron, we are glad to have you here and I'm excited to see what you're going to present
today.
So yeah, so I'm not here today to talk so much about my work on the free energy principle,
although in the discussion time I'm very happy to answer various questions about this.
I'm mostly basically towards trying to talk to you about my work related to predictive
coding and understanding how credit design works in the brain and how it relates to
different algorithms like back propagation of error.
And so this is sort of, I started this work in 2020 during my PhD at Sussex and have continued
on with it for the last like two years.
And so this is really sort of my most up to date findings and thoughts about this.
And so probably the paper that most of you will, if you know about it, you will know
is the predictive coding approximate backprop paper.
And so we'll go through that and also think about like, you know, further, you know, how
can we develop this and how is what can predictive coding say more about like how learning and
influence actually works in the brain.
So the way to start out is to think about, first off, what is credit assignment?
And so and why is it necessary?
So credit assignment is fundamentally the problem of your brain obviously has many, many, many
neurons.
And most of these neurons are quite far away from any kind of supervisory signal.
So like, you know, if you suppose you have visual neurons, most of the neurons in your
visual cortex aren't in direct contact with your retina, but yet still they need to figure
out basically, you know, when to fire, how to fire, what to do to actually produce useful
visual representations.
And so somehow information about like what to do at your retina, say, or, you know, at
the site of action must be somehow transmitted backwards through your brain.
So you can actually basically neurons deep inside your brain, which only connect other
neurons and don't have any direct supervisory signals will know what to do.
And this problem is fundamental for any sort of large neural networks.
And so this is fundamental machine learning.
And really it's the solution to this problem in machine learning by backprop, the back
propagation error algorithm, which is what essentially created all of like modern machine
learning and the fact that it actually works.
So this problem is fundamental.
And we still really have no real idea how the brain actually does perform credit assignment,
whether it performs backprop like our current machine learning networks, like our artificial
neural networks, whether it does something different.
My, I used to think it did backprop or approximated backprop, and now I think it does something
different.
So there's been some evolution during my research time, but anyhow, so that's the fundamental
problem I'm trying to tackle.
And I personally, I think this is like one of the most important questions in neuroscience.
It's really like a fundamental thing about like how the brain operates, but it's also
something that not many people study.
So like, that's actually quite a nice niche.
And as well as trying to understand the brain, this is also having answers to how the brain
could do this is actually very important as well for machine learning.
And so there's several reasons for this.
So the most obvious is really that like the brain is currently the only known like system
that actually exhibits like fully general intelligence like us.
And so if we understand how the brain works and understand like the fundamental principles
of learning in the brain, we could then implement that obviously in machines and potentially
do better than what kind of machine learning is doing.
Well, it depends on, you know, if you think how machine learning is sufficient for general
intelligence, then like it's not so important.
But if it's not, then it's obviously quite important.
And so similarly, like we think a lot of the basically the properties and algorithms we find
in the brain can actually transfer over to machine learning because the architectures
are actually incredibly similar.
And so there has been this amazing convergence in machine learning basically towards
artificial neural networks, deep learning, all this stuff is incredibly similar to the
the actual architecture of the brain, at least compared to, you know, how artificial
intelligence was before machine learning to deep learning took off.
There was like all these decision trees and tree search and logical rules and stuff.
And like, and various like SBN, none of this is actually related to what the brain does.
But the one thing that actually did work at scale is very brain like.
And I think that's very important.
And so it also means that we probably should expect a lot more transfer between like the
computer science, machine learning and neuroscience than we would sort of naively expect.
And so final reason is that personally, I think, and this is a controversial viewpoint
in neuroscience, that the brain is actually highly optimal.
And so it's well, very close to being optimal, given its constraints.
And most of these constraints relate to, you know, efficiency and also efficiency
in terms of energy efficiency in terms of space, but also efficiency in terms of like
learning speed and data in that the brain obviously has to operate with relatively
little data compared to sort of data, the art machine learning methods, where you can
like scan the entire internet and like download it and like, like, and your network see everything.
That's not possible for the brain.
It has to be able to do like adaptive behavior very quickly.
And so the learning principles that underlie the brain capability to do this could
actually be very useful for current machine learning methods as well.
So like, I'm going to give you a quick overview on how Backprop works.
So Backprop is the fundamental algorithm that does credit design in deep neural networks.
Probably like most people know this, I'm going to be quite quickly to go quite quick through here.
But if anyone has any questions like about this or anything in the talk, please just
like stop me and ask.
So essentially in Backprop, we imagine like we have a neural network.
So imagine these X's are like layers in a neural network.
And so this is typically, this is just like a simple sort of chain style architecture.
And so we define each or the activity of each layer, like X one, it's just some
function of the activity in there, zero.
And this goes through all the layers until we reach the output of the output.
We compute this loss function, which basically says like, how good is the output
of the network compared to what we want it to be?
And then given this, we can basically use compute gradients backwards.
And so the idea is we want to compute a gradient, say, of the weights like here,
given this output.
And what Backprop propagation does is using the chain rule of calculus.
It tells you that to get the weight update here, you could start here and go back one
and then compute this gradient and then multiply that by this gradient, multiply
that by this gradient and then get your weights.
And so Backprop is the sequential process of just computing the gradients, essentially
in the reverse order that the original computation happened.
And so like the first most obvious thing is if Backprop works so well in machine
learning, and like, basically, we know it works extremely well, then why doesn't
the brain just actually use Backprop?
And so obviously you could say like, maybe there are better things than Backprop,
the brain does that, that's very possible.
But also there's like serious problems with like a naive implementation of
Backprop given the circuitry we know about the brain.
And so several of these problems are one is that it's a fundamental thing in
Backprop is that there's this, there's this forward pass.
And then there is this backwards pass and the forward pass happens and then the
backwards pass happens sequentially.
And so this is not something that happens in the brain, in the brain, you have
essentially constant like influence of iterations recurrence of kind of like
transmissions going on all over, there's no like forward pass through your brain and
the backwards pass through your brain.
And so similarly, we have this other issue with sort of sequential transmission information.
So, you know, in Backprop to compute the gradient here, we first need to
compute the gradient here and then compute the gradient to the loss.
And so basically while, you know, the gradients are not actually computed.
So, you know, basically X2 here is like waiting for a bunch of time while all
these backwards gradients are computed.
And so this has to be done sequentially.
Whereas this doesn't happen in the brain, in the brain, everything appears to be
pretty parallel in that we can just like run sort of the brain, I guess.
And then everything is kind of, all the neurons are firing.
There's no like sequential sort of phase locked neurons, like first V1 fires,
then V2 fires, then V3 fires, and then on the backwards V3 fires, then V2 fires,
then V1 fires, like that's not what happens at all.
And so those are the sort of two main issues relating to parallelism.
And then there are other minor issues with the algorithm itself.
So like one issue is basically that the weight update is kind of non-local.
If you assume there's like a backwards set of circuitry to compute all these
neurons, then the weight updates here are used in the forward pass, but then they
have to somehow know about the gradients in the backwards pass.
So this sort of transmission from here to here is non-local.
And secondly, is that if we suppose we're going backwards, we need to know
what the forward weights are, but then the issue is obviously that in the brain
sort of axons like sign ups, all of this stuff is unidirectional.
So we can't go back through the same circuitry we came.
So there needs to be like this separate set of circuitry going backwards.
And then how do we keep the forward circuitry and the backward circuitry in
sync is essentially the problem.
And this is called the weight transport problem.
So there are various issues with backwrap.
And so relatively small, but like, I think quite cool.
Literature has grown up about like, how do we deal with this?
How can we make sort of biologically plausible backwrap or algorithms
to the approximate backwrap?
And so there's a whole bunch of them.
And so what really kicked off this field was this little crap paper from 2016
called random feedback alignment.
And the only thing this does is deal with the backwards weight problem by
basically saying that instead of computing the backwards weights with like
the same as the forward weights, you just take the backwards weights here
to be some totally random matrix.
And then mysteriously, that actually works quite well, at least for like
relatively small tasks.
So similarly, there's, you know, this other various set of algorithms
called equilibrium propagation, which comes out sort of of this group
by Yoshua Benjio.
And this is all about like update running the network in multiple phases
and then updating the weight of the difference between these phases.
And so that's one set of paradigm.
Another set is predict another paradigm of predictive coding, which is what
I've been mostly working on and will be telling you about today.
And so this sort of started out with this paper by James Whittington in
2017, where he showed that in some conditions, predictive coding can
approximate back up on these little chain like graphs.
Then obviously I did some work on this in 2020 and showed that you can
extend this to arbitrary graphs under different conditions.
And so this means basically any architect, any machine learning architect
you want, you could basically do back up on it with predictive coding
under certain assumptions.
And then my now collaborator, then we were, I guess we weren't collaborators.
We're only competitors either.
Yuhang Song showed like under different set of conditions are very
similar result also at the same time in 2020.
And so I'm going to talk about predictive coding network, but basically
the fundamental idea is it's an impredicted coding.
You essentially have like errors spread throughout the network.
And then these errors are sort of iteratively minimized, basically
