locally minimize their own error.
And in doing this, you can show the sort of distribution of error that
emerges at the end can be made to sort of look very similar to the
gradients in back up.
And then because of this, obviously, if you have those gradients, you
can approximate back up.
And so another thing, which is very interesting is that recently, I'm
not going to talk about this paper, but recently I also have a paper out
showing basically how all of these equilibrium propagation contrast
of heavy learning and predictive coding can all be kind of seen as the
exact same thing.
And there's like a unified framework, which can explain why all of these
approximations to back up occur.
And so that's really fun.
And I have a paper on that, but I won't be talking about that.
I'll be talking about my other fun paper this year, which is about
essentially what happens when we don't try and approximate back up.
And so that's lots of interesting things there.
But for now, we're going to continue with how the back up approximation
works.
So as well as approximating back up, there are other things you can do.
So essentially, we don't like there were multiple ways to optimize things.
Basically, all you're trying to do is minimize some loss function given in
your network and back propagation is one way to do it.
Another way is this other algorithm called target propagation, which has
a really nice intuition, actually.
So the idea is that instead of back propagating gradients, so instead of
saying like, for each weight, how should you change to minimize like the
loss function, you instead propagate this thing called a target, which is
like, for each weight, what would be the sort of optimal activity at the
next layer, the sort of local activity for you to produce.
And then if you know that optimal local activity, then you only have to
update the weight basically to maximize, to minimize the distance between
like the current activity and then the optimal target activity.
And so the key difficulty target propagation is obviously computing
these local targets.
The way they're done is like the way to think about a local target.
It is the activity at a layer that if, you know, the rest of the network was
the same, and if the activity of the layer was somehow set to this value, then
the output, then that activity would propagate through the network, such
that the output of the network would actually end up being like the correct
classification or would minimize the loss.
And so this sort of forward, this forward propagation of the target gives
you sort of the intuition behind it, but in practice, what you can do is you
can actually complete this by essentially inverting the network.
So if you have, say, a target at the end, you know, you want the target and you
know, like the layers, then basically you take the inverse of that target
through back through the layer, and then that will be the target of the next
layer and then the target of the next layer.
And so this is essentially this equation here, where the targets at the
given layer are basically the inverse of the forward mapping from the layer
above, essentially.
And then basically you can derive these local errors between the activities
and the targets, and then you can update the gradients using only these
local errors.
And this is obviously a highly local update rule, but it has issues with
sequentiality, essentially, because you have to, like in Backprop, you essentially
have to sequentially go back through the network computing the local targets
one layer at a time.
And this is because obviously the local targets, a layer L, depend on the
local targets, a layer L plus one.
So it still has the sequential problem, but if you have these targets, then
the weight update is local.
So hang on, I think.
Oh, hang on, never mind.
So anyhow, let me just check, I thought I had a slide about this.
Yeah, okay, so I'm going to skip quite a way ahead, but this is essentially
just a way to think about how the various algorithms work, I've described.
So in Backprop in the middle here, we basically have a network going up.
So this is like the forward passive model.
And then we basically set gradient sequentially backwards.
And so this is how Backprop works.
TargetProp, which I just described, has activities to the forward pass going
forward through the model.
And then sort of these local targets, these inverses being sent backwards.
And so both of these have this fundamentally sequential structure,
although TargetProp has slightly nicer locality properties than Backprop.
And then predictor coding, on the other hand, it has activities being sent up
and it has basically errors being sent down.
But the key thing is that these errors essentially are minimized locally all the
time.
And so there are always local errors that are being minimized.
And so you can run all of these minimizations in parallel, unlike in Backprop
and TargetProp, where you need this sequential backwards pass, essentially.
And so obviously it takes some iterations of this, like, some iterations of this,
like running the equilibrium convergence process is required to take the errors
back from the output, all the way to propagate them through the network.
But it's not necessarily a sequential process, is the key thing.
So all these elements can run in parallel.
Right, so now we move that forward.
Let's talk in a bit more detail about how predictor coding works.
And so the intuition is behind it.
And so I'm going to, if you guys kind of know about sort of inference in the FEP
and stuff, you probably know a lot of this, but like, basically the fundamental
idea is essentially that your brain has to do inference, like it has to infer
the causes of sensory stimuli, very similar to the FEP.
And then obviously the brain, by doing this inference, you have to have
been a powerful generator models of the environment.
And so the question is, of course, how do you obtain powerful internal
generative models?
And so predictor coding says, basically, you can do everything by sort of unsupervised
local prediction minimization.
So at the start, you know, we have, you know, we're trying to predict our
sensory data and solely by trying and failing to predict our sensory data can
rebuild a very powerful generator model of the world.
So that's like this abstract idea.
And then there's, you can specialize this to like specific architectures by
saying, like, if there's a hierarchy, you know, of levels or layers somewhere in
the brain, then each will minimize its own predictioners and sort of make
predictions of the level low.
So essentially this ischeme can be generalized to a hierarchical structure,
which is necessary to actually represent, you know, like detailed causes and
like latent states and stuff.
And then it sort of as in terms of like the implementation, what this results
is, is the predictions end up being sent downwards.
So essentially from high level layers to low level layers, where the lowest
level layer is just like the sensory input and then prediction areas are fed
back upwards.
So essentially each layer can figure out, you know, what are, what are the errors
in its predictions going downwards and then update based on that.
And so there's a lot of evidence in about, well, I like, I always like these
hallucinations of, yeah, anyway, these visual issues, basically, but there's
a lot of evidence that your brain actually works in this sort of predictive
way that it actually contains predictive models of the environment and makes
assumptions based on these predictive models.
And so this seems very obvious kind of now, I guess, but like back in the day,
people thought, you know, the brain was just doing this feed forward
sensory processing and doesn't sort of just houses the visual scene into
features.
And then this can't, this sort of feature view can't explain why we have
various visual illusions.
So like a classic example of this is sort of this kinesia triangle example.
So even though you can basically see these white triangles on top, even
though like there's actually no like lines, they sort of see these weird
illusory lines.
And this is basically because your brain, the model in your brain, individual
system has a very strong prior about like lines being continuous and shapes
and all this sort of stuff.
And so this is basically making like top down predictions like there should
be a line here and normally there isn't a line here and that's generating
prediction errors.
But basically this showing the, the not just parsing, you know, bottom of
input, but in fact, you're essentially matching your input with a top down
prediction.
And so a similar thing about the idea of prediction, although you guys kind of
seen this because I've zoomed ahead, is that if we haven't, if we don't know
what's here, this just looks like a bunch of random dots and sort of
strangeness that you can't really parse this scene.
But if I show you that, you know, there is a dog here and then I quickly go
back here and then you can, in top with this exact same visual data in a much
more effective way and you can directly parse that this is a dog, because
essentially the latent variables in your visual cortex are updated and it's
like, I now know there is a dog here.
And then this prediction comes down here and then this back gives you a
phenomenologically different representation or visual perception of the
scene.
So there's lots of evidence in this way for this kind of model based predictive
processing.
And so this is like, that's basically the standard specifications for this.
And then nowadays in machine learning, there's huge amounts of evidence that
this is fundamentally a thing that works as well.
Like if you look at any of the large scale machine learning stuff, like
GPT-3 and stuff, this is all based on unsupervised prediction of sensory input
in GPT-3 cases, this is text.
And so this seems to be a very general principle about like how to form
highly effective generative models.
Right.
So now that's an abstract high level idea of predictive coding.
And so now the idea is to try and specialize this into designing an
actual neural architecture.
And so for this, we try and say as close as possible to like really basic
machine learning machine, like multi-layer perceptrons.
And so we can think about this predictive coding network essentially being
like this hierarchy of layers, like a standard artificial neural network.
And the key difference is that instead of just having like the active activation
activity, whatever, at a layer, we instead have both activities and we have
predictioners.
And so the fundamental idea is we want to minimize these predictioners
throughout the whole network.
And so the way we do this is we first minimize predictioners and then you
minimize weights and relative to the predictioners.
But like the key, the connection to the free energy basically comes from the
fact that this sum of predictioners, you can interpret this mathematically as
a variational free energy term and hence interpret what the whole network is
doing as variational influence.
And so really this predictive coding emerges naturally from the FEP.
If you make certain assumptions about the generative model, specifically you
assume that it's Gaussian and hierarchical and all these kinds of assumptions,
then you can exactly derive predictive coding networks from this high level
abstract FEP.
So we can kind of make this a bit more specific.
So predictive coding essentially assumes that you have this hierarchical
Gaussian generative model.
So what this means is we have these latent variables x0 to xn, which all
represent a layer in the network.
And then we can basically split this up into a hierarchy of layers.
So each layer only depends on the layer below.
And where each conditional distribution here is Gaussian, where with the mean
of the actual activity, so the prediction projected upwards from the layer,
and a variance of basically some variance, which doesn't really matter that much.
