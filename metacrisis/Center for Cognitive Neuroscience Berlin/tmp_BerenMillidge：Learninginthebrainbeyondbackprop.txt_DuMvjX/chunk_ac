And so then if we do some mathematical tricks and we assume it's some
posterior variational posterior, we can basically show that the free energy as
is, you know, the fundamental object in the FEP and the fundamental object in
variational influence can be written as a sum of squared predictioners and where
the predictioners is defined between the activity to layer and the prediction
upwards from the layer below.
So the way to sort of visualize this is to think about basically that there are
multiple layers here.
So this is layer L plus one layer L and layer L minus one.
And in this normal machine learning MLP, these layers would just, there
wouldn't be any prediction error, eat it epsilon times here.
There's just these new terms and then this would be going up directly.
But here we basically introduced prediction errors at every single layer,
which represent essentially the difference between what this layer is,
what this layer is representing and like what higher up layers think
it should be representing.
This is the prediction error.
And then each layer basically optimizes its activities to minimize these
prediction errors.
And so personally, I think it's very helpful to write things as an algorithm.
So this is the sort of predictive coding algorithm very, very specifically is
what we do is we have this multi-layer network to do it, just make a prediction.
We fix the input nodes.
So basically we clamp the bottom of the network to whatever data you have.
And then you do run this, basically update the activity nodes to convergence
by minimizing prediction errors all throughout the network.
And then we just output whatever the prediction is at the output nodes.
And conversely in learning, the only thing we do different in learning is we
basically fix both the input to the data and also the output to the desired
data, aka the desired target.
And so then if we have this, we can update the activity nodes to convergence
and then update the weights given the errors at the equilibrium.
And so obviously basically in the prediction case, the only error coming
into the network is at the bottom in the light.
And then during this convergence case, this error propagates all the way up
the network slowly, but the key thing is that the error here can be reduced to
zero because basically there's only one constraint on the network.
And so it can always satisfy one constraint.
But with learning, there are now two constraints.
There's error at the input, error at the output.
Both of these errors showing convergence, they propagate throughout the whole
network, and then essentially they have to somehow balance each other.
And so the network will settle into an equilibrium where like changing the
activity in one layer will change, obviously, basically what it represents
to the prediction of the data, but also changes the prediction of the target.
And so both of these errors essentially match at equilibrium.
Then equilibrium, we can update the weights in such a way to minimize just
the basically the sum of the prediction errors at the equilibrium value.
So again, this is what the difference between predicated coding,
backfropping, targetfropping.
And so now we come to the first key result, which is how predicted coding
can approximate backfropping in general, so on arbitrary sort of computation
graphs. And so the fundamental idea here is that first off, we need to think
about what does it mean to do predictive coding on an arbitrary computation graph.
And so essentially in neural networks, we can think about like each node
here as being a specific operation.
So in the case of like multi-layer perceptrons, each node is just like a
layer and then this operation is just like the forward pass through that layer.
But like we can define arbitrary graphs, right?
We can add like another layer here and then like connect this up in some
strange way, however we want.
But the key thing is that these nodes are basically, can be thought of as sort
of the atomic operations.
And then the connections are like basically how these, basically how inputs
get fed through the atomic operation to like the next set of input, the next
sort of set of representations.
So basically the sole trick we need to do to turn any sort of arbitrary
like computation graph like this into a predictive coding network is we just
need to add our neurons to every, well, our neurons, our units, let's say,
to every single node on this computation graph.
And so basically what this represents is the difference between like the actual
value, this come, based on the value going into the computation.
And then they're different from that.
And basically the predicted value going into the computation.
And so the idea is that this, you can build networks of these things and then
run predictive coding as before.
And then the sudden cases, this actually become exactly the same as backprop.
So essentially the key prediction, well, the key result is that at the equilibrium,
you can show that the back propagation grade, the errors at equilibrium are
approximately the same as the back propagation gradients from the loss all
the way to the actual activity at that layer.
And so the way to think about this is basically that the neural, the predictor
coding will essentially distribute the errors throughout the network to find the
place with the smallest error.
And then the place with the smallest error in undecided conditions is also very
similar to the gradient of the error at, of the loss function at that point in
that if we change the error a bit, then basically we'll, that change will
propagate through the whole network and cause a change in the loss.
And then that is equivalent to the backprop gradient.
And so mathematically, you can essentially show that this, that the equilibrium
activities have the same reclusive structure as backprop.
So basically this is just a bit of algebra here, but we, this sort of is the
key line here, where we show that the errors at one layer are just equal to the
errors of the layer above times basically this derivative here.
And then this is exactly the same as the structure of backprop where errors are
replaced with these deltas, the gradients.
And so if the output layer, if the errors of the output layer are therefore the
gradients, then recursively the errors of equilibrium are always the gradients
all the way back.
So that's sort of that general result.
And you can show basically this is actually the case.
Convergence is quite easy, although it takes a while of simulation and that
it's basically you can train your networks with this stuff.
And so this is what I was doing in 2020.
And so there's also, you can think about various different conditions under
which this holds.
And so basically the, the short end, the sum of it is that this holds as long
as your gradients, your activation, your, your activities at equilibrium remain
close to the activities in the forward pass.
And so essentially the way to think about this is that here we can have these
unit terms relating the activities of the forward pass to the, the errors to
the equilibrium errors, but these are really the activities at equilibrium.
And so if the equilibrium diverges quite a way from the forward pass, then it
won't be the same as backprop which only uses the forward pass.
So the other way to think about it, if the predicted coding, the
equilibrium of predicted coding is essentially doing back propagation
through the equilibrium and not through the forward pass of the network.
And so if the forward pass of the network is close to the equilibrium, then
fantastic, we have got original backprop path, but if the forward pass of the
network is not close to the equilibrium, if the equilibrium is doing something
different, we're actually doing backprop through the equilibrium instead.
And so then the question is like, what does that actually mean?
And so this is the very interesting point I'm going to get you next.
But like, first we're just going to quickly have a survey of the literature
of how we can actually get it so that the forward pass is basically the
same as the equilibrium.
And so in the James Whittington paper, what he did is he said the ratio of
essentially the top down to the bottom of influence is small.
So the bottom of influence is essentially the forward pass going up.
And then the top down is the errors coming down.
And so if the errors are very small, then they won't influence, you know,
the actual equilibrium much.
And so it will stay close to the initialization in the forward pass.
And so mine is by basically setting the layer-wise grading to close the
free forward pass values, which will keep everything close to the equilibrium.
And then similar, you hang song showed in 2020 that if you just directly use
the first step of change, not the equilibrium anymore, but just the first update,
then this will be as close as possible to the forward pass and say we'll get
back up again.
Right.
So that's sort of what we've been working on about how predictive coding
actually approximates back up.
But essentially the key thing is that we know predictive coding.
These conditions are all kind of unsatisfying to me at least.
So this is the motivation behind this work.
These conditions are all kind of unsatisfying because if the forward
passes close to the equilibrium, then basically the equilibrium isn't doing
anything.
And what this means is that the predictive coding network is basically
not doing anything different from back up.
And it's kind of forced to do that in a kind of artificial and weird way.
And so we don't really don't want this to happen.
We actually want to be able to sort of use the full power and flexibility
that predicted coding this influence equilibrium stuff gives us.
And so then the question comes to then the question becomes if the
influence equilibrium is different from the feed forward pass, then what
does backfropping through the influence equilibrium actually mean?
So this is this basically to develop a new algorithm, which we call
inference learning.
And so essentially the idea is exactly what we said.
We first give and fix the inputs, the outputs of the network and compute
this equilibrium in the middle of all the activities, which we call the
inference phase, because in mathematical terms, it corresponds
exactly to possible to variational inference.
What we're essentially doing is we're inferring the posterior of each layer
in the network, given the input and the output layer, layers being clamped
to whatever the targets are.
Then given this posterior, you then do learning basically to maximize the
sort of log likelihood of this posterior given the weights.
And so this does something actually quite different to backfropping.
And so then the question is, does it do anything better than backfropping?
Is there any reason to think it's a nicer algorithm?
And so the answer to this is always a yes, right?
So if it hadn't been if it was like a rubbish algorithm, then we
wouldn't have any papers written about it.
So there's also selection issue.
But the key reason that we think IL inference learning is fundamentally
better than backfrop relates to interference.
And so people don't really think about this in backfrop much at all.
And but in backfrop, what is assumed is that each of the weight updates you
make in backfrop are independent of all the other weight updates.
So essentially imagine you have some error at the output, some, and then
you obviously compute two gradients, the gradient of the error with respect
to weight one, gradient to the error with respect to weight two.
Then then you basically apply these updates in parallel.
And so they're independent of each other.
What this means is you can have interference between these weight updates.
So they're like supposing the error is quite small or the weight of
data is big, then basically what you have is two updates, which both
correct the same error.
And so this can lead to like overshooting behavior.
Well, essentially what happens is like you basically go because you update
two weights, they both sort of merge together and strengthen each other.
And then you actually end up with an output, which is two, gives an error
on the other side, too strong of a change, essentially.
So if we're indicating inference learning, doesn't basically reduce this
interference quite a lot.
And this is because during the inference phase, essentially the difference
activities of each layer, they're interacting with each other because of
their errors.
