feed forward pass priors.
And so essentially we know that IL will converge to back up.
We have lots of experimental evidence for this.
And so what this kind of means is that this IL algorithm is really nice.
It is completely power.
It's completely local.
And it's basically predicted coding, which is an algorithm we know is
has fair bit of evidence for being implemented in the cortex, or at least
it's like a very promising contender.
We know that this algorithm does at least as well as back up on like many tasks
we know it can converge to minimal for loss functions.
It can be applied to arbitrary machine learning architectures.
And we also know that it does better than back up in various situations,
which are actually relevant for the brain, such as, you know, the continual
learning and the, the small data, the online learning, all these kinds of things,
which is basically where back up doesn't do well, but IL does do well.
And so furthermore, it has very nice theoretical properties, like it reduces
these weight interference effects.
One thing I want to test out is whether it actually helps deep networks
be trained effectively.
So like in classic machine learning, you can have very deep networks, but like
typically you need to use residual network with resnecks because essentially
the issue is signal propagating through the networks really bad.
And so IL might be able to help solve some of these vanishing grading issues.
You can get with back up.
And finally, IL is also much more flexible at test time behavior.
So basically, I've not already talked about this at all, but IL, instead
of clamping the input and the output of the network, you can basically take
posteriors over arbitrary subsets of the network.
So you can say like, I want to clamp some, you know, layer three of my
network to something and then tell me what the layers, like, you know,
layer four does, but also tell me what layer three does.
And this kind of clamping is unique to IL.
I can't really be done with back up, back up train networks in a very easy way.
And so this could also be a way for like the brain, basically different regions
of the brain to interact with each other.
They can interact by essentially clamping up conditioning, sending basically
data to the end, to the inside of the network, not just to the input, output
interfaces, but to anywhere in the network.
And then they can sort of read out different queries you can make to the network.
So I mean, I think personally this is a very nice algorithm.
And so we've got several papers written on this at the moment.
And I think it's a quite a nice thing in that we can actually go beyond
just sort of standard backup and like, Hey, can the brain approximate back
up? Instead, we can say like, no, there are actually better algorithms
that the brain can use, which not only are easier for the brain to implement,
more natural for it to implement, have nicer, harder listen properties, locality
properties than back up, but also do as well as back up or better.
I think that's, that's quite a big statement to make actually.
So that's been really nice to figure it all out.
And so obviously we have a lots of future work plans, like the main thing
is we want to scale up IL inference learning to these extremely large
scale models to verify that it could still do as well as back up in these situations.
We similarly have ideas about, you know, improving the way inferences
performance, because at the moment we're just doing this gradient descent
on the free energy, but in practice, this is really just a case of arbitrary,
like standard posterior influence.
And so we can use like message passing algorithms to figure this out.
We can also look at like where the causal influence works in these models.
And, you know, understanding better the conditions when IL does better
than back up essentially.
And finally, we should look at this probabilistic interpretation of this
network, whether it leads to better robustness and calibration, because
artificial neural networks are often really bad at estimating their own
uncertainty, but because we have this full probabilistic characterisation
of the network, maybe we can do better at estimating the uncertainty
than standard back up and which aren't trained to do that at all.
So thank you very much.
And that's my presentation.
So I'm just asking.
