Okay, so I have a pleasure to introduce Dr. Baron Milic from the MRC Brain Networks Dynamic
Unit at the University of Oxford.
Baron received his Doctorate in Machine Learning and Computational Neuroscience from the University
in Edinburgh in 2021 and joined the group of Rafale Bogage at Oxford as a postdoctoral
researcher that same year.
Baron's work focuses on the intersection of machine learning and computational neuroscience
and in particular he's done a lot of work on several aspects of the free energy principle
and its process theories.
For example, he has critically assessed the standard form of the expected free energy
function and he has demonstrated how predictive coding approximates back propagation on a
large class of commonly used machine learning models, thereby showing that something close
to backprop might actually be more biologically plausible than generally thought.
And one thing that I want to stress in particular is that Baron is one of a number of younger
researchers who are putting a lot of effort into writing really good reviews and tutorials
and making the free energy principle much more accessible than it notoriously used to
be, which is a merit that can hardly be overestimated.
So Baron, we are glad to have you here and I'm excited to see what you're going to present
today.
So yeah, so I'm not here today to talk so much about my work on the free energy principle,
although in the discussion time I'm very happy to answer various questions about this.
I'm mostly basically towards trying to talk to you about my work related to predictive
coding and understanding how credit design works in the brain and how it relates to
different algorithms like back propagation of error.
And so this is sort of, I started this work in 2020 during my PhD at Sussex and have continued
on with it for the last like two years.
And so this is really sort of my most up to date findings and thoughts about this.
And so probably the paper that most of you will, if you know about it, you will know
is the predictive coding approximate backprop paper.
And so we'll go through that and also think about like, you know, further, you know, how
can we develop this and how is what can predictive coding say more about like how learning and
influence actually works in the brain.
So the way to start out is to think about, first off, what is credit assignment?
And so and why is it necessary?
So credit assignment is fundamentally the problem of your brain obviously has many, many, many
neurons.
And most of these neurons are quite far away from any kind of supervisory signal.
So like, you know, if you suppose you have visual neurons, most of the neurons in your
visual cortex aren't in direct contact with your retina, but yet still they need to figure
out basically, you know, when to fire, how to fire, what to do to actually produce useful
visual representations.
And so somehow information about like what to do at your retina, say, or, you know, at
the site of action must be somehow transmitted backwards through your brain.
So you can actually basically neurons deep inside your brain, which only connect other
neurons and don't have any direct supervisory signals will know what to do.
And this problem is fundamental for any sort of large neural networks.
And so this is fundamental machine learning.
And really it's the solution to this problem in machine learning by backprop, the back
propagation error algorithm, which is what essentially created all of like modern machine
learning and the fact that it actually works.
So this problem is fundamental.
And we still really have no real idea how the brain actually does perform credit assignment,
whether it performs backprop like our current machine learning networks, like our artificial
neural networks, whether it does something different.
My, I used to think it did backprop or approximated backprop, and now I think it does something
different.
So there's been some evolution during my research time, but anyhow, so that's the fundamental
problem I'm trying to tackle.
And I personally, I think this is like one of the most important questions in neuroscience.
It's really like a fundamental thing about like how the brain operates, but it's also
something that not many people study.
So like, that's actually quite a nice niche.
And as well as trying to understand the brain, this is also having answers to how the brain
could do this is actually very important as well for machine learning.
And so there's several reasons for this.
So the most obvious is really that like the brain is currently the only known like system
that actually exhibits like fully general intelligence like us.
And so if we understand how the brain works and understand like the fundamental principles
of learning in the brain, we could then implement that obviously in machines and potentially
do better than what kind of machine learning is doing.
Well, it depends on, you know, if you think how machine learning is sufficient for general
intelligence, then like it's not so important.
But if it's not, then it's obviously quite important.
And so similarly, like we think a lot of the basically the properties and algorithms we find
in the brain can actually transfer over to machine learning because the architectures
are actually incredibly similar.
And so there has been this amazing convergence in machine learning basically towards
artificial neural networks, deep learning, all this stuff is incredibly similar to the
the actual architecture of the brain, at least compared to, you know, how artificial
intelligence was before machine learning to deep learning took off.
There was like all these decision trees and tree search and logical rules and stuff.
And like, and various like SBN, none of this is actually related to what the brain does.
But the one thing that actually did work at scale is very brain like.
And I think that's very important.
And so it also means that we probably should expect a lot more transfer between like the
computer science, machine learning and neuroscience than we would sort of naively expect.
And so final reason is that personally, I think, and this is a controversial viewpoint
in neuroscience, that the brain is actually highly optimal.
And so it's well, very close to being optimal, given its constraints.
And most of these constraints relate to, you know, efficiency and also efficiency
in terms of energy efficiency in terms of space, but also efficiency in terms of like
learning speed and data in that the brain obviously has to operate with relatively
little data compared to sort of data, the art machine learning methods, where you can
like scan the entire internet and like download it and like, like, and your network see everything.
That's not possible for the brain.
It has to be able to do like adaptive behavior very quickly.
And so the learning principles that underlie the brain capability to do this could
actually be very useful for current machine learning methods as well.
So like, I'm going to give you a quick overview on how Backprop works.
So Backprop is the fundamental algorithm that does credit design in deep neural networks.
Probably like most people know this, I'm going to be quite quickly to go quite quick through here.
But if anyone has any questions like about this or anything in the talk, please just
like stop me and ask.
So essentially in Backprop, we imagine like we have a neural network.
So imagine these X's are like layers in a neural network.
And so this is typically, this is just like a simple sort of chain style architecture.
And so we define each or the activity of each layer, like X one, it's just some
function of the activity in there, zero.
And this goes through all the layers until we reach the output of the output.
We compute this loss function, which basically says like, how good is the output
of the network compared to what we want it to be?
And then given this, we can basically use compute gradients backwards.
And so the idea is we want to compute a gradient, say, of the weights like here,
given this output.
And what Backprop propagation does is using the chain rule of calculus.
It tells you that to get the weight update here, you could start here and go back one
and then compute this gradient and then multiply that by this gradient, multiply
that by this gradient and then get your weights.
And so Backprop is the sequential process of just computing the gradients, essentially
in the reverse order that the original computation happened.
And so like the first most obvious thing is if Backprop works so well in machine
learning, and like, basically, we know it works extremely well, then why doesn't
the brain just actually use Backprop?
And so obviously you could say like, maybe there are better things than Backprop,
the brain does that, that's very possible.
But also there's like serious problems with like a naive implementation of
Backprop given the circuitry we know about the brain.
And so several of these problems are one is that it's a fundamental thing in
Backprop is that there's this, there's this forward pass.
And then there is this backwards pass and the forward pass happens and then the
backwards pass happens sequentially.
And so this is not something that happens in the brain, in the brain, you have
essentially constant like influence of iterations recurrence of kind of like
transmissions going on all over, there's no like forward pass through your brain and
the backwards pass through your brain.
And so similarly, we have this other issue with sort of sequential transmission information.
So, you know, in Backprop to compute the gradient here, we first need to
compute the gradient here and then compute the gradient to the loss.
And so basically while, you know, the gradients are not actually computed.
So, you know, basically X2 here is like waiting for a bunch of time while all
these backwards gradients are computed.
And so this has to be done sequentially.
Whereas this doesn't happen in the brain, in the brain, everything appears to be
pretty parallel in that we can just like run sort of the brain, I guess.
And then everything is kind of, all the neurons are firing.
There's no like sequential sort of phase locked neurons, like first V1 fires,
then V2 fires, then V3 fires, and then on the backwards V3 fires, then V2 fires,
then V1 fires, like that's not what happens at all.
And so those are the sort of two main issues relating to parallelism.
And then there are other minor issues with the algorithm itself.
So like one issue is basically that the weight update is kind of non-local.
If you assume there's like a backwards set of circuitry to compute all these
neurons, then the weight updates here are used in the forward pass, but then they
have to somehow know about the gradients in the backwards pass.
So this sort of transmission from here to here is non-local.
And secondly, is that if we suppose we're going backwards, we need to know
what the forward weights are, but then the issue is obviously that in the brain
sort of axons like sign ups, all of this stuff is unidirectional.
So we can't go back through the same circuitry we came.
So there needs to be like this separate set of circuitry going backwards.
And then how do we keep the forward circuitry and the backward circuitry in
sync is essentially the problem.
And this is called the weight transport problem.
So there are various issues with backwrap.
And so relatively small, but like, I think quite cool.
Literature has grown up about like, how do we deal with this?
How can we make sort of biologically plausible backwrap or algorithms
to the approximate backwrap?
And so there's a whole bunch of them.
And so what really kicked off this field was this little crap paper from 2016
called random feedback alignment.
And the only thing this does is deal with the backwards weight problem by
basically saying that instead of computing the backwards weights with like
the same as the forward weights, you just take the backwards weights here
to be some totally random matrix.
And then mysteriously, that actually works quite well, at least for like
relatively small tasks.
So similarly, there's, you know, this other various set of algorithms
called equilibrium propagation, which comes out sort of of this group
by Yoshua Benjio.
And this is all about like update running the network in multiple phases
and then updating the weight of the difference between these phases.
And so that's one set of paradigm.
Another set is predict another paradigm of predictive coding, which is what
I've been mostly working on and will be telling you about today.
And so this sort of started out with this paper by James Whittington in
2017, where he showed that in some conditions, predictive coding can
approximate back up on these little chain like graphs.
Then obviously I did some work on this in 2020 and showed that you can
extend this to arbitrary graphs under different conditions.
And so this means basically any architect, any machine learning architect
you want, you could basically do back up on it with predictive coding
under certain assumptions.
And then my now collaborator, then we were, I guess we weren't collaborators.
We're only competitors either.
Yuhang Song showed like under different set of conditions are very
similar result also at the same time in 2020.
And so I'm going to talk about predictive coding network, but basically
the fundamental idea is it's an impredicted coding.
You essentially have like errors spread throughout the network.
And then these errors are sort of iteratively minimized, basically
locally minimize their own error.
And in doing this, you can show the sort of distribution of error that
emerges at the end can be made to sort of look very similar to the
gradients in back up.
And then because of this, obviously, if you have those gradients, you
can approximate back up.
And so another thing, which is very interesting is that recently, I'm
not going to talk about this paper, but recently I also have a paper out
showing basically how all of these equilibrium propagation contrast
of heavy learning and predictive coding can all be kind of seen as the
exact same thing.
And there's like a unified framework, which can explain why all of these
approximations to back up occur.
And so that's really fun.
And I have a paper on that, but I won't be talking about that.
I'll be talking about my other fun paper this year, which is about
essentially what happens when we don't try and approximate back up.
And so that's lots of interesting things there.
But for now, we're going to continue with how the back up approximation
works.
So as well as approximating back up, there are other things you can do.
So essentially, we don't like there were multiple ways to optimize things.
Basically, all you're trying to do is minimize some loss function given in
your network and back propagation is one way to do it.
Another way is this other algorithm called target propagation, which has
a really nice intuition, actually.
So the idea is that instead of back propagating gradients, so instead of
saying like, for each weight, how should you change to minimize like the
loss function, you instead propagate this thing called a target, which is
like, for each weight, what would be the sort of optimal activity at the
next layer, the sort of local activity for you to produce.
And then if you know that optimal local activity, then you only have to
update the weight basically to maximize, to minimize the distance between
like the current activity and then the optimal target activity.
And so the key difficulty target propagation is obviously computing
these local targets.
The way they're done is like the way to think about a local target.
It is the activity at a layer that if, you know, the rest of the network was
the same, and if the activity of the layer was somehow set to this value, then
the output, then that activity would propagate through the network, such
that the output of the network would actually end up being like the correct
classification or would minimize the loss.
And so this sort of forward, this forward propagation of the target gives
you sort of the intuition behind it, but in practice, what you can do is you
can actually complete this by essentially inverting the network.
So if you have, say, a target at the end, you know, you want the target and you
know, like the layers, then basically you take the inverse of that target
through back through the layer, and then that will be the target of the next
layer and then the target of the next layer.
And so this is essentially this equation here, where the targets at the
given layer are basically the inverse of the forward mapping from the layer
above, essentially.
And then basically you can derive these local errors between the activities
and the targets, and then you can update the gradients using only these
local errors.
And this is obviously a highly local update rule, but it has issues with
sequentiality, essentially, because you have to, like in Backprop, you essentially
have to sequentially go back through the network computing the local targets
one layer at a time.
And this is because obviously the local targets, a layer L, depend on the
local targets, a layer L plus one.
So it still has the sequential problem, but if you have these targets, then
the weight update is local.
So hang on, I think.
Oh, hang on, never mind.
So anyhow, let me just check, I thought I had a slide about this.
Yeah, okay, so I'm going to skip quite a way ahead, but this is essentially
just a way to think about how the various algorithms work, I've described.
So in Backprop in the middle here, we basically have a network going up.
So this is like the forward passive model.
And then we basically set gradient sequentially backwards.
And so this is how Backprop works.
TargetProp, which I just described, has activities to the forward pass going
forward through the model.
And then sort of these local targets, these inverses being sent backwards.
And so both of these have this fundamentally sequential structure,
although TargetProp has slightly nicer locality properties than Backprop.
And then predictor coding, on the other hand, it has activities being sent up
and it has basically errors being sent down.
But the key thing is that these errors essentially are minimized locally all the
time.
And so there are always local errors that are being minimized.
And so you can run all of these minimizations in parallel, unlike in Backprop
and TargetProp, where you need this sequential backwards pass, essentially.
And so obviously it takes some iterations of this, like, some iterations of this,
like running the equilibrium convergence process is required to take the errors
back from the output, all the way to propagate them through the network.
But it's not necessarily a sequential process, is the key thing.
So all these elements can run in parallel.
Right, so now we move that forward.
Let's talk in a bit more detail about how predictor coding works.
And so the intuition is behind it.
And so I'm going to, if you guys kind of know about sort of inference in the FEP
and stuff, you probably know a lot of this, but like, basically the fundamental
idea is essentially that your brain has to do inference, like it has to infer
the causes of sensory stimuli, very similar to the FEP.
And then obviously the brain, by doing this inference, you have to have
been a powerful generator models of the environment.
And so the question is, of course, how do you obtain powerful internal
generative models?
And so predictor coding says, basically, you can do everything by sort of unsupervised
local prediction minimization.
So at the start, you know, we have, you know, we're trying to predict our
sensory data and solely by trying and failing to predict our sensory data can
rebuild a very powerful generator model of the world.
So that's like this abstract idea.
And then there's, you can specialize this to like specific architectures by
saying, like, if there's a hierarchy, you know, of levels or layers somewhere in
the brain, then each will minimize its own predictioners and sort of make
predictions of the level low.
So essentially this ischeme can be generalized to a hierarchical structure,
which is necessary to actually represent, you know, like detailed causes and
like latent states and stuff.
And then it sort of as in terms of like the implementation, what this results
is, is the predictions end up being sent downwards.
So essentially from high level layers to low level layers, where the lowest
level layer is just like the sensory input and then prediction areas are fed
back upwards.
So essentially each layer can figure out, you know, what are, what are the errors
in its predictions going downwards and then update based on that.
And so there's a lot of evidence in about, well, I like, I always like these
hallucinations of, yeah, anyway, these visual issues, basically, but there's
a lot of evidence that your brain actually works in this sort of predictive
way that it actually contains predictive models of the environment and makes
assumptions based on these predictive models.
And so this seems very obvious kind of now, I guess, but like back in the day,
people thought, you know, the brain was just doing this feed forward
sensory processing and doesn't sort of just houses the visual scene into
features.
And then this can't, this sort of feature view can't explain why we have
various visual illusions.
So like a classic example of this is sort of this kinesia triangle example.
So even though you can basically see these white triangles on top, even
though like there's actually no like lines, they sort of see these weird
illusory lines.
And this is basically because your brain, the model in your brain, individual
system has a very strong prior about like lines being continuous and shapes
and all this sort of stuff.
And so this is basically making like top down predictions like there should
be a line here and normally there isn't a line here and that's generating
prediction errors.
But basically this showing the, the not just parsing, you know, bottom of
input, but in fact, you're essentially matching your input with a top down
prediction.
And so a similar thing about the idea of prediction, although you guys kind of
seen this because I've zoomed ahead, is that if we haven't, if we don't know
what's here, this just looks like a bunch of random dots and sort of
strangeness that you can't really parse this scene.
But if I show you that, you know, there is a dog here and then I quickly go
back here and then you can, in top with this exact same visual data in a much
more effective way and you can directly parse that this is a dog, because
essentially the latent variables in your visual cortex are updated and it's
like, I now know there is a dog here.
And then this prediction comes down here and then this back gives you a
phenomenologically different representation or visual perception of the
scene.
So there's lots of evidence in this way for this kind of model based predictive
processing.
And so this is like, that's basically the standard specifications for this.
And then nowadays in machine learning, there's huge amounts of evidence that
this is fundamentally a thing that works as well.
Like if you look at any of the large scale machine learning stuff, like
GPT-3 and stuff, this is all based on unsupervised prediction of sensory input
in GPT-3 cases, this is text.
And so this seems to be a very general principle about like how to form
highly effective generative models.
Right.
So now that's an abstract high level idea of predictive coding.
And so now the idea is to try and specialize this into designing an
actual neural architecture.
And so for this, we try and say as close as possible to like really basic
machine learning machine, like multi-layer perceptrons.
And so we can think about this predictive coding network essentially being
like this hierarchy of layers, like a standard artificial neural network.
And the key difference is that instead of just having like the active activation
activity, whatever, at a layer, we instead have both activities and we have
predictioners.
And so the fundamental idea is we want to minimize these predictioners
throughout the whole network.
And so the way we do this is we first minimize predictioners and then you
minimize weights and relative to the predictioners.
But like the key, the connection to the free energy basically comes from the
fact that this sum of predictioners, you can interpret this mathematically as
a variational free energy term and hence interpret what the whole network is
doing as variational influence.
And so really this predictive coding emerges naturally from the FEP.
If you make certain assumptions about the generative model, specifically you
assume that it's Gaussian and hierarchical and all these kinds of assumptions,
then you can exactly derive predictive coding networks from this high level
abstract FEP.
So we can kind of make this a bit more specific.
So predictive coding essentially assumes that you have this hierarchical
Gaussian generative model.
So what this means is we have these latent variables x0 to xn, which all
represent a layer in the network.
And then we can basically split this up into a hierarchy of layers.
So each layer only depends on the layer below.
And where each conditional distribution here is Gaussian, where with the mean
of the actual activity, so the prediction projected upwards from the layer,
and a variance of basically some variance, which doesn't really matter that much.
And so then if we do some mathematical tricks and we assume it's some
posterior variational posterior, we can basically show that the free energy as
is, you know, the fundamental object in the FEP and the fundamental object in
variational influence can be written as a sum of squared predictioners and where
the predictioners is defined between the activity to layer and the prediction
upwards from the layer below.
So the way to sort of visualize this is to think about basically that there are
multiple layers here.
So this is layer L plus one layer L and layer L minus one.
And in this normal machine learning MLP, these layers would just, there
wouldn't be any prediction error, eat it epsilon times here.
There's just these new terms and then this would be going up directly.
But here we basically introduced prediction errors at every single layer,
which represent essentially the difference between what this layer is,
what this layer is representing and like what higher up layers think
it should be representing.
This is the prediction error.
And then each layer basically optimizes its activities to minimize these
prediction errors.
And so personally, I think it's very helpful to write things as an algorithm.
So this is the sort of predictive coding algorithm very, very specifically is
what we do is we have this multi-layer network to do it, just make a prediction.
We fix the input nodes.
So basically we clamp the bottom of the network to whatever data you have.
And then you do run this, basically update the activity nodes to convergence
by minimizing prediction errors all throughout the network.
And then we just output whatever the prediction is at the output nodes.
And conversely in learning, the only thing we do different in learning is we
basically fix both the input to the data and also the output to the desired
data, aka the desired target.
And so then if we have this, we can update the activity nodes to convergence
and then update the weights given the errors at the equilibrium.
And so obviously basically in the prediction case, the only error coming
into the network is at the bottom in the light.
And then during this convergence case, this error propagates all the way up
the network slowly, but the key thing is that the error here can be reduced to
zero because basically there's only one constraint on the network.
And so it can always satisfy one constraint.
But with learning, there are now two constraints.
There's error at the input, error at the output.
Both of these errors showing convergence, they propagate throughout the whole
network, and then essentially they have to somehow balance each other.
And so the network will settle into an equilibrium where like changing the
activity in one layer will change, obviously, basically what it represents
to the prediction of the data, but also changes the prediction of the target.
And so both of these errors essentially match at equilibrium.
Then equilibrium, we can update the weights in such a way to minimize just
the basically the sum of the prediction errors at the equilibrium value.
So again, this is what the difference between predicated coding,
backfropping, targetfropping.
And so now we come to the first key result, which is how predicted coding
can approximate backfropping in general, so on arbitrary sort of computation
graphs. And so the fundamental idea here is that first off, we need to think
about what does it mean to do predictive coding on an arbitrary computation graph.
And so essentially in neural networks, we can think about like each node
here as being a specific operation.
So in the case of like multi-layer perceptrons, each node is just like a
layer and then this operation is just like the forward pass through that layer.
But like we can define arbitrary graphs, right?
We can add like another layer here and then like connect this up in some
strange way, however we want.
But the key thing is that these nodes are basically, can be thought of as sort
of the atomic operations.
And then the connections are like basically how these, basically how inputs
get fed through the atomic operation to like the next set of input, the next
sort of set of representations.
So basically the sole trick we need to do to turn any sort of arbitrary
like computation graph like this into a predictive coding network is we just
need to add our neurons to every, well, our neurons, our units, let's say,
to every single node on this computation graph.
And so basically what this represents is the difference between like the actual
value, this come, based on the value going into the computation.
And then they're different from that.
And basically the predicted value going into the computation.
And so the idea is that this, you can build networks of these things and then
run predictive coding as before.
And then the sudden cases, this actually become exactly the same as backprop.
So essentially the key prediction, well, the key result is that at the equilibrium,
you can show that the back propagation grade, the errors at equilibrium are
approximately the same as the back propagation gradients from the loss all
the way to the actual activity at that layer.
And so the way to think about this is basically that the neural, the predictor
coding will essentially distribute the errors throughout the network to find the
place with the smallest error.
And then the place with the smallest error in undecided conditions is also very
similar to the gradient of the error at, of the loss function at that point in
that if we change the error a bit, then basically we'll, that change will
propagate through the whole network and cause a change in the loss.
And then that is equivalent to the backprop gradient.
And so mathematically, you can essentially show that this, that the equilibrium
activities have the same reclusive structure as backprop.
So basically this is just a bit of algebra here, but we, this sort of is the
key line here, where we show that the errors at one layer are just equal to the
errors of the layer above times basically this derivative here.
And then this is exactly the same as the structure of backprop where errors are
replaced with these deltas, the gradients.
And so if the output layer, if the errors of the output layer are therefore the
gradients, then recursively the errors of equilibrium are always the gradients
all the way back.
So that's sort of that general result.
And you can show basically this is actually the case.
Convergence is quite easy, although it takes a while of simulation and that
it's basically you can train your networks with this stuff.
And so this is what I was doing in 2020.
And so there's also, you can think about various different conditions under
which this holds.
And so basically the, the short end, the sum of it is that this holds as long
as your gradients, your activation, your, your activities at equilibrium remain
close to the activities in the forward pass.
And so essentially the way to think about this is that here we can have these
unit terms relating the activities of the forward pass to the, the errors to
the equilibrium errors, but these are really the activities at equilibrium.
And so if the equilibrium diverges quite a way from the forward pass, then it
won't be the same as backprop which only uses the forward pass.
So the other way to think about it, if the predicted coding, the
equilibrium of predicted coding is essentially doing back propagation
through the equilibrium and not through the forward pass of the network.
And so if the forward pass of the network is close to the equilibrium, then
fantastic, we have got original backprop path, but if the forward pass of the
network is not close to the equilibrium, if the equilibrium is doing something
different, we're actually doing backprop through the equilibrium instead.
And so then the question is like, what does that actually mean?
And so this is the very interesting point I'm going to get you next.
But like, first we're just going to quickly have a survey of the literature
of how we can actually get it so that the forward pass is basically the
same as the equilibrium.
And so in the James Whittington paper, what he did is he said the ratio of
essentially the top down to the bottom of influence is small.
So the bottom of influence is essentially the forward pass going up.
And then the top down is the errors coming down.
And so if the errors are very small, then they won't influence, you know,
the actual equilibrium much.
And so it will stay close to the initialization in the forward pass.
And so mine is by basically setting the layer-wise grading to close the
free forward pass values, which will keep everything close to the equilibrium.
And then similar, you hang song showed in 2020 that if you just directly use
the first step of change, not the equilibrium anymore, but just the first update,
then this will be as close as possible to the forward pass and say we'll get
back up again.
Right.
So that's sort of what we've been working on about how predictive coding
actually approximates back up.
But essentially the key thing is that we know predictive coding.
These conditions are all kind of unsatisfying to me at least.
So this is the motivation behind this work.
These conditions are all kind of unsatisfying because if the forward
passes close to the equilibrium, then basically the equilibrium isn't doing
anything.
And what this means is that the predictive coding network is basically
not doing anything different from back up.
And it's kind of forced to do that in a kind of artificial and weird way.
And so we don't really don't want this to happen.
We actually want to be able to sort of use the full power and flexibility
that predicted coding this influence equilibrium stuff gives us.
And so then the question comes to then the question becomes if the
influence equilibrium is different from the feed forward pass, then what
does backfropping through the influence equilibrium actually mean?
So this is this basically to develop a new algorithm, which we call
inference learning.
And so essentially the idea is exactly what we said.
We first give and fix the inputs, the outputs of the network and compute
this equilibrium in the middle of all the activities, which we call the
inference phase, because in mathematical terms, it corresponds
exactly to possible to variational inference.
What we're essentially doing is we're inferring the posterior of each layer
in the network, given the input and the output layer, layers being clamped
to whatever the targets are.
Then given this posterior, you then do learning basically to maximize the
sort of log likelihood of this posterior given the weights.
And so this does something actually quite different to backfropping.
And so then the question is, does it do anything better than backfropping?
Is there any reason to think it's a nicer algorithm?
And so the answer to this is always a yes, right?
So if it hadn't been if it was like a rubbish algorithm, then we
wouldn't have any papers written about it.
So there's also selection issue.
But the key reason that we think IL inference learning is fundamentally
better than backfrop relates to interference.
And so people don't really think about this in backfrop much at all.
And but in backfrop, what is assumed is that each of the weight updates you
make in backfrop are independent of all the other weight updates.
So essentially imagine you have some error at the output, some, and then
you obviously compute two gradients, the gradient of the error with respect
to weight one, gradient to the error with respect to weight two.
Then then you basically apply these updates in parallel.
And so they're independent of each other.
What this means is you can have interference between these weight updates.
So they're like supposing the error is quite small or the weight of
data is big, then basically what you have is two updates, which both
correct the same error.
And so this can lead to like overshooting behavior.
Well, essentially what happens is like you basically go because you update
two weights, they both sort of merge together and strengthen each other.
And then you actually end up with an output, which is two, gives an error
on the other side, too strong of a change, essentially.
So if we're indicating inference learning, doesn't basically reduce this
interference quite a lot.
And this is because during the inference phase, essentially the difference
activities of each layer, they're interacting with each other because of
their errors.
And so what this means is effectively that the errors are in predicted
coding are redistributed throughout the network flexibly and sort of
dynamically, depending on what the other bits of the network are doing.
And so this is a nice visualization from our paper we have on this, which is
basically trying to give you a simple example, different to the one I gave
you about how interference works.
So another place you can get interference is if you have two outputs of the
network.
So the idea here is we have some, some bear in some like sitting at the
stream and you know, it's, it's sitting at the river and it's expecting to
smell some fish, which it likes to eat.
And it's also expecting to hear, you know, the water, you know, running down
the river, but for whatever reason, like it's, it's, I don't know, like it's,
it can't hear the water, like it's air is like damaged or something.
I don't know.
It can't hear the water.
And so obviously this produces a prediction in it's expecting to hear
the water and it can't.
And so what this means is if you have this double layer output.
So basically the input is like everything it sees here, and then it
expects both the, to see the fish and to hear the river.
And then basically if it doesn't hear one of them, then it's placed like this.
If it doesn't hear the river, then there's an error here.
And then back from the property, obviously back propagate this error all
the way backwards through the network, right?
And then update all the weights to make it think it's less likely to actually
hear the river.
And so what this obviously does is it changes the whole network, even though
this output was actually correct.
It was actually expecting to smell the fish and it was smelling the fish.
But this output basically as soon as this network changes, then this output
itself will change again.
And so basically this by having, by this prediction of it failing to hear the
river, it will end up actually in the future predicting it's less likely to
smell the fish as well, even though that prediction is actually correct.
And this is because basically backup updates all these weights without
taking into account essentially what the rest of the network is doing.
And so this obviously causes this interference effect.
And you can see this would actually be important.
Like for these kind of animal examples where you don't want this kind of weird
interference between different sensory modalities, say, well, like prediction
as a one-modality effect, how you think about other things, other modalities.
It's kind of a strange thing to happen.
And so predictive coding basically solves this because during the inference phase,
obviously there's an error here, but this error gets distributed back through
the network, but it also gets distributed to the correct output.
And essentially it's saying like the correct output is given an error because
the rest of the network has lots of errors, there's no error here.
And so it distributes the error up to here.
And then what this means is it updates the weights to this output in a way that
takes into account the weight updates being made back here.
And so this basically maintains this prediction, even though the rest of the
network has changed out from under it essentially.
And so this interference effect actually has quite a large effect on your
networks in practice.
And so one of the things we've been looking at is actually does this make a
difference?
And the answer is yes, it does.
So if we, so one thing to think about is that this happens much greater for
deep networks and it happens much greater for, there's more interference at
higher learning rates.
And so this kind of explains why back propagation, you essentially need
really low learning rates to do anything in backprop.
Essentially because with high learning rates, these interference effects become
very large in that you're basically always overshooting the output because
and high depths, the high learning rates are always overshooting the output.
And people don't normally do like multiple output neural networks, but
this is also something that would happen a lot.
So that's one issue with backprop.
Another issue with backprop, which in the inference learning IL does better is
that IL actually takes shorter paths through basically through the weight
space to, to reach the correct output than backprop.
And so the way you can think about this is backprop makes local updates.
So we just take like a step in the best immediate direction, whereas for
all the weights individually, but what predict coding IL does is IL can
actually take steps directly towards the correct output.
This is because it can sort of accumulate information from the rest of
the network during this inference phase.
And so one way to think about this is we can actually look at this measure
called target alignment, which is basically the angle between the direct,
most direct path from input to predicted output to the target and then
the output after you've done some learning.
So basically after a wait step.
And so this is basically showing the dynamics of backprop versus IL learning
some point.
The backprop basically this, we start out here and we want to go to here and
backprop takes this really long sort of secures path through the weight space.
Whereas IL basically takes a much faster space, much shorter path through
the weight space, and this basically will result in faster learning as well.
And so we can show basically that for deep networks, there's more interference
and target alignment, this, this measure gets much worse for backprop, but
stays the same for predict configuration.
Similarly, perspective configuration can actually and certain cases train
a lot faster than backprop because age making these shorter steps and B, it
has, it's got less interference.
And so each step kind of counts for more because that's not having to undo
the effect of other weights all the time.
So similarly, we find that IL outperforms backprop in various other situations.
And so perhaps the most interesting is this small batch online training.
So it's well known that basically backprop is really bad at online training.
Online training is where you essentially just get one example at a time.
So backprop always needs like large batch sizes.
And this increases with the size of the network that you have.
And basically this is thought to sort of average out various kind of noise
in the data set, but it also helps reduce interference as well by, by
basically averaging out the interference you get from the independence of weights.
And so it's kind of crucial to predict occurring IL, there's much better than
backprop on this, it makes sense why.
But also this makes perfect sense because the brain can only do online learning.
The brain can't like look at a whole randomly selected set mini batch of
like a hundred visual scenes at a time.
It has to only, you know, it's confronted with what it has to post
is what it's confronted with at the moment.
And so in these kinds of situations, IL does much better.
Similarly, in the small data limit, when there's not a whole lot of data,
basically IL does better and learns more from individual data points than backprop.
Again, this is something similar to what the brain needs to do and is what
is machine learning is currently quite bad at doing because it uses backprop.
Similarly, we can also show interestingly that IL outperforms backprop on
continual learning tasks.
So essentially what this is, is you have basically various distributions of data
tasks, you have various tasks you have to do, the distribution changes between
tasks and then, you know, you learn one task and then you have to learn the
next task.
And usually what happens if you do this, is you have this thing called
catastrophic forgetting where when learning task two, the network will
completely forget how to do task one at all.
And this is really bad, obviously.
And this is probably not something, I mean, it kind of happens in the
way you obviously do forget stuff, but you don't have this complete
catastrophic forgetting that happens with backprop.
And so we hear each other basically, it's not great.
So these are different tasks here and basically performance collapses whenever
we switch to a new task, which isn't particularly ideal.
But what we show is basically that IL actually does slightly better and
covers a lot faster than backprop whenever there is a switch between tasks.
So we know that particular IL is better than backprop and continue learning.
And similarly, we can also show that basically IL performs equivalently to
backprop on standard machine learning training patterns.
So like this thing is as good as backprop when in, where backprop does really
well, such as, you know, large batch sizes were at IID data sets, but also just
better than backprop, interestingly in many of the exact situations where we
expect the brain to do better.
And so this is one of the reasons I think IL is super exciting.
And it's kind of much more similar to how the brain must work than obviously
how backprop works, because the key thing about IL is that it is a
fundamentally parallel and local algorithm.
Like you have this inference phase where all the network is converging and
this can be done for every single neuron in the network in parallel and
independently of all the other neurons where the information is exchanged.
Basically all the time about how the local prediction has changed, but like
you never get a situation where like neurons have to wait for what other
neurons are doing to send them, you know, how to update themselves.
This never happens in IL.
And this is why like, I think it's much close to what the brain is doing.
And obviously it builds off predictive coding and the very large amount of
evidence that predictive coding or something very similar to that is
implemented in the cortex.
So IL also has some relatively nice theoretical properties.
So like we can actually analyze it in the linear case.
And so I have a paper which will be coming out like this week, discussed
doing a lot of these kinds of theoretical analyses.
And another very interesting property is we can show basically that this
IL algorithm essentially is interpolates between what back propagation does
and what target propagation does.
And where target propagation is obviously computing these local targets.
What we can think about the IL algorithm are these sort of the errors
in equilibrium, the activities equilibrium, effectively kind of like
local targets in that there, how the network should update itself should
evolve over time in order to better match the data.
And you can kind of think of this itself as a process of Bayesian influence.
Where essentially what we have is we have a prior, which is where we
start in the forward pass.
And then we have some data given to us, which is like the correct
input target association.
And then we want to compute the posterior of that activity, given the likelihood
obviously, which is the data and the prior, which is the feasible pass
configuration.
And then this actually obviously corresponds to the posterior for the layer,
which is exactly what the equilibrium activities of the network actually
represent.
So another way we can think about this in the same sort of setting is that
IL basically competes these local targets, but with priors to make them stay
close to the feed forward pass initialization, which is obviously helpful
basically so you don't completely over fake your network to every single
data point that you get.
You want to kind of stay relatively close to what you're already thinking is
good, but then obviously also gain some advantage from using this local target
update rule.
So other things that we can show is that basically we know that IL can actually
converge to a minimum of the loss function.
So this algorithm is actually a reasonable one.
It's not actually will converge the batch as well as back up and it has very
similar convergence properties to back up.
And essentially the idea is we can think about IL is basically from this
variational EM algorithm, which it's also, we can think about this as being
related to the target pop again, and if we can think about minimizing the free
energy as basically minimizing a constrained version of the loss function
minimized by back up where the loss function is anywhere.
The sort of additional constraints are basically the stay close to your
feed forward pass priors.
And so essentially we know that IL will converge to back up.
We have lots of experimental evidence for this.
And so what this kind of means is that this IL algorithm is really nice.
It is completely power.
It's completely local.
And it's basically predicted coding, which is an algorithm we know is
has fair bit of evidence for being implemented in the cortex, or at least
it's like a very promising contender.
We know that this algorithm does at least as well as back up on like many tasks
we know it can converge to minimal for loss functions.
It can be applied to arbitrary machine learning architectures.
And we also know that it does better than back up in various situations,
which are actually relevant for the brain, such as, you know, the continual
learning and the, the small data, the online learning, all these kinds of things,
which is basically where back up doesn't do well, but IL does do well.
And so furthermore, it has very nice theoretical properties, like it reduces
these weight interference effects.
One thing I want to test out is whether it actually helps deep networks
be trained effectively.
So like in classic machine learning, you can have very deep networks, but like
typically you need to use residual network with resnecks because essentially
the issue is signal propagating through the networks really bad.
And so IL might be able to help solve some of these vanishing grading issues.
You can get with back up.
And finally, IL is also much more flexible at test time behavior.
So basically, I've not already talked about this at all, but IL, instead
of clamping the input and the output of the network, you can basically take
posteriors over arbitrary subsets of the network.
So you can say like, I want to clamp some, you know, layer three of my
network to something and then tell me what the layers, like, you know,
layer four does, but also tell me what layer three does.
And this kind of clamping is unique to IL.
I can't really be done with back up, back up train networks in a very easy way.
And so this could also be a way for like the brain, basically different regions
of the brain to interact with each other.
They can interact by essentially clamping up conditioning, sending basically
data to the end, to the inside of the network, not just to the input, output
interfaces, but to anywhere in the network.
And then they can sort of read out different queries you can make to the network.
So I mean, I think personally this is a very nice algorithm.
And so we've got several papers written on this at the moment.
And I think it's a quite a nice thing in that we can actually go beyond
just sort of standard backup and like, Hey, can the brain approximate back
up? Instead, we can say like, no, there are actually better algorithms
that the brain can use, which not only are easier for the brain to implement,
more natural for it to implement, have nicer, harder listen properties, locality
properties than back up, but also do as well as back up or better.
I think that's, that's quite a big statement to make actually.
So that's been really nice to figure it all out.
And so obviously we have a lots of future work plans, like the main thing
is we want to scale up IL inference learning to these extremely large
scale models to verify that it could still do as well as back up in these situations.
We similarly have ideas about, you know, improving the way inferences
performance, because at the moment we're just doing this gradient descent
on the free energy, but in practice, this is really just a case of arbitrary,
like standard posterior influence.
And so we can use like message passing algorithms to figure this out.
We can also look at like where the causal influence works in these models.
And, you know, understanding better the conditions when IL does better
than back up essentially.
And finally, we should look at this probabilistic interpretation of this
network, whether it leads to better robustness and calibration, because
artificial neural networks are often really bad at estimating their own
uncertainty, but because we have this full probabilistic characterisation
of the network, maybe we can do better at estimating the uncertainty
than standard back up and which aren't trained to do that at all.
So thank you very much.
And that's my presentation.
So I'm just asking.
