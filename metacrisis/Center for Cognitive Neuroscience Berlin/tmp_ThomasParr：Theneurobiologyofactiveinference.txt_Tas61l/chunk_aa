It's a great pleasure to introduce Thomas Paar today, we'll be about his background.
So he started medical school in 2012, I guess, at the UCL and did also like a bachelor there
with a focus in new science. In 2016, he started his PhD, I hope everything is correct, he started
his PhD working together with Carl Friston and Garen Rees. So I think Thomas is really like the
person who most frustrated me, when I was at the at the Philly, so a long time ago. I was fascinated
by like the idea of the free entry principle and Carl Friston and really started going, learning
about Boltzmann equations and some kind of like message passing and so on and over years and years.
And then in 2016-17, I realized that suddenly a number of papers were coming out, it became much
more accessible and on these papers there were always like one name, Thomas Paar. And so I looked
into that and I realized that there is one person out there who is so young and who plus
started his PhD, who was able to go all through this extremely complicated math and ideas so easily
that I thought I should really give up thinking about the free energy principle at all. And more
impressively, I think a couple of months ago, even like a book came out like about like Active
Influence, where he is like the first author on and I really can recommend this book. It's a
fantastic introduction to Active Influence, to the free energy principle. So it's a really,
really great pleasure to have you here tonight, Thomas, and I'm really looking forward to your
talk. Thanks for joining. Well, thank you very much for the very nice invitation and the very
nice introduction as well. First of all, can you hear me okay? Yes. And next thing is to see
whether I can successfully share my screen. Let's try this. So I'm assuming you can see the
PowerPoint and now hopefully in full screen. Perfect. Great. So today, I realized that you've
already had a talk from Ryan Smith previously, so I'm assuming he's given an excellent overview of
Active Influences as he always does. And I wanted to take the opportunity to focus in on something
a little bit more specific in this talk, which is thinking about how dealing with Active Influence
lets us think about neurobiology and how we can start to make connections and form neurobiological
theories from the principles. So I'll start just by giving an introduction that I think
will hopefully give everybody a little bit of a refresher on Active Influence and the basic
principles. And although there are some technical elements to it, I'll try to make it as intuitive
as possible. And it really doesn't matter if you get all the technical detail through this,
it's just to build some intuitions. So I tend to like to start talks with this slide, which just
shows two different sorts of system. The one on the left is one that just gradually diffuses over
time, sort of loses form becomes progressively less interesting. Whereas the one on the right,
despite having the same amount of randomness built into it, manages to maintain its form over time.
And a useful starting point in thinking about Active Influence is thinking about
what the difference between these two sorts of systems are and how the one on the right is able
to maintain its form and resist the effect of these random fluctuations that are forcing it
in all sorts of different directions. And it's often useful to reformulate this in terms of the
probability density or the change in the probability distribution of all of these little particles
over time. So again, you can see on the left, we've got something that's just diffusing out into
nothing. And on the right, we get something that behaves from a probabilistic level pretty much
statically. And it's really those on the right that we're interested in, those biological creatures
are able to maintain their form, are able to resist the effects of what the environment does to them.
Now, if we sort of write down the kinds of dynamics that we'd need for a single particle
or a single part of the system to try and maintain this form, what we get to is a system that I think
quite intuitively always moves when it can from regions of low probability to regions of high
probability. So if we interpret this final distribution it gets to as some steady state,
all we need to do to maintain that in a random environment is to always try and climb uphill
at the probability gradients. And then almost by definition, we end up spending more time in
highly probable states and less time in more improbable states. And this is sort of the starting
point really for active inference, because we now have this notion that we're climbing probability
gradients and we have some distribution that we're effectively trying to maximize.
And we can link that back to ideas like the Bayesian brain, the idea that the brain is using some
model of the world around it to generate predictions. And then is drawing inferences about
the data that it actually obtains, so sensory information coming in through our eyes, ears,
through our skin, that we can then use to draw inferences about the causes of those data.
So here we've got the idea that there's some states of the world X that are causing some
sensory data Y. We're then forming some posterior beliefs, so beliefs about the causes given with
the data. And we're doing this using a generative model that comprises a likelihood in a prior,
so prior being how plausible are the things out there in the world before we've
made any observations. The likelihood being how likely the observations we've made are given
the states of the world or given our hypotheses about what caused them. And together we refer
to these two things as a generative model. Now the posterior distribution is what happens when
we invert that model where we find the probability of some causes given the data.
And the final term we've got at the end here is referred to either as an evidence or a marginal
likelihood. And that's because in the context of Bayesian statistics we often use a marginal
likelihood as a measure of the fit of a model to the data that it's trying to explain. There's
how much evidence do those data afford some model of the world. And together we can think of these
as being an inversion of the generative model. Now the reason I've put this here is that we
can see that the dynamics that I'm showing in the upper left can be interpreted as the process of
maximising the evidence for some model, maximising the fit between the brain's model of how the world
works and how the world then engages with the model or engages with the brain by presenting it data.
And broadly there are two ways of doing that. The first is, as we've already spoken about,
it's changing your beliefs based upon new data such that you get a better fit to the world.
The other way, which I think is one of the key ideas in Active Inference, is that you
generate actions based upon your beliefs that change the world to make it more like your model.
And this is sort of the key idea that underwrites Active Inference, that all we're trying to do
is maximise the evidence for some model of our world. And we can either do that through
perception by changing our model or through action by changing the world. But together,
they come under one single objective, sometimes referred to, particularly by people like Kevin
the Acapoway, as self-evidencing. Now I want to go a little bit into the structure of generative
models and specifically the ways we can think about and the ways we can notate generative models,
because it often helps to move from the slightly more mathematical abstract description to a more
graphical notation that I think often gives a much better intuitive sense of what's going on.
So to do that, I'm going to start with this idea that model evidence is effectively what we get out
once we've integrated out all of the causes from our model, by which I mean if you take
account of all of the things our model predicts and then we take into account the prior probabilities
of all of the things that are causing those data, we can then work out what the probability of the
data are under that model. But the model itself takes account of the things that are being generated,
so our sensory data, and also the things that are causing them, simply a joint probability
distribution involving all of these things. So I'm now interpreting this graphic up on
the left as depending upon some generative model where we've effectively integrated out everything
that we don't need to determine explicitly. Now generative models will often have some
interesting structure, and in fact if they don't have interesting structures then they're not
interesting generative models. And normally that structure manifests as a factorization
of this joint distribution, but not everything depends upon everything else. And so we can
often factorize it, and here I'm just showing a completely arbitrary factorization of some
generative model where we have certain dependencies, so y depends directly upon x1 and x2, but not
directly upon x3 and so on. And the reason it's useful to think about this factorization is because
we can then express a graphical version of this model that provides a bit more intuition as to
what's going on. So the way we do that, or one way that we can do that, is using something known as a
factor graph. And the idea is that we take each of these factors in turn, we draw a square,
and then we draw an arrow to whatever's on the left of the probability factor we're interested in.
So here an arrow towards the y, because we're saying the probability of y conditioned upon or
depending upon the other two things, and then we attach those to the same square. We then move
on to the next factor, draw another square, and we then attach the relevant variables together
in exactly the same way. And we carry on doing that until we have a picture of what our model is
like. So now instead of having to look at this equation, you can look at this and say, okay,
well x3 causes x1, x4 causes x2, and together x1 and x2 relate to or together generate our data y.
This is just to demonstrate several models you may be familiar with generally, and the fact that we
we're often implicitly using these kinds of generative models without necessarily thinking
about it. So when we're performing a principal components analysis, we're often taking some
distribution of some variable x, we're then mapping that to a higher dimensional space y,
and that's our sort of model of how the data that we're working with are generated.
So what we can then do when we perform a principal components analysis is take our
high dimensional data y, work out how you go back from the y to the x, and that gives you all of your
specific principal components and the directions that have been stretched in various different ways.
Same principle applies for things like canonical variance analysis, where you've got some set of
variables then mapped to two different sorts of data by stretching them and
distorting them in various ways. And the challenge is how do you get back to the common
hidden variable common cause for both of those datasets. And we do the same sort of thing with
something like clustering analysis, where here the model says we're effectively sampling from
one of several different clusters with different probabilities. We're then generating some point
in space depending upon which cluster it falls in. And the process of inference is again undoing
this. It's finding the posterior by taking a point in a cluster and saying, well, which cluster is
that from? So going from the y to the s. So that's a sort of brief introduction to the idea of
generative models, the the role they play in active inference, some key examples and some of
the notation that I'm going to use as we go forward. What I've said so far has been relatively
abstract. And in the next section, I want to try and work from those sort of general principles
and these slightly abstract notions through to something more specific and more neurobiological.
Now, the first thing we're going to do to get there is to introduce one more abstract concept,
which is that of a Markov blanket. A Markov blanket comes up in all sorts of places in active
inference, but I'm going to use it in a very specific way here. And this is to talk about
conditional independence. And what I mean by conditional independence is that if you have
two different sets of variables, so here I've noted them as the mu and the eta,
the Markov blanket B is the set of set of variables that render the other two completely
independent to one another, that mean that if you know everything about the blanket,
knowing something about mu tells you nothing new about eta.
To give you an example of this, one of the examples that most people are familiar with
is the idea of a Markov chain, where you have a sequence of events in time. So something that
happens in the past then influences the present, which then influences the future with no direct
influence from the past to the future without going via the present. So if I know everything
there is to know about the present, knowing something about the past tells me nothing new
about the future and vice versa. So in that sense, the present is the Markov blanket that
separates the past from the future. Now, this is a very useful concept when we're dealing with
graphical models or generative models that have some interesting factorization structure,
and I'll try and explain why. The Markov blanket is often referred to as or can often be identified
by identifying variable we're interested in, so let's say we're interested in the variable x2,
then you find the parents of x2, so the things that caused it, the children of x2, so the things
it causes, and the parents of its children, and that gives you the Markov blanket of x2.
Now the significance of this is that knowing this means that if we know about the blanket,
it doesn't matter what else is going on in the generative model, you know, this might be some
tiny section of a huge generative model with many, many variables, but we don't need to know about
all those variables, all we need to know about are the blanket variables, so we can effectively
ignore everything else if what we're interested in is x2. Now, how does this then matter for
neurobiology? Well, I think the answer to that is that the brain is an extremely sparse structure,
that synaptic message passing does not involve connections between every different neuron in
the brain to every other neuron in the brain. There are a small number of, relatively speaking,
of synapses between any neuron and its neighbors or neurons elsewhere in the brain,
and so we can make an argument that if the brain is performing inference about lots of different
variables, all it needs is to know the structure of the generative model and the variables that
sit in that Markov blanket, or at least the neurons that are representing those other variables.
And that tells us that we can then, when we're inverting that model, when we're performing
inference, what I'm showing here is just one example of an inference scheme, we can, we can
express the dynamics of that neuronal message passing in terms of connections from the other
neural populations that are representing the variables in that Markov blanket. The details
of this aren't that important, but this is one example of an inference scheme known as variational
message passing, here formulated as a gradient scheme in terms of some dynamics, which has
relevance for modeling neural populations, we're interested in how the dynamics of those
populations evolve over time. And we can sort of do this recursively, we can couple together
lots of different parts of this system, where the things that are being connected up are just the
Markov blankets of different populations. Here I've expressed it in terms of an error term,
this epsilon, which effectively represents the gradient of a free energy objective,
which is used as an approximation to that marginal likelihood or evidence that I spoke
about earlier. And then some beliefs about or expectations about that variable, so we can now
use our, the errors in our predictions to update those variables. And once we've written down the
dynamics of this sort of system, we can simulate things like firing rates, we can simulate the
dynamics and the behavior of these networks of neurons passing messages between one another
as a means of performing a form of inference and thus self evidencing.
I know that this is probably still seemingly relatively abstract, but I think what will
hopefully help is if we then go through some examples thinking about specific kinds of
generative model and how that might then affect the kind of message passing we would see in a
brain. And the first step to thinking about that I think is very important when we think about
really useful generative models is they will all typically have a temporal aspect to them that
as biological creatures we deal with things that evolve in time. And so it's worth thinking about
how do you formulate a generative model that has that kind of dynamic aspect to it.
And there are several different answers to that and one of them is we use something known as
as a Taylor series approximation. So we start by saying okay at the current time what is the
value of some variable x in the world and there's maybe some continuous variable it may be
where my arm is in space. If we then want to know how it's going to evolve in time we could then say
well let's take the next element of our Taylor series approximation that's taking out of its
velocity and that tells us a little bit more about the trajectory that my arm might be on.
We can then take another value which is the current acceleration of that position and we get
a slightly better approximation to the trajectory. And the more terms we add in the greater approximation
we have of the trajectory or the greater representation we have of the trajectory just as
a series of numbers where those numbers are my current position my current velocity acceleration
etc. So we could formulate a generative model by trying to predict each of these coefficients
of this Taylor series or each of these generalized coordinates of motion as they're sometimes referred
to. An alternative is we just say at time one where will I be at time two where will I be and
we simply represent it in terms of the sequence of points over time. Often when we're discretizing
time in this way we often we also discretize space so we might say okay where am I in some
discretized scheme am I in location one two three four or five at each different time point
