and so we have two different ways of accounting for how things might evolve over time and each
of these can be useful in slightly different circumstances. So if I were dealing with a
sequential decision-making task it may be much more efficient for me to say okay at the first
move I'm going to make is this then the second one is this the third one is this and dealing
with something very sequential that might also be important in in sort of language processing
where you have to think which words you're going to put together in a sequence and we're dealing
with discrete variables over time because words are categorical instead of opposed to
continuing as opposed to lying on a continuum. However if I were dealing with movement or
the responses I'm getting from some photoreceptor in my retina it may be much more sensible for me
to be working in continuous time thinking about how things evolve at a continuous scale.
So what I'm showing here is an example of a generative model and the associated message
passing scheme when formulated in terms of this continuous time scheme. So what we've got here
is a factor graph at the top this blue element that uses exactly the same sort of formulation that
we've been discussing so far so we have some variable here which might represent our position
which then predicts our velocity at that time which then predicts the acceleration at that time.
So we have a series of predictions here about the coupling between different orders of motion
at each point those are predicting some sort of data here are wise which represents the
the current position of our sensory data the current velocity the current acceleration.
What I'm showing lower down here is the message passing scheme we get when we take account of
the Markov blankets of each of these variables so the Markov blanket here for the the velocity
would include the position and the acceleration but also the data I've got about those things
and some prior beliefs about about other variables in our model and so we'd end up connecting those
things up and although it looks like a bit of a mess of connections it's still fewer connections
than the total number connections you would get if you match everything to everything else.
What we've got here takes the form of effectively a predictive coding style scheme
which people may be familiar with and the idea that that you can you can hierarchically predict
predict some data and you can predict the thing that's predicting that data
those data and by observing the errors in your prediction of the data you update your prediction
which then cascades up a hierarchy allowing you to update subsequent predictions.
What I'm showing you here is the equivalent model formulated in terms of discrete time and in this
case discrete space as well so here we have a state at time t minus one which generates a
state at time t which generates a state at time t plus one at each of those time points we're
generating some observable outcomes which are represented as the o's with a variable pi here
that represents alternative trajectories I could I could pursue so alternative policies or plans
alternative ways I could change the sequence of events and again we have underneath the message
passing scheme that could perform inferences about this sort of model and you can see that the
message passing scheme to some extent looks like an inversion or or as loosely a sort of mirror
image of the generative model because all it's doing is taking each step that was performed
to generate some data and then inverting those steps one at a time so it's just going backwards
from the data to arrive at the causes of those data. Now what I've got shown here on the left
is just a sort of cartoon image of some of the key connections in in cortical micro circuits
so here we've got superficial pyramidal cells as spiny stellate cells as SS deep pyramidal
cells as DP and inhibitory engineer arms as the double eyes and this is clearly an oversimplification
of what is a very complex connectivity structure but the reason I'm showing this here is that
if we know something about this connectivity structure if we know about some of the key patterns
we can start to ask how can the the sorts of models that we've spoken about over the last
couple of slides then be interpreted in terms of the micro circuits that could help
could help implement these in a biological system. So what I'm going to show in the middle
is an example of a continuous states-based predictive coding style model that has all
the same elements as in the slide I showed you before but are now mapped so that some of the
connections coming in and out of it loosely map to those associated with a known cortical anatomy
and we can do exactly the same thing here with the discrete states-based model and think about how
we can assign those roles associated with the roles of different cortical layers
and then use that as a way of forming hypotheses. Now what I'm showing you on this slide is not
necessarily the last word on this and you could formulate several different hypotheses about how
these two things map together and that's where a lot of interesting science happens. This is just
one example of how we can associate what we know about cortical microcircuitry and anatomy
with what we know about the anatomy of message passing for some simple generic forms of generative
model. What I'm going to try and do over the rest of this presentation is to take the connections
coming out of this out of each of these points and to try and think about how those might or the
roles they might those might play when we take an active entrance approach and the sorts of structures
in the brain that might be involved in dealing with those dealing with those things. I'm going to
start by talking about movement and reflexes so here the kinds of things we're interested in
are the connections that leave the cortex and go to areas like the spinal cord in the motor
neurons, the predictions that we can then make about what we're seeing here in the central
plot and the G that's been circled in red is a prediction about the sort of data I might expect
to observe which may here be proprioceptive data consequent on particular sorts of action.
And we can think of this quite simply if we think about the structure of a reflex arc.
So what I've got here on the lower part is just the same marginal likelihood or model evidence
that we've dealt with before this idea of it being a measure of the fit of the model both in
terms of its accuracy, the accuracy with which it predicts some data and the simplest least complex
version of the model that will work for that. What do we do when we change the data? Well we try and
make the data more accurate in relation to our model by trying to align the data with our predictions.
So when we think of the structure of a reflex arc where we can interpret it as some descending
prediction coming from the motor cortex saying what is the proprioceptive data I expect to
observe. By comparing that with the proprioceptive data that's actually coming into the spinal cord
through the dorsal horn with that prediction we then get a potentially a mismatch between the two
that can be used to generate action via the ventral horn generating movements and contractions and
muscles that then try to fulfill these predictions. I'll show you just an example of a simulation of
an arm using active inference that has these sort of reflex arcs built in and all we're doing here
is we're just tapping on the biceps tendon where the red arrow is pointing and we're just seeing
the reflexive response that we're getting as a consequence of effectively inducing this additional
piece of sensory proprioceptive information by stretching that tendon. We induce the mismatch
between the prediction that's coming down and the data that's coming out and that
is corrected then by generating this additional movement. Interestingly we can do things by
manipulating the confidence of those predictions and sometimes get things like bigger reflexes
and hyperreflexia as the sort you might see in clinical populations with spinal cord injuries
and so here you can see the reflex is slightly larger than it was before and slightly brisker.
I'll show you another example of this same idea where now we've developed a mapping between
the microcircuit and the anatomy of the ocular motor brainstem so here starting from the superior
colliculus the the rostral interstitial nucleus of the medial longitudinal vesiculus and there is
other centers and the cranial nerves that are responsible for generating eye movements so cranial
nerve three and cranial nerve six here are seen here as resulting from these error terms so we're
predicting how I would expect my eyes to move and any error in the current position of the eyes is
corrected by generating eye movements so by inducing different priors on top of this model
of different predictions about where I'd expect the eyes to be we can actually then generate
different sorts of eye movements using predictive coding style active inference scheme where the key
thing is that the predictions can be fulfilled by actually changing the positions of our muscles
and positions of our eyes so that sort of provides a very brief overview and a couple of examples
dealing with the role of reflexes and predictions in generating movements but it doesn't really
tell us anything about how movements are chosen how movements are selected and you don't necessarily
get any intelligent behavior out of that and for that we need to start looking at different sorts
of structures and different parts of this generative model and here the key thing I want to focus on
is the role of the basal ganglia and which here we're going to associate with the computation
of something known as the expected free energy and I'll try and describe what that is a little bit
more in some of the subsequent slides so what I'm showing here is a mapping of some of the
subcortical but subcortical anatomy or more actually I should say that this is a mapping
between parts of our Bayesian message passing scheme parts of the model inversion when we're
dealing with the ability to plan and make decisions to some of the known subcortical anatomy of the
brain and the key thing I want to focus on is this G that is depicted as part of the as part of
the stride and so the things that are feeding into this are what we're getting from the cortex
here which is some prediction of the outcomes we'd expect if I were to pursue a particular
cause of action which is this subscript pi and the another sort of error term here which is how
far away are those predictions from my preferences about how the world should be my prior beliefs
about how the kind of data that I would actually actively seek out and act to get together those
are used to calculate our expected free energy which we can then use to formulate beliefs about
policies by saying that the policies we would select the plans we would engage in are those that
we would expect to be associated with the lowest expected free energy and to put that more formally
we're saying that we're going to give a prior belief that the policies the series of actions
we're going to choose are going to be those associated with the maximum information gain
tell us the most about the world around us that fulfill our preferences and then we're going to
add in an additional term here which deals with habitual type policies and things that we tend to
do because we've learned we behave in a particular way in those situations now the combination of
this information gain and preferences are often referred to as an expected free energy and I won't
go into the details here but that's simply because you can rearrange them mathematically to make them
look very much like the equation for a free energy or or marginal likelihood approximation
with an expectation around them to say that these are the what we would expect given
our predictions about the data we would obtain because here we're dealing with beliefs about
the future plans into the future where we haven't yet had those data and we need to deal with the
expectations of what those data would be under different plans that we could choose
and so here we're showing just an example of the direct pathway through the basal ganglia
which is normally thought to facilitate movement and depends upon this expected free energy
and an indirect pathway which here we've associated with these kinds of habitual drives and I'll
come back to those a little bit later on when we deal with hierarchical models
just to give you a little bit of intuition as to the information gain aspect of the expected
free energy because I think most people most people probably understand this idea of seeking
preferences and behaving to maximize some degree of reward but many people are less
familiar with the idea of seeking information into the way that that might manifest in these
kinds of models so as an example I'm going to show you just a simple simulation where we're
going to manipulate some of the different aspects of the uncertainty in the model so here
in the upper left I'm just showing a way of parameterizing the the uncertainty associated
with the data generating process so this is our likelihood precision on sensory precision
and it effectively says that when this precision is very high we can be very certain about the
outcome we'll observe given a particular state of the world whereas when it's very low we could
predict everything with very similar probability and we can do something very similar by manipulating
the uncertainty in the dynamics of the world so again when this is very high it means that
where I am now is very highly predictive of where I'll be at the next step in time
whereas when it's very low it means that pretty much anything at the next time point is is equally
probable and I'll show you a simple simulation where these two things are manipulated just to
try and give you an intuition for what it means to act to to maximize one's information about the
world so the upper simulation here top left shows four panels which can each change at some point
in time to a different color with some random probabilities and the blue line here is designed
to show effectively an eye tracking trace so it's a simulated agent who is allowed to choose which
of these panels it wants to look at at any one time and you can see it samples them with a
relatively even frequency in the middle panel what we've done is we've reduced the precision
or increased the uncertainty associated with the likelihood in the lower left that effectively is
like turning off the lights in that location it's effectively making that lower left location much more
much less informative much more noisy and so effectively what what we've got here is a system
that then ignores that it says that I can't get a good quality high quality information from there
so I'm going to look at all of the other locations rather than rather than this in the lower left
an analogy for this is if you're thinking about how you perform a scientific experiment you would
probably aim to use if you had a choice between two different measuring instruments you would
choose the one that gives you more precise measurements rather than the one that gives you
very noisy measurements in the lower left we've manipulated the uncertainty or the volatility
associated with the dynamics so here what's happened is the upper left location is associated
with much more uncertain dynamics so here what happens is that I end up sampling that upper left
location much more frequently and intuitively this makes a lot of sense because if you've looked
somewhere very recently normally you will you'll know a lot about what was there you don't need to
look back there anytime soon however if it's very volatile if it changes quite with some degree of
randomness and you have very little certainty about the state of it after you've looked away
you'll look back with a much greater frequency so we've effectively decreased the inhibition
of return in this location by increasing its volatility or decreasing the precision associated
with its with the dynamics in that location so the end of this was just to show you this sort
of emergent behavior just by having an information seeking objective by by having a prior belief
that we're going to act to minimize some expected free energy one component of which is to maximize
our information about the world now the next thing I want to come on to is the role of hierarchical
generative models and one of the key benefits of having a hierarchical model is that we can now deal
with things that evolve over a range of different timescales so you might have some things that
evolve very slowly and some things that evolve very quickly and to some extent we can separate
out two and use slowly evolving things to to to help us predict what's happening at the faster level
and here the key things to to note are all of these connections between higher cortical regions
and lower cortical regions which manifests both in the in the discrete and continuous state space
models and so it's worth them thinking about what the role of these are and how that manifests in
terms of the generative models we've been dealing with before and it's really this that links back
into the the idea of predictive coding as many people know it and you've probably seen graphics
of this sort in the past where we have a range of cortical regions that are each making predictions
about the others and so here going from going from the right to the left we've got a series of
predictions in as these dark black lines with prediction errors passed back up using these
these dashed lines that then allow us to correct at each level and all this this graphic shows
is a simplification of the graphic on on the previous slide so i want to give you an intuition
for why this is useful using a couple of examples so the first example i'm going to use is one from
the domain of active vision so imagine imagine you're doing a task now where you have to fixate
on this cross in the center and maintain that fixation and show you a stimulus but maintain
fixation on the cross. Stimulus is going to disappear and then your next task is to perform
an eye movement to the location where the stimulus appeared. It's a very simple task if the sort was
used frequently in monkey electrophysiology and all throughout neuroscience but it's an interesting
one because it has several different components to it that i think help in terms of thinking about
the utility of hierarchical models so one aspect of this is making decisions about
where you're looking at each of those time points and making inferences about which
of several alternative locations you're going to perform an eye movement to so simple form of
