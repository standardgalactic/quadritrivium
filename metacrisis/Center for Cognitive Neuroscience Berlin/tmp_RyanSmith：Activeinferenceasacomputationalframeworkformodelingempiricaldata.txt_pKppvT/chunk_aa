So, today I have the pleasure to introduce Professor Ryasmith from the Larrouette Institute
of Brain Research, the University of Tuzla, Oklahoma in the USA.
Ryasmith has received his PhD in Psychology from the University of Arizona in 2015 and
worked as a postdoctoral researcher in Arizona in the lab of Professor William Kilgoe until
2018.
He then spent a year in the lab of Professor Carl Friston as a visiting researcher and
in 2019 he is a member of the Larrouette Institute of Brain Research where he is still working
as a principal investigator.
Ryasmith is the author of many publications in the field of effective neuroscience, neuroimaging
and models of the brain and he offered a great contribution in the development and application
of the free energy principle and active inference framework which are the main topics of this
semester's seminar series and together with Carl Friston and Christopher White he recently
published a fantastic tutorial on active inference that I would like to recommend to anyone that
is approaching like me and many of us to this topic.
So Ryan we are really glad to have you here today and I'm really looking forward to your
talk.
The mic is yours.
Okay well yeah thank you very much for the kind introduction I'm super happy to be here.
I do have basically exactly an hour worth of material if I can even get through all of
it so I'm just to prepare everybody and feel free to just you know kind of cut me off if
I start going too long but I'm hoping I can get through as much of it as possible.
So let me just share my screen here, make sure this all works okay okay so yeah so like
I said so just kind of jump in here my talk officially is titled active inference as a
computational framework for modeling empirical data and the reason I say that is just because
my lab mainly is focused on trying to apply active inference models as a way of essentially
studying individual differences in behavior and largely in computational psychiatry so
and in clinical populations so part of my goal is to kind of illustrate how active inference
can be used to model empirical tasks that can then allow fitting and then you can study
individual differences in particular computational mechanisms so the general kind of outline I'll
try to give a brief kind of conceptual introduction to active inference and then I'll introduce
you and walk you through the formalism which can initially be fairly daunting but I'm hoping
that kind of by the end it'll it'll seem a lot more comprehensible so that'll include the way
that perception relates to variational free energy minimization and how that relates to
prediction error minimization. We'll talk about policy selection and expected free energy minimization
and then hopefully if I have time we'll get to active learning and then I'll briefly kind of
touch on empirical applications throughout so kind of highlighting how active inference can make
both behavioral and neural predictions that can be tested in empirical studies and I should just
say right from the bat right off the bat there's a way too much packed in here so I'm gonna point
you kind of throughout toward further resources as we go and since this will be recorded I'm hoping
people can kind of go back and and see those pointers to those so first you know what is active
inference generally so I think it's important to recognize from the start here that active inference
is a term that's actually been used in multiple ways so the term was actually initially used to
refer to a theory of predictive motor control starting around 2013 a little before which was
essentially an extension of prediction based models like predictive coding extension of those
sorts of models of perception to the idea of motor control through prediction and currently
however active inferences is more often used to refer to a different but related theory which
is a theory of predictive decision making as opposed to motor control and that's what we're
talking about in this lecture and both of these theories are grounded in Bayesian inference or
probability theory one is just about deciding what to do which is what we're talking about and
the other is just about controlling the body to enact a decision once it's made and if people
want more details on kind of the historical walkthrough and distinction between those in
more detail this is just one paper that we're just gonna put out that the beginning portion describes
that and makes that distinction or we tried to make it as clearly as we could but the basic idea
with active inference is that agents aren't simply passive observers of their environment and so
they actually they actively infer the probability of future observations given the different
possible actions that they might choose so you might start out saying okay if I go inside I predict
that I'm gonna feel warmer and then you combine those with predictions you combine that with
preferences so I want to feel warm therefore I will go inside and then the other thing that's
useful about active inference is that it also models agents making information seeking actions so
they gather information when it will be useful so for instance I'm gonna turn on my flashlight
right now because I predict this is gonna help me figure out how to get inside so all of those
things are kind of encompassed and it's like active active inference as opposed to just passive
inference now just to kind of introduce a few kind of initial starting variables here so first
we have observations oh and then we represent hidden states with the variable s and the idea is
that those are hidden because they have to be inferred from observations and then we have chi
which stands for policies which are sequences of actions where each action is represented as
you and the idea here in the figure above is we want to distinguish between the generative model
and something called the generative process and the generative model here is represented by this
joint probability of essentially it's all the it's the probability that you believe is associated
with each possible combination of observation states and policies and what that allows you to do is
kind of work out what you think will happen what you think the probabilities of different observations
given your beliefs about states and what action you choose so you might say okay what might I do
to stop this flame that I think I'm perceiving over here well probably the best policy or the best
action is just to blow just blow out the candle now there's a difference between that and the
generative process which is just the actual objects you know objects and events that are that are
out in the in the environment itself so where the generative model here is the beliefs about those
things and that could be inaccurate but there is some true states and of the world that are
generating real observations and the brains is trying to model and get a handle on those the best
as possible so this is a common depiction and the common very daunting depiction of active
inference that you'll often see in papers on the left here is the graphical model depiction in
the right is all the equations for inference to solve this graph and so we'll walk through each of
these steps in detail in a second but just to kind of motivate this I want to start out by
kind of illustrating what what active inference is actually good for to motivate why you might
want to learn it and understand it and so first is it incorporates perception learning and decision
making all within a within a single model so it allows the framework to be applied to a really
wide range of problems including perception tasks or enforcement learning tasks planning tasks among
others and the equations that I just showed that are used for inference are fully generic across
different generative model architectures so really you just need to come up with the right generative
model for whatever you're trying to model and then the the same exact equations will perform
approximately optimal inference for whatever the generative model structure is so the task is
just to figure out the right generative model structure to to simulate a particular cognitive
processor behavioral task so the other thing is it's motivated by biological plausibility so in
other words there's clear ways in which neural networks can implement all the linked equations
in the model and it has an accompanying neural process theory which is often depicted like
this with these little kind of ball neurons and synaptic connections that I'll walk through this
in a minute but the point is for well later in the talk but for the the moment the idea is just
that it provides the opportunity to make and test precise hypotheses in neuromaging studies so
including predicting ERPs in EEG studies or localized neural responses in fMRI studies and
then the last thing that I like to highlight is that it provides a unique approach for modeling
the explore explore exploit trade-offs so it kind of helps answer the question or models the agent
trying to answer the question of when do I seek reward immediately or when do I first seek out
information so I have a better idea of where the rewards are so it's a useful framework for
modeling behavioral tasks that involve this sort of information seeking and planning component
now just pointing you to some further resources this is a paper that we wrote recently that is
just kind of a review of recent applications and then I'll just point you also to a number of
empirical studies that we and others have done so these are a few that we've done modeling and
fitting models to be empirical behavioral data on information seeking and learning it's a couple
of papers that we've published looking at planning behavior and then there's a couple studies not
many today looking at neuroscientific predictions and testing those against either EEG or fMRI data
so so the idea is that it's already you know there's already it's already somewhat established or
starting to be that this is useful for like practical scientific purposes and so as they
mentioned we put out this tutorial recently that we hoped would make some of this material clear
and more accessible for people who want to use this modeling framework in their in their own
studies and so a lot of what I'll be talking about going forward will draw pretty heavily on this
some on this tutorial so I just want to do acknowledge it here and my and my co-authors on
it and as I just another place to look for more details and so in the formalism for active inference
and it's based on what are called partially observable Markov decision processes or POMDPs
and the idea here is that it's modeling hidden states of the world that evolve over time and as
I said the agent doesn't have full knowledge of the environment so it has to infer states
from observations and then it has to also infer the policies or action sequences that are most
likely to generate preferred observations based on current beliefs and I just want to make a brief
clarification for anyone who does reinforcement learning at all that the term policy is actually
a little different in reinforcement learning vector versus active inference so in reinforcement
learning or model three reinforcement learning policies often refer to mapping from states to
actions so for example just rules you know if I'm in state one then I'll go left or in I'm in state
two then I'll go right and active inference of policy refers to a possible sequence of actions of
some preset depth some preset number of actions so for example one policy might be I will go left
then left then right whereas another might be I'll go right then right then left so the idea is an
active inference you're you're evaluating the the whole sequence of actions in terms of whether
will lead to the observations that you want so just an additional notation here and just as
another kind of explicit illustration of the policy idea if we have you know in this maze we
might have kind of the starting position in this goal one policy might be this whole sequence of
10 actions whereas another policy might be this whole sequence of 14 actions in this case and the
first one to be favored here because it gets you there more quickly so the idea is that we're
evaluating everything under policies and active inference because the goal is just to choose a
policy so prior beliefs in Bayesian inference which I've been told everyone here is you know ought
to be familiar with Bayesian inference so I'm not gonna give an intro on that but priors over
states instead become priors over states conditional on policies so the estate I expect to be in if I
give a if I choose a given policy and then same thing for likelihood mappings so the observations
I expect if I'm in a state and I choose this policy same thing posterior beliefs it's just the
probability of states given some set of observations and a policy and that's often the kind of thing
as we'll see that we want to figure out in perception and then we can just solve that with
Bayesian theorem in the standard way so and then we have predictive posteriors over observations
which is going to be really important for policy selection as we'll see and so those are the
observations I expect if I choose a policy and then another thing that's fairly unique in active
inference is this idea of a prior preference distribution so probability of observations
given C which is just this variable that we use as a matrix essentially that encodes what we what
we want what observations we want over others and so higher probability here just indicates that
something's more rewarding or more desired so like I said this is a unique element in active
inference because it's using something with the form of a probability distribution to encode
relative reward values another thing to note here for people who aren't familiar with variational
inference as an approximate Bayesian inference technique we use the variable Q to denote
approximate distributions because exact Bayesian inference is just often very intractable in real
world cases so Q is essentially kind of like our best guess about whatever belief it's indexing so
here Qs given pi would be our approximate posterior over states and we would want to get that to
match the posterior the true posterior as closely as possible and Qo given pi would be our
approximate prediction about what we'll observe if we choose one policy over another and the last
kind of thing that I would want to introduce for anyone who isn't familiar is the concept of a
KL divergence and the idea here is just that smaller values for a KL divergence indicate that
two distributions are more similar and this becomes really important in action selection and
active inference because in this case the closer we can get Qo given pi with Pio given C the smaller
the KL divergence will be which means the probability is just higher that we'll get what we want so to
give you kind of a practical example so take Qo given policy one and whatever our preference
distribution is if the distributions look like this so this red distribution being the expected
observations given a policy these are pretty far apart so that would be a large KL divergence
whereas if we take policy two in this case the predicted observations under the policy are much
closer to the preferred observations and so that's kind of a smaller KL divergence and so that's
going to be a policy that has a higher chance of being selected by an active inference agent
so now to kind of get through the graphical model itself to kind of parse this so it's more
comprehensible so here arrows denote conditional dependencies circles correspond to random variables
that are updated during learning so in this case we have our s's here and our pi's and these are
just these stand in for Qs and Qpi so our approximate beliefs the squares indicate fixed
parameters which I'll walk through but these are the things that are kind of fixed within a given
trial but that are updated more slowly through learning so as I mentioned this can be a bit
dawning but when we walk through it step by step I think you'll find that it's it's more
comprehensible so this bottom part here the states and observations is what roughly corresponds
to perception in active inference models whereas this whole part on the top involving policies
and the things that policies depend on is the action selection part so we'll start by just
walking through the static and dynamic perception part of these models so that bottom part and to
do so I'll use a concrete task as an example and this is what we used in the tutorial so in this
task the agent has to choose between one of two slot machines and if they choose the right one
then they'll win four dollars whereas the other one will pay out zero dollars and if they guess
right away then they get the four dollars but they can also choose to ask for a hint so this
like information seeking action and if they do that then they'll find out which one is more likely
to pay out but there's a loss involved because then they can only win two dollars so it is
specifically this kind of information seeking versus direct reward seeking trade off and the
perception part is just going to involve this observing the hint in this case so one hidden
state factor in the model is just going to be which machine is better the left machine or the
right machine and the outcome or observation modality here is going to have three possible
observations just no hints the hint that the left machine is better and the hint that the right
machine is better so that's we're going to use to model perception in this case so to start out
we'll just pick kind of the smallest little unit in the graphical model where we just have
observations hidden states and then we have a which is the likelihood so the probability of
observations given states and then we have D which is just our prior belief about states so
observations hidden states prior beliefs about states and the likelihood mapping and then we
can solve this just by this is just a way of encoding based theorem essentially where it's
just here's your prior here's your likelihood and then you softmax this just to turn the
result back into a probability distribution and so just to give a concrete example here and in the
in the context of the explore exploit task we can start out just by assigning an equal prior
probability to L here which would the left machine being better and are here for the right
