my posterior belief and that requires a big change then your kale divergence will be large
um so we can think about it as complexity in the sense that you're having to move your beliefs
change your beliefs a lot um to um to get to your best guess um whereas this would just be
the prediction part so this is just the probability of the observations that I'm giving given the
states um in my model um so this is just how accurate are my predictions um and you can just
flip this around and just think of this as complexity plus prediction error um then because
again this is just about the accuracy of your model's predictions um so in other words to minimize
f we're trying to minimize complexity so we're trying to move our posterior beliefs as little
as possible from our prior beliefs um so the simplest change in beliefs um that we can do
while also maximizing accuracy so adjusting beliefs to make the most accurate predictions
possible um so this is another way of seeing how convergence to q s in our in the message
passing scheme we talked about before um can be understood as prediction error minimization
um and this is probably familiar to a lot of people but sometimes the question comes up
you know why worry about this complexity part why not just maximize accuracy um and the reason is
that the more complicated your model becomes the easier it is to overfit you know so in this case
I can come up with this really complicated model that will predict each observation you know
like perfectly but it's going to be really bad at predicting new observations so you
so a model like this is kind of in the middle um that um requires uh simpler uh simpler change in
beliefs um predicts it pretty well but is a lot more likely to um to actually generalize to
making accurate predictions about future observations so that's the motivation for the
complexity being useful um so now that we've talked for a fairly long time now about how
the perception process works in active inference um now we want to move on to the actual action
component um and here the idea is that action is just about control over state transitions
so essentially what we do is we have um one of these sets of transition beliefs over time
for each policy um so each policy is a possible sequence of actions so like choosing to take the
hint and then choose the left machine or just immediately choosing the left machine um and
what that corresponds to is different types of transitions so different B matrices essentially
um so each policy just entails a different sequence of state transitions um
um so like transitioning from the start state to the take the hint state or transitioning from
the start state to the choose the left machine state for example um so what we're doing then
is we're predicting future observations under different possible state transitions so if we
get this when we get this observation one we are we're trying to now after we update our beliefs
we're trying to now predict based on our current beliefs what observation two will be before we
get it um so you can actually think about this as just performing message passing in
parallel for a bunch of different um possible hidden markup models the transition models I
showed you before so one policy could just be this model where we have transition uh beliefs
you know B1 and transition beliefs B2 and then a second policy could just be one that has B1
and then a different set of transition prior beliefs so B3 um and so in the context of the um
of our kind of example model here this just requires that we add a second state factor
so our choice states um and then a second outcome modality where we can observe losing or winning
or a start state um and so the way that we would do this um is that so we did the exact same thing
as before um where we would make our observation here and we're now we have the second outcome
modality that corresponds to losing and winning and so now we have this preference distribution
where darker is higher probability this is basically saying I prefer to have a win versus a loss um so
now when we make that hint observation um we that's after we choose the hint state um and then we
update our beliefs now that the left machine is better so now our belief are we choose the
transition to the choosing the left band at stage or the left slot machine state um and then that
results in us observing a win um so that's adding the the action component so in other words the
agent just took the hint and then shows the left machine um and just to show you how this is encoded
for example one b matrix encoding uh the action of choosing the left machine would just you know
look like this right so just given any possible state I'm in I will transition to this third
state which corresponds to choosing the left machine um and then that could be for choosing the
right machine that could be for taking the hint etc so just to show you how it would actually be
encoded um so now policies themselves in these models depend um on the expected free energy
which is essentially evaluating how good is each policy and that in turn depends on c which again
is our preferred observations or or what corresponds to reward um and so the what
expected for energy ultimately amounts to is stating that the policies with the the lowest
expected for energy are those that are expected to generate preferred outcomes um while at the
same time maximizing information gain um and just to show you if you were to specify a particular
c matrix here um it might look like this um in actual models where this is tau equals one two
and three um and you might say put negative ones here right so I don't want to lose and then
certain sorts of positive numbers here I want to win with value four at time point two and
but I'll only win two dollars uh at time point three if I take the hint first um and then you
adjust softmax it and then log it and what we actually use are these log probabilities
um and I'll just note here that um this value four here we could actually vary this or it could
be something we fit in an experiment um which corresponds to what we can call preference
precision um and we'll see that higher values for that end up reducing information seeking and
increasing um reward seeking um so just to kind of go through what expected free energy is so the
variational free energy pertains to current observations um but remember that decision
making requires making um predictions about future observations under each policy and we can't
calculate f um without the observation um so what we're doing is we're taking our predicted
observations under a policy um and then we're calculating the expected free energy associated
with each policy based on those expected observations um now the the literature shows a
lot of different decompositions of expected free energy um the ones that are probably more
intuitive um are one is this one which is the epistemic value and pragmatic value um version
um and this just kind of to walk you through it this is just saying um this is um our beliefs
about our approximate posterior beliefs about states um before making an observation and this is
our approximate posterior beliefs after making an observation so this is just saying how much do I
expect beliefs will change after a new observation if I choose this policy um and note that there's
negative here which means that the more I update my beliefs so that the more my beliefs change um the uh
the the lower the expected free energy that will be so this is actually maximizing information gain
so you want your beliefs to change more um and then our pragmatic value is just the probability
of the preferred observations um so a higher lower expected free energy is going to maximize
change in beliefs so maximize information gain while also maximizing the probability of uh
of preferred outcomes or reward um the other common decomposition you'll see is this risk and
ambiguity um version and here this is just the kale divergence between the observations you expect
under a policy and your preferred observations so again this is just trying to get preferred
observations as close as possible to expected observations um and then the and then this is the
ambiguity or the corresponds to the entropy um which is a essentially a similar way of encoding
just another way of encoding how informative I expect observations to be so here for example
if in my likelihood state one will generate observation one point not with point probability
point nine and observation two with only point one whereas this one state two will generate
each with equal probability then these observations aren't very informative so I'd be driven toward
choosing this one because it can actually disambiguate states um and so this risk seeking term is really
just what I showed you before where you're trying to find the policy that gets the expected
observations to match the the preferred observations as closely as possible um so the idea is that
this this actually provides a principled approach to arbitrating um the explore exploit dilemma
where policy selection is initially information seeking so the ambiguity term dominates
then the agent ends up levering that information to bring about preferred outcomes once it's
confident um so just to kind of review so each policy in active inference entails a different
set of state transitions um and that in turn predicts a different sequence of observations
and those jointly allow evaluation of the expected free energy um and just to show you kind of in
matrix form this is um what it would look like um just based on the actual variables and then our
probability of choosing each policy is just um just normalizing or softmaxing uh negative g
so a lower g pi then just corresponds to a higher probability of choosing that policy
and then finally what we do is we um we often use this alpha parameter which is like an action
precision which just controls how likely you are to choose the action associated with the best
policy so it controls the kind of the randomness and choice um and as I mentioned when the precision
of the preference distribution goes up that ends up making people more uh risk seeking
and just to show you an example of that um this is what it looked like as you you know as you saw
before the agent takes the hint when the preference distribution is moderate um whereas if we made a
very precise preference distribution then instead the agent will typically without being confident
which one is correct just choose one of the machines right away um and in this case it it loses
so in this case it took the guess immediately and lost um so now the final um bits that I'll
walk through that are more just kind of like the some some bells and whistles that can be added but
don't necessarily always need to be added um an active inference models so one is this e um matrix
here or it's a vector an e vector and um it's just a way of encoding habits um and habit learning
so it's just a prior belief about what policy you'll choose um and then gamma is um is the expected
precision of the expected free energy um so it's it's an inverse temperature parameter that um
basically what it does is it just encodes the confidence that an agent has in its action model
so what ends up happening is is that if gamma is low so if the expected if your confidence in g
is low then that ends up increasing the influence of habits um or making your behavior more more
random and that's just encoded like this so if this number is lower then g becomes down weighted
and habits end up having a stronger effect um and just to show you this is just based on um
there's a hyper parameter um on gamma which is this beta parameter which is a rate parameter in a
gamma um distribution um and we just take the expected value of gamma which ends up just being
one over beta um and that's just that's just how you have a prior on your expected for energy
precision um uh one other thing that that often gets talked about in the um neural process theory
associated with active inference is that um there is a a scheme that um has been proposed for
how gamma your actual confidence in your expected for energy can change after new observations
I mean I won't go through this in a ton of detail but the reason I mentioned it is just that um
it's actually been the part of the neural process theory that um you know Carl Friston um has um
proposed um as a as something that could correspond to phasic dopamine responses
um and I'll just show you this um you know briefly to to go just uh and point you toward
the tutorial for more details but all you're essentially doing is you're just taking what
your beliefs were about policies before and after you get the variational free energy associated
with the observation and then um this is the actual update equation but essentially all this
is doing is it's just saying how consistent were my observations with my prior beliefs about g
so if the observations I got were very different than what the expected free energy entailed
then my confidence in the expected free energy goes down um and then I can just update beta which
then updates gamma um and so in this case just to show you you know if say in this case the
agent expected that taking the hint was the best policy and then after receiving the hint it became
more confident um and so in this case that was evidence that g was reliable and so gamma goes
up and that would be the um the simulated phasic dopamine response and then the blue here would
be the tonic change in expected change in dopamine which is this tonic change in in gamma
um and these are just uh if people want to see them a couple worked uh examples in this
tutorial um for calculating this but I'm just kind of pointing you toward the tutorial for that um
and there is one study just to point it out um by Philip Shordenbeck and um and colleagues that
did show that gamma updates simulated gamma updates um correlated with neural responses
in several regions including the midbrain um associated with dopamine but the theory otherwise
remains to be um remains to be thoroughly tested um but this also leads us to um the um the expanded
kind of full neural process theory um where the um the neural activity and the synaptic
weights are no longer just about states and predictioners instead they also encode the policy
probabilities predicted observations under policies the state probabilities under each policy
they expected for energy preferred observations um and also gamma and the uh and the uh prior beliefs
if they're included um now probably I'll um I was going to show you some examples of um how
exploitation so information seeking kind of reduces with learning um I guess I probably won't
go into that for sake of time but we can go back to it um but this is just kind of showing
essentially different behaviors if uh with a risk averse agent with a smaller preference
distribution versus a more risk seeking agent um and um again I can go back through this um maybe in
the um in the Q and A if people are interested but the the key point here is is just by um
by simulating these sorts of behaviors um over time we can also generate predicted neural
response time courses both for the the dopamine predictions um the the gamma and also for RERPs
and how those would change over trials in a task um so the the very last thing I'll cover just in
the last couple of minutes here is um is active learning um and the idea here is that there's
actually two different types of exploration in active inference so one is state exploration
which is what we've been talking about so seeking out information about hidden states um so figuring
out by taking the hint which of the two machines is better for example but there's also something
called parameter exploration and that's where you're actually seeking in for seeking um information
to update your beliefs about the parameters itself so in this case like updating your beliefs about
whatever the the reward probabilities are or or figuring out how confident how how good the
hints are for example so be learning to change your beliefs in in say like the A matrix um and
that's driven by an added term to the expected free energy when learning is included um and it's
based on using um what are called Dirichlet distributions um which are just um in Dirichlet
categorical models these are just priors that are often used on the um with categorical distributions
which which are the kinds of distributions we've been working with in my examples um so to skip
that um but basically the um what the Dirichlet concentration parameters do we just represent
them with small a's and these are essentially just beliefs in this case about the probability for
each entry in the A matrix so the probability of losing given you know choosing the left machine
etc um so and these are counts um basically every time you make a new observation then you just
add a add a value so you just increase the value of each of um of whatever one corresponds to the
combination of observations and states in that trial um so and this is the the equation for
updating which again I can probably just go into in more in more detail um but essentially it just
amounts to counting coincidences between states and observations which is a type of heavy in learning
so so say we start out with concentration parameters like this so it's just point two is all the way
across a really small numbers so we don't know whether the left machine or the right machine
is more likely to win um so let's say I observe a one here so I observe a a loss um and I believe
that I was in the right you know I chose the right bandit so now or the right slot machine
and so now I just add a one here so now this entry has a one point two instead of a point two
and if I softmax these so if I turn them each column back into probability distributions
then this one right the probability of losing given being in the right state will now have a much
higher probability um and then we can also if we want to parameterize this uh the learning equation
here with things like forgetting rates or learning rates if we want to those are things
that we could like fit to behavior in in empirical studies um and so to do this we have um this uh
extra term in the expected free energy um here um and this novelty term essentially is just doing
some of the same thing as with the epistemic term you're just trying to change your beliefs about
a as much as possible after observing a new state observation pair um and that's based on um
essentially the thing is just figuring out based on the sums um in the concentration parameters
um which essentially the column that has the smaller total number of counts will be the one
that will be favored um so just a kind of final intuition here um the ambiguity term if we if
