Well, hello everybody. It's such a great pleasure to see you all and also a great honor to be
invited and I would like to thank Jennifer Goldman for this invitation. Now, I consider
the brain as a physical and synergetic system composed of myriads of neurons. Sensation,
action, and all other brain functions require a high degree of coordination between them. But who
or what steers the neurons? There is no agent. Their cooperation is based on principles of
self-organization. The brain is a complex system and maybe we can recognize a hierarchical structure
with microscopic, mesoscopic, and macroscopic levels that may be distinguished by time,
length, and energy scales. I will start with a bottom-up approach from parts to systems.
The parts are represented by a state vector Q with components Q1, Q2, and so on. In neurodynamics,
we associate these quantities with firing rates of neurons, then treating currents,
signal neuron derivations, local field potentials, concentrations of neurotransmitters, and so on.
In the spirit of dynamic system theory, we assume that the state vector Q changes in the
course of time by an equation dQ by dt is equal to a nonlinear function of the state vector Q
at the time t and a function of control parameters. These may be external signals or internal states
such for instance consciousness. And Q is a subject to fluctuating forces that are in
neurodynamics quite essential. The basic insight we would like to start from is the idea that for
instance recognition can be considered as a nonlinear phase transitions. In fact, when we
change a parameter lambda, the system of Qs can become unstable and one or few configurations
or Q close into the course of time. There is a competition which is won by a specific configuration
of Q alone and this configuration is governed by what we call an order parameter that we call
XI. This governance is called the slaving principle, which is a very general principle and has an
important special case, namely the center manifold theorem of dynamic systems. All in all, we can
state that the state vector component QL is a function of the order of one or several order
parameters. We have applied this scheme to pattern recognition by machines or as a model of human
pattern recognition. One may show that the build up of an order parameter can be related to a
build up of consciousness as is shown by beautiful experiments by Dehain. We have the principles
of circular causality. An order parameter governs the behavior of the individual parts, but in
turn the individual parts define by means of the cooperation the order parameter. I put this
into analogy with the picture of Spinoza who says mind and body are just two sides of the same
coin. And I establish here in analogy the mind is corresponds to the order parameters where
the body corresponds to the parts of the state vector Q. The central idea here is that the behavior
of many parts is in each case governed by a few variable order parameters, which implies an enormous
reduction of dimension or in other words of complexity. As Victor has realized some years
ago, the order parameter dynamics happens on many folds and this has enabled him to deal
for instance with elliptical seizures in a very beautiful model. Here I want to talk about another
aspect, namely what I call synergetics second foundation, which is more or less a top-down
approach. It starts from Jane's maximum entropy principle. Here we consider the probability
distribution P of Q of the state vector with its components Q1, Q2 and so on. We assume that P is
normalized to unity. Jane's considers the entropy as or in my interpretation the information which
is defined by minus sum over the state vector P of Q logarithm of Q. According to his principle,
we seek the maximum of S under specific constraints. In his case, he considered
thermodynamic constraints that is with given mean energy E of Q. To present a simple example,
in a gas E of Q is just given by the kinetic energy of the molecules with mass m and velocity v.
So in this case, we have just E of Q is an integral m half v squared P v dv. The important
issue here is that one can rather easily calculate the probability distribution P of Q by means
of Lagrange multipliers lambda. And in this specific case here, we find exponential lambda
naught, one multiplier times exponential minus m half v squared lambda, lambda another multiplier.
By comparison with the Maxwell Boltzmann distribution, we realize that in the specific case,
lambda is given by one over Boltzmann constant K times absolute temperature capital T. When
we insert probability distribution P into the normalization condition and rearrange terms,
we already find exponential minus lambda naught is equal to sum over all states Q exponential
minus E of Q over KT. However, this expression is well known in statistical physics. Is this only
the partition function Z. The partition function Z can be expressed also as a free energy, namely
the left hand side of this equation can be written as exponential minus free energy over KT. The
essential point here is that we can calculate all important average functions once we know the Z
or the free energy. However, in general, this is very complicated because the free energy or Z
is very hard to calculate. To this end, to calculate it, Feynman suggested to approximate
P of Q by another distribution function P of Q A, where A is a set of really change
chooseable parameters. Then he applies the so-called Colbeck Liebler inequality, the sum over the
states Q P of Q approximate logarithm of P Q approximate divided by Q of P of Q, sorry,
is almost bigger than zero, with the exception that the probability distributions P and P A coincide.
Based on this inequality, Feynman derives then the principle that the free energy of the truces
system is always smaller than the approximate free energy. This is a very powerful principle,
but the question arises whether we can apply it also to non-equilibrium systems. In this case,
we were in the fortunate situation that we had a microscopic theory at hand, namely that of laser
light. This is work by myself and essentially Risken. In this work, the field amplitude of the
laser light is divided into a slowly varying amplitude capital A of t times rapidly oscillating
part sinus omega t, omega frequency of the light wave. The essential point here is that capital
A is a random variable whose distribution function P of A could be calculated. It is essentially given
by a normalization constant times exponential constant lambda a squared minus another constant
B A to the fourth. This probability distribution was strongly reminiscent of one that appeared in
another context, namely in the theory of phase transitions as formulated by Landauer. He coined
the expression A as order parameter. For me, the question arose whether such an expression
can be calculated or derived by means of Jane's principle.
As it turned out, this required an entirely new set of constraints, which are correlations,
or other words, polynomials of the components of the state vector Q.
Again, applying Jane's principle and Lagrange parameters, we can readily write down the
probability distribution P of Q. It is an exponential function of a Lagrange parameter lambda
naught times an exponential of minus sum over m lambda m Lagrange parameters times configurations
or polynomials of Q. Or written differently, we find that P of Q is a normalization constant
times an exponential function of a function V of Q, which we call potential.
As stated before, we applied this method to deal with pattern recognition
by computers and also as a model for human pattern recognition.
In this case, the Q j's correspond to the crave values of the pixels.
Now, at this time, I can make context contact with another approach, namely by Fritzens'
free energy principle. At the beginning, I would like to say that free energy is a little
bit misleading because it does not deal with energy at all, but other quantities,
but it lends its name from its origin from Feynman's principle.
Now, the same idea as before, namely we want to find this distribution function P of Q,
which in our cases now written, or in Fritzens' cases written, essentially as an exponential
function minus free energy. And I want to show how there is an analogy between Fritzens' free
energy and my potential V. To derive P or F, Fritzens invokes a generative model.
For instance, of the following form, he considers a brain model in which a neuron receives
signals psi from the environment and is directionally coupled to an action neuron
with activity Q. In the first step, the generative model of F is formulated by postulating the P
of Q and S conditioned by psi is equal to the normalization factor times a quadratic form
in Q, S, and psi. The essential point here is that psi and its typical rapid distribution P of psi
are not known as well as P of S and psi are not known either, where P of S or psi is a conditional
probability. To solve this problem, Fritzens invoked patient inference. That means it is an
iteration procedure based on a sequence of measurements. To make a long story short,
Fritzens can then derive in this way an explicit expression for the free energy F,
which consists of some constants that can be determined experimentally,
as well as bar, which is an average value.
And we have to compare Fritzens results with ours in which V, the potential,
consists of two parts. The first one coincides with that of Fritzens. The second one seems to be
different. However, when one looks at the minima of F and V, they coincide. This implies that our
methods are in a way equivalent with one difference. Namely, Fritzens approach is based on a stepwise
improvement, whereas in our approach, I assume that we have already done enough measurements.
Fritzens approach may be more or more detailed, whereas our approach is perhaps more simply
to be handled. Now, this comparison is performed in detail in a book called Synergetic Computities,
which has been written by Portugalian myself and will be published next year.
So I wish to thank you for your attention.
