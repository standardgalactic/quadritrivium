loss functions
and how that translates into
Bayesian inference
however the simple answer to that is
that all the loss functions that are implicit
in the illustrations here
and elsewhere are all written down
as log prize
so as long as you can write down
your loss function in terms
of a prior preference
or a prior energy in the log of the prior belief
then by definition
it becomes a Bayesian inference problem
so the question I think then
for all non-linear and
dynamical models
so that's one of the
conditions for doing this
setting up this duality
is that any energy cost has to be
a KL divergence between some passive
and some active dynamics
but there is another important requirement
which is that any noise that comes
into a system must live
in your control space
in other words if anything happened by accident
you should have been able to do it yourself
and that's the thing that's actually a very strong assumption
so for example if I'm a robot I'm kind of shaking
due to noise
in the same way we can cast that as Bayesian inference
but if somebody came and pushed me
in a way that I couldn't have pushed
myself all of a sudden that's not a
Bayesian inference problem
it has to do with the structure of the noise
and not just with the priors and the loss functions
I mean I'm going to have to defer to you
technically
you know more about having been doing this
I would say that this pushing
in the sense
when we get new visual information
we are being pushed
by a sensory and of course
the whole point of bitcalment filtering
and the generalizations
of that
of course this is not camel filtering
this is beyond extended camel filtering
in fact it's in generalized coordinates of motion
so it's generalized Bayesian filtering
but that pushing is of course just
from the sensory perspective
exactly the problem
that generalized Bayesian filtering
contends with and resolves
and let me pose the question back to you
how is pushing
the retina with visual information
any different from pushing my proprioception
by a force
to my body
my question is a bit naive
but if I understood well
your internal states
is like the brain, the state of the brain
and the external state is the outside
world of the brain right
but the dynamics
includes all states
so the attractors for example
contains also the external state
the internal state in between
well the internal states
are actually just modeling the external states
not themselves
say it again
the internal states are understood
as modeling the external states
but not themselves
I suppose that the dynamic model
was including all states
and that the dynamics was including
with all states right
all states in the outside world
but not the states that are doing the modeling
that I didn't get that
well does it matter for your question
because
I didn't see exactly the equation that you had
why the Markov-Blanket
was playing
I didn't see the Markov-Blanket
assumption
was really coming into the equations
I see
so the leap there
I apologize for rushing over that
that's actually a very non-trivial
it's basically this thing here
isn't it
I think everybody would be
fairly comfortable
or they would after half an hour with a matlab
or a pen and paper
they'd be fairly comfortable with this
and it's really
why is this still true
this basically looks as if
it has eliminated the external states
the S-bar
has the sensory active and internal states
but not the external states
and yet
this log probability
depends upon the external states
so that's the clever bit
that's the non-trivial aspect of this result
so this result still holds true
when you have to go through and actually do the integration
to show it in a non-trivial way
so this has the same form as the previous
equation
I apologize
so fx is a
curly gradient descent
on this thing here where x comprises everything
this is also true
but this probability
distribution here
depends upon the external states
so that's the clever bit
in the sense that it's as if
the internal states
and the active states
the only clever bit
it's as if the internal states
somehow
know about the external states
they know where they are
in the state space of external states
and yet they can't
because that's beyond the Markov blanket
but the very
existence of non-equilibrium
state of states means that this has to be true
and that's basically what gives these
what
it is
what gives these
systems
or certainly a focus
on the internal states of the system
and the Markov blanket
the look and feel of adaptive behavior
the look and feel
of self-organization
homeostasis and the look and feel
of a little Bayesian agent
thank you very much again
and my question is
more na√Øve
than the previous
the person
you have
the very beautiful
explanation based on
the probability
principle
and my question is
how
the biological neural network
is implementing
the probability
reasoning in the network
is it the
recruitment principle
or any other
knowledge
about how the neural
system implements
this computation
well I think that's an open question
and as open as
the best scheme that one would adopt
in the robotics context
so what you're asking is what is
the process
theory that complies
with the
with the principle
I mean the principle in of itself is almost
uninteresting because it's tautologically true
I think the hard problem is really
first of all
the form of the generative models that you're dealing with
and the actual scheme
