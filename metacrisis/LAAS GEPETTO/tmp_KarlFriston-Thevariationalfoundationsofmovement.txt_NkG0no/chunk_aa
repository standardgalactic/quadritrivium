And then we'll be at the breakfast just after his talk.
Thank you very much.
That's a great pleasure to be here.
As some of you will know, I'm slightly outside my comfort zone.
In fact, a very, very long way outside my comfort zone.
I found the talks this morning intriguing.
A lot of the rhetoric I don't understand,
but there's also a lot of curious similarities
between many of the issues that we deal with in theoretical neurobiology.
Let me move this higher.
I was just saying that I found a lot of the rhetoric and the ideas
slightly mystical that I've been hearing about,
but there's also a lot of formal similarities between some of the issues
that we deal with in theoretical neurobiology,
and in particular asking questions about how the brain
actively infers and copes and exchanges with its environment.
So I think, from your perspective,
you can regard me as the light entertainment of this workshop.
So what I'm going to do is try to overview for you
the basic principles that we bring to bear
when trying to understand action and perception in the brain
with a particular emphasis on not geometry, per se,
but certainly the information geometry and the variation...
and the variational underpinnings of that,
and hopefully touch on some of the issues
that we've been hearing about in the previous talks.
So let me overview for you what we're going to talk about.
I'm going to start off in a very abstract way,
just asking some basic questions about the nature of self-organising systems
and in particular biological systems,
and I'm going to place centre stage the boundary
between me or an agent and the outside world
with which the agent exchanges.
In particular, I'm going to focus on the statistical notion
of a Markov blanket that separates me from the rest of the world
and ask what it means to have a Markov blanket
and yet still preserve an egotic exchange with the world
that means that I can preserve certain states
or I pursue certain trajectories in terms of my internal and external states.
I'm going to illustrate the nature of those principles
just by simulating a little primordial soup
and generating a little agent within that primordial soup
and performing some experiments upon it.
Having established the basic idea and some of the principles,
I'm going to tell exactly the same story
but from the point of view of a neuroscientist
and looking at the form of the variational principles
as they might be expressed in the brain
and in particular looking at the notion of predictive coding
and its implementation in canonical microcircuits in the brain.
Then I'm going to illustrate those variational principles
with a few toy simulations
and these are going to be very, very toy from your perspective.
They're very simple, but they at least illustrate the basic idea.
I'm going to focus on action and its observation
and then if we have time,
I'm going to conclude with a simulation of eye movements
and the sampling of visual information from the world.
I'm going to start, again, I apologize for being very abstract
but I think it's quite important from my perspective at least
just to motivate the underlying, if that's going to be a gradient descent,
it's going to be a Gauss-Newton gradient descent,
the underlying dynamic that we're going to be using.
I'm going to start here with a question posed by Schrodinger.
What are some of the events in space and time
which take place within the spatial boundary of a living organism
to be accounted for by physics and chemistry?
I'm not going to answer that question,
but what I want to highlight is the notion of a spatial boundary
and the deeper question underlying this question,
how do systems induce and maintain the separation
of the spatial boundary between themselves and the rest of the universe?
He would be the first person to acknowledge
that that boundary itself is a statistical object
and in statistics that would be basically a Markov blanket.
A Markov blanket is going to be a statistical boundary
between some internal states of the system,
which are denoted by blue here,
and external states in Cian here
that provides an insulation of a statistical sword
between, say, my states and the states of the rest of the world.
How many people here know what a Markov blanket is?
Good. Could you explain to your friends what a Markov blanket is?
Yes, I think so.
You want that blue to be conditioned
in dependence of the light blue given the red.
That was very succinct and absolutely correct.
Let me briefly unpack that.
In fact, I'm joking.
I think most of you here do know what a Markov blanket is.
So can you put your hand up if you know what a Markov process is?
Yeah, so you all know essentially what a Markov blanket is.
So with a Markov process in time,
the Markov blanket is just the preceding state.
So it's just, as he says,
it's just the states you would need to know
that contain all the information about the rest of possible knowledge
in all the remaining states
in order to predict the state of reference
or the internal states here.
So if I wanted to predict this state,
given the causal influences of all other states,
it would be sufficient just to know the Markov blanket.
And I don't need to know all the remaining.
So these are now conditionally independent of these,
or vice versa, conditioned upon the Markov blanket.
And the Markov blanket comprises the parents and the children
and the parents of the children of the state.
Now, from our point of view,
there's an interesting bipartisan on that Markov blanket
that is induced just by considering states that are
not influenced by the internal states.
So for any system that can be written down in this form
in terms of statistical dependencies,
there will exist a Markov blanket.
And that Markov blanket can always be split into two sets,
which I'm going to call active states and sensory states.
And I'm going to motivate those labels just by asking you
to consider that causal structure in relation to things
that we know and love, biotic or biological things,
from single cells through to, say, brains here.
So the internal states will correspond
to all the internal states of the cell.
The active states could be the active filaments
that support the surface states of the sensory states
and provide the cell with motility.
While the sensory states are caused by external states here,
that in turn influence internal states
that cause changes in active states
that again cover back to the environment.
And exactly the same slightly sparse causal structure
can be found in the brain.
We have the internal states of the brain,
all the synaptic activities, connection strengths and the like
that influence and cause changes in our actuators
or effector organs that influence or change the external states
that are registered by sensory states
that in turn change the internal states.
So I'm just saying that for any system,
any organism or agent or robot embedded in a larger system,
we can always apply this partition
provided certain conditional independences exist.
And in fact they have to exist in order to produce
a distinction between me and the rest of the world
through my Markov blanket.
I'm now going to ask you to forget about that
because what we're going to do is now just look
at the behavior of all interesting systems
and then come back and put the Markov back into the equation
and see what the implications are
for the behavior of the sensory states,
the internal states and the action states.
So just to motivate, so this is going to be one
of a couple of slides with lots of equations
or some equations on.
And this is the basic slide that motivates
the variation approach, information geometric approach
to the dynamics of these systems.
And what I'm doing here is in an abstract way
is taking any random dynamical system
where I'm lumping together states of the world
and control states into X and adding random fluctuations.
And I'm presupposing that we're only going to be talking
about systems that have measurable characteristics
that persist over time.
So by definition they are egotic in a weak sense.
And that implies the existence of a random dynamical attractor.
So we have two states here and they trace out
a manifold or an attracting set here
as time evolves with multiple trajectories.
And we can interpret that ensemble of trajectories
or attracting set in terms of an egotic probability.
The probability that I sampled the system at a point in time,
the probability that I would find it in that state
would be denoted by the density here of these trajectories.
So this could be, say, the states of a system
whilst walking, for example.
So if this attracting set exists, if the system exists,
then it is equipped with or it has this probabilistic description
that can be described by the Fokker Planck equation.
Is this a familiar object to many people here?
Yeah, okay. Also known as a master equation,
the Conveyor forward equation, lots of different names.
For those people who don't know, it's relatively simple.
I'll explain what it does in a second.
But from our perspective, its form is not terribly important.
What is important is its solution.
Because if this attracting set exists,
then the rate of change of this probability is zero,
which means I can rearrange the solution
to express the flow of states as a function
of the gradients of the log probability distribution.
So this is the solution that I'm going to appeal to.
And it is this one solution here
that I'm going to use as a variational principle
to understand all internal state changes
and all action of the systems that exist.
Just heuristically for those people who are not familiar
with the Fokker Planck formulas.
It's very simple. All it's saying here is
