that is used to do the
variation
free energy minimization
which could be selection using Bayesian model selection
it could be
and I concur with
the enthusiasm for Gauss-Newton schemes
certainly all our simulations
essentially use a Gauss-Newton scheme
why well because
under the particular form
of approximate Bayesian inference which is not exact
given by assuming the
posterior is always Gaussian
you can always write it down as
in terms of minimizing prediction error
when you can do that you can move from
a Newton to a Gauss-Newton scheme
with all the computational savings involved
so you don't have to explicitly
evaluate the curvature
or the Hessian
and that rests upon the fact that you've made this
a plus assumption
in defining the variation of free energy
so that's part of the approximation here
masses of computational savings all sorts of wonderful things
just drop out for free
under that plus assumption
so we like
we in terms of
neuroscientists and people with very small
computers using MATLAB
like the
predicted coding formulation
or Kalman filtering
apply a Kalman gain to a prediction error
and
that
is very understandable in terms of
again I haven't got the right graphics here
but it's very understandable in terms of
the sorts of message passing you see
in the real brain
so these actually are usually thought to be equivalent
to superficial parameters of cells that live
about a millimeter from the cortical surface
and the expectations are thought
to be encoded by deep primal cells
which live about four millimeters
in the deeper layers of the cortex
the visual cortex has exactly
this hierarchical structure
it has exactly the right
sort of laminar specificity
layer specificity deep and superficial
it also has the right functional asymmetries
remember before I said
there are non-linearities in this but these are in
the generation of a prediction error
through the non-linear forward model
which means that these descending predictions are non-linear
whereas these are linear and driving
as in the Kalman filter
so I completely agree that
in the macro model
it is
well investigated in neuroscience
or brain science
but in the very micro level
the
mathematical model of the neurons
and the large cluster of neurons
I see
do we have
or do you have
the way to implement
the statistical reasoning
into those
microscopic model of neurons
no
so these predicted coding formulations
would normally be
would normally associate
continuous time
representations of
sufficient statistics
of the posterior
activity of neurons
so in that
mean field of approximation
to large numbers of neurons
there is a body of work
championed by people that seem to be over the line
but
they'll actually get into
seeing and spike this passing
and associate with that with
Bayesian belief update or belief propagation
I think the formal
distinction
is between predicted coding
and Bayesian filtering schemes
that deal with continuous states
and continuous time
whereas most of the microscopic models
actually look at the neurochemistry
of post-synaptic responses
they only work for states based models
discrete states
you're in state 1 and K
with very very large vectors
so there's a formal distinction
then you're outside the Bayesian filtering predictive coding
so you've got your choice
so there is no more question
go to lunch now
and we want to thank the speaker
again
back in the room at
145
