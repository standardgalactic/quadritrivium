All the internal states lagged over time.
Find linear mixtures that do indeed predict
the motion of external states here.
The degree of prediction colour-coded
by the depth of this cyan colouring.
Here's the most predictable motion
of the most predictable external molecule
shown as a dotted line
and the prediction as a solid line.
And what's happening occasionally,
this molecule here gets expelled from the soup
and then falls back again.
And it seems as if this thing is actually predicting
or registering these external events,
despite the fact that they are statistically conditionally independent,
they're separated by the Markov blanket.
The reason I'm showing these examples,
you can almost see the fluctuation in internal states,
the evoked response associated with this outside motion event here.
But if you look carefully,
you can see that the changes in the internal states
actually precede the external states,
which begs an important question.
Are the internal states causing the external states,
or is the external fluctuation causing
and registering a representation
of a statistical sort in the internal states?
Of course, the answer is both.
All we're seeing here is an illustration of generalised synchrony.
So I'm sure you all know this,
but this is the phenomena where originally observed by Huygens
where you have multiple clocks hanging from the same wall or beam,
and ultimately they have to come to swing in synchrony.
That is the only long-term attracting state.
This is one of his drawings here.
It illustrates two clocks suspended by the same beam.
And that's all we're seeing here.
We're seeing a generalised synchrony between internal and external states
just by the very existence of these random dynamical attractors.
And here we can think about one clock comprising the internal states,
the other clock, the external states,
and then Markov blanket, the beam or the wall from which they are suspended.
I like this example because it highlights a complete symmetry
of what's going on here.
So if you subscribe to this idea that we are trying to predict
and model our world,
what this means is that the world is trying to model you
in an exact symmetrical way.
The world is watching you in exactly the same way
that you are watching the world.
So that's all the sort of heavy lifting
done from the point of view of the self-organisation to summarise.
And the existence of a Markov blanket necessarily implies
a partition of states into internal states and Markov blanket,
namely sensory and active states, and external or hidden states.
And because active states change but are not changed by external states,
they minimise the entropy of internal states and the Markov blanket,
which means that action will appear to maintain
the structural and functional integrity of a Markov blanket.
And at the same time, internal states will appear to infer
the hidden causes of sensory states by maximising Bayesian wall leavens
and influence those hidden causes through action,
and I'm going to refer to that as active inference.
So now what I want to do is just repeat exactly the same story
but using a different metric and the sorts of ideas and perspectives
that you'd find in psychology and neuroscience.
But we're going to be appearing to exactly the same ideas.
The only slight difference here is that previously,
I had written down a simulation where the value function
or cost function or that surprise function
was an emergent property of the differential equations.
Here I'm actually going to write down the gothic density
in terms of prior beliefs about the states
that agents like to occupy.
But the actual simulations and solutions that I'm going to show you
are based upon exactly the same differential equation,
hill-time equation, and if that's the same code.
So where this sort of talk normally starts is with the Helmholtz.
So now we're talking about the brain and the brain
as an organism that constructs predictions and explanations
for its sensory inputs.
Beautifully articulated in terms by this quote,
objects are always imagined as being present in the field of vision
as would have to be there in order to produce the same impression
on the nervous system.
So the brain is providing these sending predictions
of explanations or hypotheses for the sensory input
and is updating or revising its internal hypotheses
in order to account for the sensory evidence at hand.
And this, of course, is very closely related to the notion
of perception as hypothesis testing by Richard Gregory
and has been formalized and articulated by people
like Peter Dian and Geoffrey Hinton
from a Bayesian perspective,
borrowing from variational principles,
in particular variational free energy,
invented by Richard Feynman to solve his particular quantum
electrical problems using path integral approximations
to path integral problems.
So that's what we're going to...
This is a perspective we're going to pursue.
This quote is nice in the sense that it induces this notion
of a sensory impression on the nervous system,
which, of course, is very much like the sensory states
being or reflecting external influences.
So the external states are impressing themselves
on the Markov blanket or this veil
to produce the sensory impressions.
And if it is the case that the internal states
are maximizing the Bayesian model evidence,
they are implicitly inferring the causes in the outside world
that produce that.
So let's see what that might look like,
how that might be implemented in a simple brain-like organism.
So here's our basic hill climbing equation again
in its Helmholtz form.
I'm replacing the surprise with the free energy bound here,
so we've actually got a gradient descent on the free energy here
with its divergence-free and curl-free components,
so anoidal components.
I'm just rewriting this equation in a form
that might be more familiar to many of you.
It's basically a Kalman filter.
So the anoidal component now becomes a prediction
in the sense that prediction can't change the model evidence
because there's no new data,
so it doesn't change the log probability,
it just flows on the isocontrols,
whereas the update does the hill climbing
and that's just a minimization of prediction error
weighted by the precision of the prediction error
multiplied by the precision of the random fluctuations
here.
So what's prediction error?
We've heard about that a number of times.
It's basically the difference between observations
and some reference.
Here, let's assume that we have these sensory impressions
and that we were trying to predict
the cause of these sensory impressions
given some expectation mu here.
And if I had a generative model or forward model,
given these expectations of what the sensory consequences
of these expected causes were,
I can then produce a prediction in sensory data space
or sensor space.
Look at the difference and we'll call that the prediction error.
And then all we're saying is that if a system exists,
then you should be able to write it down.
So it looks as if it is always trying to minimize its prediction error,
both by changing its internal states,
which now stand in for expectations of the causes
and through its action.
Notice that there's no imperative here
to actually model the true external states.
It is just a statement that it is sufficient
to understand the dynamics in terms of minimization of prediction error.
So the actual cause can be very different from the true cause.
So that is good in the sense that we've boiled down
all the behavior to a suppression of prediction error.
And that provides a simple or intuitive perspective
on perception and action.
In the sense we can either change our brains
to make our predictions more like the sensations
or we can actually change the way that we're sampling the sensations
to make them more like the predictions,
both in the service of minimizing prediction error.
And this provides the metaphor for action and perception
that I was talking about previously.
But if this rests upon prediction errors,
then clearly I have to have a model that generates predictions
at this point that we start to get into some of the hard problems now.
So let's just think about the nature of the generative models
that actually generate data that a humanoid robot or a human
actually samples from the world.
And let's say you set your students the task
of generating foveal sampling from a synthetic retina.
Now to do that, just to generate the data,
let's not worry about inferring or inverting any model.
Just to generate the data, you're going to have to know the hidden causes,
the what and the where of the causes of these sensory data.
So you need to know what is the object being visually palpated,
what is where we're looking,
what are the kinetics of that circadic search.
And then we'd mix these two causes with a cascade
of nonlinear convolution operators,
nonlinear mappings at different hierarchical levels
to finally produce the actual sampled data.
I've just written down that deep and dynamic generative
forward model here in terms of hidden states or causes
and hidden states per se, all subject to random fluctuations,
eventually generating sensory consequences right at the bottom.
And now if we apply our hill climbing function to this
and then rewrite the message passing implicit in this gradient descent here
using the sort of the camera filter based formulation of this,
we actually get a very, very similar architecture.
We replace the random fluctuations with prediction errors,
but the expectations which now replace the hidden states
have exactly the same form, they're just cascading down,
generating predictions of the expectations at lower levels
