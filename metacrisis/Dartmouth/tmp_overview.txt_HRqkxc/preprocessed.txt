Processing Overview for Dartmouth
============================
Checking Dartmouth/Karl Friston - 2016 CCN Workshop： Predictive Coding.txt
1. The speaker argues that many approaches in AI and decision-making, particularly those based on partial observed markov decision processes (POMDPs), are misguided because they attempt to optimize discrete state spaces over continuous beliefs, which is mathematically intractable.

2. The correct approach, according to the speaker, is to use Hamiltonian's principle of least action, which involves optimizing a functional that represents the total expected cost or surprise associated with the beliefs before any actions are taken.

3. The speaker points out that Google DeepMind has started using variational free energy in their deep generative models, which aligns with the principles the speaker advocates for. However, they note that current methods used by Google DeepMind still rely on amortization—learning to map data to beliefs through parameters—which may not fully capture context sensitivity as seen in neuroscience and the speaker's simulations.

4. The speaker anticipates that in the next five to ten years, AI will increasingly adopt a free energy formulation, which incorporates both surprise aversion and surprise-seeking for more adaptive and creative problem-solving.

5. The speaker suggests that combining free energy minimization with surprise-seeking might lead to non-tractable problems but acknowledges the importance of such an approach for moving beyond known problem spaces into new, unimagined ones.

