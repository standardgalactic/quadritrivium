I've got a really high risk and a low risk option.
But there may be a third option, which is if I don't know which is which, I should do nothing until I know more.
In using words like I don't know, I am now saying that my behavior now becomes a function of my knowledge or my uncertainty.
So I now induce different options, different behaviors, different policies and actions that rest upon my degree of belief,
which means you can't do it with value function optimization or utility function optimization or reinforcement learning.
It can't be done with queue learning. It cannot be done with a Bellman optimization scheme.
Because what you've done is you've said that there are better behaviors when I don't know what to do,
which I generally don't do anything or wait until more information comes about.
And once you write that down, you make your objective function a function of a probability distribution,
which becomes an energy, and then you integrate that over time, and then you've got to Hamilton's principle of least action.
So the simplicity post hoc is evident for me anyway. Does that make any sense?
Can I ask you a question about the general approach here, which is,
I see that you start saying that there is an urge, there is an force to reduce surprise.
So this is like I'm making an analogy with physics because that's what you are doing.
And there you start saying I can rewrite the whole thing instead of force in terms of a Fourier energy.
And then you go on and explain what it implies.
My fear is that there is a difference here between what we do in terms of behavior.
First of all, reducing behavior to just minimizing surprise, there are other forces that we cannot measure.
We cannot even measure forces, force in terms of reducing surprise in an individual.
So even though you can write these equations and describe something general,
so you can prescribe what should be the brain doing, do you think you can actually make any prediction?
I know you're doing it, but I'm asking how can you think that you can do it
because there is no measurement to tell you that's actually the case.
So in physics you could come up with any new evidence, you would write a new term into your equation
and just keep adding it and then you're always safe because there are some conservation laws
that basically will keep the Hamilton principle intact.
Well, who said there is such a thing in terms of behavior?
The simplest thing to assume, there is no such thing.
So then by doing that, if you let's say you keep adding terms because you believe that what we are doing is just reducing surprise,
this is the ultimate goal of behavior, then just keep adding terms to your free energy
and then basically the whole thing becomes a tautology
because you're assuming that you're adding terms and there is no proof or disprove for that.
So that's my main question is that how do you think it's going to work?
Right, that's a very good, I ended quite a bit weakly, but that was a very good question.
So there are lots of really interesting issues there.
So the tautology issue, the practical utility of this style of theorizing, can it ever be falsified adding things too?
So I think we could spend hours talking about any one of those.
First of all, the whole point of my style of neuroscience is that we never add anything in.
We're always obliged to, for every advance, you have to get rid of something, which was a distraction or ad hoc or a heuristic.
We've been talking about this magic parameter, the nuances, the balance between exploration and exploitation.
So that just goes, there's only one thing that's being minimized here, about everything, and that's variational free energy.
That's it, there's nothing being added.
However, of course, the free energy is a function of the gerontic model.
All the interesting, all the hard work, all the heavy lifting, understanding this biological system in this experimental context or this social context,
that really calls upon you writing down a gerontic model.
So it doesn't mean there is no free lunch.
There's still a lot of neurobiology and psychology and cognitive neuroscience and computational neuroscience to do.
It's just all at the level of the gerontic model, not the normative principles or the variational principles behind it.
Part of that drive for simplification is a drive to get to the ultimate explanation, which has to be tautological.
For me, the free energy principle is tautological.
It's as tautological as the natural selection, but beautifully so.
Once it's completely tautological, I'll be happy.
Part of that tautology comes along in a slightly technical guise called the complete class theorem.
I'm bringing that to the discussion because it speaks to, I think, a very interesting point you were essentially making.
Is there anything that you can not explain any behaviour in a real biological system that cannot be explained by this?
Because if there isn't, then what's the point?
Now, if there is no behaviour that this cannot explain, so it is provably true that for any pair of cost functions
and behaviours, there are a set of prior beliefs, prior preferences that we're talking about,
that endow the behaviour with an agent, with agency, that render that behaviour base optimal
and, by definition, therefore conforms to the free energy principle.
So what that means is that there is no behaviour that this can't describe if you can find the right prize.
So is that a weakness or a strength?
Well, in the sense of falsifiability, it's a weakness.
In the sense of actually using it practically, it's a real strength.
Because what that means is any system, normal human being or psychiatric cohort that you bring to me,
if I can solve the problem of getting the most appropriate or sufficiently good generative model that describes their behaviour,
it means I can quantify exactly the sort of person they are by their prior beliefs.
And we actually come right back down to the first question again, is what makes an agent an agent?
It's their prior beliefs. It's that attracting set that describes the sorts of states that that sort of person occupies.
So, yeah, there is a deep tautology. There's a fundamental difficulty for falsification,
but there's also a glorious insight underneath that,
which means that everything can now be written down in terms of an agent's prior beliefs.
And that if you can get the right model, you can actually estimate these things.
You can actually quantify them.
And this is one of the tenets of computational psychiatry.
It's to be able to use games, ERPs, mismatch negativities, whatever, in the service of say,
what sort of person am I looking at, quantified in terms of the prior beliefs about the way they should behave?
Carl, but aren't you making the assumption that there is just a unique set of beliefs that will match a data set?
Because if you have several beliefs, and I would think that when we minimise those type of system,
that's equivalent to several solutions, and which almost always we've got several solutions,
because we've got several minimise there.
Because it's not convex, the story that we are minimising there.
So, therefore, we have a lot of minimise, and I would say that if we look here,
that means that for any behaviour that we can measure, we are now with a set,
and that could be thrilling to know which one, a set of prior beliefs.
And so, in other words, we cannot inverse that, or am I wrong?
No, no, no, you're absolutely right, but you've taken us into metabasian land now.
OK, so just those people who may be getting a bit confused.
So, the argument, I think, let me just paraphrase it.
If we're now saying, well, how do we practically use this style of thinking?
And I'm now in the job of quantifying this person with autistic spectrum disorder,
given a bead's task, an earned task, in terms of the prior beliefs about the volatility of the environment
and the need to please the experimenter by responding within a certain timeframe.
If that is the problem, then we're now in and observing the observer,
or using Bayesian inference to make inferences about a Bayesian machine,
which is the autistic patient, which is hence the metabasian thing.
And you're absolutely right, that could be an ill-posed problem.
So, there may be the complete cluster, and it does not say that there is only one unique set of prior beliefs
that will make render the behaviour based on whether it exists.
It doesn't have to be a unique solution.
However, what will happen is that if you actually use Bayesian statistics
to infer the prior beliefs of the Bayesian or ASD subject,
you will then see, if you've got the appropriate model,
that there are a number of equally plausible solutions,
and you will also see that you haven't got enough data to disambiguate between them.
But you will also have an insight into which sorts of experiments you would need to do that disambiguation,
because you've got a generative model underneath of this.
You can do simulations to see the sorts of data that you are,
or the ways of looking at the responses that would enable you now to narrow down
these competing but equally plausible sets of prior beliefs that characterize that subject.
So, it's a very important issue,
but I think it's a generic one about how we actually apply Bayesian statistics
to understand the ways in which our data are being generated.
I think that, in that sense, it's less profound than the complete class theorem in itself,
which speaks to the sort of tautology of the game that we find ourselves in.
I think we can have time for one more short question.
I have two short ones.
Oh, that makes three.
The first one is about the semantics, and the second one is about the substance.
So, regarding the semantics, I'm wondering, it seems as though there are these two aspects to the problem.
One is inference, the other is control, and there is the epistemic cost in the context of inference,
and there is the pragmatic costs, ultimately reproduction, the well-being and survival of the organism.
And it seems as though you're declaring the primacy of surprise,
so you want to sort of absorb the pragmatic costs under the epistemic costs.
So, that's a bit of a semantic issue perhaps,
but doesn't it make more sense to do it the other way around and say,
the ultimate goal is reproduction, and epistemic benefits should be a sub-goal of that.
So, that's the semantic question.
And the question on substances, is this something that is already going on in engineering in AI, for example?
Can these principles be scaled up to real-world tasks such as visual object recognition?
If so, has it already been done, or is this an impending revolution for AI?
And if not, is it just a reinterpretation of what already exists?
Ydyn ni'n gweithio, because they were very short questions, you go ahead.
Very short. You couldn't get them shorter than that, could you?
Do you hope to try to answer them shortly?
The semantic's one.
No, I don't think so.
I take your point that one could articulate a whole theory and absorb uncertainty into cost functions.
But I think that misses the key point that I was trying to make at the beginning of the talk,
that the quantity you have to optimise is a function of probability distributions or beliefs.
And that's the key thing, which means you can never use a Bellman optimality scheme to properly solve that.
You do see heroic attempts, and those heroic attempts have been endemic for the past 40 years.
They're known as partial observed mark-up decision processes or belief state machines.
And all sorts of heroic attempts to try and recover or resolve the problem
are once you write down a discrete state space over continuous beliefs.
You can't do it, therefore you have to parameterise those continuous beliefs in those sorts of glorious and clever ways.
They don't work, they don't scale.
So people have been aware of the problem, and hence the vast literature on partial observed mark-up decision problems.
What I'm saying is that's the wrong approach.
The right approach is Hampton's principle of least action,
where the functional that you need to optimise is a function of the beliefs before you start.
And by proof of principle, I can demonstrate the veracity of that argument,
because I can solve problems which people working with partial observed MPs cannot solve.
So I think it's much more than semantics.
I think it's actually, people have got it wrong in the 20th century.
I think the Bellman's optimality equations are very beautiful construct, complete misdirection.
We should have been looking at Hamilton's principle of least action, and that's the 21st century.
Are people doing this? Yeah.
So Google DeepMind, for example, they've now, a small group within Google DeepMind,
have now started using variational free energy in their deep generative model,
not the temporal models, but certainly sort of 10-led neural networks in the context of the sort of pattern recognition approach.
So people have always rears, I think, that the variational approach was the right way to do this.
And of course you remember, most of deep learning started with Geoffrey Hinton's work,
and he came from the variational formulation.
So he was the first person to be down to propose the Helmholtz machine.
But they've taken a sort of security stream back to that early work in the 1990s.
Will it scale? I don't know, because I don't think they're quite on top of that,
because what they do is they haven't quite got, well, this is insulting,
but I hope there's anybody here from Google?
Ah, well I won't say that then.
Well anyway, they love amortisation, they love casting things in terms of learning.
So what they do is, instead of actually trying to optimise their beliefs, their expectations,
they try and optimise beautifully constructed deep nets, convolution nets.
That have parameters that would map from data to beliefs, or sufficient statistics or beliefs.
And then they learn that, because they're experts, more than well experts,
they are the experts in optimising the parameters.
That's a slight problem because it denies any context sensitivity
that we deal with the neuroscientists and I deal with them in my simulations.
I think once they get beyond amortising their deep networks, then they'll be in this game,
and then we'll find out whether it scales.
You'll need lots of computer scientists, big computers.
So I would imagine the next five to ten years, this style of approach,
and certainly the very actual free energy formulation, will become increasingly dominant in people doing artificial intelligence.
I should quip, for some people, the new AI is actually active inference.
It wasn't a coincidence that we chose that sort of rhetoric.
Yes, so maybe I could just have a comment rather than a question then.
Okay, okay.
So that's a bit straight-laced approach to free energy reduction.
It's based on surprise aversion, but I'm sure you have another talk on surprise-seeking,
curiosity, creativity, playful managerial styles,
which must have some adaptive purpose beyond our interest in horror movies and jokes.
It must help us get out of dead-end problem space to problem spaces we can't even imagine.
So the comment is, if you add surprise-seeking to free energy minimization,
I don't believe it remains tractable, so I'll leave it at that.
