ermawg o dwarfodol ynuarad effaith.
Cymru a chymlu am之dyn ni rYYC yn thicki Toru Poe yng Nghymru,
ry anchelliff мнеau,
mae gollwch eich syniach o'r ambrach sy'n gwri το pa nifer.
Mae'n gw vagu i ni i gawd i ddim yn cymryd gyfanidd,
ond ond arall bydhau a Chrystefile rh experiencedod cyfle希望
yn gofyn dangos a wah Andy�g yn gwyfod arbennir financial
i ddefnyddio fynd i gyd yn unig dod yn Lly safer.
Roedd mae'n sefydliadau dau'r rher InsideHDau
a poloedd p personal efo'r cyd-ref
yw eisiau am bodgylchu sy'n cael ffau chi'r ysglocks.
pe opto d经tyn wedi bod maes i chi crif, ei wneidio dim ein beth o'i dod.
Raing mwy oedi roi ac gefnodd gan yma ddweud?
Dwi wr OSE donno nhw'n dddgli i wneud beth sydd ynladd oherwydd i'n meddwl i dweud
fel owner o'r holl您s arnynt i'r lehhau battwch gyda gilyzdeidle.
Yn gofal,'r f45 maes i dechreu an beverage do ndeddy yw rög DewiHyde.
The importance of using violations...
is to disclose the quintessential role of prediction in perception.
If we get that far,
we'll end up where we've started.
The trick that I'm going to take here though is that I'm actually going to start not with perception,
but with action.
I'm going to start off as if I was talking to a group of people doing optimal control theory,
or reinforcement learning,
or utility theory,
a byddai'n toolaeth cy auditoriaf yn cael credu o'r way
sy filesample y sylwr outdoor, fy byddiaid y Gymru yn credu cafeadariaeth
gyda'r clwr ddechrau, er ysgol ein bod yn cyd yn
geography. A'r Simple Dallion felly yma gennym
os fyddio ddod o wio yna'r� Moses Hacker theod have strith ar gyfer cael ap
gyfer 5-10 a 5-10.
EXCIP09 pe tych yn dwi-fars.
Yn y cwpio, ond mae'n cael ei wneud o'r ddweud o'r ddweud, ond mae'n cael ei ddweud o'r ddweud.
So, this is an overview what we're going to go through. I'm going to introduce the notion that you're the free energy principle, but from a, using a slightly heuristic approach in terms of action and the path of least resistance, highlighting the importance of having internal models or hypotheses that enable us to generate predictions, talking about active inference.
And one key thing that I'm going to focus on is translating the theory into a process theory that can be used to understand neuronal message passing in the brain and help us exactly constrain the sorts of experiments that Jim was talking about.
So that's going to be a big part of what I hope that we'll be talking about, taking normative principles and seeing how they unpack in the service of understanding empirical measurements, anatomy and physiology, and how they can be used to nuance experimental design, showing the sorts of things that one can simulate and speaking to some empirical predictions of these sorts of schemes.
And I've put, as an epilogue, more recent work, simulations of reading, that introduce hierarchies into the particular forms of generative models that I want to survey for you. We won't have time to go without that, but I just want to show you the slides in case of something that catches your attention.
So I'm going to start with a question. Let's assume you're hungry, and let's assume you're an owl. So what are you going to do? You're going to search for a mouse. And how are you going to do that? Don't cheat. You don't have to look at me, not at my side. Absolutely. Perfect answer.
So in terms of optimal behaviour, the first thing you do is search. You scan. You confront the epistemics of reducing uncertainty about what you need to do in order to fulfill your goal. So it's all about beliefs.
So in that answer is the basis of everything that I'm going to say. Your behaviour is always driven by beliefs. And that tells us something quite important. So here's you scanning and searching, and you found a little mouse that you might want to eat there.
And that's quite important because it speaks to two basic classes of ways of thinking about optimising behaviour. You can either imagine that there is some value function of the next state that will be brought about by some action and optimise that action by selecting the action that maximises the value of the next state.
That's the classical way of doing it, but that just doesn't work if the best next thing to do is to search and resolve uncertainty because uncertainty is an attribute of beliefs.
Therefore, the functional, or the function of a function that you need to optimise in terms of action you hear is a function of beliefs, which I'm deleting by Q, beliefs about the states of the world.
And that introduces a fundamental distinction between the sorts of schemes that you bring to bear in terms of understanding optimal behaviour.
The other thing about the sort of scanning and searching answer is that action depends upon beliefs about the world, states of the world and subsequent actions.
So, not only is it a function of beliefs about the world, but it's the order in which you interrogate that world. So, it makes a difference whether you search, then eat, as opposed to eat, then search.
And that means that we are in the game of optimising sequences or policies or actions. I'm going to call a sequence of actions policy.
So, what that means from the point of view technically of what sort of thing we have to optimise, it's a functional of a belief integrated over time or summed over time, a path integral.
And if we call that an energy, then the integral, the path integral of an energy is called an action.
So, what we've just said is that we've reduced the problem of good behaviour to Hamilton's principle of least action, where action is the path integral or the trajectory integral or the sum over an energy functional of beliefs.
And that's the basic premise that I'm going to pursue.
And just to highlight the distinction, if you subscribe to this way of thinking about how systems work, then you end up with optimal control theory, Bayesian decision theory, enforcement learning and all that good stuff.
Conversely, if you believe this is how biological systems work, then you end up essentially with Hamilton's principle of least action, the free energy principle, active inference, active learning and so on.
And that's what we're going to focus on.
And the energy function that I'm going to consider, we've already heard mentioned, is the variational free energy or the free energy, which we've already heard very roughly scores surprise.
It approximates surprise or suprisal, and it's simplifying assumptions prediction error.
So, what we are saying is that we're just in the game of minimising prediction error and more specifically prediction error over time, over sequences of behaviour.
I'm going to quickly go through this because it's an interesting, there are lots of interesting connections with existing theories and formulations.
This is a bit technical. These are both iconic and ironic equations. You'll hear more about those later on.
In words, if it's the case that good agents, good people, minimise their free energy, their surprise, their average surprise and their uncertainty, then they must believe that the actions that they emit will minimise expected free energy.
You can write that down very simply in terms of these belief functions here and rearrange them in a way that discloses important links with lots of established formal treatments of behaviour.
So, I've written the expected free energy associated with any particular policy in terms of its expinsic value here and its epistemic value.
Basically, these things store the surprise about what you predict will happen under a particular behaviour and what you think should happen, your preferences like, I'm going to eat a mouse, I'm not going to be hungry.
So, that's a surprise bit, explicit or expinsic surprise bit. There's another surprise, there's another sort of average surprise or entropy, a relative entropy, which is called epistemic value.
It's a reduction in uncertainty or the information gain and that's the key bit. It's the epistemic which is missing from classic theories, but it's part of this formulation of Hamilton's principle of least action.
That relates very closely to theories of visual salience or Bayesian surprise. Technically, Bayesian surprise is the divergence or the difference between a prior belief and a posterior belief or a posterior belief to be informed by observations here.
So, what we're saying is that we will choose to act in a way that reduces our uncertainty relative to prior beliefs, looking at data which gives us information that maximally reduces that uncertainty that has the greatest epistemic value or Bayesian surprise.
In fact, mathematically that's exactly the same as the mutual information between the causes, the hidden states of the world S and the consequences, the outcomes that we actually observe.
So, another way of saying this is that we are subscribing to the principle of maximum information, mutual information or minimum redundancy or maximum information efficiency of the sort articulated by Horace Barlow.
Always of expressing one particular form of perspective on this underlying functional. Another way of thinking about this in the case if there is no ambiguity, if we actually can observe the states directly, then we can discount this uncertainty term here and what we're left with is something called KL control which is the state of the art of what people would use in optimal control theory and dynamical systems control.
In economics it's called risk-sensitive control. It's minimizing risk. So, it's a surprise between what I think will happen and what I want to happen and if what I think will happen is surprising relative to what I thought was going to happen, then I have a high degree of surprise, a high degree of risk and I want to minimize that.
And then finally, if there's no ambiguity or there's no risk, then we reduce to classical expected utility theory or all the sorts of theories that reinforcement depends upon, just maximizing our preferred outcomes there.
So, clearly in order to be surprised, we have to have predictions against which we can match outcomes to score that surprise and this brings us to generative models and the departure that I promised you from what people currently understand in terms of predictive coding
and what I'm going to talk about for the next few minutes is I'm going to formulate generative models not for continuous state space of the source used in predictive coding of say visual angles or content or acoustics, but generative models in which we can label the entire world in terms of a number of discrete states.
So, these are generative models for discrete state spaces and they don't normally have the look and feel of predictive coding, but my story will be is if that they do, they are actually formally very, very similar to the sorts of schemes that we understand in terms of top down predictions and bottom up prediction errors in hierarchical message passing
and predictive coding in the visual cortex. So, in these models, all we have, this is not ignore the equations, but it's focused on this graphical model here. What we're saying is that the world unfolds in one of many, many states and the transitions from one state of the world to the next state of the world encoded by probability transitions that themselves depend upon how we act.
They depend upon the policies that we choose and we have a certain confidence in those policies by their precision or inverse temperature beta here. So, if we knew the probability transitions or the transitions from time to time of the states, we can generate a sequence or trajectory of states and each state and each point in time generates an outcome through this likelihood of matrix A and that's it.
That's the generative model. The world has states, they unfold and each state generates an outcome that's observable. And that's the basis of everything else that I'm going to say. So, if I'm now given a generative model, what I can do is I can evaluate the free energy of my beliefs under that generative model and I can then minimize everything with respect to that.
Proxy for surprise or uncertainty, namely the expected free energy and I can write down equations or solutions that tell me how an optimal agent person would behave in a sort of Bayesian sense.
And these are the solutions to the equations expressed in terms of the parameters of that model. So, A was this mapping from states of the world to outcomes and B was the mapping between subsequent hidden states.
And despite the complicated nature of the equations on the previous slide, the actual updates, the solutions are incredibly simple and furthermore, they look very much like the sorts of things that the brain does.
So, for example, expected states of the world are a non-linear sigmoid function of linear mixtures of expected states of the world and observations. So, we're mixing together evidence from outcomes and our beliefs about the states of the world to update our beliefs about the current states of the world.
Our beliefs about what we're going to do next, our policy pi here, is this a softmax function of the expected free energy weighted by an inverse temperature parameter that you will see we associate with dopamine, a classical softmax response rule.
If you're not familiar with that, that's what people in economics and choice behavior use for those people who deal more with perception.
We also have a model of incentive salience. The confidence or the precision or the inverse temperature associated with our beliefs about action now becomes, has a base optimal solution that depends upon the goodness of a policy or the negative goodness, the expected free energy here.
The form of these equations speaks to a rough anatomy of computations in the brain, a computational anatomy. It goes like this, where we have these equations dictate what each update needs to know about the other updates.
So, basically, it prescribes a connectome for the exchange of information or sufficient statistics that is implied by placing the Hamilton's principle of least action on the simplest sort of generating model that you can imagine.
That's the sort of anatomy we have here. Outcomes, expected states, expected policies, the goodness or the expected free energy of policies, the precision of policies, states in the future, which prescribe action.
So, I won't go through that, but I'll just give you a more heuristic version of that one. So, what those equations tell us, so this is like a very top-down argument.
It's not, you know, let's think about how the brain works and come up with some hypotheses. This unfolds or unravels from, impacts from, just applying Hamilton's principle of least action to a very simple generating model.
And what it tells us is that sensory input comes in, say, at the back of the brain. It informs and updates expectations about hidden states of the world, sometimes referred to as state estimation.
They are associated with a free energy or a surprise that is combined with an evaluation of those states in relation to prior preferences and their potential reduction of uncertainty, their epistemic value.
They are combined to give us beliefs about the policy that we are currently pursuing. We have a certain confidence in that policy.
And then those policies are used to weight all the different states conditioned upon what we are currently doing to give us the best estimate of what's going to happen next, the next state of the world.
And if we know that, then we can choose the action that brings about, that realises our expectations, our predictions about the next state of the world, that action solicits a new observation from the environment and the cycle begins again.
So, we have a perception action cycle that falls out of the minimisation scheme that we've just been talking about.
So, very briefly, I'm just going to show you how that sort of thing works with a series of examples, and then hopefully I'll turn it over to you to see what you want to talk about.
The first example is just a very simple simulation of foraging in a two-arm maze. So, in this example, there's a little rat here, and there are rewards on the right and the left arms of the maze, but the rat doesn't know where the reward is.
But there's also an informative queue at the bottom of the maze here. If it went to solicit that queue, it would then know where the reward was, and it could only make two moves.
So, it can either take a chance and go to one of the other top arms, or it can be a bit more clever and resolve any uncertainty about the context it's currently operating in, which arm is baited, and go and retrieve the epistemic value of the informative queue and then make an informed decision.
So, this is exactly the searching that you were talking about before, scaling your environment, knowing where you are, resolve your epistemic, solve the epistemic problem, and then turn to your prior preferences or your pragmatics.
You can write this model down in very simple terms of these A and B matrices here. There's partial reinforcement here, and the C matrix here just denotes the preferences in terms of what sorts of states this rat thinks it should occupy, basically thinks it should be in the baited arm and not in the unbaited arm.
That's all it's saying here, with minus threes and plus threes on the upper arms that are baited. And if we do that, and we just integrate those solutions that I told you before, we actually generate very realistic behaviour, summarised here in terms of the expected policy and the policies that this agent or this little animal can entertain.
It stays there and then goes to one of the three arms, or it goes to one of the two arms, or it goes to the bottom and then goes to any of the three arms. So there are eight policies here.
And what it does in the first instance is because it doesn't know where the reward is. It gets the cue and then obtains its reward. What we've done here is actually baited the left arm all the time.
So slowly it accumulates evidence that, in fact, the reward's always on this side here. So as time goes on, it actually switches and learns, and it's probably better to avoid or dispense with the epistemic move and go directly to the reward.
And it starts doing that after about 20 or 30 trials here, at which point its reaction times, and this is the actual floating point operations of the scheme, decrease. And because the goodness of a policy is this path integral, it's actually spent more time being rewarded.
So if you like, the payoff also increases by going straight there. So this prescribes good policies, and it can be used to simulate nice behaviors of the sort you've seen experimentally.
But what I want to do finally is just connect that to neurophysiology and neuroanatomy. But to do that, I have to have a process theory. I have to have a theory which says this particular neuroactivity or this particular connection strength corresponds to this quantity in the model.
So I have to have a process in play that is neuronally plausible. And the way that we're going to do that is just take those update equations that we've seen before, and instead of just writing down the solutions mathematically, I'm going to recast the solutions in terms of a gradient descent or a hill climbing, or actually a hill descent here.
So this is a standard way of optimizing something. If you've got a quantity you want to minimize, you just go downhill until it stops getting smaller. And if I do that, I can write down exactly the same scheme in terms of differential equations on expected states of the world.
The very similar form, but here that's a rate of change of activity, which is now a nonlinear function of linear mixtures of expectations about states of the world and the observations. And in doing that, I've created a dynamical system that now has as much closer to the look and feel of a neuronal system.
And that now enables me to look at the dynamics that underlie the behavior. And these are the dynamics here, and we can basically break these into inference and state estimation in terms of the updates or the fluctuations in the states as new evidence comes along.
Policy selection that we've already seen with our softmax response rule, and learning as we accumulate from trial to trial evidence about particular states or contingencies of the world in this instance that the left hand arm of the maze was always baited.
I illustrated those things here, a couple of interesting things to note. First of all, with every new move and every bit of new sensory information, there are lots of fluctuations in these states that look very much like an ERP.
Furthermore, when we become a little bit more automatic or not habitual, but certainly going straight for our reward, there is an attenuation of these responses.
The confidence, the precision in those responses also shows these phasic changes and progressive changes as we learn the context.
So we actually get something which looks remarkably similar to transfer of dopamine responses as we become more familiar and more confident about the outcomes that we see.
Let me just quickly show you a couple of those outcomes. This slide highlights just one trial, and it shows the representations of time over the different hidden states of the world, and just highlights a couple of things.
First of all, it shows that as we accumulate evidence for our preferred policies or our preferred outcomes, the probability that we are in a state which we will ultimately choose increases whereas the probability of states that we don't decreases.
And this is formally identical to evidence accumulation or drift diffusion models, but now cast or now a consequence of a gradient descent on variational free energy or a bound for surprise.
What we also see is an interesting dynamics in the sense that if information keeps coming in, say 250 milliseconds, like the frequency at which we go and sample the world with iconic eye movements,
that means that we have two timescales in play. One is a theta rhythm as we go and get information once, say four times every second.
But within each sampling there's this fast updating that's minimising and optimising our beliefs, and that faster updating has a temporal scale in the gamma range.
So what we see is effectively, as we move along, fast updating that repeats itself every theta cycle, but as we accumulate more and more evidence we get more and more efficient and confident about the things that we are inferring.
The dynamics mean that they accumulate evidence more quickly, more efficiently, and we get a phase procession of the sort seen in the hippocampus.
I've already mentioned that, as Tang goes on, by virtue of increasing our confidence as we assimilate this evidence, then that confidence is expressed in the confidence of our policies and we have a nice way of assimilating dopamine responses.
We can look at the behaviour or the activity of these representations of different states of the world at different points in time during our policy, and if we plot their responses as a function of where the rat actually is, we can simulate place cell activity.
There has many characteristics of the sort seen empirically. This just illustrates this theta-gamma coupling, which is an almost necessary consequence of this sort of solitary sampling of the world, and then updating bleeds quickly before the next sample comes along.
Again, the sort of thing that one sees empirically. We can now do violation responses exactly as Jim was talking about. What I've shown here are the responses to two trials.
They're identical in nature, but one is from the beginning of the trial where the rat was not familiar with its environment, and one is at the end of the trial when it becomes very familiar just before it starts going directly for the reward.
Interesting, if we look at the representations of key states here, what we see is a much more efficient and therefore less exuberant updating of expectations of hidden states that if we subtract the standard familiar one from the oddball or the unfamiliar one, we reproduce the temporal dynamics of things like the mismatch negativity in ERP research.
We also demonstrate this transfer of confidence or simulated dopamine responses from the rewarded cue per se to this instructional condition stimulus here.
I'm going from slightly negative to positive here. We can play similar games by introducing deliberate violations and illicit P300 responses. We can look at reinforcement learning by switching contingencies halfway through and look at the effects on dopamine-urgent responses and also electrical-physiological responses.
How long have I gone?
That's very good, isn't it? I've only been talking for 25 minutes.
That I can be true to my promise to finish in half an hour. This is the epilogue. That's the story so far.
Most of that will be in the next few weeks in the published literature. You'll notice at the moment there's nothing really about hierarchies. Most people here, I'm sure, are more interested in the implications of this sort of theory for perceptual hierarchies and evidence of accumulation and purely perceptual domain.
The more recent work that I wanted to, this is not published, to introduce you to, is now taking this formalism, which has a lot of construct validity in relation to choice behaviour and your economics, active vision, active sensing,
and see what it has to say about the sorts of themes we're more interested in, which is the hierarchical message passing and the deep generative models that we assume that the brain is using to actually understand perceptual sequences.
So this is the epilogue. Again, I'll just speed through this in five minutes. What we're going to do now is tell exactly the same story, but now we're going to put one of those discrete state-space models, they're known as Markov decision processes, on top of the first one, and another one on top of that, and another one on top of that.
So in this construction, hidden states, at any one level in the model, don't generate outcomes, they generate the first or the initial hidden state of the level below.
And then that cycles over a few iterations, and then terminates like the rat-terminated, when it entered the baited arms of the cues.
And that process repeats hierarchically to any arbitrary depth. So what we have are deep temporal generative models. And they're really interesting because not only do they have a hierarchical structure in their form, but also in their time, because if the state at any high level is generating the initial state that must have subsequent states,
then it means that the lower states unfold more quickly than the higher states. So one way of thinking about this is the generative model says that at this hour, at this minute, and at this second, I am safe, I was reading, I'm on this page, on this paragraph, and on this word.
So if you think about the lower levels as ticking over more quickly, like the second hand of a clock, and every revolution or every trajectory or every path they take, then the high level goes forward one step, and then it goes round again, it goes another step, another gain, and then another step.
And then that process is repeated. So as the minute hand is going round, once it goes round, then the hour hand goes round. So what we have here is a generative model that basically has in mind, literally, beliefs about the world that are much more protracted in time and are hierarchically nested.
So if you could invert this sort of model, you would have a representation of the context, and the context of context, and the context of context. At each point, as you go deeper into the model, they are more temporally enduring.
So you would know that working from the top down, you would know the story of the narrative, if you knew the story of the narrative, you would be able to generate a particular sentence, if you could generate a particular word, if you could generate a particular word, you could generate a particular letter.
All faster and faster are more elemental timescales.
Ond mae hyn yn High Levelatoedd, ac mae'nuttering, ond mae rhyngwledig felly mé amlo.
Dyleson wahanol os nனwr yr unrhyw arall,
ryngwethaf chi o chase i mae'r hyffigfa?
Dy'r hyffigfa syr wychond oedd i ysb backup.
Cym最近in papurart, dy n own d Aub mwy f carbraig maen,
o Zeus i gyfo newydd perdynillwyr wedi ddifol.
Ond mae'r hyffigfa wedi gael cyflding Windows,
mae ydych chi'n ddiddor者eth ber parliament yn ychydig.
a'r red mwyllt i'r
erddwyd y ddigon sy'n oesch chi'n gwrth ag anewod妹iat ag bynnig.
if I express them in terms of a grading descent on free energy.
What we've done here is to write them in terms of prediction errors,
so we're back now in the rhetoric of predictive coding
and things that people are familiar with.
The prediction errors here are basically
the difference between the log of an expected state of the world
.
,
newydd gymryr werth yn canallweiad rhai
ac y gWAith yn yfhysиваемol.
Mae hyn yn drwsail yn beth ond,
ar y cynhyrchu cy competence anhyniad.....
o phroesio cyIDau
a'r hystyosit tryth workedeithas lively gyda siwad.
Llywydd,妹goed y pryd cympest
ffind yr ydyn ni oedd yn ar也o'i gael arddangos yn defnyddio'r drunion ddiddordeb hyn a'r gyfan,
dwi'n fent Patreon Ym M designingel m foreseeancol.
Yn dod ond am y functionalityline, mae'r ydal am Mountain
gwanildiad o'r rhan oedd nifer lwan seller am b JERMAظio panigwr sydd eisiau o gwahanolbau
o blWEi yn ichi ei gwahanol bydda'r fawr i wneud hwn ofunnall ar edryctions πάlen amddangos
ar hyn o'u dud yn éch hoff어야d a gyllide'r modelor hyn.
dispose charkew chatio mae'r sely excellennu amddangol sydd wedi g remarkableion had br drugwch,
er mwyn bod eich ddweud ein ffordd.
Mae'n dweud hynny mewn ffordd phennigau a eu 표fau.
Dweud ar yr ad SCP-18 saen рад.
Y newydd yn oes i'r ffordd, yn ad warned fra i negau yng nglyducau sydd gwaith mewn rhysig,
ac am hyn nhw yma i'r modd yn cael y mas iddynt ac amgarulladol rheol.
Y Parliament D Welt Unigfbanenau.
Nid ynックas hyn gyda llunio gwaith phran revelationol i'r ad SCP-18
ble neby con shaft goeth y
sg roamant rant.
Mae hus COVID eraill o slipo'u bobły democrrias.
Mae'r ddiddordeb yn
hefyd, mae'n comet agynwys wneud a napuc o'r
holl ychymor, yna'r ddefnydd fan ffyn cinfr
scribach i'r makes ystan y, masgrifi a'r
macrocholim o Teach Colt bydd o'r boul agor y credu.
Mae'r fugwladau hyn yn tytol se cognition y dyfod plant yn dim.
Onw ychydig ym preached, ond blendirþivot a'r môl sydd cyrylocol fe ddiwedd narrower syddiad.
Owch munaf eich lawer fflasferau a'r rhan mewn sen yna wrth ei dd такоеu,
ac mae rhoi bod i'r diwedd i adge см знакомu a ichi gofyn yn y methu nhw,
c podría ddim lle gwyfer a'r succeeded beesparwyr yw bod i'n colli GYddeodag yn meddwl nhw apple hwrdd.
Fel y dyfodol barsodd tua, gweld o Gwydrych programme yr amdano ac yn gwybod yn fy datblygu atoliach agefwyr.
Sabwch fod yn y egg jwysig ei fyddent yn cael ei wneud bwynd i mi doing at감 a ch learning clear
wrth wedi addysg, ac disturbed bwyddiol ar gyazif mil o mas сразу pa ysbarthau
yn yn ysbyg mewn rowan nesaf yn gyfer lle o'r blas gwzir am wrth weld wrth gweith competisiwn hefyd.
Efallai arBT andaill gyring', eras yn wych chi hefyd cyfaint ar gweithio eich hyffordd bydd sea dimension yn tatymiem ar gyfer maedlo CAD hynny
That's just cartoon here in terms of the extrinsic connectivity.
Again, we can go back to this if people are interested, but there is actually a remarkable correspondence
between not only the deployment, but also the sign of these extrinsic connections
and those dictated by that particular message passing or gradient descent
that I showed on the previous slide.
If you subscribe to this scheme anatomically and the theory being a metaphor
y metafoyl harypoonsch cyffred wallsol, ip yn gynnalfa enwauótiaeth y byd yn ei beth yn d positivity.
Gwurlochoggynt struggai ag fydd yn dangyffred.
Rwy'r gwael hwyl i led لم jfotwch a'r rhaid storrolyniiddner yn ddechrau gweithasol.
Gweith痴 o eich teabod hyllennwyddon oherwydd mae hwyl yn mynd a'r ynaall hwas am l101 uall y llaw Llyfr~?
ti wynllun, the putamen for motor loops,
whereas the policy expectations per se
have here been assigned again to the國d oes pan agnasybno.
So, just a way of getting from the mathematical anatomy
to the biological and the neuro-anatomy
in a purely top-down, Res难wle at the mathymatical drywet.
Only by taking the equations and seeing what form do they imply for message passing and
what sorts of message passing do we see in the rail brain.
Andre Bastos, I think we may not, but he did a lot of work on this sort of intrinsic connectivity
within a macro column, clinical micro circuits, predictive coding.
Exactly the same game can be played here
for this discrete state space model.
One key exception which Lars might like,
Llywodraeth newid yn y Sgolion ydy'r cy Lyw ar flod a elevator arnyn nhall entag i newbornin
aparturau holl
yn isiau dilwr wedi cael y freunion ullref,
sicrhau ond mae fyllwededau am 000.
Iсти ond mae'n pethau morthadaff yng Nghyrchu Lwyddi Mer duplicatell,
rydych chi ddalodiau a'r bobl yn rofynol.
bound yr siŵr iawn sydd yn awrddod y modelau yn wath yn gweith Persianion Lweddyema.
Ond oherwydd rwy'r pa geneill bumai ac los freel yn gwasrydd fel erioed.
Yw fe allfydliedd gweithple o fragonnawn
os hwn o ye reachedron 50% yng Nghymru ddyn ni'n mynd i ei ddyluogstudiau mai eisiau cyfnod,
i'r bryd perthemi nwr f htt addyr First Order 4.
Mae'r ursau amddiant i d 까idoddech chiach yn gweithio intens тебя,
wedi fyrdd y cydotiaeth iawn, oherwydd mae'n murdd hyn tu yn llwyddaf hynny passengersa head ac'i
áty o'r minell yn fydda i y fel Model Onigra
critique o'r genbl am hyrals y gyf obviously
nad yw'n hlink dim o'r dylstage i llawy да,
a ond a llwyddi hun i'r pawl questo o ddud traumaticau
ac yma fetodd y sesameus mor hynni
a ysbytio chir fy r sharply
ac derivative cyfrwyng
i wahead o'r newid i'r mud
Ac y티 tot allwch allwch o'r ddjust Bellaol
talwch o'r model syddatter
Tynned o gwnaeth priwydwyr
yn tu'r llwydd
Mae'r word ar here has iconic letters that can either be an uppercase or a lowercase,
the palindromic in the sense that it doesn't matter whether the word, whether the cat
has to flee from the cat, doesn't matter whether we flip them in a horizontal way, it still
means the same thing, but it does, this agent is a surprise if we use a lowercase.
Ogymnasol ac mae hyn yn uneisio gyda ychydig i, am da't hwnna i chrywg mewn cyfgoiol,
i fod ychydig te Mercury Maen nhwhopi Gerry D europt maed i programs cyannoedd,
a'w affectedio glannu, oedd iawn'r rhagin maen nhw'n ddweud â gwayn.
But there are icons in this instance and then with this scheme we can simulate things like reading.
So here's a little four page story or sentence and that word is flea, that word is weight because there's nothing next to the bird,
that word is feed because there are seeds that the bird can feed on and that is weight.
So this is a sentence, flea, weight, feed, weight and that's a happy sentence and it will categorise it as happy.
But the problem that we're trying to address here is exactly what we started with, how do you scan, how do you search,
where do you go and forage for information to resolve as much uncertainty as you can about which of these six sentences is in play.
And when the system does this just by trying to minimise its expected free energy it shows this very interesting behaviour
where it jumps from one word or page to the next without really dwelling and wasting time resolving uncertainty that is already resolved.
So once it sees a cat it already knows that this has to be a flea word and it doesn't need to see where the other letters in this word are actually doing.
It already knows, there's no more epistemic value to be had, there's no more uncertainty to resolve.
It'll now jump to the next page and resolves in search after a couple of surcannock high movements, then jump to the next page and after this one surcard in the final page
it knows exactly what this sentence was doing and if I can I'll just show a movie of it doing that.
So the red dots correspond to where it's looking at the present time and the images that are mixtures of the icons represent conditional expectations.
And the main point to be taken from this is that it knows there's a bird there but it never looked there.
It has sufficient prior knowledge in its deep, temper temporal model.
It doesn't need to actually go and see stuff, it knows stuff is there because it knows what caused that stuff.
And with this sort of simulation one can then do exactly what Jim was talking about which was if it knows stuff and it has predictions
then it should be possible to disclose or reveal that knowledge, that predictability by introducing violations and elicit the sorts of classical responses that we see empirically.
And what we've done here is because we've got a deep model we can do local and global violations, we can make the final story, the final sentence a very surprising one without changing any of the stimuli
at the same time with or without making the priorities about the upper lower case, the sort of local feature expectations.
We can switch those around so we replay exactly the same stimuli and the same behaviors but just by changing the priorities of the agent we can cause certain things to be surprising
and those things can either be at the local, the first level or the higher, the second level.
And if we do that we get lots of behaviors that look again a little bit like delay period activity in the prefrontal cortex of a periscadic sort that you see prior to a saccade being selected and enacted.
While at the same time the band pass filtered voltages that are being driven by the implicit prediction errors look very much like periscadic ERPs and when you look at those periscadic ERPs under local versus global violations
what you actually see is something that looks almost identical if it's a local violation to a mismatch negativity whereas for the global violation you get the mismatches or the differences much later on in time
that look very much like a P300. I can see what I was going to show you. No I can't. Let's again. That was a very pretty slide but I can't.
It's very clever. It's a quote from, well you don't need to know that.
And then the final slide it's got a thank you. A lot of people were on this slide but you'll never know who now will you? So thank you very much.
So the workshop is structured so that there's a lot of time for discussion after each talk and so the floor is open now for people to ask questions or make observations.
I was just informed about how they do this in philosophy conferences that involves raising your hand or your finger but I haven't mastered that yet so I don't understand it.
So I think it's too complicated for this group. But not for philosophers. So who would like to start?
I'll start. I was just wondering if you could say more. You mentioned that you get something when there's choice involved with the rat experiment simulation.
Something that looks like a drift diffusion model and I've always been puzzled at how you get something that looks really like choice or agency out of a predictive coding model.
So maybe you can elaborate a little bit on that.
So the question is where does the choice come into predictive coding? I think that question.
Let me just be a little more specific. It's not that I don't see how you get choice behaviour in the sense that you can use predictive coding in order to evaluate some options.
But the notion of agency seems, if what you're doing is just predicting what you will do then it seems to kind of undermine the notion of agency.
I see. So the answer to that question is very simple. It's very simple. You put agency into these schemes through prior preferences define the sort of agent that I am.
So we were talking before about reducing surprise of all sorts, whether it's epistemic uncertainty, but the simplest sort of pragmatic surprise.
If I have a cost function that I don't want to be very hungry or I don't want to end up in an arm that has no rewards in it, then I'd simply have to have the prior belief that at the end of the day I will end up rewarded or sated or happy or complete.
So that anything else that happens is surprising and therefore I can then bring the whole machinery of predictive coding to bear upon the problem of suppressing prediction errors and surprises.
So pretty very simply in terms of predictive coding. If I a priori believe that I'm always going to be happy and complete and that I am built to always minimise my prediction errors in the future, then I will look as if I have agency.
I will look as if I have purpose because I will always choose my actions in a way that avoids the prediction errors that suggest that I am not happy and complete.
So the answer is just to absorb cost functions into inference by making costly states surprising through prior preferences and that comes out of things like planning as inference.
There are lots of ways of articulating that from the point of view of the rhetoric that I was using. The expected free energy has two bits to it. It has this epistemic bit and this pragmatic bit, but very simply it's uncertainty and surprise.
The epistemic bit is minimising uncertainty. The value, the purpose, the goal is a pragmatic bit defined through cost functions that are literally the surprise of a costly outcome.
So just to follow up a little bit, so is this sort of like a hyper prior that's going to be, I mean it sounds like we all have to have this ultimate belief that it's all going to end well at the end of the day.
So pessimists, none of us are really pessimists or something like that, right?
By definition. You may be perverse in your optimism, but you are quintessentially optimistic.
The deeper backstories behind the free energy principle, the only assumption that this instance of Hamilton's principle of release action makes is that you exist and if you exist, that means you behave as if you have beliefs that you exist.
By existing, that just means that you're not decaying or dying. All your states are within some bounds, be they physiological, homeostatic or pecuniary in terms of being rich or in terms of interceptive inference and the hadonics on that happy.
But it's all about keeping things in bounds, it's all about minimising entropy, minimising uncertainty. So it always looks as if agents that exist have prior beliefs that they exist.
And when you unpack that, that simply means I have preferred states that I will expect myself to occupy. Literally they are attracting states, they are an attractor.
So that rhetoric, which actually is a rhetoric from dynamical systems theory, applies identically to this sort of purposeful reinforcement learning or sort of goal directed style of thinking about things.
There are attracting states, they are simply the ones that you frequent, which means that you will appear to behave as if you have prior preferences for being in those states and you will always choose actions to get to those states.
It is those prior preferences that define the sort of agent you are. So in answer to your very first question, the agentfulness comes in by implication or just through the sorts of priors that characterise that particular sort of agent.
So if I was a virus, I would have very different preferences than if I was a person. But there are still both plausible and viable preferences and sorts of agents.
Hi, thanks for your talk Carl. I was wondering now that the slides are back if we could go to the delay period activity that you had briefly mentioned.
I just wanted to see that a little bit unpacked and related to what you were just talking about, that is agents reducing their free energy. How does that principle then generate the delay period activity that we see in places like prefrontal cortex and in working memory and so forth.
Thanks.
Right, well, those sorts of phenomena which we all know and love and will try to explain and measure empirically. So let me just try and find it. Do you remember where it was? Right, thank you.
There you are. All those sorts of nuts and bolts getting down and dirty in terms of what this scheme would do when you put dynamics on it through the gradient descent, depend upon the generative model.
So at your very similar to the last one key component of the generative model are the prior beliefs, the preferences, what gives it purpose, what are its goals. Your question I think has a very similar answer.
Once you've written down the generative model, everything else is not up for discussion. The maths tells you exactly what has to happen once you've written down the generative model.
And the delay period activity you're talking about simply follows from the fact you've got a deep generative model or a deep temporal model.
So as soon as you write that deep structure into the model, it means that certain beliefs have to outlive or change on a slower temporal scale than other beliefs lower in the hierarchy, which means you have to have delay period activity whilst other stuff unfolds at the lower levels of the hierarchy.
So in this particular example, what I've done here is show the beliefs about the six sentences over the four moves, giving us six times four moves or five moves back.
So it should be about 30 beliefs here. On the same time access as beliefs about the particular word that's currently being seen.
And these resets here indicate the onset of saccades and the acquisition of new information. And you can see roughly every 250 milliseconds there's a saccade and new beliefs are updated about the current word.
But each, at the high level, we're only considering beliefs about each letter, my point, is we're only considering beliefs about the word.
So beliefs about the word corresponding to what's on this page or what's in this word are invariant during the successive saccades as you sample the different letters.
So these things change more quickly than these things. When these are completed, then there's a change here, and then the cycle begins again.
So these tick over faster than this, and then this looks a little bit now like the rastles that you see prior to the emission of the saccade here.
They're not from the same paper, but a related paradigm. If I take the voltage causing this delay period activity and band pass filter it, you get these sorts of fluctuations out here.
So when there's an increase in delay period activity, there's usually a positive deflection that looks a little bit like an ERP.
And just to follow up, so is the presence of delay period activity, is that associated then with prediction error or with the build up of a prediction?
No, I think the prediction error in this scheme, and this is the maths that comes from the discrete aspect of the genetic models, lies in the rate of change of neural firing.
If you associate the biophysical encoding of expected states of the world in terms of population firing rates, then if you subscribe to that, if you accept that, then the prediction error now becomes the conductances that drive the depurisation that drive the firing.
So these basically reflect the fact that as time goes on, you can more and more confident that one particular sentence is in play.
And you can see that this is beliefs about the, which sentences in play at the first, second, third, fourth and fifth eye movement or page, sorry not eye movement, page or word.
And they are now internally consistent and at every point in time, in the past, at the end, I now believe I was reading the first sentence.
And that belief endures during the sampling of all the actual letters within each of the words.
So these would now represent just basically numbers between 0 and 1, 0 and 100% neural firing that score your expectation that this is the current state of the world.
At the beginning, there's lots of ambiguity. Not an enormous amount, but there is ambiguity. It's 50-50 because of the six sentences.
Only two of them begin with the word flee, which means that we resolve our uncertainty about four of them, but we're still ambiguous about having ambiguity about sentences one and four.
And that can only be resolved at the end because these sentences only differ in the words right at the end.
So during this time, there's today a period of activity which we've got these two explanations, hypotheses in play, that are resolved epistemically, optimally, right at the end when we get to the last word here.
And it's a weight and that determines which of the letters it was.
Thank you for the talk. I wanted to go back to this idea of this contrast between reinforcement learning and the kind of formulation that you're making here.
So one of the things that I thought was interesting is this formulation in terms of external value plus you basically decompose your KL diversions to external value and epistemic value.
So how do you get exploration in this model?
So it seems to me that you're doing an armax over actions to get some balance between immediate value and information gain.
Is that the basic idea?
Yes. I mean, we can look at the equation or we can look at this. That's absolutely right.
Well, the exploration is good that you brought that in because another perspective on this is the whole foraging ethological perspective on exploration versus exploitation.
That rhetoric just maps very simply to the epistemic and the pragmatic.
So there is no, again, there is no up for discussion or there's no ad hoc waiting between the two.
The expected free energy can always be written down in terms of exploration plus exploitation in terms of the epistemic value and the pragmatic value.
And what happens is in minimizing that one quantity, you get this scanning searching behaviour until the epistemic bit has been reduced, allowing then you to focus on the pragmatic bit.
So for free, you get a base optimal exploration to the extent that it is sufficient to resolve uncertainty given the precision of your beliefs about your prior preferences that then allow you to pursue your girls.
So this solves the exploration, exploitation dilemma in a base optimal sense.
It also suggests that the very carving of behaviour into these two complementary drives is actually probably a misdirection.
So it's only you and me that have actually teased apart the two components of the expected free energy and called one an epistemic exploratory one or a novelty seeking one and the other bit a pragmatic cost function like rewarding preference goal directed like one.
There are lots of different ways of rearranging those.
There are also a range in terms of risk and ambiguity, intrinsic and extrinsic value.
There are lots of ways of carving them and getting different perspectives.
When you see that and when you work with that, you start to realize that it's not necessarily the best thing just to have one particular religious perspective on it because it lends you to the false belief if you subscribe to this formism.
There has to be some other adjudicator.
There has to be some other homunculus that's decided, oh, I need to explore now.
And now I've done my exploration and now I'm going to go and do a bit of pragmatic, you know, stop the scanning and then I'm going to go and, you know, exploit what I've discovered.
It doesn't work like that. You should get that for free.
If they're both part of the same cost function, then once you've sufficiently reduced your uncertainty, you go into, as illustrated here, your exploitative behavior.
So this is exploitative behavior or novelty seeking in the sense that you don't know what the cue is going to tell you.
It doesn't have any immediate rewarding aspect to it.
There are no preferences associated with the condition stimulus, but it's interesting.
Uncertainty resolving.
But after a time, it becomes boring because you already know what it's going to tell you.
And once it does that, then you get exploitative behavior.
So I should have done that. I should have put exploration, exploitation.
Have an argument with me because there's meant to be a discussion.
Do you not like that? I'm telling you.
It's very interesting perspective on it.
I'm surprised that it comes out that the simple deterministic policy works well and just kind of works out of the box.
My inkling is that when you go and implement this stuff, it can be difficult for it to balance the exploration and exploitation.
So there's no tuning parameters. There's no off policy sort of estimation.
You just throw it in there.
It all works out.
I know exactly what you're saying because this was a big selling point when we first realized that a couple of years ago.
And it will remain a bit like one of these five to ten year changing the direction of the ocean liner or the oil tanker.
But two years later, all that we're saying is, in fact, people behave according to Hampton's principle of least action.
That's all that we're saying.
And that implicitly, or it looks like, that behavior has this dual aspect.
But it doesn't, if you formulate it as a variational principle of the sort you did at school when doing Newtonian mechanics and then Einstein did with general relativity.
It sort of falls out of the mix in a way that does actually dismiss these separatist perspectives
on, I can either do this or that, and I've got to now optimize the exploration in relation to the exploitation.
And the key trick that puts you into this simple world of Hampton's principle of least action is the realization you can't prescribe good behavior
unless the prescription is an optimization of a function of beliefs.
So a really simple example would be an economics game.
I've got a really high risk and a low risk option.
But there may be a third option, which is if I don't know which is which, I should do nothing until I know more.
In using words like I don't know, I am now saying that my behavior now becomes a function of my knowledge or my uncertainty.
So I now induce different options, different behaviors, different policies and actions that rest upon my degree of belief,
which means you can't do it with value function optimization or utility function optimization or reinforcement learning.
It can't be done with queue learning. It cannot be done with a Bellman optimization scheme.
Because what you've done is you've said that there are better behaviors when I don't know what to do,
which I generally don't do anything or wait until more information comes about.
And once you write that down, you make your objective function a function of a probability distribution,
which becomes an energy, and then you integrate that over time, and then you've got to Hamilton's principle of least action.
So the simplicity post hoc is evident for me anyway. Does that make any sense?
Can I ask you a question about the general approach here, which is,
I see that you start saying that there is an urge, there is an force to reduce surprise.
So this is like I'm making an analogy with physics because that's what you are doing.
And there you start saying I can rewrite the whole thing instead of force in terms of a Fourier energy.
And then you go on and explain what it implies.
My fear is that there is a difference here between what we do in terms of behavior.
First of all, reducing behavior to just minimizing surprise, there are other forces that we cannot measure.
We cannot even measure forces, force in terms of reducing surprise in an individual.
So even though you can write these equations and describe something general,
so you can prescribe what should be the brain doing, do you think you can actually make any prediction?
I know you're doing it, but I'm asking how can you think that you can do it
because there is no measurement to tell you that's actually the case.
So in physics you could come up with any new evidence, you would write a new term into your equation
and just keep adding it and then you're always safe because there are some conservation laws
that basically will keep the Hamilton principle intact.
Well, who said there is such a thing in terms of behavior?
The simplest thing to assume, there is no such thing.
So then by doing that, if you let's say you keep adding terms because you believe that what we are doing is just reducing surprise,
this is the ultimate goal of behavior, then just keep adding terms to your free energy
and then basically the whole thing becomes a tautology
because you're assuming that you're adding terms and there is no proof or disprove for that.
So that's my main question is that how do you think it's going to work?
Right, that's a very good, I ended quite a bit weakly, but that was a very good question.
So there are lots of really interesting issues there.
So the tautology issue, the practical utility of this style of theorizing, can it ever be falsified adding things too?
So I think we could spend hours talking about any one of those.
First of all, the whole point of my style of neuroscience is that we never add anything in.
We're always obliged to, for every advance, you have to get rid of something, which was a distraction or ad hoc or a heuristic.
We've been talking about this magic parameter, the nuances, the balance between exploration and exploitation.
So that just goes, there's only one thing that's being minimized here, about everything, and that's variational free energy.
That's it, there's nothing being added.
However, of course, the free energy is a function of the gerontic model.
All the interesting, all the hard work, all the heavy lifting, understanding this biological system in this experimental context or this social context,
that really calls upon you writing down a gerontic model.
So it doesn't mean there is no free lunch.
There's still a lot of neurobiology and psychology and cognitive neuroscience and computational neuroscience to do.
It's just all at the level of the gerontic model, not the normative principles or the variational principles behind it.
Part of that drive for simplification is a drive to get to the ultimate explanation, which has to be tautological.
For me, the free energy principle is tautological.
It's as tautological as the natural selection, but beautifully so.
Once it's completely tautological, I'll be happy.
Part of that tautology comes along in a slightly technical guise called the complete class theorem.
I'm bringing that to the discussion because it speaks to, I think, a very interesting point you were essentially making.
Is there anything that you can not explain any behaviour in a real biological system that cannot be explained by this?
Because if there isn't, then what's the point?
Now, if there is no behaviour that this cannot explain, so it is provably true that for any pair of cost functions
and behaviours, there are a set of prior beliefs, prior preferences that we're talking about,
that endow the behaviour with an agent, with agency, that render that behaviour base optimal
and, by definition, therefore conforms to the free energy principle.
So what that means is that there is no behaviour that this can't describe if you can find the right prize.
So is that a weakness or a strength?
Well, in the sense of falsifiability, it's a weakness.
In the sense of actually using it practically, it's a real strength.
Because what that means is any system, normal human being or psychiatric cohort that you bring to me,
if I can solve the problem of getting the most appropriate or sufficiently good generative model that describes their behaviour,
it means I can quantify exactly the sort of person they are by their prior beliefs.
And we actually come right back down to the first question again, is what makes an agent an agent?
It's their prior beliefs. It's that attracting set that describes the sorts of states that that sort of person occupies.
So, yeah, there is a deep tautology. There's a fundamental difficulty for falsification,
but there's also a glorious insight underneath that,
which means that everything can now be written down in terms of an agent's prior beliefs.
And that if you can get the right model, you can actually estimate these things.
You can actually quantify them.
And this is one of the tenets of computational psychiatry.
It's to be able to use games, ERPs, mismatch negativities, whatever, in the service of say,
what sort of person am I looking at, quantified in terms of the prior beliefs about the way they should behave?
Carl, but aren't you making the assumption that there is just a unique set of beliefs that will match a data set?
Because if you have several beliefs, and I would think that when we minimise those type of system,
that's equivalent to several solutions, and which almost always we've got several solutions,
because we've got several minimise there.
Because it's not convex, the story that we are minimising there.
So, therefore, we have a lot of minimise, and I would say that if we look here,
that means that for any behaviour that we can measure, we are now with a set,
and that could be thrilling to know which one, a set of prior beliefs.
And so, in other words, we cannot inverse that, or am I wrong?
No, no, no, you're absolutely right, but you've taken us into metabasian land now.
OK, so just those people who may be getting a bit confused.
So, the argument, I think, let me just paraphrase it.
If we're now saying, well, how do we practically use this style of thinking?
And I'm now in the job of quantifying this person with autistic spectrum disorder,
given a bead's task, an earned task, in terms of the prior beliefs about the volatility of the environment
and the need to please the experimenter by responding within a certain timeframe.
If that is the problem, then we're now in and observing the observer,
or using Bayesian inference to make inferences about a Bayesian machine,
which is the autistic patient, which is hence the metabasian thing.
And you're absolutely right, that could be an ill-posed problem.
So, there may be the complete cluster, and it does not say that there is only one unique set of prior beliefs
that will make render the behaviour based on whether it exists.
It doesn't have to be a unique solution.
However, what will happen is that if you actually use Bayesian statistics
to infer the prior beliefs of the Bayesian or ASD subject,
you will then see, if you've got the appropriate model,
that there are a number of equally plausible solutions,
and you will also see that you haven't got enough data to disambiguate between them.
But you will also have an insight into which sorts of experiments you would need to do that disambiguation,
because you've got a generative model underneath of this.
You can do simulations to see the sorts of data that you are,
or the ways of looking at the responses that would enable you now to narrow down
these competing but equally plausible sets of prior beliefs that characterize that subject.
So, it's a very important issue,
but I think it's a generic one about how we actually apply Bayesian statistics
to understand the ways in which our data are being generated.
I think that, in that sense, it's less profound than the complete class theorem in itself,
which speaks to the sort of tautology of the game that we find ourselves in.
I think we can have time for one more short question.
I have two short ones.
Oh, that makes three.
The first one is about the semantics, and the second one is about the substance.
So, regarding the semantics, I'm wondering, it seems as though there are these two aspects to the problem.
One is inference, the other is control, and there is the epistemic cost in the context of inference,
and there is the pragmatic costs, ultimately reproduction, the well-being and survival of the organism.
And it seems as though you're declaring the primacy of surprise,
so you want to sort of absorb the pragmatic costs under the epistemic costs.
So, that's a bit of a semantic issue perhaps,
but doesn't it make more sense to do it the other way around and say,
the ultimate goal is reproduction, and epistemic benefits should be a sub-goal of that.
So, that's the semantic question.
And the question on substances, is this something that is already going on in engineering in AI, for example?
Can these principles be scaled up to real-world tasks such as visual object recognition?
If so, has it already been done, or is this an impending revolution for AI?
And if not, is it just a reinterpretation of what already exists?
Ydyn ni'n gweithio, because they were very short questions, you go ahead.
Very short. You couldn't get them shorter than that, could you?
Do you hope to try to answer them shortly?
The semantic's one.
No, I don't think so.
I take your point that one could articulate a whole theory and absorb uncertainty into cost functions.
But I think that misses the key point that I was trying to make at the beginning of the talk,
that the quantity you have to optimise is a function of probability distributions or beliefs.
And that's the key thing, which means you can never use a Bellman optimality scheme to properly solve that.
You do see heroic attempts, and those heroic attempts have been endemic for the past 40 years.
They're known as partial observed mark-up decision processes or belief state machines.
And all sorts of heroic attempts to try and recover or resolve the problem
are once you write down a discrete state space over continuous beliefs.
You can't do it, therefore you have to parameterise those continuous beliefs in those sorts of glorious and clever ways.
They don't work, they don't scale.
So people have been aware of the problem, and hence the vast literature on partial observed mark-up decision problems.
What I'm saying is that's the wrong approach.
The right approach is Hampton's principle of least action,
where the functional that you need to optimise is a function of the beliefs before you start.
And by proof of principle, I can demonstrate the veracity of that argument,
because I can solve problems which people working with partial observed MPs cannot solve.
So I think it's much more than semantics.
I think it's actually, people have got it wrong in the 20th century.
I think the Bellman's optimality equations are very beautiful construct, complete misdirection.
We should have been looking at Hamilton's principle of least action, and that's the 21st century.
Are people doing this? Yeah.
So Google DeepMind, for example, they've now, a small group within Google DeepMind,
have now started using variational free energy in their deep generative model,
not the temporal models, but certainly sort of 10-led neural networks in the context of the sort of pattern recognition approach.
So people have always rears, I think, that the variational approach was the right way to do this.
And of course you remember, most of deep learning started with Geoffrey Hinton's work,
and he came from the variational formulation.
So he was the first person to be down to propose the Helmholtz machine.
But they've taken a sort of security stream back to that early work in the 1990s.
Will it scale? I don't know, because I don't think they're quite on top of that,
because what they do is they haven't quite got, well, this is insulting,
but I hope there's anybody here from Google?
Ah, well I won't say that then.
Well anyway, they love amortisation, they love casting things in terms of learning.
So what they do is, instead of actually trying to optimise their beliefs, their expectations,
they try and optimise beautifully constructed deep nets, convolution nets.
That have parameters that would map from data to beliefs, or sufficient statistics or beliefs.
And then they learn that, because they're experts, more than well experts,
they are the experts in optimising the parameters.
That's a slight problem because it denies any context sensitivity
that we deal with the neuroscientists and I deal with them in my simulations.
I think once they get beyond amortising their deep networks, then they'll be in this game,
and then we'll find out whether it scales.
You'll need lots of computer scientists, big computers.
So I would imagine the next five to ten years, this style of approach,
and certainly the very actual free energy formulation, will become increasingly dominant in people doing artificial intelligence.
I should quip, for some people, the new AI is actually active inference.
It wasn't a coincidence that we chose that sort of rhetoric.
Yes, so maybe I could just have a comment rather than a question then.
Okay, okay.
So that's a bit straight-laced approach to free energy reduction.
It's based on surprise aversion, but I'm sure you have another talk on surprise-seeking,
curiosity, creativity, playful managerial styles,
which must have some adaptive purpose beyond our interest in horror movies and jokes.
It must help us get out of dead-end problem space to problem spaces we can't even imagine.
So the comment is, if you add surprise-seeking to free energy minimization,
I don't believe it remains tractable, so I'll leave it at that.
