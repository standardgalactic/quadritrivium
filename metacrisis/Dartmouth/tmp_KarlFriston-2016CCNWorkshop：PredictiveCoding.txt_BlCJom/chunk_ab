ac mae rhoi bod i'r diwedd i adge см знакомu a ichi gofyn yn y methu nhw,
c podría ddim lle gwyfer a'r succeeded beesparwyr yw bod i'n colli GYddeodag yn meddwl nhw apple hwrdd.
Fel y dyfodol barsodd tua, gweld o Gwydrych programme yr amdano ac yn gwybod yn fy datblygu atoliach agefwyr.
Sabwch fod yn y egg jwysig ei fyddent yn cael ei wneud bwynd i mi doing at감 a ch learning clear
wrth wedi addysg, ac disturbed bwyddiol ar gyazif mil o mas сразу pa ysbarthau
yn yn ysbyg mewn rowan nesaf yn gyfer lle o'r blas gwzir am wrth weld wrth gweith competisiwn hefyd.
Efallai arBT andaill gyring', eras yn wych chi hefyd cyfaint ar gweithio eich hyffordd bydd sea dimension yn tatymiem ar gyfer maedlo CAD hynny
That's just cartoon here in terms of the extrinsic connectivity.
Again, we can go back to this if people are interested, but there is actually a remarkable correspondence
between not only the deployment, but also the sign of these extrinsic connections
and those dictated by that particular message passing or gradient descent
that I showed on the previous slide.
If you subscribe to this scheme anatomically and the theory being a metaphor
y metafoyl harypoonsch cyffred wallsol, ip yn gynnalfa enwauótiaeth y byd yn ei beth yn d positivity.
Gwurlochoggynt struggai ag fydd yn dangyffred.
Rwy'r gwael hwyl i led لم jfotwch a'r rhaid storrolyniiddner yn ddechrau gweithasol.
Gweith痴 o eich teabod hyllennwyddon oherwydd mae hwyl yn mynd a'r ynaall hwas am l101 uall y llaw Llyfr~?
ti wynllun, the putamen for motor loops,
whereas the policy expectations per se
have here been assigned again to the國d oes pan agnasybno.
So, just a way of getting from the mathematical anatomy
to the biological and the neuro-anatomy
in a purely top-down, Res难wle at the mathymatical drywet.
Only by taking the equations and seeing what form do they imply for message passing and
what sorts of message passing do we see in the rail brain.
Andre Bastos, I think we may not, but he did a lot of work on this sort of intrinsic connectivity
within a macro column, clinical micro circuits, predictive coding.
Exactly the same game can be played here
for this discrete state space model.
One key exception which Lars might like,
Llywodraeth newid yn y Sgolion ydy'r cy Lyw ar flod a elevator arnyn nhall entag i newbornin
aparturau holl
yn isiau dilwr wedi cael y freunion ullref,
sicrhau ond mae fyllwededau am 000.
Iсти ond mae'n pethau morthadaff yng Nghyrchu Lwyddi Mer duplicatell,
rydych chi ddalodiau a'r bobl yn rofynol.
bound yr siŵr iawn sydd yn awrddod y modelau yn wath yn gweith Persianion Lweddyema.
Ond oherwydd rwy'r pa geneill bumai ac los freel yn gwasrydd fel erioed.
Yw fe allfydliedd gweithple o fragonnawn
os hwn o ye reachedron 50% yng Nghymru ddyn ni'n mynd i ei ddyluogstudiau mai eisiau cyfnod,
i'r bryd perthemi nwr f htt addyr First Order 4.
Mae'r ursau amddiant i d 까idoddech chiach yn gweithio intens тебя,
wedi fyrdd y cydotiaeth iawn, oherwydd mae'n murdd hyn tu yn llwyddaf hynny passengersa head ac'i
áty o'r minell yn fydda i y fel Model Onigra
critique o'r genbl am hyrals y gyf obviously
nad yw'n hlink dim o'r dylstage i llawy да,
a ond a llwyddi hun i'r pawl questo o ddud traumaticau
ac yma fetodd y sesameus mor hynni
a ysbytio chir fy r sharply
ac derivative cyfrwyng
i wahead o'r newid i'r mud
Ac y티 tot allwch allwch o'r ddjust Bellaol
talwch o'r model syddatter
Tynned o gwnaeth priwydwyr
yn tu'r llwydd
Mae'r word ar here has iconic letters that can either be an uppercase or a lowercase,
the palindromic in the sense that it doesn't matter whether the word, whether the cat
has to flee from the cat, doesn't matter whether we flip them in a horizontal way, it still
means the same thing, but it does, this agent is a surprise if we use a lowercase.
Ogymnasol ac mae hyn yn uneisio gyda ychydig i, am da't hwnna i chrywg mewn cyfgoiol,
i fod ychydig te Mercury Maen nhwhopi Gerry D europt maed i programs cyannoedd,
a'w affectedio glannu, oedd iawn'r rhagin maen nhw'n ddweud â gwayn.
But there are icons in this instance and then with this scheme we can simulate things like reading.
So here's a little four page story or sentence and that word is flea, that word is weight because there's nothing next to the bird,
that word is feed because there are seeds that the bird can feed on and that is weight.
So this is a sentence, flea, weight, feed, weight and that's a happy sentence and it will categorise it as happy.
But the problem that we're trying to address here is exactly what we started with, how do you scan, how do you search,
where do you go and forage for information to resolve as much uncertainty as you can about which of these six sentences is in play.
And when the system does this just by trying to minimise its expected free energy it shows this very interesting behaviour
where it jumps from one word or page to the next without really dwelling and wasting time resolving uncertainty that is already resolved.
So once it sees a cat it already knows that this has to be a flea word and it doesn't need to see where the other letters in this word are actually doing.
It already knows, there's no more epistemic value to be had, there's no more uncertainty to resolve.
It'll now jump to the next page and resolves in search after a couple of surcannock high movements, then jump to the next page and after this one surcard in the final page
it knows exactly what this sentence was doing and if I can I'll just show a movie of it doing that.
So the red dots correspond to where it's looking at the present time and the images that are mixtures of the icons represent conditional expectations.
And the main point to be taken from this is that it knows there's a bird there but it never looked there.
It has sufficient prior knowledge in its deep, temper temporal model.
It doesn't need to actually go and see stuff, it knows stuff is there because it knows what caused that stuff.
And with this sort of simulation one can then do exactly what Jim was talking about which was if it knows stuff and it has predictions
then it should be possible to disclose or reveal that knowledge, that predictability by introducing violations and elicit the sorts of classical responses that we see empirically.
And what we've done here is because we've got a deep model we can do local and global violations, we can make the final story, the final sentence a very surprising one without changing any of the stimuli
at the same time with or without making the priorities about the upper lower case, the sort of local feature expectations.
We can switch those around so we replay exactly the same stimuli and the same behaviors but just by changing the priorities of the agent we can cause certain things to be surprising
and those things can either be at the local, the first level or the higher, the second level.
And if we do that we get lots of behaviors that look again a little bit like delay period activity in the prefrontal cortex of a periscadic sort that you see prior to a saccade being selected and enacted.
While at the same time the band pass filtered voltages that are being driven by the implicit prediction errors look very much like periscadic ERPs and when you look at those periscadic ERPs under local versus global violations
what you actually see is something that looks almost identical if it's a local violation to a mismatch negativity whereas for the global violation you get the mismatches or the differences much later on in time
that look very much like a P300. I can see what I was going to show you. No I can't. Let's again. That was a very pretty slide but I can't.
It's very clever. It's a quote from, well you don't need to know that.
And then the final slide it's got a thank you. A lot of people were on this slide but you'll never know who now will you? So thank you very much.
So the workshop is structured so that there's a lot of time for discussion after each talk and so the floor is open now for people to ask questions or make observations.
I was just informed about how they do this in philosophy conferences that involves raising your hand or your finger but I haven't mastered that yet so I don't understand it.
So I think it's too complicated for this group. But not for philosophers. So who would like to start?
I'll start. I was just wondering if you could say more. You mentioned that you get something when there's choice involved with the rat experiment simulation.
Something that looks like a drift diffusion model and I've always been puzzled at how you get something that looks really like choice or agency out of a predictive coding model.
So maybe you can elaborate a little bit on that.
So the question is where does the choice come into predictive coding? I think that question.
Let me just be a little more specific. It's not that I don't see how you get choice behaviour in the sense that you can use predictive coding in order to evaluate some options.
But the notion of agency seems, if what you're doing is just predicting what you will do then it seems to kind of undermine the notion of agency.
I see. So the answer to that question is very simple. It's very simple. You put agency into these schemes through prior preferences define the sort of agent that I am.
So we were talking before about reducing surprise of all sorts, whether it's epistemic uncertainty, but the simplest sort of pragmatic surprise.
If I have a cost function that I don't want to be very hungry or I don't want to end up in an arm that has no rewards in it, then I'd simply have to have the prior belief that at the end of the day I will end up rewarded or sated or happy or complete.
So that anything else that happens is surprising and therefore I can then bring the whole machinery of predictive coding to bear upon the problem of suppressing prediction errors and surprises.
So pretty very simply in terms of predictive coding. If I a priori believe that I'm always going to be happy and complete and that I am built to always minimise my prediction errors in the future, then I will look as if I have agency.
I will look as if I have purpose because I will always choose my actions in a way that avoids the prediction errors that suggest that I am not happy and complete.
So the answer is just to absorb cost functions into inference by making costly states surprising through prior preferences and that comes out of things like planning as inference.
There are lots of ways of articulating that from the point of view of the rhetoric that I was using. The expected free energy has two bits to it. It has this epistemic bit and this pragmatic bit, but very simply it's uncertainty and surprise.
The epistemic bit is minimising uncertainty. The value, the purpose, the goal is a pragmatic bit defined through cost functions that are literally the surprise of a costly outcome.
So just to follow up a little bit, so is this sort of like a hyper prior that's going to be, I mean it sounds like we all have to have this ultimate belief that it's all going to end well at the end of the day.
So pessimists, none of us are really pessimists or something like that, right?
By definition. You may be perverse in your optimism, but you are quintessentially optimistic.
The deeper backstories behind the free energy principle, the only assumption that this instance of Hamilton's principle of release action makes is that you exist and if you exist, that means you behave as if you have beliefs that you exist.
By existing, that just means that you're not decaying or dying. All your states are within some bounds, be they physiological, homeostatic or pecuniary in terms of being rich or in terms of interceptive inference and the hadonics on that happy.
But it's all about keeping things in bounds, it's all about minimising entropy, minimising uncertainty. So it always looks as if agents that exist have prior beliefs that they exist.
And when you unpack that, that simply means I have preferred states that I will expect myself to occupy. Literally they are attracting states, they are an attractor.
So that rhetoric, which actually is a rhetoric from dynamical systems theory, applies identically to this sort of purposeful reinforcement learning or sort of goal directed style of thinking about things.
There are attracting states, they are simply the ones that you frequent, which means that you will appear to behave as if you have prior preferences for being in those states and you will always choose actions to get to those states.
It is those prior preferences that define the sort of agent you are. So in answer to your very first question, the agentfulness comes in by implication or just through the sorts of priors that characterise that particular sort of agent.
So if I was a virus, I would have very different preferences than if I was a person. But there are still both plausible and viable preferences and sorts of agents.
Hi, thanks for your talk Carl. I was wondering now that the slides are back if we could go to the delay period activity that you had briefly mentioned.
I just wanted to see that a little bit unpacked and related to what you were just talking about, that is agents reducing their free energy. How does that principle then generate the delay period activity that we see in places like prefrontal cortex and in working memory and so forth.
Thanks.
Right, well, those sorts of phenomena which we all know and love and will try to explain and measure empirically. So let me just try and find it. Do you remember where it was? Right, thank you.
There you are. All those sorts of nuts and bolts getting down and dirty in terms of what this scheme would do when you put dynamics on it through the gradient descent, depend upon the generative model.
So at your very similar to the last one key component of the generative model are the prior beliefs, the preferences, what gives it purpose, what are its goals. Your question I think has a very similar answer.
Once you've written down the generative model, everything else is not up for discussion. The maths tells you exactly what has to happen once you've written down the generative model.
And the delay period activity you're talking about simply follows from the fact you've got a deep generative model or a deep temporal model.
So as soon as you write that deep structure into the model, it means that certain beliefs have to outlive or change on a slower temporal scale than other beliefs lower in the hierarchy, which means you have to have delay period activity whilst other stuff unfolds at the lower levels of the hierarchy.
So in this particular example, what I've done here is show the beliefs about the six sentences over the four moves, giving us six times four moves or five moves back.
So it should be about 30 beliefs here. On the same time access as beliefs about the particular word that's currently being seen.
And these resets here indicate the onset of saccades and the acquisition of new information. And you can see roughly every 250 milliseconds there's a saccade and new beliefs are updated about the current word.
But each, at the high level, we're only considering beliefs about each letter, my point, is we're only considering beliefs about the word.
So beliefs about the word corresponding to what's on this page or what's in this word are invariant during the successive saccades as you sample the different letters.
So these things change more quickly than these things. When these are completed, then there's a change here, and then the cycle begins again.
So these tick over faster than this, and then this looks a little bit now like the rastles that you see prior to the emission of the saccade here.
They're not from the same paper, but a related paradigm. If I take the voltage causing this delay period activity and band pass filter it, you get these sorts of fluctuations out here.
So when there's an increase in delay period activity, there's usually a positive deflection that looks a little bit like an ERP.
And just to follow up, so is the presence of delay period activity, is that associated then with prediction error or with the build up of a prediction?
No, I think the prediction error in this scheme, and this is the maths that comes from the discrete aspect of the genetic models, lies in the rate of change of neural firing.
If you associate the biophysical encoding of expected states of the world in terms of population firing rates, then if you subscribe to that, if you accept that, then the prediction error now becomes the conductances that drive the depurisation that drive the firing.
So these basically reflect the fact that as time goes on, you can more and more confident that one particular sentence is in play.
And you can see that this is beliefs about the, which sentences in play at the first, second, third, fourth and fifth eye movement or page, sorry not eye movement, page or word.
And they are now internally consistent and at every point in time, in the past, at the end, I now believe I was reading the first sentence.
And that belief endures during the sampling of all the actual letters within each of the words.
So these would now represent just basically numbers between 0 and 1, 0 and 100% neural firing that score your expectation that this is the current state of the world.
At the beginning, there's lots of ambiguity. Not an enormous amount, but there is ambiguity. It's 50-50 because of the six sentences.
Only two of them begin with the word flee, which means that we resolve our uncertainty about four of them, but we're still ambiguous about having ambiguity about sentences one and four.
And that can only be resolved at the end because these sentences only differ in the words right at the end.
So during this time, there's today a period of activity which we've got these two explanations, hypotheses in play, that are resolved epistemically, optimally, right at the end when we get to the last word here.
And it's a weight and that determines which of the letters it was.
Thank you for the talk. I wanted to go back to this idea of this contrast between reinforcement learning and the kind of formulation that you're making here.
So one of the things that I thought was interesting is this formulation in terms of external value plus you basically decompose your KL diversions to external value and epistemic value.
So how do you get exploration in this model?
So it seems to me that you're doing an armax over actions to get some balance between immediate value and information gain.
Is that the basic idea?
Yes. I mean, we can look at the equation or we can look at this. That's absolutely right.
Well, the exploration is good that you brought that in because another perspective on this is the whole foraging ethological perspective on exploration versus exploitation.
That rhetoric just maps very simply to the epistemic and the pragmatic.
So there is no, again, there is no up for discussion or there's no ad hoc waiting between the two.
The expected free energy can always be written down in terms of exploration plus exploitation in terms of the epistemic value and the pragmatic value.
And what happens is in minimizing that one quantity, you get this scanning searching behaviour until the epistemic bit has been reduced, allowing then you to focus on the pragmatic bit.
So for free, you get a base optimal exploration to the extent that it is sufficient to resolve uncertainty given the precision of your beliefs about your prior preferences that then allow you to pursue your girls.
So this solves the exploration, exploitation dilemma in a base optimal sense.
It also suggests that the very carving of behaviour into these two complementary drives is actually probably a misdirection.
So it's only you and me that have actually teased apart the two components of the expected free energy and called one an epistemic exploratory one or a novelty seeking one and the other bit a pragmatic cost function like rewarding preference goal directed like one.
There are lots of different ways of rearranging those.
There are also a range in terms of risk and ambiguity, intrinsic and extrinsic value.
There are lots of ways of carving them and getting different perspectives.
When you see that and when you work with that, you start to realize that it's not necessarily the best thing just to have one particular religious perspective on it because it lends you to the false belief if you subscribe to this formism.
There has to be some other adjudicator.
There has to be some other homunculus that's decided, oh, I need to explore now.
And now I've done my exploration and now I'm going to go and do a bit of pragmatic, you know, stop the scanning and then I'm going to go and, you know, exploit what I've discovered.
It doesn't work like that. You should get that for free.
If they're both part of the same cost function, then once you've sufficiently reduced your uncertainty, you go into, as illustrated here, your exploitative behavior.
So this is exploitative behavior or novelty seeking in the sense that you don't know what the cue is going to tell you.
It doesn't have any immediate rewarding aspect to it.
There are no preferences associated with the condition stimulus, but it's interesting.
Uncertainty resolving.
But after a time, it becomes boring because you already know what it's going to tell you.
And once it does that, then you get exploitative behavior.
So I should have done that. I should have put exploration, exploitation.
Have an argument with me because there's meant to be a discussion.
Do you not like that? I'm telling you.
It's very interesting perspective on it.
I'm surprised that it comes out that the simple deterministic policy works well and just kind of works out of the box.
My inkling is that when you go and implement this stuff, it can be difficult for it to balance the exploration and exploitation.
So there's no tuning parameters. There's no off policy sort of estimation.
You just throw it in there.
It all works out.
I know exactly what you're saying because this was a big selling point when we first realized that a couple of years ago.
And it will remain a bit like one of these five to ten year changing the direction of the ocean liner or the oil tanker.
But two years later, all that we're saying is, in fact, people behave according to Hampton's principle of least action.
That's all that we're saying.
And that implicitly, or it looks like, that behavior has this dual aspect.
But it doesn't, if you formulate it as a variational principle of the sort you did at school when doing Newtonian mechanics and then Einstein did with general relativity.
It sort of falls out of the mix in a way that does actually dismiss these separatist perspectives
on, I can either do this or that, and I've got to now optimize the exploration in relation to the exploitation.
And the key trick that puts you into this simple world of Hampton's principle of least action is the realization you can't prescribe good behavior
unless the prescription is an optimization of a function of beliefs.
So a really simple example would be an economics game.
