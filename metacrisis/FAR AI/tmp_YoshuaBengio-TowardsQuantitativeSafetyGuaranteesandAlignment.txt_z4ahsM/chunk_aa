So, I'll have a little bit about governance and then hopefully two-third of my talk is
going to be about alignment and a direction to address alignment, which focuses on how
could we possibly get quantitative guarantees of safety based on Bayesian posteriors.
So, what do we need to avoid AI catastrophes? At least these two things. There's a more
technical problem of alignment and control. I mean, there are some political aspects like
we need to have the sufficient investments in solving those problems, in particular,
I think, from governments and not just the private sector. And second, we have, of course,
the coordination challenge, because even if we solve alignment, you would still have
humans exploiting this for their purposes and destroying at least democracy, if not the world.
And so, yeah, how do we make sure people will not abuse power and follow whatever protocols,
safety, you know, that we found as scientists. Quickly, a few words about alignment. Just
want to draw your attention to the fact that in addition to the loss of control issue that
people in safety usually think about, of course, if we make progress in alignment, we can also
reduce the issue of misuse when we design an AI system that is aligned in the sense that it
can't be jailbreak, it can't be fine-tuned or something to be misused for all sorts of bad things.
But so, I think most people here are aware that the other thing that maybe people don't
realize is that making progress in alignment also help with the ethical issues, you know,
offensive AIs, of course, but AIs that discriminate that violate human rights and privacy and
give unfair decisions. So, these are all linked with trying to deal with the alignment problem.
On the coordination side, I want to mention a few things. There's a lot of discussion
these days, which I call the open source debate, if not the open source battle.
You know, I'm a big proponent of open source, but at the same time, it's not an absolute value.
We need to make sure that it, you know, there's net benefit. And the problem is once a system is
released in an open way, you can't patch it, you can't get it back. It's irreversible proliferation.
Later, after it's been released, someone can find a way to exploit it to do something really
dangerous. The state, the regulator, doesn't have any handle anymore. So, there is the proposal that
if everybody has nuclear weapons, we'll all be safer. I don't think so. And the other aspect is
who should decide, right? Should it be the CEO of a company or should be a democratic process?
Something that gets a lot less attention in this community and elsewhere is the scenario where
it's not an alignment problem because, you know, we unintentionally created a monster.
It's because somebody wants to create a monster. That's what I call the Frankenstein scenario.
People want to see an AI at their image, or they think that superhuman AIs is the future
of intelligence and the value intelligence more than humanity. That's their choice, but I have
children and grandchildren and I see it differently. So, and then, you know, there are other things
like societal collapse that don't get enough attention, but I won't talk about and not as directly
connected to alignment, but yeah. So, against being in the realm of governance, as I said,
there's a problem of power, right? Who's going to control these big eyes of the future?
And they can be used in ways that contradict the very notion of democracy. Remember, democracy is
about sharing power. And if there are machines that are much smarter than humans in many ways,
they give the people who control them a lot of power over other humans, whether it's in the,
you know, political arena, military, economic, and so on. So, that's not good. And I think democratic
governments will want, will understand that one day and want to make sure that they control these
things, but we have to make sure it also doesn't become a military black box and we need to work
on the international governance to try to reduce the AI arms race. So, last summer, I testified
to the US Congress and there's a long document that has three main recommendations, one regarding
regulation, one regarding investments in AI safety, in particular solving the alignment problem,
but also how to evaluate the various risks. And maybe the thing that gets less discussed is
the third point, which is regulation is not going to be perfect. There's going to be some
bad actors that treaties and regulation don't, you know, really stop. And these people or organizations
will create dangerous AI's at some point in the future. So, we need to deal with that. We need to
prepare for that. We need to have kind of good, well-governed defensive AI's that will protect
against these, but we need to do it in a way that we don't create the very problem we're trying to
avoid, which means we need to make sufficient progress on the alignment challenge before we
build AI's to defend us against rogue AI's. And the AI's we build could turn against us if we don't
do the right things. So, yeah, why do we need AI's to defend against AI? Well, it should be pretty
obvious. I don't think I need to convince you guys. But here's the thing, maybe again, that
I don't think this gets as much attention, which is what I call the single point of failure
problem. And that is both for the issue of humans taking over AI for power reasons
or loss of control. So, you know, one extreme, if we have just one country and one organization
building the most powerful AI in the world, that is very dangerous. Because you have a single point
of failure of either humans taking over the control of that for malicious purposes, maybe
the operator or the CEO or the president of that country, or making a mistake. And we end up with a
dangerous rogue AI. So, you want to have a decentralization of power. But you don't want
the extreme decentralization, I think, because then you get into anarchy and the issue with open
source that I was discussing earlier. You want to also, and one reason for this is, yes, you want
to decentralize power, but these different sources of power, say different AI systems that are very
powerful, need to be governed properly. So, you need to set up the democratic governance. If
everybody has it, then it's not clear you can govern these things.
So, I've written a paper that is in the Journal of Democracy that's been out for three months about
that discusses these questions. In particular, the idea that we need a decentralized multilateral,
so across multiple countries, decentralized network of frontier AI labs that work on these
problems. So, the problem of alignment, the problem of governance, and our governed properly,
and the problem of countermeasures or defensive measures we could take with AI.
Now, I think that this should be something that democracies do together, but the oversight of
that could be something broader with the UN and things like that in the context of obtaining
a reduction of the arms race pressure and maybe other countries that are now part of the club
would agree to have similar kind of oversight. So, one condition to avoid the single point of
failure is that these various labs working on these frontier systems need to share what they're
doing with each other. So, they can't share it with the rest of the world, at least not everything.
But they should share with each other so that if one of them fails in one of the ways I've
talked about, then the others have at least comparable firing power, and they know what
sort of techniques the other guys are likely to be using. So, you don't want to end up in a situation
where the strongest AI, again, is corrupted in one way or another, and there's nothing
of comparable intelligence that is in the right hands. And by the way, what is the right hands?
Again, democracy. It's not the best system, but it's the best we have.
Yes. So, that suggests that the organizations that will do this in the long run should be heavily
governed by a democratic process, which means governments, and ideally not
in conflict of interest with commercial interest, which doesn't mean that companies can't be part
of it, but the governance needs to be really driven by defending humanity, in my opinion.
Also, a practical reason why it would be better this way is companies are competing with each
other, and so they might not want to share everything with each other. Whereas if we have
such a network of labs working together but independently under the same democratic process,
you know, it will be just natural to share just like between academics. We share and we compete
and share, right? That's the thing we know and works really well. Now, if they're not funded
by VCs, where's the money coming from? Well, I think it should come mostly from governments,
but again, we have to avoid the single point of failure. We don't want to, you know,
the next government of some very, very big country to become a dictatorship and have access to that
power. So, that's why we need to set up an international structure around these that makes
it hard for a single government to take over, or if they do, the other guys are around to defend
democracy. All right, so that covers the governance part of my presentation. I can make a pause here
if people have questions before I go on to technical things. Thank you. Okay, so the question was about
the proposed kind of approach that the frontier AI will be only developed by a certain set of labs,
and the question is like who's going to decide which labs... Democracy. What do you mean democracy?
Like, okay, we have academic community, who's going to tell which lab that they cannot develop
frontier AI? Democracy is a process. So, we take collective decisions. That's a whole point of
democracy, and we can do it in various ways. I mean, you're just going to tell the lab not to
develop AI? I didn't say that. There's going to be governance of the labs that can do that,
and I think the ultimate level of control is to make sure that the commercial interests don't
interfere. Yes, there's a question here, and another one, and then we'll stop, and I'll move on to the
technical stuff. So, I guess multi-nationally, how do you convince countries that don't value
democracy to participate in the democracy? Okay, so the way I presented it, you have two levels
of cooperation. You know, you should have cooperation between democracies, and they can
share a lot of information, and then you should have, you know, broader cooperation that includes,
say, China and Russia and so on, where we agree to something similar to what we would do with the
nuclear weapon. So, we agree with mutual oversight, which doesn't mean we share all our secrets,
but we make sure that there's enough oversight, for example, by independent actors like the UN,
that we trust that the other person is not developing these things as an offensive weapon.
But, of course, it's not going to be perfect, so we need to prepare, I mean, every country that has
the means is also going to prepare defensive countermeasures, which I think we have no choice to do.
I said there was another person here, and she didn't get the chance to speak,
and that was the last, after that, I need to go to technical things.
Yeah, thanks. Follow up on the previous question. Is it possible to start from
the position or from a term that is not democracy? So, we go much more inclusive right from the
beginning, because, as they mentioned, like, maybe a lot of the governments in the world,
or a lot, a huge population of the world, is not living under the democratic society. So,
we go all inclusive from the beginning by not starting from democracy.
If you have a concrete proposal, email me. All right.
So, I'm going to say a few motivating words about an issue which is, at least with reinforcement
learning, there are a number of arguments that have been brought forward, suggesting that as we
make the AI more capable, we may get more dangerous and more likely that we lose control. So, that's
the problem of reward hacking. But there's another way to think about it, which you can think in
terms of, like, adversarial scenarios. The more the adversary is allowed, you know, has the compute
power to find the attack that's going to work, the needle in the haystack, the more likely it is
to find it. So, if your AI has a lot of power, it might discover the weakness in our, you know,
the mismatch between our intentions and what we ask the AI to do, and we lose. So, when we
did think about this, which you can, again, like, the adversarial settings help to understand,
you might have a system like the, say, the green line, this is a, let's say, a reward function.
Sorry for my, like, poor hand drawing. And the red line, which is the estimated reward function,
where if you take a statistical sample, they will look like they're close enough. But if you
allow to optimize in order to find the places where they differ, in particular where the red
ones is large, you could find something which is essentially too good to be true. And that is why
things like CERL that are saying we should not try to find a reward function, but a distribution
over reward functions that are plausible given the information that was given, I think are just
the right way to think about it. So, that's the direction I'm going to go to. So, yeah, and to
be more specific about that's the scenario where this can happen, I think reward hacking is a good
case. I encourage you to read this paper by Michael Cohen and co-authors. So, you know, if the AI
understands the reward as, you know, when humans press plus one on the keyboard, the AIs might just
take control of the keyboard and get lots of plus ones. And once they get all that positive reward
from themselves that they control, they might not want us to find out about their scheme or to,
you know, stop their scheme because then the reward will be smaller. And so, immediately you get a
conflict. Now, it's a kind of cartoon scenario, but I'll give you a notion of how things can go
wrong. And you can see that the more powerful the AI, the more dangerous that situation gets, right?
So, what I'm going to try to get at is a scenario where we train an AI and the more compute we throw
in it, the more capable it is, the safer it's going to get. Hanka? I'm sad Michael's not here
to clarify this, but the statement even in the serial case doesn't make sense to me because
the serial case would look at the person hitting plus one as evidence about what they currently
want and the notion of taking control of your only source of evidence about this Bayesian thing you're
trying to estimate is strictly suboptimal. So, it's a serial case with very much not incentivized that.
But I might be... Yeah, so Michael has an argument that even if you're Bayesian about it, you could
still get to choose the wrong hypothesis. Just saying, you know, under the surface, things could be
even more complicated than what the sort of naive Bayesian solution would suggest. So, it's worth
really like thinking about these things. Okay, but let's explore this kind of Bayesian approaches.
I'm going to start with a simple but incomplete solution to the problem, which is let's... Oh,
why don't we just build non-agentic AI systems that are like oracles, but actually not... No,
they're not oracles. I call them AI pure scientists. So, the difference between an oracle and AI
pure scientists is that the oracle answers any question, whereas the AI pure scientist, it only
answers a different kind of question, which is come up with good explanations for the data,
come up with good theories. So, like a pure scientist, like a pure physicist doesn't do
experiments, right? They don't even give you answers to typical questions in day to day. In fact,
they're pretty bad at it. But they might come up with new theories that explain data in the world,
previous experimental results, better than has been done before. So, that sort of AI
would be very useful, at least for our scientists. Like, you know, maybe they can help our, you know,
biologists and people who are trying to solve climate and all of these things.
But, if we go back to what I said earlier, that we may have to defend against bad AI,
that is probably going to require an AI that is an agent and can maybe, you know, fight the bad guy's
life. So, it's an interesting thing maybe to do, but it's not going to be sufficient.
Let's go back to why we should be Bayesian or something of that nature.
If you consider maximum likelihood training or standard end-to-end reinforcement learning,
whether you learn the reward function or not, there's an issue that for any given
source of data, there's going to be multiple solutions, multiple neural nets, multiple theories
that explain that data. In fact, you have theoretical results from causality theory
that even with infinite amount of data, you would still have that ambiguity. And it has to do with
the problem of interventions. We can't intervene on everything. We can't intervene on the sun.
So, we have to rely on generalization. We have to rely on data that is not sufficiently complete to
guarantee that there's not another theory that explains the data as well. Okay. And this is
important because if you have the wrong theory, if there are multiple theories that I can explain
the data and how your model has picked up the wrong one, and that wrong theory could be confidently
wrong, which we see in current LLMs. And being confidently wrong could be funny, but it could
be extremely dangerous when the AI is confidently wrong about an action that could have very bad
consequences even though the AI might think it's actually quite okay. So, if we want to get
quantitative safety guarantees, that's not going to cut it. And I have a little like
toy scenario to explain really what being Bayesian means here. So, imagine you've got an AI
in robot here. It's in front of two doors. So, it has choices. Go left, go right. And based on the
data, there are two theories that are compatible with everything it has seen before. According to
the left bubble, the left theory, if it goes on the left, people die. If it goes on the right,
you know, it gets some cake. If it goes on, if you consider the other theory, on the other hand,
going on the left gives you some cake, and going on the right is neutral. So, if we're not lucky,
with 25% probability of the two theories are equally likely, we're going to die. And if we
or ever keep track of all those theories, it should be obvious what to do, right? You need to go
left or right. Yes.
