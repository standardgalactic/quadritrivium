So we have applied that in the context of Bayesian procedures in the case of causal graphs.
First paper came out last year at UAI. And there are a few more that came out. So in the first
paper, we just generated the causal graph, which is a discrete structure. And now we're starting
to work on also sampling parameters of causal conditionals, for example. And you can also
work on generating latent variables. So again, the scheme is, so let me explain this figure.
In order to generate a theory here, what we're going to do is, let's say our theory is a causal
model. So a causal model would first have to decide on what is the causal graph, like what
variables are direct causal parents of which one. So it's a graph and we can just generate that graph
one edge at a time. So we got a partial solution at each step and then we refine it, we add some
pieces, at some point we decide, okay, we're done with the graph, there's a special action that says,
okay, we're done. And then we can generate other things like the parameters of the conditionals
or potentially latent variables as well. And yeah, so that's the sort of thing we're working on.
For now, these are fairly small-scale things compared to building a full-scale world model.
But let me continue on that. If you want to learn more about G-flown S, there's iRigina,
iRigina tutorial. There's about 20 papers in the list there for my group and other groups are
starting to put out papers on these sort of things as well. Okay. Quickly, there are other things
that I think need to be done. So for example, we would like the theories that the AI generates to
be in a language that's close to natural language. In other words, that they are interpretable. So
they would be like pieces of program or logical statements on which you can reason with probabilistic
inference. But you also want to make sure that you can go back and forth through something humans
can understand for a number of reasons. One, just to check that what we're doing makes sense.
Also, because those theories can be useful by themselves for human scientists, as I mentioned
earlier. So that would limit the kind of theories that can be easily generated to the
kinds of theories that humans can also generate. But there's more to say here. But there's another
thing that's kind of complicated. The Bayesian posterior I talked about is sort of an average
overall, the theories that are compatible with the data. But what if the correct theory is sort
of alone in its gang to say this is really dangerous, then you might get a very small
probability of harm, especially if we're in a continuous space. So there are like adversarial
ways to try to deal with this, to search for a theory among those that have high probability
that would say, you know, this is a catastrophic move.
The other things, so there's still a lot of math to do around this to demonstrate safety bounds.
In particular, the neural nets I've been talking about are not going to get a perfect
you know, Bayesian posterior. They're going to be approximations. And we're going to be able to
measure the approximation error. But we need to turn that into some sort of confidence intervals
for the probabilities of harm that it will generate. And you know, and other questions
that are interesting that need to be solved. So I'll stop here. Get more questions.
Thank you for the really great talk. So one question I have is, so in the slide that you
paused upon, so the, I think it was the objective for the posterior approximation. Yes. So if I
understand correctly, you're training this Pi hat so that it approximates the posterior. Yes. But
then you're comparing it to R and R of X and Y. That's how you train it. Yes. Which is proportional
to the ground truth posterior. Yes, exactly. And how do you get that? Okay, so that you get
fairly easily. So R of X, Y is the prior times of likelihood. So that's computable. There's no
exponential calculation. It's just big because like it's the size of your data set. But you can
use mini batches, which are like stochastic approximations of the whole sum. So the log of
the likelihood is the sum of log likelihoods for each example. And you can just like in gradient
descent, you can get a stochastic gradient from this. And you only need to look at one
mini batch at a time. Thanks for a really great talk. Yeah, first off, I want to say I get really
sympathetic to the vision of wanting to scale up Bayesian inference for the purposes of AI safety.
It's kind of what it's been my PhD doing. And sort of, you know, there's a whole field working on
this called probabilistic programming. And I think more people should work on it. Some folks at MIT
like myself are working on that. I guess two questions which are related, right, you know,
to the sort of specific picture of how to do that, which is amortized Bayesian inference.
Some of us at MIT don't think that's going to work. But I think one thought is that I'm curious
how you think about the sort of flexibility of amortized Bayesian inference to different
inference problems. Because typically in amortized Bayesian inference, you train your neural network
or whatever else your regressor to approximate sum plus your pi of theta given x. But in a real
world, you may suddenly encounter a new inference problem, pi of theta given some other set of
observations. Why? That you haven't trained a posterior approximation for.
That's called generalization in machine learning. Yes, we need to do that, of course.
Right. And I'm curious like whether you think you will, you know, that if you encounter, you know,
a different set of constraints all of a sudden on the real world.
No, it's not a different set of constraints. It's just applying the same neural net to new cases.
And that's what we do every day with these models. That's what makes them so powerful that they
generalize. Is there the best generalization machines that we know right now in machine
learning? I guess I'm thinking not of the case of when the observations are the same data type.
So let's say, yes, I'm willing to believe that you can infer, like you train it in a network to
infer, say, seeing graphs from images, 2D images, right? But now you want to constrain your neural
network to generate seeing graphs not from 2D images, but from sounds or something like that.
Yes. So they need to know about sounds in the data, of course.
Maybe we can talk more about this later, but that's just a sort of...
But look at what current large frontier models do. They can handle multimodalities
and they can generalize. Now, what I'm proposing might generalize even better because it's
what they're trying to capture is the causal structure. And that's reason number one,
reason number two. So they would be more robust out of distribution changes.
That's the whole reason for being causal. And the second thing is where they might be out of
their league, they would say, I don't know. That's the whole point of being Bayesian. And by the way,
that's the real core of this talk, which is that we'll not get perfect oracles. And so we need
those oracles to be able to say reliably, I don't know. Yeah, I'm curious what you think is the
main reason or reasons that the scientist proposal would be safe from reward hacking in a way that
CIRL wouldn't. No, I think Searle is right on the right direction. But I am not comfortable with
directly modding a reward function. I think if you want to get good generalization, instead of
modeling a reward function, you want to understand human psychology, society, norms, like the underlying
causal factors to why we say those things and including theories of how we feel, which is
much more subtle than learning a reward function, like which reward function. There are many people
and it's much more complicated than that. And you want to be able to generalize across people,
across societies. So that's one aspect. But yeah, I got the inspiration for all this by reading
the Stuart Russell's book and the CIRL paper, for sure. And it's mostly about how do we do it
efficiently, right? That is the question. Yeah, thanks for the great talk, Yasha. I'm all for
amortized space inference. We've also had great successes there with prior-fitted networks.
I didn't get the one point where you were worried about the exponential number of modes that the
prior couldn't sample because couldn't you just generalize? Like if you sample millions of points
in this exponential space, then wouldn't you just generalize them? Well, that's precisely what I'm
proposing. So if you go to these equations, so think about the sum in the second line. You could
do it Monte Carlo, but then if you miss some modes, like let's say you only visit 1% of the modes,
then you could be completely wrong, 99% wrong. But if you somehow find a way to implicitly
generalize about the mode structure, because you're trying on your own net that just directly
predicts Y given X while being Bayesian, then you're okay. Even though you don't actually visit
all of the modes, you can capture, oh, it's a grid and that's the dimension of the grid. So here's
the result. Okay, perfect. Then we're on the same page. That's also what prior-fitted networks do.
Great, thanks. Thanks for the presentation. I was wondering if you have an idea in general,
or maybe this is always context dependent. So for some specific cases, how do you get a good
prior for the whole process? And how do you make sure that your prior doesn't assign zero probability
to like, let's say unsafe modes? How does that affect your safety guarantees that you could get?
You don't want to have zero quality on safe modes, but yeah, the correct answers. So you basically
have to be non-parametric. And maybe as a simple example to understand what non-parametric could
mean here, think about the space of programs. You don't limit the size of the programs.
And that's it. Now, you still need to assign a different weight to different programs.
When we do normal training of neural nets, we have priors because we don't actually just do
maximum likelihood. We have architectures, we have regularizers, we have an optimizer that prefers
simpler things in some sense. So we already have some kinds of priors in our machines.
And you know, this is a choice. We can use the usual things in machine learning like
test log likelihood in order to compare different priors. But that's like standard machine learning.
But how can you get guarantees out of that without, because our priors are implicit,
how can you be sure that they're including? Well, in my case, I'll need explicit priors
because I have to compute them. Yeah. I'm just saying priors are not so weird. They're things we
normally do anyways. Thank you for the great talk. I'm curious how you imagine this being used.
Like, I suppose it's like we train one massive G-flownet that has the Solomonov prior or whatever
and then talks things about the world, right? But then does it then like generate a bunch of
theories that then get used? But this is kind of like sampling a bunch of things and kind of in
contrast with the problem that you mentioned about the exponential number of modes, right? So like,
you can't really marginalize theories that you explain over to humans. So I guess I'm just curious
about how should use this. Okay, so one strategy that I'm working on is we don't want to actually
generate a big theory of everything. It's just too big. And it would take too much time to get
gradients if we have to wait that we generate it all that. How do human scientists do it? We
write papers. A paper is like a small little piece of theory that only describes some aspects of the
dependencies in the world, some hypotheses and some claims. And these pieces of theory are
sufficiently kind of self-contained that they can be evaluated against experiments, right? So
some data, some theory mismatch gradient. So this is how I envision this to be feasible. So the
G-flanet or whatever would only generate like little pieces of theories, just like we do. Like
we see things, oh, I mean, I walk in the street and I have all kinds of theories popping into my mind.
Oh, well, once you have trained the marginalizer, you can answer questions.
So the question is how do you use it? And you use the marginalizer. You could also use a sampler
to talk to scientists and say, oh, here's a new theory about X. I think it's zero here.
Yes, yes. I suppose we're done. I somewhat really took the last question. Thank you very much for
the amazing talk. Please thank Yosha.
