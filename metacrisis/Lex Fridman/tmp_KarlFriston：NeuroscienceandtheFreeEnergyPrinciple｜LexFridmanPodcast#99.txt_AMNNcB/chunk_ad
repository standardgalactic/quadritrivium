So what that means, the central importance of movement, I think has yet to really hit machine
learning. It certainly has now diffused itself throughout robotics, and perhaps you could say
certain problems in active vision, where you actually have to move the camera to sample this
and that. But machine learning of the data mining deep learning sort simply hasn't contended with
this issue. What it's done, instead of dealing with the movement problem and the active sampling of
data, it's just said, we don't need to worry about, we can see all the data because we've got big
data. So we need to ignore movement. So that for me is an important omission in current machine
learning. The current machine learning is much more like the oil drop. Yes. But an oil drop that
enjoys exposure to nearly all the data that we need to be exposed to, as opposed to the
tadpoles swimming out to find the right data. For example, it likes food. That's a good hypothesis.
Let's test it out. Let's go and move and ingest food, for example, and see what that is that evidence
that I'm the kind of thing that likes this kind of food. So the next natural question, and forgive
this question, but if we think of sort of even artificial intelligence systems, which has just
painted a beautiful picture of existence and life, so do you ascribe, do you find within this
framework a possibility of defining consciousness or exploring the idea of consciousness? Like
what self-awareness and expanded to consciousness, how can we start to think about
consciousness within this framework? Is it possible? I think it's possible to think about it,
whether you'll get anywhere. Again, I'm not sure that I'm licensed to answer that question.
I think you'd have to speak to a qualified philosopher to get a definitive answer there.
But certainly there's a lot of interest in using not just these ideas, but related ideas from
information theory to try and tie down the maths and the calculus and the geometry of consciousness
either in terms of sort of a minimal consciousness, even less than a minimal
selfhood. What I'm talking about is the ability effectively to plan, to have agency.
So you could argue that a virus does have a form of agency in virtue of the way that it
selectively finds hosts and cells to live in and moves around. But you wouldn't endow it with
the capacity to think about planning and moving in a purposeful way where it countenances the
future. Whereas you might anant. You might think anant's not quite as unconscious as a virus.
It certainly seems to have a purpose. It talks to its friends on route during its foraging.
It has a different kind of autonomy, which is biotic, but beyond a virus.
So there's some line that has to do with the complexity of planning that may contain an answer.
I mean, it would be beautiful if we can find a line beyond which we can say a being is conscious.
These are wonderful lines that we've drawn with existence, life, and consciousness.
It will be very nice. One little wrinkle there, and this is something I've only learned in the
past few months, is the philosophical notion of vagueness. So you're saying it would be wonderful
to draw a line. I had always assumed that that line at some point would be drawn
until about four months ago, and the philosopher talked me about vagueness. So I don't know if
you've come across this, but it's a technical concept and I think most revealingly illustrated
with at what point does a pile of sand become a pile? Is it one grain, two grains, three grains,
or four grains? So at what point would you draw the line between being a pile of sand and a collection
of grains of sand? In the same way, is it right to ask, where would I draw the line between
conscious and unconscious? And it might be a vague concept. Having said that, I agree with you entirely.
Systems that have the ability to plan. So just technically what that means is your
inferential self-evidencing, by which I simply mean the dynamics, literally the thermodynamics
and gradient flows that underwrite the preservation of your oil droplet-like form,
are described as an optimization of log Bayesian model evidence, your elbow.
So that self-evidencing must be evidence for a model of what's causing the sensory impressions on
the sensory part of your surface or your Markov blanket. If that model is capable of planning,
it must include a model of the future consequences of your active states or your action, just planning.
So we're now in the game of planning as inference. Now notice what we've made though. We've made
quite a big move away from big data and machine learning, because again, it's the consequences
of moving. It's the consequences of selecting those data or those data or looking over there.
And that tells you immediately that even to be a contender for a conscious artifact or a
is it strong AI or generalized. Then you've got to have movement in the game. And furthermore,
you've got to have a generative model of the sort you might find in say a variation auto encoder
that is thinking about the future conditioned upon different courses of action. Now that brings
a number of things to the table, which now you start to think, well, those who've got all the
right ingredients talk about consciousness. I've now got to select among a number of different
courses of action into the future as part of planning. I've now got free will. The act of
selecting this course of action or that policy or that policy or that action suddenly makes me
into an inference machine, a self evidencing artifact that now looks as if it's selecting
amongst different alternative ways forward as I actively swim here or swim there or look over here
and look over there. So I think you've now got to a situation if there is planning in the mix,
you're now getting much closer to that line if that line were ever to exist. I don't think it
gets you quite as far as self aware though. I think you and then you have to I think grapple
with the question. How would formally write down a countless or a maths of self awareness? I don't
think it's impossible to do, but I think you would be pressure on you to actually commit to a form
of definition of what you mean by self awareness. I think most people that I know would probably
say that a goldfish, a pet fish was not self aware. They would probably argue about their
favourite cat, but would be quite happy to say that their mum was self aware.
But that might very well connect to some level of complexity with planning. It seems like self
awareness is essential for complex planning. Yeah. Do you want to take that further? I think
you're absolutely right. Again, the line is unclear, but it seems like integrating yourself
into the world, into your planning is essential for constructing complex plans.
Mathematically describing that in the same elegant way as you have with the free energy
principle might be difficult. Well, yes and no. Can we just go back? That's a very important answer
you gave. I think if I just unpacked it, you'd see the truisms that you've just exposed for us.
I'm mindful that I didn't answer your question before. What's the free energy principle good
for? Is it just a pretty theoretical exercise to explain non-equilibrium steady states? Yes,
it is. It does nothing more for you than that. It can be regarded as going to sound very arrogant,
but it is of the sort of theory of natural selection or a hypothesis of natural selection.
Beautiful, undeniably true, but tells you absolutely nothing about why you have legs and
eyes. It tells you nothing about the actual phenotype, and it wouldn't allow you to build
something. So the free energy principle by itself is as vacuous as most
tautological theories. And by tautological, of course, I'm talking to the theory of
natural survival of the fittest. What's the fittest survival? Why are there cycles,
the fitter? It discards in circles. In a sense, the free energy principle has that same
deflationary tautology under the hood. It's a characteristic of things that exist. Why
do they exist? Because they minimize their free energy. Why do they minimize their free energy?
Because they exist, and you just keep on going round and round and round. But the practical
thing which you don't get from natural selection, but you could say has now manifest in things like
differential evolution or genetic algorithms at MCMC, for example, in machine learning.
The practical thing you can get is, if it looks as if things that exist are trying to
have density dynamics that look as though they're optimizing a variational free energy,
and a variational free energy has to be a functional of a generative model, a probabilistic
description of causes and consequences, causes out there, consequences in the sensorium
on the sensory parts of the Markov Planckium. Then it should, in theory, possible to write
down the generative model, work out the gradients, and then cause it to autonomously
self-evidence. So you should be able to write down oil droplets. You should be able to create
artifacts where you have supplied the objective function that supplies the gradients, that supplies
the self-organizing dynamics to non-equilibrium steady state. So there is actually a practical
application, the free energy principle, when you can write down your required evidence in terms of,
well, when you can write down the generative model, that is the thing that has the evidence. The
probability of these sensory data or this data, given that model, is effectively the thing that
the elbow or the variational free energy bounds or approximates. That means that you can actually
write down the model and the kind of thing that you want to engineer, the kind of AGI or
artificial general intelligence that you want to manifest probabilistically. And then you engineer,
that's not a hard word, but you would engineer a robot and a computer to perform a gradient descent
on that objective function. So it does have a practical implication. Now, why am I wittering
on about that? It did seem relevant to, yes. So would it be easy or would it be hard? Well,
mathematically, it's easy. I've just told you all you need to do is write down your perfect
artifact probabilistically in the form of a probabilistic generative model, a probability
distribution over the causes and consequences of the world in which this thing is immersed.
And then you just engineer a computer and a robot to form a gradient descent on that objective
function. No problem. But of course, the big problem is writing down the generative model.
So that's where the heavy lifting comes in. So it's the form and the structure of that
generative model, which basically defines the artifact that you will create, or indeed the
kind of artifact that has self-awareness. So that's where all the hard work comes,
very much like natural selection doesn't tell you in the slightest why you have eyes.
So you have to drill down on the actual phenotype, the actual generative model.
So with that in mind, what did you tell me that tells me immediately the kinds of
generative models I would have to write down in order to have self-awareness? What you said to me
was I have to have a model that is effectively fit for purpose for this kind of world in which I
operate. And if I now make the observation that this kind of world is effectively largely populated
by other things like me, i.e. you, then it makes enormous sense that if I can develop a hypothesis
that we are similar kinds of creatures, in fact the same kind of creature, but I am me and you are
you, then it becomes again mandated to have a sense of self. So if I live in a world that is
constituted by things like me, basically a social world, a community, then it becomes necessary
now for me to infer that it's me talking and not you talking. I wouldn't need that if it was on Mars
by myself or if I was in the jungle as a feral child. If there was nothing like me around, there
would be no need to have an inference that a hypothesis, ah yes, it is me that is experiencing
or causing these sounds and it is not you. It's only when there's ambiguity in play induced by the
fact that there are others in that world. So I think that the special thing about self-aware
artifacts is that they have learned to or they have acquired, or at least not equipped with,
possibly by evolution, generative models that allow for the fact there are lots of copies
of things like them around and therefore they have to work out it's you and not me.
That's brilliant. I've never thought of that. I never thought of that, that the purpose of
the really usefulness of consciousness or self-awareness in the context of planning existing
in the world is so you can operate with other things like you and like you could, it doesn't
have to necessarily be human, it could be other kind of similar creatures. Absolutely. Well,
we view a lot of our attributes into our pets, don't we? Or we try to make our robots humanoid
and I think there's a deep reason for that, that it's just much easier to read the world
if you can make the simplifying assumption that basically you're me and it's just your turn to
talk. I mean, when we talk about planning, when you talk specifically about planning,
the highest if like manifestation or realization of that planning is what we're doing now. I mean,
the human condition doesn't get any higher than this talking about the philosophy of
existence and the conversation. But in that conversation, there is a beautiful art of turn
taking and mutual inference, theory of mind. I have to know when you want to listen,
I have to know when you want to interrupt, I have to make sure that you're online,
I have to have a model in my head of your model in your head. That's the highest and most sophisticated
form of generative model where the generative model actually has a generative model of somebody
else's generative model. And I think that and what we are doing now evinces the kinds of
generative models that would support self-awareness because without that, we'd both be talking over
each other or we'd be singing together in a choir. That's not a brilliant analogy of what I'm
trying to say, but we wouldn't have this discourse. Yeah, the dance of it. Yeah, that's right. As I
interrupt, I mean, that's beautifully put. I'll re-listen to this conversation many times.
There's so much poetry in this and mathematics. Let me ask the silliest or perhaps the biggest
question as a last kind of question. We've talked about living in existence and the
objective function under which these objects would operate. What do you think is the objective
function of our existence? What's the meaning of life? What do you think is for you perhaps
the purpose, the source of fulfillment, the source of meaning for your existence as one blob
in the soup? I'm tempted to answer that again as a physicist. Free energy, I expect,
consequent upon my behavior. Technically, we can get a really interesting conversation about
what that comprises in terms of searching for information, resolving uncertainty about the
kind of thing that I am. I suspect that you want a slightly more personal and fun answer,
but which can be consistent with that. I think it's reassuringly simple
and harps back to what you were taught as a child, that you have certain beliefs about the
kind of creature and the kind of person you are. All that self-evidencing means, all that
minimizing variational free energy in an inactive and embodied way, means is fulfilling the beliefs
about what kind of thing you are. Of course, we're all given those scripts, those narratives
at the very early age, usually in the former bedtime stories or fairy stories that
I'm a princess and I'm going to be a beast who's going to transform and it's going to be a prince.
The narratives are all around you from your parents to the friends, to the society feeds
these stories and then your objective function is to fulfill... Exactly. That narrative that has
been encultured by your immediate family, but as you say also, the sort of the culture in which
you grew up, and you create for yourself. Again, because of this active inference, this inactive
aspect of self-evidencing, not only am I modeling my environment, my equinish, my external states
out there, but I'm actively changing them all the time and external states are doing the same
back, we're doing it together. So there's a synchrony that means that I'm creating my own
culture over different timescales. So the question now is for me being very selfish,
what scripts were I given? It basically was a mixture between Einstein and Shark Holmes.
So I smoke as heavily as possible, try to avoid too much interpersonal contact,
enjoy the fantasy that you're a popular scientist who's going to make a difference in a slightly
quirky way. So that's where I grew up. My father was an engineer and loved science and
he loved things like Sir Arthur Edington's Space, Time and Gravitation, which was the
first understandable version of general relativity. So all the fairy stories I was told as I was
growing up, all about these characters, I'm keeping the hobbit out of this because that was
quite fit my narrative. It was a journey of exploration, I suppose, of sorts. So yeah,
I've just grown up to be what I imagine a mild-managed Shark Holmes slash Albert Einstein would do in
my shoes. And you did it elegantly and beautifully, Carl. It was a huge honor talking today. It was
fun. Thank you so much for your time. Thank you, Shane. Thank you for listening to this conversation
with Carl Friston and thank you to our presenting sponsor, Cash App. Please consider supporting
the podcast by downloading Cash App and using code Lex Podcast. If you enjoy this podcast,
subscribe on YouTube, review it with Five Stars and Apple Podcasts, support on Patreon,
or simply connect with me on Twitter at Lex Freedman. And now let me leave you with some
words from Carl Friston. Your arm moves because you predict it will and your motor system seeks
to minimize prediction error. Thank you for listening and hope to see you next time.
