with the variable we're allowed to vary which is our p of x tilde given x this is what defines our
compression we'll set these equal to zero and what we'll find is that this p of x tilde given x
this mapping that we want to make is exactly this so you can run through this algebra yourself
it's pretty easy there's some normalization okay so we get a solution that comes from
some exponential family and it has in the exponent this distortion metric okay
so this is so let me just tell you if you run through the algebra if you write this out remember
the information value contains entropy entropy contains logs we're with with a with a prefactor
that's this p of x we're taking derivatives with respect to p of x so it's not surprising
that we end up with when we take this condition that we end up something with log of p of x
given x tilde and that's how you get things in this exponential family so in general when you
want to optimize entropy quantities you end up with solutions that have this exponential form
okay so coming back to what we'd like to do i wanted to remind you of this this form this is
what you get when you're just trying to uh when you're trying to minimize your information rate
subject to some distortion wanted to show you that because now we're going to write down
this optimization problem that i've been talking about this information bottleneck and i'm going
to sketch out the form of solution so everybody remember this i'm going to erase it
and then i'm going to sketch out the solution for the information bottleneck problem that we want
to solve okay okay so remember that we are going to compress x into x tilde and we want to retain
an information in x tilde about y some relevant variable and we've decided that a relevant variable
is the future of the stimulus so we're going to solve we're going to solve this optimization problem
so we're going to minimize the following function with respect to p of x tilde given x those are
the variables we're allowed to play with and our optimization function we want to compress
we want to compress our representation of the stimulus into our neural response subject to the
constraint that we retain as much information in x tilde as possible as possible about y
and i'm just going to be really explicit here and also add the constraint that we want this
probability distribution to be normalized so we want so we need some Lagrange multiplier for each
x that says that p of x tilde given x should be normalized okay and this is a sum over x next
so we have a Lagrange multiplier for each x that keeps this p of x tilde given x normalized
okay so you guys are all familiar with this kind of technique for solving constrained equations
i've just written out the two constraints we have here we want to we want to retain a certain
amount of information about the future and we want to make sure our probability distribution
is normalized okay so i want to say right away that we can write down given that we have this
Markov chain condition that y and x have some mutual information and we're going to map from x
to x tilde so that's our Markov chain condition we can write down p of x tilde given y and that's
just a sum on x p of x tilde given x p of x given y if we want to get from x tilde to y we have to
go through x that's all that's saying and p of x tilde is sum on x p of x tilde given x p of x okay
so i'm going to sketch this out for you like i said we're not going to go through all the
algebra together but these equations let you write down something that will be very useful
when you when you want to figure this out so the derivative of p of x tilde given y
given p of x tilde given x is just p of x given y and derivative of p of x tilde
with respect to p of x tilde given x is just p of x okay because what are we going to do
we're going to take a derivative of this guy with respect to the variables we're varying
we're going to write that all out and set it equal to zero and i thought i might have time
to do that with you but we do not but if you use these if you use these guys you'll save yourself
a lot of trouble so what are you going to do you're going to write down the information in terms of
the entropies okay so you're going to write this all out you're going to take derivatives with respect
to x tilde given x and then you're going to set that equal to zero and find the solution
i am going to quote for you the solution which i'm now as i flip through pages i realize i was
quite ambitious in how much board work i could get through today but it's all it's all very simple
it's it's simple algebra and i'm happy to um share it with you if you guys would like to see it in
gory detail so the answer you get is that p of x tilde given x for this constrained optimization
problem is just p of x tilde over partition function our normalization and then we have
something in the exponent and the thing we have in the exponent is minus beta and we get a dkl
colbeck-libler divergence between p of y given x and p of y given x tilde okay
and let me say that just for completeness this partition function is e to the minus k
and k
k is lambda of x over p of x plus beta p of y given x log p of y this is so you can check your work
if you want to go through these details okay so here is our answer
so what we found is that our distortion metric is the colbeck-libler divergence
between p of y given x compared to p of y given x tilde okay so we're comparing what we know about
this relevance variable in our original in our original variable and this relevance variable
in x to the relevance variable in our compressed representation of x and so that's
quite nice and this taken together with these equations give us a set of self-consistency
equations for p of x tilde given x and p of x there are various methods you can use to actually
find p of x tilde given x there's an iterative algorithm that's guaranteed to converge called
the Blau-Hut-Aremoto algorithm for computing this and if x and y have particular forms if the joint
distribution of x and y is Gaussian so the variables are jointly Gaussian you can actually
solve this analytically okay and I refer you to Tishbee uh and Bialik which has some typos in it
that we have corrected here 1999 and there's also Chechik Glaberson
and Tishbee 2005 something like that uh Gaussian information bottleneck okay very good
right absolutely okay so let's go back and talk about how we set this up in y
so the the function that we're trying to minimize we are trying to minimize the mutual information
between x and x tilde that's the compression part there's a minus sign here so we're keeping a fixed
amount of information between x tilde our compressed variable and this relevance variable y the beta
parameterizes that trade-off so here we're compressing here we're retaining relevant information
and this is just the normalization constraint at the end of the day the solution that we get
is one where our distortion metric is a measure of of the distance between the distribution on
y conditioned on x and y conditioned on x tilde so why is this relevance variable
variable this is the original input variable and this is our compressed representation of it so
we're saying how far away are we from representing y in x tilde than representing y in the original
formulation that's the intuition behind where this goes so now what we're actually going to do
is go measure this in ah okay good we have time go measure this in 15 well I I have 12 minutes
okay so we're going to go measure this in the brain so this is the general generalized idea
of information bottleneck you compress but you keep relevant information we said that the relevant
information was how much we can infer from the past of the stimulus about the future of the stimulus
we have correlations in the stimulus so this mutual information between x and y
this distribution constraints how much we can possibly have so if oh here are the here are
the references good and I even got the got the year right just missed an author all right so here
is the problem we solved without the normalization constraint what this lets us do is sweep out
a curve that describes how much we could possibly have about the future given how much
information we retained about the past we can't do any better than that and what we'd like to
know now is does the retina sit close to this bound so we're going to interrogate this part of
the curve this is where the values of the past that we get in the retina live so if we look at
that we compute the mutual information between the spiking of the response of the retina at
present and the stimulus in the past or the spiking of the retina at the present and the
stimulus in the future what we find is that for many groups of cells in the retina these responses
sit close to the bound so what we've been able to show is that actually there are neurons in
the retina that seem to be doing as well as possible all of these values here were open to the retina
for some given amount of information about the past the retina could have could have
could have had no information about the future it could have been representing that noise trace
of the stochastic jittering of the bar but it actually squeezed out as much information about
the future as possible which leads us to you know want to dive deeper into this question of whether
or not the retina is optimized for all prediction problems or just or or just you know a few
particular ones we got lucky with this stimulus that the retina was optimized for this and what
my group has been doing now is to play around play around with this formulation of the problem
now the reason we were able to do this which is coming back to a question we had earlier
is that we could fully describe this bound we could compute that bound because that uh
that bar motion that I showed you was Markovian it was a physical system so current position
velocity determine everything about the next time step so we have a situation where we know
exactly what parts of the stimulus go into this complete representation of the past and complete
representation of the future how you actually compute this in neural data is a little bit
more tricky and subtle and we can we can discuss that at coffee if you like
so let me ask if there are questions now if there aren't I'll show you a few more things
yeah just a short question but you say it's close to the back yes what would be apart from the
like what's the yes it says what is good and what's bad very good um let me let me let me
give you uh an answer to that so let's go here so this was the bar movie uh and remember I said
that there were these ah good okay so I said that there were these uh predictable components of the
bar um this essentially describes uh a damping coefficient of this motion without the noise
so we can move that around and now I can show you things that are close to the bound a bound
and not close to the bound so you get a sense of close what I meant by close there had to do with
those error bars so were were they within error bars of the bound and were there things that were
outside there so okay so we can play around with these parameters so I can make um I can well
this is a little bit much I can make a system with these parameters that is uh sort of close to
critically damped and uh with these parameters something that's underdamped and the intuition
that I'm trying to give you here without explaining it very well is that in a critically damped uh
situation you want to put all your bits into representing position information whereas in
the underdamped case you want to put them you want to squeeze out variance in the velocity
condition you're stuck with some amount of noise that's left over um in your response but if you
you can think about the retinal response is being summarized by its position velocity trade-off
and I can make bar motions where one trade-off is is optimal and over the other and I can play both
of these kinds of motions say to the retina and ask does it solve these prediction problems equally
I think it should be clear why uh when you have an underdamped situation position isn't a good
predictor if I imagine that I say I'm I'm here I could be going this way I could be going that way
position isn't a great predictor where I would be next I actually need to know my velocity
that's some basic intuition to give you now what we see when we look in the rat retina
at these two bounds these are the same neurons from the same rat retina we just showed this movie
where it's optimal to squeeze out variance along in the position direction and here it's optimal
to squeeze it out along velocity we see that these points I think you would agree are not
close to the bound and these points are hugging the bound so this is a rough sense of what I mean
by close maybe there are four or five sort of balls of position I could describe what's nice about
this is that the retina responded to this motion it had lots of information about the past it just
didn't have very much information about the future whereas here it it did so that's part of our
research program is to try to define the sets of uh of motions that are predictable in the
early visual system and to ask next if those are defined uh by the statistics of things that the
brain is most likely to see meaning what does the natural world look like it's not a random collection
of arbitrary motion there are particular kinds of motion that we encounter in our natural world
and do the statistics of our natural world match the prediction problems the brain over
evolutionary time has has evolved to solve optimally um I want to take more questions or I can show
you a little bit about that yeah so is it also possible that the retina system can be trained
like that I mean the for example this example the student I mean that she is much better
right so this is a very good question so the retina unfortunately I mean the retina is a
fairly static piece of their old tissue meaning there's not a lot of uh synaptic plasticity in
the retina itself that said there are a lot of adaptation mechanisms that are also kind of
hardwired in there that would let you maybe flexibly adapt to different situations so what's
going on with serena what does she actually train herself to do so there's two things she's definitely
trained herself to hit the ball so that whole motor program is is exquisitely trained and so while you
and I might be just as good at predicting where the ball will be making contact with a racket is
a whole other game and that's your motor system there's tons of plasticity it also turns out
that professional athletes have better eyes so in the pool of people that get selected to be
and you pull out of them you know professional athletes they happen to be not surprisingly
the people who are on the edge of the distribution for acuity in their vision and reaction times and
things like that so serena's retina is like our retinas maybe a little better but her motor program
is that's the thing that's crazy better okay so I have I have a couple minutes do you mind if I
show you where we're trying to take this next okay good so what I'm showing you here is that
you know we have these simple physical bar movies and we're playing with them because we can solve
this bound we can compute this bound analytically and we can ask does the retina solve this problem
optimally does it solve that problem optimally what we'd actually like to get to at the end of the
day is something that's you know more natural than that so okay let me just go here I think it took
my computer a second to uh no don't freeze no don't freeze right on the cool movie all right
let's see if I can get this to play okay good so what we did is we ran around we ran around Chicago
and we filmed things that were moving and we asked about the natural statistics of motion
because the big idea is that the brain has been tuned to the inputs it's most likely to get
so the question the context of visual prediction is what are the statistics of things you're you're
most likely to see in the world so we did some characterization of of that of that motion we
computed you know Fourier spectrum of temporal frequencies we computed something about contrast
distributions we computed something about higher-order motion a lot Jonathan Victor and co
um but what we really wanted to do was ask what higher-order features of motion are present in
these natural scenes so let's talk about that for a second so here is something that looks a little
odd to begin with but this is a video of some grasses blowing in the wind color is now on that
pixel what direction of motion it's heading and saturation is the magnitude of the flow so this
is like a flow field where instead of a bunch of vectors I've put colors to tell you direction
and instead of a magnitude of the direction I've just put saturation so white is no motion
and something really bright is strong motion in that direction so we can take this natural motion
and we can make a kind of just summary of it just based on flow and I think you guys can kind of
see some grasses blowing around you can pick out things there um you might have noticed that there
was a B movie so here's the B movie in that same representation so you can tell we've gotten rid of
like anything B texture like it's just it's just flow and what we want to ask is what are what are
the higher-order features of motion that that give rise to our to our ability to predict where these
things will be next so let's get rid of everything like texture and stuff like that so here is um
here is the B movie again and I hope now what I've done what Jared in my group did is he took
that flow field and he just applied it to white noise so this is just white noise that's flowing
the way the bees flow and just there's no B in any single frame it's just white noise so all he
kept were the oops correlations between between the pixels and so now we can ask does the retina
code for this kind of motion optimally what are the different ways in which we can filter out
aspects of the motion in natural scenes so that we can ask does the does the brain care about this
or that without having to live in this very very high dimensional space of all the pixels so I'm
gonna leave you with with that and leave you with the the question of trying to understand
how our brain forms predictions in this very abstract reduced space and what insight that
can give us about computation in the brain and how again over evolutionary time our brains have been
organized to do count efficient calculations on our environment okay thanks
