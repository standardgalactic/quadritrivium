week it is and for some strange reason he's in he's in a very um bad situation where he can't
take in information about what day of the week it is from anything he can't read the newspaper he
can't look at a calendar it nothing gets in he can't ask anyone what day of the week it is except
for one person and that's his advisor his thesis advisor is the only person who can tell him what
day of the week it is um and and she's rather diabolical so so first of all he does know
everything else about his brain is intact he's a thinking rational human being so what is his
uncertainty about what day it is he has no information about what day it is but he knows
there are seven days a week okay so his uncertainty uh is that he's you know he thinks any day of the
week it could be any of the day of the week he has no information is he has he has no reason
to believe that any day is is more likely than any other because he understands that the world
operates the way it does so he has a he has prior distribution or his his assumption about what
day of the week it is is flat and it's just equal equal probability each day of the week and this
is just one seventh one seven one seven one seven okay so what is the entropy of this distribution
we know how to compute that p log p if only if only there were eight days a week
okay then we'd know there would be three bits two two to the three is eight uh of of entropy here
but the entropy uh and here i've written an s we're calling this this is an h the the entropy
of the days of the week is 2.807 bits okay now the professor is a bit i said she was a bit diabolical
so her distribution of uh probability of of of days of the week that she says it is so
he can he can write down you know what day of the week did she say it was and he can develop some
intuition for for what she says and she very often tells him it's wednesday she very rarely tells him
it's sunday or saturday again she's a jerk um and then she also will tell him you know sort of
randomly that it's monday tuesday thursday or friday so he'd like to know what day of the week it is
what's that you would have said monday see but she she she she she wants to be a little bit kind
ah it's wednesday it's a hump day it's the middle you're fine uh and so her this entropy of this
distribution if you work it out if you calculate p log p is 2.665 bits so he can't he can't listen
to the professor and get full information about what day of the week it is because she's you know
manipulating things and she's arranged her answers in such a way that there's no linear correlation
between the day of the week and the day that she says so this is the joint distribution of the actual
day and the day that that the prof says it is so here we've got these bright spots on wednesday
and i tried to make this as best i can a circle all right um and it got a little pixelated so uh
the my my program tried to try to make this look look better but it's supposed to be just blocks
here okay so this is the joint distribution of the actual day of the week and what day it was
according to the prof so we'd like to compute the mutual information between the actual day
and what the prof says so there's no correlation there's no way you can say oh there's no way you
can make some sort of linear shift map where he says when when it's wednesday it's thursday
when she says it's thursday it's friday and so on and so forth because you've got no linear
correlation between between the days and the actual day okay so we'd like to compute the
mutual information between the what the prof says and what day it actually is so let's compute that
like so so let's compute um these conditional distributions so what's the entropy of days of
the week given that the prof says it's sunday well if we go back and we look when the prof says it's
sunday it's actually always wednesday okay so what's the entropy of this conditional distribution
this particular one well there's only one answer she gives there's no entropy there's no there's
no variability here and you can write it out long hand and show that indeed that zero okay what if
what if she says it's monday she says it's monday it's equally probable that it was tuesday or
thursday two options with that are equal probable if we do p log p on that you'll find and we do
this in log two so we're talking about bits i forgot to mention that if you do log two do log two
here it's bits if you do natural log you could call it gnats or nits if you do it base 10 it's
dats or dits okay but bits is the most common one for binary variables so here there's one bit of
information here the same if she says it's tuesday it's equally likely to be monday or friday
she says it's wednesday it's equally likely to be saturday or sunday she says thursday
she says friday if she says it's saturday it's always wednesday there's no entropy there and
if we want to compute the total mutual information between the day of the week and what the prof said
we have to average that conditional distribution over the probability of each day of the week
that it actually was those days of the week so we need to take these conditional entropies that
we just computed for particular days and multiply it by the probability of that day so we need to
know what's the probability that she says it's sunday what's the probability that she says it's
monday and this was the entropy that we started with of the days of the week which is almost three
2.807 and then we subtract what we get what we get from the prof like how how regular are her
answers and we end up with almost two bits of information we could have computed it directly
from the joint distribution here and we get exactly the same answer so you can either look at
look at the marginals or look at the joint distribution the reason so okay so the student
can get almost two bits of information out of the prof if only he knew how to decode what she was
saying so what this mutual information and he could you know maybe you know maybe someone could
compute this for him and tell him ah you could you could actually you could actually know what
day it is but he would have to know code he would have to know how to read that out what she says
so that's a good thing to remember about mutual information it will quantify the amount of statistical
dependence between two variables it doesn't tell you how to how to how to decode okay and you can
compute it either way you can compute it from the joint distribution or you can compute it from
the conditional distributions here and oftentimes when you're working with real biological data
computing this quantity or can or sampling from this joint distribution will be very different
computational problems with different biases for finite size effects in your data and you
should be careful to try both and decide which one is is is more suited to your purpose okay
so let's go into the retina this is a this is a larval tiger salamander that's recorded in the
berry lab these are pictures from Ronan Segev who now has his own lab at Ben Gurion so this is the
larval tiger salamander from which we dissected out the eye and squished the retina down onto
this glass slide with these recording electrodes that are in black the retinal ganglion cells have
been backfilled so they're these green blobs okay and the big streaks are the axons from from the
ganglion cells that are streaming towards and collecting together to form the optic nerve
so the optic nerve would punch through the retina like down here okay come together and what you're
supposed to take away from this picture is that the recording electrode density and the cell body
density is approximately matched so when you get a good squish of the retina onto this glass slide
you have the opportunity at least to record from every single cell in that region of the retina
and the retina is has a has a topographic projection of space so you know this part of the retina
sees that part of the visual world and if you can record every cell in the retina then you have a
complete potential potentially complete picture of what the brain sees in that part of the world
so that's why theorists particularly get excited about these recordings from the retina
you get not just a lot of cells and a convenient little piece in a dish you actually can see what
the brain sees and you can record from a complete neural population i'm also going to show you
recordings from rat retina that are done the same way this is a newer bigger better faster array
with again with the same sort of spacing so you get the good match between the density of the
electrodes and the density of the retinal ganglion cells just has many more it says 252 electrodes
and then the corners are reserved for grounding and this is these are data from olivier mars lab
so i told you that what we want to do is present to the retina a stimulus that has
predictable and non-predictable components and we want to oh yeah good wondering whether you
actually take some signal from the axons as well or just uh you do get so when you when you when
you squish yeah that's a very good question because you can see those axons there and the
question is do you actually pick up spikes from the axons you do they they look different uh on
they're on the on the electrodes so the the voltage trace is very different it's more
completely biphasic the if you're recording that's what a typical spike on an extracellular
electrode from the array looks like from a from a cell body and an axon spike will look more like
that so you can distinguish axon spikes from cell body spikes you also see you'll see that axon
you'll see the the the spike moving all across the array so you do you do pick up both signals and
we tend to throw away all the axon spikes just because but they're they're perfectly reasonable
data it's just convention okay so let's get back to what we wanted to do what we want to do is we
want to ask if the retina is compressing information and now we know what we mean by that uh about
the stimulus in such a way to keep as much about the future as possible so what we want to do is
we want to give the retina an option we want to give it a stimulus that has some amount of
predictable component some amount of non-predictable component and when it compresses we want to ask
does it preferentially keep the predictable part over the non predict and throw away more of the
non-predictable part or does it just do sort of a random compression that would keep a mixture of both
it's okay so we want to make that more concrete uh this moving bar stimulus that i'm showing you
obeys this equation of motion if v is the velocity of the bar x is the position of the bar
then there's some there's some drag characterized by this time constant tau this is a random white
noise variable this gamma of t it has in front of it a diffusion constant and this this uh omega
naught is the characteristic frequency of a fictive spring that's tethering this bar to the
center of the screen okay so most of you probably seen brownie in motion before this is just brownie
in motion this is an orange t nuland back process this part here and this is adding onto that uh a
thing that stabilizes the the position distribution makes it stationary okay because we want to do
this experiment over a retina where we're recording from a particular piece of it so we want this bar
to stay in one place okay so this is the distribution of positions that we have there are two predictable
components to the motion there's the spring part and there's the drag part and there's a non-predictable
part there's noise that comes in so this has both stochastic and deterministic parts of the motion
this is just a typical trace real trace at time and seconds position and pixels on the screen which
you can translate into microns by multiplying by 8.7 or something like that uh microns per pixel
when we project it onto the onto the retina that's for typical trace of this bar center of the bar
and this is the autocorrelation of the position of the bar okay so again as simona already did a
lovely introduction to the retina we're recording from many cells at once and we're writing down
the response of the retina as a binary word spike or not from each one of those cells so we're
just late we're just using ones to indicate which cell spiked as a function of time so these are
our fundamental data these binary strings and here again is a representation a more accurate
representation of the position of the bar as a function of time here is where we recorded some
spikes at time zero and we'd like to compute the mutual information between that binary word at time
zero and the position just the position in one time bin of the bar at time t plus delta t and
then we're going to vary delta t okay so here's the response the retina and we're going to compute
this mutual information like so okay so for single cells if it's well before we spiked the cell has
very little information so this is zero information about the position of the bar so here position of
the bar here also far into the future but if we trace out and we ask how much information does it
have you know a few hundred milliseconds 300 milliseconds before it has some more has more
about 200 milliseconds it has a peak of information a little bit before the the uh spiking the cell
and it has some information about the future position of the bar so if we average over many
cells in our recording this is the kind of curve we get for a single cell how much information does
it have okay we can also look at two cells now here i've normalized by the firing rate so that it's
all in sort of bits per spike yeah w of t yes this is the this is the binary word the response of
the retina so it's it's that vector of values of did that cell spike or not so if you have n cells
there are two to the n possible responses that you could have and it's the correlation that's the
it's the generalized correlation between those responses and the position of the bar okay so um
none this is a published in pns last year okay so uh here is for one cell we can also look at it
for two cells or three cells or four cells or five cells okay and i've written it in bits
per spike so that you can see you know the seven cells are going to have more spikes but how many
per spike you can see that you know the information here is somewhat redundant as we add more cells
we get less information per spike that that relationship might be flipping over here in the
future but that's a little bit thorny question so it's not surprising that we see information
bleeding into the future because there are correlations in the stimulus itself right so if
we have information about where the bar was here in the past it's not surprising that we have some
information about where it will be in the future the question we'd like to ask is a more is a finer
one a more detailed one the question is given that we had this much say so so much information about
the past of the stimulus how much could we possibly have here bleeding into the future and do we do
do we saturate some bound on the maximal amount of information we could have in the future okay
so let's go ahead and and go through that and i just want to say that what we've been talking
about here in uh mutual information and information theory is just the tip of the iceberg there's
the source coding theorem which talks talks about compressibility of signals given their entropy
the channel coding theorem which talks about uh limit of information rates and noisy channels
and so you should you should definitely go check out um uh cover in thomas uh is a good is a good
textbook for this also i'm a chi spoke is a good textbook for this i can write those down at the
end of the lecture if you'd like so we're going to take another chalkboard interlude because i want
to set up the problem that we're going to solve here and then i'll show you how we solve it for
the retina okay so i'm going to erase all this so we want to set up this information this information
bottleneck problem so that we can actually compute what the optimal compression of the stimulus would
be if we want to have as much information about the future as possible okay i think i have more
pages of algebra than we can get through in a half hour right 10 10 30 yeah so uh so we're
going to skip a few steps but um you can you can fill them in you can fill them in yourself so
we're going to imagine that x is now the stimulus that's coming in
and we want to map this on to some x tilde and this is say our representation
in the brain in the retina and recall why the future stimulus
so this is the stimulus in the past and this is the stimulus in the future and there's a
relay and we'd like to know how much information we can have in our in our compressed representation
about why but we know that the the real relationship here is between x and y that's where the information
comes through just to be clear all right so let me remind you a piece of information from from
rate distortion theory so if we have if we compute this mutual information between the past stimulus
and our compressed representation of it of course this is the sum on x and x tilde of p of x
x tilde log p of x x tilde divided by p of x okay so this defines some information rate that's in
our compression the number of average number of bits per message x tilde that we're sending
averaged over x but we'd like to know what we lost when we did this so we could compute some
distortion between x and x tilde so this d is just is just talking about what we lost when we
converted to x tilde and a typical some typical things that you use for this function could
be something like the mean squared error okay so we compute the average distortion
so the average distortion is just p of x is just averaging this distortion metric over the joint
distribution and there's a monotonic tradeoff between the rate of quantization so how you
how what information rate you have how much information you have an x tilde about x and
how much what your expected distortion is so if you have more information you can have less
distortion which kind of makes sense and the rate distortion theorem tells you that given a particular
average distortion d the minimum achievable rate that you can have for passing messages through the
system minimum of x of the mutual information between x and x tilde and the variables that you're
allowed to manipulate are p of x tilde given x but you have to have the condition that the average
distortion is less than or equal to some number d okay so this is you know how much you can compress
so that you so that you only have some amount of distortion or less okay so there's a tradeoff
between distortion and information rate and we can write that as this constrained optimization
problem we want to compress x into x tilde subject to the to the fact
subject to the constraint that we only distort by some amount d which i'm writing down as the
average distortion now you want to solve this so we'll take variational derivatives of this
