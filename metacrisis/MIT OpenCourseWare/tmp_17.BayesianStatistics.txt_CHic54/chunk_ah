towards this idea, your data.
And maybe that's maybe not such a good idea
when you have some p that's actually close to 1 half,
for example.
You're actually saying, no, I don't
want to see a p that's close to 1 half.
Just make a decision one way or another,
but just make a decision.
So it's sort of forcing you to do that.
OK, and so Jeffery's prior, so I'm running out of time,
so I don't want to go into too much details.
But we'll probably stop here, actually.
So Jeffery's priors have this very nice property
is that they actually do not care about the parametrization
of your space.
So if you actually have p, and you suddenly
decide that p is not the right parameter for Bernoulli,
but it's p squared, you could decide to parametrize this
by p squared.
Maybe your doctor is actually much more
able to formulate some prior assumption on p squared
rather than p.
You never know.
And so what happens is that Jeffery's priors
are invariant into this.
And the reason is because, well, the information carried
by p is the same as the information carried
by p squared somehow, right?
I mean, those are essentially the same, I mean, well,
yeah, they're essentially the same thing.
And so I mean, you need to have a one-to-one map, right?
Where you basically, for each parameter before you
have another parameter, so let's call eta the new parameters.
Then the PDF of the new prior indexed by eta this time
is actually also Jeffery's prior.
But this time, the new Fisher information
is not the Fisher information with respect to theta,
but it says in the Fisher information associated
to the statistical model indexed by eta.
So essentially, when you change Jeffery's prior,
when you change the parameterization of your model,
you still get Jeffery's prior for the new parameterization,
which is, in a way, a desirable property.
All right, so Jeffery's priors, just
like non-informative priors, are priors
you want to use when you want a systematic way
without really thinking about what to pick for your model.
OK, so, well, OK, I'll finish this next time.
And we'll talk about Bayesian confidence regions.
We'll talk about Bayesian estimation.
Once I have a posterior, what do I get?
And basically, the only message is
going to be that, well, you might
want to integrate against the posterior.
Find the expectation of your posterior distribution.
That's a good point estimator for theta.
And then we'll just do a couple of computation.
All right, so.
