And this function, x times 1 minus x looks like this.
We've drawn it before, right?
That was something that showed up
as being the variance of my Bernoulli.
So we know it's something that takes its maximum at 1-1.5,
and now I'm just taking the power of this guy.
So I'm really just distorting this thing
into some fairly symmetric manner, OK?
So this distribution that we actually take for p, right?
So here, I assume that p, the parameter, right?
I mean, notice that this is kind of weird.
First of all, this is probably the first time
in this entire course that we have this has this something
has a distribution when it's actually a lower case letter.
That's something you have to deal with,
because we've been using lower case letters for parameters,
and now we want them to have a distribution.
So that's what's going to happen, all right?
And this is called the prior distribution, OK?
So really, I should write something
like f of p is equal to a constant times p 1 minus p
to the a minus 1.
Well, no, actually, I should not, because then it's confusing.
OK, so let me not do this.
One thing in terms of notation that I'm going to write,
I'm going to write, when I have a constant here,
and I don't want to make it explicit,
and we'll see in a second why I don't need to make it explicit,
I'm going to write this as f of x is proportional to x 1
minus x to the a minus 1, OK?
So that's just to say equal to some constant that does not
depend on x times this thing, OK?
So if we continue with our experiment, now if p,
right, so that's the experiment where I'm trying to,
I'm drawing this data x 1 to x n, which is Bernoulli p,
if p has some distribution, it's not clear
what it means to have a Bernoulli with some random parameter.
So what I'm going to do is then I'm going to first draw my p.
Let's say I get a number 0.52, and then I'm
going to draw my data conditionally on p, all right?
So here comes the first and last flowchart of this class.
So I'm going to first, all right?
So nature first draws p, OK?
So p follows, say, some beta AA.
Then I condition on p, and then I draw x 1, x n, that are i,
i, d, Bernoulli p.
Everybody understand the process of generating this data, right?
So you first draw a parameter, and then you just
flip those independent bias coins with this particular p.
So there's this layered thing.
So now, conditionally on p, right?
So here, I have this prior about p, which was the thing.
So this is just the thought process again, right?
It's not anything that actually happens in practice.
This is my way of thinking about how the data was generated.
And from this, I'm going to try to come up with some procedure.
Just like if your estimator is the average of the data,
you don't have to understand probability
to say that my estimator is the average of the data, right?
I mean, anyone outside this room understand
that the average is a good estimator for some average behavior,
and they don't need to think of the data as being
a random variable, et cetera.
So same thing, basically.
Now, we will see.
I mean, actually, we won't.
But in this case, well, we will.
In this case, you can see that, essentially, the posterior
distribution is still a beta, all right?
So what it means is that I had this thing,
then I observed my data, and then I continue.
And here, I'm going to update my prior
into some posterior distribution, pi.
And here, this guy is actually also a beta, all right?
So pi now, P, my posterior distribution on P,
is also a beta distribution with the parameters that
are on this slide, and I'll have space to reproduce them.
So I start the beginning of this flow chart
as having P, which is a prior.
I'm going to get some observations,
and then I'm going to update what my posterior is, OK?
So this posterior is basically something
that's in Bayesian statistics was beautiful,
is as soon as you have the distribution,
it's essentially capturing all the information about the data
that you want for P. And it's not just a point, right?
It's not just an average.
It's actually an entire distribution
for the possible values of theta.
And it's not the same thing as saying, well, you know,
if theta hat is equal to x and bar in the Gaussian case,
I know that this is some mean mu,
and then maybe it has variance sigma square over n.
That's not what I mean by this is my posterior distribution,
right?
This is not what I mean.
This is going to come from this guy, right?
The Gaussian thing and the central limit theorem.
But what I mean is this guy.
And this came exclusively from the prior distribution.
If I had another prior, I would not necessarily
have a beta distribution on the output.
So when I have the same family of distributions
at the beginning and at the end of this flow chart,
I say that beta is a conjugate prior,
meaning I put in beta as a prior,
and I get betas at posterior.
And that's why betas are so popular.
Conjugate priors are really nice,
because you know that whatever you put in,
what you're going to get in the end is a beta.
So all you have to think about is the parameters.
You don't have to check again what the posterior is
going to look like, what the PDF of this guy is going to be.
You don't have to think about it.
You just have to check what the parameters are.
And there's families of conjugate priors.
Gaussian gives Gaussian, for example.
There's a bunch of them.
And this is what drives people into using specific priors
as opposed to other.
It has nice mathematical properties.
Nobody believes that the P distribution is really
distributed according to beta, but it's flexible enough
and super convenient mathematically.
All right, so now let's see for one second
before we actually go any further.
What I did, so A and B, I didn't mention it.
And here, A and B are positive numbers.
OK, they can be anything positive.
So here what I did is that I updated A into A plus the sum
of my data, and B into B plus and minus the sum of my data.
So that's essentially A becomes A plus the number of ones,
and B becomes B. Well, that's only when I have A and A, right?
So the first parameters become itself plus the number of ones,
and the second one becomes itself plus the number of zeros.
And so just as a sanity check, what does this mean?
If A goes to 0, what is the beta when A goes to 0?
We can actually read this from here, right?
So we had A, sorry, actually, let's take A goes to, no, actually,
sorry, let's just do this.
OK, let's not do this now.
I'll do it when we talk about non-informative prior,
because it's a little too messy here.
OK, so how do we do this?
How did I get this posterior distribution given the prior?
How do I update this?
Well, this is called Bayesian statistics,
and you've heard this word Bayes before,
and the way you've heard it is in the Bayes formula, right?
What was the Bayes formula?
The Bayes formula was telling you that the probability of A given
B was equal to something that depended on the probability
of B given A, right?
That's what it was.
And I mean, you can actually either remember the formula,
you can remember the definition, and this
is what P of A and B divided by P of B. So this
is P of B given A times P of A divided by P of B, right?
That's what Bayes formula is telling you, agree?
So now what I want is to have something
that's telling me how this is going to work, OK?
So what is going to play the role of those events, A and B?
Well, one is going to be the distribution of my parameter theta
given that I see the data, and this
is going to tell me what is the distribution of the data
given that I know what my parameter theta is.
But that part, if this is data and this is the parameter theta,
this is what we've been doing all along.
The distribution of the data given the parameter here
was n i i d Bernoulli P. I know that.
I know exactly what their joint probability mass function is.
Then that was what?
So we said that this is going to be my data,
and this is going to be my parameter, OK?
So that means that this is the probability of my data
given the parameter.
This is the probability given the parameter.
This is the probability of the parameter.
What is this?
What did we call this?
This is the prior.
It's just the distribution of my parameter.
Now, what is this?
Well, this is just the distribution of the data itself,
all right?
So this is essentially the distribution of this if this
was indeed not conditioned on P, right?
So if I don't condition on P, this data
is going to be a bunch of i i d Bernoulli
with some parameter, but the parameter is random, right?
So for different realization of this data set,
I'm going to get different parameters for the Bernoulli.
And so that leads to some sort of convolution.
I mean, it's not really a convolution in this case,
but it's some sort of composition of distributions, right?
I have the distribution, the randomness that comes from here,
and then the randomness that comes from realizing the Bernoulli.
So that's just the marginal distribution,
and it actually might be painful to understand what this is,
