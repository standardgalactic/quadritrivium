But maybe that's just a little too much.
So how do I do this trade-off between adding the data
and combining it with this prior knowledge?
In many ways, in many instances, essentially what's
going to happen is this 1.5 is going
to act like one new observation, essentially.
So if you have five observations,
this is just a six observation, which will play a role.
If you have a million observations,
you're going to have a million in one,
and it's not going to play so much a role.
That's basically how it goes.
That's basically how it goes.
But definitely not always, because we'll
see that if I take my prior to be a point minus 1.5 here,
it's basically as if I was discarding my data.
So essentially, there's also your ability
to encompass how strongly you believe in this prior.
And if you believe infinitely more in the prior
than you believe in the data you collected,
then, of course, it's not going to act like one more
observation.
All right, so the Bayesian approach
is a tool to, one, include mathematically our prior,
and our prior belief into statistical procedures.
So maybe I have this prior knowledge,
but if I'm a medical doctor, it's not clear to me
how I'm going to turn this into some principle way of building
estimators.
And of course, the second goal is
going to be to update this prior belief
into posterior belief by using the data, all right?
So how do I do this?
And at some point, I sort of suggested
that there's two layers.
One is where you draw the parameter at random.
And two, once you have the parameter, condition
this parameter, you draw your data.
Nobody believes this actually is happening,
that nature is just rolling dice for us
and choosing parameters at random.
But what's happening is that this idea
that the parameter comes from some random distribution
actually captures very well this idea that how you would
encompass your prior, right?
How would you say my belief is as follows?
Well, here's an example about p.
I'm 90% sure that p is between 0.4 and 0.6.
And I'm 95% sure that p is between 0.3 and 0.8.
OK, so essentially, I have this possible value of p.
And what I know is that there's 90% here between, what did I
say, 0.4 and 0.6.
And then I have 0.3 and 0.8.
And I know that I'm 95% sure that I'm in here.
And this, if you remember, this sort of
looks like the kind of pictures that I
made when I had some Gaussian, right, for example.
And I said, oh, here we have 90% of the observations.
And here we have 95% of the observations.
So in a way, if I were able to tell you
all those ranges for all possible values,
then I would essentially describe a probability
distribution for p.
And what I'm essentially saying is
that p is going to have this kind of shape.
So of course, if I tell you only twice this information
that there's 90% I'm here and I'm here between here and here
and 95% I'm between here and here,
then there's many ways I can accomplish that, right?
I could have something that looks like this, maybe, right?
I could be really, I mean, it could be like this.
I mean, there's many ways I can have this.
Some of them are definitely going to be mathematically
more convenient than others.
And hopefully, we're going to have things
that I can parameterize very well.
Because if I tell you this is this guy,
then there's basically 1, 2, 3, 4, 5, 6, 7 parameters.
So I probably don't want something that has 7 parameters,
but maybe I can say, oh, it's a Gaussian.
And all I have to do is to tell you
where it's centered and what the standard deviation is.
OK, so the idea of using this two-layer thing
where we think of the parameter p as being drawn
from some distribution is really just a way for us
to capture this information, our prior belief being,
well, there's this percentage of chances that it's there.
But the person who has a chance, I'm deliberately not
using probability here.
It's really, right?
So it's really a way to get close to this.
All right, so that's what I said.
The true parameter is not random,
but the Bayesian approach does as if it was random
and then just spits out a procedure out of this thought
process, this thought experiment.
So when you practice Bayesian statistics a lot,
you start getting automatisms.
So you start getting some things that you do
without really thinking about it, just like when you're
a statistician, the first thing you do
is can I think of this data as being Gaussian, for example?
When you're Bayesian, you're thinking about, OK,
I have a set of parameters, right?
So here, I can describe my parameter as being theta
in general in some big space parameter theta.
But what spaces did we encounter?
Well, we encountered the real line.
We encountered the interval 0, 1 for Bernoulli's.
And we encountered maybe some deposit of real line
for exponential distributions, et cetera.
And so what I'm going to need to do, if I want to model some,
if I want to put some prior on those spaces,
I'm going to have to have a usual set of tools for this guy,
usual set of tools for this guy, usual set of tools
for this guy.
And by usual set of tools, I mean,
I'm going to have to have a family of distributions
that's supported on this.
So in particular, this is the speech in which my parameter
that I usually denote by p for Bernoulli lives.
And so what I need is to find a distribution
on the interval 0, 1, just like this guy.
The problem with the Gaussian is that it's not
on the interval 0, 1.
It's going to spill out in the end.
And it's not going to be something that works for me.
And so the question is, I need to think
about distributions that are probably continuous.
Why would I restrict myself to discrete distributions that
are actually convenient?
And for Bernoulli, one that's actually
basically the main tool that everybody's using
is this so-called beta distribution.
So the beta distribution has two parameters.
So x follows a beta with parameters, say a and b,
if it has a density, f of x is equal to x to the a minus 1,
1 minus x to the b minus 1, if x is in the interval 0, 1,
and 0 for all other x's.
So why is that a good thing?
Well, it's a density that's on the interval 0, 1, for sure.
But now I have these two parameters.
And the set of shapes that I can get
by tweaking those two parameters is incredible.
I mean, it's going to be a unimodal distribution.
It's still fairly nice.
It's not going to be something that goes like this and this,
because if you think about this, what would it mean if your prior
distribution on the interval 0, 1 had this shape?
It would mean that maybe you think that p is here,
or maybe you think that p is here,
or maybe you think that p is here,
which essentially mean that you think that p can come maybe
from three different phenomena.
And there's other models that are called mixtures for that
that directly account for the fact
that maybe there are several phenomena that are aggregated in your data set.
But if you think that your data set is sort of pure
and that everything comes from the same phenomenon,
you want something that looks like maybe like this,
or maybe looks like this, or maybe is sort of symmetric.
You want to get all this stuff, right?
Maybe you want something that says, well, if I'm talking about p
being the probability of the proportion of women in the whole world,
you want something that's probably really spiked around 1-1.5,
almost the point mass, because you know,
I mean, OK, let's agree that 0.5 is the actual number.
So you want something maybe that says, OK, maybe I'm wrong,
but I'm sure I'm not going to be really that way off.
And so you want something that's really pointy.
But if it's something you've never checked, right?
And again, I cannot make references at this point,
but something where you might have some uncertainty,
then that should be around 1-1.5.
Maybe you want something that's like a little more,
allows you to say, well, I think there's more around 1-1.5,
but there's still some fluctuations that are possible, OK?
And in particular here, I talk about p
where the two parameters, a and b, are actually the same.
I call them a.
One is called scale, the other one's called shape.
Oh, by the way, sorry, this is not a density,
so it actually has to be normalized, right?
When you integrate this guy, it's going
to be some function that depends on a and b, actually.
Depends on this function through the beta function, right?
Which is this combination of gamma function.
So that's why it's called beta distribution.
But well, that's the definition of the beta function
when you integrate this thing anyway.
So I mean, you just have to normalize it.
It's just a number that depends on the nb, OK?
So here, if you take a equal to b,
you have something that essentially is
symmetric around 1-1.5, right?
Because what does it look like?
Well, it's something.
So my density f of x is going to be what?
It's going to be my constant times x times 1 minus x
to the a minus 1, right?
