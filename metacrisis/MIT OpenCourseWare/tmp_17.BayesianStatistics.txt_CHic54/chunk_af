towards some thetas than to others, what do I do?
Yeah, remove it.
And the way to remove something we multiply by
is just replace it by 1.
That's really what we're doing.
So if this was a constant, then not depending on theta,
then that would mean that we're not preferring any theta.
And we're looking sort of at the likelihood,
but not as a function that we're trying to maximize,
but as a function that we normalize in such a way
that it's actually a distribution.
So if I have pi, which is not here,
this is really just taking the likelihood, which
is a positive function, mean not integrate to 1.
So I normalize it so that it integrates to 1.
And then I just say, well, this is my posterior distribution.
Now, I could just maximize this thing
and spit out my maximum likelihood estimator,
but now I can also integrate and find
what the expectation of this guy is.
I can find what the median of this guy is.
I can sample data from this guy.
I can understand what the variance of this guy is,
which is something we did not do when we just
did maximum likelihood estimation because,
given a function, all we cared about
was the arg max of this function.
So this priors are called uninformative.
So this is just replacing this number by 1.
And if I have a or by a constant, because it still
has to be a density.
And so if I have a bounded set, I'm
just looking for the uniform distribution on this bounded
set, the one that puts constant one
over the size of this thing.
But if I have an unbounded set, what
is the density that takes a constant value
on the entire real line, for example?
What is this density?
Doesn't exist.
I mean, it just doesn't exist.
The way you can think of it is a Gaussian with the variance
going to infinity, maybe, or something like this.
But you can think of it in many ways.
You can think of the limit of the uniform between minus t
and t with t going to infinity.
But this thing is actually 0.
There's nothing there.
And so you can actually still talk about this.
You could always talk about this thing
where you think of this guy as being a constant,
remove this thing from this equation,
and just say, well, my posterior is just
the likelihood divided by the integral of the likelihood
over theta.
And if theta is the entire real line, so be it.
As long as this integral converges,
you can still talk about this stuff.
And so this is what's called an improper prior.
An improper prior is just a non-negative function
defined on theta, but it does not
have to integrate neither to 1 nor to anything.
It does not have to write.
If I integrate the function equal to 1
on the entire real line, what do I get?
Infinity, right?
I mean, it's not a proper integral,
and so it's not a proper prior, and it's
called an improper prior.
And those improper priors are usually
what you see when you start to want non-informative priors
on infinite set status.
I mean, that's just the nature of it.
You should think of it as being the uniform distribution
on some infinite set if that thing were to exist.
So let's see some examples about non-informative priors.
So if I'm on the interval 0, 1, this is a finite set,
so I can talk about the uniform prior on the interval 0, 1
for a parameter p of a Bernoulli, OK?
And so if I want to talk about this,
then it means that my prior is p follows some uniform
on the interval 0, 1, OK?
So it means that the density is, well, f of x is 1
if x is in 0, 1 and 0.
Otherwise, there's actually not even a normalization.
This thing integrates to 1.
And so now, if I look at my likelihood,
it's still the same thing.
So my posterior becomes theta x1, xn.
So that's my posterior.
I don't write the likelihood again because we still have it.
Well, we don't have it there anymore.
Is it here?
Or did I just erase it?
Yeah, the likelihood is given here.
So copy paste over there.
And so the posterior is just this thing times 1.
So you will see it in a second.
So it's p times the sum to the power sum of the xi's,
1 minus p to the power n minus sum of the xi's.
And then it's multiplied by 1 and then
divided by this integral between 0 and 1 of p sum of the xi's,
1 minus p n minus sum of the xi's dp,
which does not depend on p.
And I really don't care what this thing actually is.
So now, sorry, that's prior posterior of p.
And now I can see, well, what is this?
Well, it's actually just the beta with parameters this guy
plus 1 and this guy plus 1.
So I didn't tell you what the expectation of a beta was.
We don't know what the expectation of a beta is.
Agreed?
I mean, if I wanted to find, say, the expectation of this thing,
that would be some good estimator,
we know that the maximum of this guy,
what is the maximum of this thing?
Well, it's just this thing, right?
I mean, it's the average of the xi's, right?
That's just the maximum likelihood estimator for Bernoulli.
We know it's the average.
Do you think if I take the expectation of this thing,
I'm going to get the average?
So actually, I'm not going to get the average.
I'm going to get this guy plus this guy divided by n plus 1.
So I'm going to do as if I had, oh, sorry.
OK, let me not say it like that.
Let's look at what this thing is doing.
It's looking at the number of 0's, the number of 1's,
and it's adding 1.
And this guy is looking at the number of 0's,
and it's adding 1, OK?
Why is it adding this 1?
What's going on here?
Well, what would happen if I had, so this actually
is going to matter mostly when the number of 1's is actually
0 or the number of 0's is 0?
Because what it does is just pushes the 0 from non-zero.
And why is that something that this Bayesian method actually
does for you automatically is because when
we put this non-informative prior on p, which
was uniform on the interval 0, 1, in particular,
we know that the probability that p is equal to 0 is 0,
and the probability that p is equal to 1 is 0.
And so the problem is that, essentially,
if I did not add this 1 with some positive probability,
I would be allowed to spit out something that actually
had p hat, which was equal to 0.
In the case, if by chance, let's say I have n is equal to 3,
and I get only 0, 0, 0, right?
That could happen with probability 1 over p cubed,
sorry, 1 minus p cubed.
Then this thing is just going to not,
that's not something that I want,
and I'm actually using my prior.
So my prior is not informative, but somehow it
captures the fact that I don't want to believe
that p is going to be either equal to 0 or 1.
OK, and so that's sort of taken care of here.
OK, so let's move away a little bit from the Bernoulli example,
shall we?
I mean, we think we've seen enough of it.
And so let's talk about the Gaussian model, right?
Let's say I want to do Gaussian inference in the,
I want to do inference in a Gaussian model using
vision methods.
OK, so I'm going to actually look at, so say, OK,
so what I want is that xi, x1, xn, or say, n, 0, 1, i, i, d,
sorry, theta 1, i, i, d, conditionally on theta.
OK, so that means that pn of x1, xn, given theta,
is equal to, well, exactly what I wrote before.
So 1 square root 2 pi to the n exponential minus 1
half sum of xi minus theta squared.
OK, so that's just the joint distribution
of my n-Gaussians with mean theta.
Another question is, what is the posterior distribution?
OK, well, here I said, let's use the uninformative prior,
which is an improper prior, right?
It puts weight 1 on everyone.
It has the so-called uniform on the entire real line,
so that's certainly not a density.
But I can still just use this, right?
So all I need to do is to get this divided by normalizing
this thing, right?
So that's what I need to do.
But if I look at this, right?
So essentially, I want to understand,
so this is proportional to exponential minus 1
half sum from i equal 1 to n of xi minus theta squared.
And now I want to see this thing as a density not on the xi's,
but on theta, right?
What I want is a density on theta.
So it looks like I have chances of getting something
that looks like a Gaussian.
But if I really need to have a Gaussian,
I would need to see minus 1 half,
and then I would need to see theta minus something here.
Not just the sum of something minus theta's.
So I need to work a little bit more
so I can see what this to expand the square here.
So this thing here is going to be
