right?
I mean, in a way, it's sort of a mixture,
and it's not super nice.
But we'll see that this actually won't matter for us.
This is going to be some number.
It's going to be there, but it won't
matter for us what it is, because it actually
does not depend on the parameter,
and that's all that matters to us.
OK, so let's put some names on those things, right?
I mean, this was very informal, so let's
put some actual names on what we want to call what we call prior.
So what is the formal definition of a prior?
What is the formal definition of a posterior?
And what are the rules to update it, OK?
So I'm going to have my data, which is going to be x1, xn.
And so let's say they're IID, but they don't actually have to.
And so I'm going to have given theta.
And when I say given, it's either given
like I did in the first part of this course
in all previous chapters or conditionally on, right?
So if you're thinking like a Bayesian,
what I really mean is conditionally
on this random parameter, OK?
So it's like as if it was a fixed number.
Then they're going to have the distribution, x1, xn,
is going to have some distribution.
Let's say, let's assume for now it's pdf, pn of x1, xn, OK?
And I'm going to write theta like this.
So for example, what is this?
So let's say this is a pdf.
It could be a pmf.
Everything I say, I'm going to think of them as being pdfs.
I'm going to combine pdfs with pdf,
but I could combine pdf with pmfs, pmf with pdfs,
or pmf with pms, OK?
So everywhere you see a d, it could be an m.
All right, so now I have those things.
So what does that mean?
So here's some example, x1, xn are IID and theta1, right?
So now I know exactly what the joint pdf of this thing is.
So it means that pn of x1, xn given theta is equal to what?
Well, it's like 1 over sigma root, sorry, 1 root 2 pi
to the power n e to the minus sum from i
equal 1 to n of xi minus theta squared divided by 2, right?
So that's just the joint distribution of n iID and theta
1 random variables, OK?
So that's my pn given theta.
Now, this is what we denoted by sort of like f sub theta
before, right?
We had this subscript before, but now we just
put a bar in theta because we want
to remember that this is actually conditioned on theta, right?
But this is just notation.
You should just think of this as being just the usual thing
that you get from some statistical model, all right?
So now, that's going to be pn.
And here, I'm going to assume that theta is,
why do I put pi here?
OK.
So theta has prior distribution pi.
OK, so for example, so think of it as either PDF or PMF again.
So for example, pi of theta was what?
Well, it was some constant times theta to the a minus 1,
1 minus theta to the a minus 1, right?
So it has some prior distribution, and that's another PMF.
So now, I'm given the distribution of my x's given theta.
I'm given the distribution of my theta,
so I'm given this guy, right?
That's this guy.
I'm given that guy, which is my pi, right?
So that's my pn of x1, xn given theta.
That's my pi of theta.
And then I have here just, this is what?
Well, this is just the integral of pn x1, xn times pi of theta d
theta, right?
Overall possible sets of theta.
That's just when I integrate out my theta,
or I compute, say, the marginal distribution,
I get this by integrating, right?
That's just basic probability, conditional probabilities,
right?
Then if I had the PMF, I would just
sum over the values of theta's, OK?
So now, what I want is to find what's called,
so that's the prior distribution.
And I want to find the posterior distribution.
So it's called, it's pi of theta given x1, xn.
And so if I use Bayes' rule, I know
that this is pn of x1, xn given theta times pi of theta.
And then it's divided by the distribution
of those guys, which I will write as integral over theta
of pn x1, xn given theta times pi of theta d theta.
Everybody's with me still?
So if you're not comfortable with this,
it means that you probably need to go read your couple pages
on conditional densities and conditional PMFs
from your probability class.
There's really not much there.
It's just a matter of being able to define those quantities.
F density of x given y, this is just
what's called a conditional density.
You need to understand what this object is
and how it relates to the joint distribution of x and y,
or maybe the distribution of x or the distribution of y.
But it's the same rules.
I mean, one way to actually remember this
is this is exactly the same rules as this.
When you see a bar, it's the same thing
as the probability of this and this guy.
So for densities, it's just a comma divided by the second guy.
The probability of the second guy, that's it.
So if you remember this, you can just
do some pattern matching and see what I just wrote here.
OK?
OK.
So now I can compute every single one of these guys.
This is something I get from my modeling.
So I did not write this.
It's not written in the slides.
But I give a name to this guy that was my prior distribution.
And that was my posterior distribution.
In chapter 3, maybe, what did we call this guy?
Well, the one that does not have a name.
And that's in the box, this guy.
How did we call it?
It is the joint distribution of the x i's.
And we give you the name.
It's the likelihood, right?
This is exactly the likelihood.
This was the likelihood of theta.
And this is something that's very important to remember.
And that really reminds you that these things are really
not that different, maximum likelihood estimation
and Bayesian estimation.
Because your posterior is really just your likelihood
times something that's just putting some weights on the
Thetas, depending on where you think theta should be.
So if I had, say, a maximum likelihood estimator in my
likelihood and theta looked like this.
But my prior in theta looked like this.
I said, oh, I really want Thetas that are like this.
So what's going to happen is that I'm going to turn this
into some posterior that looks like this.
So I'm just really weighting this posterior.
This is a constant that does not depend on theta, right?
Agreed?
I integrated over Thetas, so Thetas go on.
So forget about this guy.
I have basically that the posterior distribution up to
scaling, because it has to be a probability density and not
just any function that's positive, is the product of
this guy.
It's a weighted version of my likelihood.
That's all it is.
I'm just weighting the likelihood using my
prior belief on theta.
And so given this guy, a natural estimator, if you
follow the maximum likelihood principle, would be the
maximum of this posterior.
Agreed?
That would basically be doing exactly what a maximum
likelihood estimation is telling you.
So it turns out that you can.
It's called maximum a posteriori.
And I won't talk much about this or map.
So that's maximum a posteriori.
So it's just the theta hat is the arg max of pi theta given
x1, xn.
It sounds like it's OK.
I give you a density and you say, OK, I have a density for
all values of my parameters.
You're asking me to summarize it into one number.
I'm just going to take the most likely number of those guys.
But you could summarize it otherwise.
You could take the average, right?
You could take the median.
You could take a bunch of numbers.
And the beauty of Bayesian statistics is that you don't
have to take any number in particular.
You have an entire posterior distribution.
This is not only telling you where theta is, but it's
actually telling you the difference if you actually
give as something, it gives you the posterior, right?
So now let's say the theta is a p between 0 and 1.
If my posterior distribution looks like this, or if my
posterior distribution looks like this, then those two
guys have one the same mode, right?
This is the same value.
And they're symmetric, so they also have the same mean.
So these two posterior distributions give me the
same summary into one number.
However, clearly one is much more confident than the other
one, so I might as well just speed that as a solution.
Some people can, you can do even better.
People actually do things such as drawing a random number
from this distribution.
So this is my number.
Well, that's kind of dangerous, but you can imagine you
could do this, right?
