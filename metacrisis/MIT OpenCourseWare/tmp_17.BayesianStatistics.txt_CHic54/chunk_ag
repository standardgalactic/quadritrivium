equal to exponential minus 1 half sum from i equal 1
to n of xi squared minus 2 xi theta plus theta squared.
Ah.
OK, and so now basically what I'm going to do
is everything, remember, is up to this little sign, right?
So every time I see a term that does not depend on theta,
I can just push it in there and just make it disappear.
Agreed?
OK, this term here, exponential minus 1
half sum of xi squared, does it depend on theta?
No, so I'm just pushing it here.
This guy, yes, and the other one, yes.
So this is proportional to exponential xi, sorry,
sum of the xi.
And then I'm going to pull out my theta.
The minus 1 half cancel with the minus 2.
And then I have minus 1 half sum from i equal 1
to n of theta squared, right?
Agreed?
So now what this thing looks like, well,
this looks very much like some theta minus something squared.
This thing here is really just n over 2 times theta.
So sorry, times theta squared.
So now what I need to do is to write this of the form theta
minus something, let's call it mu squared,
maybe divided by 2 sigma squared, right?
I want to turn this into that, maybe up to terms
that do not depend on theta.
That's what I'm going to try to do.
So that's called completing the square,
and that's some exercise you do.
You've done it probably already in the homework,
and that's something you do a lot when
you do Bayesian statistics in particular.
So let's do this.
Well, what is going to be the leading term?
Well, theta squared is going to be multiplied by this thing.
So I'm going to pull out my n over 2,
and then I'm going to write this as theta squared minus theta
minus something squared.
And this something is going to be 1
half of what I see in the cross product, right?
Well, I need to actually pull this thing out.
So let me write it like that first.
So that's theta squared.
And then I'm going to write it as minus 2 times 1 over n,
sum from i equal 1 to n of xi times theta, right?
That's exactly just the rewriting of what we had before.
That's the rewriting of what we had before.
And that should look much more familiar.
x squared minus a squared minus 2 blab a,
and then I missed something.
So this thing I'm going to be able to rewrite as theta minus xn
bar squared.
But then I need to remove the square of xn bar,
because it's not here.
OK?
So I just complete the square.
And then I actually really don't care what this thing actually
was, because it's going to go again
in the little alpha sign over there.
So this thing eventually is going
to be proportional to exponential of minus n over 2 times theta
minus xn bar squared.
And so we know that this is going to be
times theta minus xn bar squared.
And so we know that if this is a density that's
proportional to this guy, it has to be some n with mean
xn bar and variance.
Well, this is supposed to be 1 over sigma squared,
this guy over here, this n.
So that's really just 1 over n, OK?
So the posterior distribution is a Gaussian centered
at the average of my observations and with variance 1 over n.
OK?
Everybody's with me?
So just why I'm saying this, I mean,
this was the output of some computation,
but it sort of makes sense, right?
It's really telling me that the more observations
I have, the more concentrated this posterior is,
concentrated around what?
Well, around this xn bar.
So that looks like something we've sort of seen before,
but it does not have the same meaning somehow.
This is really just the posterior distribution,
and it's not really, I mean, it sort of says,
it's sort of a sanity check that I have this 1 over n when
I have xn bar, but it's not the same thing
as saying that the variance of xn bar was 1 over n
like we had before, OK?
So as an exercise, well, you probably will have it,
but I would recommend if you don't get it,
just try pi of theta to be equal to some n mu 1, OK?
So here, the prior that we use was completely non-informative.
What happens if I take my prior to be some Gaussian, which
is centered at mu, and it has the same variance
as the other guys, OK?
So what's going to happen here is that we're going to put a weight,
and everything that's away from mu
is going to actually get less weight, right?
And I want to know how I'm going to be updating this prior
into a posterior.
So that's right, so everybody sees what I'm saying here.
So pi of theta is just so that means
that pi of theta has the density proportional
to exponential minus 1 half theta minus mu squared, right?
So I need to multiply my posterior with this
and then see what I'd say actually going to be a Gaussian.
This is also a conjugate prior.
It's going to spit out another Gaussian.
You're going to have to complete a square again
and just check what it's actually giving you.
And so spoiler alert, it's going to look
like you get an extra observation, which
is actually equal to mu, OK?
So it's going to be the average of n plus 1 observations,
the first n ones being x1 to xn and the last one being mu.
And it sort of makes sense.
OK, so that's actually a fairly simple exercise.
But before, rather than going into more computation,
this is something you can definitely do
in the comfort of your room, I want
to talk about other types of priors, right?
So the first thing I said is, OK, there's this beta prior
that I just pulled out of my hat and that was just convenient.
Then there was this non-informative prior.
It was convenient, right?
It was non-informative.
So if you don't know anything else,
maybe that's what you want to do.
The question is, are there any other priors
that are sort of principled and generic in the sense
that the uninformative prior was generic, right?
I mean, it was equal to 1.
That's as generic as it gets.
And so is there anything that's generic as well?
Well, there's these priors that are called Jeffery's priors.
And Jeffery's prior is a prior which
is proportional to square root of the determinant
of the Fisher information of theta, OK?
And so this is actually kind of a weird thing to do, right?
It says, compute your, look at your model, right?
Your model is going to have a Fisher information.
Let's say it exists.
And because we know it does not always
exist, for example, in the multinomial model,
we didn't have a Fisher information.
And so the determinant of a matrix
is somehow measuring the size of a matrix, right?
And if you don't trust me, just think
about the matrix being of size one by one,
then the determinant is just the number that you have there.
And so this is really something that
looks like the Fisher information.
I mean, it's just basically the amount of information
is proportional to the amount of information
that you have at a certain point, OK?
And so what my prior is saying is saying, well,
I want to put more weights on those status that
are going to just extract more information from the data, OK?
So you can actually compute those things, right?
So in the first example, Jeffery's prior
is something that looks like this, right?
I mean, in one dimension, Fisher information
is essentially one over the variance, right?
So that's just one over the square root of the variance,
because I have the square root.
And when I have the uniform, sorry,
the Jeffery's prior, when I have the Gaussian case, right?
So this is the identity matrix that I
would have in the Gaussian case.
So the determinant of the identity is one,
so square root of one is one.
And so I would basically get one,
and that gives me my improper prior, my uninformative prior
that I had.
OK, so the uninformative prior one is fine.
I mean, clearly, all the Thetas carry the same information
in the Gaussian model, right?
I mean, whether I translate it here or here,
it's pretty clear that none of them
is actually better than the other.
But clearly, for the Bernoulli case,
the piece that are closer to the boundary
carry more information, right?
So I sort of like those guys because they just
like carry more information.
So what I do is that I take this function, so p1 minus p,
remember, is something that looks like this on the interval
0, 1, 0, and 1.
So this guy, 1 over square root of p1 minus p,
is something that looks like this, agreed?
And so what he's doing is sort of like
wants to push towards the piece that actually
carry more information.
I mean, whether you want to bias your data that way or not
is something you need to think about, right?
I mean, when you put a prior on your data,
on your parameter, you're sort of like biasing
