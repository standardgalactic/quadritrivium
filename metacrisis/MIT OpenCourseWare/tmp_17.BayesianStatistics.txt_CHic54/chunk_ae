All right.
So this is what works.
That's what we went through.
So here, as you notice, I don't care so much about this
part here, right?
Because it does not depend on theta.
So I know that given the product of those two things,
this thing is only the constant that I need to divide so
that when I integrate this thing over theta, it
integrates to one.
Because this has to be a probability density on theta.
So I can write this and just forget about that part, and
that's what's right on the top of this slide.
Just this notation, this sort of weird alpha or, I don't know,
infinity sine crop to the right, whatever you want to call
this, this thing is actually just really emphasizing the
fact that I don't care.
I write it because I can, and you know what it is, but you
don't actually have to, well, in some instances, you have to
compute the integral, in some instances, you don't have to
compute the integral, and a lot of Bayesian computation is
about saying, OK, it's actually really hard to compute this
integral, so I'd rather not doing it.
So let me try to find some methods that allow me to
sample from the posterior distribution without having to
compute this, and that's what's called Monte Carlo Markov
chains, or MCMC, and that's exactly what they're doing.
They're just using only ratios of things like that for
different datas, and which means that if you take ratios, the
normalizing constant is gone, and you don't need to find this
integral.
So we won't go into those details at all.
That would be the purpose of an entire course on Bayesian
inference.
Actually, even Bayesian computations would be an
entire course on its own.
There's some very interesting things that are going on
there, the interface of stats and computation.
All right, so let's go back to our example and see if we
can actually compute any of those things, because it's very
nice to give you some data, some formulas, but let's see if
we can actually do it.
In particular, can I actually recover this claim that the
posterior associated to a beta prior with Bernoulli
likelihood is actually giving me a beta again.
All right, so what was my prior?
Well, it was beta, so P was following a beta AA, which
means that P, the density, so that was pi of theta, well, I
am going to write it as pi of P, was proportional to P to the
A minus 1 times 1 minus P to the A minus 1, right?
So that's the first ingredient I need to
compute my posterior.
I really need only two, if I wanted about up to constant.
The second one was P. Well, we've computed that many
times, and we had even a nice compact way of writing it,
which was that Pn of x1, xn, given a parameter P, right?
So the density, the joint density of my data given P,
that's my likelihood, the likelihood of P, was what?
Well, it was P to the sum of the xis, 1 minus P to the n
minus sum of the xis.
Anybody wants me to parse this more, or do you remember
seeing that from maximum likelihood estimation?
Yeah?
So when you condition on random variables, you really just
treat that random variable with something that's
in the middle.
That's what conditioning does.
OK?
Yeah?
On the previous slide, for the bottom there, it's d pi of T.
Can it be d pi of T, or is it?
So d pi of T is a measure theoretic notation,
which I use without thinking, and I should not,
because I can see it upsets you.
D pi of T is just a natural way to say
that I integrate against whatever
I'm given for the prior of theta.
In particular, if theta is just the mix of a PDF
and a point mass, right?
Maybe I say that my P takes value 0.5 with probability 0.5,
and then is uniform on the interval with probability
0.5.
OK, so for this, I neither have a PDF nor a PMF,
but I can still talk about integrating with respect
to this, right?
It's going to look like if I take a function f of T, d pi of T,
is going to be 1 half of f of 1 half, right?
That's the point mass with probability 1 half at 1 half,
plus 1 half of the integral between 0 and 1 of f of T, d T.
So this is just a notation, which is actually,
funnily enough, is interchangeable with pi of d T.
But if you have a density, it's really just the density pi
of T, d T, if pi is really a density.
But that's when pi is a measure in other density.
But so everybody else forget about this.
I mean, this is not something you should really worry about.
At this point, this is more graduate level probability
classes.
But yeah, it's called measure theory,
and that's when you think of pi as being a measure.
In an abstract fashion, you don't have
to worry whether it's a density or not,
or whether it has a density even.
OK, so everybody's OK with this?
All right, so now I need to compute my posterior.
And as I said, my posterior is really
just the product of the likelihood weighted by the prior.
So hopefully, at this stage of your education,
you can multiply two functions.
So what's happening is if I multiply this guy with this guy,
well, p gets this guy to the power of this guy plus this guy.
And then 1 minus p gets the power n minus sum of xi's.
So this is always from i equal 1 to n,
and then plus a minus 1 as well.
And this is, sorry, this is up to constant,
because I still need to solve this.
And I could try to do it, but I really don't have to,
because I know that if my density has this form,
then it's a beta distribution.
And then I can just go on Wikipedia
and see what should be the normalization factor.
But I know it's going to be a beta distribution.
It's actually the beta with parameter.
So this is really my beta with parameter
sum of xi i equal 1 to n plus a minus 1.
And then the second parameter is n minus sum of the xi's plus a minus 1.
OK?
Sorry.
I just wrote what was here.
Oh, what happened to my 1?
Oh, no, sorry, sorry, sorry.
Beta has the power minus 1, right?
So that's the parameter of the beta.
And this is the parameter of the beta, right?
So beta, well, I don't think it's anywhere.
Yeah, beta is over there, right?
So I just replace a by what I see.
a is just becoming this guy plus this guy,
and this guy plus this guy.
Everybody's comfortable with this computation?
All right, so we just agreed that beta priors for Bernoulli
observations are certainly convenient, right?
And because they're just conjugate,
and we know that's what's going to come out in the end,
that's going to be a beta as well.
So I mean, I just claim it was convenient.
It was certainly convenient to compute this, right?
I mean, there was certainly some compatibility
when I had to multiply this function by that function,
and you can imagine that things could go much more wrong
than just having p to some power and p to some power,
1 minus p to some power, 1 minus p to some power.
Things were nice.
Now, this is nice, but I can also question the following
things.
Why beta, for one?
I mean, the beta tells me something,
but that's convenient, but then how do I pick a?
I know that a should definitely capture
the fact that where I want to have my p most likely located,
but it also actually captures the variance of my beta.
And so choosing different a's is going
to have different functions.
If I have a and b, if I started with the beta with parameter
here, I started with a b here.
I would just pick up the b here, agreed?
And that would just be asymmetric,
but they're going to capture mean and variance of this thing.
And so how do I pick those guys?
I mean, if I'm a doctor and you're
asking me, what do you think the chances of this drug working
on this kind of patients is, and I
have to say to spit out the parameters of a beta for you,
it might be a bit of a complicated thing to do.
So how do you do this, especially for problems?
So by now, people have actually mastered the art of coming up
with how to formulate those numbers,
but in new problems that come up, how do you do this?
What happens if you want to use Bayesian methods,
but you actually do not know what you expect to see?
Maybe this is the first time you've, I mean, to be fair,
before we started this class, I hope all of you
had no idea whether people tended to bend their head
to the right or to the left before kissing,
because if you did, well, you have too much time on your hand
and I should double your homework.
And so in this case, you have to sort of,
maybe you still want to use the Bayesian machinery.
Maybe you just want to do something nice.
It's nice, right?
I mean, it worked out pretty well.
And so what if you want to do well,
you actually want to use some priors that
have no carry, no information that basically do not
prefer any theta to another theta.
Now, you could read this slide or you could look at this formula.
We just said that this pi here was just here
to weigh some thetas more than others,
depending on our prior belief.
If our prior belief does not want to put any preference
