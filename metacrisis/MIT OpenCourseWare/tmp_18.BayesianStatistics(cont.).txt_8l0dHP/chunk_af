So I get A plus A plus n, and then those two guys cancel.
OK, and that's what you have here.
So for A is equal to 1 half, and I
claim that this is Jeffries prior, because remember,
Jeffries was what?
Well, it was square root.
It was proportional to square root of p1 minus p,
which I can write as p to the 1 half, 1 minus p to the 1 half.
So it's just the case A is equal to 1 half.
So if I use Jeffries prior, I just plug in A equals to 1 half,
and this is what I get.
So those things are going to have an impact again
when n is moderately large.
For large n, those things, whether you take Jeffries prior
or you take whatever A you prefer,
it's going to have no impact whatsoever.
But if n is of the order of 10, maybe,
then you're going to start to see some impact depending
on what A you want to pick.
OK, and in the second example, well, here we actually
computed the posterior to be this guy.
Well, here I can just read off what the expectation is.
I mean, I don't have to actually compute
the expectation of a Gaussian.
It's just xn bar.
And so in this case, there's actually no, I mean,
when I have a non-informative prior for a Gaussian,
then I have basically xn bar.
And so as you can see, actually, this
is an interesting example.
When I actually look at the posterior,
it's not something that costs me a lot to communicate to you.
There's one symbol here, one symbol here, and one symbol here.
I tell you the posterior is a Gaussian with mean xn bar
and variance 1 over n.
When I actually turn that into a posterior mean,
I'm dropping all this information.
I'm just giving you the first parameter.
So you can see there's actually much more information
in the posterior than there is in the posterior mean.
The posterior mean is just a point.
It's not telling me how confident I am in this point,
and this thing is actually very interesting.
OK, so you can talk about the posterior variance that's
associated to it.
You can talk about, as an output,
you could give the posterior mean and the posterior variance.
And those things are actually interesting.
All right, so I think this is it.
So as I said, in general, just like in this case,
you do not, the choice of the prior is being,
the impact of the prior is being washed away
as the sample size goes to infinity.
Just while I hear, there's no impact of the prior.
It was a non-informative one, but if you actually
had an informative one, CF homework, yeah.
So CF homework, you would actually
see an impact of the prior, which again would
be washed away as your sample size increases.
Here it goes away.
You just get x in bar over 1.
And actually, in these cases, you
see that the posterior distribution converges to,
sorry, the Bayesian estimator is asymptotically normal.
This is different from the distribution of the posterior.
This is just the posterior mean, which
happens to be asymptotically normal.
But the posterior mean not have a, I mean, here,
the posterior is a beta, right?
I mean, it's not normal.
OK, so there's different, those things
are two different things.
Your question?
What was the prior and the sample distribution?
All one, right?
That was the improper prior.
OK.
And the last thing you did was to make sure that you
Yeah.
Well, I mean, yeah.
So it's essentially telling you that, right?
So we said that when you have an improper,
when you have a non-informative prior,
essentially the maximum likelihood
is the maximum in posterior, right?
But in this case, there's so much symmetry
that it just so happens that the maximum,
I mean, this thing is completely symmetric around its maximum.
So it means that the expectation is equal to the maximum,
to the arc max.
I read somewhere that one of the issues with Bayesian methods
is that you choose the wrong prior to mess up your result.
Yeah, but hence, do not pick the wrong prior.
I mean, of course, it would.
I mean, it would mess up the risk.
Of course, I mean, you're putting extra information.
But you could say the same thing by saying, well,
the issue with frequentist method
is that if you mess up the choice of your likelihood,
then it's going to mess up your output.
So here, you just have two chances of messing it up, right?
You have the, well, it's gone.
So you have the product of the likelihood and the prior.
And you have one more chance to, but it's true.
If you assume that the model is right,
then of course, finding the wrong prior
could completely mess up things.
If your prior, for example, has no support on the true parameter.
But if your prior has a positive weight on the true parameter,
as n goes to infinity, you know, I mean,
OK, I cannot speak for all counter examples in the world.
But I'm sure under minor technical conditions,
you can guarantee that your posterior mean is
going to converge to what you need it to converge to.
Any other question?
All right, so I think this sort of closes
the more traditional mathematical, not mathematical,
but traditional statistics part of this class.
And from here on, we'll talk about more multivariate statistics
starting with principal component analysis.
So that's more like when you have multiple data,
we started in a way to talk about multivariate statistics
when we talked about multivariate regression.
But we'll move on to principal component analysis.
I'll talk a bit about multiple testing.
I haven't made my mind yet about what we'll talk really in December.
But I want to make sure that you have a taste
and the flavor of what is being interesting in statistics
these days, especially as you go towards more machine learning
type of questions where really the focus is on prediction
rather than the modeling itself.
I will talk about logistic regression as well,
for example, which is in generalized linear models,
which is just the generalization in the case
that y does not take value in the whole real line,
maybe 0, 1, for example, for regression.
All right, thanks.
