I know that the probability of theta is equal to 1.
And in this sense, the Bayesian confidence interval
is actually more meaningful.
So one thing I want to actually say
about this Bayesian confidence interval is that it's,
I mean here it's equal to the value 1, right?
So it really encompasses the thing that we want.
But the fact that we actually computed it using the Bayesian
posterior and the Bayesian rule did not really
matter for this argument.
All I just said was that it had a prior.
But just what I want to illustrate
is the fact that we can actually give a meaning
to the probability that theta is equal to 6
given that I see 5 and 7, whereas we cannot really
in the other cases.
And we don't have to be particularly precise in the prior
on theta to be able to give theta this, to give this meaning.
All right, so now as I said, I think the main power
of Bayesian inference is that it spits out
the posterior distribution and not just a single number
like frequentists would give you.
Then we can, say, decorate our theta hat or point estimate
with maybe some confidence interval.
Maybe we can do a bunch of tests.
But at the end of the day, we just
have essentially one number, right?
And maybe we can sort of understand
what the fluctuations of this number
are in a frequentist set up.
But the Bayesian framework is essentially
giving you a natural method.
And you can interpret it in terms
of the probabilities that are associated to the prior.
But you can actually also try to make some, right?
So a Bayesian, if you give me any prior,
you're going to actually build an estimator from this prior,
maybe from the posterior.
And maybe it's going to have some frequentist properties.
And that's what's really nice about Bayesians
is that you can actually try to give
some frequentist properties of Bayesian methods that
are built using Bayesian methodology.
But you cannot really go the other way around.
If I give you a frequentist methodology,
how are you going to say something
about the fact that there's a prior going on, et cetera?
And so this is actually one of the things
there's actually some research that's going on for this.
They call it Bayesian posterior concentration.
And one of the things, so there's something
called the Bernstein von Mises theorem, or class of theorems.
And those are essentially methods that tell you, well,
if I actually run a Bayesian method
and I look at the posterior that I get, right?
It's going to be something like this.
But now I try to study this in a frequentist point of view.
There's actually a true parameter theta somewhere.
The true one.
There's no prior for this guy.
This is just one fixed number.
Is it true that as my sample size is going to go to infinity,
then this thing is going to concentrate around theta?
And the rate of concentration of this thing,
the size of this with the standard deviation of this thing,
is something that should decay, maybe,
like 1 over square root of n or something like this.
And the rate of posterior concentration, when you characterize
it, is called the Bernstein von Mises theorem.
And so people are looking at this in some nonparametric cases.
You can do it in pretty much everything
we've been doing before, right?
You can do it for nonparametric regression estimation
or density estimation.
You can do it for, of course, you
can do it for sparse estimation if you want.
OK, so you can actually compute the posterior and so you
can think of it as being just a method somehow.
Now, the estimator I'm talking about,
so that's just the general Bayesian posterior concentration.
But you can also try to understand
what is the property of something that's
extracted from this posterior.
And one thing that we actually described
was, for example, well, given this guy,
maybe it's a good idea to think about what
the mean of this thing is, right?
So there's going to be some theta hat, which
is just the integral of theta pi theta given x1 xn.
So that's my posterior d theta, right?
So that's the posterior mean.
That's the expected value with respect
to the posterior distribution.
And I want to know, how does this thing behave?
How close it is to a true theta if I actually
am in a frequency set.
So that's the posterior mean.
But this is not the only thing I can actually spit out, right?
This is definitely uniquely defined.
If you give me a distribution, I can actually
spit out its posterior mean.
But I can also think of the posterior median.
But now, if this is not continuous,
you might have some uncertainty.
Maybe the median is not uniquely defined.
And so maybe that's not something you use as much.
Maybe you can actually talk about the posterior mode.
All right, so for example, if your posterior density looks
like this, then maybe you just want
to summarize your posterior with this number.
So clearly, in this case, it's not such a good idea,
because you completely forget about this mode.
But maybe that's what you want to do.
Maybe you want to focus on the most piqued mode.
And this is actually called maximum eposteriori.
As I said, maybe you want to sample from this posterior
distribution.
OK, and so in all these cases, these Bayesian estimators
will depend on the prior distribution.
And the hope is that as the sample size grows,
you won't see that again.
So to conclude, let's just do a couple of experiments.
So if I look at, did we do this?
Yeah, so for example, so let's find
this focus on the posterior mean.
And we know, so remember in experiment 1,
sorry, in example 1, what we had was x1, xn that were iid
Bernoulli p.
And the prior I put on p was a beta with parameter aa.
And if I go back to what we computed,
you can actually compute the posterior of this thing.
And we know that it's actually going to be,
sorry, that was uniform.
So what we get is that the posterior,
this thing is actually going to be a beta with parameter
a plus the sum.
So a plus the number of ones, and a plus the number of zeros.
And the posterior, the beta was just something
that looked like the density was p to the a minus 1, 1 minus p.
So if I want to understand the posterior mean,
I need to be able to compute the expectation of a beta,
and then maybe plug in a for a plus this guy, a minus this guy.
So actually, let me do this.
So what is the expectation?
So what I want is something that looks like the integral
between 0 and 1 of p times a minus 1, sorry, p times p a minus 1,
1 minus p b minus 1.
Do we agree that this, and then there's a normalizing constant?
Let's call it C. So this is what I need to compute.
So that's C of A and B. Do we agree
that this is the posterior mean with respect to a beta
with parameters A and B?
I just integrate p against the density.
So what does this thing look like?
Well, I can actually move this guy in here,
and here I'm going to have a plus 1 minus 1.
So the problem is that this thing is actually,
the constant is going to play a big role,
because this is essentially equal to C a plus 1 b divided
by C a b, where C a plus 1 b is just
a normalizing constant of a beta a plus 1 b.
So I need to know the ratio of those two constants.
And this is not something, I mean,
this is just a calculus exercise.
So in this case, what you get is, sorry,
in this case, you get, well, OK, so we get essentially A
divided by, I think it's a plus b.
Yeah, it's A plus b, right?
So that's this quantity, OK?
And when I plug in A to be this guy and B to be this guy,
what I get is A plus sum of the xi.
And then I get A plus this guy, A plus n minus this guy.
So those two guys go away, and I'm
left with 2A plus n, which does not work.
So yeah, no, that actually works.
And so now what I do, I can actually divide and get
this thing over there.
OK, so what you can see, the reason why this thing has
been divided is that you can really
see that as n goes to infinity, then this thing
behaves like xn bar, which is our frequentist estimator.
The effect of A is actually going away.
The effect of the prior, which is completely captured by A,
is going away as n goes to infinity.
Is there any question?
You guys have a question.
What is it?
Do you have a question?
Yeah, I mean, what is that divided by?
Oh, I do stuff.
I was just thinking about it that way.
Is that divided by what?
That's A over A plus b, and then you need to expand the rate.
Oh, yeah, yeah, then I said that this
is equal to this, right?
So that's for A becomes A plus sum of the xi's,
and b becomes A plus n minus sum of the xi's.
OK, so that's just for the posterior one.
This guy?
2A?
