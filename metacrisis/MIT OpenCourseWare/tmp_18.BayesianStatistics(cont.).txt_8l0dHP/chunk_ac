I mean, Bayesian decisions and Bayesian methods
sort of converge to frequentist methods.
They sort of, when the sample size is large enough,
they sort of lead to the same decisions.
In general, they need not be the same,
but they tend to actually, when the sample size is large enough,
to have the same behavior.
Think about, for example, the posterior
that you have when you have in the Gaussian case.
We said that in the Gaussian case, what you're going to see
is that it's as if you had an extra observation, which
was essentially given by your prior.
And now what's going to happen is that when there's just
one observation among n plus 1, it's really
going to be totally drawn, and you won't
see it when the sample size grows large.
So Bayesian methods are particularly
useful when you have a small sample size.
And when you have a small sample size,
you can actually, the effect of the prior
is going to be bigger.
But most importantly, you're not going
to have to repeat this thing over and again.
You're going to have a meaning.
You're going to have to have something
that has a meaning for this particular data set that you
have.
When I said that the probability that theta belongs to r,
and here I'm going to specify the fact
that it's a Bayesian confidence region like this one,
this is actually conditionally on the data that you've collected.
It says, given this data, given the points that you have,
just put in some numbers if you want in there,
it's actually telling you the probability
that theta belongs to this Bayesian thing,
to this Bayesian confidence region.
Here, since I've conditioned on x1, xn,
this probability is really just with respect to theta drawn
from the prior.
And so now it has a slightly different meaning.
It's just telling you that it's really
making a statement about where the regions of hyperability
of your posterior are.
Now, why is that useful?
Well, there was actually an interesting story
that goes behind Bayesian methods.
Anybody knows the story of the USS Scorpion?
Do you know the story?
So that was an American vessel that disappeared.
I think it was close to Bermuda or something.
But you can tell the story of the Malaysian Airlines,
except that I don't think it's such a successful story.
But the idea was, essentially, we're
trying to find where this thing happened.
And of course, you don't have this is a one-time thing.
You actually have you need something that works once.
You need something that works for this particular vessel.
And you don't care if you go to the Navy
and you tell them, well, here's the method.
And for 95 out of 100 vessels that you're going to lose,
we're going to be able to find it.
And they want this to work for this particular one.
And so they were looking.
And they were diving in different places.
And suddenly, they brought in this guy.
I forget his name.
I mean, there's a whole story about this on Wikipedia.
And he started collecting the data
that they had from different dives and maybe from currents.
And he started to put everything.
And he said, OK, what is the posterior distribution
of the location of the vessel given all the things
that I've seen?
And what have you seen?
Well, you've seen that it's not here.
It's not there.
And it's not there.
And you've also seen that the currents were going that way
and the wind were going that way.
And you can actually put some model
and try to understand this.
Now, given this, for this particular data that you have,
you can actually think of having a two-dimensional density that
tells you where it's more likely than that the vessel is.
And where are you going to be looking for?
Well, if it's a multimodal distribution,
you're just going to go to the highest mode first,
because that's where it's the most likely to be.
And maybe it's not there, so you're just
going to update your posterior based on the fact
that it's not there and do it again.
And actually, after two dives, I think,
he actually found the thing.
And that's exactly where Bayesian statistics
start to kick in, because you put a lot of knowledge
into your model, but you also can actually factor
in a bunch of information.
The model, you had to build a model that was actually
taking into account currents and wind.
And what you can have as a guarantee
is that when you talk about the probability that this vessel
is in this location, given what you've observed in the past,
it actually has some sense.
Whereas if you were to use a frequentist approach,
then there's no probability.
Either it's underneath this position or it's not.
So that's actually where it starts to make sense.
And so you can actually build this.
And there's actually a lot of methods
that are based on for search, that are based on Bayesian methods.
I think, for example, the Higgs boson
was based on a lot of Bayesian methods,
because this is something you need to find once.
I mean, there was a lot of prior that has to be built in.
OK, so now you build this confidence interval.
And the nicest way to do it is to use level sets.
But again, just like for Gaussians, right?
I mean, even in the Gaussian case,
I decided to go at x bar plus or minus something.
But I could go at something that's completely asymmetric.
So what's happening is that here, this method guarantees
that you're going to have the narrowest possible confidence
interval.
That's essentially what it's telling me.
Because every time I'm choosing a point starting from here,
I'm actually putting as much area under the curve as I can.
All right.
So those are called Bayesian confidence interval.
Oh, yeah.
And I promise you that we're going
to work on some example that actually sort of gives
a meaning to what I just told you with actual numbers.
So this is something that's taken from Wasserman's book.
And that's also, it's coming from a paper, from a stats paper
from Walpert and I don't know who from the 80s.
And essentially, this is how it works.
So assume that you have n equals 2 observations.
And you have y1.
So those observations are y1.
Sorry, let's call them x1, which is theta plus epsilon 1
and x2, which is theta plus epsilon 2, where epsilon 1
and epsilon 2 are i, i, d.
And the probability that epsilon i is equal to plus 1
is equal to the probability that epsilon i is equal to minus 1
is equal to 1 half.
So it's just a uniform sign, plus minus 1.
Now, let's think about, so you're
trying to do some inference on theta.
Maybe you actually want to find some inference on theta that's
actually based on, and that's based only on the epsilon x1
and x2.
So I'm going to actually build a confidence interval.
But what I really want to build is a,
but let's start thinking about how I would
find an estimator for those two things.
Well, what values am I going to be getting, right?
So I'm going to get either theta plus 1 or theta minus 1.
And actually, I can get basically
four different observations, right?
Sorry, four different pairs of observations.
Plus, plus, theta minus 1.
Agreed?
Those are the four possible observations that I can get.
Agreed?
Either they're both equal to plus 1,
the both equal to minus 1, or one of the two
is equal to plus 1, the other one to minus 1, for the epsilon.
So those are the four observations I can get.
So in particular, if they take the same value,
I know it's either theta plus 1 or theta minus 1.
And if they take a different value,
I know one of them is theta plus 1,
and one is actually theta minus 1.
So in particular, if I take the average of those two guys
when they take different values, I
know I'm actually getting theta right.
So let's build a confidence region.
So I'm actually going to take a confidence region, which
is just a singleton.
And I'm going to see the following.
Well, if x1 is equal to x2, I'm just
going to take x1 minus 1.
So I'm just saying, well, I'm never
able to resolve whether it's plus 1 or minus 1.
That actually gives me the best one.
So I'm just going to take a dive and say, well, it's just plus 1.
And then if they're different, then here I can do much better.
I'm going to actually just take the average.
OK?
OK?
Now, what I claim is that this is a confidence region.
And by default, when I don't mention it,
this is a frequentist, confidence region.
At level, 75%.
OK, so let's just check that to check that this is correct.
I need to check that the probability under the realization
of x1 and x2 that theta belongs, is one of those two guys,
is actually equal to 0.75.
Yes?
What does it mean to have a frequentist confidence interval?
