me call it a confidence region.
So a Bayesian confidence region, OK?
So it's just a random subspace, so let's call it r,
is included in theta.
And when you had the deterministic one,
we had a definition which was with respect
to the randomness of the data, right?
That's how you actually had a random subset.
You had a random confidence interval.
Here it's actually conditioned on the data,
but with respect to the randomness
that you actually get from your posterior distribution, OK?
So such that the probability that your theta belongs
to this confidence region, given x1, xn,
is say at least 1 minus alpha.
Let's just take it equal to 1 minus alpha, OK?
So that's a confidence region at level 1 minus alpha, OK?
So that's one way.
So when I actually implement Bayesian inference,
I'm actually spinning out an entire distribution.
I need to summarize this thing to communicate it, right?
I cannot just say this is this entire function.
I want to know where are the regions of high probability
where my parameter is supposed to be.
And so here, when I have this thing,
what I actually want to have is something that says,
well, I want to summarize this thing
into some subset of the real line in which I'm
sure that the area under the curve here of my posterior
is actually 1 minus alpha.
And there's many ways to do this, right?
So one way to do this is to look at level sets.
And so rather than actually, so let's say my posterior
looks like this.
I know, for example, if I have a Gaussian distribution,
I can actually take my posterior to be,
my posterior is actually going to be Gaussian.
And what I can do is to try to cut it here on the y-axis
so that now the area under the curve, when I cut here,
is actually 1 minus alpha, OK?
So I have some threshold tau.
If tau goes to plus infinity, then
I'm going to have that this area under the curve here
is going to, well, no.
So the area under the curve when tau is going to plus infinity,
think of the small, when tau is just like right here.
So this is actually going to 0, right?
And so I start here.
And then I start going down and down and down and down
until I actually get something which
is going down to 1 plus alpha.
And if tau is going down to 0, then my area under the curve
is going to, if tau is here, I'm cutting nowhere.
And so I'm getting 1, right?
Agreed?
Think of when tau is very close to 0.
I'm cutting us very far here.
And so I'm getting some area under the curve, which
is almost everything.
And so it's going to 1 as tau, going down to 0.
Yeah?
Does this only work for this picture at all?
No, it does not.
So this picture, so those two things work for all of them,
right?
But when you have a binominal, actually,
this is actually when things start to become interesting,
right?
So when we built a frequentist confidence interval,
it was always of the form x bar plus or minus something.
But now if I start to have a posterior that
looks like this, when I'm going to start cutting off,
I'm going to have two.
I mean, my confidence region is going
to be the union of those two things, right?
And it really reflects the fact that there's
this binominal thing.
It's going to say, well, with hyperbole,
I'm actually going to be either here or here.
Now, the meaning here of a Bayesian confidence region
and a confidence interval are completely distinct notions,
right?
And it's actually, I'm going to work out an example with you
so that we can actually see that sometimes, I mean,
both of them actually can come up with some crazy paradoxes.
So since we don't have that much time,
I will actually talk to you about why, in some instances,
it's actually a good idea to think
of Bayesian confidence intervals rather than frequentist ones.
So before I go into more details about what
those Bayesian confidence intervals are,
let's remind ourselves, what does it
mean to have a frequentist confidence interval, right?
So when I have a frequentist confidence interval,
let's say something like x bar n minus 1.96 sigma over root n
and x bar n plus 1.96 sigma over root n, right?
So that's the confidence interval that you
get for the mean of some Gaussian with known variance
to be equal to sigma squared, OK?
So what we know is that the meaning of this
is the probability that theta belongs to this
is equal to 95%, right?
And this, more generally, you can think of being q alpha over 2.
And what you're going to get is 1 minus alpha here, OK?
So what does it mean here?
Well, it means it looks very much like what we have here,
except that we're not conditioning on x1, xn.
And we should not, because there was a question like that
in the midterm, if I condition on x1, xn,
this probability is either 0 or 1, OK?
Because once I condition, so here, this probability actually
here is with respect to the randomness in x1, xn.
So if I condition, so let's call this thing r frack
for frequentist, well, given x1, xn,
and actually, I don't need to know x1, xn, really.
What I need to know is what xn bar is.
Well, this thing now is what?
It's 1 if theta is in r, and it's 0 if theta is not in r,
right?
That's all there is.
This is a deterministic confidence interval
once I condition on x1, xn.
So I have a number.
The average is maybe 3.
And so I get either theta is between 3 minus 0.5
or in 3 plus 0.5, or it's not.
And so there's basically, I mean,
I write it as probability, but it's really
not a probabilistic statement.
It's either it's true or not, agreed?
So what does it mean to have a frequentist confidence interval?
It means that if I were, so, and here
is where the word frequentist comes from.
It says that if I repeat this experiment over and over,
meaning that on Monday, I collect a sample of size n,
and I build a confidence interval.
And then on Tuesday, I collect another sample of size n,
and I build a confidence interval.
And on Wednesday, I do this again and again.
What's going to happen is the following.
I'm going to have my truth theta that lives here.
And then on Monday, this is the confidence interval
that I build.
So this is the real line.
The truth theta is here, and this is the confidence interval
I build on Monday.
So x bar was here, and this is my confidence interval.
On Tuesday, I build this confidence interval, maybe.
x bar was closer to theta, a bit smaller.
But then on Wednesday, I build this confidence interval.
I'm not here.
It's not in there, and that's this case.
It happens that it's just not in there.
And then on Thursday, I build another one.
I almost miss it, but I'm in there, et cetera.
Maybe here I miss again.
And so what it means to have a confidence interval,
so what does it mean to have a confidence interval at 95%?
Yeah, so it means that if I repeat this,
the frequency of times, hence the word frequentist,
at which I'm actually going to overlap,
that I'm actually going to contain theta, should be 95%.
That's what frequentist means.
So it's just a matter of trusting that,
so on one given thing, one given realization of your data,
it's not telling you anything.
It's there or not.
So it's not really something that's
actually something that assesses the confidence
of your decision, such as theta is in there or not.
It's something that assesses the confidence you have
in the method that you're using.
If you were to repeat it over and again,
it would be the same thing.
It would be 95% of the time correct.
So for example, we know that we could build a test.
So it's pretty clear that you can actually
build a test for whether theta is equal to theta not
or not equal to theta not by just checking
whether theta not is in a confidence interval or not.
And what it means is that if you actually
are doing those tests at 5%, then it
means that 5% of the time, if you do this over and again,
5% of the time, you're going to be wrong.
I mentioned my wife does market research,
and she does maybe, I don't know, 100,000 tests a year.
And if they do all of them at 1%,
then it means that 1% of the time,
which is a lot of time, when you do 100,000 a year,
it's 1,000 of them are actually wrong.
I mean, she's actually hedging against the fact
that 1% of them are going to be wrong.
That's 1,000 of them that are going to be wrong.
Just like if you do this 100,000 times at 95%,
5,000 of those guys are actually not
going to be the correct ones.
So it's kind of scary, but that's the way it is.
So that's what the frequentist interpretation of this is.
Now, as I mentioned, when we started this Bayesian chapter,
I said Bayesian statistics sort of converge to,
