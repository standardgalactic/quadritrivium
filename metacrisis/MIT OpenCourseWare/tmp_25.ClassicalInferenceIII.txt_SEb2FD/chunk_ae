size in your histogram?
How narrow do you take them?
And that depends on how many data you have.
And there's a lot of theory that tells you about the best
way of choosing the bin sizes and the best ways of smoothing
the data that you have.
A completely different topic is in signal processes, the
best topic is in signal processing, you want to do
your inference, not only you want it to be good, but you
also want it to be fast in a computational way.
You get data in real time, lots of data, you want to keep
processing and revising your estimates and your decisions
as data come and go.
Another topic that was sort of briefly touched upon in the
last couple of lectures is that when you set up a model like
a linear regression model, you choose some explanatory
variables and you try to predict y from your x variables.
You have a choice of what to take as your explanatory
variables.
Are there systematic ways of picking the right x variables
to try to estimate a y?
For example, should I try to estimate y on the basis of x
or on the basis of x squared?
How do I decide between the two?
Finally, the range these days has to do with anything big,
high dimensional, complicated models of complicated things,
and tons and tons of data.
So these days, data are generated everywhere.
Amounts of data are humongous.
Also, the problems that people are interested in tend to be
very complicated with lots of parameters.
So I need specially tailored methods that can give you good
results or decent results, even in the face of these huge
amounts of data and possibly with
computational constraints.
So with huge amounts of data, you want methods that are
simple, but still can deliver for you meaningful answers.
Now, as I mentioned some time ago, this whole field of
statistics is very different from the field of probability.
In some sense, all that we're doing in statistics is
probabilistic calculations.
That's what the theory kind of does.
But there is a big element of art.
You saw that we chose the shape of some decision regions or
rejection regions in a somewhat ad hoc way.
There's even more basic things.
How do you organize your data?
How do you think about which hypothesis you would like to
test, and so on?
There's a lot of art that's involved here.
And there's a lot that can go wrong.
So I'm going to close with a note that you can take either a
pessimistic or optimistic.
There is a famous paper that came up out a few years ago
and has been cited about 1,000 times or so.
And the title of the paper is why most published research
findings are false.
And it's actually a very good argument why it feels like
psychology or the medical science and all that.
A lot of what you see published that, yes, this drug has an
effect on that particular disease is actually false
because people do not do their statistics correctly.
There's lots of biases in what people do.
I mean, an obvious bias is that you only publish a result
when you see something.
So the null hypothesis is that the drug doesn't work.
You do your tests, the drug didn't work, OK?
You just go home and cry.
But if by accident that 5% happens, and even though the
drug doesn't work, you've got some outlier data and it
seemed to be working, then you're excited, you publish it.
So that's clearly a bias that gets results to be published
even though they do not have a solid
foundation behind them.
Then there's another thing.
I'm picking my 5%.
So when H0 is true, there's a small probability that the
data will look like an outlier.
And in that case, I publish my result.
OK, it's only 5%.
It's not going to happen too often.
But suppose that I go and do 1,000 different tests.
Test H0 against this hypothesis.
Test H0 against that hypothesis.
Test H0 against that hypothesis.
Some of these tests, just by accident, might turn out to be
in favor of H1.
And again, these are selected to be published.
So if you do lots and lots of tests, and in each one you
have a 5% probability of error, when you consider the
collection of all those tests, actually the probability of
making incorrect inferences is a lot more than 5%.
One basic principle in being systematic about such studies
is that you should first pick your hypothesis that you're
going to test, then get your data, and do your
hypothesis testing.
What would be wrong is to get your data, look at them, and
say, OK, I'm going now to test for these 100 different
hypotheses, and I'm going to choose my hypothesis to be
for features that kind of look abnormal in my data.
Well, given enough data, you can always find some
abnormalities just by chance.
And if you choose to make a statistical test, is this
abnormality present?
Yes, it will be present, because you first found the
abnormality, and then you tested for it.
So that's another way that things can go wrong.
So the moral of this story is that while the world of
probability is really beautiful and solid, you have
your axioms, every question has a unique answer that by now
you can all of you find in a very reliable way.
Statistics is a dirty and difficult business, and that's
why the subject is not over.
And if you're interested in it, it's worth taking follow-on
courses in that direction.
OK, so have good luck in the final, do well, and have a
nice vacation afterwards.
Thank you.
