H0.
Once you find what that distribution is, then you can
solve this usual problem.
I want this probability here to be 5%.
What should my threshold be?
So what does this boil down to?
Finding the distribution of capital T is, in some sense,
a messy, difficult, derived distribution problem.
From this model, we know the distribution
of the capital N i's.
And actually, we can even write down the joint
distribution of the capital N i's.
In fact, we can make an approximation here.
Capital N i is a binomial random variable.
Let's say the number of ones that I got in little n
rolls of my die.
So that's a binomial random variable.
When little n is big, this is going to be
approximately normal.
So we have normal random variables, or
approximately normal, minus a constant.
They're still approximately normal.
We take the squares of these, scale them.
So you can solve a derived distribution problem to find
the distribution of this quantity.
You can do more work, more derived distribution work,
and find the distribution of capital T.
So this is a tedious matter.
But because this test is used quite often, again, people
have done those calculations.
They have found the distribution of capital T. And
it's available in tables.
And you go to those tables.
And you find the appropriate threshold for making a
decision of this type.
OK.
Now to give you a sense of how complicated hypothesis one
might have to deal with, let's make things one level more
complicated.
So here, you can think this x is a discrete random variable.
This is the outcome of my roll.
And I had a model in which the possible values of my
discrete random variable have probabilities all equal to
1, 6.
So my null hypothesis here was a particular PMF for the
random variable capital X.
So another way of phrasing what's happened in this
problem was the question, is my PMF correct?
So this is the PMF of the result of one die roll.
You're asking the question, is my PMF correct?
Make it more complicated.
How about the question of the type, is my PDF correct?
When I have continuous data?
So I have hypothesized that the probability distribution
that I have is, let's say, a particular normal.
I get lots of results from that random variable.
Can I tell whether my results look like normal or not?
What are some ways of going about it?
Well, we saw in the previous slide that there is a
methodology for deciding if your PMF is correct.
So you could take your normal results, the data that you
got from your experiment, and discretize them.
And so now you're dealing with discrete data and use them
previous methodology to solve a discrete problem of the type
is my PDF correct.
So in practice, the way this is done is that you get all
your data, let's say data points of this kind.
You split your space into bins.
And you count how many you have in each bin.
So you get this, and that, and that, and that, and nothing.
So that's a histogram that you get from the data that you
have, like the very familiar histograms that you see after
each one of our quizzes.
So you look at this histogram, and you ask, does it look
like normal?
OK, we need a systematic way of going about it.
If it were normal, you can calculate the probability of
falling in this interval, the probability of falling in that
interval, probability of falling in that interval.
So you would have expected values of how many results or
data points you would have in this interval, and compare
these expected values for each interval with the actual ones
that you observed.
And then take the sum of squares and so on, exactly as in
the previous slide.
And this gives you a way of going about it.
This is a little messy.
It gets hard to do, because you have the difficult decision
of how do you choose the bin size.
If you take your bins to be very narrow, you would get lots
of bins with zeros, and a few bins that only have one
outcome in them, it probably wouldn't feel right.
If you choose your bins to be very wide, then you're
losing a lot of information.
Is there some way of making a test without creating bins?
This is just to illustrate the clever ideas of what
statisticians have thought about.
And here's a really cute way of going about a test, whether
my distribution is correct or not.
Instead of here, we're essentially plotting a PMF,
or an approximation of a PDF, and we ask, does it look like
the PDF we assumed?
Instead of working with PDFs, let's work with cumulative
distribution functions.
So how does this go?
The true normal distribution that I have hypothesized, the
density that I'm hypothesizing, my null hypothesis, has a
certain CDF that I can plot.
So suppose that my hypothesis, H0, is that the x's are
normal with our standard normals.
And I plot the CDF of the standard normal, which is
the sort of continuous-looking curve here.
Now, I get my data, and I plot the empirical CDF.
What's the empirical CDF?
In the empirical CDF, you ask the question, what fraction of
the data fell below 0?
You get a number.
What fraction of my data fell below 1?
I get a number.
What fraction of my data fell below 2?
And so on.
So you're talking about fractions of the data that fell
below each particular number.
And by plotting those fractions as a function of this
number, you get something that looks like a CDF.
And it's the sort of CDF suggested by the data.
Now, the fraction of the data that fell below 0 in my
experiment is, if my hypothesis were true, that
fraction is expected to be 1 half.
1 half is the value of the true CDF.
1 half is the value of the true CDF.
I look at the fraction that I got.
It's expected to be that number.
But there's randomness, so it might be a
little different than that.
For any particular value, the fraction that I got below a
certain number, the fraction of data that were below 2, its
expectation is the probability of falling below 2, which is
the correct CDF.
So if my hypothesis is true, the empirical CDF that I get
based on data should, when n is large, be very close to the
true CDF.
So a way of judging whether my model is correct or not is to
look at the assumed CDF, the CDF under hypothesis H0, look
at the CDF that I constructed based on the data, and see
whether they're close enough or not.
And by close enough, I mean I'm going to look at all the
possible x's and look at the maximum distance between those
two curves.
And I'm going to have a test that decides in favor of H0
if this distance is small, and in favor of H1 if this distance
is large.
That still leaves me the problem of coming up with a
threshold, who exactly do I put my threshold?
Because this test is important enough and is used
frequently, people have made the effort to try to understand
the probability distribution of this quite difficult random
variable.
One needs to do lots of approximations and clever
calculations.
But these have led to values and tabulated values for the
probability distribution of this random variable.
And for example, those tabulated values tell us
that if we want 5% false rejection probability, then our
threshold should be 1.36 divided by the square root of n.
So we know where to put our threshold for this particular
value if we want this particular error or error
probability to occur.
So that's about as hard and sophisticated classical
statistics gets.
You want to have tests for hypotheses that are not so
easy to handle.
People somehow think of clever ways of doing tests of this
kind, how to compare the theoretical predictions with
the observed predictions with the observed data, come up
with some measure of the difference between theory and
data.
And if that difference is big, then you
reject your hypothesis.
Of course, that's not the end of the field of statistics.
There's a lot more.
In some ways, as we kept moving through today's lecture, the
way that we constructed those rejection regions was more and
more ad hoc.
I pulled out of a hat a particular measure of fit
between data and the model.
And I said, let's just use a test based on this.
There are attempts at more or less systematic ways of
coming up with the general shape of rejection regions that
have at least some desirable or favorable
theoretical properties.
Some more specific problems that people study.
Instead of having a test, is this the correct PDF, yes or
no, I just give you data and I ask you, give me a model or a
PDF for those data.
OK, methods of this kind are of many types.
One general method is you form a histogram and then you take
your histogram and plot a smooth line that kind of fits
the histogram.
This still leaves the question of how do you choose the bin
