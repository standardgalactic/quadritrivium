exponential of the sum.
I'll spare you the details.
But this is the form of the likelihood ratio.
And the likelihood ratio test tells us that we should
calculate this quantity after we get your data and compare
with the threshold.
Now you can do some algebra here and simplify.
And by tracing down the inequalities, you're taking
logarithms of both sides and so on.
One comes to the conclusion that using a test that has a
threshold on this ratio is equivalent to calculating
this quantity and comparing it with a threshold.
Basically, this quantity here is monotonic in that quantity.
This being larger than the threshold is equivalent to
this being larger than the threshold.
So this tells us the general structure of a likelihood
ratio test in this particular case.
And it's nice because it tells us that we can make our
decisions by looking at this simple summary of the data.
This quantity, this summary of the data on the basis of
which we make our decision, is called a statistic.
So you take your data, which is a multi-dimensional vector,
and you condense it to a single number.
And then you make a decision on the basis of that number.
So this is the structure of the test.
If I get a large sum of x i's, this is evidence in favor of h
1, because here the mean is larger.
And so I'm going to decide in favor of h 1 or reject h naught
if this sum is bigger than a threshold.
How do I choose my threshold?
Well, I would like to choose my threshold so that the
probability of an incorrect decision, when h naught is
true, the probability of a false rejection, equals to a
certain number alpha, such as, for example, 5%.
So you're given here that this is 5%.
You know the distribution of this random variable.
It's normal.
And you want to find the threshold value that makes
this to be true.
So this is a type of problem that you have seen several
times.
You go to the normal tables, and you figure it out.
So the sum of the x i's has some distribution.
It's normal.
So that's the distribution of the sum of the x i's.
And you want this probability here to be alpha.
For this to happen, what is the threshold value that makes
this to be true?
So you know how to solve problems of these kinds using
the normal tables.
A slightly different example is one in which you have two
normal distributions that have the same mean.
Let's take it to be 0.
But they have a different variance.
So it's sort of natural that here, if your x's that you
see are kind of big on either side, you would choose h1.
If your x's are near 0, then that's evidence for the
smaller variance.
You would choose h0.
So to proceed formally, you again write down the form of
the likelihood ratio.
So again, the density of an x vector under h0 is this one.
It's the product of the densities of each one of the x
i's.
Product of normal densities gives you a product of
exponentials, which is exponential of a sum.
And that's the expression that you get.
Under the other hypothesis, the only thing that changes is
the variance.
And the variance in the normal distribution shows up here in
the denominator of the exponent.
So you put it there.
So this is the general structure of the
likelihood ratio test.
And now you do some algebra.
These terms are constants.
Comparing this ratio to a constant is the same as just
comparing the ratio of the exponentials to a constant.
Then you take logarithms.
You want to compare the logarithm of this thing to a
constant.
You do a little bit of algebra.
And in the end, you find that the structure of the test is
to reject h0 if the sum of the squares of the x i's is bigger
than a threshold.
So by committing to a likelihood ratio test, you are
told that you should be making your decision according to a
rule of this type.
So this fixes the shape or the structure of the decision
region, of the rejection region.
And the only thing that's left, once more, is to pick this
threshold in order to have the property that the probability
of a false rejection is equal to, say, 5%.
So that's the probability that h0 is true, but the sum of the
squares accidentally happens to be bigger than my
threshold, in which case I end up deciding h1.
How do I find the value of x i prime?
Well, what I need to do is to look at a picture more or less
of this kind.
But now I need to look at the distribution of the sum of the
x i's squared.
Actually, the sum of the x i's squared is a non-negative
random variable.
So it's going to have a distribution that's something
like this.
I look at that distribution.
And once more, I want this tail probability to be alpha.
And that determines where my threshold is going to be.
So that's, again, a simple exercise, provided that you
know the distribution of this quantity.
Do you know it?
Well, we don't really know it.
We have not dealt with this particular
distribution in this class.
But in principle, you should be able to find what it is.
It's a derived distribution problem.
You know the distribution of x i.
It's normal.
Therefore, by solving a derived distribution problem, you
can find the distribution of x i squared.
And the x i's squareds are independent of each other,
because the x i's are independent.
So you want to find the distribution of the sum of
random variables with known distributions.
And since they're independent, in principle, you can do
this using the convolution formula.
So in principle, and if you're patient enough, you will be
able to find the distribution of this random variable.
And then you sort of plot it or tabulate it and find where
exactly is the 95th percentile of that distribution.
And that determines your threshold.
So this distribution actually turns out to have a nice and
simple closed form formula.
Because this is a pretty common test.
People have tabulated that distribution.
It's called the chi-squared distribution.
There's tables available for it.
And you look up in the tables.
You find the 95th percentile of the distribution.
And this way, you determine your threshold.
So what's the moral of this story?
The structure of the likelihood ratio test tells you what
kind of decision region you're going to have.
It tells you that for this particular test, you should be
using the sum of the xi-squared as your statistic, as the
basis for making your decision.
And then you need to solve a derived distribution problem
to find the probability distribution of your statistic.
Find the distribution of this quantity under H naught.
And finally, based on that distribution, after you have
derived it, then determine your threshold.
So now let's move on to a somewhat more
complicated situation.
You have a coin.
And you are told that I tried to make a fair coin.
Is it fair?
So you have the hypothesis, which is the default, the null
hypothesis, that the coin is fair.
But maybe it isn't.
So you have the alternative hypothesis that your
coin is not fair.
Now, what's different in this context is that your
alternative hypothesis is not just one specific hypothesis.
Your alternative hypothesis consists of many alternatives.
It includes the hypothesis that P is 0.6.
It includes the hypothesis that P is 0.51.
It includes the hypothesis that P is 0.48, and so on.
So you're testing this hypothesis versus all this
family of alternative hypothesis.
What you will end up doing is essentially the following.
You get some data.
That is, you flip the coin a number of times.
Let's say you flip it 1,000 times.
You observe some outcome.
Let's say you saw 472 heads.
And you ask the question, if this hypothesis is true, is
this value really possible under that hypothesis, or
would it be very much of an outlier?
If it looks like an extreme outlier under this hypothesis,
then I reject it, and I accept the alternative.
If this number turns out to be something within the range
that you would have expected, then you keep or accept
your null hypothesis.
So what does it mean to be an outlier or not?
First, you take your data, and you condense them to a
single number.
So your detailed data actually would have been a sequence
of heads, tails, heads, tails, and all that.
Any reasonable person would tell you that you shouldn't
really care about the exact sequence of heads and tails.
Let's just base our decision on the number of heads that we
have observed.
So you somehow, using some kind of reasoning, which could
be mathematical or intuitive or involving artistry, you
pick a one-dimensional or scalar summary of the data that
you have seen.
In this case, the summary of the data is just the number of
heads.
That's a quite reasonable one.
And so you commit yourself to make a decision on the basis
