of this quantity.
And you ask the quantity that I'm seeing, does it look like
an outlier, or does it look more or less OK?
OK, what does it mean to be an outlier?
You want to choose the shape of this rejection region, but on
the basis of that single number s.
And again, a reasonable thing to do in this context would be
to argue as follows.
If my coin is fair, I expect to see n over 2 heads.
That's the expected value.
If the number of heads I see is far from the expected number
of heads, then I consider this to be an outlier.
So if this number is bigger than some threshold xi, I
consider it to be an outlier.
And then I'm going to reject my hypothesis.
So we picked our statistic.
We picked the general form of how we're going to make our
decision.
And then we picked a certain significance or confidence
level that we want.
Again, this famous 5% number.
And we're going to declare something to be an outlier
if it lies in a region that has 5% or less
probability of occurring.
That is, I'm picking my rejection region so that if h0
is true under the default or null hypothesis, there's only
5% chance that by accident I fall there.
And the thing makes me think that h1 is going to be true.
Now, so now what's left to do is to pick the value of this
threshold.
This is a calculation of the usual kind.
I want to pick my threshold, my xi number, so that the
probability that s is further from the mean by an amount
of xi is less than 5%, or that the probability of being
inside the acceptance region so that the distance from the
default is less than my threshold, I want that to be
95%.
So this is an equality that you can get using the central
limit theorem and the normal tables.
There's 95% probability that the number of heads is going
to be within 31 from the correct mean.
So the way the exercise is done, of course, is that we
start with this number, 5%, which translates to this
number, 95%.
And once we have fixed that number, then you ask the
question, what number should we have here to make this
equality to be true?
It's, again, a problem of this kind.
You have a quantity whose distribution you know.
Why do you know it?
The number of heads by the central limit theorem is
approximately normal.
So this here talks about a normal distribution.
You set your alpha to be 5% and you ask, where should I put
my threshold so that this probability of being out there
is only 5%.
Now, in our particular example, the threshold turned out to
be 31.
This number turned out was just 28 away from the correct
mean.
So this distance was less than the threshold.
So we end up not rejecting H0.
So we have our rejection region.
The way we designed it is that when H0 is true, there's
only a small chance, 5%, that we get data out there, data
that we would call an outlier.
If we see such an outlier, we reject H0.
If what we see is not an outlier, as in this case,
where that distance turned out to be kind of small, then we
do not reject H0.
An interesting little piece of language here.
People generally prefer to use this terminology to say that
H0 is not rejected by the data, instead of saying that H0
is accepted.
In some sense, they're both saying the same thing, but
the difference is sort of subtle.
When I say not rejected, what I mean is that I got some data
that are compatible with my hypothesis.
That is, the data that I got do not falsify the hypothesis
that I had, my null hypothesis.
So my null hypothesis is still alive and may be true.
But from data, you can never really prove that the
hypothesis is correct.
Perhaps my coin is not fair in some other complicated way.
Perhaps I was just lucky.
And even though my coin is not fair, I ended up with an
outcome that suggests that it's fair.
Perhaps my coin flips are not independent, as I assumed in
my model.
So there's many ways that my hypothesis could be wrong.
My null hypothesis could be wrong.
And still, I got data that tell me that my hypothesis is OK.
So this is sort of the general way that things work in science.
One comes up with a model or a theory.
This is the default theory.
And we sort of work with that theory trying to find whether
there are examples that violate the theory.
If you find data and examples that violate the theory, your
theory is falsified, and you need to look for a new one.
But when you have your theory, really no amount of data can
prove that your theory is correct.
So we have the default theory that the speed of light is
constant.
As long as we do not find any data that are
run counter to it, we stay with that theory.
But there's no way of really proving this, no matter how
many experiments we do.
But there could be experiments that falsify that theory, in
which case we need to look for a new one.
So there's a bit of an asymmetry here in how we treat the
alternative hypothesis.
H0 is sort of the default, which we'll accept until we see
some evidence to the contrary.
And if we see some evidence to the contrary, we reject it.
As long as we do not see evidence to the contrary, then
we keep working with it.
But always take it with a grain of salt.
You can never really prove that a coin has a bias exactly
equal to 1 half.
Maybe the bias is equal to 0.5001.
So the bias is not 1 half.
But with an experiment with 1,000 coin tosses, you
wouldn't be able to see this effect.
OK.
So that's how you go about testing about whether your
coin is fair.
You can also think about testing whether a die is fair.
So for a die, the null hypothesis would be that
every possible result when you roll the die has equal
probability and equal to 1 sixth.
And you also make the hypothesis that your die
rolls are statistically independent.
Die rolls are statistically independent from each other.
So I take my die, I roll it a number of times, little n.
And I count how many ones I got, how many twos I got, how
many threes I got.
And these are my data.
I count how many times I observed a specific result in
my die roll that was equal to some i.
And now I ask the question, the n i's that I observed are
they compatible with my hypothesis or not?
What does compatible to my hypothesis mean?
Under the null hypothesis, n i should be approximately equal
or is equal in expectation to n times little p i.
And in our example, this little p i is, of course, 1 sixth.
So if my die is fair, the number of ones I expect to see
is equal to the number of rolls times 1 sixth.
The number of twos I expect to see is again that same number.
Of course, there's randomness, so I do not expect to get
exactly that number.
But I can ask how far away from the expected values was i.
If my capital n i's turn to be very different from n over 6,
this is evidence that my die is not fair.
If those numbers turn out to be close to n times 1 sixth,
then I'm going to say there's no evidence that would lead me
to reject this hypothesis.
So this hypothesis remains alive.
So someone has come up with this thought that maybe the
right statistic to use or the right way of quantifying how
far away are the n i's from their mean is to look at this
quantity.
So I'm looking at the expected value of n i under the null
hypothesis.
See what I got.
Take the square of this and add it over all i's.
But also throw in this term in the denominator.
And why that term is there, that's a longer story.
One can write down certain likelihood ratios, do certain
Taylor series approximations, and there's a heuristic
argument that justifies why this would be a good form for
the test to use.
So there's a certain art that's involved in this step that
some people somehow decided that it's a reasonable thing to
do, to calculate once you get your results.
To calculate this one dimensional summary of your
result, this is going to be your statistic.
And compare that statistic to a threshold.
And that's how you make your decision.
So by this point, we have fixed the type of the rejection
region that we're going to have.
So we've chosen the qualitative structure of our test.
And the only thing that's now left is to choose the particular
threshold we're going to use.
And the recipe, once more, is the same.
We want to set our threshold so that the probability of a
false rejection is 5%.
We want the probability that our data fall in here is only
5% when the null hypothesis is true.
So that's the same as setting our threshold xi so that the
probability that our test statistic is bigger than that
threshold, we want that probability to be only 0.05.
So to solve a problem of this kind, what is it that you
need to do?
You need to find the probability distribution of
capital T.
So once more, it's the same picture.
You need to do some calculations, some sort, and
come up with the distribution of the random variable T, where
T is defined this way.
You want to find this distribution under hypothesis
