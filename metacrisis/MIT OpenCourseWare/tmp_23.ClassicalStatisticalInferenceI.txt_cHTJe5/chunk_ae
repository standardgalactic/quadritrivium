bigger confidence intervals than necessary.
So that's one option.
Another option is to try to estimate sigma from the data.
How do you do this estimation?
In special cases for special types of distributions, you
can think of heuristic ways of doing this estimation.
For example, in the case of Bernoulli random variables, we
know that the true value of sigma, the standard deviation of
a Bernoulli random variable, is the square root of theta 1
minus theta, where theta is the mean of the Bernoulli.
Try to use this formula.
But theta is the thing we're trying to estimate in the
first place.
We don't know it.
What do we do?
Well, we have an estimate for theta, the estimate produced
by our estimation procedure, the sample mean.
So I obtain my data.
I get my data.
I produce the estimate theta hat.
It's an estimate of the mean.
Use that estimate in this formula to come up with an
estimate of my standard deviation.
And then use that standard deviation in the construction
of the confidence interval, pretending that this is correct.
Well, if the number of your data is large, then we know from
the law of large numbers that theta hat is a pretty good
estimate of theta.
So sigma hat is going to be a pretty good estimate of sigma.
So we're not making large errors by using this approach.
So in this scenario here, things were simple.
Because we had an analytical formula, sigma was determined
by theta.
So we could come up with a quick and dirty estimate of sigma.
In general, if you do not have any nice formulas of this
kind, what could you do?
Well, you still need to come up with an estimate of sigma
somehow.
What is a generic method for estimating a standard
deviation?
Equivalently, what could be a generic method for
estimating a variance?
Well, the variance is an expected value of some random
variable.
The variance is the mean of the random variable inside those
brackets.
How does one estimate the mean of some random variable?
You obtain lots of measurements of that random
variable and average them out.
So this would be a reasonable way of estimating the variance
of a distribution.
And again, the weak law of large numbers tells us that this
average converges to the expected value of this, which is
just the variance of the distribution.
So we got a nice and consistent way of estimating
variances.
But now we seem to be getting in a vicious circle here.
Because to estimate the variance, we need to know the
mean.
And the mean is something we were trying to estimate in the
first place.
But we do have an estimate from the mean.
So a reasonable approximation, once more, is to
plug in here, since we don't know the mean, to plug in the
estimate of the mean.
And so you get that expression, but with a theta hat
instead of theta itself.
And this is another reasonable way of estimating the variance.
It does have the same consistency properties.
Why?
When n is large, this is going to behave the same as that,
because theta hat converges to theta.
And when n is large, this is approximately the same as
sigma squared.
So for large n, this quantity also
converges to sigma squared.
And we have a consistent estimate of the variance as
well.
And we can take that consistent estimate and use it back in
the construction of confidence interval.
One little detail.
Here we were dividing by n.
Here we're dividing by n minus 1.
Why do we do this?
Well, it turns out that's what you need to do for this
estimate to be an unbiased estimate of the variance.
One has to do a little bit of a calculation.
And one finds that that's the factor that you need to have
here in order to be unbiased.
Of course, if you get 100 data points, whether you divide by
100 or divide by 99, it's going to make only a tiny
difference in your estimate of your variance.
So it's going to make only a tiny difference in your
estimate of the standard deviation.
It's not a big deal, and it doesn't really matter.
But if you want to sort of show off about your deeper
knowledge of statistics, you throw in the 1 over n minus
1 factor in there.
So one basically needs to put together this story here, how
you estimate the variance.
You first estimate the sample mean, and then you do some
extra work to come up with a reasonable estimate of the
variance and the standard deviation.
And then you use your estimate of the standard
deviation to come up with a confidence interval, which has
these two end points.
In doing this procedure, there's basically a number of
approximations that are involved.
There are two types of approximations.
One approximation is that we're pretending that the
sample mean has a normal distribution, that's something
we're justified to do by the central limit theorem.
But it's not exact, it's an approximation.
And the second approximation that comes in is that instead
of using the correct standard deviation, in general, you
will have to use some approximation
of the standard deviation.
OK, so you will be getting a little bit of practice with
these concepts in recitation and tutorial.
And we will move on to new topics next week.
But the material that's going to be covered in the final
exam is only up to this point.
So next week is just general education, hopefully useful,
but not in the exam.
