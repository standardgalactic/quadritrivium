You look at the variance of y, which
is something that you can estimate from data.
This is how much randomness there is in y.
And compare it with the randomness
that you have in y, but conditions on x.
So this quantity tells me, if I knew x,
how much randomness would there still be in my y?
So if I know x, I have more information.
So y is more constrained.
There's less randomness in y.
This is the randomness in y if I don't know anything about x.
So naturally, this quantity would be less than 1.
And if this quantity is small, it
would mean that whenever I know x,
then y is very well known, which essentially tells me
that knowing x allows me to make very good predictions about y.
Knowing x means that I'm explaining away
most of the randomness in y.
So if you read a statistical study that
uses linear regression, you might encounter statements
of the form, 60% of students GPA is explained
by the family income.
If you read the statements of this kind,
it really refers to quantities of this kind.
Out of the total variance in y, how much variance
is left after we build our model?
So if only 40% of the variance of y
is left after we build our model,
that means that x explains 60% of the variation in y.
So the idea is that randomness in y
is caused by multiple sources, our explanatory variable,
and random noise.
And we ask the question, what percentage
of the total randomness in y is explained
by variations in the x parameter?
And how much of the total randomness in y
is attributed just to random effects?
So if you have a model that explains most of the variation
in y, then you can think that you
have a good model that tells you something useful
about the real world.
Now, there's lots of things that can go wrong
when you use linear regression, and there's many pitfalls.
One pitfall happens when you have this situation that's
called heteroscedasticity.
So suppose your data are of this kind.
So what's happening here?
You seem to have a linear model,
but when x is small, you have a very good model.
So this means that w has small variance when x is here.
On the other hand, when x is there,
you have a lot of randomness.
This would be a situation in which the w's
are not identically distributed, but the variance
of the w's of the noise has something to do with the x's.
So for different regions of our x space,
we have different amounts of noise.
What will go wrong in this situation?
Since we're trying to minimize sum of squared errors,
we're really paying attention to the biggest errors, which
will mean that we are going to pay attention to these data
points, because that's where the big errors are going to be.
So the linear regression formulas
will end up building a model based on these data, which
are the most noisy ones, instead of those data
that are nicely stacked in order.
Clearly, that's not the right thing to do.
So you need to change something and use
the fact that the variance of w changes with the x's.
And there are ways of dealing with it.
It's something that one needs to be careful about.
Another possibility of getting into trouble
is if you're using multiple explanatory variables that
are very closely related to each other.
So for example, suppose that I try
to predict your GPA by looking at your SAT the first time
that you took it, plus your SAT the second time
that you took your SATs.
I'm assuming that almost everyone takes the SAT more than once.
So suppose that you had a model of this kind.
Well, SAT in your first try and SAT in your second try
are very likely to be fairly close.
And you could think of coming up with estimates
in which this is ignored, and you build a model based on this,
or an alternative model in which this term is ignored,
and you make predictions based on the second SAT.
And both models are likely to be essentially as good
as the other one, because these two quantities are essentially
the same.
So in that case, your Thetas that you estimate
are going to be very sensitive in little details of the data.
You change your data.
You have your data, and your data tell you
that this coefficient is big and that coefficient is small.
You change your data just a tiny bit,
and your Thetas would drastically change.
So this is a case in which you have multiple explanatory
variables, but they're redundant in the sense
that they're very closely related to each other
and perhaps with a linear relation.
So one must be careful about the situation
and do special tests to make sure that this doesn't happen.
Finally, the biggest and most common blunder
is that you run your linear regression,
you get your linear model, and then you say, oh, OK,
y is caused by x according to this particular formula.
Well, all that we did was to identify a linear relation
between x and y.
This doesn't tell us anything, whether it's y that causes x
or whether it's x that causes y, or maybe both x and y
are caused by some other variable that we didn't think about.
So building a good linear model that has small errors
does not tell us anything about causal relations
between the two variables.
It only tells us that there's a close association
between the two variables.
If you know one, you can make predictions about the other,
but it doesn't tell you anything about the underlying physics
that there's some physical mechanism that
introduces the relation between those variables.
OK, that's it about linear regression.
Let us start the next topic, which is hypothesis testing.
And we're going to continue with it next time.
So here, instead of trying to estimate continuous parameters,
we have two alternative hypotheses
about the distribution of the x random variable.
So for example, a random variable
could be either distributed according to this distribution
under H0, or it might be distributed
according to this distribution under H1.
And we want to make a decision which distribution
is the correct one.
So we're given those two distributions,
and some common terminologies that one of them
is the null hypothesis, sort of the default hypothesis.
And we have some alternative hypothesis.
And we want to check whether this one is true,
or that one is true.
So you obtain a data point, and you want to make a decision.
In this picture, what would a reasonable person
do to make a decision?
They would probably choose a certain threshold, psi,
and decide that H1 is true if your data falls in this interval,
and decide that H0 is true if you fall on this side.
So that would be a reasonable way of approaching the problem.
More generally, you take the set of all possible x's,
and you divide the set of possible x's into two regions.
One is the rejection region in which you decide H1,
or you reject H0.
And the complement of that region is where you decide H0.
So this is the x space of your data.
In this example here, x was one-dimensional,
but in general, x is going to be a vector.
All the possible data vectors that you can get,
they're divided into two types.
If it falls in this set, you'd make one decision.
If you falls in that set, you make the other decision.
So how would you characterize the performance
of a particular way of making a decision?
Suppose I chose my threshold.
I may make mistakes of two possible types.
Perhaps H0 is true, but my data happens to fall here,
in which case I make a mistake.
And this would be a false rejection of H0.
If my data falls here, I reject H0.
I decide H1, whereas H0 was true.
The probability of this happening, let's call it alpha.
But there is another kind of error that can be made.
Suppose that H1 was true, but by accident,
my data happens to fall on that side.
Then I'm going to make an error again.
I'm going to decide H0, even though H1 was true.
How likely is this to occur?
This would be the area under this curve here.
And that's the other type of error that can be made.
And beta is the probability of this particular type of error.
Both of these are errors.
Alpha is the probability of error of one kind,
beta the probability of an error of the other kind.
You would like the probabilities of error to be small.
So you would like to make both alpha and beta
as small as possible.
Unfortunately, that's not possible.
There's a trade-off.
If I move my threshold this way, then alpha becomes smaller,
but beta becomes bigger.
So there's a trade-off.
If I make my rejection region smaller,
one kind of error is less likely.
But the other kind of error becomes more likely.
So we got this trade-off.
So what do we do about it?
How do we move systematically?
How do we come up with rejection regions?
Well, what the theory basically tells you
is it tells you how you should create those regions,
but it doesn't tell you exactly how.
It tells you the general shape of those regions.
For example, in this here, the theory
who tells us that the right thing to do
