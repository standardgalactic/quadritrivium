on the basis of the data, and then
uses these estimates to come up with an estimate of theta 1.
So this gives us a probabilistic interpretation
of the formulas that we have for the way
that the estimates are constructed.
If you are willing to assume that this
is the true model of the world, the structure
of the true model of the world, except that you do not
know means and covariances and variances,
then this is a natural way of estimating
those unknown parameters.
All right, so we have a closed form formula.
We can apply it whenever we have data.
Now, linear regression is a subject
on which there are whole courses and whole books
that are given.
And the reason for that is that there's
a lot more that you can bring into the topic
and many ways that you can elaborate
on the simple solution that we got
for the case of two parameters and only two random variables.
So let me give you a little bit of a flavor of what
are the topics that come up when you start
looking into linear regression in more depth.
So in our previous, in our discussion so far,
we made a linear model in which we
are trying to explain the values of one variable
in terms of the values of another variable.
We're trying to explain GPAs in terms of SAT scores,
or we're trying to predict GPAs in terms of SAT scores.
But maybe your GPA is affected by several factors.
For example, maybe your GPA is affected by your SAT score,
also the income of your family, the years of education
of your grandmother, and many other factors like that.
So you might write down a model in which I believe
that GPA has a relation, which is a linear function
of all these other variables that I mentioned.
So perhaps you have a theory of what determines performance
at college, and you want to build a model of that type.
How do we go about in this case?
Well, again, we collect data points.
We look at the Ith student who has a college GPA.
We record their SAT score, their family income,
and grandmother's years of education.
So this is one data point that is for one particular student.
We postulate the model of this form.
For the Ith student, this would be the mistake
that our model makes if we have chosen specific values
for those parameters.
And then we go and choose the parameters that
are going to give us, again, the smallest possible sum
of squared errors.
So philosophically, it's exactly the same as what
we were discussing before, except that now we're
including multiple explanatory variables in our model
instead of a single explanatory variable.
So that's the formulation.
What do you do next?
Well, to do this minimization, you're
going to take derivatives.
Once you have your data, you have a function
of these three parameters.
You take the derivative with respect to the parameters,
set the derivative equal to 0.
You get a system of linear equations.
You throw that system of linear equations to the computer.
And you get numerical values for the optimal parameters.
There are no nice closed formulas of the type
that we had in the previous slide when you're dealing
with multiple variables, unless you're
willing to go into metrics notation.
In that case, you can, again, write down closed formulas,
but they will be a little less intuitive than what
we had before.
But the moral of the story is that numerically,
this is a procedure that's very easy.
It's a problem, an optimization problem,
that the computer can solve for you.
And it can solve it for you very quickly,
because all that it involves is solving a system
of linear equations.
Now, when you choose your explanatory variables,
you may have some choices.
One person may think that your GPA has something
to do with your SAT score.
Some other person may think that your GPA
has something to do with the square of your SAT score.
And that other person may want to try
to build a model of this kind.
When would you want to do this?
Suppose that the data that you have look like this.
If the data look like this, then you
might be tempted to say where a linear model does not look
right, but maybe a quadratic model
will give me a better fit for the data.
So if you want to fit a quadratic model to the data,
then what you do is you take x squared
as your explanatory variable instead of x,
and you build a model of this kind.
There's nothing really different in models of this kind
compared to models of that kind.
They are still linear models because we
have theta's showing up in a linear fashion.
What you take as your explanatory variables,
whether it's x, whether it's x squared,
or whether it's some other function
that you chose, some general function h of x,
doesn't make a difference.
So think of your h of x as being your new x.
So you can formulate the problem exactly the same way,
except that instead of using x's, you choose h of x's.
So it's basically a question.
Do I want to build a model that explains
y's based on the values of x, or do I
want to build a model that explains y's
on the basis of the values of h of x, which
is the right value to use?
And with this picture here, we see
that it can make a difference.
A linear model in x might be a poor fit,
but a quadratic model might give us a better fit.
So this brings to the topic of how
to choose your functions h of x if you're
dealing with a real world problem.
So in a real world problem, you're just given x's and y's.
And you have the freedom of building models
of any kind you want.
You have the freedom of choosing a function h
of x of any type that you want.
So this turns out to be a quite difficult and tricky topic
because you may be tempted to overdo it.
For example, I got my 10 data points.
And I could say, OK, I'm going to choose an h of x.
I'm going to choose h of x and actually multiple h's of x
to do multiple linear regression, in which
I'm going to build a model that uses a 10th degree polynomial.
If I choose to fit my data with a 10th degree polynomial,
I'm going to fit my data perfectly,
but I may obtain a model that does something like this
and goes through all my data points.
So I can make my prediction errors extremely small
if I use lots of parameters and if I choose my h functions
appropriately.
But clearly, this would be garbage.
If you get those data points and you say,
here's my model that explains them,
that has a polynomial that keeps going up and down,
then you're probably doing something wrong.
So choosing how complicated those functions the h's should be
and how many explanatory variables to use
is a very delicate and deep topic
on which there's deep theory that tells you what you should do
and what you shouldn't do.
But the main thing that one should avoid doing
is having too many parameters in your model
when you have too few data.
So if you only have 10 data points,
you shouldn't have 10 free parameters.
With 10 free parameters, you will be able to fit your data
perfectly, but you wouldn't be able to really rely on the results
that you're seeing.
OK.
Now, in practice, when people run linear regressions,
they do not just give points estimates
for the parameters theta.
But similar to what we did for the case of estimating
the mean of a random variable, you
might want to give confidence intervals.
That sort of tells you how much randomness there
is when you estimate each one of the particular parameters.
There are formulas for building confidence intervals
for the estimates of the status.
We're not going to look at them.
It would take too much time.
Also, you might want to estimate the variance in the noise
that you have in your model.
That is, if you are pretending that your true model is
of the kind we were discussing before, namely y equals theta
plus theta 1x plus w, and w has a variance sigma squared,
you might want to estimate this because it tells you
something about the model.
And this is called the standard error.
It puts a limit on how good predictions your model can make.
Even if you have the correct theta 0 and theta 1,
and somebody tells you x, you can make a prediction about y.
But that prediction will not be accurate
because there is this additional randomness.
And if that additional randomness is big,
then your predictions will also have a substantial error in them.
There's another quantity that gets reported usually.
This is part of the computer output
that you get when you use a statistical package, which
is called r squared.
And it's a measure of the explanatory power
of the model that you have built using linear regression.
Instead of defining r squared exactly,
let me give you a sort of analogous quantity
that's involved.
After you do your linear regression,
you can look at the following quantity.
