And here's how we go about it.
Suppose we try some particular values of theta0 and theta1.
These give us a certain line.
Given that line, we can make predictions.
For a student who had this x, the model that we have would
predict that y would be this value.
The actual y is something else.
And so this quantity is the error that our model would
make in predicting the y of that particular student.
We would like to choose a line for which the predictions are
as good as possible.
And what do we mean by as good as possible?
As our criterion, we're going to take the following.
We're going to look at the prediction error that our
model makes for each particular student.
Take the square of that.
And then add them up over all of our data points.
So what we're looking at is the sum of this quantity squared,
that quantity squared, that quantity squared, and so on.
We add all these squares.
And we would like to find the line for which the sum of these
squared prediction errors are as small as possible.
So that's the procedure.
We have our data, the x's and the y's.
And we're going to find theta's, the best model of this
type, the best possible model by minimizing this sum of
squared errors.
So that's a method that one could pull out of the hat and
say, OK, that's how I'm going to build my model.
And it sounds pretty reasonable.
And it sounds pretty reasonable even if you don't know
anything about probability.
But does it have some probabilistic justification?
It turns out that, yes, you can motivate this method with
probabilistic considerations under certain assumptions.
So let's make a probabilistic model that's going to lead us
to this particular way of estimating the parameters.
So here's a probabilistic model.
I pick a student who had a specific SAT score, and that
could be done at random, but also could be done in a
systematic way.
That is, I pick a student who had an SAT of 600, a student
of 610, all the way to 1400 or 1600, whatever the right
number is.
I pick all those students.
And I assume that for a student of this kind, there's a
true model that tells me that their GPA is going to be a
random variable, which is something predicted by their
SAT score, plus some randomness, some random noise.
And I model that random noise by independent normal random
variables with zero mean and a certain variance.
So this is a specific probabilistic model.
And now I can think about doing maximum likelihood
estimation for this particular model.
So to do maximum likelihood estimation here, I need to
write down the likelihood of the y's that I have observed.
What's the likelihood of the y's that I have observed?
Well, a particular w has the likelihood of the form e to
the minus w squared over 2 sigma squared.
That's the likelihood of a particular w.
The probability, you're the likelihood of observing a
particular value of y.
That's the same as the likelihood that w takes a
value of y minus this minus that.
So the likelihood of the y's is of this form.
Think of this as just being the wi squared.
So this is the dense, and if we have multiple data, you
multiply the likelihoods of the different y's.
So you have to write something like this.
Since the w's are independent, that means that the y's are
also independent, the likelihood of a y vector is
the product of the likelihoods of the individual y's.
The likelihood of every individual y is of this
form, where w is yi minus these two quantities.
So this is the form that the likelihood function is going
to take under this particular model.
And under the maximum likelihood methodology, we want
to maximize this quantity with respect to theta0 and theta1.
Now to do this maximization, you might as well consider the
logarithm and maximize the logarithm, which is just the
exponent up here.
Maximizing this exponent because we have a minus sign is
the same as minimizing the exponent without the minus sign.
Sigma squared is a constant, so what you end up doing is
minimizing this quantity here, which is the same as what we
had in our linear regression method.
So conclusion, you might choose to do linear regression in
this particular way, just because it looks reasonable or
plausible, or you might interpret what you're doing
as maximum likelihood estimation in which you
assume a model of this kind where the noise terms are
normal random variables with the same distribution
independent identically distributed.
So linear regression implicitly makes an assumption of
this kind.
It's doing maximum likelihood estimation as if the world was
really described by a model of this form and with w's
being random variables.
So this gives us at least some justification that this
particular approach to fitting lines to data is not so
arbitrary, but it has a sound footing.
OK, so then once you accept this formulation as being a
reasonable one, what's the next step?
The next step is to see how to carry out this minimization.
This is not a very difficult minimization to do.
The way it's done is by setting the derivatives of this
expression to 0.
Now, because this is a quadratic function of theta 0 and
theta 1, when you take the derivatives with respect to
theta 0 and theta 1, you get linear functions of theta 0
and theta 1, and you end up solving a system of linear
equations in theta 0 and theta 1.
And it turns out that there's a very nice and simple formulas
for the optimal estimates of the parameters
in terms of the data.
And the formulas are these ones.
I said that these are nice and simple formulas.
Let's see why.
How can we interpret them?
Suppose that the world is described by a model of this
kind, where the x's and y's are random variable, and where
w is a noise term that's independent of x.
So we're assuming that a linear model is indeed true, but
not exactly true.
There's always some noise associated with any particular
data point that we obtain.
So if a model of this kind is true, and the w's have zero
mean, then we have that the expected value of y would be
theta 0 plus theta 1, expected value of x.
And because w has zero mean, there's no extra term.
So in particular, theta 0 would be equal to expected value
of y minus theta 1, expected value of x.
So let's use this equation to try to come up with a reasonable
estimate of theta 0.
I do not know the expected value of y, but I can estimate it.
How do I estimate it?
I look at the average of all the y's that I have obtained.
So I replace this.
I estimate it with the average of the data I have seen.
Here, similarly with the x's, I might not know the expected
value of x's, but I have data points for the x's.
I look at the average of all my data points.
I come up with an estimate of this expectation.
Now, I don't know what theta 1 is, but my procedure is going
to generate an estimate of theta 1, call it theta 1 hat.
And once I have this estimate, then a reasonable person
would estimate theta 0 in this particular way.
So that's how my estimate of theta 0
is going to be constructed.
It's this formula here.
We have not yet addressed the harder question, which
is how to estimate theta 1 in the first place.
So to estimate theta 0, I assume that I already
had an estimate for theta 1.
The right formula for the estimate of theta 1
happens to be this one.
It looks messy, but let's try to interpret it.
What I'm going to do is I'm going to take this model.
For simplicity, let's assume that the random variables have
zero means and see how we might try to estimate theta 1.
Let's multiply both sides of this equation by x.
So we get y times x equals theta 0 plus theta 0 x
plus theta 1 x squared plus x w.
And now take expectations of both sides.
If I have zero mean random variables,
the expected value of yx is just the covariance of x with y.
I have assumed that my random variables have zero means,
so the expectation of this is 0.
This one is going to be the variance of x.
So I have theta 1 variance of x.
And since I am assuming that my random variables have zero
mean and I'm also assuming that w is independent of x,
this last term also has zero mean.
So under such a probabilistic model,
this equation is true.
If we knew the variance and the covariance,
then we would know the value of theta 1.
But we only have data.
We do not necessarily know the variance and the covariance,
but we can estimate it.
What's the reasonable estimate of the variance?
The reasonable estimate of the variance
is this quantity here divided by n.
And the reasonable estimate of the covariance
is that numerator divided by n.
So this is my estimate of the mean.
I'm looking at the squared distances from the mean.
And I average them over lots and lots of data.
This is the most reasonable way of estimating
the variance of our distribution.
And similarly here, this quantity,
the expected value of this quantity
is the covariance of x with y.
And if we have lots and lots of data points,
this quantity here is going to be a very good estimate
of the covariance.
So basically, the way what this formula does
is one way of thinking about it is
that it starts from this relation, which is true exactly,
but estimates the covariance and the variance
