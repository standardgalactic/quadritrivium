So this blue curve here is our g of x, which is the
conditional expectation of theta given that x is equal to
little x.
So it's a curve because it consists of, in our example,
it consists of three straight segments.
But overall, it's nonlinear.
It's not a single line through this diagram.
And that's how things are in general.
g of x, our optimal estimate, has no reason to be a linear
function of x.
In general, it's going to be some complicated curve.
So how good is our estimate?
I mean, you reported your estimate of theta based on
x, and your boss asks you, what kind of error do you
expect to get?
Having observed a particular value of x, what you can
report to your boss is what you think the mean squared
error is going to be.
We observed a particular value of x.
So we're conditioning, and we're living in this universe.
Given that we have made this observation, this is a true
value of theta.
This is the estimate that we have produced.
This is the expected squared error given that we have made
a particular observation.
Now, in this conditional universe, this is the expected
value of theta given x.
So this is the expected value of this random variable inside
the conditional universe.
So when you take the mean squared of a random variable
minus the expected value, this is the same thing as the
variance of that random variable, except that it's the
variance inside the conditional universe.
Having observed x, theta is still a random variable.
It's distributed according to the posterior
distribution.
Since it's a random variable, it has a variance.
And that variance is our mean squared error.
So this is the variance of the posterior distribution of
theta, given the observation that we have made.
So what is the variance in our example?
If x happens to be here, then theta is uniform over this
interval.
And this interval has length 2.
Theta is uniformly distributed over an
interval of length 2.
This is the posterior distribution of theta.
What is the variance?
Then you remember the formula for the variance of a uniform
random variable.
It is the length of the interval squared divided by 12.
So this is 1 third.
So the variance of theta, the mean squared error, is going
to be 1 third whenever this kind of picture applies.
This picture applies when x is between 5 and 9.
If x is less than 5, then the picture is a little different.
And theta is going to be uniform over a smaller interval.
And so the variance of theta is going
to be smaller as well.
So let's start plotting our mean squared error.
Between 5 and 9, the variance of theta, the posterior
variance, is 1 third.
Now, when x falls in here, theta is uniformly distributed
over a smaller interval.
The size of this interval changes linearly over that
range.
And so when we take the square of that, the square size of
that interval, we get a quadratic function of how
much we have moved from that corner.
So at that corner, what is the variance of theta?
Well, if I observe an x that's equal to 3, then I know
with certainty that theta is equal to 4.
Then, in a very good shape, I know exactly
what theta is going to be.
So the variance, in this case, is going to be 0.
If I observe an x that's a little larger, then theta is
now random, takes values on a little interval.
And the variance of theta is going to be proportional to
the square of the length of that little interval.
So we get a curve that starts rising quadratically from
here and goes up towards 1 third.
At the other end of the picture, the same is true.
If you observe an x which is 11, then theta can only be
equal to 10.
And so the error in theta is equal to 0.
There's zero error variance.
But as we obtain x's that are slightly less than 11, then
the mean squared error again rises quadratically.
So we end up with a plot like this.
What this plot tells us is that certain measurements are
better than others.
If you're lucky and you see x equal to 3, then you're lucky
because you know theta exactly what it is.
If you see an x which is equal to 6, then you're sort of
unlucky because it doesn't tell you theta with great
precision.
Theta could be anywhere on that interval.
And so the variance of theta, even after you have observed
x, is a certain number, 1 third in our case.
So the moral to keep out of that story is that the error
variance, or the mean squared error, depends on what
particular observation you happen to obtain.
Some observations may be very informative.
And once you see a specific number, then you know exactly
what theta is.
Some observations might be less informative.
You observe your x, but it could still leave a lot of
uncertainty about theta.
So conditional expectations are really the cornerstone of
Bayesian estimation.
They're particularly popular, especially in engineering
contexts.
They're used a lot in signal processing, in
communications, control theory, and so on.
So that makes it worth playing a little bit with their
theoretical properties and get some appreciation of a few
subtleties involved here.
No new math in reality in what we're going to do here, but
it's going to be a good opportunity to practice
manipulation of conditional expectations.
So let's look at the expected value of the estimation
error that we obtain.
So theta hat is our estimator.
It's the conditional expectation.
Theta hat minus theta is what kind of error do we have?
If theta hat, if our estimate is bigger than theta, then we
have made a positive error.
If it's on the other side, we have made a negative error.
Then it turns out that on the average, the errors cancel
each other out on the average.
So let's do this calculation.
Let's calculate the expected value of the error given x.
Now by definition, the error is expected value of theta hat
minus theta given x.
We use linearity of expectations to break it up as
expected value of theta hat given x minus expected value of
theta given x.
And now what?
Our estimate is made on the basis of the data of the x's.
If I tell you x, then you know what theta hat is.
Remember that the conditional expectation is a random variable
which is a function of the random variable on which
you're conditioning on.
If you know x, then you know the conditional expectation
given x.
You know what theta hat is going to be.
So theta hat is a function of x.
If it's a function of x, then once I tell you x, you know
what theta hat is going to be.
So this conditional expectation is going to be theta hat
itself.
Here, this is just by definition theta hat.
And so we get equality to 0.
So what we have proved is that the conditional, no matter
what I have observed, and given that I have observed
something, on the average, my error is going to be 0.
This is a statement involving equality of random variables.
Remember that conditional expectations are random
variables because they depend on the thing you're
conditioning on.
0 is sort of a trivial random variable.
This tells you that this random variable is identically
equal to the 0 random variable.
More specifically, it tells you that no matter what value for
x you observe, the conditional expectation of the error is
going to be 0.
And this takes us to this statement here, which is an
equality between numbers.
No matter what specific value for capital X you have
observed, your error on the average is going to be equal
to 0.
So this is a less abstract version of this statement.
This is an equality between two numbers.
It's true for every value of x.
So it's true for, in terms of this random variable, being
equal to that random variable.
Because remember, according to our definition, this random
variable is the random variable that takes this specific
value when capital X happens to be equal to little x.
Now, this doesn't mean that your error is 0.
It only means that your error is as likely in some sense to
fall on the positive side as to fall on the negative side.
So sometimes your error will be positive, sometimes negative.
And on the average, these things cancel out and give you a 0
on the average.
So this is a property that's sometimes giving the name.
We say that theta hat is unbiased.
So theta hat, our estimate, does not have a tendency to be
on the high side.
It does not have a tendency to be on the low side.
On the average, it's just right.
So let's do a little more playing here.
Let's see how our error is related to an arbitrary
function of the data.
Let's do this in a conditional universe and look at this
quantity.
Now, when you know x in a conditional universe where x
is known, then h of x is known.
And so you can pull it outside the expectation.
In the conditional universe where the value of x is given,
