It can be interpreted in the following way.
The expected value of theta is the estimate that you would
come up with if you didn't have any information about theta.
If you don't make any observations, this is the
best way of estimating theta.
But I have made an observation, x.
And I need to take it into account.
I look at this difference, which is the piece of news
contained in x.
That's what x should be on the average.
If I observe an x which is bigger than what I expected
it to be, and since x with theta are positively
correlated, this tells me that theta should also be bigger
than its average value.
Whenever I see an x that's larger than its average value,
this gives me an indication that theta should also probably
be larger than its average value.
And so I'm taking that difference and multiplying it
by a positive coefficient.
And that's what gives me a curve here that
has a positive slope.
So this increment, the new information contained in x, as
compared to the average value we expected a priori, that
increment allows us to make a correction to our prior
estimate of theta.
And the amount of that correction is guided by the
covariance of x with theta.
If the covariance of x with theta were 0, that would mean
there's no systematic relation between the two.
And in that case, obtaining some information from x doesn't
give us a guide as to how to change the estimate of theta.
If that were 0, we would just stay with this particular
estimate.
We're not able to make a correction.
But when there's a non-zero covariance between x and theta,
that covariance works as a guide for us to obtain a better
estimate of theta.
Then how about the resulting mean squared error?
In this context, it turns out that there's a very nice
formula for the mean squared error obtained from the best
linear estimate.
What's the story here?
The mean squared error that we have has something to do with
the variance of the original random variable.
The more uncertain our original random variable is, the more
error we're going to make.
On the other hand, when the two variables are correlated, we
exploit that correlation to improve our estimate.
And the bigger that correlation is, this row here is the
correlation coefficient between the two random variables.
When this correlation coefficient is larger, this
factor here becomes smaller.
And our mean squared error becomes smaller.
So think of the two extreme cases.
One extreme case is when rho is equal to 1.
So x and theta are perfectly correlated.
When they're perfectly correlated, once I know x,
then I also know theta.
And the two random variables are linearly related.
In that case, my estimate is right on the target.
And the mean squared error is going to be 0.
The other extreme case is if rho is equal to 0, the two
random variables are uncorrelated.
In that case, the measurement does not help me estimate
theta.
And the uncertainty that's left, the mean squared error, is
just the original variance of theta.
So the uncertainty in theta does not get reduced.
So moral, the estimation error is a reduced version of the
original amount of uncertainty in the
random variable theta.
And the larger the correlation between those two random
variables, the better we can remove uncertainty from the
original random variable.
I didn't derive this formula, but it's just a matter of
algebraic manipulations.
We have a formula for theta hat.
Subtract theta from that formula.
Take squared, take expectations, and do a few lines of
algebra that you can read in the text.
And you end up with this really neat and clean formula.
Now, I mentioned in the beginning of the lecture
that we can do inference with theta and x is not just
being single numbers, but they could be
vector random variables.
So for example, we might have multiple data that give us
information about x.
This discussion here, there are no vectors here.
So this discussion was for the case where theta and x were
just scalar one-dimensional quantities.
What do we do if we have multiple data?
Suppose that theta is still a scalar.
It's one-dimensional.
But we make several observations.
And on the basis of these observations, we want to
estimate theta.
The optimal least mean squares estimator would be again the
conditional expectation of theta given x.
That's the optimal one.
And in this case, x is a vector.
So in the general estimator we would use would be this one.
But if we want to keep things simple and we want our
estimator to have a simple functional form, we might
restrict estimators that are linear functions of the data.
And then the story is exactly the same as we discussed before.
I constrain myself to estimating theta using a
linear function of the data.
So my signal processing box just applies a linear function.
And I'm looking for the best coefficients, the coefficients
that are going to result in the least possible squared error.
This is my squared error.
This is my estimate minus the thing I'm trying to estimate,
squared, and then taking the average.
How do we do this?
Same story as before.
This quantity, the x's and the theta's get averaged out
because we have an expectation.
Whatever is left is just a function of the
coefficients of the a's and of b.
As before, it turns out to be a quadratic function.
Then we set the derivatives of this function of a's and b's
with respect to the coefficients.
We set it to 0.
And this gives us a system of linear equations.
It's a system of linear equations that's satisfied by
those coefficients.
It's a linear system because this is a quadratic function of
those coefficients.
So to get closed formulas in this particular case, one
would need to introduce vectors and matrices and
matrix inverses and so on.
The particular formulas are not so much what
interests us here.
Rather, the interesting thing is that this is simply done
just using straightforward solvers of linear equations.
The only thing you need to do is to write down the correct
coefficients of those nonlinear equations.
And the typical coefficient that you would get would be what?
Let's say a typical coefficient would be, let's take a
typical term of this quadratic when you expand it.
You're going to get a term such as a1 x1 times a2 x2.
When you take expectations, you're left with a1 a2 times
expected value of x1 x2.
So a typical, so this would be, it would involve terms
such as a1 squared, expected value of x1 squared.
You would get terms such as a1 a2, expected value of x1 x2.
And lots of other terms here would have a 2.
So you get something that's quadratic in your
coefficients, and the constants that show up in your
system of equations are things that have to do with
expected values of squares of your random variables or
products of your random variables.
To write down numerical values for these, the only thing
you need to know are the means and variances of your
random variables.
If you know the mean and variance, then you
know what this thing is.
And if you know the covariances as well, then
you know what this thing is.
So in order to find the optimal linear estimator in the
case of multiple data, you do not need to know the entire
probability distribution of the random variables that are
involved.
You only need to know your means and covariances.
These are the only quantities that affect the
construction of your optimal estimator.
We could see this already in this formula.
The form of my optimal estimator is completely
determined once I know the means, variance, and covariances
of the random variables in my model.
I do not need to know the detailed distribution of
the random variables that are involved here.
So as I said, in general, you find the form of the optimal
estimator by using a linear equation solver.
There are special examples in which you can get closed
form solutions.
The nicest simplest estimation problem one can think of is
the following.
You have some uncertain parameter, and you make
multiple measurements of that parameter in the
presence of noise.
So the w i's are noises.
i corresponds to your i-th experiment.
So this is the most common situation that you
encounter in the lab.
If you're dealing with some process, you're trying to
measure something, you measure it over and over.
Each time your measurement has some random error, and then
you need to take all your measurements together and come
up with a single estimate.
So the noises are assumed to be independent of each other,
and also to be independent from the value of the true
parameter.
Without loss of generality, we can assume that the noises
have zero mean, and they have some variances that we
assume to be known.
Theta itself has a prior distribution with a certain
mean and a certain variance.
So the form of the optimal linear estimator is really
nice.
