Well, maybe you cannot see it right away, because this
looks messy, but what is it really?
It's a linear combination of the x's and the prior mean,
and it's actually a weighted average of the x's and the
prior mean.
Here, we collect all the coefficients that we have at
the top.
So the whole thing is basically a weighted average.
This is the 1 over sigma i squared is the weight that we
give to xi, and in the denominator, we have the
sum of all the weights.
So in the end, we're dealing with a weighted average.
If mu was equal to 1, and all the xi's were equal to 1, then
our estimate would also be equal to 1.
Now, the form of the weights that we have is interesting.
Any given data point is weighted inversely proportional to
the variance.
What does that say?
If my ith data point has a lot of variance, if wi is very
noisy, then xi is not very useful, is not very reliable,
so I'm giving it a small weight.
Large variance, a lot of error in my xi, means that I should
give it a smaller weight.
If two data points have the same variance there of
comparable quality, then I'm going to give them equal
weight.
The other interesting thing is that the prior mean is
treated the same way as the x's.
So it's treated as an additional observation.
So we're taking a weighted average of the prior mean and
of the measurements that we're making.
The formula looks as if the prior mean was just another
data point.
So that's a weight of thinking about Bayesian estimation.
You have the real data points, the x's that you observe.
You also had some prior information.
This plays a role similar to a data point.
Interesting note that if all random variables are normal
in this model, this optimal linear estimator happens to
be also the conditional expectation.
That's a nice thing about normal random variables, that
conditional expectations turn out to be linear.
So the optimal estimate and the optimal linear estimate
turn out to be the same.
And that gives us another interpretation of linear
estimation.
Linear estimation is essentially the same as
pretending that all random variables are normal.
So that's sort of a side point.
Now, I'd like to close with a comment.
You do your measurements and you estimate theta on the
basis of x.
Suppose that instead you have a measuring device that
measures x cubed instead of measuring x.
And you want to estimate theta.
Are you going to get a different estimate?
Well, x and x cubed contain the same information, telling
you x is the same as telling you the value of x cubed.
So the posterior distribution of theta given x is the same
as the posterior distribution of theta given x cubed.
And so the means of these posterior distributions are
going to be the same.
So doing transformations to your data does not matter if
you're doing optimal least squares estimation.
On the other hand, if you restrict yourself to doing
linear estimation, then using a linear function of x is not
the same as using a linear function of x cubed.
So this is a linear estimator, but where the data are the
x cubes.
And we have a linear function of the data.
So this means that when you're using linear estimation, you
have some choices to make, linear on what?
Sometimes you want to plot your data on an ordinary scale
and try to plot a line through them.
Sometimes you plot your data on a logarithmic scale and try
to plot a line through them.
Which scale is the appropriate one?
Here it would be a cubic scale.
And you have to think about your particular model to decide
which version would be a more appropriate one.
Finally, when we have multiple data, sometimes these
multiple data might contain the same information.
So x is one data point, x squared is another data point,
x cubed is another data point.
The three of them contain the same information, but you can
try to form a linear function of them.
And then you obtain a linear estimator that has a more
general form as a function of x.
So if you want to estimate your theta as a cubic function of
x, for example, you can set up a linear estimation model of
this particular form and find the optimal
coefficients, the a's and the b.
All right, so the last slide just gives you the big picture
of what's happening in Bayesian inference.
It's for you to ponder.
Basically, we talked about three possible estimation
methods, maximum posteriori, where we mean squared error
estimation and linear mean squared error estimation, or
least squares estimation.
And there's a number of standard examples that you
will be seeing over and over, restation tutorial, homework,
and so on, perhaps on exams even, where we take some nice
priors on some unknown parameter, we take some nice
models for the noise or the observations, and then you need
to work out posterior distributions and the various
estimates and compare them.
