The following content is provided under a Creative
Commons license.
Your support will help MIT OpenCourseWare continue to
offer high-quality educational resources for free.
To make a donation or view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
ocw.mit.edu.
So we're going to finish today our discussion of Bayesian
inference, which we started last time.
As you probably saw, there's not a huge lot of concepts
that we're introducing at this point in terms of specific
skills of calculating probabilities, but rather it's
more of an interpretation and setting up the framework.
So the framework in Bayesian estimation is that there is
some parameter which is not known, but we have a prior
distribution on it.
These are beliefs about what this variable might be.
And then we obtain some measurements, and the
measurements are affected by the value of that parameter
that we don't know.
And this effect, the fact that x is affected by theta, is
captured by introducing conditional probability
distribution.
The distribution of x depends on theta.
It's a conditional probability distribution.
So we have formulas for these two densities, the prior
density and the conditional density.
And given that we have these, if we multiply them, we can
also get the joint density of x and theta.
So we have everything that there is to know in this
setting.
And now we observe the random variable x.
Given this random variable, what can we say about theta?
Well, what we can do is we can always calculate the
conditional distribution of theta, given x.
And now that we have the specific value of x, we can
plot this as a function of theta.
And this is the complete answer to a Bayesian inference
problem.
This posterior distribution captures everything there is
to say about theta.
That's what we know about theta.
Given the x that we have observed, theta is still
random.
It's still unknown.
And it might be here, there, or there, with several
probabilities.
On the other hand, if you want to report a single value for
theta, then you do some extra work.
You continue from here.
And you do some data processing on x.
Doing data processing means that you apply a certain
function on the data.
And this function is something that you design.
It's the so-called estimator.
And once that function is applied, it outputs an
estimate of theta, which we call theta hat.
So this is sort of the big picture of what's happening.
Now, one thing to keep in mind is that even though I'm
writing single letters here, in general, theta or x could be
vector random variables.
So think of this.
It could be a collection theta 1, theta 2, theta 3.
And maybe we obtain several measurements.
So this x is really a vector x1, x2, up to xn.
All right, so now how do we choose a theta to report?
There are various ways of doing it.
One is to look at the posterior distribution and report the
value of theta at which the density or the PMF is highest.
This is called the maximum posteriori
probability estimate.
So we pick a value of theta for which the posterior is
maximum and we report it.
An alternative way is to try to be optimal with respect to a
mean squared error.
So what is this?
If we have a specific estimator, g, this is the
estimate it's going to produce.
This is the true value of theta, so this is our
estimation error.
We look at the square of the estimation error and look at
the average value.
We would like this squared estimation error to be as
small as possible.
How can we design our estimator g to make that error as
small as possible?
It turns out that the answer is to produce as an estimate the
conditional expectation of theta given x.
So the conditional expectation is the best estimate that you
could produce if your objective is to keep the mean
squared error as small as possible.
So this statement here is a statement of what happens on
the average over all thetas and all x's that may happen in
our experiment.
Actually, the conditional expectation as an estimator
has an even stronger property.
Not only it's optimal on the average, but it's also optimal
given that you have made a specific observation.
No matter what you observe, let's say you observe the
specific value for the random variable x.
After that point, if you're asked to produce a best
estimate theta hat that minimizes this mean squared
error, your best estimate would be the conditional
expectation given the specific value that you have observed.
These two statements say almost the same thing, but this
one is a bit stronger.
This one tells you no matter what specific x happens, the
conditional expectation is the best estimate.
This one tells you on the average over all x's that
may happen, the conditional expectation is the best
estimator.
Now this is really a consequence of this.
If the conditional expectation is best for any specific x,
then it's the best one even when x is left random and you are
averaging your error over all possible x's.
OK, so now that we know what is the optimal way of
producing an estimate, let's do a simple example to see how
things work out.
So we start with an unknown random variable, theta, which
is uniformly distributed between 4 and 10.
And then we have an observation model that tells us that
given the value of theta, x is going to be a random variable
that ranges between theta minus 1 and theta plus 1.
So think of x as a noisy measurement of theta plus
some noise, which is between minus 1 and plus 1.
So really the model that we are using here is that x is equal
to theta plus u, where u is uniform on minus 1 and plus 1.
So we have the true value of theta, but x could be theta
minus 1, or it could be all the way up to theta plus 1.
And x is uniformly distributed on that interval.
That's the same as saying that u is uniformly distributed
over this interval.
So now we have all the information that we need.
We can construct the joint density.
And the joint density is, of course, the prior density times
the conditional density.
We got both of these.
Both of these are constants.
So the joint density is also going to be a constant, 1, 6
times 1 half.
This is 1 over 12.
But it is a constant, not everywhere, only on the range
of possible x's and theta's.
So theta can take any value between 4 and 10.
So these are the values of theta.
And for any given value of theta, x can take values from
theta minus 1 up to theta plus 1.
So here, if you can imagine a line that goes with slope 1,
and then x can take that value of theta plus or minus 1.
So this object here, this is the set of possible x and
theta pairs.
So the density is equal to 1 over 12 over this set.
And it's 0 everywhere else.
So outside here, the density is 0.
The density only applies at that point.
All right, so now we're asked to estimate theta in terms of
x.
So we want to build an estimator, which is going to be a
function from the x's to the theta's.
That's why I chose the axis this way, x to be on this axis
theta on that axis.
Because the estimator we're building is a function of x.
Based on the observation that we obtained, we want to
estimate theta.
So we know that the optimal estimator is the conditional
expectation given the value of x.
So what is the conditional expectation?
If you fix a particular value of x, let's say in this range,
so this is our x, then what do we know about theta?
We know that theta lies in this range.
Theta can only be somewhere between those two values.
And what kind of distribution does theta have?
What is the conditional distribution of theta given x?
Well, remember how we built conditional distributions from
joint distributions.
The conditional distribution is just a section of the joint
distribution applied to the place where we're conditioning.
So the joint is constant, so the conditional is also going
to be a constant density over this interval.
So the posterior distribution of theta is uniform
over this interval.
So if the posterior of theta is uniform over that interval,
the expected value of theta is going to be the midpoint of
that interval.
So the estimate which you report if you observe that
theta is going to be this particular point here.
It's the midpoint.
The same argument goes through even if you obtain an x
somewhere here.
Given this x, theta can take a value between these two
values.
Theta is going to have a uniform distribution over this
interval.
And the conditional expectation of theta given x is going to
be the midpoint of that interval.
So now if we plot our estimator by tracing midpoints in
this diagram, what you're going to obtain is a curve that
goes, it starts like this.
Then it changes slope so that it keeps track of the midpoint.
And then it goes like that again.
