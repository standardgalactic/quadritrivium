this quantity becomes just a constant.
There's nothing random about it.
So you can pull it out of the expectation and write things
this way.
And we have just calculated that this quantity is 0.
So this number turns out to be 0 as well.
OK.
Now, having done this, we can take
expectations of both sides.
And now let's use the law of iterated expectations.
Expectation of a conditional expectation gives us the
unconditional expectation.
And this is also going to be 0.
So here, we use the law of iterated expectations.
OK.
OK.
Why are we doing this?
We're doing this because I would like to calculate the
covariance between theta tilde and theta hat.
That is, ask the question, is there a systematic relation
between the error and the estimate?
So to calculate the covariance, we use the property
that we can calculate its covariances by calculating the
expected value of the product minus the product of the
expected values.
And what do we get?
This is 0 because of what we just proved.
And this is 0 because of what we proved earlier, that the
expected value of the error is equal to 0.
So the covariance between the error and any function of x
is equal to 0.
Let's use that to the case where the function of x who
we're considering is theta hat itself.
Theta hat is our estimate.
It's a function of x.
So this 0 result would still apply.
And we get that this covariance is equal to 0.
So that's what we proved.
Let's see.
What are the morals to take out of all this?
First is you should be very comfortable with this type of
calculation involving conditional expectations.
The main two things that we're using are that when you
condition on a random variable, any function of that
random variable becomes a constant and can be pulled
out the conditional expectation.
The other thing that we're using is the law of
iterated expectations.
So these are the skills involved.
Now on the substance, why is this result interesting?
This tells us that the error is uncorrelated with the
estimate.
What would happen if that were, what's a hypothetical
situation where this would not happen?
Whenever theta hat is positive, my error tends to be
negative.
Suppose that whenever theta hat is big, then you say, oh, my
estimate is too big.
Maybe the true theta is on the lower side, so I expect my
error to be negative.
That would be a situation that would violate this
condition.
This condition tells you that no matter what theta hat is,
you don't expect your error to be on the positive side or on
the negative side.
Your error will still be 0 on the average.
So if you obtain a very high estimate, this is no reason for
you to suspect that the true theta is lower than your
estimate.
If you suspected that the true theta was lower than your
estimate, you should have changed your theta hat.
If you make an estimate, and after obtaining that estimate,
you say, I think my estimate is too big, and so the error is
negative.
If you thought that way, then that means that your estimate
is not the optimal one, that your estimate should have
been corrected to be smaller.
That would mean that there's a better estimate than the one
you used, but the estimate that we are using here is the
optimal one in terms of mean squared error.
There's no way of improving it, and this is really captured
in that statement.
That is, knowing theta hat doesn't give you a lot of
information about the error and gives you, therefore, no
reason to adjust your estimate from what it was.
Finally, a consequence of all this.
This is the definition of the error.
Send theta to this side, send theta tilde to that side, you
get this relation.
The true parameter is composed of two quantities, the
estimate, and then the error that I got with a minus sign.
These two quantities are uncorrelated with each other.
Their covariance is 0, and therefore, the variance of
this is the sum of the variances of these two quantities.
So what's an interpretation of this equality?
There is some inherent randomness in the random
variable theta that we're trying to estimate.
Theta hat tries to estimate it, tries to get close to it, and
if theta hat always stays close to theta, since theta is
random, theta hat must also be quite random, so it has
uncertainty in it.
And the more uncertain theta hat is, the more it moves
together with theta, so the more uncertainty it
removes from theta.
And this is the remaining uncertainty in theta.
The uncertainty that's left after we've done our estimation.
So ideally, to have a small error, we want this quantity
to be small, which is the same as saying that this quantity
should be big.
In the ideal case, theta hat is the same as theta.
That's the best we could hope for.
That corresponds to zero error, and all the variance, all
the uncertainty in theta is absorbed by the
uncertainty in theta hat.
Interestingly, this relation here is just another variation
of the law of total variance that we have seen at some
point in the past.
I will skip that derivation, but it's an interesting fact,
and it can give you an alternative interpretation of
the law of total variance.
OK, so now let's return to our example.
In our example, we obtained the optimal estimator, and we
saw that it was a nonlinear curve, something like this.
I'm exaggerating the corner a little bit to show that it's
nonlinear.
This is the optimal estimator.
It's a nonlinear function of x, which means nonlinear
generally means complicated.
Sometimes, the conditional expectation is really hard to
compute, because whenever you have to compute
expectations, you need to do some integrals.
And if you have many random variables involved, it might
correspond to a multi-dimensional integration.
We don't like this.
Can we come up maybe with a simpler way of estimating
theta, of coming up with a point estimate, which still
has some nice properties?
It has some good motivation, but is simpler.
What does simpler mean?
Perhaps linear.
Let's put ourselves in a straight jacket and restrict
ourselves to estimators that are of this form.
My estimate is constrained to be a linear
function of the axis.
So my estimate, my estimator is going to be a curve, a
linear curve.
It could be this.
It could be that.
Maybe it would want to be something like this.
I want to choose the best possible linear function.
What does that mean?
It means that I write my theta hat in this form.
If I fix a certain a and b, I have fixed the functional
form of my estimator.
And this is the corresponding mean squared error.
That's the error between the true parameter and the
estimate of that parameter.
We take the square of this.
Now the optimal linear estimator is defined as one
for which this mean squared error is smallest possible
over all choices of a and b.
So we want to minimize this expression
over all a's and b's.
How do we do this minimization?
Well, this is a square.
You can expand it, write down all the terms in the
expansion of the square.
So you're going to get a term expected value of theta squared.
You're going to get another term a squared
expected value of x squared, another term which is b
squared.
And then you're going to get various cross terms.
What you obtain, or what you have here, is really a
quadratic function of a and b.
So think of this quantity that we're minimizing as some
function h of a and b.
And it happens to be quadratic.
How do we minimize a quadratic function?
We set the derivative of this function with respect to a and
b to 0, and then do the algebra.
After you do the algebra, you find that the best choice for
a is this one.
So this is the coefficient next to x.
This is the optimal a.
And the optimal b corresponds of the constant terms.
So this term and this times that together are the
optimal choices of b.
So the algebra itself is not very interesting.
What is really interesting is the nature of the result that
we get here.
If we were to plot the result on this particular example, you
would get a curve that's sort of something like this.
Sort of it goes through the middle of this diagram and
is a little slanted.
In this example, x and theta are positively correlated.
Bigger values of x generally correspond to
bigger values of theta.
So in this example, the covariance between x and
theta is positive.
And so our estimate consists.
