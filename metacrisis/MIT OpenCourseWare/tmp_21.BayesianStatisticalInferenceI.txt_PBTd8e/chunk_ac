Now, if theta is continuous, then we're dealing with
estimation problems.
But the story is once more the same.
You're going to use the Bayes rule to come up with the
posterior density of theta given the data that you have
observed.
Now, just for the sake of the example, let's come back to
this picture here.
Suppose that something is flying in the air.
And maybe this is just an object in the air
around close to the earth.
So because of gravity, the trajectory that it's going to
follow is going to be a parabola.
So this is the general equation of a parabola.
zt is the position of my object at time t.
But I don't know exactly which parabola it is, so the
parameters of the parabola are unknown quantities.
What I can do is to go and measure the position of my
object at different times.
But unfortunately, my measurements are noisy.
And now I'm trying what I want to do is to model the motion
of my object.
So I guess in this picture the axis would be t going this
way and z going this way.
And on the basis of the data that I get, these are my
axes, I want to figure out the thetas.
That is, I want to figure out the exact
equation of this parabola.
Now, if somebody gives you probability distributions for
theta, these would be your priors.
So this is given.
We need the conditional distribution of the axis given
the thetas.
Well, we have the conditional distribution of z given the
thetas from this equation.
And then by playing with this equation, you can also find
how is x distributed if theta takes a particular value.
So you do have all the densities that you might need.
And you can apply the Bayes rule.
And at the end, your end result would be a formula for the
distribution of theta given the x that you have observed.
Except for one sort of complication or to make
things more interesting.
Instead of these axes and thetas being single random
variables that we have here, typically those axes and
thetas will be multi-dimensional random variables or will
correspond to multiple ones.
So this little theta here actually stands for a
triple of theta 0, theta 1, and theta 2.
And that x here stands for the entire sequence of axes that
we have observed.
So in reality, the object you're going to get at the end
after inference is done is a function that you plug in the
values of the data.
And you get a function of the thetas that tells you the
relative likelihoods of different theta triples.
So what I'm saying is that this is no harder than the
problems that you have dealt with so far, except perhaps for
the complication that usually in interesting inference
problems, your thetas and axes are often vectors of random
variables instead of individual random variables.
Now, if you are to do estimation in a case where you
have discrete data, again, the situation is no different.
We still have a base rule of the same kind, except that
densities get replaced by PMFs.
If x is discrete, you put a P here instead of putting an F.
So an example of an estimation problem with
discrete data is similar to the polling problem.
You have a coin.
It has an unknown parameter, theta.
This is the probability of obtaining heads.
You flip the coin many times.
What can you tell me about the true value of theta?
A classical statistician at this point would say, OK, I'm
going to use an estimator, the most reasonable one, which
is this.
How many heads did I obtain in n trials?
Divide by the total number of trials.
This is my estimate of the bias of my coin.
And then a classical statistician would continue
from here and try to prove some properties and argue that
this estimate is a good one.
For example, we have the weak law of large numbers that
tells us that this particular estimate converges in
probability to the true parameter.
This is a kind of guarantee that's useful to have.
And the classical statistician would pretty much close the
subject in this way.
What would a Bayesian person do differently?
A Bayesian person would start by assuming a prior
distribution on theta.
Instead of treating theta as an unknown constant, they would
say that theta was picked randomly, or pretend that it
was picked randomly, and assume a distribution on theta.
So for example, you might assume, if you don't know
anything more, you might assume that any value for the bias
of the coin is as likely as any other value of the bias of
the coin.
And this way, assume a probability distribution
that's uniform.
Or if you have a little more faith in the manufacturing
process that created that coin, you might choose your
prior to be a distribution that's centered around one
half, and it's fairly narrowly centered around one half.
That would be a prior distribution in which you
say, well, I believe that the manufacturer tried to make my
coin to be fair, but they often make some mistakes.
So I believe it's approximately one half, but not quite.
So depending on your beliefs, you would choose an appropriate
prior for the distribution of theta.
And then you would use the Bayes rule to find the
probabilities of different values of theta based on the
data that you have observed.
So no matter which version of the Bayes rule that you use,
the end product of the Bayes rule is going to be either a
plot of this kind or a plot of that kind.
So what am I plotting here?
This axis is the theta axis.
These are the possible values of the unknown quantity that
we're trying to estimate.
In the continuous case, theta is a continuous random
variable.
I obtain my data, and I plot the posterior probability
distribution after observing my data.
And I'm plotting here the probability density for theta.
So this is a plot of that density.
In the discrete case, theta can take finitely many values,
or a discrete set of values.
And for each one of those values, I'm telling you how
likely is that value to be the correct one, given the data
that I have observed.
And in general, what you would go back to your boss and
report after you've done all your inference work would be
either a plot of this kind or of that kind.
So you go to your boss who asks you, what is the value of
theta?
And you say, well, I only have limited data.
I don't know what it is.
It could be this with so much probability, or there's
probability, OK, let's throw in some numbers here.
There's probability 0.3 that theta is this value.
There's probability 0.2 that theta is this value, 0.1 that
it's this one, 0.1 that it's this one, 0.2 that it's that
one, and so on.
Now bosses often want simple answers.
They say, OK, you're talking too much.
What do you think theta is?
And now you're forced to make a decision.
If that was the situation, and you have to make a decision,
how would you make it?
Well, I'm going to make a decision that's most likely
to be correct.
If I make this decision, what's going to happen?
Theta is this value with probability 0.2, which means
there's probability 0.8 that I make an error
if I make that guess.
If I make that decision, this decision has probability 0.3
of being the correct one.
So I have probability of error 0.7, so on.
So if you want to just maximize the probability of
giving the correct decision, or if you want to minimize the
probability of making an incorrect decision, what
you're going to choose to report is that value of theta for
which the probability is highest.
So in this case, I would choose to report this
particular value, the most likely value of theta, given
what I have observed.
And that value is called the maximum
posteriori probability estimate.
It's going to be this one in our case.
So picking the point in the posterior PMF that has the
highest probability, that's the reasonable thing to do.
This is the optimal thing to do if you want to minimize the
probability of an incorrect inference.
And that's what people do usually if they need to
report a single answer, if they need to report a single
decision.
How about in the estimation context?
If that's what you know about theta.
Theta could be around here, but there's also some sharp
probability that it is around here.
What's the single answer you would give to your boss?
One option is to use the same philosophy and say, OK, I'm
going to find the theta at which this posterior density is
highest, so I would pick this point here and report this
particular theta.
So this would be my theta, again, theta map, the theta
that has the highest posteriori probability, just
because it corresponds to the peak of the density.
But in this context, the maximum
posteriori probability, theta, was the one that was most
likely to be true.
In the continuous case, you cannot really say that this
is the most likely value of theta.
In a continuous setting, any value of theta has zero
probability, so when we talk about densities.
So it's not the most likely.
It's the one for which the density around the probabilities
of that neighborhood are highest.
So the rationale for picking this particular estimate in the
