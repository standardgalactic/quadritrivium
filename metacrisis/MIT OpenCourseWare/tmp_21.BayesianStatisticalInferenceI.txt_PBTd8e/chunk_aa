The following content is provided under a Creative
Commons license.
Your support will help MIT OpenCourseWare continue to
offer high-quality educational resources for free.
To make a donation or view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
ocw.mit.edu.
It's about real phenomena out there.
So we have real stuff that happens.
So it might be an arrival process to a bank that we're
trying to model.
This is reality.
But this is what we have been doing so far.
We have been playing with models of probabilistic
phenomena.
And somehow we need to tie the two together.
The way these are tied is that we observe the real world.
And this gives us data.
And then based on this data, we try to come up with a model
of what exactly is going on.
For example, for an arrival process, you might ask the
model in question, is my arrival process Poisson or is it
something different?
If it is Poisson, what is the rate of the arrival process?
Once you come up with your model and you come up with the
parameters of the model, then you can use it to make
predictions about reality or to figure out certain hidden
things, certain hidden aspects of reality that you do not
observe directly, but you try to infer what they are.
So that's where the usefulness of the model comes in.
Now this field is, of course, tremendously useful.
And it shows up pretty much everywhere.
So we talked about the polling examples in the last couple
of lectures.
This is, of course, a real application.
You sample, and on the basis of the sample that you have,
you try to make some inferences about, let's say,
the preferences in a given population.
Let's say, in the medical field, you want to try whether a
certain drug makes a difference or not.
So people will do medical trials, get some results.
And then from the data, somehow you need to make sense of
them and make a decision.
Is the new drug useful or is it not?
How do we go systematically about a question of this type?
Sexier, more recent topic, there is this famous Netflix
competition, where Netflix gives you a huge table of
movies and people.
And people have rated movies, but not everyone has watched
all the movies in there.
You have some of the ratings.
For example, this person gave a four to that
particular movie.
So you get a table that's partially filled.
And Netflix asks you to make recommendations to people.
So this means trying to guess, this person here, how much
would they like this particular movie?
And you can start thinking, well, maybe this person has
given somewhat similar ratings with another person.
And if that other person has also seen that movie, maybe
the rating of that other person is relevant.
But of course, it's a lot more complicated than that.
And this has been a serious competition where people have
been using every heavy machinery that there is in
statistics, trying to come up with good
recommendation systems.
Then other people, of course, are trying to analyze
financial data.
Somebody gives you the sequence of values, let's say, of the
S&P index.
You look at something like this, and you can ask
questions.
How do I model these data using any of the models that we
have in our bag of tools?
How can I make predictions about what's going to happen
afterwards?
And so on.
On the engineering side, anywhere where you have noise,
inference comes in.
Signal processing, in some sense, is just an inference
problem.
You observe signals that are noisy, and you try to figure
out exactly what's happening out there, or what kind of
signal has been sent.
Maybe the beginning of the field could be traced a few
hundred years ago, where people would make astronomical
observations of the position of planets in the sky.
They would have some beliefs that perhaps the orbit of a
planet is an ellipse.
Or if it's a comet, maybe it's a parabola, hyperbola, I
don't know what it is.
But they would have a model of that.
But of course, astronomical measurements would not be
perfectly exact.
And they would try to find a curve that fits these data.
How do you go about choosing this particular curve on the
base of noisy data, and try to do it in a
somewhat principled way?
OK.
So questions of this type, clearly the applications are
all over the place.
But how is this related conceptually with what we
have been doing so far?
What's the relation between the field of inference and the
field of probability as we have been practicing until now?
Well, mathematically speaking, what's going to happen in
the next few lectures could be just exercises or homework
problems in the class based on what we have done so far.
That is, you're not going to get any new facts about
probability theory.
Everything we're going to do will be simple applications of
things that you already do know.
So in some sense, statistics and inference is just an
applied exercise in probability.
But actually, things are not that simple in the
following sense.
If you get a probability problem, there's a correct
answer, there's a correct solution, and that correct
solution is unique.
There's no ambiguity.
The theory of probability has clearly defined rules.
These are the axioms.
You're given some information about probability
distributions.
You're asked to calculate certain other things.
There's no ambiguity.
Answers are always unique.
In statistical questions, it's no longer the case that the
question has a unique answer.
If I give you data and I ask you what's the best way of
estimating the motion of that planet, reasonable people can
come up with different methods.
And reasonable people will try to argue that my method has
these desirable properties, but somebody else may say, here's
another method that has certain desirable properties.
And it's not clear what the best method is.
So it's good to have some understanding of what the
issues are, and to know at least what is the general class
of methods that one tries to consider, how does one go
about such problems.
So we're going to see lots and lots of different inference
methods.
We're not going to tell you that one is better than the
other, but it's important to understand what are the concepts
between those different methods.
And finally, statistics can be misused really badly.
That is, one can come up with methods that you think they're
sound, but in fact, they're not quite that.
I will bring some examples next time and talk a little more
about this.
So I want to say, you have some data.
You want to make some inference from them.
What many people will do is to go to Wikipedia, find a
statistical test that they think it applies to that
situation, plug in numbers, and present results.
Are the conclusions that they get really justified, or are
they misusing statistical methods?
Well, too many people actually do misuse statistics, and
conclusions that people get are often false.
So it's important to, besides just being able to copy
statistical tests and use them, to understand what are the
assumptions between the different methods, and what
kind of guarantees they have, if any.
All right, so we'll try to do a quick tour through the field
of inference in this lecture and the next few lectures that
we have left this semester, and try to highlight at a very
high level the main concept skills and
techniques that come in.
Let's start with some generalities and some general
statements.
One first statement is that statistics or inference
problems come up in very different guises, and they may
look as if they are of very different forms.
Although at some fundamental level, the basic issues turn
out to be always pretty much the same.
So let's look at this example.
There's an unknown signal that's being sent.
It's sent through some medium, and that medium just takes
the signal and amplifies it by a certain number.
So you can think of somebody shouting.
There's the air out there.
What you shouted will be attenuated through the air
until it gets to a receiver.
And that receiver then observes this, but together
with some random noise.
Here I meant s.
s is the signal that's being sent.
And what you observe is an x.
So when you observe x, so what kind of inference
problems could we have here?
In some cases, you want to build a model of the physical
phenomenon that you are dealing with.
So for example, you don't know the attenuation of your
signal, and you try to find out what this number is based on
the observations that you have.
So the way this is done in engineering systems is that you
design a certain signal, you know what it is, you shout a
particular word, and then the receiver listens.
And based on the intensity of the signal that they get, they
try to make a guess about a.
