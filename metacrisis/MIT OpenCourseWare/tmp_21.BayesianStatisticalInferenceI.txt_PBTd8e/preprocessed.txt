The following content is provided under a Creative
Commons license.
Your support will help MIT OpenCourseWare continue to
offer high-quality educational resources for free.
To make a donation or view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
ocw.mit.edu.
It's about real phenomena out there.
So we have real stuff that happens.
So it might be an arrival process to a bank that we're
trying to model.
This is reality.
But this is what we have been doing so far.
We have been playing with models of probabilistic
phenomena.
And somehow we need to tie the two together.
The way these are tied is that we observe the real world.
And this gives us data.
And then based on this data, we try to come up with a model
of what exactly is going on.
For example, for an arrival process, you might ask the
model in question, is my arrival process Poisson or is it
something different?
If it is Poisson, what is the rate of the arrival process?
Once you come up with your model and you come up with the
parameters of the model, then you can use it to make
predictions about reality or to figure out certain hidden
things, certain hidden aspects of reality that you do not
observe directly, but you try to infer what they are.
So that's where the usefulness of the model comes in.
Now this field is, of course, tremendously useful.
And it shows up pretty much everywhere.
So we talked about the polling examples in the last couple
of lectures.
This is, of course, a real application.
You sample, and on the basis of the sample that you have,
you try to make some inferences about, let's say,
the preferences in a given population.
Let's say, in the medical field, you want to try whether a
certain drug makes a difference or not.
So people will do medical trials, get some results.
And then from the data, somehow you need to make sense of
them and make a decision.
Is the new drug useful or is it not?
How do we go systematically about a question of this type?
Sexier, more recent topic, there is this famous Netflix
competition, where Netflix gives you a huge table of
movies and people.
And people have rated movies, but not everyone has watched
all the movies in there.
You have some of the ratings.
For example, this person gave a four to that
particular movie.
So you get a table that's partially filled.
And Netflix asks you to make recommendations to people.
So this means trying to guess, this person here, how much
would they like this particular movie?
And you can start thinking, well, maybe this person has
given somewhat similar ratings with another person.
And if that other person has also seen that movie, maybe
the rating of that other person is relevant.
But of course, it's a lot more complicated than that.
And this has been a serious competition where people have
been using every heavy machinery that there is in
statistics, trying to come up with good
recommendation systems.
Then other people, of course, are trying to analyze
financial data.
Somebody gives you the sequence of values, let's say, of the
S&P index.
You look at something like this, and you can ask
questions.
How do I model these data using any of the models that we
have in our bag of tools?
How can I make predictions about what's going to happen
afterwards?
And so on.
On the engineering side, anywhere where you have noise,
inference comes in.
Signal processing, in some sense, is just an inference
problem.
You observe signals that are noisy, and you try to figure
out exactly what's happening out there, or what kind of
signal has been sent.
Maybe the beginning of the field could be traced a few
hundred years ago, where people would make astronomical
observations of the position of planets in the sky.
They would have some beliefs that perhaps the orbit of a
planet is an ellipse.
Or if it's a comet, maybe it's a parabola, hyperbola, I
don't know what it is.
But they would have a model of that.
But of course, astronomical measurements would not be
perfectly exact.
And they would try to find a curve that fits these data.
How do you go about choosing this particular curve on the
base of noisy data, and try to do it in a
somewhat principled way?
OK.
So questions of this type, clearly the applications are
all over the place.
But how is this related conceptually with what we
have been doing so far?
What's the relation between the field of inference and the
field of probability as we have been practicing until now?
Well, mathematically speaking, what's going to happen in
the next few lectures could be just exercises or homework
problems in the class based on what we have done so far.
That is, you're not going to get any new facts about
probability theory.
Everything we're going to do will be simple applications of
things that you already do know.
So in some sense, statistics and inference is just an
applied exercise in probability.
But actually, things are not that simple in the
following sense.
If you get a probability problem, there's a correct
answer, there's a correct solution, and that correct
solution is unique.
There's no ambiguity.
The theory of probability has clearly defined rules.
These are the axioms.
You're given some information about probability
distributions.
You're asked to calculate certain other things.
There's no ambiguity.
Answers are always unique.
In statistical questions, it's no longer the case that the
question has a unique answer.
If I give you data and I ask you what's the best way of
estimating the motion of that planet, reasonable people can
come up with different methods.
And reasonable people will try to argue that my method has
these desirable properties, but somebody else may say, here's
another method that has certain desirable properties.
And it's not clear what the best method is.
So it's good to have some understanding of what the
issues are, and to know at least what is the general class
of methods that one tries to consider, how does one go
about such problems.
So we're going to see lots and lots of different inference
methods.
We're not going to tell you that one is better than the
other, but it's important to understand what are the concepts
between those different methods.
And finally, statistics can be misused really badly.
That is, one can come up with methods that you think they're
sound, but in fact, they're not quite that.
I will bring some examples next time and talk a little more
about this.
So I want to say, you have some data.
You want to make some inference from them.
What many people will do is to go to Wikipedia, find a
statistical test that they think it applies to that
situation, plug in numbers, and present results.
Are the conclusions that they get really justified, or are
they misusing statistical methods?
Well, too many people actually do misuse statistics, and
conclusions that people get are often false.
So it's important to, besides just being able to copy
statistical tests and use them, to understand what are the
assumptions between the different methods, and what
kind of guarantees they have, if any.
All right, so we'll try to do a quick tour through the field
of inference in this lecture and the next few lectures that
we have left this semester, and try to highlight at a very
high level the main concept skills and
techniques that come in.
Let's start with some generalities and some general
statements.
One first statement is that statistics or inference
problems come up in very different guises, and they may
look as if they are of very different forms.
Although at some fundamental level, the basic issues turn
out to be always pretty much the same.
So let's look at this example.
There's an unknown signal that's being sent.
It's sent through some medium, and that medium just takes
the signal and amplifies it by a certain number.
So you can think of somebody shouting.
There's the air out there.
What you shouted will be attenuated through the air
until it gets to a receiver.
And that receiver then observes this, but together
with some random noise.
Here I meant s.
s is the signal that's being sent.
And what you observe is an x.
So when you observe x, so what kind of inference
problems could we have here?
In some cases, you want to build a model of the physical
phenomenon that you are dealing with.
So for example, you don't know the attenuation of your
signal, and you try to find out what this number is based on
the observations that you have.
So the way this is done in engineering systems is that you
design a certain signal, you know what it is, you shout a
particular word, and then the receiver listens.
And based on the intensity of the signal that they get, they
try to make a guess about a.
So you don't know a, but you know s, and by observing x,
you get some information about what a is.
So in this case, you're trying to build a model of the
medium through which your signal is propagating.
So sometimes one would call problems of this kind, let's
say system identification.
In a different version of an inference problem that comes
with this picture, you've done your modeling.
You know your a, you know the medium through which the
signal is going, but it's a communication system.
This person is trying to communicate something
to that person.
So you send the signal s, but that person receives a noisy
version of s.
So that person tries to reconstruct s based on x.
So in both cases, we have a linear relation between x and
the unknown quantity.
In one version, a is the unknown, and we know s.
In the other version, a is known, and we try to infer s.
Mathematically, you can see that it's essentially the same
kind of problem in both cases, although the kind of
practical problem that you're trying to solve is a
little different.
So we will not be making any distinctions between problems
of the model building type as opposed to models where you
try to estimate some unknown signal and so on, because
conceptually, the tools that one uses for both types of
problems are essentially the same.
Next, very useful classification of inference problems.
The unknown quantity that you're trying to estimate could
be either a discrete one that takes a small number of
values.
So this could be discrete problems, such as the radar
problem we encountered back a long time ago in this class.
So there's two possibilities.
An airplane is out there, or an airplane is not out there.
And you're trying to make a decision between these two
options, or you can have other problems in which you have,
let's say, four possible options.
You don't know which one is true, but you get data, and
you try to figure out which one is true.
In problems of this kind, usually you want to make a
decision based on your data, and you're interested in the
probability of making a correct decision.
You would like that probability to be as high as possible.
Estimation problems are a little different.
Here you have some continuous quantity that's not known, and
you try to make a good guess of that quantity, and you would
like your guess to be as close as possible to the true
quantity.
So the polling problem was of this type.
There was an unknown fraction f of the population that had
some property, and you try to estimate f as
accurate as you can.
So the distinction here is that usually here the unknown
quantity takes a discrete set of values.
Here the unknown quantity takes a
continuous set of values.
Here we're interested in the probability of error.
Here we're interested in the size of the error.
Broadly speaking, most inference problems fall either in
this category or in that category, although if you
want to complicate life, you can also think or construct
problems where both of these aspects are
simultaneously present.
OK, finally, since we're in classification mode, there's a
very big, important dichotomy into how one goes about
inference problems.
And here there's two fundamentally different
philosophical points of view, which is how do we model the
quantity that is unknown?
In one approach, you say there's a certain quantity that
has a definite value.
It just happens that I don't know it, but it's a number.
There's nothing random about it.
So think of trying to estimate some physical quantity.
Let's say you're making estimates, and you're making
measurements, you try to estimate the mass of an
electron, which is a sort of universal physical constant.
There's nothing random about it.
It's a fixed number.
You get data because you have some measuring apparatus.
And that measuring apparatus, depending on what the results
that you get, do are affected by the true mass of the
electron, but there's also some noise.
You take the data out of your measuring apparatus, and you
try to come up with some estimate of that quantity
theta.
So this is definitely a legitimate picture, but the
important thing in this picture is that this theta is
written as lower case, and that's to make the point that
it's a real number, not a random variable.
There's a different philosophical approach,
which says, well, anything that I don't know, I should
model it as a random variable.
Yes, I know.
The mass of the electron is not really random.
It's a constant, but I don't know what it is.
I have some vague sense, perhaps, what it is, perhaps
because of the experiments that some other people carried out.
So perhaps I have a prior distribution on the possible
values of theta.
And that prior distribution doesn't mean that nature is
random, but it's more of a subjective description of my
subjective beliefs of where do I think this constant number
happens to be.
So even though it's not truly random, I model my initial
beliefs before the experiment starts.
In terms of a prior distribution, I view it as a
random variable.
Then I observe another related random variable through some
measuring apparatus, and then I use this again to create an
estimate.
So these two pictures, philosophically, are very
different from each other.
Here we treat the unknown quantities as unknown numbers.
Here we treat them as random variables.
When we treat them as random variables, then we know pretty
much already what we should be doing.
We should just use the Bayes rule based on x, find the
conditional distribution of theta.
And that's what we will be doing mostly over this lecture
and the next lecture.
Now in both cases, what you end up getting at the end is an
estimate.
But actually, that estimate is what kind of object is it?
It's a random variable in both cases.
Why?
Even in this case, where theta was a constant, my data are
random.
I do my data processing, so I calculate a function of the
data, the data random variables.
So out here, we output something which is a
function of a random variable.
So this quantity here will be also random.
It's affected by the noise in the experiment that I have
been doing.
That's why these estimators will be denoted by upper case
thetas.
And we will be using hats.
Hats usually in estimation means an estimate of something.
All right, so this is the big picture.
We're going to start with the Bayesian version.
And then the last few lectures, we're going to talk about the
non-Bayesian version or the classical one.
By the way, I should say that statisticians have been
debating fiercely for 100 years whether the right way to
approach statistics is to go the classical way or the
Bayesian way.
And there have been tides going back and forth between the
two sides.
These days, Bayesian methods tend to become a little more
popular for various reasons.
We're going to come back to this later.
All right, so in Bayesian estimation, what we got in our
hands is Bayes' rule.
And if you have Bayes' rule, there's not a lot that's left
to do.
We have different forms of the Bayes' rule, depending on
whether we're dealing with discrete data and discrete
quantities to estimate or continuous data and so on.
In the hypothesis testing problem, the unknown quantity
theta is discrete.
So in both cases here, we have a p of theta.
We obtain data, the x's, and on the basis of the x that we
observe, we can calculate the posterior distribution of
theta given the data.
So to apply to use Bayesian inference, what do we start
with?
We start with some priors.
These are our initial beliefs about what theta might be.
That's before we do the experiment.
We have a model of the experimental apparatus.
And the model of the experimental apparatus tells
us, if this theta is true, I'm going to see x's of that kind.
If that other theta is true, I'm going to see x's that are
somewhere else.
That models my apparatus.
And based on that knowledge, once I observe, I have these
two functions in my hands.
We have already seen that if you know those two functions,
you can also calculate the denominator here.
So all these functions are available.
So you can compute, you can find a formula for this
function as well.
And as soon as you observe the data, the x's, you plug in
here the numerical value of those x's.
And you get a function of theta.
And this is the posterior distribution of theta given the
data that you have seen.
So you've already done a fair number of
exercises of this kind.
So let me not say more about this.
And there's a similar formula, as you know, for the case where
we have continuous data.
If the x's are continuous random variable, then the
formula is the same, except that x's are described by
densities instead of being described by
probability mass functions.
OK.
Now, if theta is continuous, then we're dealing with
estimation problems.
But the story is once more the same.
You're going to use the Bayes rule to come up with the
posterior density of theta given the data that you have
observed.
Now, just for the sake of the example, let's come back to
this picture here.
Suppose that something is flying in the air.
And maybe this is just an object in the air
around close to the earth.
So because of gravity, the trajectory that it's going to
follow is going to be a parabola.
So this is the general equation of a parabola.
zt is the position of my object at time t.
But I don't know exactly which parabola it is, so the
parameters of the parabola are unknown quantities.
What I can do is to go and measure the position of my
object at different times.
But unfortunately, my measurements are noisy.
And now I'm trying what I want to do is to model the motion
of my object.
So I guess in this picture the axis would be t going this
way and z going this way.
And on the basis of the data that I get, these are my
axes, I want to figure out the thetas.
That is, I want to figure out the exact
equation of this parabola.
Now, if somebody gives you probability distributions for
theta, these would be your priors.
So this is given.
We need the conditional distribution of the axis given
the thetas.
Well, we have the conditional distribution of z given the
thetas from this equation.
And then by playing with this equation, you can also find
how is x distributed if theta takes a particular value.
So you do have all the densities that you might need.
And you can apply the Bayes rule.
And at the end, your end result would be a formula for the
distribution of theta given the x that you have observed.
Except for one sort of complication or to make
things more interesting.
Instead of these axes and thetas being single random
variables that we have here, typically those axes and
thetas will be multi-dimensional random variables or will
correspond to multiple ones.
So this little theta here actually stands for a
triple of theta 0, theta 1, and theta 2.
And that x here stands for the entire sequence of axes that
we have observed.
So in reality, the object you're going to get at the end
after inference is done is a function that you plug in the
values of the data.
And you get a function of the thetas that tells you the
relative likelihoods of different theta triples.
So what I'm saying is that this is no harder than the
problems that you have dealt with so far, except perhaps for
the complication that usually in interesting inference
problems, your thetas and axes are often vectors of random
variables instead of individual random variables.
Now, if you are to do estimation in a case where you
have discrete data, again, the situation is no different.
We still have a base rule of the same kind, except that
densities get replaced by PMFs.
If x is discrete, you put a P here instead of putting an F.
So an example of an estimation problem with
discrete data is similar to the polling problem.
You have a coin.
It has an unknown parameter, theta.
This is the probability of obtaining heads.
You flip the coin many times.
What can you tell me about the true value of theta?
A classical statistician at this point would say, OK, I'm
going to use an estimator, the most reasonable one, which
is this.
How many heads did I obtain in n trials?
Divide by the total number of trials.
This is my estimate of the bias of my coin.
And then a classical statistician would continue
from here and try to prove some properties and argue that
this estimate is a good one.
For example, we have the weak law of large numbers that
tells us that this particular estimate converges in
probability to the true parameter.
This is a kind of guarantee that's useful to have.
And the classical statistician would pretty much close the
subject in this way.
What would a Bayesian person do differently?
A Bayesian person would start by assuming a prior
distribution on theta.
Instead of treating theta as an unknown constant, they would
say that theta was picked randomly, or pretend that it
was picked randomly, and assume a distribution on theta.
So for example, you might assume, if you don't know
anything more, you might assume that any value for the bias
of the coin is as likely as any other value of the bias of
the coin.
And this way, assume a probability distribution
that's uniform.
Or if you have a little more faith in the manufacturing
process that created that coin, you might choose your
prior to be a distribution that's centered around one
half, and it's fairly narrowly centered around one half.
That would be a prior distribution in which you
say, well, I believe that the manufacturer tried to make my
coin to be fair, but they often make some mistakes.
So I believe it's approximately one half, but not quite.
So depending on your beliefs, you would choose an appropriate
prior for the distribution of theta.
And then you would use the Bayes rule to find the
probabilities of different values of theta based on the
data that you have observed.
So no matter which version of the Bayes rule that you use,
the end product of the Bayes rule is going to be either a
plot of this kind or a plot of that kind.
So what am I plotting here?
This axis is the theta axis.
These are the possible values of the unknown quantity that
we're trying to estimate.
In the continuous case, theta is a continuous random
variable.
I obtain my data, and I plot the posterior probability
distribution after observing my data.
And I'm plotting here the probability density for theta.
So this is a plot of that density.
In the discrete case, theta can take finitely many values,
or a discrete set of values.
And for each one of those values, I'm telling you how
likely is that value to be the correct one, given the data
that I have observed.
And in general, what you would go back to your boss and
report after you've done all your inference work would be
either a plot of this kind or of that kind.
So you go to your boss who asks you, what is the value of
theta?
And you say, well, I only have limited data.
I don't know what it is.
It could be this with so much probability, or there's
probability, OK, let's throw in some numbers here.
There's probability 0.3 that theta is this value.
There's probability 0.2 that theta is this value, 0.1 that
it's this one, 0.1 that it's this one, 0.2 that it's that
one, and so on.
Now bosses often want simple answers.
They say, OK, you're talking too much.
What do you think theta is?
And now you're forced to make a decision.
If that was the situation, and you have to make a decision,
how would you make it?
Well, I'm going to make a decision that's most likely
to be correct.
If I make this decision, what's going to happen?
Theta is this value with probability 0.2, which means
there's probability 0.8 that I make an error
if I make that guess.
If I make that decision, this decision has probability 0.3
of being the correct one.
So I have probability of error 0.7, so on.
So if you want to just maximize the probability of
giving the correct decision, or if you want to minimize the
probability of making an incorrect decision, what
you're going to choose to report is that value of theta for
which the probability is highest.
So in this case, I would choose to report this
particular value, the most likely value of theta, given
what I have observed.
And that value is called the maximum
posteriori probability estimate.
It's going to be this one in our case.
So picking the point in the posterior PMF that has the
highest probability, that's the reasonable thing to do.
This is the optimal thing to do if you want to minimize the
probability of an incorrect inference.
And that's what people do usually if they need to
report a single answer, if they need to report a single
decision.
How about in the estimation context?
If that's what you know about theta.
Theta could be around here, but there's also some sharp
probability that it is around here.
What's the single answer you would give to your boss?
One option is to use the same philosophy and say, OK, I'm
going to find the theta at which this posterior density is
highest, so I would pick this point here and report this
particular theta.
So this would be my theta, again, theta map, the theta
that has the highest posteriori probability, just
because it corresponds to the peak of the density.
But in this context, the maximum
posteriori probability, theta, was the one that was most
likely to be true.
In the continuous case, you cannot really say that this
is the most likely value of theta.
In a continuous setting, any value of theta has zero
probability, so when we talk about densities.
So it's not the most likely.
It's the one for which the density around the probabilities
of that neighborhood are highest.
So the rationale for picking this particular estimate in the
continuous case is much less compelling than the rationale
that we had in here.
So in this case, reasonable people might choose different
quantities to report, and a very popular one would be to
report, instead, the conditional expectation.
So I don't know what theta is.
Given the data that I have, theta has this distribution.
Let me just report the average over that distribution.
Let me report the center of gravity of this figure.
And in this figure, the center of gravity would probably be
somewhere around here, and that would be a different
estimate that you might choose to report.
So center of gravity is somewhere around here, and
this is the conditional expectation of theta, given
the data that you have.
So these are two, in some sense, fairly reasonable ways of
choosing what to report to your boss.
Some people might choose to report this.
Some might choose to report that.
And a priori, there's no compelling reason why one
would be preferable than the other one, unless you set
some rules for the game, and you describe a little more
precisely what your objectives are.
But no matter which one you report, a single answer, a
point estimate, doesn't really tell you the whole story.
There's a lot more information conveyed by this posterior
distribution plot than any single number
that you might report.
So in general, you may wish to convince your boss that it's
worth their time to look at the entire plot, because that
plot sort of covers all the possibilities.
It tells your boss, most likely, who are in that range,
but there's also a distinct chance that our theta happens
to lie in that range.
Now let us try to perhaps differentiate between these
two and see under what circumstances this one might
be the better estimate to perform.
Better with respect to what?
We need some rules, so we're going to throw in some rules.
And as a warm-up, we're going to deal with the problem of
making an estimation if you had no information at all,
except for a prior distribution.
So this is a warm-up for what's coming next, which would be
estimation that takes into account some information.
So we have a theta, and because of your subjective
beliefs or models by others, you believe that theta is
uniformly distributed between, let's say, 4 and 10.
You want to come up with a point estimate.
Let's try to look for an estimate, call it c, in this
case, I want to pick a number with which to estimate the
value of theta, and I'm going to, I will be interested in the
size of the error that I make.
And I really dislike large errors, so I'm going to focus
on the square of the error that I make.
So I pick c, theta has a random value that I don't know,
but whatever it is, once it becomes known, it results
into a squared error between what it is and what I guessed
that it was.
And I'm interested in making a small error on the average,
where the average is taken with respect to all the possible
and unknown values of theta.
So the problem, this is a least squares formulation of the
problem, where we try to minimize the least squares error.
How do you find the optimal c?
Well, we take that expression and expand it, and it is using
linearity of expectations, expected for the square minus
2c expected theta.
Plus c squared.
That's the quantity that we want to minimize
with respect to c.
Take the derivative, to do the minimization, take the
derivative with respect to c and set it to 0.
So that differentiation gives us from here minus 2 expected
value of theta plus 2c is equal to 0.
And the answer that you get by solving this equation is that
c is the expected value of theta.
So when you do this optimization, you find that
the optimal estimate, the thing you should be reporting, is
the expected value of theta.
So in this particular example, you would choose your
estimate c to be just the middle of these values, which
would be, yes, 7.
And in case your boss asks you, how good is your estimate?
How big is your error going to be?
What you could report is the average size of the
estimation error that you are making.
We picked our estimate to be the expected value of theta.
So for this particular way that I'm using to do my
estimation, this is the mean squared error that I get.
And this is a familiar quantity.
It's just the variance of the distribution.
So the expectation is the best way to estimate a quantity
if you're interested in the mean squared error.
And the resulting mean squared error is the variance itself.
How will this story change if we now have data as well?
Now, having data means that we can compute posterior
distributions or conditional distributions.
So we get transported into a new universe where instead of
working with the original distribution of theta, the
prior distribution, now we work with the conditional
distribution of theta given that the
data that we have observed.
Now, remember our old slogan that conditional models and
conditional probabilities are no different than ordinary
probabilities, except that we live now in a new universe
where the new information has been taken into account.
So if you use that philosophy and you're asked to minimize
the squared error, but now that you live in a new universe
where x has been fixed to something, what would the
optimal solution be?
It would again be the expectation of theta, but
which expectation?
It's the expectation which applies in the new conditional
universe in which we live right now.
So because of what we did before, by the same calculation,
we would find that the optimal estimate is the expected
value of theta, but the optimal estimate that takes into
account the information that we have.
So conclusion, once you get your data, if you want to
minimize the mean squared error, you should just report
the conditional estimation of this unknown quantity based
on the data that you have.
So the picture here is that theta is unknown.
You have your apparatus that creates measurements.
So this creates an x.
You take an x, and here you have a box that does
calculations.
It does calculations and out, it spits out the conditional
expectation of theta given the particular data that you
have observed.
And what we have done in this class so far is, to some
extent, developing the computational tools and skills
to do this particular calculation, how to calculate
the posterior density for theta, and how to calculate
expectations, conditional expectations.
So in principle, we know how to do this.
In principle, we can program a computer to take the data and
to spit out conditional expectations.
Somebody who doesn't think like us might instead design a
calculating machine that does something differently and
produces some other estimate.
So we went through this argument, and we decided to
program our computer to calculate
conditional expectations.
Somebody else came up with some other crazy idea for how to
estimate the random variable.
They came up with some function g, and they programmed it,
and they designed a machine that estimates thetas by
outputting a certain g of x.
That would be an alternative estimator.
Which one is better?
Well, our own, we convinced ourselves that this is the
optimal one in a universe where we have fixed the
particular value of the data.
So what we have proved so far is a relation of this kind.
In this conditional universe, the mean squared error that I
get, I'm the one who's using this estimator, is less than
or equal than the mean squared error that this person will
get, the person who uses that estimator.
For any particular value of the data, I'm going to do better
than the other person.
Now, the data themselves are random.
If I average over all possible values of the data, I should
still be better off.
If I'm better off for any possible value of x, then I
should be better off on the average over all possible
values of x.
So let us average both sides of this quantity with respect to
the probability distribution of x.
If you want to do it formally, you can write this
inequality between numbers as an inequality between random
variables.
And it tells that no matter what that random variable turns
out to be, this quantity is better than that quantity.
Take expectations of both sides, and you get this
inequality between expectations over all.
And this last inequality tells me that the person who's using
this estimator, who produces estimates according to this
machine, is going to have on the average over all less than
or equal to the mean squared estimation error that's less
than or equal to the estimation error that's produced by the
other person.
In a few words, the conditional expectation estimator is the
optimal estimator.
It's the ultimate estimating machine.
That's how you should solve estimation problems and report
a single value if you're forced to report a single value and
if you're interested in estimation errors.
OK, well, we could have told you that story, of course, a
month or two ago.
This is mostly about interpretation, about
realizing that conditional expectations have a very nice
property.
But other than that, any probabilistic skills that come
into this business are just the probabilistic skills of being
able to calculate conditional expectations, which you already
know how to do.
So conclusion, all of optimal Bayesian estimation just
means calculating and reporting conditional expectations.
Well, if the world was that simple, then statisticians
wouldn't be able to find jobs if life is that simple.
So real life is not that simple.
There are complications, and that perhaps makes their life a
little more interesting.
So some complications that come into the work of a Bayesian
statistician is in the first place.
Before it gets to the complications, one complication
is that we deal with vectors instead of just single random
variables.
I use the notation here as if x was a single random variable.
In real life, you get several data.
Does our story change?
Not really, same argument.
Given all the data that you have observed, you should still
report the conditional expectation of theta.
But what kind of work does it take in order to report this
conditional expectation?
One issue is that you need to cook up a plausible prior
distribution for theta.
How do you do that?
In a given application, this is a bit of a judgment call.
What prior will you be working with?
And there's a certain skill there of not making silly choices.
A more pragmatic, practical issue is that this is a formula
that's extremely nice and compact and simple that you can
write with minimal ink.
But behind it, there could be hidden a huge amount of
calculation.
So doing any sort of calculations that involve
multiple random variables really involves calculating
multidimensional integrals.
And multidimensional integrals are hard to compute.
So implementing actually this calculating machine here may
not be easy, might be complicated computationally.
It's also complicated in terms of not being able to derive
intuition about it.
So perhaps you might want to have a simpler version, a
simpler alternative to this formula that's easier to work
with and easier to calculate.
So we will be talking about one such simpler
alternative next time.
So again, to conclude, at a high level, Bayesian
estimation is very, very simple, given that you have
mastered everything that has happened in this course so far.
There are certain practical issues.
And it's also good to be familiar with the concepts and
the issues that, in general, you would prefer to report the
complete posterior distribution.
But if you're forced to report a point estimate, then
there's a number of reasonable ways to do it.
And perhaps the most reasonable one is to just
report the conditional expectation itself.
