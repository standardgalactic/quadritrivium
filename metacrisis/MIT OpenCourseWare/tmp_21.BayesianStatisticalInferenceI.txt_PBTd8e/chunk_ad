continuous case is much less compelling than the rationale
that we had in here.
So in this case, reasonable people might choose different
quantities to report, and a very popular one would be to
report, instead, the conditional expectation.
So I don't know what theta is.
Given the data that I have, theta has this distribution.
Let me just report the average over that distribution.
Let me report the center of gravity of this figure.
And in this figure, the center of gravity would probably be
somewhere around here, and that would be a different
estimate that you might choose to report.
So center of gravity is somewhere around here, and
this is the conditional expectation of theta, given
the data that you have.
So these are two, in some sense, fairly reasonable ways of
choosing what to report to your boss.
Some people might choose to report this.
Some might choose to report that.
And a priori, there's no compelling reason why one
would be preferable than the other one, unless you set
some rules for the game, and you describe a little more
precisely what your objectives are.
But no matter which one you report, a single answer, a
point estimate, doesn't really tell you the whole story.
There's a lot more information conveyed by this posterior
distribution plot than any single number
that you might report.
So in general, you may wish to convince your boss that it's
worth their time to look at the entire plot, because that
plot sort of covers all the possibilities.
It tells your boss, most likely, who are in that range,
but there's also a distinct chance that our theta happens
to lie in that range.
Now let us try to perhaps differentiate between these
two and see under what circumstances this one might
be the better estimate to perform.
Better with respect to what?
We need some rules, so we're going to throw in some rules.
And as a warm-up, we're going to deal with the problem of
making an estimation if you had no information at all,
except for a prior distribution.
So this is a warm-up for what's coming next, which would be
estimation that takes into account some information.
So we have a theta, and because of your subjective
beliefs or models by others, you believe that theta is
uniformly distributed between, let's say, 4 and 10.
You want to come up with a point estimate.
Let's try to look for an estimate, call it c, in this
case, I want to pick a number with which to estimate the
value of theta, and I'm going to, I will be interested in the
size of the error that I make.
And I really dislike large errors, so I'm going to focus
on the square of the error that I make.
So I pick c, theta has a random value that I don't know,
but whatever it is, once it becomes known, it results
into a squared error between what it is and what I guessed
that it was.
And I'm interested in making a small error on the average,
where the average is taken with respect to all the possible
and unknown values of theta.
So the problem, this is a least squares formulation of the
problem, where we try to minimize the least squares error.
How do you find the optimal c?
Well, we take that expression and expand it, and it is using
linearity of expectations, expected for the square minus
2c expected theta.
Plus c squared.
That's the quantity that we want to minimize
with respect to c.
Take the derivative, to do the minimization, take the
derivative with respect to c and set it to 0.
So that differentiation gives us from here minus 2 expected
value of theta plus 2c is equal to 0.
And the answer that you get by solving this equation is that
c is the expected value of theta.
So when you do this optimization, you find that
the optimal estimate, the thing you should be reporting, is
the expected value of theta.
So in this particular example, you would choose your
estimate c to be just the middle of these values, which
would be, yes, 7.
And in case your boss asks you, how good is your estimate?
How big is your error going to be?
What you could report is the average size of the
estimation error that you are making.
We picked our estimate to be the expected value of theta.
So for this particular way that I'm using to do my
estimation, this is the mean squared error that I get.
And this is a familiar quantity.
It's just the variance of the distribution.
So the expectation is the best way to estimate a quantity
if you're interested in the mean squared error.
And the resulting mean squared error is the variance itself.
How will this story change if we now have data as well?
Now, having data means that we can compute posterior
distributions or conditional distributions.
So we get transported into a new universe where instead of
working with the original distribution of theta, the
prior distribution, now we work with the conditional
distribution of theta given that the
data that we have observed.
Now, remember our old slogan that conditional models and
conditional probabilities are no different than ordinary
probabilities, except that we live now in a new universe
where the new information has been taken into account.
So if you use that philosophy and you're asked to minimize
the squared error, but now that you live in a new universe
where x has been fixed to something, what would the
optimal solution be?
It would again be the expectation of theta, but
which expectation?
It's the expectation which applies in the new conditional
universe in which we live right now.
So because of what we did before, by the same calculation,
we would find that the optimal estimate is the expected
value of theta, but the optimal estimate that takes into
account the information that we have.
So conclusion, once you get your data, if you want to
minimize the mean squared error, you should just report
the conditional estimation of this unknown quantity based
on the data that you have.
So the picture here is that theta is unknown.
You have your apparatus that creates measurements.
So this creates an x.
You take an x, and here you have a box that does
calculations.
It does calculations and out, it spits out the conditional
expectation of theta given the particular data that you
have observed.
And what we have done in this class so far is, to some
extent, developing the computational tools and skills
to do this particular calculation, how to calculate
the posterior density for theta, and how to calculate
expectations, conditional expectations.
So in principle, we know how to do this.
In principle, we can program a computer to take the data and
to spit out conditional expectations.
Somebody who doesn't think like us might instead design a
calculating machine that does something differently and
produces some other estimate.
So we went through this argument, and we decided to
program our computer to calculate
conditional expectations.
Somebody else came up with some other crazy idea for how to
estimate the random variable.
They came up with some function g, and they programmed it,
and they designed a machine that estimates thetas by
outputting a certain g of x.
That would be an alternative estimator.
Which one is better?
Well, our own, we convinced ourselves that this is the
optimal one in a universe where we have fixed the
particular value of the data.
So what we have proved so far is a relation of this kind.
In this conditional universe, the mean squared error that I
get, I'm the one who's using this estimator, is less than
or equal than the mean squared error that this person will
get, the person who uses that estimator.
For any particular value of the data, I'm going to do better
than the other person.
Now, the data themselves are random.
If I average over all possible values of the data, I should
still be better off.
If I'm better off for any possible value of x, then I
should be better off on the average over all possible
values of x.
So let us average both sides of this quantity with respect to
the probability distribution of x.
If you want to do it formally, you can write this
inequality between numbers as an inequality between random
variables.
And it tells that no matter what that random variable turns
out to be, this quantity is better than that quantity.
Take expectations of both sides, and you get this
inequality between expectations over all.
And this last inequality tells me that the person who's using
this estimator, who produces estimates according to this
machine, is going to have on the average over all less than
or equal to the mean squared estimation error that's less
than or equal to the estimation error that's produced by the
other person.
In a few words, the conditional expectation estimator is the
optimal estimator.
It's the ultimate estimating machine.
That's how you should solve estimation problems and report
a single value if you're forced to report a single value and
if you're interested in estimation errors.
OK, well, we could have told you that story, of course, a
month or two ago.
This is mostly about interpretation, about
realizing that conditional expectations have a very nice
property.
But other than that, any probabilistic skills that come
into this business are just the probabilistic skills of being
able to calculate conditional expectations, which you already
know how to do.
So conclusion, all of optimal Bayesian estimation just
means calculating and reporting conditional expectations.
Well, if the world was that simple, then statisticians
