So, hello everyone, welcome to this second webinar of the IEEE Signal Processing Society
Autonomous System Initiative, just a quick introduction.
So, today we're going to have a Professor Karl Friestung, just some advertising say for the next talk.
We are going to have in this series, in January, on 30, we have a Professor Cavallaro from Wimbledon University of London.
And on February 28, we're going to have Professor Axel Jancz from Latvian.
So, I also would like to remind you that we are going to have a workshop,
organized in conjunction with the IEEE ICASP 2023, that will be in June in Rhode Island, in Greece.
Of course, we are currently organizing new, very interesting, hopefully, talks, webinars for the upcoming months.
So, the idea is to have one talk per month.
So, let me briefly introduce Professor Friestung.
I think everybody know Professor Friestung, anyway, you know, is one of the most influential neuroscientists in history, say.
He was cited up to now over 300,000 times, is for sure an authority on brain imaging and theoretical neuroscience,
especially in the use of physics-inspired statistical methods to model neuroimaging data and other random dynamical systems.
He is the key architect of the free energy principle and active inference.
Actually, the title of his talk today will be active inference.
So, I kindly ask you all to keep silent during the presentation.
That was going to last for around one hour.
We're going to have 30 minutes after the presentation for Q&A.
And you are, of course, encouraged to put the questions after the presentation.
Thank you.
Thank you, Carl.
The floor for you.
Well, thank you very much for the introduction.
And it's quite easy to be the most influential neuroscientist in history,
because neuroscientists were invented about 10 or 20 years ago.
So, slightly cheating, but thank you again for that nice introduction.
Thank you for the opportunity to speak to you.
And particularly, thank you for attending, given it's so close to Christmas.
So, I should apologize.
I'm not going to really be talking about signal processing.
What I'm going to try to do is describe a mechanics for making sense of sequential or ordered data
of the kind that the brain might use and to try and develop that mechanics
from a first principle account of sense making and indeed decision making.
And that is, as we have already heard, active inference.
And I'm going to call active inference self-evidencing.
And I'll try and explain what I mean by self-evidencing.
I'll overview the different perspectives on self-evidencing in the first slide,
but then just rehearse the underlying imperatives and objective functions
from the point of view of people in artificial intelligence research and machine learning.
And then what we'll do is we'll drill down into the mechanics of the belief updating
and the self-evidencing using numerical studies simulations.
And at the end, I will try to take on the challenge of simulating a very simple form of reading.
Or sampling data sequentially and trying to make sense of those data.
So self-evidencing, what do I mean by that?
Well, the idea here, the basic premise is that everything we do, everything that we think
and every act that we deploy on the world can be cast as a form of inference.
So perception is perceptual inference, and I'm going to talk about action as planning as inference.
And the imperative for both perception and action is to optimize beliefs about unknown states of the world.
Say, for example, S here that are generating observations O.
And I'm going to denote those beliefs by Q.
Similarly, we're going to have random variables, U control variables, actions about which we have beliefs
that we're going to optimize with respect to a function of those beliefs Q,
given a certain action or choice or decision.
So framing the problem in this way means that I'm going to ask everything as belief updating,
optimizing beliefs about S and U here, namely the Q here,
with respect to a functional, a variational free energy functional of beliefs about states
and an expected free energy functional G of beliefs about what I am doing, my actions.
So just to give you an intuition as to the nature of these functional, the variational free energy, for example.
I've just written down various ways of interpreting this free energy functional.
So technically, it's what's called an evidence lower bound or an evidence upper bound in this instance,
that provides a bound or an approximate equality to the log of the probability of some outcomes given me,
given some system that's sampling those outcomes.
And typically, those outcomes that have a high probability of me sampling will be those that characterize me as a phenotype,
me as an agent, me as an artifact.
And in that sense, they are, by definition, those kinds of outcomes that I would typically or characteristically want to sample those preferred outcomes.
So they are intrinsically valuable in the sense that they are the kind of outcomes that I would expect to, expect to encounter.
And read in that way, we can now interpret this variational free energy in terms of a value function of outcomes.
And from that, we can understand the imperatives behind reinforcement learning, optimal control theory and engineering.
And if I was an economist, that would correspond to expected utility theory.
Another perspective is afforded by information theory.
So the negative of this log probability is known as self information and information theory or surprise or more simply surprise,
namely the implausibility that I would encounter or observe these particular outcomes.
And therefore, by optimizing the free energy of this value here, I'm implicitly minimizing my surprise.
And I can understand that in terms of the max principle, the principle of maximum mutual information,
alternatively or equivalently, the principles of minimum redundancy and indeed the free energy principle.
And that's nice because the average of the self information is entropy, which means that this maximization here,
corresponds to minimizing the entropy of my sensory exchanges with the world or the outcomes that I encounter.
And of course, that's the holy grail of self organization in the physics and non equilibria.
For example, in synergetics, and if I was a physiologist, it would just be a statement of homeostasis,
keeping my outcomes within viable bounds, treating those outcomes as essential variables, basically,
and stopping them dissipate or diverge into regimes that would not be characteristic of me and not be consistent with my physiological survival.
There's a final interpretation here, which licenses the notion of self evidencing.
Instead of this M representing me, it could represent me as a model of how my observations were generated.
And on that reading, this quantity here, the probability of some observable data given a model can be understood in terms of Bayesian model evidence
and therefore trying to maximize the Bayesian model evidence.
I can now understand this imperative here in terms of the Bayesian brain hypothesis.
I can understand it in terms of data assimilation and evidence accumulation,
as if incident things like predictive coding and compressing sound files or indeed as an explanation for message passing in the brain.
So that's the reason I'm calling itself evidencing action and perception in the service of maximizing the evidence for my models of how those data were generated.
We can now regard then perception as belief updating and planning as inference now becomes a description of choosing the right, the most appropriate or self evidencing ways forward, namely gathering those data which supply evidence for my existence.
Lots of ways of understanding the nature of this free energy function.
I've just written it down here in full in terms of a negative energy and an entropy term where crucially the entropy pertains to the entropy of my beliefs about latent states generating data.
And the negative energy has two terms here often referred to in terms of the likelihood of certain observations given the latent causes of states and the prior probability of those states before observing the data.
And written down like that, you can see that this principle of optimizer extremizing the free energy just is James's maximum entropy principle under constraints.
And those constraints are supplied by the negative energy, namely my generative model that can be decomposed into a likelihood and a prior.
So this construct is nice. Here I've written it down using an expression which will be more familiar for people in machine learning.
So here I've just rearranged the terms and expressed it in terms of an expected log evidence and a KL divergence between my beliefs about the states generating my observations and the underlying posterior beliefs of those states given some observations.
And because this quantity here can never be less than zero, what we have is F is always going to be less than it's going to be a lower bound on the log evidence and hence an evidence low bound or an elbow of the sort you'd see or use in things like variational auto encoders.
The importance of having a generative model, I think, should be emphasized here.
Because you're optimizing the evidence for a particular model, you always have a generative model, the likelihood and the prior in mind or explicitly articulated, which means that once you've optimized it, you always have explainable artificial intelligence because you know how these data were caused under your generative model.
What we will see later is that this also provides or prescribes the optimal kind of data that you might want to go and gather or mine in terms of minimizing your uncertainty about the model, namely maximizing the evidence for your model, leading to notions of design
optimality in terms of designing the right data mining, moving from big data to smart data, leading to notions of smart data foraging epistemics of the kind that might be associated with a generalized artificial intelligence.
Here's another way of writing down this free energy functional, this variational free energy.
And I've written it here in a way that a statistician would understand it. And again, just by rearranging the terms, I can now express it as a mixture of accuracy and complexity, where accuracy is just the goodness of fit.
So I've expected log likelihood of some outcomes or observables or data expected under my beliefs about how they were caused in terms of these latent states.
And then we have this term, this complexity term, which is the divergence between my posterior or approximate posterior beliefs Q out of my prior beliefs.
So this complex term is very important, it scores literally the degree to which I change my mind on observing some data. It's the information gain that it actually plays the role of a cost in terms of maximizing the evidence.
So what that means is if I'm going to maximize my evidence, I'm going to conform to Occam's principle, I am going to try to find the simplest explanation that retains an accuracy, an accurate but simple explanation.
So to get the balance between the accuracy and the complexity right, thereby maximizing the evidence.
This is interesting from the point of view of implementation and various ways of framing the cost of a particular explanation.
I can read this complexity as a complexity or computational cost, leading to notions of bounded rationality and approximate Bayesian inference.
It also has implications from the point of view of engineering and the Van Neumann bottleneck and the like, in the sense that, given Landau's principle, that this complexity provides a lower bound on the amount of thermodynamic energy I require to do my belief updating.
So to put this very simply, if you're doing it in the best way possible, you're doing it in the quickest and most energy efficient way possible.
Another way of thinking about this decomposition into accuracy and complexity is to think about the implications of what you are trying to optimize when it comes to the future consequences of a decision or an action.
And this is basically the main message of this presentation that exactly the same constructs become very useful when thinking about the imperatives for selecting or inferring what plan or policy I'm going to pursue.
And actually what we're going to see is that the accuracy that I would expect if I committed to a particular plan becomes exactly the same imperative that underlies optimal Bayesian design, namely that which maximizes the information gain.
In a similar way, the expected complexity becomes something called risk, which is exactly the same quantity that underwrites Bayesian decision theory, where we're reading now loss functions in terms of expected surprise, defined in terms of that value that we started with.
And the two together basically can be regarded as self evidencing. So rest of the talk really is just unpacking this slide this observation and illustrating it using numerical studies and some neurobiological examples, but to motivate it more intuitively.
I'm going to ask you to think about this problem. Imagine you're an owl, and then you're hungry. And then if I was there in person, I would ask somebody usually on the front row, what are you going to do.
And they generally respond quite correctly, well I'm going to search my food I'm going to look for my food and they'd be absolutely right that would be the first thing that a somebody predator that was hungry would do.
And that answer.
As within it to quite fundamental implications and I want to illustrate those by deliberately comparing contrasting to sorts of mechanics or formalisms that you might want to use in order to specify optimal behavior or choices.
What I will do is actually repair this dialectic like later on and show that one is a special case of the other, but for the moment I want to draw a bright line between two different kinds of formulations.
The first depends upon the notion of a value function of the states of the world that would follow if I acted in this way you sub tour at the present time.
And if this value function exists, then I can simply choose for any given current state the action that maximizes the value, and I could therefore create a state action policy pie that returns the best thing to do for every given state that I find myself in.
However, this kind of approach won't work in terms of searching for your food. And that's almost self evident because searching for your food is the action that you would take to reduce your uncertainty about where your food is located.
But uncertainty is an attribute of your beliefs about the world, not the actual state of the world. So that tells you immediately the optimal action has to be not a function of states of the world, but a functional function of a function of beliefs about states of the world.
This tells you the optimal action depends upon beliefs about states as opposed to states per se. Also searching for food tells you something else.
It means that the order in which you do things matters. It matters whether I try to eat my food and then search for it as opposed to trying to search for it and then trying to eat it.
Which means that there's a different kind of construction here for the optimal policies, a sequential policy, which means that I now have to consider the function, not just of any belief about the current state but all states into the future under a given sequence of actions.
And I've written it down like that. So that I can now refer to this G which is going to be an expected free energy as an energy functional, which means that the sum of the path integral over time is now going to be an action as would be called by a physicist, a path integral or a time integral.
of energy here, namely action. So this kind of construction in which we also now have to optimize our beliefs about the consequences of our actions by maximizing the free energy.
And can be best described and are simply described in this instance as a principle of stationary action and in this case, maximizing this evidence lower bound, and it's expected version, the expected free energy G here.
So I introduced that just to contrast it with the Bellman optimality principle that would attend this kind of construct here. And I'm sure a lot of you will be familiar with lots of instances of
schemes that operate under the Bellman optimality principle, control theory, dynamic programming, deep RL based in decision theory and so on.
And then the equivalent schemes under this principle of stationary action would be the free energy principle itself and active inference.
And what we will see is notions of artificial curiosity and in robotics, for example, intrinsic motivation that speak to resolving uncertainty, the epistemics that underlie optimal Bayesian design and crucially sequential policy optimization.
We're talking about ads and trajectories here, as opposed to states that ensue point of time by time, moving from Markov decision processes, for example, to partially observed Markov decision processes where beliefs about unobservable or latent states start to figure much more
than actually in the maths. So, and just to rehearse now the functional forms of these variation free energies and expected free energies I've just written them out again here.
So if you don't like or remember all the maths, please don't worry about the equations here. I'm using them almost iconically, just to illustrate what I think is a very beautiful symmetry between the variation free energy which we're trying to optimize in
relation to our beliefs about states generating data, and the expected free energy G that is a functional of both of those beliefs and the policies that I would entertain in terms of how I'm going to move forward into the future.
So here's again is the free energy written in terms of a mixture of accuracy and complexity that can be rearranged very simply to demonstrate that it is an evidence lower bound, given this KL divergence can be never be less than zero.
And here are here is the equivalent expression for the expected free energy, and it's called expected simply because we're taking an expectation under the predicted outcomes that I would encounter if I pursued this policy.
So we're conditioning now upon a policy, but the functional form remains very similar, where we can see that the complexity becomes risk, the inaccuracy becomes ambiguity, and the divergence and the evidence become expected information gain and cost or negative cost, respectively.
So let me just unpack that very briefly related to things which I'm sure many of you will be familiar with. And let's just look at the components of the expected free energy.
And what I'm going to do is I'm going to take various sources of uncertainty off the table and see if I can get back to the Bellman optimality like formulation of optimal behavior.
But let's ignore, in the first instance, the expected value or if I multiply by minus one the the surprise or the expected cost.
So I prefer everything. I have no particular preferences. So all I'm left with now is this quantity here. So what is this quantity? Well, it's just the KL divergence between beliefs about states of the world.
Given some observations that I would get in the future if I pursued some policy relative to those beliefs without those observations.
So this is just the expected information gain. It's just the degree to which I have resolved my uncertainty by getting those data, if I pursue this policy.
And in the neuroscience is in particular the visual search literature, this is known as expected Bayesian surprise. And for those of you in information, say you recognize this immediately as simply the mutual information between causes and consequences, the
latent causes of observable consequences conditioned upon a particular policy.
So let me now take one kind of uncertainty out of the game. And let's ignore the ambiguity. What does that mean? Well, it just means that there is no uncertainty about the states of the world, given some observations and vice versa.
Which means I can replace all the S's with O's and the risk term now reduces to either the KL divergence between Q beliefs about states or future observations, which are now random variables because they haven't occurred yet, and my prior beliefs.
So what does that mean? Well, it just means that the risk is the divergence between what I anticipate will happen and what a priori I prefer to happen.
So it scores the divergence between what I think is going to happen and what I prefer to happen. And in engineering, this is known as KL control and economics, it could also be read as a form of risk sensitive control in the sense that there is no ambiguity.
Finally, let me remove all reducible uncertainty and assume that there is no more information to be gained. I know everything that I can know about the consequences of my action and the hidden or latent states of the world.
And I've just left with the expected cost or the negative expected value or utility. And of course this takes us right back to the Bellman optimality principle and expected utility theory where because there is any uncertainty, I don't have to worry about beliefs, and I can now deploy a standard reinforcement learning or Q learning or any other scheme that rests upon the Bellman optimality principle.
So in summary, what we're saying is that this expected free energy, this expected bound on the evidence for my models of how the world works, of the sensed world can be decomposed into expected value and expected information gain.
And where expected value is just the objective function that underwrites the Bayesian decision theory and leads to optimal Bayesian decisions under uncertainty given some prior preferences or cost functions.
In addition, I'm going to do this whilst exploring in the sense of maximising my expected information gain. And this is the Bayes Optimal Experimental Design Imperative. Basically, if I had to choose certain data points, what data points would I choose to be most informative and resolve the most
uncertainty about my hypotheses or my model of how those data were generated. Exactly the same kind of objective function you'd see in active learning. And together we have essentially Bayes Optimal Decision Theory and Bayes Optimal Design constituting active inference and learning.
So I'm going to spend the remaining part of the presentation just, you know, providing you with a couple of work examples and deal with one of the more interesting kinds of generative models under which we can self evidence, namely, deep,
temporarily deep generative models. But before I go then, let me just rehearse the foundations of the kind of modelling that we use to try and understand the message passing and the implementation of this kind of self evidencing.
Generally, what we do is work with Markov Decision Processes, which we can write down in a very simple way in terms of probabilistic graphical models. So for example, here, again, please ignore the equations, everything that we need to know is in graphical form here.
And what we're assuming is that any given world is a world generates outcomes or observables from latent states via likelihood mapping usually denoted by a, and the dynamics in this world are parameterized or modeled in this
graphical model by transition tensors or matrices that shift every time step the state of the world into the next state, where crucially, these transitions depend upon action, namely the policy, where the policy itself depends upon the expected free energy that we have just rehearsed.
And the nice thing about this depiction of the structure of a generative model in this graphical form is that if there exists this graphical model, there is always, if you like, a conjugate or an equivalent graphical model that
is being called a factor graph in this instance, a normal style or for me style factor graph. And these are really useful. And basically what you do is you sort of switch the edges to the nodes and vice versa so now the nodes become factors, and the edges now correspond to beliefs or the sufficient
characteristics of various probability distributions that you've asked amongst the nodes in order to do the belief updating that maximizes the evidence low bound, and as I've written it down here.
And this is, if you like, standard off the shelf, belief propagation or variational message passing that any particular generative model would demand, simply because for every given generative model there is a factor graph and for every factor graph, you can now just go and get some off the
variational message passing that you know will maximize the evidence low bound or the variational free energy. And I've just written down the functional form of these things here to describe some of the messages so let's for example just take
beliefs about the, the latent state at this point in time and what we can see is that they receive messages from the observables the sensory evidence at hand, and messages from the future, and messages from the past that put constraints on the most likely state that is sandwiched between
them, and these messages are contextualized by the policy that I'm currently pursuing. And the functional forms here very remarkable similarity to the functional forms of equations that we're using computational neuroscience, when we try and simulate this kind of belief propagation and updating and
indeed learning in the, in the human brain. So we can, I've just rewritten these variational updates here that ensue from the message passing on the left hand side, and written down the corresponding message passing graph that we think might operate in a very simple picture of a brain.
And the interesting thing is, I repeat, these update equations are very, very simple, and also very similar to what we'd use in neuroscience so for example, we have to only update update our beliefs about things we don't know we only don't know the latent states and the policies, and I've
actually included here a softmax or precision parameter, reflecting the confidence in my beliefs about the what I'm going to do next gamma, or the inverse of gamma which is beta here, referred to as precision.
If we just focus on perception and policy selection, then perception just is updating expected expectations about states in the world that is a nonlinear function say a voltage current function of linear mixtures of observations representations expectations about the past and
the future that are mixed through with these tensors likelihood and prior tensors here that have a very simple parameterization to produce when when applying the softmax operator.
The optimal belief, the elbow or evidence maximizing belief about hidden state of the world.
The policy selection just reduces to a classical softmax response rule, predicated on the expected free energy.
It reduces to a canonical kind of associative plasticity with associative terms decay terms here, here illustrated for D which parameterizes the initial latent states and action selection as we have discussed, and simply follows from the policy
selection, namely selecting the next action or control variable from the most likely policy.
So with that we can simulate all kinds of behavior. I'll give you one brief example just to illustrate the most notable Arnold feature of the kind of behavior that ensues from this self evidence or active inference.
So here, imagine that there's a little mouse, and it is in a teammates and can make two moves, and the mouse wants to find a reward or a preferred outcome denoted here by the red ball that can either be on the left or the right of the upper arm.
And because the rat or sorry the mouse does not know where, whether the reward is going to be on the left or the right hand side, and can only make two moves and furthermore, once the mouse commits to a particular army has to stay there.
So we can gamble and go to the left and if it's right, it will spend two moves with the reward. If it's wrong, it will spend no moves with the reward so on average, it will be right 50% of the time.
I think that's interesting and emphasize the special role of beliefs and belief updating. We've introduced a third kind of outcome or cube in the bottom arm of the teammates, where the color of this instructional queue tells the mouse, where the reward is.
The mouse has got a choice. It can use its first move to go and look at the queue in the bottom of the maze, and then move directly to where it knows the reward is spending one move with the reward on every trial.
So still from the point of view of the expected reward, it's still 50% of the time with the reward. But of course, it's now much more confident about what's what is going to what is going to happen, and what it is going to do because it's responding to this expected
expectation game this epistemic affordance that is part of this expected free energy.
And that's basically what I want to show you the these graphics and matrices are just meant to illustrate how simple it is to write down these terms of transitions amongst various states, and the different policies, and the outcomes in terms of whether there is a reward at the different
levels of the reward. The prior preferences here just written down in terms of log probabilities or log odds ratios, and specifying that the reward that the preferences here are just to be with the reward.
And everything else is equally preferred. So what kind of behavior do we get when we integrate that message passing scheme that belief updating scheme.
Well, here what I've done is just illustrated the outcomes of some simulated trials 32 trials, where the colors of these circles here denote whether the reward was on the left or the right hand side.
The image here, the sort of gray bars here, describe the probability distribution over various policies so the policies are only two moves that go down to the left or down to the right or it could stay and go to the right or it could go down and then go back to the middle again.
So a finite number of sequences of actions, which the mouse is trying to evaluate in terms of the expected free energy, and then forming posterior beliefs that enable it to plan via inference about what it is likely to do next, and then select the most likely behavior.
These show the outcomes in terms of the expected reward or expected prior preference here and reaction times, and this is just the scoring the learning of the initial states the context was the reward on the right or the left.
And as one might expect when the mouse first starts the trial, it will always go and respond to this epistemic affordance engaged in this epistemic poraging before becoming exploitative and going to secure its reward.
But in this instance, we've played a trick on the mouse.
