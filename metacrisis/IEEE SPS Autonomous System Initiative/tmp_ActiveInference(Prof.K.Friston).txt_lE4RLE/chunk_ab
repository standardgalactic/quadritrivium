We've done after the first few trials, we've left the reward on the left hand side.
And that has an interesting implication, because as time goes on, the mouse learns that the reward is always on the left hand side, which means that there's less and less information gain afforded by going down to the instructional queue.
And the exploitative policy becomes less and less attractive, as expected information gain starts to fall below the expected value of going straight to the reward, which it is now, or the mouse is now fairly confident, it knows where the reward is.
Indeed, after trial 20, there's this switching behavior from expiration to exploitation, simply because the mouse has now become familiar with its environment, and there is no more reducible uncertainty.
And therefore, the epistemic affordance has given way to the expected utility, or the pragmatic affordance of going and sitting with its reward for two moves directly.
It's very generic behavior. It dissolves the exploration exploitation dilemma, simply by putting the two imperatives of exploration exploitation together in the same functional that underwrites the self evidencing and inherits from the functional form of this bound on model evidence.
I won't go into this in any detail. If I was speaking to a neuroscience audience all sorts of interesting things one can do now, because we've actually got a, an in silica or synthetic mouse, who was updating her beliefs and learning, we can now go and perform simulated neuroscience experiments we can look at the belief updating
through the lens of neural activity and look at firing rates associated with the chosen and chosen option of this done in neurobiology and electrophysiology, we can think of the representations as fulfilling the role of a working memory of a prospective
sort that changes as time marches on, and we can even go and record the activity of various representations of locations and simulate place fields, they sell like activity, and do all sorts of things including things like the mismatch negativity paradigm.
I want to close now by taking you to a slightly more interesting kind of generative model and illustrating the sorts of behaviors that one can simulate, and also briefly look at the corresponding neural correlates of this belief updating the self evidencing under deep generative models.
And so what do I mean by deep generative models well let's go back to that dual aspect rendering of a generative model in terms of a graphical model, in terms of Markov decision process, and it's corresponding or conjugate
graph describing the requisite message passing. What I'm going to do now is take this Markov decision process and replicated but place another one on top that crucially unfolds more slowly in time.
So the depth of these hierarchically composed Markov decision processes in this deep model is accompanied by a separation of time scales, so that the transitions at the higher level, provide a context for the faster transitions at the lower
level, basically destroying any simple Markovian property and rendering now the general model effectively semi Markovia or non Markovian by virtue of the separation of time scales.
And the way that this in this example, this is done is usually by identifying some state of the world that endures for the unfolding of the fastest states at the lower level.
And the first one obvious candidate is the initial state so the initial state is always the same. The first state is always the same.
During the unfolding of states or the state transitions of the path of trajectory during this epoch. So effectively the higher level is providing the initial conditions for the lower level.
So from the point of view of the factor graph, and this nicely illustrates the bottom up and top down message passing that inherits from the this deep architecture.
So from the point of view of the lower level, the top down level is actually providing empirical prior constraints that afford a context sensitivity to this little time epoch of exchange with the world.
And this context will change in the next epoch in a lawful way, as modeled by the, the higher level and message passing here.
So from the point of view of the higher level, the lower level is actually providing evidence that you are in this particular context, as opposed to that particular context.
So there's a mutual reciprocal exchange of sufficient statistics or beliefs that enable this non Markovian generative model of non Markovian outcomes that has this inherent time and context sensitivity.
To close, I just want to illustrate what that looks like in practice when you deploy these models, but to generate sequences of observables, and then try to recognize the underlying causes of those observations.
So what I'm going to do is present very briefly, a deep model of reading, a simple model of reading, in the sense that we're not talking about letters which talking about sort of an iconographic script, where each word is composed of different
letters and different arrangements on four points on a two by two grid here. So for example, if there's a bird icon next to a cat icon, the implicit word is free.
If the bird is next to some seeds, the implicit word, the semantic if you like, is feed, and if there's nothing there, we just wait.
And this would be a suitable kind of labelling of latent states that will be required to generate observed outcomes through simply sampling the letters of a word.
So what are those outcomes? Well, I just need to specify what the agent could see if it foveated one of these letters here, and it would see either nothing, some seeds, a bird, or a cat, and these are discrete outcomes.
The second outcome is where am I looking, a proprioceptive outcome. And if I knew exactly which word I was looking at, and where I was looking, so I was looking at one, and the word was feed, and I would see a bird, and I would feel myself looking at location one.
And that would be fit for purpose for a very simple generative model of epistemic foraging or epistemic expiration, just scanning the letters of a word.
And I would have to equip it with a deeper architecture that would be more apt to model reading.
What would I need to do to do that? Well, I would need to be able to generate the word that the agent is currently sampling, which means my hidden states would now have to become a sequence of words, namely a sentence.
I would need to know what the sentence was, and I'd need to know where in the sentence I was currently looking, at which word I was currently looking.
So, for example, if I knew that the sentence was free weight, feed, and weight, and I knew that I was looking at the second word, then that would be weight.
And therefore, I now know what word I'm looking at, and if I know where I was looking, I can now tell you exactly what I would see.
So this would be an appropriate generative model for a very simple kind of reading where the whole universe just contains these six sentences here.
And now the idea is, can we simulate evidence accumulation over separate, separable time scales that enables an agent to infer not just what word they are currently reading, but also the context,
namely the sentence that that word came for, and do that in real time using this belief update scheme or variational message passing scheme under this kind of deep generative model.
And indeed one can and these are the kind of results that ensue. And so, at the top here what I've shown are the four words of the sentence that the, the simulated subject was reading.
And the red dots correspond to actions as a kind of eye movements where she looked in the sentence together the right kind of information with the most expected information game that would resolve her uncertainty about what she was currently seeing
in terms of the first level words, second level sentences.
And what we see here is something which is very characteristic of the way that you and I read, namely, we jump very quickly to the most informative letters and icons that provide disambiguating information.
So, for example, the very first discard, she sees a cat, and therefore she knows that the first word must be flee because to see a cat means that there has the only word with a cat in is flee.
So she's resolved, there is no more resolvable uncertainty about remaining and sampling the remaining letters of this word. And so she jumps to the next word, which is possibly a weight and confirms that by sampling by chance to no outcomes,
she jumps to the next one, it's a bird that's a bit ambiguous, it could be a flee or feed, she confirms it's a feed by going to the second letter in that word, and as soon as she's resolved that uncertainty jumps to the end, and discovers that that is another weight word.
So here are the sort of the posterior beliefs that are being updated during time during these organic eye movements at the first level in terms of what are the letters that are sub tending beliefs or belief updating about the sentence, which is shown
in the second level in the upper row. And what one can see from this is that only at the end is there going to be a resolution of uncertainty about which sentence she was reading simply because it's just the last word that resolves that
uncertainty. Another way of showing those data of the synthetic data is in terms of looking at the posterior expectations or beliefs about different states, namely, in this instance, at the high level, which sentence am I looking at, and which word am I looking
at as a function of successive eye movements. And one can see immediately that the belief updating at the low level is much quicker.
So for the first, it's a card or two. I confirm that I'm looking at the letter flee, and then move to the next letter, and then it's, it's weight, or I accumulate my beliefs to believe that it is weight, and then move to the next, the next word here.
And in contrast, the belief updating about the word is much slower.
And indeed, because only these two sentences begin with flee immediately this subject or if I was doing this I would believe that there has to be either sentence one or four, but I can't resolve that uncertainty until the end because the only disambiguating word is at the last point that
is weight versus flee. As soon as I see that then I can resolve my uncertainty about which word I am looking at, one I've inferred that it is weight at the lower level.
These are the same data shown here, but what I've done here is just filter them using the same filters that we use in EEG or neurophysiology, electrophysiology research in neuroscience, just to illustrate the similarity between these, these patterns of synthetic neuronal representation
in neural firing, or changes in neural firing as reflected in local field potentials and electrographic cortigraphic signals.
And I'm showing a couple of examples in closing and that speak to this kind of pattern of neuronal responses that we presume is reflecting the same kind of belief updating a message passing in the simulation.
For example, here, if we treat these as raster plots of neural firing, they look very similar to empirical pre-psychadic delay period activity in the prefrontal cortex, while these fluctuations in the filtered time series look very similar to periscadic field
potentials during active vision, showing these characteristic fast responses at the lower level here in visual area number two, and these slower responses of reflecting this belief updating at a high level here, the area TE in a monkey.
So again, this is not terribly interesting from the point of view of signal processing, but it is interesting that the same dynamics on these structured graphs, suggested by this off the shelf kind of belief propagation or message passing looks or produces patterns of activity of the kind that we can actually see in the functioning brain.
So I will summarize everything that I just said in a much more concise way by borrowing from the words of Helmholtz.
Each movement we make by which we alter the appearance of objects should be thought of as an experiment designed to test whether we have understood correctly the inherent relations of the phenomena before us.
This is their existence in definite spatial relations. And with that, it only remains for me to thank those people whose ideas I've been talking about, and of course to thank you for your attention, thank you very much indeed.
Thank you. Thank you, Carla.
Very interesting.
Very inspiring talk. Thank you so much. So we have, of course, time for questions. So, please, it's great to open your microphone, put questions, raise your hand, or write in the chat.
I have a question. Hello. Hi, please.
I have a question.
Carl, are you able to integrate your friend?
Hello, can you hear me? Conscious perception versus unconscious perception. So can you model consciousness in that framework?
Great question. I think if you can, it depends what sort of level of consciousness you'd be happy with.
Whether you perceive something consciously, so you can see something where you don't perceive it, versus you see it and you perceive it as well, like there might be something in your visual scene, you don't actually realize it, it's there.
Yeah, I think there are probably two levels. I mean, certainly from the point of view of Herman Hellholtz and the notion of unconscious inference as visual perception, then certainly this is exactly stealing and implementing those, you know, those ideas in modern
sense, they believe in message-passing schemes. So I'd certainly say that your unconscious perceptual inference is exactly what's going on here.
You could argue that the posterior belief would be the content of a percept, and that's sort of very simple-minded sort of mapping between the mathematical quantities.
So if you believe in the beliefs of a mathematical sort to a perceptual belief, then yes, you could argue that there is a kind of consciousness, but I think what you're talking about is the qualitative experience of the percept, that I know that I am seeing red, or I know that this word is, I think that's a lot more subtle.
I think you can model it in this kind of framework, but only under very special conditions, and those special conditions mean you have to have a kind of mental action that involves updating the precision of the beliefs that render the form of the
message-passing dependent upon your internal action state. So if this was a predictive coding scheme, then that would correspond to the Kalman gain, for example.
If you now get control over the Kalman gain, then that control can be read as a kind of internal action, and at that point, then you become an agent of your perception.
And I believe that that's the closest you can start to get to having a phenomenal qualitative experience. You have to have a change, an active change in the Kalman gain, or the precision, or the attention that you're affording these particular messages as opposed to those particular messages.
You start doubting your perception, like you don't believe what you're seeing, and then maybe you doubt yourself what's happening, and don't believe what I'm seeing, and yeah, maybe that's the issue then, yeah.
Yes, yeah, and of course, and then of course that would change your attention if you thought, did I see this or didn't I see that, and then you would focus, and again, without speaking to this mental action that you have to have this sort of explicit attentional aspect.
So a vanilla Kalman filter, I don't think would have that, but you could engineer, and people are trying to do this, you could actually engineer this sort of dynamic estimation of the precision of these mappings, and I think you could be getting closer to conscious perception.
It wouldn't be self-conscious, but it may have the mechanisms that would be sufficient for that phenomenal consciousness.
I guess unconscious perception doesn't change your actions, or it's not so clear.
No, I think it can, I think reflexes, certainly homeostatic reflexes would be modelled like that.
In fact, interestingly, you're often not aware of those reflexes simply because they are subject to psychological or physiologically, something called sensory attenuation, which physiologically just means you ignore those signals
by reducing the synaptic efficacy. Mathematically, it would be in a predictive coding formulation of the message passing or a Bayesian filter.
It would be like switching the Kalman gate to zero temporarily, and then what happens is that the prediction errors are forced back out to engage reflexes, autonomic reflexes on the muscle.
So I would imagine that quite a lot of active inference, certainly in the interceptive domain, actually is completely sub-personal and completely automatic.
And it's just those things that we can't explain that suddenly require us to attend to and to think about that we would actually perceive.
That's a good point.
Thank you.
Hello, I have a question.
Can I ask?
Yes, please, please.
Okay.
First, thank you very much for the very interesting talk.
When you were talking about the decision of mouse, you shortly mentioned about gambling. So is there any theoretical connection between gambling theory in particular Kelly criterion and this active inference?
I have never been asked that question before.
Certainly, active inference or the formalism that I just described under these Markov decision processes has been used to address the multi armed bandage problem as your economical decision theoretic problem in a Bayes way, a Bayes optimal fashion.
So gambling and choice behavior under uncertainty is exactly what this kind of scheme is meant to deal with because, you know, because I repeat, once you put uncertainty quantification into the game, you have to deal with belief updating and the mechanics of
sort of passing messages about belief, hence the, you know, the photograph and the variational message passing.
In a sense, active inference is all about gambling.
However, if you meant gambling from the point of view of a pathological behavior, and the prior beliefs that people who become addictive, for example, get engaged with, then I think that's a slightly more searching question.
And usually, and indeed, it's the kind of question that my colleagues, indeed at me, my colleagues in psychiatry and psychology become very exercised by one interesting aspect of applying active inference to abnormal behavior and psychopathology
read as false inference or odd kinds of influences is something called the complete class theorem, which really comes from sort of statistics.
And what that says is for any pair of behaviors and loss functions, there are always some private beliefs that render you Bayes optimal.
So what that means is, you can always describe any given behavior under any given loss function in terms of one of these active inference schemes.
But that explanation may requires you to actually identify the priors that make this behavior Bayes optimal, which basically means that you can, you know there exists some private beliefs that describe uniquely this person's behavior.
So sometimes people are interested in fitting these active inference choice behavior models to individuals behavior, and then trying to adjust the prize in the model in order to match the behavior of the synthetic model the digital twin if you like, to the real person.
And then, and try to understand what this person's priorities were that cause them to gamble in this kind of way. And it's a really interesting field because you know there's anecdotal evidence from psychophysics and psychology that people say with schizophrenia jump to conclusions.
So that would suggest that they have imprecise prior beliefs or that they may be unable to wait until more definitive evidence arises, or they may be certain conditions associated with an inability to decrease the precision of sensory input and that would
mean that their priorities means that they would be always attending to sensory input and that's been used as an explanation for certain conditions like autism and the like.
So that, you know, you can, you can use this machinery to try and emulate certain kinds of behavior such as gambling behavior, and, and do that in a quantitative and formal way by explaining somebody's choice behavior in terms of their
prior beliefs, a particular point point in the model.
So I'm not quite sure which, which, which sort of gambling you were talking about but does that answer your question, roughly. Yeah, so my question was more the technical part that you mentioned at the beginning that it is actually the same so the active inference.
The same concept of them. So Kelly character and actually makes the best decision based on some beliefs in the gambling story.
Yes, as you mentioned, so it's the same. Okay, thank you.
There's a gentleman called Dimitri Markovich, I think in two ways you know you look at Dresden Dresden I think is in Dresden, who spent a lot of time looking at sort of various UTC and Thompson sampling in the context of multi armed bandit and gambling problems and trying to cast that in the
in the most general way, where usually these active inference models are the most general way of framing the problem. Of course, there are lots of interesting issues about deep three searches in the panning side, which usually discriminate one screen from another
scheme, but certainly the mechanics of it will be formally identical.
Thank you.
Thank you.
Other questions.
I have a question.
Please.
I have a call for a comprehensive sentences of active inference from the fundamental basis. However, my question is tailored towards active inference for continuous state space, most especially where the state space is random in nature, which demands random actions, probably.
Now we learn an optimal prior.
So the optimal prior include evidence from the past and evidence from the from the future, or evidence of the uncertainties that we don't know.
How do we learn a model of the uncertainties that we do not know, especially in random environments that at each instance in time, what you learn as optimal prior during action selection might be different.
If the agent is faced with with with a surprise that was not captured in the optimal prior.
How does your action selection validate the uncertainties that was not captured in the preferred observation.
That was an excellent question. I think actually that question pertains to both continuous state space and discrete state space models.
You know, I think what you're talking about here, you know, if you were an economist, it would be sort of radical uncertainty.
You know, as sort of the ultimate unknown unknowns, or probably more simply as volatility, where where you could you could imagine volatility being a known unknown or an unknown known.
So if I think speak to, you know, how do you put that sort of context sensitivity into generative models where the, you know, where it is the uncertainty that is the key to resolving what is the optimal thing to do.
It's a great question and the solution to the problems that you bring to the table.
I think are important to identify both on one of your neuroscience but also I would imagine in rendering any artificial intelligence super processing system sufficiently context sensitive.
The way I look at this is the first of all, separating the inference and learning problem into three levels and then identifying where the important uncertainty lies.
The first level of, if you like, unknowns corresponds to the states that you could encounter, and that would, you're recognizing those states and belief updating to arrive at the best probabilistic description of those states will be inference.
And then you have the parameters of the generative model that are learned over longer periods of time. And then finally, you're going to have the very structure of the model structure of the graph itself.
Irrespective of the particular parameterization of say the connections or the, or the probabilistic mappings, and that would be known as structural learning.
And at each level you can have uncertainty that has to be modeled in your generative model, or has to be part of your, of your inversion scheme.
And if it's not you can run into trouble and running into trouble is exactly, you know, I think what your question speaks to, you know, how do you cope with this well to cope with it you have to be able to model the possibility that you have not encountered this structure or this,
that your, your model, that your, well, let's take it step by step. First of all, at the level of parametric learning, say the, so, at the level of state estimation.
What happens if some random, if some random effect or stochastic aspect or indeed, you know, a non stochastic aspect of your environment changes.
And in order to detect that, basically you're talking about how do you cope with volatility in the, in the states in order to cope without you have to have a model of that.
And what generally happens in my world is that you immediately have to have an extra level that models slow contextual changes, both in terms of the, you know, the sort of the expected outcomes but also implicit in a probabilistic model,
the volatility or the uncertainty in the state transitions. And I've seen, you know, perhaps a clear example of that would be something called a hierarchical Gaussian filter, developed by a colleague of mine, Chris Mathis,
who started in Zurich, but now he's in Denmark.
And what he does, he really just focuses on the uncertainty. So it's a deep model, and it's a Gaussian process like model, and it's got layer upon layer upon layer.
But the only thing that the higher layers provide constraints on at the lower level is exactly the variance of the random fluctuations.
So it's effectively a volatility model. It sort of recognizes when you're in a very predictable context.
But then the variance or the inverse variance, the precision of various transitions or effects will itself change in his model, usually using an auto regression model, so a random jump.
So he, you have to actually like equip your model with this deep terminal structure in order to estimate the volatility.
And that sort of ideology I think carries through both to the parametric learning, and also the very structure of the model itself.
I've never gone this far in terms of artificial intelligence or computational neuroscience, but in my life as a data analyst, we quite often have this problem that we've got some data and we have a generative model,
the state space model of this sort of complex system, we've inverted the model, we don't quite know its form so we have sort of, you know, 10 or 100 of these different model the different configurations.
So now we have a probability distribution the posterior over model space itself. And that can be very useful when it comes to things like basic models averaging, or just evaluating the plausibility of this kind of structure versus that kind of structure.
But again, the point I think you're driving at, you actually have to have this hierarchical level and this space of the, to be able to quantify the uncertainty at that level to get this sort of to get this context from time sensitivity.
If you don't do that you'll never know and you'll just be committed to the wrong model, because you haven't explored.
Yeah, yeah, thank you. So we need to learn optimal hierarchical models in our prior beliefs, then absolutely select actions to our preferred model.
Now in the action selection, we cannot select multiple actions so we take the mean of the variance of the actions towards the preferred model.
That's another excellent question. Yes.
So here is another instance of stochasticistiness.
So, you have a choice in active inference both in continuous state spaces and discrete state spaces but much better explored in the discrete space space.
You're absolutely right. So you're planning as inference. The objective of that is just basically to infer a probability distribution over a set of plausible policies and they're plausible by having a non zero posterior.
And then the question is well which one do you do which one do you select.
There's no rule here. You know, if you're, if you are a physicist.
That distribution actually is the distribution over the paths a system would take. So, you know, if you wanted to simulate a system you'd actually sample a policy from that distribution.
It's going to be a bit like sort of matching them isn't it, you know, agents choose or select randomly policies in proportion to their plausibility.
But what we actually do is, is as you say, take, take the, take the most likely policy stimulation.
The reason we do that is that very often, when you're actually explaining individual behavior, you have to put that randomness back in because people do actually have a matching like stochastic sampling from possible policies.
So, but when you want to simulate the average behavior of this kind of subject over an infinite number of trials, then you can use the most likely policy. So most of our simulations, just take the most likely policy to specify the next action.
If you use the, this, this, this kind of simulation model to fit observe behavior, then you actually sample your action from the probability distribution over all the, all the plausible policies.
So it is, it is a, it is a vexed an interesting question.
Thank you very much.
Yes, and this is here. Thanks.
Other questions.
The audience.
I have a quick one myself.
Yeah, we do show us several examples, also with quite limited of course number of actions and policies.
In the real world, of course, the complexity is much more higher.
And so the question is, is the same, the same approach, let's say to say the same explanation and model can be applied when the number of actions, number of possible actions are actually increasing or
possibly going to infinity. I mean, when you have, like in real life, of course, you can take whatever action you, your mind test you.
So do you think how, how things are more complex in this case.
Again, that's a great question. It's something that entertains a lot of my colleagues and indeed myself you have the notion of how do you scale up to high dimensional action spaces and state spaces.
And so there are a number of different approaches to this, the generic approach to scaling up usually rests upon some kind of amortization.
So there's a lot of thorough techniques for machine learning, where you can, you can very quickly map from outputs to believe states for example you can learn to infer those things that can be inferred.
But as you not as you point out that's not really going to help you in deep three searches over large repertoires of actions at each point in the, in the, in the tree.
And that's still to a certain extent, an outstanding problem.
However, there are two things I think which mitigate that three things which mitigate against that, that, that challenge of scaling up to high dimensional action spaces and latent spaces.
The first one is purely theoretical, and it comes back to the foundation of the interpretation of the thing that we are trying to optimize, namely the marginal likelihood or the model evidence.
And remember that the model evidence is accuracy minus complexity. So if you can minimize the complexity you're maximizing the evidence, which tells you immediately that the course grading of any given model of any time series data will have an optimal course
grading, and it will be the simplest low dimensional manifold for the course this bins of a discrete state space that you can get away with to preserve a reasonable amount of accuracy.
So there's always, if you like, a mathematical pressure to actually scale down to actually get the simplest model with the smallest number of dimensions, the smallest number plausible possible policies sufficient to explain your active sampling of these data.
So that's something I think is often missed, especially in the drive to, you know, sort of big data and more and more complicated deep learning deep RL models.
And that's going in the wrong direction from the point of view of maximizing model evidence, you need to remove the redundant parameters are actually course brain a lot.
The second thing which mitigates that problem is that once you've formulated your planning, your, your, your research for planning into the deep future.
Once you've formalize that in terms of this kind of belief updating. There are some very natural constraints on the depth of that research, very simple ones, for example, because you've now got uncertainty in the game.
