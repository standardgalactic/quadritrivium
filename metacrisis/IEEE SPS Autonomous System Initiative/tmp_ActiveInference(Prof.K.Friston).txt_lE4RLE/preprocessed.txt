So, hello everyone, welcome to this second webinar of the IEEE Signal Processing Society
Autonomous System Initiative, just a quick introduction.
So, today we're going to have a Professor Karl Friestung, just some advertising say for the next talk.
We are going to have in this series, in January, on 30, we have a Professor Cavallaro from Wimbledon University of London.
And on February 28, we're going to have Professor Axel Jancz from Latvian.
So, I also would like to remind you that we are going to have a workshop,
organized in conjunction with the IEEE ICASP 2023, that will be in June in Rhode Island, in Greece.
Of course, we are currently organizing new, very interesting, hopefully, talks, webinars for the upcoming months.
So, the idea is to have one talk per month.
So, let me briefly introduce Professor Friestung.
I think everybody know Professor Friestung, anyway, you know, is one of the most influential neuroscientists in history, say.
He was cited up to now over 300,000 times, is for sure an authority on brain imaging and theoretical neuroscience,
especially in the use of physics-inspired statistical methods to model neuroimaging data and other random dynamical systems.
He is the key architect of the free energy principle and active inference.
Actually, the title of his talk today will be active inference.
So, I kindly ask you all to keep silent during the presentation.
That was going to last for around one hour.
We're going to have 30 minutes after the presentation for Q&A.
And you are, of course, encouraged to put the questions after the presentation.
Thank you.
Thank you, Carl.
The floor for you.
Well, thank you very much for the introduction.
And it's quite easy to be the most influential neuroscientist in history,
because neuroscientists were invented about 10 or 20 years ago.
So, slightly cheating, but thank you again for that nice introduction.
Thank you for the opportunity to speak to you.
And particularly, thank you for attending, given it's so close to Christmas.
So, I should apologize.
I'm not going to really be talking about signal processing.
What I'm going to try to do is describe a mechanics for making sense of sequential or ordered data
of the kind that the brain might use and to try and develop that mechanics
from a first principle account of sense making and indeed decision making.
And that is, as we have already heard, active inference.
And I'm going to call active inference self-evidencing.
And I'll try and explain what I mean by self-evidencing.
I'll overview the different perspectives on self-evidencing in the first slide,
but then just rehearse the underlying imperatives and objective functions
from the point of view of people in artificial intelligence research and machine learning.
And then what we'll do is we'll drill down into the mechanics of the belief updating
and the self-evidencing using numerical studies simulations.
And at the end, I will try to take on the challenge of simulating a very simple form of reading.
Or sampling data sequentially and trying to make sense of those data.
So self-evidencing, what do I mean by that?
Well, the idea here, the basic premise is that everything we do, everything that we think
and every act that we deploy on the world can be cast as a form of inference.
So perception is perceptual inference, and I'm going to talk about action as planning as inference.
And the imperative for both perception and action is to optimize beliefs about unknown states of the world.
Say, for example, S here that are generating observations O.
And I'm going to denote those beliefs by Q.
Similarly, we're going to have random variables, U control variables, actions about which we have beliefs
that we're going to optimize with respect to a function of those beliefs Q,
given a certain action or choice or decision.
So framing the problem in this way means that I'm going to ask everything as belief updating,
optimizing beliefs about S and U here, namely the Q here,
with respect to a functional, a variational free energy functional of beliefs about states
and an expected free energy functional G of beliefs about what I am doing, my actions.
So just to give you an intuition as to the nature of these functional, the variational free energy, for example.
I've just written down various ways of interpreting this free energy functional.
So technically, it's what's called an evidence lower bound or an evidence upper bound in this instance,
that provides a bound or an approximate equality to the log of the probability of some outcomes given me,
given some system that's sampling those outcomes.
And typically, those outcomes that have a high probability of me sampling will be those that characterize me as a phenotype,
me as an agent, me as an artifact.
And in that sense, they are, by definition, those kinds of outcomes that I would typically or characteristically want to sample those preferred outcomes.
So they are intrinsically valuable in the sense that they are the kind of outcomes that I would expect to, expect to encounter.
And read in that way, we can now interpret this variational free energy in terms of a value function of outcomes.
And from that, we can understand the imperatives behind reinforcement learning, optimal control theory and engineering.
And if I was an economist, that would correspond to expected utility theory.
Another perspective is afforded by information theory.
So the negative of this log probability is known as self information and information theory or surprise or more simply surprise,
namely the implausibility that I would encounter or observe these particular outcomes.
And therefore, by optimizing the free energy of this value here, I'm implicitly minimizing my surprise.
And I can understand that in terms of the max principle, the principle of maximum mutual information,
alternatively or equivalently, the principles of minimum redundancy and indeed the free energy principle.
And that's nice because the average of the self information is entropy, which means that this maximization here,
corresponds to minimizing the entropy of my sensory exchanges with the world or the outcomes that I encounter.
And of course, that's the holy grail of self organization in the physics and non equilibria.
For example, in synergetics, and if I was a physiologist, it would just be a statement of homeostasis,
keeping my outcomes within viable bounds, treating those outcomes as essential variables, basically,
and stopping them dissipate or diverge into regimes that would not be characteristic of me and not be consistent with my physiological survival.
There's a final interpretation here, which licenses the notion of self evidencing.
Instead of this M representing me, it could represent me as a model of how my observations were generated.
And on that reading, this quantity here, the probability of some observable data given a model can be understood in terms of Bayesian model evidence
and therefore trying to maximize the Bayesian model evidence.
I can now understand this imperative here in terms of the Bayesian brain hypothesis.
I can understand it in terms of data assimilation and evidence accumulation,
as if incident things like predictive coding and compressing sound files or indeed as an explanation for message passing in the brain.
So that's the reason I'm calling itself evidencing action and perception in the service of maximizing the evidence for my models of how those data were generated.
We can now regard then perception as belief updating and planning as inference now becomes a description of choosing the right, the most appropriate or self evidencing ways forward, namely gathering those data which supply evidence for my existence.
Lots of ways of understanding the nature of this free energy function.
I've just written it down here in full in terms of a negative energy and an entropy term where crucially the entropy pertains to the entropy of my beliefs about latent states generating data.
And the negative energy has two terms here often referred to in terms of the likelihood of certain observations given the latent causes of states and the prior probability of those states before observing the data.
And written down like that, you can see that this principle of optimizer extremizing the free energy just is James's maximum entropy principle under constraints.
And those constraints are supplied by the negative energy, namely my generative model that can be decomposed into a likelihood and a prior.
So this construct is nice. Here I've written it down using an expression which will be more familiar for people in machine learning.
So here I've just rearranged the terms and expressed it in terms of an expected log evidence and a KL divergence between my beliefs about the states generating my observations and the underlying posterior beliefs of those states given some observations.
And because this quantity here can never be less than zero, what we have is F is always going to be less than it's going to be a lower bound on the log evidence and hence an evidence low bound or an elbow of the sort you'd see or use in things like variational auto encoders.
The importance of having a generative model, I think, should be emphasized here.
Because you're optimizing the evidence for a particular model, you always have a generative model, the likelihood and the prior in mind or explicitly articulated, which means that once you've optimized it, you always have explainable artificial intelligence because you know how these data were caused under your generative model.
What we will see later is that this also provides or prescribes the optimal kind of data that you might want to go and gather or mine in terms of minimizing your uncertainty about the model, namely maximizing the evidence for your model, leading to notions of design
optimality in terms of designing the right data mining, moving from big data to smart data, leading to notions of smart data foraging epistemics of the kind that might be associated with a generalized artificial intelligence.
Here's another way of writing down this free energy functional, this variational free energy.
And I've written it here in a way that a statistician would understand it. And again, just by rearranging the terms, I can now express it as a mixture of accuracy and complexity, where accuracy is just the goodness of fit.
So I've expected log likelihood of some outcomes or observables or data expected under my beliefs about how they were caused in terms of these latent states.
And then we have this term, this complexity term, which is the divergence between my posterior or approximate posterior beliefs Q out of my prior beliefs.
So this complex term is very important, it scores literally the degree to which I change my mind on observing some data. It's the information gain that it actually plays the role of a cost in terms of maximizing the evidence.
So what that means is if I'm going to maximize my evidence, I'm going to conform to Occam's principle, I am going to try to find the simplest explanation that retains an accuracy, an accurate but simple explanation.
So to get the balance between the accuracy and the complexity right, thereby maximizing the evidence.
This is interesting from the point of view of implementation and various ways of framing the cost of a particular explanation.
I can read this complexity as a complexity or computational cost, leading to notions of bounded rationality and approximate Bayesian inference.
It also has implications from the point of view of engineering and the Van Neumann bottleneck and the like, in the sense that, given Landau's principle, that this complexity provides a lower bound on the amount of thermodynamic energy I require to do my belief updating.
So to put this very simply, if you're doing it in the best way possible, you're doing it in the quickest and most energy efficient way possible.
Another way of thinking about this decomposition into accuracy and complexity is to think about the implications of what you are trying to optimize when it comes to the future consequences of a decision or an action.
And this is basically the main message of this presentation that exactly the same constructs become very useful when thinking about the imperatives for selecting or inferring what plan or policy I'm going to pursue.
And actually what we're going to see is that the accuracy that I would expect if I committed to a particular plan becomes exactly the same imperative that underlies optimal Bayesian design, namely that which maximizes the information gain.
In a similar way, the expected complexity becomes something called risk, which is exactly the same quantity that underwrites Bayesian decision theory, where we're reading now loss functions in terms of expected surprise, defined in terms of that value that we started with.
And the two together basically can be regarded as self evidencing. So rest of the talk really is just unpacking this slide this observation and illustrating it using numerical studies and some neurobiological examples, but to motivate it more intuitively.
I'm going to ask you to think about this problem. Imagine you're an owl, and then you're hungry. And then if I was there in person, I would ask somebody usually on the front row, what are you going to do.
And they generally respond quite correctly, well I'm going to search my food I'm going to look for my food and they'd be absolutely right that would be the first thing that a somebody predator that was hungry would do.
And that answer.
As within it to quite fundamental implications and I want to illustrate those by deliberately comparing contrasting to sorts of mechanics or formalisms that you might want to use in order to specify optimal behavior or choices.
What I will do is actually repair this dialectic like later on and show that one is a special case of the other, but for the moment I want to draw a bright line between two different kinds of formulations.
The first depends upon the notion of a value function of the states of the world that would follow if I acted in this way you sub tour at the present time.
And if this value function exists, then I can simply choose for any given current state the action that maximizes the value, and I could therefore create a state action policy pie that returns the best thing to do for every given state that I find myself in.
However, this kind of approach won't work in terms of searching for your food. And that's almost self evident because searching for your food is the action that you would take to reduce your uncertainty about where your food is located.
But uncertainty is an attribute of your beliefs about the world, not the actual state of the world. So that tells you immediately the optimal action has to be not a function of states of the world, but a functional function of a function of beliefs about states of the world.
This tells you the optimal action depends upon beliefs about states as opposed to states per se. Also searching for food tells you something else.
It means that the order in which you do things matters. It matters whether I try to eat my food and then search for it as opposed to trying to search for it and then trying to eat it.
Which means that there's a different kind of construction here for the optimal policies, a sequential policy, which means that I now have to consider the function, not just of any belief about the current state but all states into the future under a given sequence of actions.
And I've written it down like that. So that I can now refer to this G which is going to be an expected free energy as an energy functional, which means that the sum of the path integral over time is now going to be an action as would be called by a physicist, a path integral or a time integral.
of energy here, namely action. So this kind of construction in which we also now have to optimize our beliefs about the consequences of our actions by maximizing the free energy.
And can be best described and are simply described in this instance as a principle of stationary action and in this case, maximizing this evidence lower bound, and it's expected version, the expected free energy G here.
So I introduced that just to contrast it with the Bellman optimality principle that would attend this kind of construct here. And I'm sure a lot of you will be familiar with lots of instances of
schemes that operate under the Bellman optimality principle, control theory, dynamic programming, deep RL based in decision theory and so on.
And then the equivalent schemes under this principle of stationary action would be the free energy principle itself and active inference.
And what we will see is notions of artificial curiosity and in robotics, for example, intrinsic motivation that speak to resolving uncertainty, the epistemics that underlie optimal Bayesian design and crucially sequential policy optimization.
We're talking about ads and trajectories here, as opposed to states that ensue point of time by time, moving from Markov decision processes, for example, to partially observed Markov decision processes where beliefs about unobservable or latent states start to figure much more
than actually in the maths. So, and just to rehearse now the functional forms of these variation free energies and expected free energies I've just written them out again here.
So if you don't like or remember all the maths, please don't worry about the equations here. I'm using them almost iconically, just to illustrate what I think is a very beautiful symmetry between the variation free energy which we're trying to optimize in
relation to our beliefs about states generating data, and the expected free energy G that is a functional of both of those beliefs and the policies that I would entertain in terms of how I'm going to move forward into the future.
So here's again is the free energy written in terms of a mixture of accuracy and complexity that can be rearranged very simply to demonstrate that it is an evidence lower bound, given this KL divergence can be never be less than zero.
And here are here is the equivalent expression for the expected free energy, and it's called expected simply because we're taking an expectation under the predicted outcomes that I would encounter if I pursued this policy.
So we're conditioning now upon a policy, but the functional form remains very similar, where we can see that the complexity becomes risk, the inaccuracy becomes ambiguity, and the divergence and the evidence become expected information gain and cost or negative cost, respectively.
So let me just unpack that very briefly related to things which I'm sure many of you will be familiar with. And let's just look at the components of the expected free energy.
And what I'm going to do is I'm going to take various sources of uncertainty off the table and see if I can get back to the Bellman optimality like formulation of optimal behavior.
But let's ignore, in the first instance, the expected value or if I multiply by minus one the the surprise or the expected cost.
So I prefer everything. I have no particular preferences. So all I'm left with now is this quantity here. So what is this quantity? Well, it's just the KL divergence between beliefs about states of the world.
Given some observations that I would get in the future if I pursued some policy relative to those beliefs without those observations.
So this is just the expected information gain. It's just the degree to which I have resolved my uncertainty by getting those data, if I pursue this policy.
And in the neuroscience is in particular the visual search literature, this is known as expected Bayesian surprise. And for those of you in information, say you recognize this immediately as simply the mutual information between causes and consequences, the
latent causes of observable consequences conditioned upon a particular policy.
So let me now take one kind of uncertainty out of the game. And let's ignore the ambiguity. What does that mean? Well, it just means that there is no uncertainty about the states of the world, given some observations and vice versa.
Which means I can replace all the S's with O's and the risk term now reduces to either the KL divergence between Q beliefs about states or future observations, which are now random variables because they haven't occurred yet, and my prior beliefs.
So what does that mean? Well, it just means that the risk is the divergence between what I anticipate will happen and what a priori I prefer to happen.
So it scores the divergence between what I think is going to happen and what I prefer to happen. And in engineering, this is known as KL control and economics, it could also be read as a form of risk sensitive control in the sense that there is no ambiguity.
Finally, let me remove all reducible uncertainty and assume that there is no more information to be gained. I know everything that I can know about the consequences of my action and the hidden or latent states of the world.
And I've just left with the expected cost or the negative expected value or utility. And of course this takes us right back to the Bellman optimality principle and expected utility theory where because there is any uncertainty, I don't have to worry about beliefs, and I can now deploy a standard reinforcement learning or Q learning or any other scheme that rests upon the Bellman optimality principle.
So in summary, what we're saying is that this expected free energy, this expected bound on the evidence for my models of how the world works, of the sensed world can be decomposed into expected value and expected information gain.
And where expected value is just the objective function that underwrites the Bayesian decision theory and leads to optimal Bayesian decisions under uncertainty given some prior preferences or cost functions.
In addition, I'm going to do this whilst exploring in the sense of maximising my expected information gain. And this is the Bayes Optimal Experimental Design Imperative. Basically, if I had to choose certain data points, what data points would I choose to be most informative and resolve the most
uncertainty about my hypotheses or my model of how those data were generated. Exactly the same kind of objective function you'd see in active learning. And together we have essentially Bayes Optimal Decision Theory and Bayes Optimal Design constituting active inference and learning.
So I'm going to spend the remaining part of the presentation just, you know, providing you with a couple of work examples and deal with one of the more interesting kinds of generative models under which we can self evidence, namely, deep,
temporarily deep generative models. But before I go then, let me just rehearse the foundations of the kind of modelling that we use to try and understand the message passing and the implementation of this kind of self evidencing.
Generally, what we do is work with Markov Decision Processes, which we can write down in a very simple way in terms of probabilistic graphical models. So for example, here, again, please ignore the equations, everything that we need to know is in graphical form here.
And what we're assuming is that any given world is a world generates outcomes or observables from latent states via likelihood mapping usually denoted by a, and the dynamics in this world are parameterized or modeled in this
graphical model by transition tensors or matrices that shift every time step the state of the world into the next state, where crucially, these transitions depend upon action, namely the policy, where the policy itself depends upon the expected free energy that we have just rehearsed.
And the nice thing about this depiction of the structure of a generative model in this graphical form is that if there exists this graphical model, there is always, if you like, a conjugate or an equivalent graphical model that
is being called a factor graph in this instance, a normal style or for me style factor graph. And these are really useful. And basically what you do is you sort of switch the edges to the nodes and vice versa so now the nodes become factors, and the edges now correspond to beliefs or the sufficient
characteristics of various probability distributions that you've asked amongst the nodes in order to do the belief updating that maximizes the evidence low bound, and as I've written it down here.
And this is, if you like, standard off the shelf, belief propagation or variational message passing that any particular generative model would demand, simply because for every given generative model there is a factor graph and for every factor graph, you can now just go and get some off the
variational message passing that you know will maximize the evidence low bound or the variational free energy. And I've just written down the functional form of these things here to describe some of the messages so let's for example just take
beliefs about the, the latent state at this point in time and what we can see is that they receive messages from the observables the sensory evidence at hand, and messages from the future, and messages from the past that put constraints on the most likely state that is sandwiched between
them, and these messages are contextualized by the policy that I'm currently pursuing. And the functional forms here very remarkable similarity to the functional forms of equations that we're using computational neuroscience, when we try and simulate this kind of belief propagation and updating and
indeed learning in the, in the human brain. So we can, I've just rewritten these variational updates here that ensue from the message passing on the left hand side, and written down the corresponding message passing graph that we think might operate in a very simple picture of a brain.
And the interesting thing is, I repeat, these update equations are very, very simple, and also very similar to what we'd use in neuroscience so for example, we have to only update update our beliefs about things we don't know we only don't know the latent states and the policies, and I've
actually included here a softmax or precision parameter, reflecting the confidence in my beliefs about the what I'm going to do next gamma, or the inverse of gamma which is beta here, referred to as precision.
If we just focus on perception and policy selection, then perception just is updating expected expectations about states in the world that is a nonlinear function say a voltage current function of linear mixtures of observations representations expectations about the past and
the future that are mixed through with these tensors likelihood and prior tensors here that have a very simple parameterization to produce when when applying the softmax operator.
The optimal belief, the elbow or evidence maximizing belief about hidden state of the world.
The policy selection just reduces to a classical softmax response rule, predicated on the expected free energy.
It reduces to a canonical kind of associative plasticity with associative terms decay terms here, here illustrated for D which parameterizes the initial latent states and action selection as we have discussed, and simply follows from the policy
selection, namely selecting the next action or control variable from the most likely policy.
So with that we can simulate all kinds of behavior. I'll give you one brief example just to illustrate the most notable Arnold feature of the kind of behavior that ensues from this self evidence or active inference.
So here, imagine that there's a little mouse, and it is in a teammates and can make two moves, and the mouse wants to find a reward or a preferred outcome denoted here by the red ball that can either be on the left or the right of the upper arm.
And because the rat or sorry the mouse does not know where, whether the reward is going to be on the left or the right hand side, and can only make two moves and furthermore, once the mouse commits to a particular army has to stay there.
So we can gamble and go to the left and if it's right, it will spend two moves with the reward. If it's wrong, it will spend no moves with the reward so on average, it will be right 50% of the time.
I think that's interesting and emphasize the special role of beliefs and belief updating. We've introduced a third kind of outcome or cube in the bottom arm of the teammates, where the color of this instructional queue tells the mouse, where the reward is.
The mouse has got a choice. It can use its first move to go and look at the queue in the bottom of the maze, and then move directly to where it knows the reward is spending one move with the reward on every trial.
So still from the point of view of the expected reward, it's still 50% of the time with the reward. But of course, it's now much more confident about what's what is going to what is going to happen, and what it is going to do because it's responding to this expected
expectation game this epistemic affordance that is part of this expected free energy.
And that's basically what I want to show you the these graphics and matrices are just meant to illustrate how simple it is to write down these terms of transitions amongst various states, and the different policies, and the outcomes in terms of whether there is a reward at the different
levels of the reward. The prior preferences here just written down in terms of log probabilities or log odds ratios, and specifying that the reward that the preferences here are just to be with the reward.
And everything else is equally preferred. So what kind of behavior do we get when we integrate that message passing scheme that belief updating scheme.
Well, here what I've done is just illustrated the outcomes of some simulated trials 32 trials, where the colors of these circles here denote whether the reward was on the left or the right hand side.
The image here, the sort of gray bars here, describe the probability distribution over various policies so the policies are only two moves that go down to the left or down to the right or it could stay and go to the right or it could go down and then go back to the middle again.
So a finite number of sequences of actions, which the mouse is trying to evaluate in terms of the expected free energy, and then forming posterior beliefs that enable it to plan via inference about what it is likely to do next, and then select the most likely behavior.
These show the outcomes in terms of the expected reward or expected prior preference here and reaction times, and this is just the scoring the learning of the initial states the context was the reward on the right or the left.
And as one might expect when the mouse first starts the trial, it will always go and respond to this epistemic affordance engaged in this epistemic poraging before becoming exploitative and going to secure its reward.
But in this instance, we've played a trick on the mouse.
We've done after the first few trials, we've left the reward on the left hand side.
And that has an interesting implication, because as time goes on, the mouse learns that the reward is always on the left hand side, which means that there's less and less information gain afforded by going down to the instructional queue.
And the exploitative policy becomes less and less attractive, as expected information gain starts to fall below the expected value of going straight to the reward, which it is now, or the mouse is now fairly confident, it knows where the reward is.
Indeed, after trial 20, there's this switching behavior from expiration to exploitation, simply because the mouse has now become familiar with its environment, and there is no more reducible uncertainty.
And therefore, the epistemic affordance has given way to the expected utility, or the pragmatic affordance of going and sitting with its reward for two moves directly.
It's very generic behavior. It dissolves the exploration exploitation dilemma, simply by putting the two imperatives of exploration exploitation together in the same functional that underwrites the self evidencing and inherits from the functional form of this bound on model evidence.
I won't go into this in any detail. If I was speaking to a neuroscience audience all sorts of interesting things one can do now, because we've actually got a, an in silica or synthetic mouse, who was updating her beliefs and learning, we can now go and perform simulated neuroscience experiments we can look at the belief updating
through the lens of neural activity and look at firing rates associated with the chosen and chosen option of this done in neurobiology and electrophysiology, we can think of the representations as fulfilling the role of a working memory of a prospective
sort that changes as time marches on, and we can even go and record the activity of various representations of locations and simulate place fields, they sell like activity, and do all sorts of things including things like the mismatch negativity paradigm.
I want to close now by taking you to a slightly more interesting kind of generative model and illustrating the sorts of behaviors that one can simulate, and also briefly look at the corresponding neural correlates of this belief updating the self evidencing under deep generative models.
And so what do I mean by deep generative models well let's go back to that dual aspect rendering of a generative model in terms of a graphical model, in terms of Markov decision process, and it's corresponding or conjugate
graph describing the requisite message passing. What I'm going to do now is take this Markov decision process and replicated but place another one on top that crucially unfolds more slowly in time.
So the depth of these hierarchically composed Markov decision processes in this deep model is accompanied by a separation of time scales, so that the transitions at the higher level, provide a context for the faster transitions at the lower
level, basically destroying any simple Markovian property and rendering now the general model effectively semi Markovia or non Markovian by virtue of the separation of time scales.
And the way that this in this example, this is done is usually by identifying some state of the world that endures for the unfolding of the fastest states at the lower level.
And the first one obvious candidate is the initial state so the initial state is always the same. The first state is always the same.
During the unfolding of states or the state transitions of the path of trajectory during this epoch. So effectively the higher level is providing the initial conditions for the lower level.
So from the point of view of the factor graph, and this nicely illustrates the bottom up and top down message passing that inherits from the this deep architecture.
So from the point of view of the lower level, the top down level is actually providing empirical prior constraints that afford a context sensitivity to this little time epoch of exchange with the world.
And this context will change in the next epoch in a lawful way, as modeled by the, the higher level and message passing here.
So from the point of view of the higher level, the lower level is actually providing evidence that you are in this particular context, as opposed to that particular context.
So there's a mutual reciprocal exchange of sufficient statistics or beliefs that enable this non Markovian generative model of non Markovian outcomes that has this inherent time and context sensitivity.
To close, I just want to illustrate what that looks like in practice when you deploy these models, but to generate sequences of observables, and then try to recognize the underlying causes of those observations.
So what I'm going to do is present very briefly, a deep model of reading, a simple model of reading, in the sense that we're not talking about letters which talking about sort of an iconographic script, where each word is composed of different
letters and different arrangements on four points on a two by two grid here. So for example, if there's a bird icon next to a cat icon, the implicit word is free.
If the bird is next to some seeds, the implicit word, the semantic if you like, is feed, and if there's nothing there, we just wait.
And this would be a suitable kind of labelling of latent states that will be required to generate observed outcomes through simply sampling the letters of a word.
So what are those outcomes? Well, I just need to specify what the agent could see if it foveated one of these letters here, and it would see either nothing, some seeds, a bird, or a cat, and these are discrete outcomes.
The second outcome is where am I looking, a proprioceptive outcome. And if I knew exactly which word I was looking at, and where I was looking, so I was looking at one, and the word was feed, and I would see a bird, and I would feel myself looking at location one.
And that would be fit for purpose for a very simple generative model of epistemic foraging or epistemic expiration, just scanning the letters of a word.
And I would have to equip it with a deeper architecture that would be more apt to model reading.
What would I need to do to do that? Well, I would need to be able to generate the word that the agent is currently sampling, which means my hidden states would now have to become a sequence of words, namely a sentence.
I would need to know what the sentence was, and I'd need to know where in the sentence I was currently looking, at which word I was currently looking.
So, for example, if I knew that the sentence was free weight, feed, and weight, and I knew that I was looking at the second word, then that would be weight.
And therefore, I now know what word I'm looking at, and if I know where I was looking, I can now tell you exactly what I would see.
So this would be an appropriate generative model for a very simple kind of reading where the whole universe just contains these six sentences here.
And now the idea is, can we simulate evidence accumulation over separate, separable time scales that enables an agent to infer not just what word they are currently reading, but also the context,
namely the sentence that that word came for, and do that in real time using this belief update scheme or variational message passing scheme under this kind of deep generative model.
And indeed one can and these are the kind of results that ensue. And so, at the top here what I've shown are the four words of the sentence that the, the simulated subject was reading.
And the red dots correspond to actions as a kind of eye movements where she looked in the sentence together the right kind of information with the most expected information game that would resolve her uncertainty about what she was currently seeing
in terms of the first level words, second level sentences.
And what we see here is something which is very characteristic of the way that you and I read, namely, we jump very quickly to the most informative letters and icons that provide disambiguating information.
So, for example, the very first discard, she sees a cat, and therefore she knows that the first word must be flee because to see a cat means that there has the only word with a cat in is flee.
So she's resolved, there is no more resolvable uncertainty about remaining and sampling the remaining letters of this word. And so she jumps to the next word, which is possibly a weight and confirms that by sampling by chance to no outcomes,
she jumps to the next one, it's a bird that's a bit ambiguous, it could be a flee or feed, she confirms it's a feed by going to the second letter in that word, and as soon as she's resolved that uncertainty jumps to the end, and discovers that that is another weight word.
So here are the sort of the posterior beliefs that are being updated during time during these organic eye movements at the first level in terms of what are the letters that are sub tending beliefs or belief updating about the sentence, which is shown
in the second level in the upper row. And what one can see from this is that only at the end is there going to be a resolution of uncertainty about which sentence she was reading simply because it's just the last word that resolves that
uncertainty. Another way of showing those data of the synthetic data is in terms of looking at the posterior expectations or beliefs about different states, namely, in this instance, at the high level, which sentence am I looking at, and which word am I looking
at as a function of successive eye movements. And one can see immediately that the belief updating at the low level is much quicker.
So for the first, it's a card or two. I confirm that I'm looking at the letter flee, and then move to the next letter, and then it's, it's weight, or I accumulate my beliefs to believe that it is weight, and then move to the next, the next word here.
And in contrast, the belief updating about the word is much slower.
And indeed, because only these two sentences begin with flee immediately this subject or if I was doing this I would believe that there has to be either sentence one or four, but I can't resolve that uncertainty until the end because the only disambiguating word is at the last point that
is weight versus flee. As soon as I see that then I can resolve my uncertainty about which word I am looking at, one I've inferred that it is weight at the lower level.
These are the same data shown here, but what I've done here is just filter them using the same filters that we use in EEG or neurophysiology, electrophysiology research in neuroscience, just to illustrate the similarity between these, these patterns of synthetic neuronal representation
in neural firing, or changes in neural firing as reflected in local field potentials and electrographic cortigraphic signals.
And I'm showing a couple of examples in closing and that speak to this kind of pattern of neuronal responses that we presume is reflecting the same kind of belief updating a message passing in the simulation.
For example, here, if we treat these as raster plots of neural firing, they look very similar to empirical pre-psychadic delay period activity in the prefrontal cortex, while these fluctuations in the filtered time series look very similar to periscadic field
potentials during active vision, showing these characteristic fast responses at the lower level here in visual area number two, and these slower responses of reflecting this belief updating at a high level here, the area TE in a monkey.
So again, this is not terribly interesting from the point of view of signal processing, but it is interesting that the same dynamics on these structured graphs, suggested by this off the shelf kind of belief propagation or message passing looks or produces patterns of activity of the kind that we can actually see in the functioning brain.
So I will summarize everything that I just said in a much more concise way by borrowing from the words of Helmholtz.
Each movement we make by which we alter the appearance of objects should be thought of as an experiment designed to test whether we have understood correctly the inherent relations of the phenomena before us.
This is their existence in definite spatial relations. And with that, it only remains for me to thank those people whose ideas I've been talking about, and of course to thank you for your attention, thank you very much indeed.
Thank you. Thank you, Carla.
Very interesting.
Very inspiring talk. Thank you so much. So we have, of course, time for questions. So, please, it's great to open your microphone, put questions, raise your hand, or write in the chat.
I have a question. Hello. Hi, please.
I have a question.
Carl, are you able to integrate your friend?
Hello, can you hear me? Conscious perception versus unconscious perception. So can you model consciousness in that framework?
Great question. I think if you can, it depends what sort of level of consciousness you'd be happy with.
Whether you perceive something consciously, so you can see something where you don't perceive it, versus you see it and you perceive it as well, like there might be something in your visual scene, you don't actually realize it, it's there.
Yeah, I think there are probably two levels. I mean, certainly from the point of view of Herman Hellholtz and the notion of unconscious inference as visual perception, then certainly this is exactly stealing and implementing those, you know, those ideas in modern
sense, they believe in message-passing schemes. So I'd certainly say that your unconscious perceptual inference is exactly what's going on here.
You could argue that the posterior belief would be the content of a percept, and that's sort of very simple-minded sort of mapping between the mathematical quantities.
So if you believe in the beliefs of a mathematical sort to a perceptual belief, then yes, you could argue that there is a kind of consciousness, but I think what you're talking about is the qualitative experience of the percept, that I know that I am seeing red, or I know that this word is, I think that's a lot more subtle.
I think you can model it in this kind of framework, but only under very special conditions, and those special conditions mean you have to have a kind of mental action that involves updating the precision of the beliefs that render the form of the
message-passing dependent upon your internal action state. So if this was a predictive coding scheme, then that would correspond to the Kalman gain, for example.
If you now get control over the Kalman gain, then that control can be read as a kind of internal action, and at that point, then you become an agent of your perception.
And I believe that that's the closest you can start to get to having a phenomenal qualitative experience. You have to have a change, an active change in the Kalman gain, or the precision, or the attention that you're affording these particular messages as opposed to those particular messages.
You start doubting your perception, like you don't believe what you're seeing, and then maybe you doubt yourself what's happening, and don't believe what I'm seeing, and yeah, maybe that's the issue then, yeah.
Yes, yeah, and of course, and then of course that would change your attention if you thought, did I see this or didn't I see that, and then you would focus, and again, without speaking to this mental action that you have to have this sort of explicit attentional aspect.
So a vanilla Kalman filter, I don't think would have that, but you could engineer, and people are trying to do this, you could actually engineer this sort of dynamic estimation of the precision of these mappings, and I think you could be getting closer to conscious perception.
It wouldn't be self-conscious, but it may have the mechanisms that would be sufficient for that phenomenal consciousness.
I guess unconscious perception doesn't change your actions, or it's not so clear.
No, I think it can, I think reflexes, certainly homeostatic reflexes would be modelled like that.
In fact, interestingly, you're often not aware of those reflexes simply because they are subject to psychological or physiologically, something called sensory attenuation, which physiologically just means you ignore those signals
by reducing the synaptic efficacy. Mathematically, it would be in a predictive coding formulation of the message passing or a Bayesian filter.
It would be like switching the Kalman gate to zero temporarily, and then what happens is that the prediction errors are forced back out to engage reflexes, autonomic reflexes on the muscle.
So I would imagine that quite a lot of active inference, certainly in the interceptive domain, actually is completely sub-personal and completely automatic.
And it's just those things that we can't explain that suddenly require us to attend to and to think about that we would actually perceive.
That's a good point.
Thank you.
Hello, I have a question.
Can I ask?
Yes, please, please.
Okay.
First, thank you very much for the very interesting talk.
When you were talking about the decision of mouse, you shortly mentioned about gambling. So is there any theoretical connection between gambling theory in particular Kelly criterion and this active inference?
I have never been asked that question before.
Certainly, active inference or the formalism that I just described under these Markov decision processes has been used to address the multi armed bandage problem as your economical decision theoretic problem in a Bayes way, a Bayes optimal fashion.
So gambling and choice behavior under uncertainty is exactly what this kind of scheme is meant to deal with because, you know, because I repeat, once you put uncertainty quantification into the game, you have to deal with belief updating and the mechanics of
sort of passing messages about belief, hence the, you know, the photograph and the variational message passing.
In a sense, active inference is all about gambling.
However, if you meant gambling from the point of view of a pathological behavior, and the prior beliefs that people who become addictive, for example, get engaged with, then I think that's a slightly more searching question.
And usually, and indeed, it's the kind of question that my colleagues, indeed at me, my colleagues in psychiatry and psychology become very exercised by one interesting aspect of applying active inference to abnormal behavior and psychopathology
read as false inference or odd kinds of influences is something called the complete class theorem, which really comes from sort of statistics.
And what that says is for any pair of behaviors and loss functions, there are always some private beliefs that render you Bayes optimal.
So what that means is, you can always describe any given behavior under any given loss function in terms of one of these active inference schemes.
But that explanation may requires you to actually identify the priors that make this behavior Bayes optimal, which basically means that you can, you know there exists some private beliefs that describe uniquely this person's behavior.
So sometimes people are interested in fitting these active inference choice behavior models to individuals behavior, and then trying to adjust the prize in the model in order to match the behavior of the synthetic model the digital twin if you like, to the real person.
And then, and try to understand what this person's priorities were that cause them to gamble in this kind of way. And it's a really interesting field because you know there's anecdotal evidence from psychophysics and psychology that people say with schizophrenia jump to conclusions.
So that would suggest that they have imprecise prior beliefs or that they may be unable to wait until more definitive evidence arises, or they may be certain conditions associated with an inability to decrease the precision of sensory input and that would
mean that their priorities means that they would be always attending to sensory input and that's been used as an explanation for certain conditions like autism and the like.
So that, you know, you can, you can use this machinery to try and emulate certain kinds of behavior such as gambling behavior, and, and do that in a quantitative and formal way by explaining somebody's choice behavior in terms of their
prior beliefs, a particular point point in the model.
So I'm not quite sure which, which, which sort of gambling you were talking about but does that answer your question, roughly. Yeah, so my question was more the technical part that you mentioned at the beginning that it is actually the same so the active inference.
The same concept of them. So Kelly character and actually makes the best decision based on some beliefs in the gambling story.
Yes, as you mentioned, so it's the same. Okay, thank you.
There's a gentleman called Dimitri Markovich, I think in two ways you know you look at Dresden Dresden I think is in Dresden, who spent a lot of time looking at sort of various UTC and Thompson sampling in the context of multi armed bandit and gambling problems and trying to cast that in the
in the most general way, where usually these active inference models are the most general way of framing the problem. Of course, there are lots of interesting issues about deep three searches in the panning side, which usually discriminate one screen from another
scheme, but certainly the mechanics of it will be formally identical.
Thank you.
Thank you.
Other questions.
I have a question.
Please.
I have a call for a comprehensive sentences of active inference from the fundamental basis. However, my question is tailored towards active inference for continuous state space, most especially where the state space is random in nature, which demands random actions, probably.
Now we learn an optimal prior.
So the optimal prior include evidence from the past and evidence from the from the future, or evidence of the uncertainties that we don't know.
How do we learn a model of the uncertainties that we do not know, especially in random environments that at each instance in time, what you learn as optimal prior during action selection might be different.
If the agent is faced with with with a surprise that was not captured in the optimal prior.
How does your action selection validate the uncertainties that was not captured in the preferred observation.
That was an excellent question. I think actually that question pertains to both continuous state space and discrete state space models.
You know, I think what you're talking about here, you know, if you were an economist, it would be sort of radical uncertainty.
You know, as sort of the ultimate unknown unknowns, or probably more simply as volatility, where where you could you could imagine volatility being a known unknown or an unknown known.
So if I think speak to, you know, how do you put that sort of context sensitivity into generative models where the, you know, where it is the uncertainty that is the key to resolving what is the optimal thing to do.
It's a great question and the solution to the problems that you bring to the table.
I think are important to identify both on one of your neuroscience but also I would imagine in rendering any artificial intelligence super processing system sufficiently context sensitive.
The way I look at this is the first of all, separating the inference and learning problem into three levels and then identifying where the important uncertainty lies.
The first level of, if you like, unknowns corresponds to the states that you could encounter, and that would, you're recognizing those states and belief updating to arrive at the best probabilistic description of those states will be inference.
And then you have the parameters of the generative model that are learned over longer periods of time. And then finally, you're going to have the very structure of the model structure of the graph itself.
Irrespective of the particular parameterization of say the connections or the, or the probabilistic mappings, and that would be known as structural learning.
And at each level you can have uncertainty that has to be modeled in your generative model, or has to be part of your, of your inversion scheme.
And if it's not you can run into trouble and running into trouble is exactly, you know, I think what your question speaks to, you know, how do you cope with this well to cope with it you have to be able to model the possibility that you have not encountered this structure or this,
that your, your model, that your, well, let's take it step by step. First of all, at the level of parametric learning, say the, so, at the level of state estimation.
What happens if some random, if some random effect or stochastic aspect or indeed, you know, a non stochastic aspect of your environment changes.
And in order to detect that, basically you're talking about how do you cope with volatility in the, in the states in order to cope without you have to have a model of that.
And what generally happens in my world is that you immediately have to have an extra level that models slow contextual changes, both in terms of the, you know, the sort of the expected outcomes but also implicit in a probabilistic model,
the volatility or the uncertainty in the state transitions. And I've seen, you know, perhaps a clear example of that would be something called a hierarchical Gaussian filter, developed by a colleague of mine, Chris Mathis,
who started in Zurich, but now he's in Denmark.
And what he does, he really just focuses on the uncertainty. So it's a deep model, and it's a Gaussian process like model, and it's got layer upon layer upon layer.
But the only thing that the higher layers provide constraints on at the lower level is exactly the variance of the random fluctuations.
So it's effectively a volatility model. It sort of recognizes when you're in a very predictable context.
But then the variance or the inverse variance, the precision of various transitions or effects will itself change in his model, usually using an auto regression model, so a random jump.
So he, you have to actually like equip your model with this deep terminal structure in order to estimate the volatility.
And that sort of ideology I think carries through both to the parametric learning, and also the very structure of the model itself.
I've never gone this far in terms of artificial intelligence or computational neuroscience, but in my life as a data analyst, we quite often have this problem that we've got some data and we have a generative model,
the state space model of this sort of complex system, we've inverted the model, we don't quite know its form so we have sort of, you know, 10 or 100 of these different model the different configurations.
So now we have a probability distribution the posterior over model space itself. And that can be very useful when it comes to things like basic models averaging, or just evaluating the plausibility of this kind of structure versus that kind of structure.
But again, the point I think you're driving at, you actually have to have this hierarchical level and this space of the, to be able to quantify the uncertainty at that level to get this sort of to get this context from time sensitivity.
If you don't do that you'll never know and you'll just be committed to the wrong model, because you haven't explored.
Yeah, yeah, thank you. So we need to learn optimal hierarchical models in our prior beliefs, then absolutely select actions to our preferred model.
Now in the action selection, we cannot select multiple actions so we take the mean of the variance of the actions towards the preferred model.
That's another excellent question. Yes.
So here is another instance of stochasticistiness.
So, you have a choice in active inference both in continuous state spaces and discrete state spaces but much better explored in the discrete space space.
You're absolutely right. So you're planning as inference. The objective of that is just basically to infer a probability distribution over a set of plausible policies and they're plausible by having a non zero posterior.
And then the question is well which one do you do which one do you select.
There's no rule here. You know, if you're, if you are a physicist.
That distribution actually is the distribution over the paths a system would take. So, you know, if you wanted to simulate a system you'd actually sample a policy from that distribution.
It's going to be a bit like sort of matching them isn't it, you know, agents choose or select randomly policies in proportion to their plausibility.
But what we actually do is, is as you say, take, take the, take the most likely policy stimulation.
The reason we do that is that very often, when you're actually explaining individual behavior, you have to put that randomness back in because people do actually have a matching like stochastic sampling from possible policies.
So, but when you want to simulate the average behavior of this kind of subject over an infinite number of trials, then you can use the most likely policy. So most of our simulations, just take the most likely policy to specify the next action.
If you use the, this, this, this kind of simulation model to fit observe behavior, then you actually sample your action from the probability distribution over all the, all the plausible policies.
So it is, it is a, it is a vexed an interesting question.
Thank you very much.
Yes, and this is here. Thanks.
Other questions.
The audience.
I have a quick one myself.
Yeah, we do show us several examples, also with quite limited of course number of actions and policies.
In the real world, of course, the complexity is much more higher.
And so the question is, is the same, the same approach, let's say to say the same explanation and model can be applied when the number of actions, number of possible actions are actually increasing or
possibly going to infinity. I mean, when you have, like in real life, of course, you can take whatever action you, your mind test you.
So do you think how, how things are more complex in this case.
Again, that's a great question. It's something that entertains a lot of my colleagues and indeed myself you have the notion of how do you scale up to high dimensional action spaces and state spaces.
And so there are a number of different approaches to this, the generic approach to scaling up usually rests upon some kind of amortization.
So there's a lot of thorough techniques for machine learning, where you can, you can very quickly map from outputs to believe states for example you can learn to infer those things that can be inferred.
But as you not as you point out that's not really going to help you in deep three searches over large repertoires of actions at each point in the, in the, in the tree.
And that's still to a certain extent, an outstanding problem.
However, there are two things I think which mitigate that three things which mitigate against that, that, that challenge of scaling up to high dimensional action spaces and latent spaces.
The first one is purely theoretical, and it comes back to the foundation of the interpretation of the thing that we are trying to optimize, namely the marginal likelihood or the model evidence.
And remember that the model evidence is accuracy minus complexity. So if you can minimize the complexity you're maximizing the evidence, which tells you immediately that the course grading of any given model of any time series data will have an optimal course
grading, and it will be the simplest low dimensional manifold for the course this bins of a discrete state space that you can get away with to preserve a reasonable amount of accuracy.
So there's always, if you like, a mathematical pressure to actually scale down to actually get the simplest model with the smallest number of dimensions, the smallest number plausible possible policies sufficient to explain your active sampling of these data.
So that's something I think is often missed, especially in the drive to, you know, sort of big data and more and more complicated deep learning deep RL models.
And that's going in the wrong direction from the point of view of maximizing model evidence, you need to remove the redundant parameters are actually course brain a lot.
The second thing which mitigates that problem is that once you've formulated your planning, your, your, your research for planning into the deep future.
Once you've formalize that in terms of this kind of belief updating. There are some very natural constraints on the depth of that research, very simple ones, for example, because you've now got uncertainty in the game.
If you go out in into the future, deeper into the tree, the uncertainty starts to remove any useful information from going any further or any deeper into that tree.
So there's a natural depth that again is mathematically prescribed, just because you're now propagating beliefs into the future, as opposed to, you know, just decide if this that happens.
If you have uncertainty, you never have to do a deep research because there's no point because you're so uncertain about what's going to happen. There's no point thinking that far into the future.
So there's always an optimal depth between search and that's usually, in my experience, much shorter than people than people appreciate.
The third point is a much more practical observation, and it speaks to, you know, your question made me smile in the, in the, you know, in the sense that there's an infinite number of actions we can take and you're absolutely right.
Of course, you know, if I just take the simplest and probably really important action that I actually have to select nearly, you know, several times a second is where to look next.
I could, in principle, in terms of R and theta, look at an infinite number of points in the next 250 milliseconds and, you know, that kind of active vision is vitally important for our survival.
So how on earth does the brain do it? Well, it does it in parallel. So it has a something called the superior colliculus, which is basically a map that if you stimulate your eyes are drawn to that point in the map.
And one way of looking at this salience map, this layer of cells is actually a two dimensional representation of all possible actions.
So the answer, very simply, the way that the biological system scale up to large action spaces, not in time, you know, we don't just talk about one step ahead like policies, where to look next, but in terms of the number is instituted in parallel.
So if you do it biometrically or neuromorphically, I don't think the scaling, the scaling issue is insurmountable.
Okay, yeah. So do you think it's possible, like as we think as we have in this model, this latent space, state space, do you think it's possible to have a similar concepts also in the action space of some kind of latent action space or some,
let's say, action at a different level, higher level action.
Yeah, no, yeah, I think that's again, an exciting and important observation. Yeah, absolutely.
And in fact, that's that that that simulated reading example actually had that so there were low level actions where there were little micro saccades around the word to look at different letters, and then there were high level actions that jumped to the next word.
So I'm sure in our real brains, there are different levels where different levels can actually send predictions to our motor or autonomic systems to illicit illicit actions.
I think that, you know, that's, that's absolutely true.
But also, I think that's an interesting question from the point of view of control theory and, you know, can you actually compose continuous movements in terms of a succession of fixed points.
And, you know, it may be that that's how we work, and I certainly talking to friends in your robotics that that that may be how to basically model or generate the predictions that drive the, the trajectories of motion.
It's not a question of prescribing a complete path, you just have a series of fixed points which are basically become attractors for this discrete epoch of updating, and then you change to the next attractor so you're, you're pulling along.
You're pulling in continual in physical space than say a robotic arm, you're prescribing the street fixed point attractors at discrete points in time, and that are pulling the trajectory around an orbit, say for walking or for, you know, for reaching
limits. So I think this notion of discretization of action is is probably very important practically.
And, you know, and once you start to think about continuous dynamical trajectories and continuous state or space faces as a selection as a sequence of fixed stable unstable points.
There's interesting maths about sort of heteroclinic cycles and the algebras that live in a discrete state space that you can now deploy to start to characterize and generate continuous continuous dynamics.
Yeah, I think that's, yeah, I mean, I don't know that there has been that much work in my field on it but it's, I think it's a really important opportunity to try and put together the continuous with the, with the discrete in the, in the action debate, yeah.
Yeah.
Okay, thank you.
Okay.
My question from the audience.
Probably not. I think we are.
I mean, it's here.
So, let's thank again.
Professor three stone. Yeah, best wishes to everybody. Thank you for attending. Thank you, Carl.
Thank you. Happy Christmas everybody.
Bye bye. Thank you.
Thank you.
