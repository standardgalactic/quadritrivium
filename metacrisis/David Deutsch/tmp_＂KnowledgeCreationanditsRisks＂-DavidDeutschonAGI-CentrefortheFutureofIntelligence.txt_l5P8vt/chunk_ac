So if something isn't general, then it's not an AGI.
The question is what kind of program is an AGI?
I mean, we are GIs.
I think there are very strong arguments why we must be.
And the difference between just an AGI and an AGI is qualitative.
So, well, perhaps I haven't understood your question.
I just asked a follow-on question, which is if all GIs are persons, then are all GIs conscious?
Do you think there's a link there?
Well, I think so.
But we don't know what consciousness is and we don't know how to make an AGI.
And we don't know the theory of AGI's and so on.
We don't know what qualia are.
We don't know any of those things.
I think I'd be very surprised if those five or six things, free will is another one, can be implemented.
Any of them can be implemented without the others.
But if they can, this will raise interesting moral issues because our enlightenment morality is intimately linked with epistemology.
It's like, if you and I disagree about something morally, we ought to be able to discuss it rationally and agree.
Now, if that isn't true, if something has moral significance but is fundamentally unable to be creative, let's say,
then that raises the moral issue about whether that should have the same moral status as somebody who's fully G.
But I myself don't think that problem will arise.
I can't imagine it arising.
But for one thing, this ability that humans have evolved extremely fast.
So, and we can see, we can guess at least why it did, why it was useful or rather why the genes contributed to their own replication.
Now, if that was possible, let's say without qualia, then why on earth did the tremendous machinery of qualia evolve
if it wasn't practically useful to evolution?
So, I think they must be connected, but we shall see.
I had a follow-up question regarding political knowledge versus will.
And we give that example, the very rosy-coloured court child in India can now access all the knowledge in the world.
But I would like to question that by bringing up the issue of malevolence, of people who willingly and knowingly hinder the spread of knowledge,
whether that is of access to knowledge or whether that is fake news.
I mean, I would say the average court child in India that way would not be able to access most of the knowledge on the internet
because it was not written in their language, because they might not be educated,
because it might not be fed well enough to be able to spend time and energy on this.
There might be structural reasons why they would not have internet access.
So, is an increase in knowledge going to solve that?
Yes. So, just because there are people who don't yet have access to the internet,
that in no way indicates that giving access to the other billions of people was a bad idea.
All it is, is a problem.
And a problem like a few people, a small percentage of people in the world don't yet have internet access,
is what is sometimes called a first world problem, except it no longer is.
It is a problem of success.
We wouldn't think that somebody was being deprived of the internet before the internet had been invented.
And we wouldn't think that it is somehow an indictment of our society, of our entire world,
that not everybody has it yet, at a time when only a few thousand people had it.
It just wasn't conceivable.
Malevolence, I did talk about.
Given that there will always be malevolence, I don't know whether that is so or not,
but given, it's supposed that there always will be.
The cure for that is also creativity on the part of the non malevolent people.
In other words, penal policy and improvements in culture and education and so on,
so that the degree of malevolence and the number of malevolent people can be gradually reduced,
and also their capacity to hurt everybody can be reduced.
So we must arrange so that terrorists trying to make this virus that's going to murder everybody,
proceed more slowly because of their perverted ideology or something,
despite having knowledge of biochemistry and whatever,
that the speed of their project will always be less than the speed of those who are trying to invent cures,
and not just specific cures, but the knowledge of how to make cures in general.
This is what's going to keep our civilization in existence.
And that's why I think that moratoriums and so on are perverse to try and do this,
because they are targeting only the good guys.
Isn't there, in a sense, a fundamental distinction between GIs and AGI's that we might create,
in the sense that we've been in beautiful emotions in the most honest sense,
including any kind of desire to connect it to our freedom of will.
In the sense that we create an AGI, we're always in a position to decide whether that AGI has emotions,
and if so, isn't that ultimately connected to the morality of the rights we give to such an AGI,
whether it can be enslaved in the first place or not?
Would you say that the emotion was something that just emerges as soon as you've got something?
Like I said just now, I think it's most plausible that it's inextricable.
If there were, yes, then there would indeed be a moral issue,
and building an AGI with perverse emotions that lead it to immoral actions would be a crime,
but there's a much wider category of crimes with a similar outcome,
namely educating the AGI with evil ideologies.
That can be done whether or not they have emotions.
I suppose at the time of the height of the Enlightenment,
some people would have thought that our emotions are an impediment to being moral,
and now I think it's more that people think having the right emotions is a necessity for being moral.
I think this is the wrong way of thinking about it.
We are universal. The AGI's will be universal.
What specific kind of program they have will determine their actions,
and many of those are indeed evil,
so we have to use what we know to prevent that happening.
When you were saying about the urn and pulling out the flat balls,
one thing that I think you know is from Boston,
I should have given you credit for that.
If we were unlucky and the laws of physics were different,
such that it was really, really easy to make nuclear bombs,
for example, then couldn't there actually be a flat ball in the air?
Isn't there some way that the laws of physics could have been such that there would be a black ball in the air?
Yes, there could have been. Let me give an example.
Suppose the laws of physics were that there are Olympian gods who are watching everything we do,
and when we get too big for our boots,
when they judge that we have a bit too much hubris, they slap us down.
Now, that's a black ball.
Logically, it could be true. It could be there.
The black ball about nuclear weapons being easier and so on,
if that had been so, then one possibility is that
the knowledge of how to cope with that would have evolved earlier.
That is, there would have been nuclear wars, say, in the 18th century,
and the evolution of political culture would have been heavily influenced by that.
The survivors might have wanted that never to happen again, that kind of thing.
Or, like I said, like with the malevolent Greek gods scenario,
it might have been that the laws of physics will extinguish us.
But the laws of physics do not have it in for us.
If they wipe us out, it will be because we have not created the knowledge to prevent that.
It won't be because of the malevolent god things.
All such ideas are bad explanations.
What makes knowledge in your view, is it capacity to solve a problem of relevance?
Yes, so I've gone through five or six definitions of knowledge
in the time that I've been writing about it.
My current definition of knowledge is information with causal power.
That's the definition that comes naturally to construct a theory
because it means you're thinking of knowledge as being a component
of the programming of a universal constructor.
If a bit of information is needed to make a constructor do a particular thing,
then that piece of information is knowledge.
That includes moral knowledge, mathematical knowledge,
knowledge of abstractions as well because mathematicians are physical objects.
If information makes them do something, then it's knowledge and so on.
An explanatory knowledge is a special kind.
Just knowledge in general, knowledge as in, for example, in genes.
Knowledge in genes is dumb knowledge.
It's non-explanatory and therefore it has a finite scope.
There are certain barriers that it can't cross,
whereas explanatory knowledge can cross any barrier
because it doesn't have to have a sequence of viable intermediate forms.
That's also why once you have the capacity to create explanatory knowledge,
you can create any and that's all there is.
There isn't a more powerful means of processing information than that
or of affecting the world.
Can I also come back to the question of political knowledge and political will?
We often hear corporations are greedy, now these CEOs are greedy,
that we're done to get at this Malavalin's question,
but the truth is it's a lot scarier than that.
Nobody, no one person is evil and responsible for capitalism.
There's some kind of abstract structure that the profit moment
that is determined, that is literally killing the planet.
You can't point to a certain set of Malavalin actors as a way to get beyond that.
I don't know, how does simply having political knowledge
or knowledge of climate catastrophe somehow work against this,
you could almost call it A.G. non-I,
but there's no intelligence behind capital.
It's just a kind of non-intelligent abstract force that seems to some extent
impervious to the type of humanistic knowledge we're talking about.
This theory that there is this systemic dependency.
Sorry, our internet went out on this end and it went out right as Ryan concluded his question.
So if you can just start up with answering Ryan's question.
Yeah, well is it a coincidence that just as I was about to give a marvelous answer to this question
about systemic malevolence, it shuts me down.
So yes, it is.
I think this thing you were talking about in general is a thing that the Bay Area people call MOLOC.
It's something which is a general property of a system
which makes the system do what the members whose actions add up to the system don't want.
And I think that all theories of that kind are just false.
So to cut the long story short, all of them assume that the people concerned are not created.
The analysis of the situation is always of the form, well, all the participants are facing this decision
where they have something to gain and something to lose and they maximize their local benefit.
And as a result, all of them are dumped into deep shit.
And so you'll notice about that story and you'll notice about every MOLOC story that the human participants are just ciphers.
They just do automatically what this particular version of the MOLOC story says they're going to do.
And that is never accurate.
It is something that can arise momentarily as a problem along with every other problem.
We have every other kind of problem all the time, so there's nothing unusual about that.
But when it's recognized as a problem, people wonder about it.
They start accusing each other of behaving in that way and defending themselves by saying,
well, what other way could I behave?
And then people think creatively about how they can change the thing so that all the farmers in the valley
that would have benefited from the dam and whatever it is, and none of them wanted to pay,
somebody comes along and invents an idea that they can all get behind and undertake to pay for.
Sometimes, because that's a creative act in itself, there's no guarantee that somebody can instantaneously come to it,
come up with it, but the argument that somehow the system of doing things by persuasion
and doing things by individual rights and property and so on,
should be replaced by something that uses the boot stamping on a human face.
It doesn't work because the knowledge, again, this just assumes the government or whoever does the stamping has that knowledge.
Well, if they have that knowledge, someone else could have that knowledge too.
The government doesn't consist of a lot of the kings or kings with divine right who have some different access to knowledge from ordinary people.
They're just people too.
And if the knowledge to build the dam or to build the park or whatever the story is,
if the knowledge doesn't exist, then the park isn't going to be made until someone invents that knowledge.
Or until somebody works out how to do without the park or whatever.
That's a good question about the distant far future.
If the American goes very, very well in terms of knowledge acquisition and we build systems that can universally gather new knowledge
and we solve all the problems, we keep adding more variables for the end,
and maybe we expend the energy of the sun, but then move on to the stars,
the very distant future of successful knowledge acquisition.
What does this end game look like?
Can you have all the knowledge?
Is there a plan?
I think not.
If you adopt my view that the growth of knowledge consists of converting problems into better problems,
then the idea of the ultimate problem which then can't be solved because it's the best problem doesn't make sense, does it?
That whole picture might be false, but I'm generally speaking a follower of Karl Popper.
The Popperian way of looking at this is that he who tries to prophesy the growth of knowledge is in a state of sin.
That's not a quotation, that's just my paraphrase.
I can't imagine what physics theories are going to be invented in the next 10 years, let alone in the next 10 billion years,
but on general grounds there is no argument or reasonable scenario that we know of today
that can even provide a framework for envisaging the cessation of problems.
If we run out of problems, wouldn't that itself be a problem?
Another definitional question.
The word wisdom is one that is sometimes defined in relation to knowledge,
and I think I remember Alfred North White had talked about wisdom as the handling of knowledge.
