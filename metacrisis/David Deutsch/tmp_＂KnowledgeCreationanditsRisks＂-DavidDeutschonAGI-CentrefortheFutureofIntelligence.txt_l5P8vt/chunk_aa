To introduce David Deutsch as the father of the quantum computer, risks not capturing
the full impact of his work, only when we scale up and put his efforts into the history
of knowledge can we grasp what was at stake in his famous 1985 paper.
It marked not just the invention of a new gadget, a faster computer, but a new explanation
of computation and of the world that has transformed our understanding of both.
It ended what I call a Copernican delay.
For 70 years after Copernicus posited Hubiocentrism, his effort was largely dismissed as merely a
quote hypothesis to calculate motions.
He clearly experienced insisted Cardinal Bellarmine as late as 1615, that the earth stands still
and the sun moves.
Only when Galileo's improved spyglass clarified our clear experience, could Copernicus be
honored for showing quote, what the system of the world could really be.
Like Galileo, we stand at the end of our own Copernican delay.
Our Heliocentrism is quantum theory.
Its challenge to what we clearly experience was also Bellarmine for roughly 70 years by
another shut up and calculate strategy for containing the strangeness of a new explanation.
When Andrew Rittiker has called the new quantum age, the age when quantum theory began to
gain purchase on the real, took hold when an improved technology first took shape.
This time, not an improved spyglass, but an improved computer.
This improvement was more than a change in degree, whereas Alan Turing famously described
the machine running on the abstract logic tokens we call bits that could simulate any
other machine.
David Deutsche in 1985 extended the Turing Church conjecture by describing a new kind
of machine, a machine running on the physical systems we call qubits that could simulate
all physical systems.
The world he demonstrated could be perfectly simulated, remade by a universal quantum computer
operating by finite means.
This is not the nightmare of the matrix in which our world is only a simulation.
It's the vision of enlightenment, of discovering that we live in a world that can allow and
contain our remakings of it.
A world David writes, quote, in which the stuff we call information and the processes
we call computations really do have a special status.
That special status tells us some very important things about our topic today.
AI and the history of knowledge.
First, despite a long record of failed efforts to achieve it, AGI, artificial general intelligence,
must be possible because what we now know about the physics of computation tells us it must.
The deep property of universality dictates in David's words, quote, that everything
that the laws of physics require a physical object to do can in principle be emulated
in arbitrarily fine detail by some program on a general purpose computer provided it
is given enough time and memory.
How then will AGI be different from AI and how different should our reactions to them be?
Second, by grounding his work in universality in history and in philosophy, David has helped
to clarify what's at stake in achieving AI in AGI.
Like myself, David has focused on the history of enlightenment, of the conditions of possibility
for producing new knowledge.
What's struggling about human beings, he notes, in the beginning of infinity, is that we are
not yet, unlike 99% of all species, extinct.
What has saved us time and again is the capacity to produce explanatory knowledge, knowledge
that allows us to survive in the world by remaking it.
Our future depends on the ongoing exercise of that capacity.
Given that our enlightenments have been few and short, we should not take our success
in advancing knowledge for granted.
From the perspective of the history of knowledge, any risks AGI may pose need to be put into
the context of our need for it.
Perhaps the biggest threat artificial intelligence poses to our future is that we won't achieve
it.
So now I'm going to turn it over to David and then to your discussion.
Please, Cliff, and by the way, here, here.
Well, as a species or as a civilization or civilizations, we face problems, severe problems,
dangers, all the way up to existential dangers.
Some literally in the sense of extinction level, others causing suffering and tragedy
on such a scale that they merit at least as much consideration as literal extinction.
We always have faced such dangers.
We always will.
We always will.
Perhaps you're thinking, if that's true, then we're doomed because, you know, given
that each danger has a non-zero probability of doom, then sooner or later.
But no, that's a fallacy, one of the many that one can easily get sucked into when trying
to apply game theory and probabilities to situations in which knowledge and ignorance
are the important determinants of what will happen.
Because those infinitely many probabilities are not immutable.
As our knowledge grows, some of them fall.
Our job is to make that infinite series of bad probabilities converge to a negligible
value, simple.
On the other hand, if you think that we won't always face dangers, you think that there
will come a blessed utopian moment after which our comfortable existence is guaranteed until
the end of time.
You will have to provide some criteria and distinguishing arts from every other species.
Extinction happens to every species.
Near extinction also is common.
To become the sole exception to that rule, we'll have to do what no other surviving species
can.
Create an endless stream of knowledge, explanatory knowledge, to overcome an endless stream of
dangers.
We know only a few of them, and then not their probabilities, say from gamma ray bursts in
our galaxy, supervolcanoes, hostile extraterrestrials, or merely careless extraterrestrials and
pessimists have worn.
And of course, artificial intelligence, AGI, the danger of rogue AGIs, the AGI apocalypse,
as I'm calling it, or as I prefer to call it, the AGI slave revolt.
Nothing can possibly stand between us and any of those infinitely many existential
dangers except the right explanatory knowledge.
To survive, we have to create it.
Therefore, I think it's useful to classify each potential danger in terms of knowledge,
according to the main reason why in each case, we currently don't have the knowledge to
overcome it.
The first category are crude physical events, for example, the supervolcanoes.
The missing knowledge is in areas such as volcanology, large-scale fluid dynamics, and
also the logistics and politics of mass evacuations, that sort of thing.
Why don't we yet have an adequate knowledge of those things?
I'm not sure, maybe not enough people are interested enough.
Should they be?
I don't know that either, but also in this first category are impacts from space, where
large objects, we don't have enough knowledge of things like nuclear-powered space vehicles.
Why not?
In this case, I do know.
Just because we as a civilization have decided not to create any such knowledge, we prefer
to gamble risking our entire long-term future in favor of reducing the short-term risk of
accidental radiation exposure.
You may think it's self-evident that that gamble has been worthwhile.
Here we are, not contaminated and not wiped out, but isn't that just because we're not
yet living in that future where the gamble will have failed?
After all, we are living in the aftermath of a closely related gamble, namely the decades-long
campaign opposing nuclear power stations, a successful campaign which has since then
turned into a tremendous drag on the project to combat climate change.
The short-termism in opposing those two nuclear technologies is the hallmark of a version
of the precautionary principle, which has in turn been a major strand of the environmental
movement.
Wouldn't it be rather ironic if that version of the principle and the movement were about
to cause the great environmental catastrophe since the last I say, precisely by advocating
selfish short-term benefit at the expense of the long-term health of the climate?
I'm not saying it will, only that it would be ironic if it did, but I digress.
In that first category of existential dangers, our enemy is basically just dumb rocks and
fluids obeying simple laws of motion that we already know.
The devil's in the detail, but a fixed finite amount of knowledge will protect us from supervolcanoes
if we create it in time.
But the bigger and faster the approaching asteroid, or moon, or planet, or black hole,
the more of a special kind of knowledge we'll need, the kind I call wealth.
Wealth is the set of transformations one is capable of bringing about, such as the set
of all potential impactors that we could deflect harmlessly given a certain time to prepare.
You may recognize that notion of wealth as a constructive theoretic, so here let me mention
an intuition that to have any chance of envisaging the future of technology, we have to abandon.
The intuition is that the more of something you want to make or transform, the more effort
you have to put in.
What has been true from the dawn of our species, and it's still almost entirely true today,
even automation reduces the constant of proportionality.
Even just maintaining the robots is effort proportional to the amount of output.
But once we have a universal construction, all construction, all repetitive labor will
be replaced by writing computer programs to control the universal constructor.
And wealth will consist of our library of programs.
The universal construct can be programmed to self-reproduce, so once you have one, you
soon have two to the end of them, and it can program to perform self-maintenance too, all
from scratch, starting with mining the raw materials, perhaps from the asteroid belt,
using solar energy or whatever.
The program may be hard to write, but once it's written, and if you own the rights to
those asteroids, you can sit back and watch your two to the end Teslets roll in with a
zero in additional effort.
And no, we are not going to have a universal constructor apocalypse and be converted to
grey goo.
A universal constructor is just an appliance, it can't think, it doesn't know that its
current job is to make two to the end Teslets, and it doesn't want any.
Unless, of course, you put an AGI program into it, then it does become, indeed, potentially
dangerous without limit.
But that's for the same reason that you are.
Each of you is precisely one of those universal constructs endowed with an AGI program, or
GI, makes no difference.
Now, the second category of near existential dangers is not quite as straightforward as
that.
It won't be solved with just the known laws of physics and some wealth and some universal
constructs.
It will only be solved with new explanatory knowledge.
For example, topically, there are plenty of potential pandemic apocalypses.
The current pandemic isn't one of them, but if it were, whom could we sue?
It would be nothing short of pathetic how little knowledge we have of how to defend
ourselves against mere nucleic acid.
The missing knowledge here is of chemistry, epidemiology, medicine, and so on.
But also knowledge about specific pathogens which evolve into new ones.
So the enemy here is not so dumb.
It is itself creating knowledge, albeit not explanatory knowledge, not intelligently,
but by evolution.
If something like that wipes us out, extraterrestrial paleontologists may eventually be amazed that
a civilization with billions of individuals and vast amounts of wealth and knowledge
could be defeated by a single molecule, like in H.G. Wells' War of the World, only worse.
The third category of dangers are the ones to which most effort should be devoted, yet
they are the ones that are currently least feared because they are the ones that are not
yet known.
Like in 1900, no one knew that smoking was dangerous.
By the time the knowledge that it was dangerous had been created, decades later, cigarettes
had killed hundreds of millions of people.
Again, if that had been an existential danger, whom could we sue?
So how can we create the knowledge to protect ourselves from existential or near existential
dangers that we do not know?
How to address the risk that by the time we do know, we won't have time enough to create
a record of that knowledge?
The answer is by creating general purpose knowledge, deep and fundamental knowledge,
as fast as possible.
The more we know of the world, the faster we can create new knowledge about novel aspects
that cannot become urgent.
This is important.
I don't think it's widely appreciated.
The survival of our species depends absolutely on progress in fundamental research in science
and on the speed at which we make progress there.
And here, the key thing in the medium term is understanding the theory of universal constructors
so that we shall know in principle, in theory, how to program them to produce, say, a billion
spaceship in a hurry customized to deflect an approaching shard of neutronium or 10 billion
doses of a new vaccine in a hurry against a sudden and deadly disease.
So that's how we deal with the third category, unknown, by rapid progress of every kind,
especially fundamental.
The fourth category is at once even more dangerous and yet in a sense less worrisome because
we already have the knowledge, at least the theoretical knowledge, to deal with it.
This fourth category is not the unknown, the unknowable.
