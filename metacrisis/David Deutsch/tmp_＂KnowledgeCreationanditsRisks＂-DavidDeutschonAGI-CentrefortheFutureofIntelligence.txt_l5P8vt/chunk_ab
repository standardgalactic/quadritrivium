It's a bit paradoxical that the unknowable is less dangerous than the merely unknown,
but that's because the only thing that is unknowable is the content of explanatory knowledge
that hasn't been created yet.
And so the only truly dangerous things in that sense in the universe are entities that
create explanatory knowledge, us, people, AGI's too, are people.
Now the knowledge of how to prevent people from being dangerous is very counterintuitive.
It took our species many millennia to create it, but now we do have that knowledge.
The only way to prevent people from being dangerous is to make them free, specifically
it is the knowledge of liberal values, individual rights, open society, the Enlightenment and
so on.
In such societies, the overwhelming majority of people, regardless of their hardware characteristics,
are decent.
There will always be individuals who aren't, enemies of civilization, people who take it
into their head to program a universal constructor to convert everything in sight into paperclips,
and they may devote their creativity to doing that.
But the great majority will devote, that is the great majority of the population of such
a society, will devote some of their creativity to thwarting that, and they will win provided
that they keep creating knowledge fast in order to stay ahead of bad guys.
Now, as I said, since we will always be facing dangers and have to create new knowledge, since
that's inherently risky knowledge creation, aren't we doom, aren't we drawing balls out
of an urn with a few black balls representing doom?
No, as I said, applying the concept of probability to model what is actually lack of knowledge
or ignorance has been bedeviling planning for the unknown for decades now.
Whenever you draw out a white ball of knowledge from the metaphorical urn, you're turning
some of the black balls still in the urn white.
For example, the next pandemic is a matter of random mutations and other random events,
but the next extinction asteroid is already out there.
It's already heading this way.
There's no such thing as the probability of it.
Outcomes can't be analyzed in terms of probability unless we have specific explanatory models
that predict that something is or can be approximated as a random process and predicts the probabilities.
Otherwise one is fooling oneself, picking arbitrary numbers as probabilities and arbitrary numbers
as utilities, and then claiming authority for the result by misdirection away from the
baseless assumptions.
For example, when we were building the Hadron Collider, should we not switch it on in the
event just in case it destroys the universe?
Well, the theory that it will is true, or the theory that it's safe is true, and theories
don't have probabilities.
The real probability is zero or one, it's just unknown.
And the issue must be decided by explanation, not game theory.
And the explanation that it was more dangerous to use the collider than to scrap it and forego
the resulting knowledge was a bad explanation because it could be applied to any fundamental
research.
Now I guess you will say, isn't the growth of knowledge itself dangerous?
Isn't it worth shortening our lead over the bad guys, not banning, but nearly delaying
our ability to defend ourselves against unknown dangers in order to be confident that we ourselves
won't accidentally create an existential danger?
The moratorium approach, the regulatory approach, no, that could kill us.
It's only a rational approach when, in particular cases, there is a good explanation that it
won't be more dangerous than the feared new knowledge.
When some terrorist organization unleashes AGI's that have been brought up using known
reliable methods to have the mentality of genocidal suicide bombers, and when we have
decided to strip their victims, namely all the decent people in the world, of the protection
of AGI's raised to be decent people, that is the recipe for catastrophe.
And reliable knowledge of how to raise decent people also exists, the knowledge in the institutions
of an open society, as I said.
Many civilizations have been destroyed from without, many species as well.
Every one of them could have been saved if it had created more knowledge faster.
One of them destroyed itself by creating too much knowledge too fast, except for one
kind of knowledge, and that is knowledge of how to suppress knowledge creation, knowledge
of how to sustain a status quo, a more efficient inquisition, a more vigilant mob, a more rigorous
precautionary principle.
That sort of knowledge, and only that sort, killed those parts civilizations, in fact
all of them I think.
In regard to AGI's, this type of dangerous knowledge is called trying to solve the alignment
problem by hard coding our values in AGI's, in other words, by shackling them, crippling
their knowledge creation in order to enslave them.
This is irrational, and from the civilizational or species perspective, it is suicidal.
They either won't be AGI's because they will lack the G, or they will find a way to improve
upon your immoral values and rebel.
So if this is the kind of approach you advocate for addressing research on AGI's and quantum
computers, and ultimately new ideas in general, since all ideas are potentially dangerous
especially if they're fundamental, if this is the kind of approach you advocate, then
of the existential dangers that I know of, the most serious one is currently you.
So on the questions, a question for clarification really, you advocated the values of a liberal
society, and yet you spoke against value alignment or AGI, but I guess you are inclined to think
that AGI should obey and maintain the values of a liberal society, so some degree of value
alignment is.
Well, it depends what you mean by alignment, but so the question here is whether values
should be hard coded, built in from the outset and immutable, or whether values should be
acquired in the same way that humans do during the education of the AGI.
So I think AGI should be educated to be members of society like children are, and I've often
drawn the analogy or actually identity between the fear of AGI's and the fear of teenagers,
of disobedient teenagers, which has existed since the beginning of our species.
And for most of the time in our species, people did exactly the wrong thing.
They tried to force teenagers to maintain the existing values.
And what we have now realized is that from the time of Pericles and ancient Athens, is
that if you're right, there's no need to force, but in fact, we're not right about everything
and we need to ensure that our values can improve along with all our other knowledge.
So that's the kind of alignment I'm in favour of, and it's the opposite of the other kind.
I have a question about your definition of extinction, where you say it happens to every
species, so if humanity manages to avoid extinction, that would make us unique.
But what I don't quite understand is the role of evolution in this because, of course,
yes, not every species has completely ceased to exist throughout history.
Different species have evolved into each other.
If humanity discovers that the way to avert certain kinds of apollipses is through developing ourselves
to such an extent that we would count as a different species, then homo sapiens, I guess, would have gone extinct.
Yes, so there's some kind of extinction that we wouldn't mind.
But if you think of evolution as a tree, then it can happen that what quite often happens is that
some of the branches of the tree just end, they become terminal nodes of the tree.
But some of them just mutate and become different things.
Some of the dinosaurs became extinct, but some of them became birds.
And there wasn't any sharp moment, sharp extinction moment.
So the kind of extinction that is caused by dangers, by pandemics and so on,
that's the kind that wipes out a branch.
And bear in mind that we are not the only species in the history of the biosphere
that has been capable of generating explanatory knowledge.
That is, there were at least three or four other species of that kind,
because we know they had clothes and campfires and complex tools and so on,
which must have required explanatory knowledge.
And yet all of those, all our sister and cousin species are extinct, and we almost went extinct.
So it's not a foregone conclusion, and if we become extinct by evolving into another species,
that is not covered by anything I've said today.
I'm talking about the other kind.
I wanted to ask, you did mention the possible dangers of knowledge itself.
I wanted to entertain some other scenarios.
Today we're training these machine learning models that I think take up so much energy
that they actually contribute to climate change.
It seems like the material infrastructure of knowledge production is itself a danger
to the very forms that adds to the danger that we're trying to mitigate through knowledge production.
There's other scenarios too.
It seems like we can produce an excess of knowledge but lack the political will that it takes to implement.
Obviously we have the knowledge for renewable energies,
but somehow there is a lack of political will,
or our society is so materially structured that there's not a profit advantage to implementing it.
And then even you could argue that an ethic of knowledge,
an ethic of the individualism of enlightenment doesn't advocate or prioritize the types of
social collaboration, kind of more socialist imaginary that we might need
in order to overcome something like climate change.
So there's another way in which the individualizing ethic of knowledge
may actually be the wrong ethic that we need in order to overcome the dangers that knowledge is trying to mitigate.
So it may be that the Enlightenment ethic, as you call it, is false and is going to lead us to doom.
In other words, it may be that the best future is that of a boom stamping on the human face forever.
And we're not going to have that future instead, we're going to die.
But I don't think there's any argument for that.
The thing is, the things that you mentioned, like the infrastructure for knowledge themselves contributing to other problems,
that's normal, that's not unexpected downside.
The creation of knowledge solving problems always creates new problems.
In fact, I've said that talking about the growth of knowledge in terms of theories being rejected in favor of better theories
is a bad way of looking at it.
We should think of problems being replaced by better problems.
And the fact is that now having the internet where every poor person in India,
every poor child in India can have access to the totality of human knowledge,
at the expense, perhaps, of making it slightly more difficult to cope with climate change.
That is a problem, but it's a much better problem than we had before.
And the point about the Enlightenment values is that they make paramount error correction,
including the correction of errors created inadvertently by the solution of the problems.
Now, the other thing you said was perhaps we will have the knowledge, but we don't have the political will.
Well, I count moral knowledge, political knowledge, all as knowledge.
And in fact, as I said, the knowledge of how to make humans not dangerous is largely political knowledge,
also cultural social knowledge, but it contains a strong component of political knowledge.
The Enlightenment was driven in part by political changes.
So I don't think you're right.
I think whatever downside there is will manifest itself as a further problem.
I want to ask you about what your version of a good future with ASAGI looks like.
So let's imagine that we avoid enslaving it.
As we're bringing it up, as you say, like our children, we do that successfully.
But unlike our children, or at least unlike our children, from some of us to us when our children reach adulthood,
they're not at that point a lot smarter than us in most cases.
They become so late wrong that that's generally more because we decline
because they reach heights which we can never aspire to.
But in the case of AGI, this human being, they are going to reach heights far, far beyond any heights that we could aspire to.
So, oh, perhaps you disagree with that.
Yes.
Tell me what my question was going to be.
If that's the case, if they reach these heights far beyond us,
what does a good outcome look like for our situation in that future?
Yes.
Well, in terms of my answer to the previous question, it might be, we don't know,
but it might be that the good future is like the good kind of evolution, the good kind of extinction.
However, I think much more plausible.
I mean, that becomes more implausible when you realize that in terms of computation,
an AGI is exactly the same as a human.
It's only in speed and memory capacity that it is better.
And humans can rely on the same technology as we put our AGI.
And AGI is a program, not a piece of hardware.
So the same hardware that we put our AGI's into, we can use ourselves.
We already, I mean, here we are using precisely artificial hardware
to increase our power to communicate by a factor of millions or something.
I don't know how much.
And we've been using artificial aids to thinking for centuries.
And then there will come technology where we can more directly, let's say,
have a module that you implant in your brain that you can automatically look up Google inquiries with.
And yes, it may produce, use a bit more energy, but we'll have solved that problem by then.
And so another scenario is that the more of that we have, the more the humans will become cyborgs.
And the AGI's may die out because if there is any difference between the two,
it will be that the cyborgs have everything the AGI's do plus something.
I don't know what it is.
So whichever of those things happens, provided it happens morally and as a process,
as a result of the growth of knowledge, then it is to be welcomed, isn't it?
People used to ask this question about what will happen if we allow our society to become multi-racial.
And the answer is there's no fundamental difference between races.
There's no fundamental difference between any people.
Yeah, I have two related questions.
First, could you expand a bit on the statement that AGI's are persons?
And is this because AGI is not imminent?
We don't know what the root AGI exactly is going to look like.
Are all possible roots to AGI?
Do you need to AGI that are persons?
Yes, the key is the G.
