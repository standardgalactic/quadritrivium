Welcome. You are about to view a recorded talk of the Mathematical Consciousness Science
online seminar series held in 2020. This seminar series aims to explore the role of mathematics
in the scientific study of consciousness and hopes to connect researchers who have an
interest in this topic. While every session of the seminar consists of a talk and discussions,
the latter are not recorded and the following will only contain the talk. We hope you will
enjoy it. For further information please visit our website seminar.math-consciousness.org.
I'm going to go through the basic idea from or approach the basic ideas from a couple of
perspectives just to put everything into context. I want to in particular end up at this notion of
a dual aspect information geometry and how that emerges and how it inherits from predictive
processing or constructivist views of the brain. The second agenda is just to start to fine grain
or drill down on different kinds of inference that may map to different aspects of sentience
and consciousness. So that's where I'm trying to go. Let me start by a nod to where much of these
ideas come from. So I came into this really from the point of view of predictive coding and
predictive processing as it's now currently known. The idea of being here that we actively
construct explanations for our sensorium. So this is nicely illustrated by this oil painter
in the 16th century famed for doing still lives that when viewed from another perspective have
a very different interpretation. So before you saw a bowl of vegetables and now you see a face the
point being made over the point that he was making is that this face this explanation this
hypothesis this fantasy for what caused the sensory impressions is coming from the inside out. So
this turns on its head the 20th century view of information processing where the information comes
from the outside in impresses itself on to use Helmholtz as a nervous mechanism and somehow
information is extracted to conclude there is a face and more looks at it from an active or
constructive point of view where you're checking or using the sensory information to check revise
update your beliefs that you already have in mind about what could have caused those sensory
inputs and indeed this is exactly the view championed by Helmholtz objects are always imagined
as being present in the field of vision as would have to be there in order to produce the same
impression on the nervous mechanism. That idea I think has had quiet traction since its introduction
for example the ideas of Richard Gregory about perception as hypothesis testing
the notion formally has been taken up to great effect by people in machine learning like Jeffrey
Hinton and Peter Diane boring ideas from Richard Feynman specifically variational free energy
formulations in quantum electrodynamics and making that work or unpacking that in the context of the
Bayesian brain or more generally an inference gloss on this ability to confirm your hypotheses
about what caused your sensory data at this point in time. So just taking that notion of sensory
impressions what we have at hand is the challenge of understanding how you and I make sense of the
shadows on our sensory epithelia so what are causing those shadows that we are witness to
impinging upon our eyes and our ears and indeed our interceptors. The solution to that problem
that inference problem you can just lift from standard Bayesian statistics or machine learning
in the latter days and one ends up with a relatively simple scheme which can be construed
in terms of something called Bayesian filtering in the neurosciences it's now become known as
predictive coding but it is the same thing and all it's saying is that if I want to optimize
my expectations the the belief I have about the causes of some sensory impression let's say this
was our sensory input and all I have to do is to minimize a quantity that we can write out as
essentially a precision weighted prediction error so this equation here is a differential
equation that dot means rate of change this means that this is an equation of motion or a flow
so flow on a functional a free energy potential here there is a function of the expectation
and the sensory impressions here and this is a form of a Kalman filter or
predictive coding scheme so this is the prediction and these are the prediction weighted prediction
errors so what are the prediction the prediction errors well let's say we had this sensory input
and we had this expectation about what could have caused this sensory impression here and the idea
is that we can generate through a generative model what we would have seen if this was correct
and this provides a prediction that is subtracted from the sensation to form a prediction error
such that if you were able to minimize the prediction error so that these two things were
the same you have effectively found an explanation or a hypothesis that is a sufficient explanation
to explain your sensorium so the crucial point here is that it is a sufficient explanation it's
not necessarily the veridical explanation you will never know what's out there but if you can get
through like minimizing your prediction errors then you've got a sufficiently good model to
navigate and survive so that perspective provides a nice formal intuitive way of understanding
perception it also enables you to accommodate action in the in the service of minimizing
prediction error because if the only imperative is say to minimize prediction error which will
later become this free energy functional then there are two ways we can do that so if prediction
error is sensations minus predictions we can either change the predictions in our head to make them
more like the sensations or we can change the sensations by acting upon the world to resample
to palpate in a different way to make the sensations more like the predictions so both action
and perception by changing sensations and predictions are just playing the same game
hand in hand trying to minimize that prediction error so we now have if like as an intuitive picture
of active inference inference under action where you are in charge of harvesting those data that
you predict so in some sense your actions now become self-fulfilling prophecies and we'll see
an example of that later so that's the the other sort of the kind of introduction to these ideas
I might give a psychology audience or a psychophysics audience a neurobiological audience
what I want to do now is to change the temper of the talk and just put that to one side and see if
we can get to the same conclusions and the same kind of mathematical phenomenology from a purely
first principle statistical approach and if we can what implications does that have for sentience
so the way I'm going to do that is quickly go through a treatment of the statistics of light
of self-organization with a special emphasis on Markov blankets and I've written a gothic
systems here it should really be non-equilibrium steady state the implications of those non-equilibrium
steady states with Markov blankets in terms of information geometry and I'm going to illustrate
that with a numerical simulation of some active matter or a primordial soup and I'm going to close
with different grades of sentience and active inference that I'm hoping will speak to different
kinds of consciousness of the sort that you will be more familiar talking about so let's start with
this question posed by Skrodinger how can the events in space and time which take place within
the spatial boundary of a living organism be accounted for by physics and chemistry so we're
not going to address that question head on although we will implicitly answer it what we're going to
do first of all is just focus on this notion of a spatial boundary and he would be the first person
to acknowledge that a boundary is itself a statistical object a construct and for those
people who don't know what a Markov blanket is I'll just quickly take you through it
imagine you've got some universe and these nodes of this graph here are states of some
universe some systemic states and they are influencing each other or they have statistical
or probabilistic dependencies denoted by the arrows here if I just identify some
set of states here we call those internal states then the Markov blanket comprises the parents
the children and the parents of the children of these internal states and its role or its
importance is that it provides a statistical shield or set of states that mean that if I wanted to
predict what's going to happen to me my internal states now given the entire rest of the universe
I would only ever need to know these blanket states that enshroud me here so all the information
out there pertinent to what's going on in here is contained within the blanket states so technically
these states are independent of these states given or conditioned upon the state of the blanket
states now I'm going to make a further move which will make a lot more sense in the next slide
I'm just going to introduce a bipartisan of the blanket states into active states
that influence external states but do not influence internal states and sensory states
that influence internal states but are not influenced by the internal states and you may
be asking why have I done that well if one just looks at this probabilistic structure this very
elemental statement of a dynamical world that somehow is coupled through dependencies
and causal structures then we can look at these blanket states and in particular the
partition into sensory active states in conjunction with the internal states that they encompass
as the states of a particle and by a particle I mean something that could be as big as a brain
or as small as an electron or something of the size of a cell all of these things particular
things having common a mark of blanket and in one sense they have to because if you didn't have this
conditional independence you would not be able to measure in a statistical sense the difference
between the thing or the particle and anything else it would become statistically part of the
universe and so I'm just sketching out why that or how that impacts for example in the brain we can
consider all the internal brain states all the synaptic activity efficacy connectivity
of inside the brain as the internal brain states and they influence active states or autonomous
nervous system or motor effectors that cause changes in the external states that then impinge
back on the sensory states that in turn influence the internal states and the missing dependencies
really here and here define this mark of blanket so there's an exchange between the internal
and the outside but it has to be mediated with this with this bi-directional and permeable
interface that is the mark of blanket where the sensory states mediate the influence of
outside on the inside and the active states conversely the inside on the outside and exactly
the same structure holds for a little cell where the surface states could constitute the
and the cell receptors could constitute the sensory states affecting the internal
milieu of the cell in terms of its intracellular processing and chemicals active states could
be active filaments that support the sensory states that endow the cell with a motility
that change the external milieu that again reports back and influences the sensory states
so what we have here that inherits just from having a mark of blanket which every
particle or thing must have is a circular causality that's very reminiscent of the action
perception cycle I use the word circular causality deliberately because that's going to come back
and do some heavy lifting later on right so what I want to do now is just put a bit of statistical
physics on the mark of blanket but to do so I'm going to have to just back off a little bit and
ask you to forget about the mark of blanket for a moment and just bear with me while we drill down
on self-organization in any arbitrary system and then we're going to see what happens when we put
the mark of blanket back in play again so what I've done here is just produce a little graphic
of some universe or some system here just depicted in terms of two states one state here
and one state here so to any one point in time the state of the universe is a point and as time
goes on it moves around in the state space tracing at a manifold or an attracting set
that in fact is called mathematically an attracting set or a random dynamical attractor
or a pullback attractor now the kinds of particles that we're going to be talking about or systems
are those that endure over time and in that instance that licensed me to understand the
density of these trajectories as they unfold over time say for example I was getting up in the morning
having my coffee going out to work well in fact that's what I used to do before coronavirus
but then you know following the my my normal trajectory over the day
of course this could actually represent my heartbeat or it could be on a timescale over years
we have christmas and eastern or some holidays irrespective of the timescale of the nature of
these states this kind of object must be in play and it means that I can interpret the density of
these trajectories in terms of the probability that if you sampled me at any one point in my
heartbeat or any time during the day or during the year this also denotes a probability you will
find me in that state and I'm going to refer to that as a non-equilibrium steady state density
now the important move that I can make just by stating or stipulated defining the kinds of
systems with that we're interested in as dynamical systems that have this that these dynamics that
conform to a long form that endure over time means I can derive from the certain implications
which actually take us back to that predictive coding scheme that we took we started off with
so I won't go through this in detail but just to reassure you that there exists a simple
mathematical description of the the way in which this non-equilibrium steady state density
evolves over time as a function of the amplitude of random fluctuations on the flow
f of the states as a function of where they are in state space and the p corresponding to the
this non-equilibrium steady state density here so this is a Fokker-Planck equation you may know it
as the master equation you may know it as a Schrodinger equation these are all equivalent
formulations of this basic density dynamics or ensemble dynamics for us we can do something
quite useful here because we just said that these this object this probability density does not in
and of itself change over time so we can set this to zero and we can look at the solution
to this Fokker-Planck equation and when we do so we see something very interesting what we see is
that we can express the flow of states at any point in state space here as effectively a mixture
of a gradient descent or in this case a gradient ascent on the log of this probability over state
space so this inherits from something called the Helmholtz decomposition this amplitude of the
random fluctuations determines the degree to which I'm hill climbing this Q here denotes
circuitous or solenoidal flow around probability gradients I'm going to unpack that for you in
the next slide but the key point to take from this is if things exist in particular those things
with Markov blankets because to have a thing you have to have a Markov blanket then we can write
down this kind of equation and this kind of equation essentially means you are doing a gradient
flow or gradient ascent or descent on some quantity that is related to the probability
of me being in a particular state so for those people not familiar with the Fokker-Planck equation
I just want to illustrate that imagine that I have a piece of ink and I'm dropping it
into some solvent say glass of water and what I would expect to happen normally is that the ink
molecules would dissipate due to the random fluctuations and distribute themselves over the
entire volume of the solvent that's not the kind of system that we're interested in here though
because we just said these systems gather themselves up together and are restricted to their attracting
set or the pullback attractor so this is what this kind of system would look like is it would look
as if it was endowed with some capacity to do inverse diffusion to actually diffuse up
concentration gradients and that's exactly what this equation says what it's saying is that
the flow in order for this attracting set to be realized to attain this non-equilibrium
steady state has to have two components one it has to climb the gradients of the probability
not strictly speaking the log probability of the kinds of states it is found in
and this q-term here is this circular flow around the iso probability
that it contours basic structure that we're going to predicate the arguments on
what I want to do with this slide is just convince you or at least provide some sort of
graphical evidence that that formalism that we've just briefly rehearsed underwrites nearly
all of physics as we know it so special cases of this you will have done at school so when the
amplitude of those random fluctuations on the flow goes to zero we arrive at conservative
systems of the sort that would be useful for describing massive bodies, gravitation,
Newtonian laws, Lagrangian mechanics where it's all about this circular flow hence the
planets revolving around the sun and the moon around the earth we can look at the other
special case where we eliminate the solenoid flow or at least increase the amplitude of random
fluctuations and then what we have is a mechanics that applies to fluctuating systems basically
fluctuation dissipation theorem and all of statistical mechanics and statistical thermodynamics
with the usual constants and proportionality here Belsman's constant and all the things you've
done about entropy and enthalpy again at school and we can make another move we can just say
somewhat arbitrarily although there are good reasons for doing it we can just express that
non-equilibrium steady state density as the product of two complex roots and if we do that
and follow it through we get quantum physics and this is actually just a restatement of that
Boca Planck equation that we saw earlier on however if you remember the reason we're doing this
is to see what are the implications of having a Markov blanket and this is where it starts to
get more interesting from our perspective so there's another kind of physics out there that is
unique to systems that have this bipartition into internal and external separated by the
blanket states and that's this Bayesian mechanics which we've mentioned in terms of this
perception action cycle so remember before that everything has to be described as this
this solenoidal gradient flow on the negative on the logarithm of the
the non-equilibrium steady state density that has to be true for the internal states and the
active states namely the blanket states which sorry and the active states where the blanket
states comprises sensory in the active states notice what I've also done here I've swapped out
the log probability here for a free energy functional that is a function of the blanket
states and this object here which will become quite crucial later on which is a
probability distribution over the internal states that is parameterised by the expected
internal states a bit subtle that but that's going to come very important later on but the two
key things that emerge from this relatively straightforward analysis first of all the internal
and active states which will call autonomous states share the same function so it'll look as if
my internal brain and the way I act have tried to do the same thing so we come back this notion
that they're both trying to minimise say prediction error or in this instance more generally
minimising by this gradient flow this variational free energy functional
clearly because of the structure of the Markov blanket this variational free energy
does not depend upon the external states so notice that this thing here is not a function
of the external states it's a function of a function of the external states that parameterised
by the internal states so I'll just preempt or foreshadow some of the dual aspect geometry
conclusion so what we have here is a gradient flow that can be written down as a functional not
of external states but as a function of external states but as a functional of beliefs about
probabilistic beliefs about external states that is never in and of itself directly influenced by
the external states but just what the internal states think about or parameterise
what the internal states parameterise in terms of a belief distribution over the external states
so that's a bit heavy so let me make it a little bit lighter so this free energy functional
world could have many interpretations it clearly on the present analysis
undergirds all of the self-organisation that we've been talking about I'm just making the
point here again I'll try to go through this quickly the point I'm it is being made here
is that this basic structure this basic mechanics where internal states of a system and their
action upon their world can be construed or it looks as if they are minimising this free energy
functional here of their blanket data their sensory data and what they currently encode
it looks very much like for example action and beliefs trying to minimise free energy or maximise
the negative of that basically value where value is simply the probability or the log
probability of your blanket states being in a particular configuration so this if you like can
be viewed as a sort of a mathematical backdrop to reinforcement learning optimal control theory
economist expected utility theory if on the other hand you had been brought up in terms of
information theory then this value function the log probability of the kinds of blanket states
I am likely to be found in the ones I prefer to be found in that define me and just take the
negative of that logarithm that's known as self-information and information theory
try this call it surprise or we'll just call it surprise this free energy quantity is enough
a bound on that surprise here and if we think of this optimise eight from the point of view
of optimisation what we are saying is that we're trying to minimise our surprise and in so doing
we can unpack that in terms of things like integration information theory minimum redundancy
principle of Horace Barlow and indeed the free energy principle that we're going to occasionally
mention throughout this presentation that's nice because the expected value if I do this on
average over time the expected value of self-information or surprise is entropy which means it looks as
if this minimisation take on this gradient flow will be in the service of resisting the dissipation
and decay and disorder entailed by having a high entropy and it'll look as if it's consistent with
the principles of self-organisation or synergetics if you're in laser physics or if you're a physiologist
it'll look like homeostasis just keeping things within bounds minimising homeostatic surprises
if you're a statistician you would look at this quantity say oh this is a probability of some
blanket states or data that I have access to from the out that are a product of the outside world
under some model here so here the two this has been a mark of blanket but now I'm going to say no
it this is actually we from a statistical point of view it's going to stand in for a model and this
is known as Bayesian model evidence also known as the marginal likelihood because I've marginalised
out all the external causes of my blanket states here so then we can sort of speak to people in
who like the Bayesian brain hypothesis people who look at the brain as accumulating evidence
and get back to predictive processing and its special cousins or variants such as predictive
coding oh yeah so the mark of blanket this was sent to me by one of my young colleagues who
moved over to America he's just had a little baby and his wife bought him a mark of blanket so if
you want to mark off blanket you can get these in America so here's little Keira making her
inferences first inferences about the world through through her own mark that's mark off
on the blanket itself right so information geometry I'm going to use numerical simulations here
just to show how these things arise and how you can think about what we've talked about in terms
of a Bayesian mechanics in terms of something called generalised synchrony so but let me just
take you through this with with an illustration what we've done here is just create a little
universe a primordial soup by writing down equations of motion for 128 little particles
or macromolecules each of which has their autonomous dynamics in fact based upon a rinse
attractor and we've equipped these this ensemble of synthetic molecules with strong and weak
attractions and repulsions the details are irrelevant you can generally get this kind
of behaviour from any set of differential equations the reason we wrote these down
is that now we know exactly the influences that one particle exerts over another particle
and because we know that we can actually go in and identify a mark off blanket
and then ask the question is there a little mark off blanket or a little thing or a particle
inside this soup and if so does it look as if it's conforming to the Bayesian brain or conforming to
Horace Barlow's principle of minimum redundancy so I'm showing the same data here
but where I've colour coded all of those molecules once they have attained non-equilibrium steady
state as to whether there are external states that are hidden beyond the mark off blankets so
therefore sometimes referred to as hidden states sensory states that as is typically in these
simulations are underwritten or above the active states in red that then cells enshroud the internal
states what we see here is something that's not unlike a little prion or viral particles
that particle just wriggling around in its primordial soup having attained this
non-equilibrium steady state and coursing in terms of its state space over this attracting set
so this allows us to ask a question do the internal states the blue states appear to infer
the causes of their sensory states the magenta states in other words are they they look as if
they're doing a gradient flow on this model evidence this Bayesian model evidence sometimes
referred to by people like Jacob Hoey as self-evidencing and indeed they do it's illustrated in this
graphic here so what we've done here is find a mixture of internal states that predicts a
mixture of the motion of the external states so this will be a little bit like finding a
neuronal population or a mixture of neurone neurons in a small brain that responded to
say motion in the outside world communicated by via the sensory states so the visual epithelium
and when we look for this and we try to identify those mixtures that maximally show a correlation
we do indeed find that the internal states are in some way parameterizing or encoding or representing
the behavior of the external states despite the fact they can't see them directly
so what we've done here is just plot the prediction of the external state as a function of the
internal state the expected internal state over time along with 90 confidence intervals and the
actual external motion in cyan here and you can see that there's a pretty good inference going on
interestingly remember before I was talking about the the expected internal states that capital
mu that actually is important because what it allows me to do is to sort of look at the expected
internal states and expected external states and just see this synchronization manifold that
on average when the internal states are like this the external states are roughly like this
if I go through and just pick out excursions here and overlay themselves on each other to get at
these averages here what we see is something very much like what we see in electromagnetic
studies of the human brain and cognitive neuroscience for example which is a simulated event-related
potential so here's one from the empirical literature and this is just overlaying these
fluctuations of internal states that are themselves predictive of external states so I've used the
term synchronization manifold here deliberately because all we're looking at here is something
that has been known for centuries in fact introduced the notion was introduced by
historian Christian Huygens as synchronization of chaos or generalized synchronization
so for those of you who haven't come across this before it's most famously articulated by the
observation that when you have many clocks suspended from the same wall or the same beam
in terms of engineering we call them loosely coupled dynamical systems they will inevitably come
to swing in unison and in synchrony so that is the only attracting set that they can possibly occupy
and that exactly describes this generalized synchrony between the outside and the inside
that we're witnessing here so from our point of view this is actually a drawing by Huygens himself
of two clocks suspended from the same beam one clock we can call the internal states
the outside world can be the other clock and the beam become the blanket states with active
and sensory components here I like this perspective because it highlights the pure
symmetry of this structure so if you subscribe to the notion that you're trying to infer your
universe then you also are committed to the idea that your universe is actually trying to infer
you which actually is an interesting perspective when it comes to things like eco niche construction
and the like anyway just looking at the that that synchronization manifold a little bit more
closely it has within it a structure which holds the key to this dual aspect information
geometry that I have repeatedly mentioned it's a very simple consequence if I simply plot a little
state space with internal states here and external states there and I do so for every given blanket
state say sensory input then it has to be the case there must be a probability distribution
for any given blanket state over the internal states and the external states now that's self
evident it's obvious but it has a fundamental implication what it means is that for every
blanket state there is an expected internal state that effectively via this synchronization
manifold parameterizes a belief distribution over the external states so just by having a
mark of blanket and conditioning the inside and the outside on any given blanket state
it must be the case that on average the internal states in some sense represent the external states
and we can formalize that in terms of information geometry I'll just put this slide up and if you
want to talk about what is information geometry we can do that this slide sorry this panel is just
meant to intimate that there are there are two kinds of information geometries going on here
there's a kind of information geometry that you would be dealing with if you were a physicist
dealing with the statistical thermodynamics of this little soup it's a probability distributions
over the internal states in and of themselves and from that you can spin off dissipation
fluctuation dissipation interval fluctuation theorems the second law of thermodynamics should
you want to but there's another kind of information geometry going on here because for every expected
internal state there's another probability distribution over external states about external
states that has its own information geometry which means as I move around in my internal
state space I am moving in two spaces at the same time one is the space of the likely configurations
on average that I will occupy on the inside but at the same time I'm moving around in an
information geometry which is about things on the outside the causes of my blanket states or my
sensorium so in summary internal states parameterized probabilistic beliefs about external states and
equip them with a dual aspect geometry so let me just summarize we've done all the heavy lifting
now in this talk and so let's just summarize where we come from as you know under this
physicists take on self-organization of things that are equipped with Markov blanket the existence
of a Markov blanket necessarily implies a partition of states into internal states
and their blanket states namely sensory and active states and external states that are hidden
behind the blanket and because active states change but are not changed by external states
they will appear to minimize the entropy of external states and their Markov blanket
and this means that action will appear to minimize the structural and functional integrity of the
Markov blanket and this has been framed very elegantly by people like Varela and his colleagues
in terms of auto cohesus internal states appear to infer the hidden causes of sensory states by
maximizing Bayesian model evidence and influence those causes through action and we would put a
Bayesian gloss on that in terms of an inactive kind of inference and in virtue of the implicit
generalized synchrony that must be there if there's a Markov blanket that must be there if
something exists there exists a dual aspect information geometry in which expected internal
states parameterize conditional beliefs over external states again conditioned upon the Markov
blanket so let me just close now by taking the considerations to the next level predicated on
this representational or dual aspect geometry and see what they might mean for sentience but to do
that I've got to if you like provide a taxonomy of different kinds of self-evidencing that you get
from this framework so here's our Markov blanket again just to remind you we've got our Markov
blanket partition here with the Markov blankets and we can lump the the internal states and the
active states as these sort of free energy minimizing prediction error minimizing states
that lend a system a degree of autonomy we can call them autonomous states I'm going to
know that by alpha in a minute the blanket states comprise a sense of inactive states and then
together these can be thought of as the particle in question either a brain or a the synis
don't look at the equations what I want to do here is just show you where two different ways
of unpacking this Bayesian mechanics come from depending upon how far you want to go
in terms of the mathematical formalism so largely what we've done so far is start off
with the assumption that there is a universe that has long run dynamics with random fluctuations
and flows and we've quantified or enumerated the amplitude of these random fluctuations in terms
of this gamma here this inverse precision and if this is true at non-equivalent steady state
when things have characteristics can measured over time then the solution requires this
relationship to be true the flow is related to the gradients of surprise
if the thing in addition has a mark of blanket so it must have then this applies to this and we
now have this hand in hand internal perception like dynamic and active dynamic both in the
surface of minimizing or minimizing suprisal that we can write in terms of this variational
function that depends upon this parameterized belief about the external states mathematically
that's a variational density or approximate posterior density parameterized by the internal
states and we can write down things like the free energy principle as a principle a variational
principle of least action where the action is in this instance denoted by the active states
but action also has another meaning action in physics means the time or the path integral
of energy so there's a whole other layer to this kind of argument which i'm going to show you in
a second but let me just see how far we can get with this kind of formalism so if i commit to that
those dynamics and just write down some of these promoted distributions and just integrate
those equations of motion then you can get quite a long way in terms of simulating active
sentence or active inference so what we've done here is is just write down a generative model
in terms of that probability over external states and the blanket the consequences the blanket states
so that we can derive a free energy functional write down the equations of motion in this instance
just for your interest this little agent has the belief that there is
an invisible object point that's pulling its finger around a trajectory that is itself prescribed by
a Lorenz attractor and by suitably configuring some non-linear mappings when it acts to fulfill
those beliefs to minimize its prediction errors in this instance the predictions about what it
should see and feel that it's actually doing so it's causing its sensations by moving its hand
around we can get it to simulate things like writing and because we are generating that writing
and that action we can think of this in terms of action but also action observations so by
replaying this in the absence of any proprioceptive predictions we can start to simulate
action observation and mirror neuron like behaviors in silico i won't go into any details
all i wanted to do with this slide is say that just with these very fast gradient flows of a
predictive processing predictive coding like flavor you can get quite a long way in terms
of simulating autonomous behavior both in terms of inferring and acting and indeed inferring
what you see being enacted but there's a different kind of autonomous behavior that may be important
for understanding the way that we actually operate as opposed to say an insect or a virus
or a thermostat might might operate and that's in terms of action as a path integral into the
future so there's a whole other bunch of physics you can bring to the table here again leveraging
the contributions of Richard Feynman in terms of the path integral formulation of quantum
electrodynamics we can tell this same story but not in terms directly of the Fokker-Pank equation
but it's equivalent in terms of a path integral formulation where now we're talking not about
variation free energies but variational actions that are integrals of this free energy functional
or its basis which is the suprisal over time and this can be decomposed into all sorts of
interesting components of kinetic energy path independent path dependent terms that's from
our point of view not so interesting it's the homologue that ensues when we adopt the Markov
blanket and what comes out of that now is a slightly different perspective on the way that we can
look as if we are minimizing the path integral of variational free energy and this leads us to
the notion of planning as inference it leads us to choosing actions now that we think will in the
future have a trajectory or path through state space that is the least surprising on average
or conforms to our prior beliefs about the sorts of states that I will encounter in the future
so now put simply we have a statistical physics explanation or calculus for planning and this
is the last one the last slide now this is just an illustration when you put that kind of mechanics
in silico and simulate action now not simply doing the gradient percent on
variational free energy but actually choosing paths of action into the future planning
to minimize on average the expected variational free energy in the future you can start to simulate
things like um saccadic searches hunting for information it's going to reduce the expected
surprise uncertainty by looking over there as opposed to looking over there in brief this
simulation is this showing the the evidence accumulation through an epistemic foraging
of the visual field that is mediated by little saccadic eye movements gathering information
to resolve uncertainty about these conditional beliefs about the causes the external states of
the sensory input here disambiguating between upright sideways and an inverted face so this
slide summarizes the if you like the endpoint of this most elaborated form of free energy
minimization um what it says is you can think of this as sort of senses sensory information
coming in you evaluate your or you change your internal states to optimize your beliefs about
external states through this variational or approximate posterior conditional
belief distribution here simply by minimizing free energy that can be broken down into
inaccuracy and complexity and then you use that to work out what would happen in the future if I
did this or that and that entails a minimization or that selection entails a minimization of the
expected free energy and interesting the expected inaccuracy becomes ambiguity the expected complexity
becomes risk so now what we have is a calculus from first principles that gives you a description
of agents that are risk minimizing and information seeking in the sense that they want to minimize
their ambiguity and indulge in that kind of visual palpation that does this epistemic
foraging or responding to epistemic affordances that we saw in the previous slide so in summary
well this is not a summary this is levels of conversation that we can now have hopefully if
we have time about how far you can take this and what different levels of generative models
would be required to support different kinds of active engagement with your world via sensations
that could be a sensorium could have sentience what does what what's the difference between
unconscious inference in the sense of Helmholtz and literally thinking and planning about the
future does that require deep generative models and the consequences of action what does that mean
for autonomy and agency how do things become how do we lose phenomenal transparency we haven't
talked about precision but that may be an important aspect this encoded of uncertainty
in these belief distributions of external states and what does it mean to have the hypothesis
the fantasy that I am a self and does this underwrite subjective consciousness and
self-awareness so these are questions that I don't have answers to but I thought might
be the kinds of questions that at least some people in the audience might want to might want to
speak to so I'll give the last word to Helmholtz again objects are always imagined as being present
in the field of vision as would have to be there in order to produce the same
impression on the nervous mechanism so with that I just wanted to thank those people
whose ideas that I've been talking about and of course thank you for your attention thank you very
much indeed
