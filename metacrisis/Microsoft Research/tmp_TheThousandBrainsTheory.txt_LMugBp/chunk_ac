Our hypothesis was that grid cells also exist in the neocortex in every column.
And now they're creating reference frames for objects that you interact with in the world,
both physical and abstract objects.
And they represent, they represent the location of that column, that cortical column's input
in that reference frame.
And they're needed for learning the structure of objects in the world and for navigating
our limbs in different parts of our body relative to those objects.
So we have now detailed different parts of this in a series of papers.
This is a visual picture of the same thing I just told you.
So here's, on the left side, the entorhinal cortex.
These are two rooms a rat might be in.
These letters represent locations in the room.
So when the rats in this location, these cells have one activity we'll call it A. When they're
in this section, different activity called B. In different sections here.
And it doesn't matter how I got to C. I can go from A to B to C or direct to A to C. Whenever
the rat's in that position or you're in that position, that activity occurs.
The same thing is going over here, but now just going back to the fingertip, you have
objects like my coffee cup or a pen.
And there is basically a reference frame of points around this thing.
And I've just labeled some of them here.
And as you move your finger relative to these objects, those cells are changing.
And they're representing the location where that sensory input is relative to those objects.
The way grid cells work is really cool, it's fascinating, but it's not easy to describe
in a very short talk.
So in a longer talk, I could tell you all about it, but just trust me, there's a really
cool way.
It's not like Cartesian coordinates.
There is no zero point here.
It's a self-referential reference frame.
And neurons do this, and nature figured out how to do it.
It's really cool.
So then we went back and said, OK, so we didn't know how these cells were representing location.
And we said, we do not know.
There are grid cells coming in these modules, so I call them grid cell modules.
So in every cortical column, there are grid cell modules.
And then last year, we published a paper which describes in very detail how these modules
work related to here, there's an interaction back and forth, so how do they figure out
where you are at the same time they're touching.
And so we worked out the detailed mechanisms for this.
OK, I'm now going to jump forward to today.
And today, most of this has been published, but not all of it.
We're going to do that this year.
We I'm just going to tell you sort of the complexity of what we think is going on in
a cortical column.
Yes?
I had a question about what you said just before.
If I just give you a cup, your eyes will be closed, you won't know exactly where your
finger lands on.
You'll still be able to reconstruct the cup by moving it around.
That suggests that you need to maintain a hypothesis about the initial location.
Yes.
Not just the single location.
Yes, yes.
So, like in simultaneous calculations, nothing to do with it.
Yes.
So how does the brain?
Supervisor, I was going to talk about this.
The question for those who can't hear or hear people online is, how does the brain keep
tracking multiple hypotheses, I'm going to rephrase it.
How does the brain keep tracking multiple hypotheses, because it doesn't know where it is yet.
But it might have some idea where it is, because given what I sensed, I can be in these sorts
of things, in these sorts of places, right?
One of the discoveries we made a number of years ago, and Supervisor is going to talk
about this, so I'll just give you a clue about it, everything in the, all the activations
in the brain are sparse, meaning most of the time, very few cells are active, and very
few, most cells are inactive.
And the way the brain represents uncertainty is it forms unions of patterns.
It activates multiple patterns simultaneously, at the same time.
And because they're sparse, it doesn't get confused.
Surprisingly, if it's not obvious, you might be right up front, but the brain can actually
activate multiple guesses at the same time, in the same set of cells.
And what you see in the neural tissue, when there's uncertainty, you have a lot higher
activity rates, and when you're certain, it gets very, very sparse.
And so, basically, there isn't like a buffer someplace, it's not a classic probability
distribution, it's literally a union of hypothesis that are happening at the same time.
And the mechanisms we showed here show how that union gets resolved.
That's an important part, because we think this way the brain represents information
is really critical, and Supercell is going to talk about that.
Okay, I hope I answered that sufficient for now.
Let me just, I'm not going to explain all of that, I'm just going to jump through it.
We now believe it's going on in every column, we believe there's actually two reference
frames.
There's these two sets of cells in 6A and 6B, and so it's able to, and it's able to represent
sort of two spaces at once.
What the column learns, it learns, first of all, when it's observing something, it learns
the dimensionality of the object.
There's no assumption about, is this a one-dimensional, a two-dimensional, a three-dimensional, and
an n-dimensional object.
It can learn the dimensionality of the object.
In that dimensionality, it can learn the morphology of the object.
What features exist at what point in that space?
It can learn the changes in morphology.
We detailed in this paper last year of how this could go about.
But like, literally, here's a laptop, and when the lid goes down and up, I have to learn
that to change the morphology of this.
So this thing has to be able to learn those things, it's like the behaviors of objects.
It's able to learn both compositional and recursive structures.
So here is an example of a compositional object.
I have this coffee cup, and there's a logo on it.
The logo was learned before I had a coffee cup and I had a logo.
Now I've learned a new object with the coffee cup and the logo.
I don't want to have to relearn the logo.
I want to be able to sign the logo to the coffee cup.
And so that's a compositional object, and it also can learn recursive structures in the
sense I could have a logo with a coffee cup and the coffee cup could have a logo on it
and so on.
And recursive structures are essential for language with other things.
So of course, this system also has a motor output, so it generates motor behaviors, and
it complied to any kind of object, a physical object or an abstract object.
If you were a cortical column, you do not know what your inputs represent.
And you actually don't know what your motor outputs represent.
You're just this bunch of neural tissue up there, and you're trying to figure out how
to model the input space that you're given, given some ability to generate some behaviors.
And so if you attach this to a part of the retina, you'll learn very simple visual objects
here.
And if you attach it to some output of some other regions, you might get some very abstract
things.
Okay.
So we're just going to leave this for a moment.
You have 150,000 copies of this in your brain.
So how does this all come back together again?
I'm going to go back.
So this is where the title of my talk is, The Thousand Brains of Intelligence.
I mentioned earlier the classic view of a hierarchical feature extraction model, which
is underlying basically all convolutional neural networks these days.
But the brain is not like that.
The brain has some hierarchical constructions, but I mentioned earlier that most of the connections
are not that way.
So here, what we're going to say over here, the ultimate view is instead of having, we
have all these columns, and they're all doing the same thing, all doing this complex modeling.
If I sense a cup, like I touch it and I look at it, maybe at the same time, you're going
to invoke a whole bunch of models at different levels in this hierarchy, all about the cup.
Now these models are not identical.
They vary in different ways.
Well, of course, the ones on the left here are all going to be sort of vision-related.
The ones on the right here are going to be all somatic sensory or touch-related.
But here, they're going to be on different parts of your retinal space.
So there'll be some models of this part of the retina built in this part of the retina.
But also, there are even in any of these modalities, these are like different parts of your skin.
Even here, you might have some models built on color, primarily, others more different
types of black and white.
Here you might have some models that have more impact of temperature, which would tell
you the material surface.
Others may not.
It doesn't really matter.
You have this array of columns all modeling the same object.
And then these long-range connections, which I'm just hinting at here, basically allow
all these models to vote.
Even at the lowest levels, even at the primary visual region and the primary somatic sensory
region, we find these connections going across, which make no sense in the hierarchical model.
But they make sense here.
Everybody's trying to guess what's going on.
And so this allows you to resolve ambiguity, it allows you to do inference much faster
without movement.
And it's why you have a single percept of the world.
You may have thousands of models of the object being observed at the same time, but what
you're perceiving is this sort of crystallization across this upper layer, which says, yeah,
we're all agreeing now, this is a coffee cup.
And lots of different people are contributing to this right in the moment, but it doesn't
matter.
And these different models can come in and out based on obscuring of data and so it
doesn't really matter.
We all know it's a coffee cup.
So that's what we call this the 1,000 brain series of intelligence.
My last slide here is the following, and then I'm going to turn over the super time.
So the question is, will these principles be essential for the future of AI?
We care.
It doesn't matter how brains work.
There's a lot of success going on in AI right now, and most of these things, some of them,
but most of these things I just talked about are not part of that.
Well, as I said earlier, I believe some of these will be, and if we think about where
AI is going and what we want it to be and how crude our systems are today compared to
what they could be.
So I'll just hear something I think are absolutely essential in the medium and the long term,
so not the very short term right now.
If we really want to get to the future of AI, you're going to have to have systems at our
sensory motor, their sensory motor learning and inference.
You cannot learn the structure of the world without moving.
You can learn only a very impoverished models of the world, and so we learn mostly by moving
through the world.
I can't learn what this building is unless I move through the world.
You talked about the slam thing earlier.
So in my mind, AI and robotics are not separable.
They're not really separate problems.
They're really the same problem.
These models are going to be based on object-centric reference frames.
That's very clear to me now.
They'll have to do the way that grid cells work, maybe.
Maybe not.
I don't know, but it's going to be worked on object-centric reference frames.
The way grid cells do it is pretty cool, so that might be the right way of going about it.
And then there are going to be many small models with voting.
This allows, it has a lot of advantages doing this way.
Robustness is one, but it's hard to imagine you can really build a true, complete AI system
that didn't work this way.
In the short term or in the near term, there's some things we can do.
This is my clue into Subitite's talk.
He's going to talk about these things right now.
I mentioned earlier these sparse representations.
That's the way brain works, and that leads to a very strong robustness in the representational
