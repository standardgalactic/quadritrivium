Well, it gives me great pleasure today to welcome to Microsoft Research, Subutai and
Jeff Hawkins.
Subutai Ahmed and Jeff Hawkins have been very gracious to come down all the way and make
a very precise visit here.
I think if I start introducing them, we will be here for a long time and go through going
through all their accomplishments, but I will just touch upon the main points starting with
Jeff.
And Jeff has had a lifelong interest in neuroscience.
He created NewMentor and is focused on neurocortical theory.
I think you all probably know that he invented the Palm Pilot, which is just incredibly cool.
And ever since that time, he has been focusing on neuroscience and doing great things here.
And in 2002, he founded the Redwood Neuroscience Institute, where he served as director for
about three years.
And we are going to be incredibly honored to hear about his thoughts on neuroscience.
And then we have Subutai, who is the vice president of research at NewMentor.
And he's going to talk to us today about deep learning, vision, and he has been an entrepreneur
and his detailed theory of the neurocortex.
He holds a PhD in computer science from UIUC, and we are going to have lots of fun.
So thank you very much for being here.
All right, great.
Thanks, Dave.
Well, I hope we have fun.
So we always try to have fun.
Just a couple of words about how this meeting came about.
Subutai and I were in Seattle back in December.
We spent a day at the Allen Brain Institute where we gave a talk.
And at the end of that day, through a mutual acquaintance, we came over here to Microsoft
and had dinner with Sacha Nadella and Eric Harwitz and a few other people just to talk
about what's going on in brain theory.
And after that dinner, we said, hey, maybe it would make sense to come and talk about
our work here at Microsoft Research.
So that's the genesis of why we're here.
And so let's just jump right into it.
The title of our talk is the Thousand Brain State of Intelligence, a framework for understanding
the neocortex and building intelligent machines.
The way this is going to work, I'm going to talk for half the talk, and Subutai is going
to talk to the other half of the talk, so you're going to hear both of us.
So just a word about Nementa.
Nementa is a small company.
We're in Redwood City, Northern California.
And we've been around for about 14 years.
We have about 10 employees.
The way to think about it, just think about it as a research lab.
That's what we are.
We're a research lab.
It's a somewhat independent research lab.
And we have a scientific mission.
Our scientific mission is to reverse engineer the neocortex.
And I'm going to talk about that.
Our goal there is a biologically accurate theory.
This is a biological research mission.
This is not to look for inspiration about how the brain works.
We're not interested in theories inspired by the brain.
We really want to understand how the neurons in the brain work.
We test our theories via empirical data and simulation.
We don't do experimental work ourselves, but we do that with a collaboration of others
in published data.
And everything we do is open and published.
So everything I'm going to present here today has been published in peer review journals.
A few exceptions that haven't been published yet, but we don't hold anything back and there's
no secrets of what we do.
We have a second mission, which is really second in both in priority.
And that is the following, is that to take what we've learned from neocortical theory
and apply it to AI.
Now when I got interested in this field almost 40 years ago, I felt immediately back then
that if we're ever going to build truly intelligent machines, we have to understand how the brain
works.
And I felt that this was really the path there.
I still believe that.
So for the last three years at Nementia, we have done almost nothing on this second mission.
And the reason is because we were having so much success on the first mission.
And we put all that aside just to focus on this stuff.
I'm going to tell you about that.
But in the last few months, Subotai has been reengaging on the machine learning side of
our mission statement.
And he's going to talk about the recent progress we've started to make there.
Okay, so that's who we are.
If just to remind you, the neocortex is about 75% of a human brain by area.
And the other 25% are things that are like the autonomic nervous system, controlling
heart rate and breathing, various basic behaviors such as reflex reactions or even things like
walking and running or controlled by your brain.
Your emotions are in the old brain.
But the neocortex dominates our brain and it's the organ of intelligence.
So anything you think about, anywhere from our perceptions to language of all different
types, mathematics, engineering, science, art, literature, everything that you guys
are paid to do here comes out of your neocortex.
And that's my neocortex is speaking and yours is listening.
So that's the organ we want to understand.
The first question you might think about is what does the neocortex do?
And this is, it took me really a while to totally understand this completely.
Some people think, oh, I'll get some inputs from the sensors and then you act.
That's not really true.
What the neocortex does is it builds a model of the world.
So when you're born, that structure is there, but it really doesn't know anything.
And you have to learn a model of the world.
And your model of the world is extremely complex and rich.
There are tens of thousands of objects you know in the world, you know how they look
and how they feel and how they sound.
You know where these objects are located and how they interact with one another.
Objects have behaviors and you have to learn those too.
So this, my smartphone is an object that has behaviors.
It has all these complex behaviors as I manipulate it, things change on it.
That's all stored in the neocortex.
Even simple things like a door in this room has the behaviors.
It opens, it handles, learns and so on.
You have to learn all that stuff.
The neocortex learns both physical and abstract things.
So you can literally learn how a coffee cup is and what it looks like and how it feels.
But you also can learn concepts related to something like democracy or something like
that and things you've never been able to experience before.
But what it builds is a very complex model of the world.
And why does it do that?
The advantage of this from an evolutionary point of view is it's a predictive model,
meaning it's constantly predicting what's going to happen next in the world.
It's constantly predicting what you're going to see and what you're going to feel, what
you're going to hear.
You're not aware of most of those predictions, but they're happening.
And because it's a predictive model and it's this very complex model of the world, you
can use it to generate behaviors.
So you may sit in this room and listen to me not really doing anything for a while,
but I'm changing your model of the world slightly through this talk.
And later you might behave differently based on that model.
So the question is, how is this structure, which fits in your head, basically learn this
very complex model of the world?
Now if you were to take this out of your head and iron it flat, it'd be about this big.
It's about the size, it's about 1500 square centimeters, 15 inches on a side.
It's just a little bit thicker than this, it's about two and a half millimeters thick.
And so if you could do this and you look at the surface of it, there's no demarcations.
You can't see what anything looks, it's all like one uniform sheet.
And but we do know it's divided in the regions, or functional regions.
And they're in a human, they don't really know, but the estimate's a little bit over
100 different regions that you're neocortical to divide into.
And there are regions that are responsible for vision and hearing and touch and language
and so on, and they're connected together through the white matter in this very complex
way.
Okay, so if you were, if I'm just going to show you here, here I've highlighted a few
of these.
So there's a bunch of visual regions in the back of your brain here.
There's some auditory regions where these primary, where this is where the sensory input
comes into.
So your eyes project to the back of your head, your ears project to the tumble lobes, your
body projects your sensory, your touch, somatosensors touch to this area across the top like that.
And the story that's told to how this works, this is a conventional view, which you can
already guess is really not correct at all.
But the way this classically has been viewed is you have something like your eye and you
have a retina, and it projects to this first region back here, and that somehow extracts
simple features, and then that projects to the next region which extracts more complex
features.
So in the brain you do this three or four steps, and then all of a sudden you have cells that
represent entire objects.
So it's sort of a feature extraction in a hierarchy sort of paradigm.
It's presumed something similar is happening in the other sensory modalities, also not
at all clear how, because you don't have this regular topology on your skin as you do in
the retina.
But the same idea is supposedly happening, and then somehow there's some multimodal objects
occurring in all these other areas that really people don't really understand what they're
doing at all.
So we have somewhat of an idea what's going on in these primary sensory regions, but not
really very well, and then somehow all this other stuff is happening up there.
And so that's the story that's told in a classic view.
If you get a neuroscience textbook, it'll talk about that.
Here's the reality.
Here's a picture of, we can't do this for humans, but this is a primate, this is a macaque
monkey.
This is a very famous picture in neuroscience, if you don't know that.
This came out in 1991 by these two scientists, Feldman and Vanessa.
And these rectangles here are regions of the monkeys in your cortex.
Smaller than ours, but basically the same idea.
And so these are all these different regions, and each one of these lines here represents
millions of nerve fibers connecting them in various ways.
And this thing is really, really crazy complicated, and it doesn't look simple like that thing
up there.
And we can make a few observations about this.
The vast majority of the connections between regions here are not hierarchical at all.
They go all over the place.
So 40% of all possible connections between regions exist, and many regions get input
from 10 or more other regions.
And since this diagram was made 30 years ago, we now know there's even far more connections
that they missed that are going across from all different places.
So in this diagram, you have the touch sensors on the left and the vision regions on the right,
and they're sort of presented as a hierarchy, but it's not really like that at all.
This is what, our brains are like this, but even more complicated.
So somehow this is the organ of intelligence, and we want to understand all this, which
seems quite daunting.
So the next thing we can do is look at the detailed circuitry in the neocortex.
This is within the two and a half millimeter thickness of the neocortex, and see what the
circuitry looks like in there.
And this has been studied for well over 120 years now.
These pictures are made by Cajal back in 1899.
This is the two and a half millimeter thickness.
This is the top of the, towards the skull and towards the center of the brain here.
And what they're showing in here back in those days, we'd see there's these cell types,
these individual cell bodies, and you can see there's sort of a stratification going
on.
And then here they're showing the axons and the dendrites, these are the processes that
come out of the cell body that connect to other neurons, and you can see these are predominantly
vertical.
So there are literally, I'm not joking, tens of thousands of papers have been published
on the architecture in the neocortex that's represented here first back in the 1800s.
Here's a picture that I made, which is showing some of the dominant connections between here.
We spent a lot of time reading these papers, so you can see there's these different layers,
they label them one through six, but that's really a misnomer, it's just sort of a rough
guide, and there's these very prototypical projections that go between different layers
and different cells in different layers, and there's different types of connections that
indicated those here by blue and green, and this is very much more complicated than this.
But this is sort of a prototypical circuitry, and we can make a few general observations
about this.
In general observation, there are dozens of different types of neurons, and by different
types, I mean they have different sort of response properties, different connectivity
properties, maybe different sort of gene expressions, and so on.
They're roughly organized into layers.
Most of the connections in the neocortex are across the layers, so the input comes into
layer four, and then it goes up and down and back and forth like this, and there's very
limited connections horizontally, there's a few layers that predict long distances.
So information comes up and down, up and down, and then it spreads on a few layers
of long distances.
One of the surprising things is that all regions in the neocortex, and people didn't know this
until 20 years ago or so, have a motor output.
So we think about, oh, there's a sensory input that gets, you know, the part that gets your
input from the eyes, and then it goes up and down the heart, and then you do some behavior.
It turns out that everywhere you look, there are these cells in layer five, which project
some place in the rest of the body of the brain to make movement.
So even in the primary visual cortex, the first part that gets input from the retina
has cells that project back to an old part of the brain that moves the eyes, and regions
of the auditory cortex project old parts of the brain that move your head, and so every
sense in the brain is a sensory motor issue.
There's no pure sensory input.
It's sensory motor all the way, every region is sensory motor.
Now, a couple of observations here.
This circuitry and its complexity is remarkably the same everywhere.
It doesn't matter if you look at a region that's doing language or vision or hearing
or some unknown function, it looks like this.
Now this is an area of contention because you can find differences in the different regions.
There are some regions that have more of this cell type and less of that cell type, some
are a little bit thicker, some are a little thinner, some have an extra special little
thing here and that, but the variation between the different areas of the neocortex is remarkably
small compared to the commonality.
So that's an incredible observation and another thing we can say about this is this is a very
complex circuit and it's going to do something complex.
It's not going to do a simple function.
This is the complexity there for a reason.
Okay, now how do you make sense of all this?
The first guy who made sort of the really big observation is this guy here at Vernon
Mound Castle, he's a neurophysiologist at Johns Hopkins and he published his idea in
1978 in sort of a 50-page essay which is somewhat famous and it's one of the most incredible
ideas of all time.
I put it up there with Dharan in terms of the significance of his observation.
He said the reason that all the regions of the neocortex look the same because they're
all doing the same intrinsic function.
They're all doing the same thing and what makes a visual area of vision and a language
area language and some of the sensory areas touch is what you connect it to.
So if you take a region of cortex and you connect it to an eye, you get vision.
If you take a region of cortex and you connect it to ears, you get hearing and that the brain
got big by just replicating the same thing.
He then said a cortical column which is a little bit under a square millimeter, I'll
just call it a square millimeter, but a little bit under a square millimeter contains all
the essential circuitry that you'll see everywhere.
And so a cortical column is the unit of replication and if you can understand what a cortical
column does, then you understand the whole thing.
You can visualize it like this.
You can say, okay, here's the neocortex and these are these columns.
Now they don't look like this.
You actually can't see them like this and not stacked up like this, but functionally
and anatomically one can argue this is the way it is and there's a few places in some
brains where you actually can see columns, but mostly it's not.
It was more like, hey, you won't see them, but they really do exist.
So now in a human, we have 150,000 of these things.
We have 150,000 copies of the same basic circuitry.
There's a corollary to what Bruno Malkowski also proposed, which I came up with.
I don't know if anyone ever pointed out, but if you actually believe what he said that
every column is doing the exact same thing, then by definition, every column must perform
the same functions that the entire neocortex does.
There's no other place for things to happen.
So if I'm going to have prediction occurring someplace in the brain, it's going to be occurring
every column.
If I'm going to learn sequences and be able to play back sequences, that's going to occur
in every column and so on.
But this is such a crazy idea that most neuroscientists really don't know what to do about it.
It's like a tired imagine, but that's what he proposed and it turned out to be true.
So I'm going to jump forward here.
We've done many, many years of research on this and we've been teasing apart different
aspects of how neurons work and how dendrites work and how synapses work and how these circuitries
in the neocortex work.
I'm going to skip all of that into something that happened three years ago.
So we had built up a base knowledge about a lot of things going on there, but three
years ago we had an insight which blew the whole thing open.
That insight started with this coffee cup.
I was literally in my office and I was playing with this coffee cup and I asked a very simple
question.
I said, am I touching this thing with my finger and as I move my finger around, I make predictions
about what it's going to feel and I said to myself, what does the cortex need to know
to predict what my finger is going to feel when I move it and touch the lip?
I can imagine.
I can imagine that feeling.
What does it take to know that?
I said, first of all, it has to know that it's holding a coffee cup because different
objects really did different predictions.
The second thing it needs to know, it needs to know where the location of my finger is
in the reference frame of the coffee cup, relative to the coffee cup.
It doesn't matter where the coffee cup is to me, its orientation doesn't matter, but
I have to know where my finger is in the reference frame of the coffee cup.
I need to know where it is.
I need to know where it will be after I execute a movement.
I'm going to execute a movement and while I'm executing it, the brain has to predict
where the new location will be and then, based on the model of the coffee cup, it can predict
what it's going to sense.
I very quickly realized that this is a universal property in the cortex and that every part
of your skin, when I'm holding this with my hand like this, every part of my body is
predicting what it's going to sense as it moves over this coffee cup and every part has
to know where it is in the reference frame of the coffee cup.
It has some sense of location in the objects it's manipulating.
This was a new idea.
We then ran with this and we published our first paper on this in 2017 and we explained
a lot of the detailed mechanisms of what we think is going on here.
I'll give you just the highlights of it.
You have this your hand and then your finger touching this cup and that typically goes
into layer four and this is the primary input layer in any region and that's your sense
feature.
There's this other very major connection between these cells in layer six and layer four which
is well documented and it has a certain type of effect on layer four.
It's indicated by blue here and we propose that this is representing the location relative
to the object.
Now we have two things here, the sensation and the location relative to the object.
Then we can integrate over time and form a representation of what the object itself
is.
If you think about this is going to represent the cells here, what their activity is going
to represent the coffee cup, these cells are going to be changing based on both the sensory
input and where it is in the cup and you can essentially integrate over time and build
a model of a cup.
Really a model of a cup, the morphology of the cup is like what features are at locations
and your brain has to know this.
We detailed the actual mechanism of how the neurons do this in quite detail.
I'm not going to go through that today.
I'll touch on a little bit later.
We then say well you got multiple columns going on here and so imagine you have different
parts of your skin touching the cup at the same time.
If you just have one finger you have to, in order to either learn the cup or infer the
cup you have to move the finger up, imagine you're reaching into a dark box and you're
trying to recognize what this is.
You have to move your finger to do that.
But here if you're reaching and I grab it with my hand at once I don't have to do that.
I can recognize a cup with one grasp and the reason that is is because there's these long
range connections in this upper layer here, layer two, three, that go across large areas
of the neocortex and these represent a voting mechanism.
So each column is getting some different input, has a guess as to what it might be feeling.
It doesn't really know but they vote together and they settle on what the only thing that's
copacetic with the locations and the senses they have.
So we model this, we show how this works.
We then, you can make the same argument that's going on in all your senses.
You might think vision is different but it's not.
The retina is the best way to think about the retina.
It's just a topological array of sensors just like your skin is a topological array of sensors.
And each column in the primary visual cortex is looking at a small part of the visual object.
Nobody looks at the whole thing.
And so like each part of the visual cortex is like looking through a little straw just
like it's like a fingertip.
And if they had to look through a straw I have to move the straw around to see what I'm
looking at but if I have all those columns active at once bingo I recognize the object.
So we walk through all the details of this but there's a big question.
How is it possible for neurons to establish a reference frame of an object that doesn't
even know what it is and then know where it is on that object?
How could that be done with real neurons?
And so that was, we proposed in this paper where to find the answer and that proposal
turned out to be correct.
So now we're going to skip to that.
It turns out that there are, is a very well studied thing in the brain called the enterional
cortex which is not part of the enterional cortex, it's part of the old brain.
And it has, it creates reference frames.
And what we propose is that the cells, the cell types that revolved a long time ago in
the enterional cortex are now existing in the enterional cortex.
So let me just tell you about grids.
These things are called grid cells.
You might have heard of them.
They're very famous.
They were first discovered by the Moser team in 2005.
And what they do is they create reference frames for an environment.
It's been studied in rats mostly.
And blue here is the enterional cortex.
But we have it too.
You and I have a small enterional cortex and it has these grid cells in it.
And what it does when I mean by an environment is like this room.
This room is an environment.
So my grid cells right now have established a reference frame in this room.
And their activity tells me where I am in the room.
Even if I close my eyes and I walk over here, I have a sense that I'm in a different location
in the room.
I know I've moved.
I know my, the old spot was over there.
I know I'm a little bit closer to this wall, further from that wall.
And so even without any sensory input, there's an internal sort of system for tracking where
you are in this room.
And that's what's happening.
So they represent, they create this reference frame.
They represent the location of the body in the room or the environment.
And this is useful for building maps of rooms, like where are the things in the room?
And for navigation, like how far am I from that door?
And how many steps would I have to take to get there?
The grid cells provide a metric space for doing this.
And this evolved in the old part of the brain for navigation, which was one of the first
things the animals had to do when they started moving around the world.
Our hypothesis was that grid cells also exist in the neocortex in every column.
And now they're creating reference frames for objects that you interact with in the world,
both physical and abstract objects.
And they represent, they represent the location of that column, that cortical column's input
in that reference frame.
And they're needed for learning the structure of objects in the world and for navigating
our limbs in different parts of our body relative to those objects.
So we have now detailed different parts of this in a series of papers.
This is a visual picture of the same thing I just told you.
So here's, on the left side, the entorhinal cortex.
These are two rooms a rat might be in.
These letters represent locations in the room.
So when the rats in this location, these cells have one activity we'll call it A. When they're
in this section, different activity called B. In different sections here.
And it doesn't matter how I got to C. I can go from A to B to C or direct to A to C. Whenever
the rat's in that position or you're in that position, that activity occurs.
The same thing is going over here, but now just going back to the fingertip, you have
objects like my coffee cup or a pen.
And there is basically a reference frame of points around this thing.
And I've just labeled some of them here.
And as you move your finger relative to these objects, those cells are changing.
And they're representing the location where that sensory input is relative to those objects.
The way grid cells work is really cool, it's fascinating, but it's not easy to describe
in a very short talk.
So in a longer talk, I could tell you all about it, but just trust me, there's a really
cool way.
It's not like Cartesian coordinates.
There is no zero point here.
It's a self-referential reference frame.
And neurons do this, and nature figured out how to do it.
It's really cool.
So then we went back and said, OK, so we didn't know how these cells were representing location.
And we said, we do not know.
There are grid cells coming in these modules, so I call them grid cell modules.
So in every cortical column, there are grid cell modules.
And then last year, we published a paper which describes in very detail how these modules
work related to here, there's an interaction back and forth, so how do they figure out
where you are at the same time they're touching.
And so we worked out the detailed mechanisms for this.
OK, I'm now going to jump forward to today.
And today, most of this has been published, but not all of it.
We're going to do that this year.
We I'm just going to tell you sort of the complexity of what we think is going on in
a cortical column.
Yes?
I had a question about what you said just before.
If I just give you a cup, your eyes will be closed, you won't know exactly where your
finger lands on.
You'll still be able to reconstruct the cup by moving it around.
That suggests that you need to maintain a hypothesis about the initial location.
Yes.
Not just the single location.
Yes, yes.
So, like in simultaneous calculations, nothing to do with it.
Yes.
So how does the brain?
Supervisor, I was going to talk about this.
The question for those who can't hear or hear people online is, how does the brain keep
tracking multiple hypotheses, I'm going to rephrase it.
How does the brain keep tracking multiple hypotheses, because it doesn't know where it is yet.
But it might have some idea where it is, because given what I sensed, I can be in these sorts
of things, in these sorts of places, right?
One of the discoveries we made a number of years ago, and Supervisor is going to talk
about this, so I'll just give you a clue about it, everything in the, all the activations
in the brain are sparse, meaning most of the time, very few cells are active, and very
few, most cells are inactive.
And the way the brain represents uncertainty is it forms unions of patterns.
It activates multiple patterns simultaneously, at the same time.
And because they're sparse, it doesn't get confused.
Surprisingly, if it's not obvious, you might be right up front, but the brain can actually
activate multiple guesses at the same time, in the same set of cells.
And what you see in the neural tissue, when there's uncertainty, you have a lot higher
activity rates, and when you're certain, it gets very, very sparse.
And so, basically, there isn't like a buffer someplace, it's not a classic probability
distribution, it's literally a union of hypothesis that are happening at the same time.
And the mechanisms we showed here show how that union gets resolved.
That's an important part, because we think this way the brain represents information
is really critical, and Supercell is going to talk about that.
Okay, I hope I answered that sufficient for now.
Let me just, I'm not going to explain all of that, I'm just going to jump through it.
We now believe it's going on in every column, we believe there's actually two reference
frames.
There's these two sets of cells in 6A and 6B, and so it's able to, and it's able to represent
sort of two spaces at once.
What the column learns, it learns, first of all, when it's observing something, it learns
the dimensionality of the object.
There's no assumption about, is this a one-dimensional, a two-dimensional, a three-dimensional, and
an n-dimensional object.
It can learn the dimensionality of the object.
In that dimensionality, it can learn the morphology of the object.
What features exist at what point in that space?
It can learn the changes in morphology.
We detailed in this paper last year of how this could go about.
But like, literally, here's a laptop, and when the lid goes down and up, I have to learn
that to change the morphology of this.
So this thing has to be able to learn those things, it's like the behaviors of objects.
It's able to learn both compositional and recursive structures.
So here is an example of a compositional object.
I have this coffee cup, and there's a logo on it.
The logo was learned before I had a coffee cup and I had a logo.
Now I've learned a new object with the coffee cup and the logo.
I don't want to have to relearn the logo.
I want to be able to sign the logo to the coffee cup.
And so that's a compositional object, and it also can learn recursive structures in the
sense I could have a logo with a coffee cup and the coffee cup could have a logo on it
and so on.
And recursive structures are essential for language with other things.
So of course, this system also has a motor output, so it generates motor behaviors, and
it complied to any kind of object, a physical object or an abstract object.
If you were a cortical column, you do not know what your inputs represent.
And you actually don't know what your motor outputs represent.
You're just this bunch of neural tissue up there, and you're trying to figure out how
to model the input space that you're given, given some ability to generate some behaviors.
And so if you attach this to a part of the retina, you'll learn very simple visual objects
here.
And if you attach it to some output of some other regions, you might get some very abstract
things.
Okay.
So we're just going to leave this for a moment.
You have 150,000 copies of this in your brain.
So how does this all come back together again?
I'm going to go back.
So this is where the title of my talk is, The Thousand Brains of Intelligence.
I mentioned earlier the classic view of a hierarchical feature extraction model, which
is underlying basically all convolutional neural networks these days.
But the brain is not like that.
The brain has some hierarchical constructions, but I mentioned earlier that most of the connections
are not that way.
So here, what we're going to say over here, the ultimate view is instead of having, we
have all these columns, and they're all doing the same thing, all doing this complex modeling.
If I sense a cup, like I touch it and I look at it, maybe at the same time, you're going
to invoke a whole bunch of models at different levels in this hierarchy, all about the cup.
Now these models are not identical.
They vary in different ways.
Well, of course, the ones on the left here are all going to be sort of vision-related.
The ones on the right here are going to be all somatic sensory or touch-related.
But here, they're going to be on different parts of your retinal space.
So there'll be some models of this part of the retina built in this part of the retina.
But also, there are even in any of these modalities, these are like different parts of your skin.
Even here, you might have some models built on color, primarily, others more different
types of black and white.
Here you might have some models that have more impact of temperature, which would tell
you the material surface.
Others may not.
It doesn't really matter.
You have this array of columns all modeling the same object.
And then these long-range connections, which I'm just hinting at here, basically allow
all these models to vote.
Even at the lowest levels, even at the primary visual region and the primary somatic sensory
region, we find these connections going across, which make no sense in the hierarchical model.
But they make sense here.
Everybody's trying to guess what's going on.
And so this allows you to resolve ambiguity, it allows you to do inference much faster
without movement.
And it's why you have a single percept of the world.
You may have thousands of models of the object being observed at the same time, but what
you're perceiving is this sort of crystallization across this upper layer, which says, yeah,
we're all agreeing now, this is a coffee cup.
And lots of different people are contributing to this right in the moment, but it doesn't
matter.
And these different models can come in and out based on obscuring of data and so it
doesn't really matter.
We all know it's a coffee cup.
So that's what we call this the 1,000 brain series of intelligence.
My last slide here is the following, and then I'm going to turn over the super time.
So the question is, will these principles be essential for the future of AI?
We care.
It doesn't matter how brains work.
There's a lot of success going on in AI right now, and most of these things, some of them,
but most of these things I just talked about are not part of that.
Well, as I said earlier, I believe some of these will be, and if we think about where
AI is going and what we want it to be and how crude our systems are today compared to
what they could be.
So I'll just hear something I think are absolutely essential in the medium and the long term,
so not the very short term right now.
If we really want to get to the future of AI, you're going to have to have systems at our
sensory motor, their sensory motor learning and inference.
You cannot learn the structure of the world without moving.
You can learn only a very impoverished models of the world, and so we learn mostly by moving
through the world.
I can't learn what this building is unless I move through the world.
You talked about the slam thing earlier.
So in my mind, AI and robotics are not separable.
They're not really separate problems.
They're really the same problem.
These models are going to be based on object-centric reference frames.
That's very clear to me now.
They'll have to do the way that grid cells work, maybe.
Maybe not.
I don't know, but it's going to be worked on object-centric reference frames.
The way grid cells do it is pretty cool, so that might be the right way of going about it.
And then there are going to be many small models with voting.
This allows, it has a lot of advantages doing this way.
Robustness is one, but it's hard to imagine you can really build a true, complete AI system
that didn't work this way.
In the short term or in the near term, there's some things we can do.
This is my clue into Subitite's talk.
He's going to talk about these things right now.
I mentioned earlier these sparse representations.
That's the way brain works, and that leads to a very strong robustness in the representational
model in brains.
And in the neuron model that we use, I didn't talk about that.
Subitite will talk about that.
We model neurons quite differently than people at the point neurons are used in artificial
neural networks.
Real neurons don't work like that.
We're going to talk about this, but some of these properties of real neurons are essential
for continuous and online learning.
And Subitite's going to talk about that.
So not that you were going to applaud, but if you were, you can hold it.
And now I'm going to switch.
You have a question?
Yeah.
Do you describe this kind of flat set of sensory motor grounded columns?
I didn't say it was flat.
I said it's mostly non-hierarchical.
I've been hearing it so far.
Then you make these hints of being hierarchical.
So I'm wondering where that hierarchy comes from and is like a conceptual hierarchical
column also grounded in sensory motory.
All right.
So you got the idea that every column is doing the same thing, right?
In reality, if you look at the visual regions in the neocortex, the first three visual regions,
V1, V2, and V4, all receive direct input from the retina.
It's not just going into V1.
It's projecting multiple levels up, but not to all levels.
So one of the things that's going on there is that these different regions here actually
are going to be modeling objects at different spatial sizes on the retina.
So if I had the smallest possible thing I could see, like the smallest print that I
could read, I'm telling you that's being recognized in V1.
Most people would be very surprised to hear that.
Now in addition to that projecting over multiple regions, there is convergence from V1 to V2
and from V2 to V4.
So there's both hierarchy and non-hierarchy going on at the same time.
We don't have a detailed model of how the hierarchical composition works yet.
So fine, but that's what the reality of what it looks like.
So it's not completely flat.
It's definitely not flat.
But it's definitely a lot flatter than most people think.
And very, very few people would think that even in these primary regions and the secondary
regions of somatosentric cortex, that real object recognition is going on.
I'll give you a couple of data points you might find interesting.
In a mouse, and mice can see pretty interesting.
They can do a lot of interesting manipulation and so on.
Mice really have only V1.
They have almost no V2.
Almost all the vision occurs in the primary visual region.
And in humans and monkeys, if you take the monkey, like the macaque I talked about earlier,
25% of the entire neocortex in the monkey is V1 and V2.
These regions are huge, and all the other regions are very much smaller.
And that, too, tells us that most of what we know about vision is occurring in these
lower regions here.
So it's just moving this idea that somehow you've got these feature extractions going
at some big thing up here.
It's just the opposite.
The big stuff is happening down here.
And then there's this convergence as you go up here, and there's more high level concepts.
We don't really understand exactly how that all has formed.
Yeah?
It seems that the success of all of this is hinges on the ability of all these motorists
to actually do different hypotheses a little bit differently.
Yeah.
Do you sense that variability is just an after-effect of the topological differences of the brain,
or is actually there's an arbitrary that says, you must be different?
Make sure I understand the question.
Are you saying that those differences, or is that genetic, or is it learned?
Part of the differences is managed through a higher level of process, or actually they're
just a consequence of the brain being in a cell.
Well, let me try to answer that question.
First of all, the basic theory says the differences are really primarily just where they are and
what they're getting input from, okay?
Much of that is genetically determined.
At birth, you have this structure here like that, and there's also, at birth, there are
differences.
I mentioned that.
There are differences between these regions.
So evolution has figured out that, you know, in some regions I might want a little bit
more of this cell.
At birth, you have these differences.
But primarily, it's just where they are and what the topological input is.
Now, the system is very flexible.
So if you were born without a retina, or you're just blind, generally blind, these regions
don't atrophy.
It's surprising what happens is they get inverted, and they start coming this way to them, and
they actually operate in backwards.
So there's a lot of, like, learning flexibility going on here, but nobody's in charge.
Maybe I didn't understand your question.
No, no, no.
Because it would be that, then it's just not enough to put all these columns together.
Then there's a successful physical structure that led to this thing being revised.
But it's not as, it's not as, it's not precise at all.
It's like you can hook these things up almost anyway, it'll work.
The question is, if you hook it up a little bit differently, you might get a better result.
So for example, let me give you another example.
In normal humans, the size of V1, the area of V1, one of the largest regions in the brain,
in normal humans it varies by a factor of three.
Now, presumably, the people, what I've read is the people who have larger V1s are better
at high acuity, low, you know, if I was really working with very small things, I'd be better
at that.
But the people who don't, they're not as good as that.
They both see, they all think they're normal, life goes on, but some are going to be a little
bit better than some others than others.
So there's a hell of a lot of flexibility in this system, and there's all kinds of
right, you know, trauma, you've knocked some of these things out, everything keeps working,
you rewire stuff, they keep working, one guy at Sargon Caser rewired the ferret's brain
and it kept working, you know, so it's, you know, yeah, you can tune it and make it better,
but it's kind of robust just the way you do it.
Yeah.
So every column has a sensory input and a motor output.
Yeah.
Well, it has an input, it's not all sensory, right, it could be from other columns.
Right.
But are they usually related, for example, is the input that comes from the tip of my
finger coming to a place where the motor output is?
Yes, yes, yes, that's right.
So in this case, in the case of the finger, literally there's a part, the part that's
getting input from the fingertip projects part back down to the spinal cord, which would
be most likely connected to the muscles that would eventually move the finger.
In the retina, it's a little different because the retina moves all at once.
And so in the retina, all the columns in the visual cortex project to this one thing called
the spur of the colliculus, which moves the eye en masse.
So there's less of a topology there, because I can't move my, you know, my eyes differently
in different parts of my eyes, in different ways.
But that's certainly true with the somatosensory system, there's a topological arrangement
to that.
Yes, over there.
This is the only half to talk to, you know, I'm just reminding you.
I'm going to take this one, the next question, then I'm going to let Supitai go.
Aren't the individual columns really topologically separated, or is it just the convenience that
we isolate?
It's mostly a convenience.
Remember this picture here?
I said this is not what it really looks like.
You don't see this, but there's a lot of evidence that as you, if you were to move a probe across
the cortex, even though you don't see these columns, there's a lot of evidence that there
is a sort of a physiological break, not a physical break.
You can't see a dividing line, but there is a physiological break between what this section
here represents and what this section here represents.
And so that's been documented.
And there's a few places like in the rodent somatocentric cortex, they have a very special,
their whiskers are a very special organ, it's an active sensing organ, it's almost like
our fingers and so on, rats and mice move their whiskers in an active way.
And in the barrel cortex, they call it, in the rodent, you actually literally do see
columns.
There's one column per whisker.
It's a direct evidence that this is what's going on, that the whisker is like a fingertip,
I mean rats has an array of whiskers and it's got an array of columns and those you can
see.
But mostly it's a physiological change going across the surface of the cortex.
So it's real, it's not just one continuous thing, but you don't see it physically, mostly.
What about topologically, are there connections between the neighboring things?
In the cortex, and I might end up with this, maybe we can talk about it later, in the cortex
there's really only two sets of layers of cells that project across columns broadly.
And I mentioned that, those are both voting layers, remember I said there's two reference
frames, they're both voting.
So one of the cell types in layer five is the long range projections and one of the cell
types in layer two, three is the long range connections and those guys are all voting
and trying to, because local columns will typically be sensing the same thing.
So within some area here, these guys are all sensing the same thing, so this propagation
of like, hey what are we all seeing happens in some broader area, and so you see those
connections spread over this, but as I said, most circuitry is within a column.
Okay, I'm going to end it right there, and subitide is going to pick up, and we're here
all day so if people want to talk about this longer you can do it.
Oh, now you're going to switch presentations there.
Well, as Jeff said, we've been really focused on the neuroscience for a long time and recently
we started re-engaging on here and my talk's going to be a little bit different, I'm going
to focus on a couple of aspects of what we're doing in here and the basic idea here is,
how can we take what we've learned from the neuroscience and from the neocortex and apply
them to practical systems in a way that we can improve some of the shortcomings and maybe
improve some of the current techniques.
So my talk's going to focus on a couple of different fundamental areas, so Jeff didn't
touch on these directly, but a lot of these are underlie the theories that we're working
on.
So the first area is robustness, which is a pretty key area, and as we discussed the
brain is remarkably robust and resilient to noise from the outside, internal faults and
all sorts of things.
And what we think is that the way that the brain represents information using sparse
patterns of activity is pretty critical in achieving this robustness.
So I'm going to talk about exactly how we think about that and then what we've done
very recently is incorporate its sparse city into deep learning networks in a way that
mimics what we think is going on in the brain and I'll show you some of the results from
that.
The next two are more focused on the learning aspects and I'm going to be talking about continuous
learning and unsupervised learning, these are big research areas in machine learning
as well as in neuroscience, and it turns out that these two are actually handled in
a very similar way in the brain, and it's really the same mechanisms and operations
that lead to both.
And so this is actually really one topic, not two.
And what we think is going on here, in order to really explain that, I'm going to dive
into a little bit more detail about how biological neurons work and how they operate on their
inputs and how they learn.
Although we haven't implemented this in the context of deep learning networks, we do have
some results on real-world data sets, so I'll show you how this concept applies there as
well.
Okay, so let's dive into sparsity and robustness.
So this is a picture of a pyramidal neuron.
This is the most common type of neuron in your neocortex.
Paramidal neurons have thousands of synapses, anywhere from 3,000, actually some places
up to 30,000 synapses.
And what's remarkable is that what's shown here is the dendritic tree, where all the
inputs are coming into the neuron.
And in a single neuron, the dendritic tree is chopped up into tiny little segments, and
within each segment, as few as 8 to 20 synapses can recognize a pattern.
And this is remarkable.
If you consider that thousands of neurons are sending input into it, and this activity
is extremely noisy, how can you possibly recognize patterns robustly using such a tiny fraction
of the available connection?
So this is something that was puzzling us for a while, and we think we understand some
of the combinatorics and math behind this now.
So I'll explain that here.
So here's kind of an abstract view of that, and I'm going to consider binary sparse vector
matching.
So here's kind of a stylized dendrite, and here's a set of inputs that are feeding into
this dendrite.
You can represent the connections on the dendrite as a sparse binary vector with n components
in it, and a 1 here will correspond to an actual connection with a neuron here.
And these connections are learned over time.
And then you can also represent the input as a sparse binary vector with n dimensions
to it, with a 1 being an active unit there.
OK?
What we care about here is, when are there matches, and what are the errors that can
happen when you have matching?
So if you look at the dot product between these two, it counts the overlap between them,
and if the overlap is greater than some threshold, we say it's recognized the pattern.
And we can sort of investigate that simple operation in this context and see what it
looks like.
So here's a picture of the space of possible vectors, and it looks like it turns out the
combinatorics of sparse vectors is really interesting as it relates to robustness and
other properties.
So here the gray circle here represents all possible vectors, and the white circles represent
individual, let's say, dendritic weights or each dendritic segments.
So if you look at x1, for example, you might want to match patterns against that.
Now there's a parameter theta, which controls how precise this match has to be.
And the lower the theta, the more noise you can tolerate.
So if theta is really small, you can tolerate all sorts of changes to the vector, and you
will still match.
The problem, of course, is that as you do that, the risk of false positives increases.
So as you decrease theta, the volume of the set of vectors that match this white circle
increases, but the space is fixed, so you're going to have much higher chance of matching
some other pattern that was potentially corrupted.
So we can count this, and we can figure out exactly what this probability is.
So for completely uniform vectors, we can calculate exactly the ratio of the white sphere to the
gray sphere.
So the numerator counts all of the patterns that will match a candidate vector, and the
denominator is the size of the whole space.
And I'm not going to walk you through the derivation of this, but what's really interesting
here is that as you increase the dimensionality, the size of the space grows much, much faster
than the size of these white circles.
And so the ratio of these two drops very rapidly to zero.
And what this means is that you can maintain extremely robust, noisy matches with a fairly
low theta with a very small chance of false positives.
So it's a pretty remarkable property of these sparse representations.
So this graph shows a simulated version of this.
So here I have, let's say, a dendrite with 24 synapses on it, theta of 12, so you can
tolerate roughly 50% noise in here, and you're looking at inputs with different varying levels
of activity.
And what this graph shows is that the chance of false positives decreases exponentially
with the dimensionality.
So this is this ratio that I was talking about.
And as long as the inputs are sparse, you get extremely low error rates.
So here, if the activity has 128 inputs and you roughly have 2,000 dimensions, your error
rate is down to the 10 to the minus 8, so pretty low.
The other interesting thing to note is this horizontal dotted line there.
If the activity coming in is dense, so in this case about half the number of units,
the error does not decrease.
The combinatorics are not in your favor in that case.
So the error is more or less flat.
The dimensionality doesn't impact it.
So as long as things are sparse and high-dimensional, you get into this really nice regime where
things are extremely robust.
And we wanted to see if this kind of property would hold in deep networks as well.
And of course, with deep networks, you don't work with binary vectors.
You work with scalar-valued vectors, so we wanted to see if the same property would
hold there.
And it turns out it does.
So this is a similar simulation except with scalar vectors.
And the combinatorics that I alluded to still work for scalar vectors.
And a dot product, as long as any of the components are zero, it's not going to affect the match.
So those basic combinatorics are still in play.
However, with scalar vectors, the values, the magnitude of the values are important.
And so in order to get this nice regime, normalization is important.
You have to make sure that both of your vectors are roughly in the same range.
And as long as you are careful about that, you get the same basic properties.
It's not quite as nice error rates as in the binary case, but you still get error rates
that decrease exponentially with dimensionality.
Yeah?
So is this the function's method of independent analysis for when you've got ready to match
the company?
Yeah.
So here, I'm specifically focused on dot products, but you could imagine other functions
would work.
Because you want to ignore the zeros, if either one is zero.
But in real life, how would you do something that's consistent with the above and not the
same?
Yeah.
So this relies on a uniform distribution of vectors.
And reality is not going to be all that.
So the less uniform it is, the worse these properties get.
And so I kind of flip it around and say it's kind of the job of the learning algorithm
and the job of the system to try to enforce uniform entropy or maximize entropy as much
as possible.
Yeah.
The brain does that.
Yeah.
Because of the connection.
Yeah.
The way the inhibition works in the brain is you don't want the same cells to be active
over and over again.
You want an uniform distribution of activity, even if it's very small.
And it's an important point.
And it's hard to measure exactly in the brain, but the correlations in general in the brain
are extremely low.
Okay.
So how can we put this into deep learning systems?
So what I've done is created a differentiable sparse layer.
So on the left, I'm showing a vanilla hidden layer in a neural network.
So you have some input from the layer below.
You have a linear weighted sum of those inputs followed by a ReLU or some other nonlinearity.
And the sparse layer that I've created is very similar to that.
The main differences are as follows.
So first of all, the weight matrix, instead of being dense, is sparse.
So most of the weights are actually zero and they're maintained as zero throughout.
So that's if those connections just didn't exist.
The second thing is there's a, the ReLU is replaced by a K winners layer.
And what this does is just maintain the outputs of the top K units and the rest are set to
zero.
So with ReLU, you just maintain anything above zero.
Here we're maintaining only the top K units.
And you can treat the gradient exactly as you do with ReLU.
It's one for the ones that are winning and zero for everything else.
The one problem with that is very easy in this formulation to get a few units that win
out and stay strong.
And so then you don't get this kind of uniform distribution that you want.
So what we've included is a boosting term that favors units with low activation frequency.
So there's some target level of activity that's determined by the sparsity of your layer.
And if some units are below that average, if their average activation is below that,
you boost their chance of winning in the sorting.
But the output is not affected by the boosting term.
It's just which ones are chosen as the winners.
And this, again, helps maximize the overall entropy of this.
And we've shown this in some past papers.
So it's a very simple construction.
You have sparse weights and sparse activations.
You can also create convolutional layers that are using the same mechanism.
In the results that show you, I did not use sparse weights for the convolutions because
the filter sizes are pretty small.
But in principle, you could do the exact same thing there.
So we've tried this on two different data sets on MNIST and on Google speech commands.
I'll show those results here.
So here I'm showing one and two layer dense networks and one and two layer sparse networks,
the basic test scores.
For MNIST, state-of-the-art test set accuracy, if you don't use data augmentation, is between
98.3 and 99.4.
So both of them are in that range.
The dense networks are a little bit better, as you can see.
But what's really interesting is when you start testing with noisy data sets, and this
plot shows accuracy as you increase the level of noise and the input.
You can see that the sparse networks do dramatically better than the dense networks here.
And here's some examples of noisy versions of MNIST images and the dense and sparse results.
So here's images with 10% noise, and they're still about the same.
Here you have 30% noise and the inputs, and you can see that the sparse network still
does really well, and the dense one doesn't.
And here is with 50% noise.
So that was encouraging.
We want to try it on some harder data sets as well.
So I looked at the Google speech commands data set.
So this is something that Google released a couple of years ago.
There's 65,000 utterances of one-word phrases.
This is harder than MNIST, and state-of-the-art is around 95 to 97.5% for 10 categories.
And again, I tested accuracy with noisy sounds as well.
And then as before, so I have two different types of two-layer dense networks and then
two different sparse networks.
The basic test set accuracies are about the same for dense and sparse, in this case.
But the noise score, and here you can think of this as an area under the curve.
It's sort of the total number of correct classifications under all the noise levels.
You can see, again, that the sparse networks do significantly better than the dense networks,
as we kind of expected from the math.
It's interesting that this super sparse network is one where only 10% of the weights are non-zero
in the upper layers.
And it's remarkable that it actually even gets reasonable test scores here.
Yeah.
So this is really fascinating, because in some neural architectures, automatic architecture
search methods we have been storing here at MSR, we are finding that if you include
sparse city as part of the search itself, so that you do well on regularization inside
your search procedure, you can come up with much more efficient models.
But we have that sparse city at the level of connection.
For example, take dense network, everything is connected to everything, but you can use
sparse city, you don't even need 2% of the connections, and it makes for a much more
efficient network.
And all the connections make the tensors exponentially bigger.
So would you have any comments on what happened at the connections level, at least empirically,
we are finding that, but added with, because you have these interesting layers which are
sparse within them.
Exactly, yeah.
So in order to really get the properties, both vectors have to be sparse.
So the weight vectors as well as the input vectors have to be sparse to really get the
robustness properties.
And I don't know if you were looking at robustness, you may have been just looking at test set
accuracy.
There it's either one will work, but if really to get the noise robustness properties, you
need to have both of them.
Both need to be sparse.
So dense with sparse connections is not as good.
Yeah.
And the 2% number is also interesting.
That's sort of roughly the level of sparse city you see in a lot of areas of the brain
actually.
So that's another interesting factor.
Yeah.
So it's remarkable how much.
Yeah.
Go ahead.
Have you ever did any noise within the layers?
Yeah.
So dropout, for example.
So I tried training with dropout.
It hurts the sparse networks.
So another way to say it is you don't need to use dropout.
And the dense networks sometimes helps.
You have to really tune the dropout rate, but in no case is it anywhere close to the sparse
thing.
So dropout as a regularizer is known to help a little bit and sometimes help with test
set accuracy, but in terms of if you use sparse networks, you just don't need to worry about
dropout.
Okay.
I'm going to switch to the second topic here, which is unsupervised and continuous learning.
And here I'm going to dive back into the neuroscience briefly.
So here's our favorite neuron again, the pyramidal neuron.
It's got about 3,000 to 10,000 synapses, as I mentioned.
And these neurons have a very complicated kind of dendritic morphology here.
And they have different kind of functional properties in the different areas.
So in this green area near the center of the cell, the soma, is where most of the feed-forward
inputs go.
And this acts like your typical neuron that you're used to.
It's a weighted sum plus a nonlinearity.
These inputs tend to drive the cell, and it's sort of the classic point neuron.
But the amazing thing is this is actually only 10% of the synapses on the cell.
And 90% of the synapses are in these other distal areas, these blue areas.
And as I mentioned earlier, in these areas, as few as 8 to 20 clustered synapses can generate,
can detect a pattern.
And they generate what's known as a dendritic spike, an NMDA spike.
This spike travels to the center of the cell, but it does not cause the cell to fire.
So you get this event, this recognition event, that seems to have no impact on the cell in
terms of firing rate.
But if you look inside the cell, it turns out it does actually prime the cell to fire
more strongly in the future.
And these neurons can detect hundreds of these independent sparse patterns all throughout
the dendritic tree.
They're all completely independent.
And so for a long time, it was really puzzling, what is the point of all these synapses if
it doesn't have any direct impact like this?
And so what we think is going on is that these dendritic areas are playing different functions.
So you have the feedforward pattern.
This defines the basic pattern that the cell is recognizing.
And what's going on here is that the synapses in these lower distal areas are detecting
sparse local patterns of activity of nearby neurons, and these are acting as contextual
predictions.
So when some of these patterns are detected, it's going to then prime the cell to be firing
more strongly in the future.
And then the synapses up on the top are getting top-down inputs, mostly from regions above.
And these are also detecting sparse patterns, and they invoke top-down expectations.
So this is a slightly different kind of prediction, but it's also a type of prediction.
So what's going on is that you have this neuron that's trying to predict its own activity
in lots of different contexts.
And then if you look at the learning rules in here that people have discovered, there's
really three very basic learning rules outside of this green area.
And the basic things here are if a cell becomes active, if it fires for some reason, if there
was a prediction.
So in the past, if it fires because of the green input, if there happened to be a prediction
in the past, so some dendritic segment caused a prediction here, we're going to reinforce
that segment.
So we're going to reinforce only the synapses in that segment and not in the rest of the
cell.
There was no prediction, but the cell still fires, which is going to start growing connections
to some random segment on the cell.
And these connections will sub-sample from the input that's coming in.
It's going to be a sparse sampling.
If however the cell was not active, if there was a prediction, that means there was an
incorrect prediction, and we're going to slightly weaken the segment that caused that prediction.
The three very simple learning rules.
And here just to point out, learning consists of growing new connections, and each learning
event is creating a sub-sample consisting of a very sparse vector.
Each neuron can be associated with hundreds of these sparse contextual patterns, and essentially
each neuron is constantly trying to make predictions and learn from its mistakes.
Notice that there's no supervision here, there's no batch notion here.
These learning rules are constantly occurring, so everything is continuously learning.
And it turns out that because these vectors are sparse, and remember, if you're in the
right regime, they're going to be really far apart in this space and not interfere with
one another.
So as long as these vectors are really sparse, there's no interference, and you can keep learning
things without corrupting previous values or other learned things.
So this is another benefit of these highly sparse representations.
So this is a very simple kind of learning scenario here.
So it turns out you can take, you can build a network of these neurons we've done, and
you can get a very powerful predictive learning algorithm.
And I'm not going to walk you through the details of the algorithm, but essentially you
have groups of cells.
Each cell is associating some past activity as context for its current activity.
Just one time step in the past.
It learns continuously, and it turns out it generally does not forget any of the past
patterns because of the sparse representations.
These networks can actually learn really complex high markup order sequences, which means
you can impact the current state based on input that happened many time steps in the
past.
Even though the learning rule is Markovian, it's only looking at the previous state.
So there's a kind of a dynamic programming aspect to it.
And then everything is sparse, so not only can you learn continuously without forgetting,
but it's also extremely fault tolerant of these networks.
And so I'll just show one simple result, and this was published a couple of years ago.
This network works really well with streaming data sources.
So here's a case of New York City taxi demand.
So this is a data set that's released by the New York City Metropolitan Authority.
And you see a typical kind of weekly pattern here.
There's seven kind of bumps here.
And the basic task is to predict taxi demand in the future.
If you look at the error of prediction here, we've tested our network, which is these HTM
networks, which stands for Hierarchical Temple Memory, it's the name of our algorithm.
We've compared it against a bunch of other techniques.
And the error rate of the HTM is approximately the same as the best LSTM network.
So they're about the same error rate.
However, what's interesting is what happens when the statistics change.
And the HTM networks, because they're continuously learning, they adapt very
rapidly to changes in the statistics.
So this is error rate over time.
Here's the case where the statistics of the sequence has changed.
You can see that the error for both HTM and LSTM goes up pretty high.
But then the HTM error rapidly drops back to the baseline rate.
Whereas the LSTM takes quite a long time before it drops back.
And this is true even if you keep retraining the LSTM.
And you can play with the retraining window and all of that, and it doesn't matter.
And that's because LSTMs are fundamentally batch systems.
And there's no notion of recency in the samples.
And it takes a long time before the change statistics are a significant
percentage of the overall data set.
And if you don't train on enough, then the error rate's just high all over the place.
So the kind of continuous learning rule that I described is perfect for
adapting really quickly to the changing statistics.
So just as a summary again here, the way that neurons operate and
the way these dendritic segments operate leads to a very simple,
continuous unsupervised learning rule that can learn continuously without
kind of forgetting previous patterns.
Okay, so I've talked about robustness and continuous learning.
We have kind of a long roadmap of things to do, as Jeff alluded to.
So within robustness, we've only tried on relatively small problems here.
So I'd like to try it on much larger problems.
And it'd be really interesting to test with adversarial systems as well,
to see whether these sparse networks can actually hold up against many of
the adversarial attacks.
With continuous learning, this has not been integrated into a deep learning system.
The algorithm I showed you were just one layer system.
So I think it can be integrated in and
we can keep the same philosophy philosophies in there as we integrate in.
And we can implement these predictive learning rules within deep learning systems.
And I think that will may help enable continuous learning and
unsupervised learning in a very rich way in deep learning systems.
And then beyond that, there's the full 1000 Brains idea.
Jeff talked about the voting mechanisms and voting across sensory modalities and
across regions will add some really interesting robustness properties as well
as other properties.
We want to move to a case where there's many, many small models across
sensory modalities that are hypothesizing what their sensory inputs
are detecting and then voting to resolve ambiguity there.
And then I think it's critical to move to scenarios where at every layer
of a deep learning system, you have inherent object-specific reference frames.
And this is gonna allow much faster learning and much better generalization
because you'll be much more invariant to changes in perspective, for example.
If you can represent things in their own reference frames, okay?
One of the reasons we're here is to see if there's any opportunities for
collaboration and we'd love to discuss with any of you if you're interested in
any of these ideas and see how we can apply them together.
Here's some ideas for possible projects in the range of applications.
I mentioned this idea of testing robustness in adversarial systems.
This is not something we have a lot of expertise in.
It would be great to work with someone who's looking into that and security.
There are tons of security implications here as well.
I think we could test with different domains, such as robotics,
natural language processing, Internet of Things.
And as we incorporate these things as differentiable systems,
they can be applied to just about any deep learning architecture and paradigm,
including, of course, recurrent networks and reinforcement learning and so on.
So it would be great to work with people who have expertise in that and
really see how far we can scale these things.
And then specifically, in terms of larger problems,
MNIST and Google Speech Commands are still relatively toy problems.
We want to attack much larger problems.
As a small lab, it's really hard for us to do ImageNet style stuff.
So we'd love to work with anyone who wants to scale these networks.
There's a lot of really interesting things that can be done in terms of
acceleration and power efficiency.
I didn't really talk about the power advantages of sparse representations,
but they're pretty dramatic in here.
Unfortunately, sparse computations are not very well suited to GPUs.
And they're really well suited to FPGAs and other computations.
So I think accelerating these things are going to be quite challenging and
interesting, okay?
So hopefully some of you guys are interested in these things and come talk to us.
And then there's a picture of our research team and our contact info here.
So thank you.
So I don't know how you want to handle it now.
You want us to, I have people who want to hang out, what time is it by the way?
It's about quarter to 12.
So we went over our hour.
We're here, so if people want to just hang out and answer questions, great.
There's actually one more question.
Sure.
This is adjet.
Learning also happens when we see and watch images and
movies about the world on a screen.
Why doesn't that confuse the coverage sensor motor system?
Is it understood how the brain is switching between the physical world and
these projections?
So I think the question, if I understand it correctly, is when we look at a flat
screen, even with maybe just one eye, so we don't even have stereophobic vision at
all, how is it that we build this depth model of the world and so on?
That's a great question.
We have some clues as the answers of that, that it looks as this, I talked about
those grid cells as representing location.
It looks like they are, not only are they driven by your internal motor commands,
but they can be driven by sensory clues such as flow.
And so the way, I use this example a lot.
Like matching you're watching someone play a third person's first person
shooting game, I never play these games.
But you're going through this maze and you're running around.
You have, you're following this thing and you know where that player is and
you know where they are on the map.
And so all that's happening, the whole grid cell thing and
all those location things are happening, even though there isn't this depth thing.
So it doesn't require, it doesn't require that you have some three-dimensional
camera, it doesn't require stereoptic vision.
And what it requires is that there are sensory clues such as flow.
And you look in the neocortex, these things are highly represented in the cortex.
So various types of vector flow fields that are occurring are driving the system,
even though there isn't a three-dimensional sensor.
It's an interesting question, why I'm looking at you right now,
why do you appear there and not on my retina, right?
My perception of you are there even though you're actually on my retina, right?
So it's the same problem really in some sense.
How does I know you're there and not here?
And it's because of these sensory clues that are really extracted early on from,
if you look at all the sensory streams, both tactile and vision,
they have this sense of motion and flow built into them.
So you have tactile senses which detect movement and your visual and your retina as well.
So I think that's a general answer to that question, but maybe not really super detailed answer.
The people who went to the cinema, or who were in the 20th century and the 30th century,
I'm not aware of the real thinking that this...
Oh, the train's going to come out of...
Yeah, they want you there and they say,
oh my god, the train's going to hit us, you know, yeah.
It's a fascinating thing, I just try to expand it to even make,
just realize how crazy it is that you perceive everything at a distance all the time.
How is that?
I mean, in hindsight, it's so obvious that everything has a location representation.
This was not obvious three years ago.
We just said, oh, I know it's out there, I'm not sure how, but how do you know it's out there?
What kind of neurons are representing it?
Anyway, it's a great question and we don't really have all the answers to it,
but that's the basic gist of the answer, yeah.
When you go to language, which is, I guess, less physical, what role do the columns play there?
So the question's about language and it's less physical,
how do we understand this theory in terms of that?
We don't really understand, but I'll give you a couple of clues here.
First of all, language consists of words and whether they're written words,
whether they're spoken words, whether they're sign language,
those are all physical objects that you can model.
So you've got columns in your auditory cortex that are building auditory models of words.
You've got columns in your visual cortex that are building visual models of the world
and they can vote, so that's why I can hear a partial thing and see something,
maybe watch your, and I can put these together.
So we start off with atoms of language that are really physical objects,
but then how do they get their conceptual nature?
We don't know the answer to that question yet,
but there's some very interesting things that happened recently in neuroscience.
We derive this theory based on the idea that how it is that I touch objects or see objects.
There's a whole other set of research that's just come on in the last couple of years
where they're studying humans using fMRI,
and they've shown that even when you're thinking about conceptual objects in your head,
you're imagining various things,
that there's evidence that there are grid cells underlying it.
So they've discovered this using very clever imaging techniques
where you can sit in an fMRI machine.
I had this slide that came up or like it showed you about birds,
but so no one really understands this yet,
but the evidence is very, very clear that this is what's going on.
So how do you map conceptual objects like words into this location space?
We don't really know yet.
I mentioned the issues of recursion, which is a big part of what language is,
if you read about Chomsky and so on.
So all these things are triangulating, saying that is what's going on.
We just don't understand it yet completely,
but we do know it's going to be based on location frames,
it's going to be based on recursion of location frames.
We have all this evidence which is triangulating on that,
and that's just a fascinating thing to think about, but-
And of course, the circuitry of the brain that is responsible for language
looks identical to the video art.
Yeah.
So it has to be the same basic function.
So to me, this feels more in the...
To me, the big hurdle we overcame was just understanding
how this reference frame concept applies throughout everything.
And now it's more turning the crank and going down these different pieces
and explaining how it is that we do all these different components
and how do we put together a broader theory of concepts in language
and abstract concepts.
We don't really know yet, but it's going to be...
It's all in front of us, all the pieces are there.
So you just have to put them...
Just think about them correctly.
That's my thinking.
So is there any part of HTM that talks about different brain regions
and why that structure exists?
When you say different brain regions outside of the neocortex,
or like the different old parts of them?
Well, in neocortex, certainly,
we don't explicitly state that, but it's essentially...
I kind of alluded to it earlier.
A column has some input, and it's going to model that input.
It's going to model the sensory model...
Build the sensory model model of that input.
That input can't be very large.
It has to be fairly small.
You can't have a very, very super high-dimensional input to that thing.
So what you see is that the topology in the brain
is that some part of the retina projects to a single column,
another part projects to a single column, and so on.
So if you could build a visual system with one column,
and it would be like looking through a straw,
and that would be a complete visual system,
and it would learn by moving the straw around,
and it would be like moving this little window around
and looking at stuff, and that would work.
It's pretty straightforward just to expand out
to have a whole bunch of those working at the same time.
And so I don't think we haven't modeled that, per se.
ARC focus has been on really what does the column do,
and with the belief that once you understand what the column does,
the rest of it becomes pretty easy.
It was really the tricky part to figure out what a column does.
So we're not able to scale up to that stuff.
We abandoned all of that a while ago.
We just said, let's focus on a column.
Let's focus on one column, and we just
got to nail that one column.
So all of our simulations have been very small.
I mean, small than tens of thousands of neurons,
all would be contained in a single column.
And to scale up to human brain size stuff,
A, it's not theoretically important at this point in time,
but also we don't really have the ability to do that.
But I think it'd be fairly simple to do.
OK, and on the flip side, do you have any notion of a mini column?
Yeah, we do.
I didn't talk about mini columns.
For those who don't know, a mini column is a structure.
These are physical structures.
You can see them in the neocortex.
They're somewhere between 30 and 80 microns wide.
That's really small.
There's several hundred of them in a cortical column.
Just one point to note that mini columns are only
visible in primates.
So people who study rats don't see them.
That doesn't mean they don't exist.
You just can't see them.
So the equivalent could be there,
but they just don't see them.
The network that Subitai mentioned earlier,
and he said he wasn't going to talk about it, which
is the one that sort of learned sequences and built
on this neuron model we have.
In fact, all the networks we talk about that I mentioned here,
how all this stuff works, it's built on mini columns.
And whoops, what was that?
That was my phone.
And I can tell you briefly what we think.
There's a whole bunch of parts of this.
But a mini column is essentially one
of the ways we use it, is that all the cells in the mini column
have the same basic feed forward response property.
So if I find a V1 neuron that's response to an edge,
this is known neuroscience, all those cells
have the same sort of visual response property.
But in context of real world animal moving about
and observing real things, it becomes very sparse.
And so at any point in time, only one of those cells
becomes active.
So it's a way of taking an input and sparsifying it
in context.
So you can take, if I had no context,
all the cells become active.
And I basically say I have some set of features.
In context, you say I have a unique representation
of that input.
It's the same input, but it's a very, very unique representation.
All this is in the 2016 paper.
All detailed and gory detail about how this works.
Yeah, and we're right in the moment,
I'm expanding the concept of mini columns
because I actually think, and I'm not certain of this yet,
I talked about these grid cell modules.
I'm working on the idea that the grid cell modules may actually
be one per mini column.
They're very small.
And so each mini column could be representing not only a feature,
but it could also be representing a part of the location space.
So it's integral to the structure.
None of this has to be done this way
in an artificial neural network.
You don't have to have these physical structures.
You can arrange the things any way you want.
But in the brain, it looks like we do have roles
for mini columns.
So do you think that grid cells would provide
part of the context?
Yes, exactly what they are.
This was the clue.
So there's two ways we did it.
With the sequence memory, remember when we were talking
about the taxi data?
The context there was the previous state of the same cells.
It's like, where am I in this sequence?
This previous state tells me what to predict my next state.
All we did to get this whole sensory motor inference system
working is add another input, which is the grid cells.
So the grid cells are basically saying,
you can look at your previous state,
but you can also look at the location.
And which one of those works better for you?
It's such a spatial and temporal.
Yeah, spatial and temporal.
And it learns on its own which that layer four cell, we believe,
actually learns on its own which are the proper contacts
in which to make a prediction.
So you said you have a very small team and no lab.
No wet lab.
No wet lab.
We have a lab.
Several points in the box.
We had this idea and it proves to be correct.
So can you talk a little bit about the process
of how you were about showing these hypotheses correct?
Well, OK.
So I talked about it briefly, but I'll just go through it again.
There's two basic ways.
One is verifying via empirical data
and the other is via simulation.
And so the empirical data is the first thing we do
is we go through existing literature.
There are just incredible amounts of existing literature
that nobody knows about just because we all forgot about it.
So we say, hey, we have this idea that dendrites ought
to do this.
Can we find it?
Someone find that one.
Or can we find falsification for it?
We very often can find falsification for our theories
and we go back to square one.
We don't accept anything which is not biologically accurate.
Then we go and talk to the experimental labs
and we say, here's what we think.
What do you guys think about this?
Sometimes they can find data that they haven't published.
They say, yeah, we have this data.
We didn't think about it that way, but let's go look at it.
Or they publish it, but that's been very fruitful.
So we have collaborations with labs.
Some people want to test our data with new experiments,
but that takes forever.
A typical rat experiment could take two years
from start to finish.
And so some of that's going on, but we can't wait for it.
There's another thing I'll point out
that there's something I like to use which most people,
some people just think you shouldn't do that.
But the number of constraints you're
satisfying at any point in time with a theory
is an indication of how good it is.
So if there are 25 constraints, like these
are biological strengths, we know
the neurons look like this in the samples.
Do this, and this, and this, and this.
At first, it makes your theories much, much harder.
How do I satisfy all those constraints simultaneously?
It's almost impossible.
But when you actually get an answer
which satisfies many constraints,
you're almost certain it's right.
And that is proving itself true over and over and over again.
It's not proof, but we work on it.
And if you loosen up your constraints,
let me just give some biological inspiration.
You can come up with anything.
If you really want to satisfy the real biology,
it is really hard.
It is not easy to come up with solutions, trust me.
And so it takes us a long time to come up
with solutions to these problems in times of years.
Yeah.
I have a question that's more to my cover boy.
So obviously what you're trying to do
is kind of translate some of those insights
into some applications.
So one thing that kind of stands out
in this particular talk is, for example,
the column or mini column, whatever,
is extremely important.
And isn't it time to kind of upgrade
the model of the neural net?
Like we are still dealing with individual neurons.
And now we learn that those units kind of very
agnabative units are extremely important as units
in their own rights.
And I don't think I've ever seen a neural net model
built on the notion of a column or something like that.
Do you ever try to do that?
First of all, I 100% agree with you.
I think we have to move beyond just layers
of simple point neurons.
And it's sort of what I alluded to here.
And so for me, the first step was really making sure
we have sparsity handled.
And now I'm focusing on integrating the neuron model.
So not just point neuron, but the dendritic structure
in there and then a larger scale structures
like the mini column and a cortical column.
And that's going to be necessary
for including object-centric reference frames
and including motor input
and having a true sensory motor predictive system.
And I think this is going to really pay
a huge dividends down the road.
I mean, we think this is the future of AI.
So this is really clear.
We think this, the architecture we talked about here
is going to be out 20 years from now.
This is what people are going to be building.
How do you get there?
How do you move people in that direction?
And these are complex problems, things to implement.
So we're thinking through that process
of how do we get there?
How do we start?
One step at a time.
It's interesting to think about what Jeff Hinton did recently
with his capsules.
He had this intuition that some sort of location
and relative location stuff had to be important,
but he didn't have the deep neuroscience knowledge
that we'd have.
And so we have a much richer idea
of what's going on there than he does.
But it's the same basic intuition
that we need to go into the different
sort of representational framework
and that incorporates much more sophistication
in each column to do it.
Right now, the traditional artificial neural networks
are really very simple.
It's almost like one neuron.
But God, it's the same thing.
And that's why you have to have 100 levels of them
and they're kind of limited and they have a lot of problems.
Yeah, maybe I'll point out convolutional neural networks
were originally inspired by the biology a little bit.
And so they have your filters, feature detectors
followed by a pooling step.
Well, that corresponds in this kind of diagram
to input coming into layer four,
going up to layer two, three, and then up to the next level.
If you count the number of synapses in a cortical column
that match that model, it's less than 1%.
So it doesn't match 99% of what's going on in our brain
if you look at the individual connections.
So all of this other complexity
has to be incorporated.
You're saying these two layers without the blue part,
without the blue part, just this input to here
and input to there, yeah.
That's basically a convolutional neural network.
And so you have to incorporate all of this structure
eventually to get, we think, in order
to get truly intelligent systems.
Have you seen anyone do these glues there?
No, I mean, I think, well, capsules
is the closest that I've seen.
And it does incorporate some of these intuitions in there.
But by and large, no, I think most people are stuck with.
So you should know that this theory we presented here
was built on insight three years ago.
The first publications were a year ago.
We just published the omnibus paper about this in December.
I presented it to a large 700 neuroscientists
in October in Europe at the Human Brain Summit.
It's all very new.
The people I've been thinking about this,
this idea of location stuff, you won't find it anywhere
in the neuroscience literature other than at the R&D.
So we're just starting on this path.
And one of the reasons we're here
is to see how quickly we can get people to work with us
on this and adopt it and how people react to it.
What do you think of this?
Do you buy it?
Do you believe it?
Does it bother you that your systems don't look like this now?
This is a tremendous research roadmap
for machine intelligence in general in here.
There's so many rich ideas in here.
And we know this is how the brain works now.
We have strong confidence in that.
And we can point to which of these structures,
the exact benefits is going to have down the road
for practical systems.
So things do have to move this way, I think.
Is that all of this paper you mentioned, one of the latest
in the talk?
It's the 2018 one.
Yeah, if you go to numenta.com, there's a paper section.
So they're all listed there.
That was in front of you in December, didn't it?
Yeah.
You probably have to run for lunch with Eric.
OK.
OK.
Eric, we don't have to leave.
OK.
Yeah.
So it's good.
Unfortunately, we'll have to extract this a little bit.
You got our emails, addresses there?
So please free to reach out and interact with them.
Yeah.
Yeah.
