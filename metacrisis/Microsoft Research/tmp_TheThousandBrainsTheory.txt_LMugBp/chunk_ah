I'm working on the idea that the grid cell modules may actually
be one per mini column.
They're very small.
And so each mini column could be representing not only a feature,
but it could also be representing a part of the location space.
So it's integral to the structure.
None of this has to be done this way
in an artificial neural network.
You don't have to have these physical structures.
You can arrange the things any way you want.
But in the brain, it looks like we do have roles
for mini columns.
So do you think that grid cells would provide
part of the context?
Yes, exactly what they are.
This was the clue.
So there's two ways we did it.
With the sequence memory, remember when we were talking
about the taxi data?
The context there was the previous state of the same cells.
It's like, where am I in this sequence?
This previous state tells me what to predict my next state.
All we did to get this whole sensory motor inference system
working is add another input, which is the grid cells.
So the grid cells are basically saying,
you can look at your previous state,
but you can also look at the location.
And which one of those works better for you?
It's such a spatial and temporal.
Yeah, spatial and temporal.
And it learns on its own which that layer four cell, we believe,
actually learns on its own which are the proper contacts
in which to make a prediction.
So you said you have a very small team and no lab.
No wet lab.
No wet lab.
We have a lab.
Several points in the box.
We had this idea and it proves to be correct.
So can you talk a little bit about the process
of how you were about showing these hypotheses correct?
Well, OK.
So I talked about it briefly, but I'll just go through it again.
There's two basic ways.
One is verifying via empirical data
and the other is via simulation.
And so the empirical data is the first thing we do
is we go through existing literature.
There are just incredible amounts of existing literature
that nobody knows about just because we all forgot about it.
So we say, hey, we have this idea that dendrites ought
to do this.
Can we find it?
Someone find that one.
Or can we find falsification for it?
We very often can find falsification for our theories
and we go back to square one.
We don't accept anything which is not biologically accurate.
Then we go and talk to the experimental labs
and we say, here's what we think.
What do you guys think about this?
Sometimes they can find data that they haven't published.
They say, yeah, we have this data.
We didn't think about it that way, but let's go look at it.
Or they publish it, but that's been very fruitful.
So we have collaborations with labs.
Some people want to test our data with new experiments,
but that takes forever.
A typical rat experiment could take two years
from start to finish.
And so some of that's going on, but we can't wait for it.
There's another thing I'll point out
that there's something I like to use which most people,
some people just think you shouldn't do that.
But the number of constraints you're
satisfying at any point in time with a theory
is an indication of how good it is.
So if there are 25 constraints, like these
are biological strengths, we know
the neurons look like this in the samples.
Do this, and this, and this, and this.
At first, it makes your theories much, much harder.
How do I satisfy all those constraints simultaneously?
It's almost impossible.
But when you actually get an answer
which satisfies many constraints,
you're almost certain it's right.
And that is proving itself true over and over and over again.
It's not proof, but we work on it.
And if you loosen up your constraints,
let me just give some biological inspiration.
You can come up with anything.
If you really want to satisfy the real biology,
it is really hard.
It is not easy to come up with solutions, trust me.
And so it takes us a long time to come up
with solutions to these problems in times of years.
Yeah.
I have a question that's more to my cover boy.
So obviously what you're trying to do
is kind of translate some of those insights
into some applications.
So one thing that kind of stands out
in this particular talk is, for example,
the column or mini column, whatever,
is extremely important.
And isn't it time to kind of upgrade
the model of the neural net?
Like we are still dealing with individual neurons.
And now we learn that those units kind of very
agnabative units are extremely important as units
in their own rights.
And I don't think I've ever seen a neural net model
built on the notion of a column or something like that.
Do you ever try to do that?
First of all, I 100% agree with you.
I think we have to move beyond just layers
of simple point neurons.
And it's sort of what I alluded to here.
And so for me, the first step was really making sure
we have sparsity handled.
And now I'm focusing on integrating the neuron model.
So not just point neuron, but the dendritic structure
in there and then a larger scale structures
like the mini column and a cortical column.
And that's going to be necessary
for including object-centric reference frames
and including motor input
and having a true sensory motor predictive system.
And I think this is going to really pay
a huge dividends down the road.
I mean, we think this is the future of AI.
So this is really clear.
We think this, the architecture we talked about here
is going to be out 20 years from now.
This is what people are going to be building.
How do you get there?
How do you move people in that direction?
And these are complex problems, things to implement.
So we're thinking through that process
of how do we get there?
How do we start?
One step at a time.
It's interesting to think about what Jeff Hinton did recently
with his capsules.
He had this intuition that some sort of location
and relative location stuff had to be important,
but he didn't have the deep neuroscience knowledge
that we'd have.
And so we have a much richer idea
of what's going on there than he does.
But it's the same basic intuition
that we need to go into the different
sort of representational framework
and that incorporates much more sophistication
in each column to do it.
Right now, the traditional artificial neural networks
are really very simple.
It's almost like one neuron.
But God, it's the same thing.
And that's why you have to have 100 levels of them
and they're kind of limited and they have a lot of problems.
Yeah, maybe I'll point out convolutional neural networks
were originally inspired by the biology a little bit.
And so they have your filters, feature detectors
followed by a pooling step.
Well, that corresponds in this kind of diagram
to input coming into layer four,
going up to layer two, three, and then up to the next level.
If you count the number of synapses in a cortical column
that match that model, it's less than 1%.
So it doesn't match 99% of what's going on in our brain
if you look at the individual connections.
So all of this other complexity
has to be incorporated.
You're saying these two layers without the blue part,
without the blue part, just this input to here
and input to there, yeah.
That's basically a convolutional neural network.
And so you have to incorporate all of this structure
eventually to get, we think, in order
to get truly intelligent systems.
Have you seen anyone do these glues there?
No, I mean, I think, well, capsules
is the closest that I've seen.
And it does incorporate some of these intuitions in there.
But by and large, no, I think most people are stuck with.
So you should know that this theory we presented here
was built on insight three years ago.
The first publications were a year ago.
We just published the omnibus paper about this in December.
I presented it to a large 700 neuroscientists
in October in Europe at the Human Brain Summit.
It's all very new.
The people I've been thinking about this,
this idea of location stuff, you won't find it anywhere
in the neuroscience literature other than at the R&D.
So we're just starting on this path.
And one of the reasons we're here
is to see how quickly we can get people to work with us
