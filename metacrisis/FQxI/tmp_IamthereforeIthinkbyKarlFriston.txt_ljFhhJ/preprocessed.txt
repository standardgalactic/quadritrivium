The talks in our three bits, I'm going to start off in a physically way, talking about
statistics of life and bring to the table a key notion based upon Markov Blankets and
the idea that to talk about things you have to be able to formally distinguish between
what constitutes a thing and what constitutes anything else that is not that thing. And I'm
going to predicate that separation on Markov Blankets. I'm then going to look at the same
emergent principles from that physics perspective and tell essentially the same story from the
point of view of a neuroscientist or a neurobiologist and look at the same sort of dynamics and
message passing and indeed we are going to see the same formal similarities with predictive
coding in hierarchical neural networks. And then if we have time I'll take that to the next level
and ask some questions about planning as inference. So we're coming back using this sort of take on
perception and translating it into an inactive context. I'm going to talk about active inference
and how we actually choose to sample the data, the sensory data about which we make inferences.
So I'm going to start where other people have started basically
scrolling us what is life and then specifically this question here, how can the events in space
and time which take place within the spatial boundary of a living organism be accounted for
by physics and chemistry? And what I want to pull out of that question is the notion of
the spatial boundary. And I'm going to suppose that that boundary is constituted or given
by a Markov Blanket. So I'm going to cartoon some states of some universe. I'm going to identify
some states, a high dimensional vector, a state vector of internal states of a system,
a particle that I want to separate from the rest of the universe. And then the Markov Blanket
constitutes or is constituted by the parents, the children and the parents of the children
where the edges or the arrows here denote conditional dependencies, causal influences of
a probabilistic sort. So if you're familiar with any Markovian process, the present is the Markov
Blanket that insulates the past from the future. So it's a permeable because it's based upon
conditional independences. It's a permeable boundary or barrier or blanket allowing in the
case of just a simple time process, informations we saw in Milo's talk to go in one direction.
But more generally in this instance, it can be permeated as it's open in both directions but
provides a separation in the sense that if I wanted to predict the internal states from the
rest of the universe, it is sufficient to know the blanket states because there's a conditional
independence between the internal states and the external states. So everything I need to know
about the internal states in terms of being able to predict it probabilistically is contained
within the Markov Blanket. And I'm going to make a further move here. I'm going to partition
the blanket into active states in red and sensory states here where sensory states
are not influenced by internal states, whereas active states are not influenced by external
states. I'm not saying that necessarily both of those subsets are non-empty but for any given
Markov Blanket, you one can always find that by partition. And I'm going to motivate that partition
this by inviting you to think about how those conditional dependences would manifest in some
of our favourite biological organisms or systems, for example the brain here. So we can have the
internal states of the brain corresponding to all the neural activity and the connectivity
that we've been talking about and those produce changes in our active states, our actuators,
our muscles, or to autonomic nerve systems that change the external states, that then influence
our sensory epithelia, our sensory states to produce changes in the internal states,
and so the cycle continues and indeed we're going to sell this as the basis of a perception
action cycle. And exactly the same conditional independence structure can be seen in a simple
cell, intercellular states affecting say active filaments that support and provide
motility to the surface, the sensory states here that are pushed into the external milieu,
providing influences, a sensorium on the cell receptors or surface of the cell that affect
the internal states. And again you've got this open communication between the internal and the
external states mediated by this two-way traffic across the blanket states but is separated into
sensory and active high roads. So what I'd like you to do for the moment is just forget about
the Markov blanket. What I'm going to do is just run through some basic treatment of non-equilibrium
steady state and then we're going to put the Markov blanket back into play and see what it
looks like and see how one might interpret the ensuing dynamics. So what I wanted to do here
is just set up a description of the sort or the kind of systems that we're going to try
and understand. Those systems have to have a Markov blanket in order to be separated from the
non-system. Furthermore they have to exist over non-triple periods of time so I'm going to assume
that that is a long period of time and I'm going to assume that we can describe the system in terms
of a long-run equation with a flow and some random fluctuations here and almost stipulatively the
systems I'm interested in are those that attain a non-equilibrium steady state. I'll just cartoon
that here in terms of two states coursing a trajectory over time on the attracting set or
pullback attractor you can think about this as you getting up in the morning having your coffee
going to work or you could think about this as some very itinerant attracting set that would
describe the beat of your heart or indeed longer time scales Christmas and Easter. Whatever you want
to however you want to interpret this the key thing is that technically this pullback attractor
requires you to keep revisiting certain states of being that you once occupied and can be measured
as being in so you have particular characteristics that endure over time. Just a little point of
contact here in a very trivial sense because I'm predicating everything that follows on this
long-run system or formulation here in a sense I've actually baked into everything a causality
in the sense that x causes the flow of x and furthermore because this is a differential
equation with respect to time there's also an arrow of time built into this and right from the
beginning I've both got a time and causality underwriting everything else that follows and what
does follow well I can describe the density dynamics here in terms of the Fokker Planck equation
but crucially because I'm only interested in non-equilibrium steady state the rate of change of
the density dynamics is zero which means I can rearrange the Fokker Planck equation to express
the flow in terms of the log probability of this non-equilibrium steady state distribution so
doing that by P of x here and furthermore what I've done here is use the Helmholtz decomposition
just to split that flow at non-equilibrium steady state into a divergence free part
and a solenoidal curl free part here divided by gamma that plays the role of the amplitude
half the covariance of the random fluctuations and this solenoidal part here so the non-physicists
in the audience I thought it'd be intuitive just to unpack this in terms of
heuristically is imagine that you're dropping a drop of oil into some solvent and normally what
we'd expect to happen is the random fluctuations would disperse it but what were the sorts of
systems that we're interested in here have this property of they gather themselves up again to
attain some form open systems to attain some form of steady state and the way that I like to think
about this is just decomposition this Helmholtz decomposition means that it must be the case
that you've got this sort of flow or say ink molecules here up the concentration gradient
so you've got a gradient flow up this log probability gradient here where the solenoidal flow
that breaks the detailed bounds circulates around on these iso probability contours
so that's the basic setup that I'm using that would apply to any system that has attained some form
of existence in the sense of a non-equilibrium steady state what about the Markov blanket
I'm going to first of all just make the point that this setup is basically just a lot of
foundational in an educational sense physics so for example I can quench or reduce the amplitude
of the random fluctuations to zero and then I can rewrite that flow equation that Helmholtz decomposition
in terms of Lagrangian or classical dynamics and recover Newton's laws of motion I can focus in
on the amplitude of the random fluctuations and if I equip those random fluctuations with some
units they play the role of temperature and from that we can derive stochastic thermodynamics and
all the inherent fluctuation theorems I could even in a slightly tongue-in-cheek way simply work with
the square root of this density here and this equation can be re-expressed in terms of the
Schrodinger wave equation what I wanted to point out here though is that there's all this good physics
can be extended if we now consider the Markov blanket so the rest of the talk is going to be
really focusing on a Bayesian mechanics that is compliant and entirely consistent with all these
formulations but is specifically concerned with the dynamics of that partition including the Markov
blanket and in particular the change of the internal states and the active states basically the
states that constitute me in an autonomous way that are not influenced by external states
and that brings something interesting to the to the table it means that I can write down
my internal states and my active states as a solenoidal gradient flow on that log probability
that I can upper bound in terms of a free energy functional which we'll describe in a second
that is itself interesting because that free energy functional is just a functional of the
states that constitute me as a particle and that's going to acquire a particular interpretation
when I associate the internal states as playing the role or stupidively parameterizing a belief
about the external states so we now have a mechanics where we can deal with the beliefs that I hold
in virtue of having particular physical internal states about the external states and now we're
going to interpret this quickly the first way I'm going to interpret this is just by
trying to establish links with established or as extend formulations in the life sciences
so here's our generalized gradient flow here or solenoidal gradient flow I've just unpacked it
please forgive the font substitution errors I just unpacked it in terms of active states
and internal states which we're going to liken to action and perception I'm just making the point
that be the blanket states here play the role of a value in the sense that both action and
perception according to this gradient flow are trying to well look as if they are trying to
increase the log probability of these particular blanket states given it a particular markoff
blanket so I can interpret this as a value function they think the sorts of states that
attract me or it will appear that I am attracted to I was trying to return to even if I am an open
system from that we can spin off expected utility theory and economics optimal control theory
this is just self-information from the information theoretic point of view a surprise which is
upper bounded by this variational free energy which means that exactly the same dynamics can
be interpreted as trying to minimize surprise or minimize self-information maximize efficiency
minimize redundancy and indeed the free energy principle what I'm talking about expectation
of that is going to be entropy so it looks as if this the average flow of internal and active states
is trying to minimize the the dispersion of my blanket states and of course that
is the holy grail of self-organization synergetics and indeed if you're a physiologist it's just
homeostasis the interpretation I want to focus on though is the one that we heard about
in the implicitly in the in the previous two talks this quantity in statistics is known as
model evidence and if this interpretation holds is it means that any system that exists in the
sense of possessing a mark of blanket that has attained an equilibrium steady state well look
as if it's trying to maximize its Bayesian model evidence which suggests that there's an implicit
model of the world encoded by its internal states and that's basically the the story I'm going to
tell and this for fun I was sent this by a young student who's now working in my 11 in America
this is his daughter I think about two months ago and his wife had bought him this blanket with
Markov on the front so this making the point you can actually buy a Markov blanket in America now
should you want them so this is a little keyer making inferences of the world through her Markov
blanket anyway and to close then I'm just going to tell a story which licenses an interpretation
of this gradient flow in terms of self-evidencing prediction predictive coding and very much all
the same themes that have been underwriting the previous three presentations and I'm going to
tell that story as if I was talking to a room of psychologists and this story goes a long way back
and it casts the brain as or any system with internal states as a constructive system as a
dynamic statistical organ that's trying to create explanations and hypotheses for its sensations
so this is nicely illustrated by this artist who's famed for doing oils that when viewed
from the right perspective give you a very different explanation as to how they were caused
so if you now see a face there you made that face it's from the inside out it was
not actually in the sensory signals and that's the the basic theme I think most beautifully
articulated by Helmholtz so objects are always imagined as being present in the field of vision
as would have to be there in order to produce the same impression on the nervous system
again speaking to the notion it's it's a model or a hypothesis that you are using the data
in order to test the quality of your explanation for your sensory and that's very closely related
to ideas of perceptions hypothesis testing by Richard Gregory in psychology ideas that have been
used by great if to great effect by people like Jeffrey Hinton and Peter Diane that we now see
in variants of deep learning like variational autoencoders that rest upon
variational free energy of the source used by Richard Feynman and of course Bayesian probability
theory so objects are always imagined as being present in the field of visionals that have to
be there in order to produce the same impression on the nervous system so this resonates very much
with the notion this Markov blanket upon which the world the universe is projecting its sensory
impressions and if I am look as if I am trying to model the causes or the external states that
generated those sensory states my job is effectively or it will appear as if my job is to work out
what caused these shadows so one very plausible architecture which you've actually already heard
about so I'll just go through this bit quickly can be written down as a version of this gradient
flow here by rearranging the terms the solenoidal term and the gradient flow term here in terms of
two bits that define in engineering what we Bayesian filtering or a linear case a cowman
filter basically a prediction and an update where the update here has been expressed in terms of
prediction as this is a gradient term times the amplitude the random fluctuations so heuristically
what we're saying here is this is if this was my sensory impression and I had some expectation
encoded by my physical internal states and this would be exactly the same as say the hypothesis
in Max's talk about the functional form that were best explained these noisy data on perhaps
non-noisy data I will put this hypothesis to the data if I could generate it with some generative
model what would the data look like what would the sense states look like if this was true I can
then simply compare the prediction with the actual sensation and the prediction error is just the
difference so all of this is all this is saying basically is that it must be the case that internal
states and action will be minimizing prediction error it doesn't necessarily mean of course that
you actually know what's going on the outside and so I would have just said that we can apply
exactly the same idea to perception and action and move in a way that minimizes our prediction
error we can then spin out the architect is in exactly the same way that we saw yesterday in
terms of hierarchal message passing where we have the ascending and descending streams that are
usually cast in terms of ascending prediction errors and descending predictions it's just an
expression of this gradient flow and then I would have closed with the next level which is basically
how do you choose paths through the future in terms of actions and then impact the recurrent
references to complexity and accuracy and pointing out this variation for energy is
as has been already been discussed the difference between complexity and actually in the key role
of this complexity here and then looking at what these terms come when you look at them in the
future and link those two various ideas in neuroscience and in economics and they're
finished by showing an example of visual foraging and optimizes that I'd have given the last word
to Helmholtz and then I was thanked the people whose ideas I've been talking about thank you very much
