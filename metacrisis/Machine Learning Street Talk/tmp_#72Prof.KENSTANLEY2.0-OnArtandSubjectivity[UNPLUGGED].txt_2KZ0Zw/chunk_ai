So we just aren't really adapted to that.
Does it mean that I can't understand?
I don't think it means I can't understand.
But it's just not as flexible, it's not as elastic.
And so I guess it's going somewhere in the middle.
Maybe just a little bit of Keith's going to the middle.
I don't, like, yeah, I don't think either extreme,
like we were completely specialized.
I don't think so.
We're like absolutely like generalist
in the most like flexible sense.
No, we're somewhere in the middle.
But I think that the toolbox we have is sufficient,
I do believe, to ultimately capture anything.
I do think we could do that.
Yeah, so I would argue it's probably an open question.
I think this may be a case where maybe you're being too certain
because when you talk about like these dimensionality things,
you know, I know there are many mathematical structures
that exhibit very different, you know,
fundamentally different behavior and say five dimensions
versus six dimensions versus eight.
And I think, sure, people have figured that out.
And we did that by externalizing that intelligence,
writing down symbolics, doing a bunch of equations.
But I think it would be fair to say that no human being
has ever claimed that they could grok that in their mind.
Like they can do it by virtue of this externalized intelligence,
but to really hold it in their brain and kind of intuit over it.
You know, my guess would be there probably are limitations
to what we can do.
And that's one of the things that excites me
about the potential for HEI.
I don't know if we're going to get to it,
but if we ever do get to it, it would be really interesting
to see what it's capable of.
Unshackled by the fact that it, you know,
that we evolved in a three plus one dimensional environment,
you know, to survive.
Yeah, good points.
Those are really good points.
That's interesting to think about.
Yeah, you're right.
I hadn't thought of that angle about the AI has this potential
to break out of that box.
And that is an interesting thing about AGI.
So yeah, okay, point taken.
Yeah, so I am saying something extreme
if I claim that we can understand everything.
That is extreme.
Right.
I guess I'll still stick with my claim,
but I might take a lot of effort, I guess.
Well, the way I see it, and I think you may be right about this,
at least from this perspective,
is that it may well be the case that something like,
you know, second order logic or category theory
or these sorts of logics that we've already discovered,
it may turn out to be the case
that they're mathematically sufficient
to describe any conceivable phenomenon
that we'll observe in this universe.
And so I guess I would say you could be right
that our languages and our methods that we developed
kind of externalized from us
and something that we participate in
have reached this kind of ultimate level of generality.
I just think it's a little bit beyond
what a single human mind,
at least at this phase of our evolution,
can comprehend.
That's interesting,
because that actually starts to go into this issue
of what it means to understand.
And this debate about do these AIs really understand
and yeah, there's like different kind of levels of that.
So it's true that when I say like,
we can understand everything,
it's a little unclear what I mean by understand.
Like does it mean just apply the right logical language
to describe the phenomenon,
even though we don't really get that like flash feeling
of like, wow, I really get it.
And maybe that, maybe they're right.
Maybe that is out of reach.
Keith, why don't you just go and look up
our definition of understanding.
I can give you the definition of reasoning
that we came up with,
which is the ability to derive new knowledge
from existing knowledge and experience.
Right, but this reasoning thing is a big thing.
Do you know when we say neural networks don't reason?
A lot of it has to do with this notion of extrapolation.
And people talk about the very geometric notion
of extrapolation,
but we're talking about being able to execute a function
in some logical discrete space.
So to be able to take something we know
and extrapolate it into a new situation,
spoken like Gary Marcus,
and I'm sure you've had this conversation with Gary many times.
Yeah, Gary has very strong feelings
about understanding this, true.
Yeah, very interesting feelings about that.
So it just took me a bit to go look up
because we did have some episode
where we were really getting into defining
some of these concepts with Gary.
Right, and at least here's how I define these.
So maybe we get your take on, it could be fun.
So it's that reasoning is the act
of deriving new knowledge from prior knowledge
plus new information.
Semantics, a mapping from structures,
whether mathematical, logical, symbolic,
or other structures to physical reality.
And understanding we had as the act of deriving
new semantic mappings from prior semantics
plus new knowledge.
That's esoteric, but that's how we defined it.
So understanding is really was the ability to like,
okay, if I have a world model
that understands something about physics,
that like gravity exists and balls roll and whatever,
and somebody gives me some new knowledge,
which says, hey, this ball is actually hollow.
From kind of my understanding of physics,
I can now, and I just use the word,
because I understand this semantic model
and you give me this new knowledge,
I can now derive new semantics.
I can say, well, the ball is gonna behave now in this way.
In other words, I have a new mapping to this physical world
because I've gained new knowledge.
That's kind of the way in which we perceived understanding.
So with like, this was in the context of natural language,
you know, processing, let's say,
or systems that do that, GPT-3 or whatever,
because it doesn't have this semantic model of the world.
If you say something like, the beer fell off the table,
it may not be able to derive that now the floor is wet
and somebody might slip if they fall,
that that requires this kind of extra level of understanding.
Yeah, and that's filling in the gaps as well.
So a lot of NLU people say that the one thing
neural networks can't do is extrapolate
over the missing information.
And that's a great example.
So you could reason that we've just knocked the beer off the table,
now the floor is wet.
So there's a kind of exponential space of missing information
that we need reasoning to fill in.
Okay, I have to admit that I, it's probably disappointing you
because I'm not, I don't really like definitions.
I just never find definition discussions engaging
are really helpful to me.
I really, again, often find that appeal to definition
is often just a way of escaping an uncomfortable situation.
And I want to go towards the uncomfortable situation.
So it's like, we often will say,
well, no one's really clearly defined consciousness.
Like, first, before I'm going to discuss it,
you need to define it to my satisfaction.
Well, okay, we're obviously not going to discuss it then.
Like, I'll never satisfy you.
Right, right.
And so it's often in the AI, decades of discussion,
what is intelligence?
And we see it in open-endedness,
we start having a problem, like in this small field,
like there's open-endedness workshops that come
and like half the papers were just like,
what is long pages and pages of definition in terms.
It's like, are we going to ever do anything?
Or just we're going to argue about this for the next decade.
Like, what are we even talking about
is what we're going to talk about.
And I feel like a lot of this is not necessary.
I feel like I can talk about consciousness.
I can't define it for you to your satisfaction.
I can talk about intelligence.
I can make progress on intelligence.
I can talk about open-endedness.
I don't care really what the definition is.
It's just you're going to use it to stop me from talking.
So the only thing I'll defend there is that, I mean,
I, again, I'm a pragmatist.
I believe in defining things to the extent
necessary to communicate.
And so we have to have, it's kind of like going back
to the whole hyper-specialization leading to innovation.
If you just have a divergent thing,
I think I made this point in our first video,
you're just going to end up with a universe of gray goo.
Like the really fascinating thing is that
because there are these constraints of you
need to survive in order to pass on your information,
you wind up with this kind of beautiful tapestry
of hyper-specialized things that recombine
