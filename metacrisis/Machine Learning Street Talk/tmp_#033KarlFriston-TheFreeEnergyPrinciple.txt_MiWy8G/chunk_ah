this grand unified theory of the brain or indeed any system. Now, I've been speaking
to one of our friends on the show. He's given us two questions. This guy is Dr. Hari Valpola.
He's an applied theoretical neuroscientist and he's the CEO of Curious AI, which is a
reinforcement learning based startup. And he's a very accomplished scientist actually. And he says
it does make sense to claim that there's a unified cortical algorithm. And anatomically,
the cortex is very uniform. And although there are adaptations to specific tasks,
they seem to be variations of a single unified template. And in biology,
function follows form. However, he says that to claim that everything follows from free energy
minimization is just overly simplistic in his view. He says it's obvious that for the cortex,
goal-orientated learning and perception is crucially important. He says the cortex isn't
just learning and perceiving all the structure and information it receives from the sensors.
Learning is heavily modulated by task demand and attention. And so is perception itself. So
he gives this example, he says, if you take a young child around nine months old,
the critical period for learning the phonemes of your mother tongue,
and you play some speech from the radio, the result is nothing. But if you play the same
speech during a social interaction, the child will learn to discriminate the phonemes in the speech.
I think it's an excellent observation. And it takes us into a sort of world which we haven't
really been discussing so far, which is the different flavors of free energy minimization
and how it might be manifest. And so just coming back to the point that structure matters,
structures of the brain being very finely attuned to and adapted to the context in which
they're making their inferences. One would normally cast that in terms of structure learning.
If I was a statistician, there are at least three, if not four levels of maximizing model
evidence, i.e. minimizing variational free energy, also known as marginal likelihood.
I can make inferences to reduce my uncertainty about states that change with time.
I can do learning, which is reducing uncertainty or increasing the marginal likelihood
of the parameters of my generative model. I could also think about the hyperparameters that
encode the usually cast in terms of precision or negentropy of various distributions. And that,
interestingly, speaks to ignoring things that we're talking about previously with Connor. Sometimes
it's best to ignore in a base optimal sense. If you've got very inconsistent and incompatible
information, that's one base optimal explanation for ignoring. And that speaks to intermediate
temporal scale of free energy minimization. But the one I want to get to beyond that is structure
learning, also known as Bayesian model selection. So if I was a statistician, I wanted to score
the quality of my generative model, my hypothesis, my convolution model, whatever it was,
then I would score it with a free energy approximation or bound on the marginal likelihood
of the model evidence. Then I take another model, another hypothesis, and I would compare
the adequacy of these two hypotheses using a procedural Bayesian model comparison.
And that is formally identical to choosing the hypothesis with the least free energy.
So the free energy scoring the evidence for this hypothesis or that hypothesis.
So you have now a mathematical description of what incognitive science would be known as
structure learning, optimizing the very structure of your model. Does it have three
layers? Is my deep network fit for purpose? Should I add another layer? How many units
to have in each layer? Do I have a more sparse connectivity? Do I use linear rectifying?
All of these structural aspects can, in principle, be evaluated in terms of the
free energy or the underlying marginal likelihood or the evidence.
So you have this world of structure learning. Now, that world itself can proceed at different
time scales. It can proceed in terms of the development of a particular architecture or
phenotype. If you're a developmental psychologist, you would understand this structure learning
from a new developmental point of view as the brain grows, experienced dependent learning
that is driven or can be at least described by free energy minimization or self-evidencing or
maximizing model evidence increasing through the addition of different connections and
different cortical layers and cortical areas or bringing them online through changes in
connectivity that can actually progress right through until the early 20s in your development.
You would see that or you could describe that in terms of structure learning
over somatic time. But the exactly the same principles apply at an evolutionary time scale.
So this notion of free energy minimization as Bayesian model selection
speaks to a simple understanding of evolution or as nature's way of doing natural Bayesian model
selection, i.e. natural selection, where you operationally associate the adaptive fitness
with the probability that this phenotype exists, which is just the evidence that it is there,
the probability of finding that phenotype in place. I should also say that all of these
different expressions of free energy minimization at different spatial and temporal scales are all
exactly consistent with some of the fundamentals of cybernetics and the inception of theories of
self-organization, such as, for example, the good regulator theorem. It is on some reading
provably true that for any system or agent to regulate its environment, it has to embody or
be a good model of that environment. What does that mean? It's structure must somehow recapitulate
the structure of the environment generating that it has to control or it has to engage with.
So what does that tell you? It tells you that the structure, including all those canonical
microcircuits, including those visual hierarchies of our brains, for example, or the multiple
hierarchical layers in a deep neural network, have to be there if it is the case that the
world they're trying to model has a deep structure, if things are hierarchically generated.
We know that to be the case for us because most of our world is caused by other creatures like us,
as we're currently witnessing. We know that there is a hierarchical and dynamical aspect to it,
where the hierarchies, say, interpersonal interactions transcends just to individuals,
but groups of people, conspecifics, families, societies, political affiliations,
theological affiliations, or geographical hierarchical structure. So it is hardly
unsurprising that the delicate, deep hierarchically structured connectivity we find in their brain
and in a variational autoencoder is a natural thing that has emerged from the evolution of
these architectures that all are conforming with the principle of free energy minimization.
It really resonates with a lot of the things that we've been speaking about on our podcast here,
because in machine learning, of course, there are inductive priors built into models. Even the
neural network has this hierarchical organization, which is an inductive prior, but we've spoken a
lot about meta-learning as well. And this thing that you're saying, that there's a structurally
relevant inductive pathway which could be learned. But the last question from Hari is,
he wants to know about the cerebellum, for example. He said, there's plenty of evidence that it
learns by supervised learning and can perfectly well handle regulation tasks without any models.
And he cites innate reflexes, which train a complex feed-forward controller without any
internal models. I mean, as a bit of a side thing here, Val Polo is in a way implying that the
free energy principle is kind of unsupervised. That's the way he's talking about it, which
there might be some truth to that, but we'll come back to that. But he also cites the basal ganglia,
the hippocampus, the amygdala, and the superior colliculus. Each of them learns,
and their learning algorithms definitely don't seem to have much to do with the free
energy minimization in his opinion. He says that the basal ganglia is a kind of reinforcement
learning, and the hippocampus is a one-shot learning. And the amygdala and the superior
colliculus are supervised learning for specific tasks. And this is super interesting, right?
Because he cites Kahneman system one and system two of thinking. There's this famous test where,
if you give monkeys a sequence of cards with a hidden pattern on, which need to be classified
into two classes, humans suddenly click. They see the hidden rule because their system two kicks in,
and they start getting 100% accuracy very quickly. But you don't see that with monkeys.
So Val Polo was saying that he doesn't see how that phenomenon could be explained by the free
energy principle. Right. Again, a whole raft of really interesting points there. All of those things
can be explained by the free energy principle. And indeed, there's a small literature on all of
those things. If people are interested, you can get into the literature. I have to say,
because the literature may not be the familiar literature for four people in machine learning.
It's really, he's asking some fundamental questions about functional anatomy, probably more
specifically the computational anatomy that we can ascribe to various structures and hierarchies
and connectomes in the brain. So, for example, recent thinking about the cerebellum is that it
plays the role of actually a supervisor. So it may well be the case that the cerebellum
plays a role in the amortization of carefully acquired skilled movements that become increasingly
skilled as the cerebellum watches the cortex, and in particular the premotor cortex,
supplementary motor area, and the motor cortex per se, hierarchically exchange messages,
all prescribing those predictions for that reflexive kind of predictive cutting we're talking
about before that are delivered to the spinal cord and the motor reflex arcs.
But of course, that sort of low level description of motor control does rest upon this sort of
deeply structured and informed hierarchy of predicting what am I going to do next,
because the cerebellum can watch that. It can watch the cortex learning how to compose
particular movements, and if it can therefore learn how to do that, then it can amortize it.
So I'm assuming that we all know what amortization is, but what I mean here is that we can offset
or defer the cost of the Bayesian belief updating that we've talked about before,
entails a computational complexity and therefore also a thermodynamic cost
by actually hardwiring and learning the mappings from certain beliefs to certain sort of predictions
say that drive motor reflexes. In that sense, the cerebellum can, I think, be very usefully
understood in terms of being involved in supervised learning, but I'd actually turn it on its head
and basically say it's been supervised by the cortex, but in a way that allows the cerebellum
to tell the cortex, well, normally you do it like that. Why don't you just do it quickly
and efficiently and bake in those mappings without having to worry about the actual belief
updating and converging to dynamically on a gradient flow to converging to a free energy
minimum. And that story, I think, can also be used to explain the particular role of, say,
the dorsal and ventral striatum and the basal ganglia in arbitrating between whether we do a
sort of system one versus a system. Do we think about stuff? Do we do our belief updating and a
bit of planning as inference, or do we just do what we've always done, habitually respond quickly
and efficiently by harnessing something that has already been amortized? And that, I think,
is a really interesting interpretation of habitation versus deliberative thinking, which from
the point of view of a neuroscientist would be the equivalent of system one versus system two.
In that sort of edge between machine learning and reinforcement learning sometimes
refer to as model free versus model based. And I would not subscribe to that in the sense that
the habit has to be learned from the good old fashioned belief updating and learning.
You can't, you're not given habits, you're given reflexes, but to learn a habit, to learn,
this is the kind of thing that I usually do in this situation. And if I just do it, I can dispense
from the computational cost, thereby minimizing the pathological free energy by doing the habit.
I noticed I slipped in there that because it's a principle of least action, that entails a path
integral. So the amount of time it takes to minimize your free energy or get to your conclusion
matters, which means you have to do it quickly. To minimize the action, you've got to do it quickly,
which means there's a natural pressure to habitize and amortize in the service of minimizing free
energy. And I could go on in terms of telling you what I know about the functional anatomy of the
amygdala. But perhaps it's best just to say that there's a glorious game to be played here. In fact,
it's the bread and butter of a jobbing neuroscientist, certainly a systems neuroscientist, to understand
the computational architectures and the nature of message passing implied by either
belief updating or predictive coding or variational message passing, how it is enacted physiologically
on a neural anatomy that has a deep structure. So this is our job. And everybody started
scratching the surface in the past decades with brain imaging and other tools at our disposal.
This is very machine learning bias. I'm not sure if you're familiar with this.
Recently, there has been a paper that says that predictive coding
up can approximate back propagation in arbitrary graphs. Do you have any take on this?
Yes, it is. I'm sure a friend of mine has contributed to that.
Yes, no, absolutely. So by coincidence, I think we're having a
NeurIPS workshop on Saturday that sort of affords different perspectives on backprop.
I have to say, from my perspective, that's completely unsurprising. And if there's any sense
that prop has been demeaned as something which is not absolutely central to everything,
then I think that's probably wrong. So from my perspective, it is invariably the case
that the gradients with respect to anything of variation free energy can be written down as
a prediction error, which means that if you believe that the universe of interesting things
that basically is performing a gradient flow on a variational free energy functional,
then you also believe that they are via the chain rule minimising their prediction error.
So for me, backprop is just, if you like, a nice linearised chain rule-esque way statement of
things that work flow on free energy gradients that can always be written down with a back
propagation of prediction errors. It doesn't surprise me, the slightest that predictive coding
belongs to this class of algorithm as does back propagation of errors. And I would actually submit
everything that works should be described along those lines.
I think that's one of the first times I've heard neuroscience actually say something positive
about backprop. So that's really fascinating. So I have a second question and that is,
in your own personal opinion, what has been the most interesting progress in systems neuroscience
you've seen since you first proposed the free energy principle, whether it's directly related to
it or not? Standing back from a non-personal point of view, the whole pragmatic turn towards
embodiment that we've seen at the 10th century in cognitive science has really been a very big
thing that has been underwritten and also has been lent momentum by theoreticians like ourselves.
Within that, the role of predictive processing, that's become a meme now. So you can't write a
paper in cognitive neuroscience now without a nod to predictive processing at some level
of an inactive and embodied situated contextualized sort. So that's been a slow and really important
shift, which is presenting all sorts of really interesting issues. In terms of exciting developments
in the neurosciences, an increasing quest for understanding the nature of these epistemic
affordances which drive us, and in particular in terms of social neuroscience, eco niche construction,
communication with others, and how we forage for information. That seems to be in the context
in the context of usually social interactions, also touching upon an understanding of feelings
and emotion and how resolving uncertainty about bodily states in my world that will be
interceptive inference gets into this game as an integral part of these deep hierarchical
models that try to explain everything, what role do literally gut feelings have in my
propositional beliefs about where I am or who I am and how I should be behaving.
So challenging questions that are now starting to address key aspects of
intelligence from the point of view of neuroscience, such as the emergence of
selfhood and what underpins it and self in relation to others. So we're now talking about
cultural eco niche construction and multi-agent interactions. So that's been very exciting.
So I guess maybe to end the show, I'd like to ask you on potentially a controversial question,
I don't know. I've been trying to find your official answer on this and I haven't been
able to find it, but if we take for granted that the free energy principle explains a great deal
about how certain systems of the kind we've been elucidating will function, does it explain the
origin of those systems? So does the free energy principle, for example, have anything to say
about abiogenesis on earth, on the origin of life on earth, that first spark that created
systems which can then follow a trajectory guided by the free energy principle or is that outside of
its scope? I'm sorry to end on a deflation, but I'm afraid it's outside of its scope. I say that
sort of in a rigorous way because remember all of the interesting math and all this sort of
physics of sentience, all this Bayesian mechanics, all inherits from the assumption of a non-equilibrium
