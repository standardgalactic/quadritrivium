what happens when you do that is that stuff on the inside of the Markov blanket has to have a kind
of synchrony with stuff on the outside, and in virtue of those conditional dependences that
stipulatively define the Markov blanket, you can now treat the internal states as parameterizing
probability distributions or belief distributions about external states. So there's quite a
fundamental bit of information geometry that you've been to the table when you have a Markov blanket,
that suddenly you can interpret the machinations on the inside in your computer or your variational
autoencoder or your brain as having beliefs of a Bayesian sword about what caused these data,
what's going on the outside. So that's absolutely crucial for the whole sort of active inference
and certainly representational or realist or anti-realist sort of philosophical interpretations.
So the Markov blanket is absolutely central. Everything inherits from the Markov blanket,
so everything else was in play before. We have the Planck equation, we have the Schrodinger-Awaybe
equation, we have variational principles of stationary action and pathological formulations,
that's all there. All you need to do is to drop the Markov blanket in and then you get active
inference. So to move on to your more challenging question, does a hurricane or the eternal flame
have a Markov blanket? Well, strictly speaking, no it doesn't because you've got this fuzzy leaking
that is from the point of view of somebody who's wanted to simplify things as a physicist,
very irritating, but of course it's also incredibly important and challenging. So
where would you go to try and understand these fluctuating blankets where you have exchanges? So
just technically you've got external states and then you've got blanket states that intervene
between the internal states and the external states and yet with something like a candle flame
or a hurricane, you seem to have this exchange that once an external state becomes a blanket state
and once a blanket state becomes an external state. And I worry about this every time I cut my
fingernails or have my hair cut. At what point did my Markov blanket become an external state? So
there's clearly a bit of work to be done mathematically to accommodate Markov blankets
as themselves, dynamical and random objects. My guess is that you're going to probably go back
to the work of Erkov, who was one of the key intellectual architects of Erkutistina, Gothic
theory. And he was at one point preoccupied by the notion of wandering sets. So if we think of
the Markov blanket as a partition of all the states of the universe into lots and lots of
particles that comprise the internal states and their Markov blankets. And if you consider that
partition into particles as a partitioning into subsets, and then you bring wandering
sets into play, you can now start to see a mechanics and a mathematics where you do actually
can now actually explain things like hurricanes and candles. And just for interest, literally a
couple of weeks ago, there was a nice paper in frontiers treating the biosphere as a Markov blanket.
So there is practically an importance of pressures in order to formalize what are the
Markov blankets of these large scale structures. And my guess is that the notion of wandering
sets and the time scales over which internal states exchange with external states and internal
states, external states exchange with blanket states are going to be an important issue.
And I say that because, of course, we develop what point when I am born and what point am I
died? What happens to my Markov blanket? Reproduction, 3D printers printing themselves,
is a species of 3D printers, one big Markov blanket? Or do I drill down and just say Markov
blanket only really exists for the lifetime of a 3D printer? So all of these issues, I think,
speak to exactly what you're drilling down on, which is how can you accommodate fluctuations and,
if you like, physics of non-equilibria, not of states of the universe, but of the blanket or
the partitions that define the Markov blankets. If we can imagine into the future, maybe, and if
in the spirit of the free energy principle, we maintain uncertainty about the free energy principle,
it may be the case that as we extend it to, say, wandering sets with wandering points,
we may develop a mathematics of more of a distance metric. This cloud of points is currently this
distance from a particular wandering point and they're orbiting around and there may be some
dynamics in there. And then if we follow a similar analysis, we might end up back at
the free energy principle plus another term, for example, like there may be an extra term over
there, which could be important. Would that be kind of a fair speculation of what might happen?
Yes, that's very clever. So I often say in moments of vanity and pride that the free energy
principle is one of the very few principles that conforms to itself. It's trying to provide an
accurate but minimally complex explanation for everything. And the only other thing that I know
that conforms to itself is the principle of natural selection and the sense that
the theories of natural selection themselves evolved for Perian third world. And in that sense,
this notion that the free energy principle should accommodate a judicious amount of uncertainty
about itself is absolutely right. Yeah, that's a very clever observation. In terms of the form,
I think that's probably right as well. I mean, you're asking me to think about where we might be
in say 10 years time. And so this is just hand waving. But certainly one could imagine
variational principles of stationary action applied not to probability distributions over states,
but probability distributions over partitions and sex and getting into sort of things like
category theory and the like. So one can see a sketch or an image or the future kind of
map that might conserve the relatively simple principles that underwrite the free energy
principle, but now applied at sort of one level up as it were. So not to the actual system itself,
but to the probabilistic configurations and the partitions that define that principle.
If I can zoom out a little bit here. So the free energy principle is a lot about the idea of we
want to gain information to create a better and more accurate model of the world around us.
But there are situations in game theory and decision theory where that's actually not fitness
enhancing. So two examples here, you might not want to learn the face of your kidnapper because he's
more likely to kill you. And there's the other thing where you might, so this is an information
you don't want to gain. And sometimes it's also useful to have false beliefs. For example, it can
be useful to think you're smarter than you actually are or more confident than you actually are,
because that will help you in social situations. Yeah, that's a fascinating area. And I see those
sort of themes in many different contexts, wishful thinking, deliberately avoiding certain sources
of information, not wanting to open a letter that tells you whether you've passed your exams or
a letter from the doctor that contains the results of your recent scan for cancer, for example.
There are things that we don't want to know. So we're talking about very metacognitive things.
In the sense that I never heard that before about not wanting to know the face of your
kidnapper. But that's a beautiful example. It really does tell you that objective functions
have to be about beliefs and the consequences of belief states, not states of the world.
So if there's one example that tells you you're not going to settle with the Bellman optimality
principle, that's going to be it. But it also tells you that many of these real world problems
have to contend with the fact that the kind of external states that you are dealing with
are composed of sentient creatures like you and that they also have beliefs. So now you get into
the game of putting lots of active inference agents together and multi-agent scenarios and how you
infer the degree of sophistication with which you infer the intentions of others and the belief
states of others. And all that gets into these very deep and enriched generative models that
themselves now represent the belief states of other active inference, your optimization machines
as it were. One other level I think that you can understand these sometimes paradoxical
or counterintuitive expressions of optimality is just to acknowledge that the whole of the
Bayesian brain hypothesis, the whole basis of indeed free energy principle is a statement about
prior beliefs. And as such, prior beliefs are going to be in the context of self-evidencing
necessarily optimistic. So if you remember before, I was reading prior beliefs about the outcomes
consequent on the behavior as preferences. And the only reason I say preferences is because
if those are the kind of states that I typically find myself in, so if I'm a physicist, those the
attracting set of my non-equilibrium steady state, then it looks as if I will always be working towards
these prior states. So they look literally mathematically attracting in the sense of
being that attracting set, but also psychologically attractive and valuable, and therefore the
preferred states that I worked towards. So baited into a Bayesian reading of the information
of theoretic imperatives for choice is an optimism. You're always moving towards or trying to solicit
evidence that confirms that you are an eternal and mortal, well-loved, warm creature. So
it has to be like that. It can't be any other way. Manifestations of that I always find appealing
because they're just statements of the fact it couldn't be any other way. I have to believe myself
to be an expert in this or very proficient in that in order to realize those preferred fantasies
through action to actually secure the evidence. Yes, I am that kind of thing. Without those priors,
without those optimistic priors, you would not get the kind of active inference that underwrites
our existence to come back to some of the phrases that Tim was using to introduce the free energy.
We had this very interesting point from Connor about there may be gain-theoretic
pressures, let's say, to drive down accuracy in some sense. There could also be, and I'm not sure
if this strays outside the free energy principle or not, but implementation factors. So the lesson
is back to Maxwell's demons and all the very sophisticated kind of information engine analysis
that people have done from then until now has shown that the actual recording and computation of
information is an entropic process that generates entropy. And so even though the free energy
principle does have a term in there, which is the entropy of my model per se, I'm not sure if that
captures the entropy associated. And I mean the thermodynamic entropy, if you will, associated
with actually maintaining and operating that model. So it may be the case, for example, that
you reach a point where increasing my intelligence a little bit more cost me much more to operate
and maintain than it actually buys me and improved accuracy, for example. Where do you see that trade
off does or doesn't fit into the free energy principle? I think the trade off fits very
neatly. In fact, you could say it was your enlarged part constituent of the free energy
principle in the following sense. And again, you've preempted everything that I might have said
about this, but I'll say it again. So the complexity that we were talking about. So if you recall,
log evidence and any associated bounds like free energy or an elbow and edlin slow bound
in machine learning can be written as a as the accuracy minus the complexity. And the complexity
is just the divergence between your posterior and your prior belief distributions. Now that is a
complexity cost. It's also exactly the the cost that underwrites some of universal computation
computational formulations of generalized or universal intelligence. It is the thing that
drives compression in predictive coding or engineering applications of it. It is the
thing that you're going to submit who would emphasize if he were part of our conversation,
that sort of complexity minimization. So that is a really and possibly the more important part of
the free energy, the accuracy is well understood, but doing it in a minimally complex and a minimally
maximally compressed way, that's the heart of it. That's important because by Landau's principle
and the Jinsky equality, that also directly one to one dictates the number of jewels that you will
expend in belief updating when you move from your prior to the posterior. So recall, the complexity
is just the relative entropy between the prior and the posterior. It is the degree to which you
change your mind in the face of this new data or this new sensory evidence and therefore is exactly
the changing or the erasure of bits of information that can be measured in natural
units using natural logarithms that via Landau's principle has an exact thermodynamic cost in
jewels. So when we're talking about informationally efficient complexity minimizing schemes,
compression schemes, from my perspective anything that minimizes variational free energy or belief
updating that minimizes variational free energy, you are also exactly talking about the most
efficient way of doing it thermodynamically. So what that means is if you want to build the best
kind of computer, one simple way of scoring how good your computer is, is to take two machines
that have the same accuracy and the one that uses less electricity is the best one. And that will
also by definition provide the least complex account and thereby minimize its free energy.
So I think that's a really important practical observation that what we're aspiring to here
are really cheap and cheerful machines that can do their sort of their synthesis.
Just to make it a bit more concrete, what I'm wondering is in the free energy principle term
with the complexity because I completely agree and get the point that's the crux. That's the most
important term. I'm wondering if maybe there should be a multiplier there, like an alpha,
something like a Boltzmann constant that allows me to tweak the relative balance between accuracy
and complexity. Yeah, temperature parameter. Yeah, that's a great question. There are two answers
to that question. First of all, absolutely not. The whole point of dissolving that exploration,
exploitation dilemma, the whole point of putting the information gain in the same space and in the
same currency and on the same footing as your log prior preferences, your reward, your utility,
your Belman-esque like imperatives, is that there is a seamless exchange in terms of
gnats, natural units between the decomposition of your expected free energy in terms of this
intrinsic value and this extrinsic utilitarian pragmatic value. One of the benefits of removing
that alpha or beta or temperature coefficient on the complexity versus the accuracy that for
the expected free energy now becomes risk and ambiguity is that now you can talk about reward
in terms of gnats. You can talk about how rewarding information is because they're just some arbitrary
carving up of a single expression that the expected free energy. That seems to me important
because that is if you like the sort of in terms of a dimensional or unit analysis,
that is the thing that truly does dissolve the exploration, exploitation dilemma and puts the
value of information alongside the value of a money or the value of a fruit drop if you're doing
animal experiments. So absolutely not. You really want to totally avoid any temptation to start
adding ad hoc hyperparameters like temperature to this beautiful construct which explains everything.
The other answer is absolutely yes, but in order to acknowledge that if you're just trying to explain
the necessary properties dynamics of systems that self-organized to some non-equilibrium
steady state, you are saying nothing about the nature of that steady state other than it is at
steady state. So it could be very high entropy steady state. It could be very low entropy steady
state. It could be very hot. It could be very cold. You haven't really committed to any kind of
steady state which means that it's slightly disingenuous to say that the imperative for
everything is to minimize say a free energy functional. It's not. That's why very carefully
Hamilton's principle of stationary action is just keeping it flat at steady state. It could
be very high. It could be very low. And that starts to beg the question, what kinds of steady
states are non-equilibrium? Are we really interested in simulating, reproducing, describing?
And if you get under the hood in terms of the relative entropy and the mutual informations
shape these steady states, and in particular the distributions over this Markovian partition
into inside and outside the blanket states, it turns out that one important attribute or
description of the state depends upon the way in which the active states increase the mutual
information between the internal and the external. And if you parameterize that with a particular
parameter and we will call it alpha as a nod to your question, then suddenly you really do need
this alpha. And this alpha gets in exactly as you say as a knob on the expected complexity versus
the expected accuracy or ambiguity. So what that means is in a more general formulation of the
free energy principle or certainly its application to understanding active inference,
there would be this extra parameter that really tells you're dealing with systems that
are exquisitely structured in the sense that they occupy a very small number states,
they could occupy in a particular way that is active, that they actively construct their
mutual information between the inside and the outside. At the moment, that's not in the literature.
So there are ongoing debates amongst the younger people who love the maths of this about whether
we just need a generic KL divergence or whether we need to exclude bits to get to expected free
energy. And by acknowledging there are different kinds of non-equilibrium steady states and that
in so doing, you have to really think about the relative importance for this steady state of
technically the risk and the ambiguity and the sensory entropy. So that's again, that's a bit
of an open question, which I'm hoping will be resolved in about a year's time. At which point
your alpha will occur and I'll try to call it alpha in your honor. Maybe I should have gone
with Omega, that way it's the last parameter we ever need, but thank you. Absolutely fascinating.
There's been a lot of emphasis on a single all-encompassing learning principle,
