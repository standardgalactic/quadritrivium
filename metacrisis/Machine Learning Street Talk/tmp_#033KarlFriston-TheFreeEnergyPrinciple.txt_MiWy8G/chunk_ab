and if in the spirit of the free energy principle,
we maintain uncertainty about the free energy principle.
Yes, that's very clever.
So I often say in moments of vanity and pride
that the free energy principle is one of the very few principles that conforms to itself.
It's trying to provide an accurate but really complex explanation for everything.
And the only other thing that I know that conforms to itself
is the principle of natural selection,
in the sense that the theories of natural selection themselves
evolved for perium, third world.
And in that sense, this notion that the free energy principle
should accommodate a judicious amount of uncertainty about itself
in absolute light, yeah, that's a very clever observation.
Friston would argue that certain states of the world
and certain states of knowledge would kind of transform themselves
conforming to probability theory.
The free energy principle implies that non-equilibrium steady state systems
must directly model such states of knowledge
and maintain them according to the rules of probable inference
in order to continue existing.
In other words, survival fitness requires inference.
We had this very interesting point from Connor about
there may be game theoretic pressures, let's say,
to drive down accuracy in some sense.
The free energy principle is a lot about the idea
of we want to gain information to create a better
or more accurate model of the world around us.
But there are situations in game theory and decision theory
where that's actually not fitness enhancing.
You might not want to learn the face of your kidnapper
because he's more likely to kill you.
Yeah, that's a fascinating area.
And I see those sort of themes in many different contexts.
I've never heard that before about not wanting to know the face of your kidnapper.
But that's a beautiful example.
It really does tell you that objective functions have to be about beliefs
and the consequences of belief states, not states of the world.
So if there is one example that tells you you're not going to
settle with the Belmont optimality principle, that's going to be it.
But it also tells you that many of these real world problems
have to contend with the fact that the kind of external states
that you are dealing with are composed of sentient creatures like you
and that they also have beliefs.
Keith, why don't we talk about the main formalism
for the free energy principle?
Sure.
So the free energy principle comes down to the equation
that we're showing here on the screen,
which is that the free energy is the summation of two important pieces.
The energy, which is a measure of how well your model fits the observations
or the evidence that you're observing.
And then the second piece, which in a lot of ways is really the more important piece,
of the free energy is the entropic contribution.
And so that's the KL divergence, if you will, between your model of the world
and then the actual hidden model of the world.
And the divergence between those two, it has two important pieces to it.
And I want to kind of call out, I want to highlight one here,
which lets circle right there.
That's purely the model entropy.
Okay.
So that's actually the negative of the model entropy in the KL divergence.
And so as the model gains greater and greater entropy,
and note that that's a positive quantity there, okay,
then up in the free energy that's being subtracted from the free energy.
So the higher the entropy of your model, the lower the free energy.
So as you're trying to minimize free energy, you've got two competing criteria.
One is to fit observations better and better,
but that will require more and more complex models,
which will have lower and lower entropy,
and therefore they will be subtracting less from the free energy.
So what you're trying to do is find the model that fits the data well,
while also maintaining a high entropy.
And the reason why that's important for survival
is that if you have a higher entropy model,
it's maintaining greater flexibility to adapt to incoming information.
So if you recall, log evidence and any associated bounds like free energy
or an elbow and evidence load bound in machine learning
can be written as the accuracy minus the complexity.
And the complexity is just the divergence
between your posterior and your prior belief distributions.
So that is a really and possibly the more important part
of the free energy, the accuracy is well understood,
but doing it in a minimally complex and a maximally compressed way,
that's the heart of it.
So recall, the complexity is just the relative entropy between the prior
and the posterior.
It is the degree to which you change your mind
in the face of this new data or this new sensory evidence.
The free energy principle equally weights accuracy and complexity.
Might it be the case that we need to have a temperature parameter,
like a kind of knob to tweak the relative contribution
between accuracy and complexity?
Would such a parameter be useful?
I'm wondering is in the free energy principle term with the complexity,
I'm wondering if maybe there should be a multiplier there,
like an alpha, something like a Boltzmann constant
that allows me to tweak the relative balance between accuracy and complexity.
There are two answers to that question.
First of all, absolutely not.
The whole point of dissolving that exploration, exploitation dilemma,
the whole point of putting the information gain in the same space
and in the same currency and on the same footing
as your log prior preferences, your reward, your utility,
your Belmonesque-like imperatives,
is that there is a seamless exchange in terms of
NATs, natural units, between the decomposition of your expected free energy
in terms of this intrinsic value and this extrinsic utilitarian pragmatic value.
The other answer is absolutely yes, but in order to acknowledge
that if you're just trying to explain the necessary properties dynamics
of systems that self-organize to some non-equilibrium steady state,
you are saying nothing about the nature of that steady state
other than it is at steady state.
So it could be very high entropy steady state,
it could be very low entropy steady state,
it could be very hot, it could be very cold.
You haven't really committed to any kind of steady state,
which means that it's slightly disingenuous to say
that the imperative for everything is to minimize,
say a free energy function and if you parameterize that
with a particular parameter and we will call it alpha
as a nod to your question, then suddenly you really do need this alpha
and this alpha gets in exactly as you say as a knob on the expected complexity
versus the expected accuracy or ambiguity.
That's not in the literature.
So there are ongoing debates amongst the younger people
who love the maths of this about whether we just need a generic KL divergence
or whether we need to exclude bits to get to expected free energy.
You have to really think about the relative importance
for this steady state of technically the risk and the ambiguity
and the sensory entropy.
So that's again, that's a bit of an open question,
which I'm hoping will be resolved in about a year's time.
At which point your alpha will occur
and I'll try to call it alpha in your honor.
One of the previous guests to our show Dr Hari Valpolar,
the CEO of Curious AI and a computational applied neuroscientist,
he sent us in a couple of questions for Friston actually
and the answer was quite interesting.
But the crux of it is the brain is highly specialized
and is the free energy principle an oversimplification
for what's going on in the brain?
If you take a young child around nine months old,
the critical period for learning the phonemes of your mother tongue
and you play some speech from the radio, the result is nothing.
But if you play the same speech during a social interaction,
the child will learn to discriminate the phonemes in the speech.
Structure matters, structures of the brain being very finely attuned to
and adapted to the context in which they're making their inferences.
One would normally cast that in terms of structure learning,
also known as Bayesian model selection.
So this notion speaks to a simple understanding of evolution
or as nature's way of doing natural Bayesian model selection,
i.e. natural selection,
where you operationally associate the adaptive fitness
with the probability this phenotype exists,
which is just the evidence that it is there,
the probability of finding that phenotype in place.
It is, on some reading,
provably true that for any system or agent to regulate its environment,
it has to embody or be a good model of that environment.
What does that mean?
It's structure must somehow recapitulate the structure of the environment
generating that it has to control or it has to engage with.
So it is hardly unsurprising that the delicate, deep,
hierarchically structured connectivity we find in their brain
and in a variational autoencoder is a natural thing
that has emerged from the evolution of these architectures
that all are conforming with the principle of free energy minimization.
What about Daniel Kahneman's System 1 and System 2 of cognition?
Do those fit into the free energy principle?
He cites Kahneman's System 1 and System 2 of thinking.
There's this famous test where if you give monkeys a sequence of cards
with a hidden pattern on which need to be classified into two classes,
humans suddenly click.
They see the hidden rule because their System 2 kicks in
and they start getting 100% accuracy very quickly,
but you don't see that with monkeys.
So Valpolio is saying that he doesn't see how that phenomenon
could be explained by the free energy principle.
Recent thinking about the cerebellum is that it plays the role of actually a supervisor.
So it may well be the case that the cerebellum plays a role in the amortization
of carefully acquired skilled movements that become increasingly skilled
as the cerebellum watches the cortex.
In that sense, the cerebellum can, I think, be very usefully understood
in terms of being involved in supervised learning,
but I'd actually turn it on its head and basically say,
it's being supervised by the cortex,
but in a way that allows the cerebellum to tell the cortex,
well, normally you do it like that.
Do we do our belief updating and a bit of planning as inference,
or do we just do what we've always done,
habitually respond quickly and efficiently by harnessing
something that has already been amortized?
And that, I think, is a really interesting interpretation.
Habitization versus deliberative thinking,
which from the point of view of a neuroscientist would be the equivalent of system one versus system two.
In that sort of edge between machine learning and reinforcement learning,
sometimes referred to as model-free versus model-based,
it's the bread and butter of a jobbing neuroscientist,
certainly a systems neuroscientist,
