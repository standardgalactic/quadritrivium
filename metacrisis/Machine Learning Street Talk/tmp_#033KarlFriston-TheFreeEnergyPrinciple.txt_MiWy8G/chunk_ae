of predictive coding or Bayesian or Kalman-like filtering
in the moment.
And that's relatively straightforward to show,
and there was a hint of the requisite mathematical apparatus
you needed to make those assertions in the introduction
and specifically mark off blankets
and separate me from not me.
So that would be grade one, if you like, prediction.
It's just that we have dynamics
and the dynamics required of self-organisation
to non-equilibrium steady state require some
in-the-moment kind of prediction.
So where is the higher end anticipatory of dynamics?
Well, that comes in through Feynman's pathogenical formulation
and thinking not just about the flow of the dynamics
of self-organisation at this point in time,
but trajectories into the future
and the probability distributions over those trajectories
and particularly the states that act upon the outside,
if you like, formally characterise what we do
over extended periods of time.
So when you bring the path integral formalism to bear
on these non-equilibrium steady state distributions,
then you can sort of say what kinds of policies
as I move into the future are more likely
and which are less likely.
And then things get much more interesting
and we talk about sort of ambiguity and risk
and what the imperatives are for long-term behaviours.
And underneath all that, there is an interpretation
that I'm really trying to predict
or you can interpret this in terms of inferring
the most likely paths, basically as resting upon
a prediction of states of the world in the future
conditioned upon a particular sequence of actions or policy.
One of the things that I think is very interesting
about the free energy formulation
is that prediction is half the story.
So getting accurate predictions about the future
while very important is juxtaposed with keeping your options open,
keeping a flexible mind, keeping a high entropy model of the world
so that as you encounter perhaps new situations,
it has the flexibility to adapt more quickly,
that incoming information.
And that because it's a repeated game,
you're not just trying to optimise the very next step only,
but the entire trajectory of your existence, if you will.
And that's captured in the free energy principle
but inside the KL divergence part of that
is the entropy of my model of the world.
And the larger that is, the lower the free energy is.
This interesting interplay, and you also point out in a video too,
that when it's this repeated game,
it's related in a way to what in other fields is called
the exploration versus exploitation kind of trade-off,
like in the multi-armed bandit world
where I can take an action that's maximally exploitative
but I may not learn very much.
And then on the other hand, I can take an action
that's purely just to learn about the world
but may not achieve much of my other objective.
And what I love about that is it explains
why we're always so tempted to push the button,
why people always want to press the button
and see what happens or go out and explore,
even if there's no immediate benefit.
And it has so many connections to things like novelty search
and multi-armed bandit analysis.
So it's quite interesting.
Yeah, you've been on so many fascinating points there.
Yeah, and robotics, intrinsic motivation in robotics,
artificial curiosity and a Schmidhofer-like sensual,
which itself all comes back down
to this minimizing the complexity
and paradoxically celebrating uncertainty.
So if you allow me, you made certainly good points there.
I can't resist just highlighting the first of all,
predictions only half the story, absolutely.
I mean, why are we predicting?
Well, we're predicting in order to infer what to do next.
So the doing, the action, the inactive aspect
really becomes the central thing in terms of, well,
things that are written into the description
like self-organization, how I organize myself, what I do.
And if you're in data science,
that would be basically how do I optimally data mine?
Can I find easy and optimum design principles written
into the formulas and the variation formulas
and the free energy principle?
So I think just saying that perception is great
if you're just studying the visual cortex
or you're interpreting some given pixelated image.
But the real challenge is really,
how do you go and take that picture
and how do you move your eyes around
and sample that information
that's going to make much more sense of the world?
So it's all in the service of what I'm going to do.
You mentioned the KL divergence,
the central role of relative entropies
in this sort of variational construct.
I think that your formula is so important.
So a very simple perspective on free energy,
which I think is useful for students,
is to remind them that the free energy
is some expected potential,
some expected utility, if you like,
and complimented by the entropy.
And in minimizing the free energy,
you're also trying to maximize the entropy,
which seems sometimes counterintuitive.
But it is exactly that,
which is really mandated by things like Ockens' principle
and very practically relevant.
So if you don't do that,
if you don't put that uncertainty into the game,
so you don't commit to a very precise explanation
for these data,
then you're going to run into things like sharp minima
and you're going to be searching for resolutions of that
in terms of broadening your uncertainty,
flattening that free energy landscape
to try and secure those flat minima
where you can be more reasonably assured
that you've got some global minima,
say, in standard deep learning
or a machine learning context.
That would be a story you could tell
just about the composition of why free energy
rests upon entropies and relative entropies.
But you took us straight to the heart
of the sort of deeper in-time inference
that comes along with the active inference,
which is this sort of this implicit imperative
to reduce uncertainty that can often be described
in terms of responding to salience
or epistemic affordance.
Just say, if I did that, what would happen?
What would I know?
What information would I gain?
Novelty you mentioned.
So technically, what we tend to do
is to talk about the resolution of uncertainty
or the information gain
that is formally described by KL divergence
between my beliefs about states of the world,
with and without those sensory samples or observations
that I would get if I did that.
So that KL divergence can apply to unknown states of the world
and that would be driving behaviors
like looking over there
to make sure that my hypothesis
that that was a butterfly or a bird or a candle
was the best explanation
for these sort of peripheral visual sensations.
But you've also got unknowns, for example,
in a deep learning scenario, the connection strengths
in the brain parameters that underwrite
our brain connectivity that encode the lawful contingencies
and statistical regularities of likelihood mappings
or probability transition matrices.
So these don't change quickly in time,
but they still are equipped with a posterior belief
or some stochastic representation
about which you can reduce uncertainty.
And then we call that novelty.
So if you can get yourself into a situation
where you reduce your uncertainty
about the contingencies
and the parameters of your generative model,
then you're responding to, again, an epistemic affordance.
There is exactly this resolution of uncertainty.
Formally, we'd write that down as an expected free energy.
I don't know if this helps, but one way I find useful
to think about this and indeed explain it to students
is free energy is abound on log evidence,
simply the probability of some data under a model
of how I thought those data were generated.
And you can always split that into accuracy and complexity.
So if I was a statistician and I wanted to minimize
my physics free energy or maximize my machine learning free energy,
which is cause is just an evidence lower bound
and the sort you find in a variational autoencoder.
What am I doing?
If I decompose the log evidence into accuracy and complexity,
what I'm trying to do is minimize the complexity
of my accurate expectations and then we get back to
minimum description lengths, the underlie,
say, you know, call monograph complexity formulations
of most efficient coding and the like.
So you've got this way of looking at self-evidencing
in the sense of just forming good beliefs
of how my data were generated in terms of providing the simplest
but most accurate explanations or accounts of the data at hand.
So if we come back to this to the previous discussion
about what would happen in the deep future.
So I did a deep research over all the sequences
of moves that I could make.
What would the expected accuracy
and the expected complexity look like?
