Welcome back to the Machine Learning Street Talk YouTube channel and podcast.
So today we have an incredibly special guest, Professor Carl Friston.
It was one of the most fascinating conversations we've ever had on Street Talk.
This is an old school professor.
He went to Cambridge in 1980 and he's one of these kind of eccentric polymath types
that sits on the old kind of Chesterfield chair with all of the springs coming out
and smoking a pipe or something like that.
Professor Friston is most well known certainly in the machine learning domain
for his free energy principle or active inference,
which is a kind of reinforcement learning flavour of that, if you like.
But he's got an incredible background.
He is an expert in psychiatry and cognitive neuroscience and physics and Bayesian statistics.
It's so strange just to have all of this knowledge embodied in one person.
So it really was quite an interesting conversation actually.
Now the free energy principle has been called apostolate,
a natural law, an imperative, an unfalsifiable principle.
It's been called many things.
It aims to give an almost universal understanding between the mind, life and the environment.
So how did the free energy principle come about?
The free energy principle, as you have just described it, started really when I was a student
aspiring to put together maths and psychology, so it gets back to mathematical formalisms,
the principles that underlie the sentient behaviour with which we are gifted.
And the product of that was the free energy principle.
The free energy principle sets the foundation for planning as inference
by explicitly modelling the world and its states as beliefs.
It balances accuracy with entropy, which maintains the flexibility needed
to continually adapt to future outcomes and explorations.
But the more interesting game is, I think, better cast in terms of planning as inference,
enabling you to roll out much further into the future and ask,
well, what would happen if I did that?
What would my beliefs be about the state of the world in the long term future?
So I think prediction in its full and glorious anticipatory sense really takes centre stage.
Features of reality itself, such as self-organised behaviour and even quantum,
they seem to require some kind of probabilistic Bayesian belief update on world states.
For example, the path integration formalism from Richard Feynman.
It essentially averages over many probabilistically weighted paths,
in other words, functions over beliefs,
and has proven crucial to the subsequent development of quantum electrodynamics.
To understand the first principles that underlie sentient behaviour,
you have to understand the dynamics of self-organisation
in a particular self-organisation of systems that are open to the environment.
That comes in through Feynman's path integral formulation
and thinking not just about the flow or the dynamics of self-organisation at this point in time,
but trajectories into the future and the probability distributions over those trajectories,
and particularly the states that act upon the outside.
And then things get much more interesting.
You can interpret this in terms of inferring the most likely paths,
basically as resting upon a prediction of the states of the world in the future
conditioned upon a particular sequence of actions or policy.
The heart of the free energy principle and what sets it apart from alternatives
is the strict balance between accuracy and simplicity, evidence and entropy.
One of the things that I think is very interesting about the free energy formulation
is that prediction is half the story.
So getting accurate predictions about the future while very important
is juxtaposed with keeping your options open, keeping a flexible mind,
keeping a high entropy model of the world so that as you encounter perhaps new situations,
it has the flexibility to adapt.
In the central role of relative entropies in this sort of variational construct,
I think that your formula that is so important in minimizing the free energy,
you're also trying to maximize the entropy, it seems sometimes counterintuitive,
but it is exactly that which is really mandated by things like Ockens' principle
and very practically relevant.
So if you don't do that, if you don't put that uncertainty into the game,
then you're going to run into things like sharp minima
and you're going to be searching for resolutions of that in terms of broadening your uncertainty,
flattening that free energy landscape to try and secure those flat minima
where you can be more reasonably assured that you've got some global minima,
say in standard deep learning or a machine learning context.
So this led Friston to believe that goal-directed behavior,
essentially planning based on goals, is insufficient.
We need to have a more sophisticated system that can reason about the uncertainty of our beliefs.
We see echoes of this in machine learning.
We spoke to Kenneth Stanley recently about novelty search
and in his formalism, he explicitly avoids objectives
and he thinks that we should use novelty because of the inherent deception in objective search.
Similarly in classic machine learning, we use regularization
to stop the machine learning algorithm overfitting the training set.
Friston argues that the free energy principle combines all of these different paradigms.
We're getting quite close to the center of the bullseye here
and we're talking about the dichotomy between belief-free and belief-based methods.
You said in your papers that goal-directed behavior is fine for learning kind of basic habitual policies.
Of course, normally there's this value function that needs to be either computed directly
in the non-stochastic setting using the Bellman equations or some other method,
but your approach is a stark departure away from having this value function.
Again, you said everything that I could possibly say,
I'll send you on my next lecture tour, that'll take the pressure off me.
Absolutely, and of course, I forgot to just highlight this sort of exploitation,
exploration dilemma that has dulled 20th century thinking, minimizing this mixture.
That just is the expected free energy.
You've got exploration and exploitation solved for free in the right order.
So you normally see food in the fridge before you start preparing your meal.
You don't do it the other way around, but you said it.
You said that the Bellman optimality principle is only fit for purpose
if there exists a value function of states that will ensue if I commit to that action.
Let's talk about Markov blankets.
Markov blankets are probably the one piece of jargon,
which you're going to hear today more than any other term.
We have an innate common sense notion of things and the boundaries that separate them.
However, capturing this mathematically is more difficult than we might expect.
The Markov blanket concept formalizes these notions
and underpins the free energy principle.
One of the pivotal concepts in the free energy principle is Markov blankets.
Tell us about them.
They're relatively straightforward.
If you imagine here on our screen, we're showing an image
and if every single one of these little discs that you see there was a state
and if you have a system that can be partitioned,
then the red dots that are in the middle are the thing itself.
The blue dots that are outside are the universe at large, the external environment,
and the yellow and orange dots in between form the Markov blanket.
And what the Markov blanket requirements are
is that the red is able to interact with orange.
Orange is able to interact with yellow and yellow is able to interact with the blue.
But what we don't have is we don't have blue being directly
affected and vice versa by red.
So we don't have this.
So instead, we have these boundary sets.
We partitioned all the state space into an internal group, the red ones,
an external group, the blue, and these blanket states that sit in between the two
and facilitate the interaction between external and internal.
And why these are important for the free energy principle is they set up these
conditional independencies, which say that the blue states, as long as I know yellow and orange,
they're independent informationally from the red states.
And likewise, the red states, so long as I know the blanket states are independent of the external ones.
And yet future conceptions might rely on even more general concepts
that allow for Markov blankets, like, for example, wandering sets.
I'd like to explore that, as you put it, maliciously engineered false dichotomy
a little bit further, but in a different context.
And it's one of the Markov blanket.
For example, you were with Sean Carroll long ago.
And I think he asked, does a hurricane have a Markov blanket?
And you said, well, no, it doesn't.
And that's quite annoying, actually, that if it's enough of a boundary,
enough of a blanket, if you will, I can still think about it in useful ways.
Is that true?
I mean, do we really need an exact Markov blanket?
Or is it flexible enough that it can be quite a fuzzy boundary?
There's an excellent question.
And the honest answer is, I don't really know at this stage.
But what is known, and in not knowing that I mean that in a positive way,
that this is a future challenge that I think people will contend with in the next few years.
So in an idealized formulation, the Markov blanket exists at non-equilibrium steady states.
So it exists in eternity in a very crisp and well-defined way
that there are specific conditional independences
that allow for a vicarious coupling between the inside and the outside.
So it's the device that gets you out of 20th century equilibrium physics
that we're not talking about closed systems anymore.
We're talking about non-equilibrium steady states.
So we're really in the sort of the things that people like Sean Carroll
and a lot of other people are contending with at the moment,
the physics of open systems and self-organized systems.
And the Markov blanket plays an incredibly crucial role here
in demarcating the edge of the system as it were.
And it allows you to identify something.
Stuff on the inside of the Markov blanket has to have a kind of
synchrony with stuff on the outside.
And in virtue of those conditional dependences that stipulatively define the Markov blanket,
you can now treat the internal states as parameterizing
probability distributions or belief distributions about external states.
So there's quite a fundamental bit of information geometry
that you've been to the table when you have a Markov blanket
that suddenly you can interpret the machinations on the inside
in your computer or your variational autoencoder or your brain
as having beliefs of a Bayesian sword about what caused these data,
what's going on the outside.
So the Markov blanket is absolutely central.
Everything inherits from the Markov blanket.
So to move on to your more challenging question,
does a hurricane or the eternal flame have a Markov blanket?
Strictly speaking, no, it doesn't because you've got this fuzzy leaking
that is from the point of view of somebody who's wanted
to simplify things as a physicist, very irritating.
But of course, it's also incredibly important and challenging.
So where would you go to try and understand these fluctuating blankets
where you have exchanges?
If I worry about this every time I cut my fingernails
or have my hair cut, at what point did my Markov blanket become an external state?
So there's clearly a bit of work to be done mathematically
to accommodate Markov blankets as themselves, dynamical and random objects.
My guess is that you're going to probably go back to the work of Erkov,
who was one of the key intellectual architects of Erkutistina,
Gothic theory, and he was at one point preoccupied by the notion of wandering sets.
So if we think of the Markov blanket as a partition
of all the states of the universe into lots and lots of particles
that comprise the internal states and their Markov blankets,
and if you consider that partition into particles,
as a partitioning into subsets, and then you bring wandering sets into play,
you can now start to see a mechanics and a mathematics
where you can now actually explain things like hurricanes and candles.
3D printers printed themselves.
Is a species of 3D printers one big Markov blanket?
Or do I drill it down and just say Markov blanket only really exists
for the lifetime of a 3D printer?
So all of these issues, I think, speak to exactly what you're drilling down on,
which is how can you accommodate fluctuations?
And if you like, physics of non-equilibria,
not of states of a universe, but of the blanket or the partitions
that define the other Markov blankets.
Does the free energy principle conform to itself?
If we can imagine into the future, maybe,
