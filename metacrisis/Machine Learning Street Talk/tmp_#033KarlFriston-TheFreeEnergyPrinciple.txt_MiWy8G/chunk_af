And if you write that down, they come out
as things like risk and ambiguity.
So the expected accuracy or more precisely the expected
inaccuracy, the negative accuracy,
the negative log probability of your data under beliefs
about their causes becomes ambiguity,
becomes the conditional uncertainty if you like
about the observations if you knew them.
And the expected complexity becomes risk
or it has exactly the same form
that you'd find in KL control theory
or risk sensitive control in economics.
It's just the relative entropy or the KL divergence
between what you think will happen if you did that
and what a priori you think is going to happen,
which normally people treat as a sort of the preferred outcome,
the kind of things that a creature or a system
or an agent like me would normally encounter.
So that ambiguity is actually the thing
that you were talking about,
the reason why we push that button or open that door
or go on that trip or go into that Google page.
It is written into, it is if you like,
the homologue of our aspirations for accurate accounts
of all the sensory and other data
that we have to assimilate.
When we consider the expectation in the future
conditioned on what we'd actually do.
We're getting quite close to the center of the bullseye here
and we're talking about the dichotomy between
belief free and belief based methods.
So you've been sketching out this really interesting idea
that we can take a subsequent actions
by maximizing the expectation over a generative model
of states in distribution and subsequent policies.
But the alternative is so-called goal directed behavior.
Now you said in your papers that goal directed behaviors
is fine for learning kind of basic habitual policies,
but you cite this wonderful example with an owl.
You say that an owl doesn't necessarily know
where the mouse is in the field.
There might be some ambiguity, there might be some uncertainty.
And using a simple goal directed policy,
the owl would just go straight to the mouse
and try and eat the mouse.
Now it's quite interesting to contrast your approach
to other forms of let's say Bayesian reinforcement learning
or traditional stochastic reinforcement learning.
Of course, normally there's this value function
that needs to be either computed directly
in the non-stochastic setting using the Bellman equations
or some other method.
But your approach is a stark departure away
from having this value function.
Again, you've said everything that I could possibly say.
I'll send you on my next lecture tour.
That'll take the pressure off me.
Absolutely.
And of course, I forgot to just to highlight
this sort of exploitation, exploration dilemma
that has dogged 20th century thinking.
Of course, that just dissolves in the 21st century
because risk and ambiguity, sometimes you can exchange
the terms of this expected free energy formulation
and re-express that as an intrinsic and extrinsic value
where the intrinsic value is something that people in robotics
not official curiosity would recognize
as that sort of epistemic information gain part.
And the extrinsic value is just the expected utility
or the expected reward.
In minimizing this mixture, that just is the expected free energy.
You've got exploration and exploitation solved for free
in the right order.
So you normally see food in the fridge
before you start preparing your meal.
You don't do it the other way around.
That's what you were hinting at in terms of the owl
opening its fridge and searching.
It's slightly disingenuous of me, I think,
to use such a stark contrast dialectic
between what is in essence a whole bunch of things
predicated on the Bellman optimality principle
and contrast that with another way of doing it,
which is essentially a variational Hamilton's principle
of station reaction.
So the free energy principle inherits from the physics
of non-equilibria that you will find
from Feynman's pathological formulation
of quantum electrodynamics right through to Hamilton's equations
of motion.
So this is the variational principle
that is the first principle that we were mentioning before.
So the question is, do you choose between
variational principles of station reaction
or do you go for Bellman optimality principles?
And in that choice, what's the big commitment you're making?
And you said it, you said that the Bellman optimality principle
is only fit for purpose if there exists a value function of states
that will ensue if I commit to that action.
Whereas the variational principles of least action
being a path integral of an energy
and an energy being a log probability of a probability
distribution immediately tells you that the objective function,
if you want to cast it in terms of normative theories
of optimization, the objective function
is a function of a probability distribution.
So it has to be belief-based.
If you read a probability distribution as a Bayesian belief,
so if you allow me to talk about beliefs simply
as conditional probability distributions
in the sense of belief propagation or Bayesian beliefs,
then we know that from physics,
the objective function, the Lipunov function,
the cost function has to be a functional function of Bayesian beliefs.
So that comes back to your point,
that if you go down the Bellman route, the RL route,
the optimal control route,
then you are predicating everything on a value function of states.
If, on the other hand, you go down a variational or a physics route,
then your objective function has to be a functional of beliefs about states.
And that's where you get the resolution of uncertainty.
That's where searching gets into the mix,
because to search is to resolve uncertainty.
But uncertainty and surprise and all of these other attributes of belief updating
are all attributes of probabilistic beliefs, probability distribution.
So you know you have to have a functional of probability distributions
in order to account for sentient behavior.
And I would say, including goal-directed behavior.
Why is that disingenuous?
Well, it's disingenuous because, of course,
what you can do is take this more general formalism
than this variational formalism and take out of it all the uncertainty.
And then you can get back to the Bellman-Octomality principle.
So really, there's not a dialectic here.
It's just that the Bellman-Octomality principle deals with a special
but very ubiquitous and pragmatically very important case
where you can discount various sources of uncertainty.
So common examples here.
Let's ignore the partially observed aspect of a problem.
Let's assume that our observations tell us everything we need to know
about latent states causing data or latent states of the plant that I'm trying to control.
So that takes ambiguity out of the expected theology before you do anything.
What are you left with?
Well, you're left with the expected risk.
What's that?
That's the expected complexity.
So that's just a probabilistic measure, a relative entropy measure,
of the distance probabilistically between your prior preferences,
your goals, your rewarding states, and what you think will happen.
So if you say, look, now let's assume for simplicity that it doesn't matter
every which way that I can do this.
The uncertainty about what's going to happen is not going to change.
That means that risk now doesn't play any role anymore
other than to score the expected utility or the expected log of your prior preferences.
That just is Bayesian decision theory.
So you can get quite easily back to what was deliberate but maliciously engineered dichotomy.
You can dismiss that just by acknowledging that you only need to worry about uncertainty in epidemics
when you put uncertainty and not knowing into the mix.
I'd like to explore that, as you put it, maliciously engineered false dichotomy
a little bit further, but in a different context.
It's one of the Markov blanket.
For example, you were with Sean Carroll not long ago, and I think he asked,
does a hurricane have a Markov blanket?
And he said, well, no, it doesn't.
And that's quite annoying, actually, that it doesn't have a conveyor.
Maybe the eternal flame is sort of the nemesis because it doesn't.
We get to this, when is a pile of sand?
A pile of sand.
What I'm curious about is, does the concept of a Markov blanket,
for the purposes of contemplating the free energy principle,
can it be a fuzzy Markov blanket?
So just for an inanimate example, I can imagine a Markov blanket around the moon and one around the earth,
but on the other hand, they gravitationally interact.
But I don't think that precludes me from thinking about dimensions of the free energy principle,
even if there are interactions that pierce the boundary, if you will.
But still, if it's enough of a boundary, enough of a blanket, if you will,
I can still think about it in useful ways.
Is that true?
I mean, do we really need an exact Markov blanket,
or is it flexible enough that it can be quite a fuzzy boundary?
There's an excellent question, and the honest answer is, I don't really know at this stage,
but what is known, and in not knowing, I mean that in a constant way,
that this is a future challenge that I think people will contend with in the next few years,
and I'll try to explain why that's important from my perspective.
So the crisp and clear consequences of having a Markov blanket is when one unpacks it right
through to the path integral formulation, the implications for the good trajectories
in terms of minimizing expected free energy really do rest upon there being a well-defined
Markov blanket, which, as you say, is the mechanism that allows for these connections
and influences at a distance that would keep the Moon in orbit around the Earth, for example,
whilst at the same time accommodating the conditional independences that enable me to
distinguish the Moon from something else. So in an idealized formulation, the Markov blanket
exists at non-equilibrium steady state, so it exists in eternity in a very crisp and well-defined
way that there are specific conditional independences that allow for a precarious coupling between the
inside and the outside. So it's the device that gets you out of 20th century equilibrium physics,
that we're not talking about closed systems anymore, we're talking about non-equilibrium
steady states, so we're really in the sort of the things that people like Sean Carroll and
a lot of other people are contending with at the moment, the physics of open systems and
self-organizing systems. And the Markov blanket plays an incredibly crucial role here in demarcating
the edge of the system as it were, and it allows you to identify something, so anything has to have
a Markov blanket, strictly speaking, for a particular period of time. Just as an aside,
