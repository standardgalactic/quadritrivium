Welcome back to the Machine Learning Street Talk YouTube channel and podcast.
So today we have an incredibly special guest, Professor Carl Friston.
It was one of the most fascinating conversations we've ever had on Street Talk.
This is an old school professor.
He went to Cambridge in 1980 and he's one of these kind of eccentric polymath types
that sits on the old kind of Chesterfield chair with all of the springs coming out
and smoking a pipe or something like that.
Professor Friston is most well known certainly in the machine learning domain
for his free energy principle or active inference,
which is a kind of reinforcement learning flavour of that, if you like.
But he's got an incredible background.
He is an expert in psychiatry and cognitive neuroscience and physics and Bayesian statistics.
It's so strange just to have all of this knowledge embodied in one person.
So it really was quite an interesting conversation actually.
Now the free energy principle has been called apostolate,
a natural law, an imperative, an unfalsifiable principle.
It's been called many things.
It aims to give an almost universal understanding between the mind, life and the environment.
So how did the free energy principle come about?
The free energy principle, as you have just described it, started really when I was a student
aspiring to put together maths and psychology, so it gets back to mathematical formalisms,
the principles that underlie the sentient behaviour with which we are gifted.
And the product of that was the free energy principle.
The free energy principle sets the foundation for planning as inference
by explicitly modelling the world and its states as beliefs.
It balances accuracy with entropy, which maintains the flexibility needed
to continually adapt to future outcomes and explorations.
But the more interesting game is, I think, better cast in terms of planning as inference,
enabling you to roll out much further into the future and ask,
well, what would happen if I did that?
What would my beliefs be about the state of the world in the long term future?
So I think prediction in its full and glorious anticipatory sense really takes centre stage.
Features of reality itself, such as self-organised behaviour and even quantum,
they seem to require some kind of probabilistic Bayesian belief update on world states.
For example, the path integration formalism from Richard Feynman.
It essentially averages over many probabilistically weighted paths,
in other words, functions over beliefs,
and has proven crucial to the subsequent development of quantum electrodynamics.
To understand the first principles that underlie sentient behaviour,
you have to understand the dynamics of self-organisation
in a particular self-organisation of systems that are open to the environment.
That comes in through Feynman's path integral formulation
and thinking not just about the flow or the dynamics of self-organisation at this point in time,
but trajectories into the future and the probability distributions over those trajectories,
and particularly the states that act upon the outside.
And then things get much more interesting.
You can interpret this in terms of inferring the most likely paths,
basically as resting upon a prediction of the states of the world in the future
conditioned upon a particular sequence of actions or policy.
The heart of the free energy principle and what sets it apart from alternatives
is the strict balance between accuracy and simplicity, evidence and entropy.
One of the things that I think is very interesting about the free energy formulation
is that prediction is half the story.
So getting accurate predictions about the future while very important
is juxtaposed with keeping your options open, keeping a flexible mind,
keeping a high entropy model of the world so that as you encounter perhaps new situations,
it has the flexibility to adapt.
In the central role of relative entropies in this sort of variational construct,
I think that your formula that is so important in minimizing the free energy,
you're also trying to maximize the entropy, it seems sometimes counterintuitive,
but it is exactly that which is really mandated by things like Ockens' principle
and very practically relevant.
So if you don't do that, if you don't put that uncertainty into the game,
then you're going to run into things like sharp minima
and you're going to be searching for resolutions of that in terms of broadening your uncertainty,
flattening that free energy landscape to try and secure those flat minima
where you can be more reasonably assured that you've got some global minima,
say in standard deep learning or a machine learning context.
So this led Friston to believe that goal-directed behavior,
essentially planning based on goals, is insufficient.
We need to have a more sophisticated system that can reason about the uncertainty of our beliefs.
We see echoes of this in machine learning.
We spoke to Kenneth Stanley recently about novelty search
and in his formalism, he explicitly avoids objectives
and he thinks that we should use novelty because of the inherent deception in objective search.
Similarly in classic machine learning, we use regularization
to stop the machine learning algorithm overfitting the training set.
Friston argues that the free energy principle combines all of these different paradigms.
We're getting quite close to the center of the bullseye here
and we're talking about the dichotomy between belief-free and belief-based methods.
You said in your papers that goal-directed behavior is fine for learning kind of basic habitual policies.
Of course, normally there's this value function that needs to be either computed directly
in the non-stochastic setting using the Bellman equations or some other method,
but your approach is a stark departure away from having this value function.
Again, you said everything that I could possibly say,
I'll send you on my next lecture tour, that'll take the pressure off me.
Absolutely, and of course, I forgot to just highlight this sort of exploitation,
exploration dilemma that has dulled 20th century thinking, minimizing this mixture.
That just is the expected free energy.
You've got exploration and exploitation solved for free in the right order.
So you normally see food in the fridge before you start preparing your meal.
You don't do it the other way around, but you said it.
You said that the Bellman optimality principle is only fit for purpose
if there exists a value function of states that will ensue if I commit to that action.
Let's talk about Markov blankets.
Markov blankets are probably the one piece of jargon,
which you're going to hear today more than any other term.
We have an innate common sense notion of things and the boundaries that separate them.
However, capturing this mathematically is more difficult than we might expect.
The Markov blanket concept formalizes these notions
and underpins the free energy principle.
One of the pivotal concepts in the free energy principle is Markov blankets.
Tell us about them.
They're relatively straightforward.
If you imagine here on our screen, we're showing an image
and if every single one of these little discs that you see there was a state
and if you have a system that can be partitioned,
then the red dots that are in the middle are the thing itself.
The blue dots that are outside are the universe at large, the external environment,
and the yellow and orange dots in between form the Markov blanket.
And what the Markov blanket requirements are
is that the red is able to interact with orange.
Orange is able to interact with yellow and yellow is able to interact with the blue.
But what we don't have is we don't have blue being directly
affected and vice versa by red.
So we don't have this.
So instead, we have these boundary sets.
We partitioned all the state space into an internal group, the red ones,
an external group, the blue, and these blanket states that sit in between the two
and facilitate the interaction between external and internal.
And why these are important for the free energy principle is they set up these
conditional independencies, which say that the blue states, as long as I know yellow and orange,
they're independent informationally from the red states.
And likewise, the red states, so long as I know the blanket states are independent of the external ones.
And yet future conceptions might rely on even more general concepts
that allow for Markov blankets, like, for example, wandering sets.
I'd like to explore that, as you put it, maliciously engineered false dichotomy
a little bit further, but in a different context.
And it's one of the Markov blanket.
For example, you were with Sean Carroll long ago.
And I think he asked, does a hurricane have a Markov blanket?
And you said, well, no, it doesn't.
And that's quite annoying, actually, that if it's enough of a boundary,
enough of a blanket, if you will, I can still think about it in useful ways.
Is that true?
I mean, do we really need an exact Markov blanket?
Or is it flexible enough that it can be quite a fuzzy boundary?
There's an excellent question.
And the honest answer is, I don't really know at this stage.
But what is known, and in not knowing that I mean that in a positive way,
that this is a future challenge that I think people will contend with in the next few years.
So in an idealized formulation, the Markov blanket exists at non-equilibrium steady states.
So it exists in eternity in a very crisp and well-defined way
that there are specific conditional independences
that allow for a vicarious coupling between the inside and the outside.
So it's the device that gets you out of 20th century equilibrium physics
that we're not talking about closed systems anymore.
We're talking about non-equilibrium steady states.
So we're really in the sort of the things that people like Sean Carroll
and a lot of other people are contending with at the moment,
the physics of open systems and self-organized systems.
And the Markov blanket plays an incredibly crucial role here
in demarcating the edge of the system as it were.
And it allows you to identify something.
Stuff on the inside of the Markov blanket has to have a kind of
synchrony with stuff on the outside.
And in virtue of those conditional dependences that stipulatively define the Markov blanket,
you can now treat the internal states as parameterizing
probability distributions or belief distributions about external states.
So there's quite a fundamental bit of information geometry
that you've been to the table when you have a Markov blanket
that suddenly you can interpret the machinations on the inside
in your computer or your variational autoencoder or your brain
as having beliefs of a Bayesian sword about what caused these data,
what's going on the outside.
So the Markov blanket is absolutely central.
Everything inherits from the Markov blanket.
So to move on to your more challenging question,
does a hurricane or the eternal flame have a Markov blanket?
Strictly speaking, no, it doesn't because you've got this fuzzy leaking
that is from the point of view of somebody who's wanted
to simplify things as a physicist, very irritating.
But of course, it's also incredibly important and challenging.
So where would you go to try and understand these fluctuating blankets
where you have exchanges?
If I worry about this every time I cut my fingernails
or have my hair cut, at what point did my Markov blanket become an external state?
So there's clearly a bit of work to be done mathematically
to accommodate Markov blankets as themselves, dynamical and random objects.
My guess is that you're going to probably go back to the work of Erkov,
who was one of the key intellectual architects of Erkutistina,
Gothic theory, and he was at one point preoccupied by the notion of wandering sets.
So if we think of the Markov blanket as a partition
of all the states of the universe into lots and lots of particles
that comprise the internal states and their Markov blankets,
and if you consider that partition into particles,
as a partitioning into subsets, and then you bring wandering sets into play,
you can now start to see a mechanics and a mathematics
where you can now actually explain things like hurricanes and candles.
3D printers printed themselves.
Is a species of 3D printers one big Markov blanket?
Or do I drill it down and just say Markov blanket only really exists
for the lifetime of a 3D printer?
So all of these issues, I think, speak to exactly what you're drilling down on,
which is how can you accommodate fluctuations?
And if you like, physics of non-equilibria,
not of states of a universe, but of the blanket or the partitions
that define the other Markov blankets.
Does the free energy principle conform to itself?
If we can imagine into the future, maybe,
and if in the spirit of the free energy principle,
we maintain uncertainty about the free energy principle.
Yes, that's very clever.
So I often say in moments of vanity and pride
that the free energy principle is one of the very few principles that conforms to itself.
It's trying to provide an accurate but really complex explanation for everything.
And the only other thing that I know that conforms to itself
is the principle of natural selection,
in the sense that the theories of natural selection themselves
evolved for perium, third world.
And in that sense, this notion that the free energy principle
should accommodate a judicious amount of uncertainty about itself
in absolute light, yeah, that's a very clever observation.
Friston would argue that certain states of the world
and certain states of knowledge would kind of transform themselves
conforming to probability theory.
The free energy principle implies that non-equilibrium steady state systems
must directly model such states of knowledge
and maintain them according to the rules of probable inference
in order to continue existing.
In other words, survival fitness requires inference.
We had this very interesting point from Connor about
there may be game theoretic pressures, let's say,
to drive down accuracy in some sense.
The free energy principle is a lot about the idea
of we want to gain information to create a better
or more accurate model of the world around us.
But there are situations in game theory and decision theory
where that's actually not fitness enhancing.
You might not want to learn the face of your kidnapper
because he's more likely to kill you.
Yeah, that's a fascinating area.
And I see those sort of themes in many different contexts.
I've never heard that before about not wanting to know the face of your kidnapper.
But that's a beautiful example.
It really does tell you that objective functions have to be about beliefs
and the consequences of belief states, not states of the world.
So if there is one example that tells you you're not going to
settle with the Belmont optimality principle, that's going to be it.
But it also tells you that many of these real world problems
have to contend with the fact that the kind of external states
that you are dealing with are composed of sentient creatures like you
and that they also have beliefs.
Keith, why don't we talk about the main formalism
for the free energy principle?
Sure.
So the free energy principle comes down to the equation
that we're showing here on the screen,
which is that the free energy is the summation of two important pieces.
The energy, which is a measure of how well your model fits the observations
or the evidence that you're observing.
And then the second piece, which in a lot of ways is really the more important piece,
of the free energy is the entropic contribution.
And so that's the KL divergence, if you will, between your model of the world
and then the actual hidden model of the world.
And the divergence between those two, it has two important pieces to it.
And I want to kind of call out, I want to highlight one here,
which lets circle right there.
That's purely the model entropy.
Okay.
So that's actually the negative of the model entropy in the KL divergence.
And so as the model gains greater and greater entropy,
and note that that's a positive quantity there, okay,
then up in the free energy that's being subtracted from the free energy.
So the higher the entropy of your model, the lower the free energy.
So as you're trying to minimize free energy, you've got two competing criteria.
One is to fit observations better and better,
but that will require more and more complex models,
which will have lower and lower entropy,
and therefore they will be subtracting less from the free energy.
So what you're trying to do is find the model that fits the data well,
while also maintaining a high entropy.
And the reason why that's important for survival
is that if you have a higher entropy model,
it's maintaining greater flexibility to adapt to incoming information.
So if you recall, log evidence and any associated bounds like free energy
or an elbow and evidence load bound in machine learning
can be written as the accuracy minus the complexity.
And the complexity is just the divergence
between your posterior and your prior belief distributions.
So that is a really and possibly the more important part
of the free energy, the accuracy is well understood,
but doing it in a minimally complex and a maximally compressed way,
that's the heart of it.
So recall, the complexity is just the relative entropy between the prior
and the posterior.
It is the degree to which you change your mind
in the face of this new data or this new sensory evidence.
The free energy principle equally weights accuracy and complexity.
Might it be the case that we need to have a temperature parameter,
like a kind of knob to tweak the relative contribution
between accuracy and complexity?
Would such a parameter be useful?
I'm wondering is in the free energy principle term with the complexity,
I'm wondering if maybe there should be a multiplier there,
like an alpha, something like a Boltzmann constant
that allows me to tweak the relative balance between accuracy and complexity.
There are two answers to that question.
First of all, absolutely not.
The whole point of dissolving that exploration, exploitation dilemma,
the whole point of putting the information gain in the same space
and in the same currency and on the same footing
as your log prior preferences, your reward, your utility,
your Belmonesque-like imperatives,
is that there is a seamless exchange in terms of
NATs, natural units, between the decomposition of your expected free energy
in terms of this intrinsic value and this extrinsic utilitarian pragmatic value.
The other answer is absolutely yes, but in order to acknowledge
that if you're just trying to explain the necessary properties dynamics
of systems that self-organize to some non-equilibrium steady state,
you are saying nothing about the nature of that steady state
other than it is at steady state.
So it could be very high entropy steady state,
it could be very low entropy steady state,
it could be very hot, it could be very cold.
You haven't really committed to any kind of steady state,
which means that it's slightly disingenuous to say
that the imperative for everything is to minimize,
say a free energy function and if you parameterize that
with a particular parameter and we will call it alpha
as a nod to your question, then suddenly you really do need this alpha
and this alpha gets in exactly as you say as a knob on the expected complexity
versus the expected accuracy or ambiguity.
That's not in the literature.
So there are ongoing debates amongst the younger people
who love the maths of this about whether we just need a generic KL divergence
or whether we need to exclude bits to get to expected free energy.
You have to really think about the relative importance
for this steady state of technically the risk and the ambiguity
and the sensory entropy.
So that's again, that's a bit of an open question,
which I'm hoping will be resolved in about a year's time.
At which point your alpha will occur
and I'll try to call it alpha in your honor.
One of the previous guests to our show Dr Hari Valpolar,
the CEO of Curious AI and a computational applied neuroscientist,
he sent us in a couple of questions for Friston actually
and the answer was quite interesting.
But the crux of it is the brain is highly specialized
and is the free energy principle an oversimplification
for what's going on in the brain?
If you take a young child around nine months old,
the critical period for learning the phonemes of your mother tongue
and you play some speech from the radio, the result is nothing.
But if you play the same speech during a social interaction,
the child will learn to discriminate the phonemes in the speech.
Structure matters, structures of the brain being very finely attuned to
and adapted to the context in which they're making their inferences.
One would normally cast that in terms of structure learning,
also known as Bayesian model selection.
So this notion speaks to a simple understanding of evolution
or as nature's way of doing natural Bayesian model selection,
i.e. natural selection,
where you operationally associate the adaptive fitness
with the probability this phenotype exists,
which is just the evidence that it is there,
the probability of finding that phenotype in place.
It is, on some reading,
provably true that for any system or agent to regulate its environment,
it has to embody or be a good model of that environment.
What does that mean?
It's structure must somehow recapitulate the structure of the environment
generating that it has to control or it has to engage with.
So it is hardly unsurprising that the delicate, deep,
hierarchically structured connectivity we find in their brain
and in a variational autoencoder is a natural thing
that has emerged from the evolution of these architectures
that all are conforming with the principle of free energy minimization.
What about Daniel Kahneman's System 1 and System 2 of cognition?
Do those fit into the free energy principle?
He cites Kahneman's System 1 and System 2 of thinking.
There's this famous test where if you give monkeys a sequence of cards
with a hidden pattern on which need to be classified into two classes,
humans suddenly click.
They see the hidden rule because their System 2 kicks in
and they start getting 100% accuracy very quickly,
but you don't see that with monkeys.
So Valpolio is saying that he doesn't see how that phenomenon
could be explained by the free energy principle.
Recent thinking about the cerebellum is that it plays the role of actually a supervisor.
So it may well be the case that the cerebellum plays a role in the amortization
of carefully acquired skilled movements that become increasingly skilled
as the cerebellum watches the cortex.
In that sense, the cerebellum can, I think, be very usefully understood
in terms of being involved in supervised learning,
but I'd actually turn it on its head and basically say,
it's being supervised by the cortex,
but in a way that allows the cerebellum to tell the cortex,
well, normally you do it like that.
Do we do our belief updating and a bit of planning as inference,
or do we just do what we've always done,
habitually respond quickly and efficiently by harnessing
something that has already been amortized?
And that, I think, is a really interesting interpretation.
Habitization versus deliberative thinking,
which from the point of view of a neuroscientist would be the equivalent of system one versus system two.
In that sort of edge between machine learning and reinforcement learning,
sometimes referred to as model-free versus model-based,
it's the bread and butter of a jobbing neuroscientist,
certainly a systems neuroscientist,
to understand the computational architectures and the nature of message passing
implied by either belief updating or predictive coding
or variational message passing.
How it is enacted physiologically on a neuroanatomy,
that has a deep structure.
Connor asks Friston about predictive coding.
There has been a paper that says that predictive coding
can approximate back propagation in arbitrary graphs.
I'm sure a friend of mine has contributed to that,
and if there's any sense that prop has been demeaned as something which is not
absolutely central to everything, then I think that's probably wrong.
So from my perspective, it is invariably the case
that the gradients with respect to anything of variational free energy
can be written down as a prediction error,
which means that if you believe that the universe of interesting things
is performing a gradient flow on a variational free energy functional,
then you also believe that they are, by the chain rule, minimizing their prediction error.
I think that's one of the first times that a premier scientist
actually say something positive about backdrop.
And does the free energy principle tell us anything about the origin of life,
for example, abiogenesis?
If we take for granted that the free energy principle
explains a great deal about how certain systems of the kind we've been elucidating will function,
does it explain the origin of those systems?
Is it almost unavoidable that we will end up
with these deeply structured, delicate, Markov-blanketed systems
that become increasingly rich and have longer and longer information lengths
and more itinerant dynamics as the universe unfolds?
It would be great to know the answer to that
and to have a tractable mathematical formulation of that.
I'm afraid that the free energy principle in its vanilla form,
as it currently stands, does not give you that.
All it says is, if that thing has evolved, these are the properties that it must possess.
But at least it can tell us we should continue exploring
and looking around to address that uncertainty.
Yes, of course, some better free energy principle.
Next, we're going to talk about Richard Feynman's path integral.
This is something that Friston referred to many times during the session today.
Yeah, so the path integral formulation, let's say,
which is essential for quantum electrodynamics, for one thing,
is quite interesting.
And actually, it should be familiar in one sense to, let's say, Bayesians
or people who do a lot of conditional probability theory,
because they may recall that the probability of, say, a particular event
is actually equal to the summation of the probability of that event
given, let's say, some other state of the world,
over all possible states of that A. In other words, this is marginalization
or the marginal probability.
If you have a bunch of possible worlds,
what you have to do is sum over all possible worlds.
And when that's applied to physics, say, in the case of quantum electrodynamics,
what it's saying is that if you have many possible ways in which, let's pretend,
photon could come from A and end up over B,
it could travel along any one of these many paths here,
that if you're considering the total probability of that actually happening,
you have to sum over all these possible states waiting by their amplitude.
And that's a very beautiful consequence, because in some real sense,
we don't think that the photon, say, travels every one of those possible paths,
but it's as if it did.
And why this is important in the free energy principle
is that when we live in a universe where you have to perform
this summation over all possible worlds,
that means that inherently we need a system that can model
probabilities and can transform those probabilities
accurately according to the laws of probability theory.
And another interesting consequence of it is that
if we're playing this repeated game where we're trying to survive over time
and we're getting in new information all the time
about these many possible future paths,
we have to model the world in a flexible way.
We have to keep an entropic model,
a model that has a high entropy that's able to adjust
as new information comes in about those probable paths.
How did he find it talking to Professor Friston?
Oh, I really, I have to say, I very much enjoyed talking to Professor Friston.
When I went into the preparation for this call some weeks back,
I would say I was probably more skeptical of the free energy principle
than I am now.
But as we've learned a lot over the last few weeks,
and then really having a chance to pose some questions to Professor Friston
and hear his answers.
Yeah, I think this is a very interesting principle.
And I think we'll find increasingly as we become more sophisticated
with both the modeling of the principle and the application of the principle
that we are going to find many systems that conform to it.
So I find it quite interesting and completely enjoyed talking to him.
I wish I lived next door to him.
I'd probably hop over there once in a while to have a sherry
and just discuss deep topics.
He has such an intuitive understanding of deep topics
that it's sometimes hard to keep pace.
But if you can, it's very fun.
Yeah, because if you look at the free energy principle on Wikipedia,
it even says at the top of the article that it's very complicated.
Even experts in the field just have trouble getting their heads wrapped around it.
And when you listen to Professor Friston talk,
he clearly is a polymath.
He has deep knowledge of so many different fields.
And it's really interesting to have all of that embodied in one person
because the level of cross-pollination is really quite impressive.
But by the same token, there's so much jargon
and he's diving into so many different areas
that it does make it quite difficult to follow what he's talking about.
I think that Wikipedia has got it wrong that it's complicated.
I think that we often make the mistake of thinking
that things that are hard to understand are inherently complicated.
And I think that's wrong.
I think simple and easy are very, very different concepts.
It's possible for simple concepts to be very difficult to understand
and indeed to take human civilization thousands of years to discover.
And I think this, I think the free energy principle
falls under the category of simple but very difficult to understand
very deep, deep concepts.
Almost like some of the concepts that rest
at the foundation of mathematical logic or probability theory.
When you start to think about puzzles, this sentence is false.
Or if you think about what is the meaning of probability?
Is it a frequency?
Is it a degree of rational belief?
Why or why not?
These are very deep concepts.
At the end, they can be expressed relatively simply on paper
but still be very difficult to understand.
Thank you very much for the very informed questions.
I got a sense you knew the answers before you asked them.
That's how I felt.
Anyway, we really hope you've enjoyed the episode today.
We've had so much fun making it.
Remember to like, comment and subscribe.
We love reading your comments and we'll see you back next week.
The New Yorkers are rude and blunt.
You know, they're very direct and they're really assholes about it.
Well, Germans are polite but blunt.
Like, Germans will very politely tell you your code is completely unacceptable
but they'll be very polite about it.
It's kind of slightly similar to southern culture
or classic southern culture in the US.
It's very polite but straightforward.
Like, they don't like beating around the bush.
But I would say they don't go as far to the rude spectrum
while being blunt.
Like, they still are straightforward but politely.
I'm super sensitive to people beating around the bush.
Yeah, that's it.
That's why it was funny that when you told me
you were like a traitor in New York,
I was like, yeah, this makes a lot of sense.
Like, straightforward and polite.
I love Germans.
I worked with a few of them in my lab and in graduate school.
And I also like how rule-based they are.
Like, there are rules and you follow the rules.
When you open the door, you close the door.
You close it.
You don't leave it half open.
It's like, I love it.
It's awesome.
It's true.
It's both a blessing and a curse.
I used to work with some radioactive dyes.
And I'd disposed of one in the wrong container.
So I'd disposed of like the betas in the alpha container
or something.
And it caused like a cost, like a couple hundred dollars
or something.
The German postdoc there was so like pissed at me but polite.
He's like, just keeps going on and on about how can you put
A in the B container, right?
That's just not how it happens.
That should have never happened.
I don't know what we should do about this,
but something needs to happen to make sure you never do that again.
Yeah, that is such a German thing.
I've heard that exact conversation with people before.
He would keep, just wouldn't let this go.
I'm like, yeah, I'm sorry.
I screwed up.
You know, it wasn't a good enough explanation.
It was like, what is broken that we need to fix?
So this never happens again.
That is the most German thing I've ever heard.
Hello, hello.
Hello, Professor Friston.
Great to meet you.
Nice to see you.
Fantastic.
Now I'm not an expert on etiquette.
Is it Dr. Friston or Professor Friston, which is more correct?
In England, it would be Professor Friston.
Okay.
In the States, I think I've heard that teachers are called professors,
so they prefer to be called doctors.
In the UK, it's a professor.
Gotcha.
And tech industry is first names only.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast
with my two compadres, Connor Lehi and MIT PhD, Dr. Keith Duggar.
Now today we have a very special guest, Professor Friston.
He is a British neuroscientist at University College London
and an authority on brain imaging.
He studied natural sciences, physics and psychology
at the University of Cambridge in 1980
and completed his medical studies at King's College Hospital in London.
A strong indicator of Professor Friston's illustrious career
is that he's been cited over 260,000 times with an H index of 239.
In 2016, he was ranked the most influential neuroscientist on Semantic Scholar
and Carl Friston pioneered and developed the single most powerful technique
for analyzing the results of brain imaging studies.
He's currently a Wellcome Trust principle fellow
and scientific director of the Wellcome Trust Centre for Neuroimaging.
His main contribution to theoretical neurobiology
is the variational free energy principle,
also known as active inference in the Bayesian brain.
The free energy principle is a formal statement
that the existential imperative for any system
which survives in the changing world can be cast as an inference problem.
The probability of existing as the evidence that you exist, if you will.
You can always interpret anything which exists
as being separate to the environment it exists in.
Carl asserts that existence is a kind of interface
between the interior and the exterior.
So the free energy principle,
which is closely related to the Bayesian brain hypothesis,
is a simple postulate with complicated implications.
Any adaptive change in the brain,
or indeed any system which exists,
will minimize surprise or free energy as Carl puts it.
This minimization could be on any timescale
or on any system which resists a tendency
to disorder from single cell organisms to social networks.
The Bayesian brain hypothesis states that the brain
is confronted with ambiguous sensory evidence,
which it interprets by making inferences
about the hidden states which caused the sensory data.
So is the brain an inference engine?
The key concept separating Friston's idea
from traditional stochastic reinforcement learning methods
and even Bayesian reinforcement learning methods
is moving away from goal-directed optimization.
Belief-based updating combines the ambiguous information
with prior beliefs about the nature of the world.
The missing information problem is something
which dogs many areas of machine learning
as we discussed on our GPT-3 episode last week.
Implementing the Bayes rule directly
is often computationally intractable,
and computer scientists drew inspiration from the physics world
for creating approximate inference techniques
called variational Bayesian methods,
and Carl's active inference method
uses these techniques to great effect.
Anyway, Professor Friston,
it's an absolute honor to have you on the show.
Welcome, and how did you come up with this exciting principle?
Well, first of all, let me congratulate you
on that beautiful introduction.
You've said everything that I could possibly say
in the next hour and a half.
So I'll just try and recapitulate what you said.
The road to the free energy principle,
as you have just described it,
started really when I was a student,
aspiring to put together maths and psychology.
So in those days, it would have been known
as mathematical psychology.
Nowadays, computational neuroscience
and leading of computational neuroscience
into machine learning.
So from the start, that was the ambition.
More practically though,
as your brief resume indicated,
I got a bit distracted by becoming a doctor
and then a psychiatrist
out of clinical compassion,
but also partly out of an interest
in understanding how the brain works.
And then, inevitably,
one gets back to mathematical formalisms,
the principles that underlie the sentient behaviour
with which we are gifted.
And the product of that was the free energy principle
or active inference
in a more cognitive neuroscience setting.
I'd like to ask you about two related topics
that have interested in machine learning lately.
One is that I was interested if you were familiar
with the work of Jeff Hawkins from Numenta,
who is a unsung hero among machine learning inspirations.
And he had a quote saying,
it is the ability to make predictions
about the future that is the crux of intelligence.
And recently, with machine learning progress,
such as with GPT-3,
which is a better and better predictive model,
and some would argue, including myself,
also becomes more and more intelligent in a meaningful way.
So my question is,
is prediction really all it takes for intelligence,
or is there more to it?
I think prediction figures very centrally
in the sort of dynamics that we're talking about.
I can answer that question from two perspectives,
often cast in terms of the high road and the low road
to a formal understanding of things like active inference.
So the low road would really be a bottom-up approach,
thinking, what does the brain do?
And on that view or on that approach,
the big move in the 21st century
has been towards predictive processing.
It started with predictive coding
as a nice metaphor for message passing in the brain,
now generalized to subsume action and decision making
and choices and epistemic foraging.
So Andy Clarke coined the phrase,
predictive processing to accommodate that.
And in that sort of formulation of sentient behavior,
prediction is absolutely essential at two levels.
And from the point of view of predictive coding,
obviously it's baked into the title
that you're trying to predict what you will see
under some belief about the sort of late in states
generating some data,
or you could formulate that in terms of compression,
minimizing message length,
and efficient making sense of data or unpacking data.
The prediction in that sense is, I think,
not quite the kind of prediction that you're asking about,
which has this sort of temporal aspect,
sort of the forecasting or anticipatory aspect of prediction.
However, once you move predictive coding
into a sort of Bayesian filtering setting,
and you put dynamics in play,
then when you're trying to predict the trajectories
in the moment,
you are implicitly predicting a short-term future.
So I think sort of prediction in the dynamical
or the temporal sense starts to bite a game.
Beyond that, there is in the neurosciences
an appreciation that that's not the end of the game.
You can get a long way on this low road of understanding
in terms of understanding how we act and perceive
and how action is in the service of perception vice versa
through treating motor behaviour responses
of an embodied brain in terms of reflexes.
And this would be very much a predictive coding
explanation of the action perception cycle,
essentially equipping predictive coding with reflexes
of this sort of in-the-moment sort,
that you could write down in terms of differential equations
or a Kalman-Busey filter.
But the more interesting game is,
I think better cast in terms of planning as inference.
So actually including what I'm doing as a random variable
in your inference problem
and enabling you to roll out much further into the future
and ask, well, what would happen if I did that?
What would my beliefs be about the state of the world
in the long-term future?
And that gives a very different aspect to prediction
that you're predicting the consequences of any move on the world
of any data that you might want to sample.
How will that reduce my uncertainty?
What information will I gain?
What relevance does that have
for the kinds of preferred outcomes
that characterise me as a goal-directed creature?
So I think prediction in its full and glorious
anticipatory sense really takes centre stage.
And when people talk about active inference,
it's sort of implicitly they're talking about
this sort of fuller deep temporal approach
to inferring what should I do next?
And then from those inferences,
selecting the next move to make.
So in that sense, the sort of hierarchical temporal models
that Dalip and Jeff Hawkins have been talking about
for decades as well.
And I can remember reviewing as a handling editor
the first publication in plus computational biology
on this work, and it was refreshing.
And I think that that's exactly the kind of
planning as inference in the future,
consequent on or conditioned upon the different kinds
of policies or sequential sequences of actions
that I could entertain, bringing time for centre stage
into the inference problem.
The high road I should just add,
so using a completely different language now
that I would use as a physicist
to promote the importance of prediction,
takes a slightly different view.
It says, well, to understand the first principles
that underlie sentient behaviour,
you have to understand the dynamics of self-organisation
and in particular, self-organisation of systems
that are open to the environment,
to the eco-leash, to the heat bath, the heat reservoir.
And when one does that, you can use dynamics
and random dynamical systems to say the kind of moves
and changes of the systemic states must have this form
of predictive coding or Bayesian or Kalman-like filtering
in the moment.
And that's relatively straightforward to show,
and there was a hint of the requisite mathematical apparatus
you needed to make those assertions in the introduction
and specifically mark off blankets
and separate me from not me.
So that would be grade one, if you like, prediction.
It's just that we have dynamics
and the dynamics required of self-organisation
to non-equilibrium steady state require some
in-the-moment kind of prediction.
So where is the higher end anticipatory of dynamics?
Well, that comes in through Feynman's pathogenical formulation
and thinking not just about the flow of the dynamics
of self-organisation at this point in time,
but trajectories into the future
and the probability distributions over those trajectories
and particularly the states that act upon the outside,
if you like, formally characterise what we do
over extended periods of time.
So when you bring the path integral formalism to bear
on these non-equilibrium steady state distributions,
then you can sort of say what kinds of policies
as I move into the future are more likely
and which are less likely.
And then things get much more interesting
and we talk about sort of ambiguity and risk
and what the imperatives are for long-term behaviours.
And underneath all that, there is an interpretation
that I'm really trying to predict
or you can interpret this in terms of inferring
the most likely paths, basically as resting upon
a prediction of states of the world in the future
conditioned upon a particular sequence of actions or policy.
One of the things that I think is very interesting
about the free energy formulation
is that prediction is half the story.
So getting accurate predictions about the future
while very important is juxtaposed with keeping your options open,
keeping a flexible mind, keeping a high entropy model of the world
so that as you encounter perhaps new situations,
it has the flexibility to adapt more quickly,
that incoming information.
And that because it's a repeated game,
you're not just trying to optimise the very next step only,
but the entire trajectory of your existence, if you will.
And that's captured in the free energy principle
but inside the KL divergence part of that
is the entropy of my model of the world.
And the larger that is, the lower the free energy is.
This interesting interplay, and you also point out in a video too,
that when it's this repeated game,
it's related in a way to what in other fields is called
the exploration versus exploitation kind of trade-off,
like in the multi-armed bandit world
where I can take an action that's maximally exploitative
but I may not learn very much.
And then on the other hand, I can take an action
that's purely just to learn about the world
but may not achieve much of my other objective.
And what I love about that is it explains
why we're always so tempted to push the button,
why people always want to press the button
and see what happens or go out and explore,
even if there's no immediate benefit.
And it has so many connections to things like novelty search
and multi-armed bandit analysis.
So it's quite interesting.
Yeah, you've been on so many fascinating points there.
Yeah, and robotics, intrinsic motivation in robotics,
artificial curiosity and a Schmidhofer-like sensual,
which itself all comes back down
to this minimizing the complexity
and paradoxically celebrating uncertainty.
So if you allow me, you made certainly good points there.
I can't resist just highlighting the first of all,
predictions only half the story, absolutely.
I mean, why are we predicting?
Well, we're predicting in order to infer what to do next.
So the doing, the action, the inactive aspect
really becomes the central thing in terms of, well,
things that are written into the description
like self-organization, how I organize myself, what I do.
And if you're in data science,
that would be basically how do I optimally data mine?
Can I find easy and optimum design principles written
into the formulas and the variation formulas
and the free energy principle?
So I think just saying that perception is great
if you're just studying the visual cortex
or you're interpreting some given pixelated image.
But the real challenge is really,
how do you go and take that picture
and how do you move your eyes around
and sample that information
that's going to make much more sense of the world?
So it's all in the service of what I'm going to do.
You mentioned the KL divergence,
the central role of relative entropies
in this sort of variational construct.
I think that your formula is so important.
So a very simple perspective on free energy,
which I think is useful for students,
is to remind them that the free energy
is some expected potential,
some expected utility, if you like,
and complimented by the entropy.
And in minimizing the free energy,
you're also trying to maximize the entropy,
which seems sometimes counterintuitive.
But it is exactly that,
which is really mandated by things like Ockens' principle
and very practically relevant.
So if you don't do that,
if you don't put that uncertainty into the game,
so you don't commit to a very precise explanation
for these data,
then you're going to run into things like sharp minima
and you're going to be searching for resolutions of that
in terms of broadening your uncertainty,
flattening that free energy landscape
to try and secure those flat minima
where you can be more reasonably assured
that you've got some global minima,
say, in standard deep learning
or a machine learning context.
That would be a story you could tell
just about the composition of why free energy
rests upon entropies and relative entropies.
But you took us straight to the heart
of the sort of deeper in-time inference
that comes along with the active inference,
which is this sort of this implicit imperative
to reduce uncertainty that can often be described
in terms of responding to salience
or epistemic affordance.
Just say, if I did that, what would happen?
What would I know?
What information would I gain?
Novelty you mentioned.
So technically, what we tend to do
is to talk about the resolution of uncertainty
or the information gain
that is formally described by KL divergence
between my beliefs about states of the world,
with and without those sensory samples or observations
that I would get if I did that.
So that KL divergence can apply to unknown states of the world
and that would be driving behaviors
like looking over there
to make sure that my hypothesis
that that was a butterfly or a bird or a candle
was the best explanation
for these sort of peripheral visual sensations.
But you've also got unknowns, for example,
in a deep learning scenario, the connection strengths
in the brain parameters that underwrite
our brain connectivity that encode the lawful contingencies
and statistical regularities of likelihood mappings
or probability transition matrices.
So these don't change quickly in time,
but they still are equipped with a posterior belief
or some stochastic representation
about which you can reduce uncertainty.
And then we call that novelty.
So if you can get yourself into a situation
where you reduce your uncertainty
about the contingencies
and the parameters of your generative model,
then you're responding to, again, an epistemic affordance.
There is exactly this resolution of uncertainty.
Formally, we'd write that down as an expected free energy.
I don't know if this helps, but one way I find useful
to think about this and indeed explain it to students
is free energy is abound on log evidence,
simply the probability of some data under a model
of how I thought those data were generated.
And you can always split that into accuracy and complexity.
So if I was a statistician and I wanted to minimize
my physics free energy or maximize my machine learning free energy,
which is cause is just an evidence lower bound
and the sort you find in a variational autoencoder.
What am I doing?
If I decompose the log evidence into accuracy and complexity,
what I'm trying to do is minimize the complexity
of my accurate expectations and then we get back to
minimum description lengths, the underlie,
say, you know, call monograph complexity formulations
of most efficient coding and the like.
So you've got this way of looking at self-evidencing
in the sense of just forming good beliefs
of how my data were generated in terms of providing the simplest
but most accurate explanations or accounts of the data at hand.
So if we come back to this to the previous discussion
about what would happen in the deep future.
So I did a deep research over all the sequences
of moves that I could make.
What would the expected accuracy
and the expected complexity look like?
And if you write that down, they come out
as things like risk and ambiguity.
So the expected accuracy or more precisely the expected
inaccuracy, the negative accuracy,
the negative log probability of your data under beliefs
about their causes becomes ambiguity,
becomes the conditional uncertainty if you like
about the observations if you knew them.
And the expected complexity becomes risk
or it has exactly the same form
that you'd find in KL control theory
or risk sensitive control in economics.
It's just the relative entropy or the KL divergence
between what you think will happen if you did that
and what a priori you think is going to happen,
which normally people treat as a sort of the preferred outcome,
the kind of things that a creature or a system
or an agent like me would normally encounter.
So that ambiguity is actually the thing
that you were talking about,
the reason why we push that button or open that door
or go on that trip or go into that Google page.
It is written into, it is if you like,
the homologue of our aspirations for accurate accounts
of all the sensory and other data
that we have to assimilate.
When we consider the expectation in the future
conditioned on what we'd actually do.
We're getting quite close to the center of the bullseye here
and we're talking about the dichotomy between
belief free and belief based methods.
So you've been sketching out this really interesting idea
that we can take a subsequent actions
by maximizing the expectation over a generative model
of states in distribution and subsequent policies.
But the alternative is so-called goal directed behavior.
Now you said in your papers that goal directed behaviors
is fine for learning kind of basic habitual policies,
but you cite this wonderful example with an owl.
You say that an owl doesn't necessarily know
where the mouse is in the field.
There might be some ambiguity, there might be some uncertainty.
And using a simple goal directed policy,
the owl would just go straight to the mouse
and try and eat the mouse.
Now it's quite interesting to contrast your approach
to other forms of let's say Bayesian reinforcement learning
or traditional stochastic reinforcement learning.
Of course, normally there's this value function
that needs to be either computed directly
in the non-stochastic setting using the Bellman equations
or some other method.
But your approach is a stark departure away
from having this value function.
Again, you've said everything that I could possibly say.
I'll send you on my next lecture tour.
That'll take the pressure off me.
Absolutely.
And of course, I forgot to just to highlight
this sort of exploitation, exploration dilemma
that has dogged 20th century thinking.
Of course, that just dissolves in the 21st century
because risk and ambiguity, sometimes you can exchange
the terms of this expected free energy formulation
and re-express that as an intrinsic and extrinsic value
where the intrinsic value is something that people in robotics
not official curiosity would recognize
as that sort of epistemic information gain part.
And the extrinsic value is just the expected utility
or the expected reward.
In minimizing this mixture, that just is the expected free energy.
You've got exploration and exploitation solved for free
in the right order.
So you normally see food in the fridge
before you start preparing your meal.
You don't do it the other way around.
That's what you were hinting at in terms of the owl
opening its fridge and searching.
It's slightly disingenuous of me, I think,
to use such a stark contrast dialectic
between what is in essence a whole bunch of things
predicated on the Bellman optimality principle
and contrast that with another way of doing it,
which is essentially a variational Hamilton's principle
of station reaction.
So the free energy principle inherits from the physics
of non-equilibria that you will find
from Feynman's pathological formulation
of quantum electrodynamics right through to Hamilton's equations
of motion.
So this is the variational principle
that is the first principle that we were mentioning before.
So the question is, do you choose between
variational principles of station reaction
or do you go for Bellman optimality principles?
And in that choice, what's the big commitment you're making?
And you said it, you said that the Bellman optimality principle
is only fit for purpose if there exists a value function of states
that will ensue if I commit to that action.
Whereas the variational principles of least action
being a path integral of an energy
and an energy being a log probability of a probability
distribution immediately tells you that the objective function,
if you want to cast it in terms of normative theories
of optimization, the objective function
is a function of a probability distribution.
So it has to be belief-based.
If you read a probability distribution as a Bayesian belief,
so if you allow me to talk about beliefs simply
as conditional probability distributions
in the sense of belief propagation or Bayesian beliefs,
then we know that from physics,
the objective function, the Lipunov function,
the cost function has to be a functional function of Bayesian beliefs.
So that comes back to your point,
that if you go down the Bellman route, the RL route,
the optimal control route,
then you are predicating everything on a value function of states.
If, on the other hand, you go down a variational or a physics route,
then your objective function has to be a functional of beliefs about states.
And that's where you get the resolution of uncertainty.
That's where searching gets into the mix,
because to search is to resolve uncertainty.
But uncertainty and surprise and all of these other attributes of belief updating
are all attributes of probabilistic beliefs, probability distribution.
So you know you have to have a functional of probability distributions
in order to account for sentient behavior.
And I would say, including goal-directed behavior.
Why is that disingenuous?
Well, it's disingenuous because, of course,
what you can do is take this more general formalism
than this variational formalism and take out of it all the uncertainty.
And then you can get back to the Bellman-Octomality principle.
So really, there's not a dialectic here.
It's just that the Bellman-Octomality principle deals with a special
but very ubiquitous and pragmatically very important case
where you can discount various sources of uncertainty.
So common examples here.
Let's ignore the partially observed aspect of a problem.
Let's assume that our observations tell us everything we need to know
about latent states causing data or latent states of the plant that I'm trying to control.
So that takes ambiguity out of the expected theology before you do anything.
What are you left with?
Well, you're left with the expected risk.
What's that?
That's the expected complexity.
So that's just a probabilistic measure, a relative entropy measure,
of the distance probabilistically between your prior preferences,
your goals, your rewarding states, and what you think will happen.
So if you say, look, now let's assume for simplicity that it doesn't matter
every which way that I can do this.
The uncertainty about what's going to happen is not going to change.
That means that risk now doesn't play any role anymore
other than to score the expected utility or the expected log of your prior preferences.
That just is Bayesian decision theory.
So you can get quite easily back to what was deliberate but maliciously engineered dichotomy.
You can dismiss that just by acknowledging that you only need to worry about uncertainty in epidemics
when you put uncertainty and not knowing into the mix.
I'd like to explore that, as you put it, maliciously engineered false dichotomy
a little bit further, but in a different context.
It's one of the Markov blanket.
For example, you were with Sean Carroll not long ago, and I think he asked,
does a hurricane have a Markov blanket?
And he said, well, no, it doesn't.
And that's quite annoying, actually, that it doesn't have a conveyor.
Maybe the eternal flame is sort of the nemesis because it doesn't.
We get to this, when is a pile of sand?
A pile of sand.
What I'm curious about is, does the concept of a Markov blanket,
for the purposes of contemplating the free energy principle,
can it be a fuzzy Markov blanket?
So just for an inanimate example, I can imagine a Markov blanket around the moon and one around the earth,
but on the other hand, they gravitationally interact.
But I don't think that precludes me from thinking about dimensions of the free energy principle,
even if there are interactions that pierce the boundary, if you will.
But still, if it's enough of a boundary, enough of a blanket, if you will,
I can still think about it in useful ways.
Is that true?
I mean, do we really need an exact Markov blanket,
or is it flexible enough that it can be quite a fuzzy boundary?
There's an excellent question, and the honest answer is, I don't really know at this stage,
but what is known, and in not knowing, I mean that in a constant way,
that this is a future challenge that I think people will contend with in the next few years,
and I'll try to explain why that's important from my perspective.
So the crisp and clear consequences of having a Markov blanket is when one unpacks it right
through to the path integral formulation, the implications for the good trajectories
in terms of minimizing expected free energy really do rest upon there being a well-defined
Markov blanket, which, as you say, is the mechanism that allows for these connections
and influences at a distance that would keep the Moon in orbit around the Earth, for example,
whilst at the same time accommodating the conditional independences that enable me to
distinguish the Moon from something else. So in an idealized formulation, the Markov blanket
exists at non-equilibrium steady state, so it exists in eternity in a very crisp and well-defined
way that there are specific conditional independences that allow for a precarious coupling between the
inside and the outside. So it's the device that gets you out of 20th century equilibrium physics,
that we're not talking about closed systems anymore, we're talking about non-equilibrium
steady states, so we're really in the sort of the things that people like Sean Carroll and
a lot of other people are contending with at the moment, the physics of open systems and
self-organizing systems. And the Markov blanket plays an incredibly crucial role here in demarcating
the edge of the system as it were, and it allows you to identify something, so anything has to have
a Markov blanket, strictly speaking, for a particular period of time. Just as an aside,
what happens when you do that is that stuff on the inside of the Markov blanket has to have a kind
of synchrony with stuff on the outside, and in virtue of those conditional dependences that
stipulatively define the Markov blanket, you can now treat the internal states as parameterizing
probability distributions or belief distributions about external states. So there's quite a
fundamental bit of information geometry that you've been to the table when you have a Markov blanket,
that suddenly you can interpret the machinations on the inside in your computer or your variational
autoencoder or your brain as having beliefs of a Bayesian sword about what caused these data,
what's going on the outside. So that's absolutely crucial for the whole sort of active inference
and certainly representational or realist or anti-realist sort of philosophical interpretations.
So the Markov blanket is absolutely central. Everything inherits from the Markov blanket,
so everything else was in play before. We have the Planck equation, we have the Schrodinger-Awaybe
equation, we have variational principles of stationary action and pathological formulations,
that's all there. All you need to do is to drop the Markov blanket in and then you get active
inference. So to move on to your more challenging question, does a hurricane or the eternal flame
have a Markov blanket? Well, strictly speaking, no it doesn't because you've got this fuzzy leaking
that is from the point of view of somebody who's wanted to simplify things as a physicist,
very irritating, but of course it's also incredibly important and challenging. So
where would you go to try and understand these fluctuating blankets where you have exchanges? So
just technically you've got external states and then you've got blanket states that intervene
between the internal states and the external states and yet with something like a candle flame
or a hurricane, you seem to have this exchange that once an external state becomes a blanket state
and once a blanket state becomes an external state. And I worry about this every time I cut my
fingernails or have my hair cut. At what point did my Markov blanket become an external state? So
there's clearly a bit of work to be done mathematically to accommodate Markov blankets
as themselves, dynamical and random objects. My guess is that you're going to probably go back
to the work of Erkov, who was one of the key intellectual architects of Erkutistina, Gothic
theory. And he was at one point preoccupied by the notion of wandering sets. So if we think of
the Markov blanket as a partition of all the states of the universe into lots and lots of
particles that comprise the internal states and their Markov blankets. And if you consider that
partition into particles as a partitioning into subsets, and then you bring wandering
sets into play, you can now start to see a mechanics and a mathematics where you do actually
can now actually explain things like hurricanes and candles. And just for interest, literally a
couple of weeks ago, there was a nice paper in frontiers treating the biosphere as a Markov blanket.
So there is practically an importance of pressures in order to formalize what are the
Markov blankets of these large scale structures. And my guess is that the notion of wandering
sets and the time scales over which internal states exchange with external states and internal
states, external states exchange with blanket states are going to be an important issue.
And I say that because, of course, we develop what point when I am born and what point am I
died? What happens to my Markov blanket? Reproduction, 3D printers printing themselves,
is a species of 3D printers, one big Markov blanket? Or do I drill down and just say Markov
blanket only really exists for the lifetime of a 3D printer? So all of these issues, I think,
speak to exactly what you're drilling down on, which is how can you accommodate fluctuations and,
if you like, physics of non-equilibria, not of states of the universe, but of the blanket or
the partitions that define the Markov blankets. If we can imagine into the future, maybe, and if
in the spirit of the free energy principle, we maintain uncertainty about the free energy principle,
it may be the case that as we extend it to, say, wandering sets with wandering points,
we may develop a mathematics of more of a distance metric. This cloud of points is currently this
distance from a particular wandering point and they're orbiting around and there may be some
dynamics in there. And then if we follow a similar analysis, we might end up back at
the free energy principle plus another term, for example, like there may be an extra term over
there, which could be important. Would that be kind of a fair speculation of what might happen?
Yes, that's very clever. So I often say in moments of vanity and pride that the free energy
principle is one of the very few principles that conforms to itself. It's trying to provide an
accurate but minimally complex explanation for everything. And the only other thing that I know
that conforms to itself is the principle of natural selection and the sense that
the theories of natural selection themselves evolved for Perian third world. And in that sense,
this notion that the free energy principle should accommodate a judicious amount of uncertainty
about itself is absolutely right. Yeah, that's a very clever observation. In terms of the form,
I think that's probably right as well. I mean, you're asking me to think about where we might be
in say 10 years time. And so this is just hand waving. But certainly one could imagine
variational principles of stationary action applied not to probability distributions over states,
but probability distributions over partitions and sex and getting into sort of things like
category theory and the like. So one can see a sketch or an image or the future kind of
map that might conserve the relatively simple principles that underwrite the free energy
principle, but now applied at sort of one level up as it were. So not to the actual system itself,
but to the probabilistic configurations and the partitions that define that principle.
If I can zoom out a little bit here. So the free energy principle is a lot about the idea of we
want to gain information to create a better and more accurate model of the world around us.
But there are situations in game theory and decision theory where that's actually not fitness
enhancing. So two examples here, you might not want to learn the face of your kidnapper because he's
more likely to kill you. And there's the other thing where you might, so this is an information
you don't want to gain. And sometimes it's also useful to have false beliefs. For example, it can
be useful to think you're smarter than you actually are or more confident than you actually are,
because that will help you in social situations. Yeah, that's a fascinating area. And I see those
sort of themes in many different contexts, wishful thinking, deliberately avoiding certain sources
of information, not wanting to open a letter that tells you whether you've passed your exams or
a letter from the doctor that contains the results of your recent scan for cancer, for example.
There are things that we don't want to know. So we're talking about very metacognitive things.
In the sense that I never heard that before about not wanting to know the face of your
kidnapper. But that's a beautiful example. It really does tell you that objective functions
have to be about beliefs and the consequences of belief states, not states of the world.
So if there's one example that tells you you're not going to settle with the Bellman optimality
principle, that's going to be it. But it also tells you that many of these real world problems
have to contend with the fact that the kind of external states that you are dealing with
are composed of sentient creatures like you and that they also have beliefs. So now you get into
the game of putting lots of active inference agents together and multi-agent scenarios and how you
infer the degree of sophistication with which you infer the intentions of others and the belief
states of others. And all that gets into these very deep and enriched generative models that
themselves now represent the belief states of other active inference, your optimization machines
as it were. One other level I think that you can understand these sometimes paradoxical
or counterintuitive expressions of optimality is just to acknowledge that the whole of the
Bayesian brain hypothesis, the whole basis of indeed free energy principle is a statement about
prior beliefs. And as such, prior beliefs are going to be in the context of self-evidencing
necessarily optimistic. So if you remember before, I was reading prior beliefs about the outcomes
consequent on the behavior as preferences. And the only reason I say preferences is because
if those are the kind of states that I typically find myself in, so if I'm a physicist, those the
attracting set of my non-equilibrium steady state, then it looks as if I will always be working towards
these prior states. So they look literally mathematically attracting in the sense of
being that attracting set, but also psychologically attractive and valuable, and therefore the
preferred states that I worked towards. So baited into a Bayesian reading of the information
of theoretic imperatives for choice is an optimism. You're always moving towards or trying to solicit
evidence that confirms that you are an eternal and mortal, well-loved, warm creature. So
it has to be like that. It can't be any other way. Manifestations of that I always find appealing
because they're just statements of the fact it couldn't be any other way. I have to believe myself
to be an expert in this or very proficient in that in order to realize those preferred fantasies
through action to actually secure the evidence. Yes, I am that kind of thing. Without those priors,
without those optimistic priors, you would not get the kind of active inference that underwrites
our existence to come back to some of the phrases that Tim was using to introduce the free energy.
We had this very interesting point from Connor about there may be gain-theoretic
pressures, let's say, to drive down accuracy in some sense. There could also be, and I'm not sure
if this strays outside the free energy principle or not, but implementation factors. So the lesson
is back to Maxwell's demons and all the very sophisticated kind of information engine analysis
that people have done from then until now has shown that the actual recording and computation of
information is an entropic process that generates entropy. And so even though the free energy
principle does have a term in there, which is the entropy of my model per se, I'm not sure if that
captures the entropy associated. And I mean the thermodynamic entropy, if you will, associated
with actually maintaining and operating that model. So it may be the case, for example, that
you reach a point where increasing my intelligence a little bit more cost me much more to operate
and maintain than it actually buys me and improved accuracy, for example. Where do you see that trade
off does or doesn't fit into the free energy principle? I think the trade off fits very
neatly. In fact, you could say it was your enlarged part constituent of the free energy
principle in the following sense. And again, you've preempted everything that I might have said
about this, but I'll say it again. So the complexity that we were talking about. So if you recall,
log evidence and any associated bounds like free energy or an elbow and edlin slow bound
in machine learning can be written as a as the accuracy minus the complexity. And the complexity
is just the divergence between your posterior and your prior belief distributions. Now that is a
complexity cost. It's also exactly the the cost that underwrites some of universal computation
computational formulations of generalized or universal intelligence. It is the thing that
drives compression in predictive coding or engineering applications of it. It is the
thing that you're going to submit who would emphasize if he were part of our conversation,
that sort of complexity minimization. So that is a really and possibly the more important part of
the free energy, the accuracy is well understood, but doing it in a minimally complex and a minimally
maximally compressed way, that's the heart of it. That's important because by Landau's principle
and the Jinsky equality, that also directly one to one dictates the number of jewels that you will
expend in belief updating when you move from your prior to the posterior. So recall, the complexity
is just the relative entropy between the prior and the posterior. It is the degree to which you
change your mind in the face of this new data or this new sensory evidence and therefore is exactly
the changing or the erasure of bits of information that can be measured in natural
units using natural logarithms that via Landau's principle has an exact thermodynamic cost in
jewels. So when we're talking about informationally efficient complexity minimizing schemes,
compression schemes, from my perspective anything that minimizes variational free energy or belief
updating that minimizes variational free energy, you are also exactly talking about the most
efficient way of doing it thermodynamically. So what that means is if you want to build the best
kind of computer, one simple way of scoring how good your computer is, is to take two machines
that have the same accuracy and the one that uses less electricity is the best one. And that will
also by definition provide the least complex account and thereby minimize its free energy.
So I think that's a really important practical observation that what we're aspiring to here
are really cheap and cheerful machines that can do their sort of their synthesis.
Just to make it a bit more concrete, what I'm wondering is in the free energy principle term
with the complexity because I completely agree and get the point that's the crux. That's the most
important term. I'm wondering if maybe there should be a multiplier there, like an alpha,
something like a Boltzmann constant that allows me to tweak the relative balance between accuracy
and complexity. Yeah, temperature parameter. Yeah, that's a great question. There are two answers
to that question. First of all, absolutely not. The whole point of dissolving that exploration,
exploitation dilemma, the whole point of putting the information gain in the same space and in the
same currency and on the same footing as your log prior preferences, your reward, your utility,
your Belman-esque like imperatives, is that there is a seamless exchange in terms of
gnats, natural units between the decomposition of your expected free energy in terms of this
intrinsic value and this extrinsic utilitarian pragmatic value. One of the benefits of removing
that alpha or beta or temperature coefficient on the complexity versus the accuracy that for
the expected free energy now becomes risk and ambiguity is that now you can talk about reward
in terms of gnats. You can talk about how rewarding information is because they're just some arbitrary
carving up of a single expression that the expected free energy. That seems to me important
because that is if you like the sort of in terms of a dimensional or unit analysis,
that is the thing that truly does dissolve the exploration, exploitation dilemma and puts the
value of information alongside the value of a money or the value of a fruit drop if you're doing
animal experiments. So absolutely not. You really want to totally avoid any temptation to start
adding ad hoc hyperparameters like temperature to this beautiful construct which explains everything.
The other answer is absolutely yes, but in order to acknowledge that if you're just trying to explain
the necessary properties dynamics of systems that self-organized to some non-equilibrium
steady state, you are saying nothing about the nature of that steady state other than it is at
steady state. So it could be very high entropy steady state. It could be very low entropy steady
state. It could be very hot. It could be very cold. You haven't really committed to any kind of
steady state which means that it's slightly disingenuous to say that the imperative for
everything is to minimize say a free energy functional. It's not. That's why very carefully
Hamilton's principle of stationary action is just keeping it flat at steady state. It could
be very high. It could be very low. And that starts to beg the question, what kinds of steady
states are non-equilibrium? Are we really interested in simulating, reproducing, describing?
And if you get under the hood in terms of the relative entropy and the mutual informations
shape these steady states, and in particular the distributions over this Markovian partition
into inside and outside the blanket states, it turns out that one important attribute or
description of the state depends upon the way in which the active states increase the mutual
information between the internal and the external. And if you parameterize that with a particular
parameter and we will call it alpha as a nod to your question, then suddenly you really do need
this alpha. And this alpha gets in exactly as you say as a knob on the expected complexity versus
the expected accuracy or ambiguity. So what that means is in a more general formulation of the
free energy principle or certainly its application to understanding active inference,
there would be this extra parameter that really tells you're dealing with systems that
are exquisitely structured in the sense that they occupy a very small number states,
they could occupy in a particular way that is active, that they actively construct their
mutual information between the inside and the outside. At the moment, that's not in the literature.
So there are ongoing debates amongst the younger people who love the maths of this about whether
we just need a generic KL divergence or whether we need to exclude bits to get to expected free
energy. And by acknowledging there are different kinds of non-equilibrium steady states and that
in so doing, you have to really think about the relative importance for this steady state of
technically the risk and the ambiguity and the sensory entropy. So that's again, that's a bit
of an open question, which I'm hoping will be resolved in about a year's time. At which point
your alpha will occur and I'll try to call it alpha in your honor. Maybe I should have gone
with Omega, that way it's the last parameter we ever need, but thank you. Absolutely fascinating.
There's been a lot of emphasis on a single all-encompassing learning principle,
this grand unified theory of the brain or indeed any system. Now, I've been speaking
to one of our friends on the show. He's given us two questions. This guy is Dr. Hari Valpola.
He's an applied theoretical neuroscientist and he's the CEO of Curious AI, which is a
reinforcement learning based startup. And he's a very accomplished scientist actually. And he says
it does make sense to claim that there's a unified cortical algorithm. And anatomically,
the cortex is very uniform. And although there are adaptations to specific tasks,
they seem to be variations of a single unified template. And in biology,
function follows form. However, he says that to claim that everything follows from free energy
minimization is just overly simplistic in his view. He says it's obvious that for the cortex,
goal-orientated learning and perception is crucially important. He says the cortex isn't
just learning and perceiving all the structure and information it receives from the sensors.
Learning is heavily modulated by task demand and attention. And so is perception itself. So
he gives this example, he says, if you take a young child around nine months old,
the critical period for learning the phonemes of your mother tongue,
and you play some speech from the radio, the result is nothing. But if you play the same
speech during a social interaction, the child will learn to discriminate the phonemes in the speech.
I think it's an excellent observation. And it takes us into a sort of world which we haven't
really been discussing so far, which is the different flavors of free energy minimization
and how it might be manifest. And so just coming back to the point that structure matters,
structures of the brain being very finely attuned to and adapted to the context in which
they're making their inferences. One would normally cast that in terms of structure learning.
If I was a statistician, there are at least three, if not four levels of maximizing model
evidence, i.e. minimizing variational free energy, also known as marginal likelihood.
I can make inferences to reduce my uncertainty about states that change with time.
I can do learning, which is reducing uncertainty or increasing the marginal likelihood
of the parameters of my generative model. I could also think about the hyperparameters that
encode the usually cast in terms of precision or negentropy of various distributions. And that,
interestingly, speaks to ignoring things that we're talking about previously with Connor. Sometimes
it's best to ignore in a base optimal sense. If you've got very inconsistent and incompatible
information, that's one base optimal explanation for ignoring. And that speaks to intermediate
temporal scale of free energy minimization. But the one I want to get to beyond that is structure
learning, also known as Bayesian model selection. So if I was a statistician, I wanted to score
the quality of my generative model, my hypothesis, my convolution model, whatever it was,
then I would score it with a free energy approximation or bound on the marginal likelihood
of the model evidence. Then I take another model, another hypothesis, and I would compare
the adequacy of these two hypotheses using a procedural Bayesian model comparison.
And that is formally identical to choosing the hypothesis with the least free energy.
So the free energy scoring the evidence for this hypothesis or that hypothesis.
So you have now a mathematical description of what incognitive science would be known as
structure learning, optimizing the very structure of your model. Does it have three
layers? Is my deep network fit for purpose? Should I add another layer? How many units
to have in each layer? Do I have a more sparse connectivity? Do I use linear rectifying?
All of these structural aspects can, in principle, be evaluated in terms of the
free energy or the underlying marginal likelihood or the evidence.
So you have this world of structure learning. Now, that world itself can proceed at different
time scales. It can proceed in terms of the development of a particular architecture or
phenotype. If you're a developmental psychologist, you would understand this structure learning
from a new developmental point of view as the brain grows, experienced dependent learning
that is driven or can be at least described by free energy minimization or self-evidencing or
maximizing model evidence increasing through the addition of different connections and
different cortical layers and cortical areas or bringing them online through changes in
connectivity that can actually progress right through until the early 20s in your development.
You would see that or you could describe that in terms of structure learning
over somatic time. But the exactly the same principles apply at an evolutionary time scale.
So this notion of free energy minimization as Bayesian model selection
speaks to a simple understanding of evolution or as nature's way of doing natural Bayesian model
selection, i.e. natural selection, where you operationally associate the adaptive fitness
with the probability that this phenotype exists, which is just the evidence that it is there,
the probability of finding that phenotype in place. I should also say that all of these
different expressions of free energy minimization at different spatial and temporal scales are all
exactly consistent with some of the fundamentals of cybernetics and the inception of theories of
self-organization, such as, for example, the good regulator theorem. It is on some reading
provably true that for any system or agent to regulate its environment, it has to embody or
be a good model of that environment. What does that mean? It's structure must somehow recapitulate
the structure of the environment generating that it has to control or it has to engage with.
So what does that tell you? It tells you that the structure, including all those canonical
microcircuits, including those visual hierarchies of our brains, for example, or the multiple
hierarchical layers in a deep neural network, have to be there if it is the case that the
world they're trying to model has a deep structure, if things are hierarchically generated.
We know that to be the case for us because most of our world is caused by other creatures like us,
as we're currently witnessing. We know that there is a hierarchical and dynamical aspect to it,
where the hierarchies, say, interpersonal interactions transcends just to individuals,
but groups of people, conspecifics, families, societies, political affiliations,
theological affiliations, or geographical hierarchical structure. So it is hardly
unsurprising that the delicate, deep hierarchically structured connectivity we find in their brain
and in a variational autoencoder is a natural thing that has emerged from the evolution of
these architectures that all are conforming with the principle of free energy minimization.
It really resonates with a lot of the things that we've been speaking about on our podcast here,
because in machine learning, of course, there are inductive priors built into models. Even the
neural network has this hierarchical organization, which is an inductive prior, but we've spoken a
lot about meta-learning as well. And this thing that you're saying, that there's a structurally
relevant inductive pathway which could be learned. But the last question from Hari is,
he wants to know about the cerebellum, for example. He said, there's plenty of evidence that it
learns by supervised learning and can perfectly well handle regulation tasks without any models.
And he cites innate reflexes, which train a complex feed-forward controller without any
internal models. I mean, as a bit of a side thing here, Val Polo is in a way implying that the
free energy principle is kind of unsupervised. That's the way he's talking about it, which
there might be some truth to that, but we'll come back to that. But he also cites the basal ganglia,
the hippocampus, the amygdala, and the superior colliculus. Each of them learns,
and their learning algorithms definitely don't seem to have much to do with the free
energy minimization in his opinion. He says that the basal ganglia is a kind of reinforcement
learning, and the hippocampus is a one-shot learning. And the amygdala and the superior
colliculus are supervised learning for specific tasks. And this is super interesting, right?
Because he cites Kahneman system one and system two of thinking. There's this famous test where,
if you give monkeys a sequence of cards with a hidden pattern on, which need to be classified
into two classes, humans suddenly click. They see the hidden rule because their system two kicks in,
and they start getting 100% accuracy very quickly. But you don't see that with monkeys.
So Val Polo was saying that he doesn't see how that phenomenon could be explained by the free
energy principle. Right. Again, a whole raft of really interesting points there. All of those things
can be explained by the free energy principle. And indeed, there's a small literature on all of
those things. If people are interested, you can get into the literature. I have to say,
because the literature may not be the familiar literature for four people in machine learning.
It's really, he's asking some fundamental questions about functional anatomy, probably more
specifically the computational anatomy that we can ascribe to various structures and hierarchies
and connectomes in the brain. So, for example, recent thinking about the cerebellum is that it
plays the role of actually a supervisor. So it may well be the case that the cerebellum
plays a role in the amortization of carefully acquired skilled movements that become increasingly
skilled as the cerebellum watches the cortex, and in particular the premotor cortex,
supplementary motor area, and the motor cortex per se, hierarchically exchange messages,
all prescribing those predictions for that reflexive kind of predictive cutting we're talking
about before that are delivered to the spinal cord and the motor reflex arcs.
But of course, that sort of low level description of motor control does rest upon this sort of
deeply structured and informed hierarchy of predicting what am I going to do next,
because the cerebellum can watch that. It can watch the cortex learning how to compose
particular movements, and if it can therefore learn how to do that, then it can amortize it.
So I'm assuming that we all know what amortization is, but what I mean here is that we can offset
or defer the cost of the Bayesian belief updating that we've talked about before,
entails a computational complexity and therefore also a thermodynamic cost
by actually hardwiring and learning the mappings from certain beliefs to certain sort of predictions
say that drive motor reflexes. In that sense, the cerebellum can, I think, be very usefully
understood in terms of being involved in supervised learning, but I'd actually turn it on its head
and basically say it's been supervised by the cortex, but in a way that allows the cerebellum
to tell the cortex, well, normally you do it like that. Why don't you just do it quickly
and efficiently and bake in those mappings without having to worry about the actual belief
updating and converging to dynamically on a gradient flow to converging to a free energy
minimum. And that story, I think, can also be used to explain the particular role of, say,
the dorsal and ventral striatum and the basal ganglia in arbitrating between whether we do a
sort of system one versus a system. Do we think about stuff? Do we do our belief updating and a
bit of planning as inference, or do we just do what we've always done, habitually respond quickly
and efficiently by harnessing something that has already been amortized? And that, I think,
is a really interesting interpretation of habitation versus deliberative thinking, which from
the point of view of a neuroscientist would be the equivalent of system one versus system two.
In that sort of edge between machine learning and reinforcement learning sometimes
refer to as model free versus model based. And I would not subscribe to that in the sense that
the habit has to be learned from the good old fashioned belief updating and learning.
You can't, you're not given habits, you're given reflexes, but to learn a habit, to learn,
this is the kind of thing that I usually do in this situation. And if I just do it, I can dispense
from the computational cost, thereby minimizing the pathological free energy by doing the habit.
I noticed I slipped in there that because it's a principle of least action, that entails a path
integral. So the amount of time it takes to minimize your free energy or get to your conclusion
matters, which means you have to do it quickly. To minimize the action, you've got to do it quickly,
which means there's a natural pressure to habitize and amortize in the service of minimizing free
energy. And I could go on in terms of telling you what I know about the functional anatomy of the
amygdala. But perhaps it's best just to say that there's a glorious game to be played here. In fact,
it's the bread and butter of a jobbing neuroscientist, certainly a systems neuroscientist, to understand
the computational architectures and the nature of message passing implied by either
belief updating or predictive coding or variational message passing, how it is enacted physiologically
on a neural anatomy that has a deep structure. So this is our job. And everybody started
scratching the surface in the past decades with brain imaging and other tools at our disposal.
This is very machine learning bias. I'm not sure if you're familiar with this.
Recently, there has been a paper that says that predictive coding
up can approximate back propagation in arbitrary graphs. Do you have any take on this?
Yes, it is. I'm sure a friend of mine has contributed to that.
Yes, no, absolutely. So by coincidence, I think we're having a
NeurIPS workshop on Saturday that sort of affords different perspectives on backprop.
I have to say, from my perspective, that's completely unsurprising. And if there's any sense
that prop has been demeaned as something which is not absolutely central to everything,
then I think that's probably wrong. So from my perspective, it is invariably the case
that the gradients with respect to anything of variation free energy can be written down as
a prediction error, which means that if you believe that the universe of interesting things
that basically is performing a gradient flow on a variational free energy functional,
then you also believe that they are via the chain rule minimising their prediction error.
So for me, backprop is just, if you like, a nice linearised chain rule-esque way statement of
things that work flow on free energy gradients that can always be written down with a back
propagation of prediction errors. It doesn't surprise me, the slightest that predictive coding
belongs to this class of algorithm as does back propagation of errors. And I would actually submit
everything that works should be described along those lines.
I think that's one of the first times I've heard neuroscience actually say something positive
about backprop. So that's really fascinating. So I have a second question and that is,
in your own personal opinion, what has been the most interesting progress in systems neuroscience
you've seen since you first proposed the free energy principle, whether it's directly related to
it or not? Standing back from a non-personal point of view, the whole pragmatic turn towards
embodiment that we've seen at the 10th century in cognitive science has really been a very big
thing that has been underwritten and also has been lent momentum by theoreticians like ourselves.
Within that, the role of predictive processing, that's become a meme now. So you can't write a
paper in cognitive neuroscience now without a nod to predictive processing at some level
of an inactive and embodied situated contextualized sort. So that's been a slow and really important
shift, which is presenting all sorts of really interesting issues. In terms of exciting developments
in the neurosciences, an increasing quest for understanding the nature of these epistemic
affordances which drive us, and in particular in terms of social neuroscience, eco niche construction,
communication with others, and how we forage for information. That seems to be in the context
in the context of usually social interactions, also touching upon an understanding of feelings
and emotion and how resolving uncertainty about bodily states in my world that will be
interceptive inference gets into this game as an integral part of these deep hierarchical
models that try to explain everything, what role do literally gut feelings have in my
propositional beliefs about where I am or who I am and how I should be behaving.
So challenging questions that are now starting to address key aspects of
intelligence from the point of view of neuroscience, such as the emergence of
selfhood and what underpins it and self in relation to others. So we're now talking about
cultural eco niche construction and multi-agent interactions. So that's been very exciting.
So I guess maybe to end the show, I'd like to ask you on potentially a controversial question,
I don't know. I've been trying to find your official answer on this and I haven't been
able to find it, but if we take for granted that the free energy principle explains a great deal
about how certain systems of the kind we've been elucidating will function, does it explain the
origin of those systems? So does the free energy principle, for example, have anything to say
about abiogenesis on earth, on the origin of life on earth, that first spark that created
systems which can then follow a trajectory guided by the free energy principle or is that outside of
its scope? I'm sorry to end on a deflation, but I'm afraid it's outside of its scope. I say that
sort of in a rigorous way because remember all of the interesting math and all this sort of
physics of sentience, all this Bayesian mechanics, all inherits from the assumption of a non-equilibrium
steady state that entails a Markov blanket. As such, it's only about steady states. It's not
where that steady state came from. So there are lots of really interesting questions. There's a
great paper by Kate Jeffries and there's lots of work by people at Los Alamos and around the world
trying to say, is it almost unavoidable that we will end up with these deeply structured,
delicate Markov blanketed systems that become increasingly rich and have longer and longer
information lengths and more itinerant dynamics as the universe unfolds? It would be great to
know the answer to that and to have a tractable mathematical formulation of that. I'm afraid
though that the free energy principle in its vanilla form as it currently stands does not
give you that. No, all it says is if that thing has evolved, these are the properties that it
must possess. At least it can tell us we should continue exploring and looking around to address
that uncertainty. Yes, of course. So, better free energy principle. Professor Friston, thank you
very much for joining us this evening. It's been an absolute honor to have you on the show.
It's been a wonderful talk with you. Thank you very much indeed.
Thank you so much. It's truly a pleasure.
Thank you very much. They're very informed questions. I got a sense you knew the answers
before you asked them. That's how I felt.
Look, I'm going to be honest with you. After talking to him myself, I think he has a very
intuitive understanding of very deep topics and a massive depth of knowledge. Therefore,
when he tries to communicate that, I think he just, what's the right way to say this? He leaves
behind a trail that's a little bit hard for us to follow because he's just moving at a very
kind of deep and fast rate, in my opinion. He was trying to help us understand. He operated
on such a high level that it's not always easy to follow. I tell him that that's just the result
of externalized intelligence. Oh, man. You might get your alpha constant.
I would have called it something like Kappa after my first name. Alpha's good. I mean,
start with the first one. Why do we need to go? Alpha's good. It's the backsally ambiguous
answer that still fits the data. Just hedge fund I worked at. We had a problem one day and lost a
couple million dollars or something, not me personally, but the fund. We called up one of
the guys who maintains our market data feeds and we're like, was the ticker that had the problem
today the NASDAQ ticker or the NICY ticker? He goes, yes. We're like, yes or no question.
So that was kind of an answer for a person there. It's like, do we need alpha? No. And yes.
Anyway, we really hope you've enjoyed the episode today. We've had so much fun making it.
Remember to like, comment, and subscribe. We love reading your comments and we'll see you back next week.
