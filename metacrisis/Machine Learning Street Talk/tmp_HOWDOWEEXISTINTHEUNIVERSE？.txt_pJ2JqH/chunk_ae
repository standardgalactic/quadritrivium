where it comes from an artificial neural net what is the what is the thing that you put that into
to make a decision and and and you can't I think consider you know all of us have an internal
feeling of how we think things work and how we think we work but we can't even other than by
extrapolation we can't even be sure that other people other different human minds have the same
point of view about what's going on and so I'm curious from the outside from a kind of pure
you know data science point of view I've got this bag of bits and it's doing certain things
how do I tell whether this bag of bits is meeting your criterion of having sort of a
counterfactual planning model of the world right and so much like the empirical problem and indeed
the hard problem of consciousness you can't the whole point of the persistence that we were talking
about before read as the persistence of of the boundary of the Markov blanket means you will
never know observing something what's going on behind the Markov blanket it's unknowable because
if it were noble or measurable you'd have to destroy the Markov blanket and the thing in
of itself would not exist and certainly if you wait a minute wait a minute I'm not I'm not assuming
that I let's take let's take the case that in an artificial neural net we can look at every bit
we we don't happen to be yet technologically able to do that for brains but we can imagine I think
a time in which we can non-destructively look at every bit of what's going on in the brain
and then the question is we're seeing this complicated bit pattern we're seeing in an
artificial neural net which is easy to measure we're seeing it in a brain which happens to be
technologically hard to measure right now and then we're asking the question is this pattern of bits
that we're seeing an example of something which is a plausible sort of self-reflecting or you know
counterfactual generating modeling the world internally kind of thing or is it just a bag of
bits that follow certain rules well I mean I think I think my answer would be the same that you will
you will never know you but you could certainly infer that I mean I'm smiling because you're
just describing my day job again my entire life is being built around getting beneath the blanket
using things like brain imaging usually not invasive but sometimes invasive when you have to
implant electrodes you have safe deep brain stimulation like and and and measuring the
internal dynamics the bits the patterns and the computational architectures implicit in the in
the wiring and the and the the neuroanatomy and the connectome and then making a best guess and
trying to infer is this the kind of message passing or pattern or updating or dynamics or
architecture that could be explained in terms of a evidence maximizing free energy minimizing
belief updating process under some kind of generative model and if so what kind of generative
model is you know we get quite a long way with that I mean one of my favorite examples is you know
brains like ours have distilled a fundamental conditional independence between
whatness and awareness into their anatomy so you're the top half of the brain does
all the awareness and does all the the space or metric space your specific pointing activity or
looking activity whereas the bottom streams of the and this is the inferior temporal lobe or the
temporal lobe are much more concerned with the whatness of things and so that you know there
are certain and architectural features of both the dynamics and the the connected sparse to the
coupling that we were just talking about before which give you strong clues as to the kind of
generative model are this kind of person this kind of artifact clearly has a generative model
in which her lived world entails things that are objects that might exist but knowing what
something it doesn't tell you where it is and vice versa and immediately you think well that's
very consistent with my lived world this metric world full of objects your physical or visual
objects that can be in different places speaking to your your very interesting example before about
you know if I move something is it the same thing well no it's clearly not the same thing but
you know if you can sort of coarse grain and factorize the world into whatness and awareness and
have some equivalence or equivalence so well actually this is like the same whatness but just
now in a different awareness so you've partitions statistically in your model and the physical
instantiation of that model that that sort of statistical regularity that thing that persists
even if the molecules of the object don't persist but the regularity the pattern the coarse-grained
explanation for for for the sensory impressions generated by that thing you put that into the
anatomy so you can you pursue that so what would you be looking for if you've got some
kind of agent that has beliefs about the future and then beliefs about itself having beliefs
about the future which I think is what you're implying by the self-modeling and you be looking
for a hierarchical structure you'd be looking for certain Markov blankets or conditional
independences instantiated in the absence of literally neural connections or wires on a bus
literally a sparse coupling that had a hierarchical aspect and then you'd be looking for the
the correlates of the message passing the amount of electricity used for beyond that particular
risk so your view is that it's kind of a a series of the watcher watching the watcher
type thing and that in some sense I mean perhaps your view is that sort of the the higher
cognitive function is literally a you know a series of hierarchical levels where you're
sort of progressively abstracting from the world that is at some lowest level you've got the world
as it is and all those you know all those photons coming in all those neuron firings happening
at the first level and then that you're somehow progressively abstracting compressing reducing
that to eventually come to presumably you know to the to the point where you make a decision
should I do this or should I do that lots of lots of sensory input has come in and you're
kind of filtering it down so I'm curious if you look at you know other animals not humans
um do you imagine that it's the same kind of I mean I mean it's an interesting and and quite
kind of I would say sort of it's a it's a very definite view of of what it means to be a thinking
thing would be that you're always taking all the details of the world and always trying to
crush it down to the point where you make a decision about what to do next and that that
would be a thing where you could imagine observing that there's all that compression happening to
decide what to do next now I will say that I don't think that's specific to brains I mean imagine
that you've got a rock it's perched on some you know uh hilltop or something and lots of things
happen to the rock there's rain there's wind there's all kinds of things and eventually one day the
rock starts rolling down the hill in other words from all of those details from all those details
about what the rocks the environment that the rock was exposed to eventually it makes a definitive
decision and I'm curious what the distinction is between that kind of sort of many things happen
and you know the many things have to take place you know the rock is eroded away and this happens
and that happens and it's a whole hierarchy of actions and then eventually one day the rock rolls
down the hill so how would you distinguish that from from kind of your view of kind of the
the sort of hierarchy of action in brains I mean other than that in that particular case the timescale
might be years in the case of the rock and the timescale for brains might be seconds or something
yes that's a great question a great challenge um so but just to pick up on that that notion
of of timescale um as soon as you have a hierarchical um message passing scheme or dynamics or or
physics of the kind that I think we're talking about here there's a separation of timescales
as you get hierarchically more abstract or deeper if you like um into your observer um
so part of the hierarchy will almost inevitably entail a um the kind of temper persistence that
you spoke about earlier in terms of the observer in reading her world as things that endure over
time including me uh including out your ourselves um that is a gift of the dynamics that usually
supervene at the deepest hierarchical levels of any you imagine say centripetal kind of hierarchy
so I think that's an important observation that you know you can't sort of commit to a single
time frame to understand the contextualization of fast-moving high-dimensional content in the
course of the course-graining that there's also a course-graining over time which we call context
for example um so that would be one important aspect when trying to understand um could you
call a rock eventually being knocked from its perch as a decision I would say probably not
because you haven't got the you know the sort of the you know the scaling brand over time um it's
certainly an event but you'd hardly call it and you know you could hardly um call this uh any kind
of something that it persists over time you know from my point of view it certainly would not be an
expression of a pullback attractor if the rock however um fell down the hill and started walking
back up again that kind of behavior would suggest to me ah there's something going on of interest
under the hood and I when I cracked the rock I'd expect to see some hierarchical architecture
and some itinerant dynamics that would support that kind of separation separation of temporal
scales but before I sliced open the rock or put you in a brain imager I wouldn't know you know I just
have to guess based upon your behavior your active state so your the the the boundary that separates
what's going on in your inside from from me as part of your your outside right but I mean the
problem is this is uh the well one will never know is a reasonable thing to say on the other
hand if one's trying to make a distinction between observer like things well this there's a couple
of levels of distinction here there's the distinction between just everything and things
and then things and observer like things and I think the um and I think several of these
distinctions are kind of complicated and I'm I'm I'm you know in order to have something which seems
like a a kind of a a scientifically progressing kind of set of ideas it seems like one really has
to be able to make a a definite distinction between sort of the you know how do I tell from the bag of
bits that I have a an observer like thing as opposed to just a thing and to to kind of make
that a bit more technological uh you know we've got all these AI systems and we've got everybody
talking about whatever they might mean by AGI which I think is a kind of foolish concept but
that's a different issue um and uh uh you know should one imagine that uh the to make a more
sort of human agency like AI that we necessarily have to have some notion of hierarchy I mean
you know in in your average LLM there is in a sense one level of hierarchy in the sense there's
the feed forward of what the LLM does when it reaches you know when it tries to figure out the
next token and there's the kind of you know big feedback loop of uh you know seeing the previous
tokens and then deciding what to and then uh sort of iterating uh to to to to figure out what the
next token should be so I'm curious whether you see whether you think that it's sort of a fundamental
idea of the way that things like us work that there's a deeper hierarchy and if you think that
that has implications for the sort of the technology very high yes I do I'm very aware you're being
very courteous you're asking me questions all the time which is uh very generous of you uh but yes
I think that that that that hierarchical structure that I'm sorry I'm just you see I'm I'm trying to
live some version of your principle or not as the case may be that I'm I'm doing a you know
seeking the unknown type thing yeah but Keith and I please go ahead well Keith and Tim promised me
I'd learn all about the rouliand which I would I would like to we won't have time now but I would
like to have pressed you on that because you know I find that quite quite intriguing but we've we've
now gone into um um sort of uh you know large language models and the kind of dynamics I think
I think that's absolutely right I think I you know I if I want let me put it this way if I knew
what the generative model was I could actually just simulate belief updating under that generative
model I could basically just integrate under the the principle at least action a free engine
minimizing device and it would look as if it was behaving intentionally and planning and in a base
optimal way it would also do a base optimal KL control it would do everything you wanted
if I could write down the generative model but to write down the generative model I have to know
exactly what kind of world what kind of user what kind of ecosystem is this particular computer
designed for um and furthermore I'd have to be able to explain it you know so immediately you've
got if you give me a computer if you give me a you know a piece of artificial intelligence
that is completely explainable and by which I mean you can write down the generative model
then I will understand this then yes I could tell you exactly this is a stone this is self-aware
this is not self-aware but it hasn't you know it still has a kind of agency of a
perspective so this thing's like a thermostat this thing you know but wait a minute what once
you say you have a model that says everything about what it does the thing has no chance to have
something that anybody would consider to be something like free will because you're basically
saying you're you're you're stipulating that the thing is just acts in a predictable way
yeah I think maybe I'm gonna jump in here and say I don't I don't think you need free will in
order to have agency so I think if I even if I had a machine that was taking in inputs and
conducting an analysis and as a result of that analysis deciding to raise a red flag or you know
walk up a hill I still think that's agency even if tell me what you mean by agency
what yeah so what I mean by that is is literally that it's performing
a computation about sensory inputs and as a result of that computation deciding some
some output but so any old classifier any old machine learning classifier does that
doesn't it's not performing it's not performing an action though right but it could be if you
connected to a robot or something it can perform plenty of actions and as soon as you do then it
then it becomes and then it becomes an agent you know acting acting in the world okay so I mean
the you know image identifying cat flap that decides is it a cat or not that cat flap has
agency in your definition I would I would say it does it has a very simple form of agency but I
think that that's so so your definition of agency is that there is there are a large number of
possible inputs you are sort of you know whittling that down to something about which
a decision can be made and then you are actuating in the world perhaps so for example does it have
agency in some in some environment yeah okay does it have agency if the only thing it does
is to change a bit somewhere inside itself in other words that bit might be the bit
that is on some iobus that causes it to actuate the cat flap but that bit might be at the beginning
just an internal bit inside in a sense the brain of the AI so how would you distinguish those two
things you say it has to have that flow out to the outside world in order to be validly an agent
or to have agency or can it have agency with respect to so for example can there be a notion
of agency if the end result of your thinking is that you think a particular thought is that
enough yeah or does agency require actuation in the real world yeah so I think the way I the way
I would frame it and look this is to both of you to to criticize it or not but the way I would frame
it is if I'm affecting a change in the same environment that I'm perceiving then I'm an agent
so it's like if I'm if I'm taking an inputs from environment a okay and then affecting a change in
environment b and a and b are totally decoupled then I wouldn't call that thing an agent that's
something like a bridge between two different two different environments but as soon as I can
enact changes in the same environment that I'm perceiving I think that that's ordinarily what
what a person would perceive as being an agent you know it could be very simple that a roach
a roach is an agent you know maybe it has a very simple neural circuitry but that assumes that
there's sort of an external environment and I guess this relates to the whole Markov blanket
business of whether you can distinguish the outside environment sparsely connected to your
internal states or whether you are you're merely affecting so you're saying if you've got agency
so let me see if I understand this there's sort of a sparse connection from the outside world
coming in and that is you know there's a lot of detail on the outside world there are sparse
connections coming into you and you're saying you have agency if you can back propagate so to speak
not in the sense of back propagation but if you can if you can go back out through those sparse
connections to affect that big complicated outside world then you say you have agency that it requires
this sort of two-way thing that you're both bringing it in through that sparse connection
and pushing it out through this sparse connection is that correct Carl I'm curious whether that's
whether you would agree disagree change that um that that definition yeah I was just relaxing see
the exchange um yeah I think you'd have to acknowledge that there's a I think two distinct
use of the words agency you know I you know I can certainly argue for Keith's position that simply
being able to couple back and complete that sort of circular causality if I was you're talking to
neurobiologists this would be the action perception cycle just being able to act upon the world
is one very simple definition of an agent and that and that view of thermostat would be
would be you know a kind of agent but the other the other sense I think we've we've been using it
as in a slightly more nuanced sense which is the the ability to actively select a particular course
of action from a number of counterfactual alternatives and that requires a much more
expressive kind of generative model so you know I I think that I'm not sure whether the bright lines
you could argue that they're sort of vague or graded distinctions with one way of thinking
