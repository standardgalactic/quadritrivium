about this is how far into the future do you take your planning so if I'm a thermostat I can just do
that on sort of you know first and second temporal derivatives I can sort of you know like a common
filter all right sorry um like the kind of linear quadratic control that you would get if you
interpret the control or something PID perfect just a few derivatives in the moment um that will give
me um the kind of agency that that is very reflexive and autonomic and automatic and the
kind of thing that we you know you could that could in principle be found in autopilots and
thermostats and you know many things that we use around the house um you could argue that just by
having those um those state estimates and their higher derivatives that there is a sort of future
pointing aspect because you can just sort of project out the path you know you lose uncertainty
very quickly but you can certainly sort of think of those um those higher derivatives of
so the coefficients of a Taylor expansion of a little path into the future but so think you know
what you know at what point though do you do do you go into the future um sufficiently far that
you can call it a plan so what point and perhaps there isn't a hard answer perhaps it's just a
great it's just a question of how tall are you how deep is your temporal horizon um and how
you know deep can you infer it given this anatomy given the structure of the thermostat and the
or the um the time rollouts you know in say um you know all right so I'm curious if you've got
your average AI LLM whatever else and I go and I can probe that system what do I look for to look
for planning and counterfactual the construction of counterfactuals in other words I've got I've
read out a trillion bits from my LLM in action how can I tell if it's thinking counterfactually
right first of all you could just you know do do what a you know any life scientist will do which
is basically either kill it and dissect it and look at it look at its anatomy or ping it in
some way and try to infer its causal architecture or let it run in its natural's environment and
then look at for correlations measuring the functional connect the other do structural
equation model or or what I mean in the case of an LLM you know it's just a bag of bits on my
computer I know what every bit is there's no sort of you know inference by no complicated
inference I know the bits the question is even given those bits you know at the lowest level of
bits I know what it is if you ask me at a coarser level what is it thinking for example I probably
don't have a clue because I don't have a way to uh you know I don't have an you know it seems like
one of the key problems of neuroscience we don't have a way to describe and perhaps this is what
you're working towards you know we don't have a way to describe how brains work at an intermediate
level between kind of the neuron firings and the words that get spoken by the brain type thing in
other words that that um and so in the case of you know the AI the LLM whatever else I'm curious
because that's a case where we don't have any limitation on the underlying data we know what
every bit is and so now my question is can I run what test would I run what kind of correlation
would I measure what kind of what would I do to say hey you're a counterfactually thinking AI
and this one over here you know you could have a a counterfactuality index or something for different
LLMs of how counterfactually can they think how would you measure that I'm sorry I keep on asking
questions but I'm just I'm so curious well I don't know if this is related Carl but I know
you had a recent nature paper I believe it was about rat cortical neurons where you know there
was there was some quantitative analysis by which you determined that it was performing you know
Bayesian um or active inference I don't know if it's it's related and how you quantify that but
that could be a an avenue that's very smart to you because I was actually just thinking about
should I introduce that as an example of how you practically do this you basically reverse engineer
the generative model that could account for this these bits and the anatomy upon which these bits
play that's a very difficult game though unless you've got very very basically what you're saying
so essentially what you're saying there is there is an underlying thing that the LLM or whatever
does and you know all its bits yeah now you try and yourself construct another model like you might
use another LLM to try to construct a model for the first LLM that is explainable it has to be
explainable that that's the other the other one's explainable yeah right I mean that the problem is
I understand that science tends to be about making an explainable narrative for how things work in
the world the problem is I think that that's at odds with some of the things that you will need
to have something where you can genuinely say that has a model of itself because I think that
to have in a sense the itself is doing things which are not explainable at that level if everything
that we did was fully explainable I don't think we would ever consider ourselves to for example have
choice or free will otherwise if we could always explain I don't think that's to be fully explainable
though because there is there is the uncertainty baked into at least the free energy principle so
if I have a generative model of my own actions in the world there's quite a bit of uncertainty
and so I'm computationally bounded maybe I carve off 20% of my computational resources to that to
that activity but it's not going to be a it won't have perfect fidelity okay okay so so then the
question then that I would ask is okay you're saying that even though it's not going to get it
right all the time you're saying that that even getting it right some of the time is and having
kind of a a self model that is an approximation because because you know a very simple approximation
is just to say it's going to generate another token that's a very coarse approximation you're
saying you want a finer approximation but not so fine an approximation that you really actually
capture all of the dynamics it's some intermediate level of sort of coarse-grained approximation
that is sort of you know good enough but not too good I mean I I do think these distinctions are
I mean I think some of these distinctions are sort of core to the notion of what observers like us
can be like and I think it's interesting to try to home in on you know just what is the right level
here because I my own guess is that some of what we perceive about the universe so let me give an
extreme example it'll be interesting to figure out whether the fact that we perceive space to be
roughly three-dimensional is a consequence of some aspect of us as observers it's not obvious to me
I think it's possible that that's the case and in other words that that we could perfectly well
describe the universe as being one-dimensional where you know we have to chase along all these
worms that sort of you know navigate through space so to speak there's sort of space filling curves
arranged through space but that will be a very weird and inefficient way for us for observers like us
to describe the universe and so my my you know this is why I'm curious about kind of what is the
nature of observers because I think that the nature of observers once we understand it better
we're going to say why do we see the universe the way we see it and the answer is going to be
because we happen to be observers like we are just as we can say why do we see the night sky
that we see well it's because we happen to be at this particular place in this particular galaxy
and so on so well this you know this maybe in the spirit of asking you a bit about the ruley ad
is please go ahead and I think this links back because you asked a couple times you know can
you imagine a universe in which it's just not possible for there to be persistent observers
like us that are able to survive because there's not enough you know predictability and I kept
thinking you might be in a very good position to answer that question because one takeaway I took
from a lot of your work on computation and the cellular automaton really a new kind of science
is that look of all the possible calculations out there most of them are pretty boring they're
either very random or just some boring you know line that goes on forever there's this tiny tiny
sliver of computations that have this this chaos this this advanced nonlinear kind of complexity
you know so I'm curious when you're when you're building the ruley ad and you start off with
some type of substrate some type of hypergraph and there's a set of rules from which you know the
laws of physics emerge you know how much fine-tuning has to go in there like of all the possible rules
which what percentage is it a small sliver that produce interesting behavior and what's your
intuition tell you about of that sliver that produces interesting behavior what fraction of
that would allow for observers like us right well so everything happens in the ruley ad so it's not
we don't get to choose or fine-tune anything about the ruley ad itself what we do get to choose and
fine-tune is what part of it we are sampling now when I say choose it's like there is an entity
that is sampling every possible part of it but an entity like us is that there are aspects of
what's going on that we sample now when you say things that are interesting that's a deeply sort
of human-centric thing to say because what's interesting to us is a certain set of things that
well what's interesting to us is what's interesting to us I don't think there's a try as we might
with I don't know you know any kind of you know information theoretic you know algorithmic
information theoretic you know all of these things in the end I think devolve into well it depends
on how you do the coarse-graining it depends on what you think the underlying generative model is
etc etc etc and all that reflects right back on us in other words I don't think there's an absolute
notion of what's you know what's interesting and what's not you know when you talk about compression
for example this question of compression at least is always it's like it's a question of well what's
the machine that is doing the decompression if you say you know you can have if if you're doing
something like you know traditional you know Shannon style information theory you're you're usually
using things like block coding and so on where it's a very definite simple model of what the
compression decompression process can be it's some so I think you know I think this question of
what's interesting it turns right back on us I don't think there's an absolute answer to that
and you know when you ask well what about all the all the alien intelligences that aren't like us
well most of those alien intelligences will be so different from us that we won't recognize them
we won't you know our ai's are perhaps our first really good example of an alien intelligence
that by design we arrange to be quite aligned with us I mean we don't even get that for your
average whale or something we don't know how you know we can't really understand its intelligence
because it wasn't built to be aligned with us so to speak our ai's are built to be aligned with us
so we get to have a much more sort of I think you know it's much more plausible to have sort of a
a communication with that sort of human aligned intelligence or human aligned kind of set of
things that are going on that there's there's a lot of interesting stuff to discuss but unfortunately
I am quite running out of time here so so even though I this seems very unfair because I've I've
been been uh would you like to come back for around 2.0 or were we uh commit or we commit to
exploring more about the rolyard yeah right well I'd be um but but maybe I mean if we have just a
a moment more I mean Carl if you if you uh it sounded like you had sort of a pent-up
set of questions here and I'd be um uh maybe at least a preview of the of the pent-up questions
or something would be it would be interesting okay very briefly because I appreciate you've
got the rest of your day to to to live out in California which is where I presume you are
oh I'm I'm in I'm in the east coast actually so I'm all right three hours okay but I'm but I'm a
late scheduled guy so so I was just really interested in um you're listening to to you
um talk about the rolyard and thinking about us as observers as part or a slice or a slither of
of this um whether there will be any mileage in using the notion um of bounded computation
read as a a mark-off boundary or a mark-off blanket based upon the implicit what I imagine
is some kind of um adjacency matrix that inherits from the sort of hyper-graphical construction of
the rolyard so as I was wondering if there's any way of of sort of um thinking about identifying
certain patterns that do the sort of diverging emerging again and therefore give the impression
at a certain degree of course graining of persistence in the in the sense of there being a
you know like a pullback attractor an attracting set of arrangements and that that could be
operationally defined with a mark-off blanket which is trivial today once you got the adjacency
matrix you can easily identify the mark-off blanket of any given set of states and then you
could call those an observer so I just wanted to know whether that real simple really simply
simple minded view of the rolyard as basically being whose anatomy was described by some neighborhood
relationships at a very very small scale um was was an apt starting point for understanding the
nature of the rolyard right right right so I mean essentially what you're saying is can you separate
off things objects that have sort of sparse connection to other parts of the system yes it's
unfortunately more complicated than that okay so an example of something where you can separate off
aspects of it are black holes where because what makes even the structure of space is there isn't
any notion of space until you have activity in space so you're building up this causal graph
of all the different events that are happening and in order to even have a notion of space
you need a kind of kind of whole ocean of those events that that's that's what makes space so in
a sense there is necessarily connection of some kind between everything in space by virtue of those
many events having happened the only place where you can sort of separate that off is when you have
an event horizon and where you can sort of say no there's really no dependence from this to that
or at least no dependence uh when you when you're not dealing with multi-way systems and quantum
mechanics and so on so I don't think it's quite as simple as that I mean is there a way to kind of
understand the statistics of the Rulliard and ask not whether there is any correlation there certainly
is a correlation because otherwise space wouldn't hang together but whether there is a significant
correlation a correlation at some coarse-grained level I think that's what you have to go for
and that's again just technically vastly more complicated because you have to define
what's a useful coarse-graining procedure the very definition of what a useful coarse-graining
procedure is is going to lead you into questions of observers and so I don't think it's a it's a
vastly more complicated at a technical level it's a vastly more complicated kind of question
but it's I think that's a very reasonable thing to to be going for it just isn't easy to achieve
except in these rather straightforward very space-time oriented cases like event horizons
but I think that that's uh you know I I would love to have a a good characterization of you
know I've got a piece of Rulliard and I can view this piece as being an observer like peace as
opposed to another piece that would be a lovely thing to be able to do um I don't know how to do
that and in a sense in a sense that was what I was pushing you on but the difference between a
you know rock and a brain is is asking for that same kind of thing I think it is yet more difficult
in the case of the Rulliard just because it builds up everything and so you you kind of
you don't get so one of the things to understand and this relates to thinginess and mock-up
blankets and so on perhaps is this notion that is very critical to science as we have as we've
pursued science so far that you can have isolated things in the world that you can have an experiment
you do where you say this is an experiment it's on this thing and nothing else in the world affects
the thing I'm looking at that's a you know that notion of factorizability about the world is something
that I think and I think that's what you're going for with this kind of mock-up blanket idea if I'm
understanding correctly and that notion of factorizability I think in some fundamental sense what
the Rulliard tells us is there isn't ultimate factorizability there never will be the only question
is for the things that we choose to look at is there some effective theory that we can think of
as factorizable at least enough that we can have sort of definite thoughts without always being
affected by other things in that happen in the world and I you know I've well it's interesting
more to think about here I like your I think this notion of observers who kind of actively
pick for themselves the path of predictability I think is interesting I still feel like I need
to untangle that idea a little bit more I feel like there's a there's a bit more and you've
kind of indicated that you see there as being some tautological character to it I kind of feel like
it has a a I I would suspect that there is something which you are something interesting
which you're effectively taking as axiomatic about observers and perhaps perhaps that I mean in a sense
one of the things I'm trying to do right now is to think about what aspects of observers
are in fact there but we've always thought they were obvious in other words the idea
that there could be pure motion you know that's always seemed completely self-evident and obvious
but yet I think it's something one has to derive and so I'm sort of curious about as you
peel back you know what things are there about observers that have always seemed totally obvious
to us but in fact it might not be that way and and trying to understand that and that's
well that's kind of a yeah before you could have pure emotion I guess you even have to have a thing
that that's moving right and it sounds like in the context of the rule we add do we even have a
mechanism of identifying persistent things or not I mean is there well is yeah it's a bit
coarse right now I mean we can identify things like black holes because they have a very definite
structure in their causal graphs right to identify things like electrons is something we are not yet
able to do although I have a this very strong suspicion that which was even really highlighted
by this video that that we just made electrons and black holes are not that different I think
