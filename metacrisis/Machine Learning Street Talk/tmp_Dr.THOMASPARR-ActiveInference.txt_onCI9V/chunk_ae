that could go either way, or a common cause of both things that depends upon something else.
And typically the way you do that is you intervene on the system. You randomize at the beginning
to make sure that people are assigned to different treatment groups at random,
so that you break that dependency upon something prior to it. And then anything that happens going
forward is going to depend on the intervention that you're doing. So I think that's probably the
key thing that gives you causality or perhaps defines causality. It's the idea that an intervention
is what will change it. If you intervene in one thing, that should then in a way that doesn't
necessarily match its natural distribution if you hadn't intervened at all, and then see what the
effect is. Yes, yes. I mean, and by the way, Judea Pearl is really interesting. I want to study
his book, The Book of Why. It's one thing that we've really dropped the ball on, actually.
But I suppose one way to think about it is if you go back to the core physical, in physics,
there's a whole bunch of equations to describe the world we live in. And those equations don't
have, they don't say anything about causality, and they're even reversible. And then you can think,
okay, well, maybe it's a little bit like the free energy principle. It's a lens,
like really, there's only dynamics. But when you look at these dynamical systems,
then behaviors emerge, and somewhere up that chain, you can say, okay, now we've got causality,
and it's something which is statistically efficacious to build it into our models. But
where does it come from? Well, it comes from us, doesn't it? It's a hypothesis to explain a particular
pattern of dynamic. Yes. And we might infer causation based upon, again, a particular pattern
of how one thing reacts to another. So if you imagine you've got the classic physics example,
billiard balls bouncing into one another, how do you know that the collision of one ball with
another is causative of the subsequent motion of the second ball? And you could argue that that's
due to a particular pattern of which variables affect which other variables and the particular
exchange between them. And this comes back quite nicely to things like the physics perspective
on the free energy principle, the idea that actually one could see the location of a particular
ball as being, you know, maybe it's internal state, and then the action that that then causes
is perhaps the, or in fact, you could say that the action is the position of the ball, the force
that results from that action is the sensory state of the next ball, which then changes its
velocity to then change its action relative to something else. You can sort of rearrange those
labels slightly, but there is a directional element to it. And in that sort of pattern of
causation, you really do expect the position of one ball to have an effect on the rate of change,
or in fact, even the rate of rate of change of the second ball, which again, I think brings us
back to those kinds of dynamical descriptions of causality where one thing might affect how
another thing changes. So you almost get it from the dynamics itself. But again, to some extent,
it comes back to semantics, doesn't it? It comes back to what do we mean by cause? Well, I suppose
cause is a hypothesis as to a particular configuration of things. But then you've got to
write down what does that hypothesis mean? What's my model of what a causation involves?
Yes, yes. I mean, we were just talking about, you know, build building these models. And one of
the bright differences from machine learning is that we need to build a generative model by hand.
So we have to define these these variables, and some of them are presumably observed, and some of
them are not observed. They're inferred. And that process seems like you would need to have a lot
of domain expertise. And it seems like something which is at least has a degree of subjectivity.
I mean, we were just talking about causality, for example, there are many ways you could model
the risk of cancer from smoking. It seems like there are many, many different ways of building
those models. So that subjectivity is interesting. I mean, are there principled ways of building
these models? Yes. And in a sense, it all comes back to the same thing again, it comes back to
which model minimizes the surprise the best. And but there are interesting questions amongst that.
So how do you actually choose the space of models that you want to compare? So you're right to say
that that that often there is some specific prior information that's put into models and active
inference. And very often we do end up sort of building models by hand to demonstrate a specific
outcome or a specific cognitive function. But there's no reason why it has to be that way.
You can build models through exposure to data, where where the models are selecting the data to
best build themselves. But the question is how you do that, how you start to add on additional
things, how you start to change the structure of your model. But there's a lot of ongoing research
into that. And I think there are now methods that are coming out that will allow you to allow an
active inference model to build itself. And the way it will do that will be sort of adding on
additional states and potential causes, adjusting beliefs about the mappings and the distributions
and the parameters of given this than that, adding an additional paths that different
or different transitions that systems will pursue. So it's a fascinating area. I think
it's one that's still a growing area. But it's this idea of structure learning of comparing
each alternative model based upon its free energy or model evidence or surprise as a way of
minimizing that by being able to better predict things.
Yeah, I mean, that's something that we humans, we seem to do really well. So we can, first of all,
via abduction, we can select relevant models to explain behavior, you know, what we observe.
But we also have the ability to create models. In fact, I think of intelligence as the ability
to create models. So we experience something. And I now construct a model to explain this
and similar experiences in experience space. But in a machine, it's really difficult. So in
machine learning, there's this bias variance trade off. So we deliberately reduce the size
of the approximation space to make it computationally tractable. And when we're talking about
building these models, just from observational data, it feels like there's an exponential
blow up of possible models. So I can imagine there might be a whole bunch of heuristics around
library learning or having modules. So these modules have worked well over there. So we'll
try composing together known modules rather than starting from scratch every single time. I mean,
what kind of work is being done there? I mean, I think I think you're right about, you know,
it's not going to be worth starting from scratch every time. You can sort of build models by saying,
okay, let's start with something very simple with a sort of known structure. And I think it's
sensible to use some priors in that rather than starting from complete, completely nothing,
because there are some things that we know about in the world. And there's no point hiding that
from the models we're trying to build. And that might be a simple structural thing like things
evolve in time. So one thing is conditioned upon the next is conditioned upon the next.
And things now will influence the data I observe things well in the past might not anymore.
But then then there's the question of, well, how can a model then grow? What are the things that
you can add to it or subtract from it? And subtraction is another key element. Because you
could take this whole problem from the other direction, and you could say, well, let's start
with a model that just has everything in it and take away bits until we've got the model that's
relevant to where we are at the moment. And we know that during development, there's a lot of
synaptic pruning that goes on and removal of synapses that we have when we're much younger
compared to compared to as you get older. So what can you add on? Well, it depends what your model
looks like. So if your model says there's a set of states that can evolve over time, there are
a set of outcomes that are generated, well, we know what the outcomes are, we know what the
data are, because we know what our sensory organs are. So it's the states that are going to change
so do we add in more states? Do we allow them to take more alternative values? Do we allow
their transitions to change in more than one different way? Which ones can I change? Which
ones can I not change? And it's really just asking these questions that helps you to grow your model.
So you say, well, let's try it. If I allow this state to take additional values, if it's not
providing a sufficiently good explanation for how things are at the moment. And if that improves
your prediction, that's good and you keep it and if it doesn't, then you get rid of it. Do I now need
to include additional state factors? So you could either say there is one sort of state of the world
that can take multiple different values, or you could actually this is contextualized by something
completely separate. So where am I along an x coordinate? You also need to know where you are
along y coordinate to be able to contextualize what you're predicting. So it's just asking what is in
a model? How do you build a model almost gives you the answers to the ways or the directions in
which you can grow it. The other thing you can then do when you're trying to work out how to grow it
is to say, well, let's treat this as the same sort of problem as exploring my world,
selecting actions that will then give me more information about the world. You could say,
well, actually, now let's treat my exploration of model space as being a similar process of
exploration. Which of these possible adjustments to my model might lead to a less ambiguous mapping
between what I'm predicting or what's in my world and what I'm currently predicting?
Yes, it rather brings me back to our comments about the space or the manifold that the models
sit on, whether they would have a kind of contiguity or whether they would have a gradient.
I guess I'm imagining a kind of topological space that the models would sit on. I don't know whether
it's worth bringing in. Obviously, you're a neuroscientist and the way brains work,
we must do this. Of course, there's this idea of nativism. Some psychologists think that we have
these models built in from birth and then the other school of thought is that we're just a
complete blank slate. If you read Jeff Hawkins, he talks about the neocortex as this magical
thing that just builds models on the fly. But perhaps one difference at least between brains
and machines is the multi-modality, which is to say we have so many different senses that
creates a gradient or that makes it tractable. Because when a model from a particular sensation
and starts predicting well, we can rapidly optimise and go in the right direction.
Because the problem seems to be that there are so many directions where we can go in,
doing some kind of monotonic gradient optimisation will often lead us into the wrong part of the
search space, so we've wasted our time. Yeah, I think that's a really good point,
absolutely. As soon as you know how one thing works or how vision works, I suppose vision
and proprioception is a good example, isn't it? If I recognise where my hand is and I can
make a good estimate of that visually, then that helps me tune my joint position sense as to where
my arm might be. And it's always fascinating to see situations where that breaks down, so there
are a number of conditions where if you lose your joint position sense, you're perfectly okay holding
your arm out like that until you close your eyes, at which point you start getting all these interesting
twitches and changes. So yes, the multimodality I think probably is a really key thing that really
does help constrain the other senses because you're just getting more information about each thing.
Maybe we should just talk about chapter 10 in general, because that was kind of like the
homecoming chapter, if you like sort of bringing together some of the ideas. So can you sketch
that out for me? Yeah, so I think towards the end of the book, the idea was to try and bring together
a lot of the themes that had been discussed earlier on, but to also make the point that,
well, I'll come back to one of the things you said earlier was about how it seems we're talking about
lots of different things from different perspectives, but actually they're really the same thing.
So we talked about how surprise is also a measure of steady state of energies of various sorts of
of statistics and model comparison of homeostatic set points, you know, that all of these things
can be seen through the same lens. But again, taking one of those inversions, you can invert
that lens and say, well, actually, you can start from the same thing and now project back into
all of these different fields. And I think that's a useful thing to do because I think it helps foster
multidisciplinary work, helps to engage people from different fields and areas,
and helps us know what's happening elsewhere so that you're not just duplicating everything that
people have already done. So I think it's really important to have those connections to different
areas. And the chapter 10 from the book was an aim to try and connect to those different areas,
whether it be to things you've spoken about, like cybernetics and inactivism,
and just to try and understand the relationship between each of them.
Well, I mean, quite a lot of people use this as a model of, you know, just things like
sentience and consciousness in general. And I often speak about the strange bedfellows of
the free energy principle. So, you know, there are, you know, autopoietic and activists and
phenomenologists and, you know, people talking about sentience and consciousness, you know,
obviously you're a clinician, you know, you're working in a hospital. So it's just this
incredible conflation of different people together, and they all bring their own lexicon with them.
But maybe we should just get on to this kind of sentience and consciousness thing, because that
seems quite mysterious. We almost come back to one of the themes we've spoken about a few times,
which is that the specific words we use for things in the effect that different people,
that has on different people. So some people, I think, would probably get very angry with the idea
of using sentience to describe some of the sort of simulations and models that we would develop.
But that comes down to what you mean by sentience. And I think one of the key things for sentience
is the aboutness we were talking about before. The idea that our brains or any sentient system
really is trying to try not to anthropomorphise too much, but it's almost impossible to do in
this setting, isn't it? Not trying to, but that the dynamics of some system internally to the system
are reflective of what's going on external to it, and that you can now start to see those dynamics
as being optimization of beliefs. And those beliefs are about what's happening in the outside world
and about how I'm affecting the outside world. And I think that probably gets to the root of
at least a definition of sentience and one that I'd be happy with, which is just the
dynamics of beliefs about what's external to us and how we want to change it.
And there are very few things other than that sort of inferential formalism that give you that.
Yes, I mean, in a way, one thing I like about it is, I mean, we are talking as physicists,
so we are materialists. It's very no-nonsense. It's quite reductive as well, because there are
those who believe that these kind of qualities that we're speaking about, certainly with
conscious experience, for example, that it's not reducible to these kind of simple explanations
that we're talking about, that it has a different character. David Chalmers talks about a philosophical
zombie. So for example, you might behave just like a real human being, but you could be divorced of
conscious experience. So he says that you can think of behavior, dynamics, and function,
and conscious experience as something entirely different. But as an observer, you would never
know. So yeah, it feels very no-nonsense, doesn't it? But that wouldn't be satisfying to a lot of
people. No, it probably wouldn't. You're right. Yeah, and particularly when you get onto questions
like consciousness as well, I mean, I think it does become very, very difficult, because once
you're putting forward or advocating a theoretical framework that seems like it's supposed to have
all the answers. I mean, in reality, it doesn't. I mean, I think it's a useful framework to be
able to ask the right questions or to be able to articulate your hypotheses. So if you think that
consciousness is based upon the idea of having some sense of trajectory of temporal extent and
different worlds I can choose between or different futures I can choose between, that might be a key
part of it. But for some people, that's not what they mean by consciousness.
I found in a particular reading books by people like Anil Seth on this sort of topic, I found one
of the interesting comparisons being the questions about consciousness versus questions about life.
And we almost don't ask what life is anymore. It doesn't necessarily seem that mysterious,
just because we've had so much of an understanding of the processes involved in life, the dynamics of
life and the way biology works, it's still much more to go. But the question of what life is just
doesn't seem as relevant today as I suspect it did many years ago with those sorts of questions
that were being posed. And perhaps we'll see the same thing with questions like consciousness.
Yeah, it's interesting though how vague many of these concepts are. And it's quite an interesting
thought experiment just to get someone to explain just an everyday thing, you know, like what happens
when you throw coffee on the floor. And just keep asking why. And just observing how incoherent and
incomplete the explanations are. And it's the same thing with life, it's the same thing of
consciousness, it's the same thing of causality, agency, intelligence, all of these different things.
And I guess most people don't spend time digging into their understandings of these things and
realizing how incoherent and incomplete they are. Life is quite an interesting one in particular,
because I think one of the achievements of active inference is blurring the definition of or the
