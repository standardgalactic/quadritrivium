I suppose the key things I would be thinking about are, is there a clean way of defining
a boundary for a meme? Is there something that the meme is doing to the outside world?
Is there something that the outside world is doing to the meme?
And I think if you're able to define those things convincingly, then perhaps there is a form of
agent that may be non physical, if that's how you choose to define it. But then I'm not sure what
physical means in this setting. Is there also an account of saying, well, actually,
if you can write down the dynamics of how a meme propagates through a network,
is that any different writing down the dynamics of another sort of physical system?
Yes, possibly not. But it is really interesting to me that something like language could be seen
as a life as a super organism, or even something like religion. And it seems to tick all of the
boxes that we talk about with a gentleness in physical agents, which is to say, let's say
a religion or even nationalism, you could say that the state of the Netherlands has certain
objectives. And clearly, there's a two way process here. So the state affects our behavior. And we,
our collective behavior influences the state. But this then, I think the reason why people don't
like to think in this way is we have psychological priors. So we are biased towards seeing a
gentleness in individual humans, but we tend not to think of non physical or diffuse things as being
agents. Yes, I think that's probably right. And again, it sort of brings us back to this whole
issue about the language that we use, that it comes laden with lots of prior beliefs about what it
means, which may vary from person to person. And there comes a point where you say, how important
is it that I commit to using this particular word to mean this particular thing in this setting?
But again, in your example of taking a nation or nation state as being a form of organism at a
higher level or form of agent, if you can show that there is a way of summarizing the dynamics of
that system, maybe some high order summary of the behavior of people in that system, voting
intentions, I don't know, you might then be able to show that it behaves in exactly the same way
mathematically as individuals within that system. Yeah. So this brings me on a little bit too. I've
been reading this book called The Mind is Flat by Nick Chaito and I'm speaking to him on Friday.
And his main take is that, I guess you could call him a connectionist, he's friends with Jeffrey
Hinton. And his main take is that there is no depth to the mind. So for years, psychologists have
built these abstract models to reason about how we think. So we do planning, and we do reasoning,
and we have perception, and we do this, and we do that. And also, we try and generate explanations
for our behavior. So we do this kind of post hoc confabulation. But when you study it, it's incredibly
incoherent and inconsistent. And he was talking all about how the brain is actually a kind of
predictive system, right? So we have these very sparse incoherent inputs, and we sometimes see
things that aren't there. And I think you speak about this in your book that there was a really
big shift. I think you referred to it as the Helms-Hotsian idea that the brain is a kind of
prediction machine, rather than our brain just kind of like building a simulacrum of the world
around us. I mean, how do you think about that as a neuroscientist? Yeah, I mean, I think prediction
has to be a key part of it. And the reason it's a key part of it is that it's a way of coupling
us to our world that without prediction, you know, if you're purely simulating what might be going on
without actually then correcting your simulation based upon what's actually going on or the input
you're getting from the world, then you're not going to get very far. So prediction is just an
efficient way of dealing with the issue of how do I update my beliefs? How do I update if you want
to call it a simulation? My simulation, my internal simulation of what's going on outside.
And once you cease to have that constraint, once the world ceases to constrain the simulation,
that's the point at which you start, as you say, hallucinating, seeing things that aren't there
and developing beliefs that just bear no relationship to or little relationship to reality.
Yeah, interesting. So I mean, one thing this Nick Chaitaguay was saying was that we see a
complex system and we adopt what Daniel Dennett calls the intentional stance. And that is I have
a self model, I have a model of your mind, and I observe behavior and I kind of impute
onto you a model and I can generate explanations. So as I say, Thomas did that because he must
have wanted to do this. And I guess you could argue that all of this is just a confabulation.
It's just an instrumental fiction. It's a way for us to explain behavior, but it doesn't really exist.
But then there's the question of, well, it's not that it doesn't exist. It's just that your mind
is incomprehensibly complex. So it's not that the mind is shallow. I prefer to think of it as
the mind has so much depth that it's beyond our cognitive horizon. And depth, I think, is an
interesting notion as well. I mean, it's the idea that comes under a lot of machine learning and
the idea of deep learning neural networks with multiple layers. And I think you're right that
depth is an important part of our generative models as well, of our brains models of the world.
And part of that comes from the fact that the world actually does separate out into a whole
different series of temporal scales of things that happen slowly, that contextualize things that
happen more quickly, that contextualize things that are even faster than that. And so one good
example of depth might be that if you're reading a book, then you have to bear in mind which page
you're on within that page, which sentence or which paragraph you want, within paragraph,
which sentence, within the sentence, which word, within the word, which letter. And by combining
your predictions sort of both down the system that way, but then updating your predictions
all the way back up again, you start to be able to make inferences about the overall narrative
that you're reading. The other thing you mentioned that I thought was interesting was the idea of
confabulation and of how we come to beliefs about other people's behavior. And I think the same
thing is also true about our own behavior and sort of making an inference about what we've done.
And this comes all the way back to the sense of agency again, doesn't it? It comes back to the
idea that I am inferring, I'm behaving in this way for this reason, because I've chosen to do this,
because I had this goal in mind. And to come back to the other question, is that real? Or is it
simply an inference about what I've done? I would suggest that it's certainly an inference about
what I've done, whether or not it's real. Giovanni and I put together some simulations
and some theoretical work a couple of years ago after a discussion at a conference about or a
workshop about machine understanding, suggesting that machine intelligence is one thing, but
actually understanding why you've come to a particular conclusion. ChatGPT being able to
explain to you why it came up with a specific sequence of words or why a convolutional neural
network classified an image in a particular way is one of the big issues really, and there are
solutions coming up, but it's one of the big issues in the deep learning community as to how
you have that transparency in terms of what the models are doing and why they're doing it.
Giovanni and I put together some work following that, looking at
understanding of our own actions from an active inference perspective, and there it was very much
framed as I have a series of hypotheses of things I might do, of reasons why I might do that.
And then after observing myself behaving in a particular way, I can then use my own behavior
as data that I then have to come up with an explanation for. And it's very interesting to
see what happens if you start depriving that of aspects of its behavior and to see the confabulations
that result from that. I can't remember where it came from originally, the idea of hallucinations
being a perception generally being effectively a constrained hallucination, where you take your
hallucination, your simulation of what's going on, and then you fix it to what's actually coming in.
But you could argue that actually a lot of our understanding about what we're doing is also
just a constrained confabulation in exactly the same way.
Yes, which is very ironic because people diminish GPT and because they say it's just
confabulating, whereas the preeminent neuroscientists of the day do basically make the same argument
about how the brain works, and even our communication now on conditioning your simulator.
So the semantics are drawn by your own model in simulation of the world,
rather than being the simulacrum of mine. You spoke about machine understanding,
I mean, there's this Chinese Rem argument. And we're in a really interesting time now because
we have artifacts that behave in a way which is isomorphic in many ways.
And it's so tempting to say, well, we're different. And you could make the ontological argument,
but this psychological argument is a big one as well, which is we're different because we have
beliefs, motives, volition, desires, we have all of these things.
But as we were just saying before, this is all post hoc confabulated.
We actually don't have consistent beliefs and desires. It's just a fiction.
Was it a fiction or is it a plausible explanation?
Well, I guess the thing that breaks it for me is the incoherence and inconsistency,
because you would think that we would be fully fledged human agents if we had consistent beliefs
and desires. And it's not to say that we don't because it feels like some of our goals are
they grounded in some way, like we need to eat food. But we think of ourselves as being
unique as humans, because we have higher level goals and beliefs that aren't necessarily instrumental
to eating food. And I guess those things in particular might be confabulatory.
Yes. So on the volition thing, that's something that really interests me.
An active inference agent is we draw a boundary around a thing and it can act in the environment
and it has preferences. And essentially, it has a generative model where it can produce these
plans, these policies, if you like, and at the end of every single plan is an end state.
So it's got all of these different goals in mind, if you like. And in the real world,
real in big air quotes, these things emerge. But when we design these agents, we need to
somehow impute the preferences onto them. So it feels like they have less agency if we
impute the preferences. Would you agree with that? Interesting question.
And a very relevant question in the current number of industry related applications of
active inference. I think we were speaking about earlier, there are a number of companies now
that have been set up looking at use of active inference based principles for various problems.
Companies like Versus that we spoke about before and Stan Hope AI that I do some work with as well.
And the issue there is very much, it's a different kind of issue to the biological
issue of describing how things work. And it's the issue of saying, if I now want to design an
agent to behave in a particular way, as you say, am I taking some agency away from that?
There are a couple of things to think about there. I suppose one is thinking about
do biological agents actually select their own preferences to begin with?
And I think most people would probably say they don't most of the time. There may be certain
circumstances where they do or where a particular preference might be conditionally dependent upon
the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else. But it's
not that I'm actually selecting this is what I want to want. There is a famous quote here,
but I can't remember what it is. I don't know whether you do. No.
No, it's escaped me about wanting what you want or wanting what you do or something along those
lines. Anyway, the point I'm making is that, to some extent, our preferences are given to us
effectively through a process of evolution, natural selection, previous experience that has
affected what is a good set of states to occupy. And those will often be a good set of states that
help my survival, that help the persistence of the species that I'm a part of. And arguably,
the same thing is true when you as a designer of a particular algorithm or an agent are giving it
a set of preferences. From its perspective, it's never selected them anyway. And that's the same
as you or I not necessarily having selected our preferences. There's one additional element that
I think is interesting to think about. And one of my colleagues and collaborators,
Nor Sajid, has done a lot of interesting work on this, which is the idea of learning your own
preferences, of actually saying, let's create an agent that isn't given preferences to begin with,
but is allowed to learn as it behaves what sort of goal states it ends up in.
And there you get some very interesting results. So she showed that these sorts of agents
may end up doing things that you just don't want them to do, that they end up forming a
particular pattern of being or a particular way of being that you as a designer might never have
envisaged. For example, in an environment with lots of potential holes that it can fall into,
some of these agents just become hole dwellers. They just decide, I found that the first few
times I did this task, I fell into the hole. So I've decided I'm probably the sort of creature
that likes living in a hole. So that's a situation where you can give it a certain agency. And maybe
that agency is the ability to sort of disagree with what you as a designer might expect or want
from it. Yes. This is so interesting. I mean, we're getting a little bit into, we'll have a
discussion about cybernetics and externalism. But so what you're describing there is the reason
why AI systems today are not sophisticated is because they are convergent. And that's usually
because they don't actually have any agency. So one of the hallmarks of the physical real
systems in the real world is that they have these divergent properties. And that's because you have
lots of independent agents following their own directiveness doing epistemic foraging. So
interesting stepping stones get discovered. And sometimes those stepping stones aren't what the
designer of the system would have liked, as you just said. So there's an interesting kind of paradox
there of how much agency do you want to imbue in the agents. But the other paradox is the physical
and social embeddedness. Because as you just said, cynically, we don't have as much agency as we
think we do, because we're embedded in the dynamics around us. And being part of this
overall system means that our agency is defined not just by our boundary, but it's by the history
of the system. It's the history of us sharing information of all of the things around us.
And all of these things inform what we do and what our preferences are. And then you say, well,
we can just drop a brand new agent in the system. And it doesn't quite work because it's a fish out
of water. It's not embedded in the ways that things that emerged in that system were in the
first place. But this does get us onto this discussion of externalism. So part of the fiction
of how we think about cognition is that we think of ourselves as islands that don't share information
dynamically with the outside world. And of course, active inference is a way of bridging
these two schools of thought. So can you kind of bring that in?
I mean, I think you've already done it in a sense. I'm not sure what else there is for me to say on
that. I'll try my best. So yes, I mean, active inference is about, well, it's about aboutness.
It's the idea that our brains and our internal state evolves in such a way that reflects beliefs
about what's outside. And I think that's one of the key things that you have to have for any sort
of intelligent system. And that doesn't necessarily exist with other approaches that exist in
neuroscience or artificial intelligence. It is that, and I'll just repeat that, it's very much
being, the aboutness is the key thing that what's happening in my head is a reflection or is a
description in some way is about what's happening outside my head. And maybe that's the link with
this sort of externalism. But it's not just unidirectional either. It's the fact that
I'm forming beliefs about what's happening in the outside world, but I'm also the one influencing
the outside world to change it to fit with the beliefs I have about how it should be.
Yes. Yes. So there's a kind of model. So we draw these boundaries. And we model the world around
us. And we influence the world around us. And that's essentially what active inference is.
I guess it might be useful just to sketch out the cognitive science idea of an activism or
cybernetic. So there were folks who really railed against this idea of representationalism,
which is this idea of model building in principle. And active inference is an integrated approach
where we allow some model building, but we also think of the world itself as being its own best
representations. How do we kind of bridge those two ideas? Yes. And I confess, I'm always lost in
the distinction between the sort of inactivists, radical inactivists, the sort of different levels
of stance you can take on this. And I think it comes down to that, that from an active inference
perspective, both your representations, if that's the right word, the beliefs you have about the world,
whether or not that meets the criteria for representation from an inactivist perspective
is very important. But it is only important in terms of how you act. If your beliefs did not
affect how you acted, clearly natural selection would not have selected you to form those beliefs.
I think it's the simple way of putting it. So let's talk about some of the kind of
the mathematical underpinnings here. So I think probably one of the main concepts we
should start on is this idea of surprise. And maybe we can talk about it in general terms,
