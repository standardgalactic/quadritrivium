and then we can move on to Bayesian surprise. So why is surprise so important in the free energy
principle? Well, it's central to it. It is the key thing that matters. And we talk about the free
energy principle. But in a sense, free energy is really there as a proxy for surprise. So yes,
what do we mean by surprise? And it's another one of those things like the high road and
the low road that you can approach from several different angles or several different lines of
attack. If you were modeling something, if you were a Bayesian, so if you took a particular
stance on probability theory and wanted to know, given my model, given my hypothesis,
what's the evidence for it? What you would normally do is calculate something known as
a marginal likelihood, which is just a measure of the fit between your model and the data that
you have that you're trying to explain. That fit trades off various different things. So it can
trade off how accurately your model is explaining the data against how far you've had to deviate
from your prior beliefs or from your initial assumptions in order to arrive at that explanation.
So that marginal likelihood, that evidence is effectively just the negative or the inverse
of surprise. So that that's one perspective on it, the better the fit, the simpler and most
accurate my explanation for something, the less surprised I will be by it. Another perspective
on surprise is just this more colloquial sense. It's the idea that, given what I would predict,
how far out of that prediction is it? One could take a more biological perspective on it and say,
imagine we are, well, we are homeostatic systems that have some set points. We want to keep our
temperature within a certain range, our blood pressure within a certain range, our heart rate
within a certain range. If we find ourselves deviating from that, that is effectively a surprise
because we're not where we expect to be. And so we enact various changes to bring
those parameters back in range. So we might, if our blood pressure is too low, we might increase
our heart rate to bring our blood pressure back up to the range we expect it to be in.
And that is, in a sense, what active inference is all about. It's just this idea of keeping things
within that minimally surprising range. But of course, once you put dynamics on it, once you
start unfolding that in time, you end up having to not just deal with how surprising things are now,
but you've got to try and anticipate surprise and behave in such a way that you allostatically
control your sensory inputs, both your intraceptive inputs like heart rate and blood pressure,
etc., but also your extraceptive sensations, your vision, your audition, and the like.
And there's almost no end to the perspective you could take on surprise. Another perspective
on it is that it's a reflective of, in a physical system, the improbability of being in a particular
state. From a lot of physics perspectives, improbability is also associated with energy.
It takes energy to bring things into less probable states. And without inputting energy into a system,
it will generally end up in its most probable state in the absence of that.
You think of things like Boltzmann's equation and the relationship there between energy and
probability. And that also has a link then to the idea of either a Hamiltonian or indeed a
steady state distribution, which is just what is the distribution things will end up in if left
to their own devices for a certain amount of time until things have probabilistically converged.
And that means that if I would construct a probability distribution over where things
will be at a long point of time in the future, there will come a point at which that probability
won't change any further. And the tendency of physical systems to go to those more probable
states is exactly the same as the tendency to avoid surprising states. And again, we could
sort of go on for a while, but I won't on sort of other ways of conceptualizing it. But hopefully
that sort of explains why it's such an important thing that underpins so much of what we do.
We're either trying to sort of evolve as a physical system towards more probable states.
Or we are homeostatic or allostatic organisms trying to maintain our internal parameters within
the right set points. Or we are more colloquially just trying to avoid things that are different to
what we predict. Or we are statisticians trying to fit our model to the world as best we can.
And all of those things come under the same umbrella of surprise.
Free energy comes in because surprise is not a trivial thing to compute.
Mathematically, it's often either intractable mathematically or computationally. And so it's
just not efficient to be able to calculate. But free energy is a way of then approximating
that surprise. It's a way of coming up with something that is close enough to it. Or
even more precisely as an upper bound on surprise. So if you're at the lowest point of your free energy,
then that limits how high your surprise can be. The key additional thing in free energy is that
the distance between that bound, your free energy and your surprise depends on how good your beliefs
about the world are. And that's where perception comes in. That by getting the best beliefs you
possibly can, you minimize the distance between your free energy and which is up a bounding of
surprise and the surprise itself. So then any further reduction in free energy, you would expect
to also result in a decrease. Sorry, any further decrease in free energy would also result in a
further decrease in surprise. I mean, there's a few things that struck me. I mean, first of all,
what struck me is that we're using the language of things like statistical mechanics and Bayesian
statistics and information theory, things like entropy and so on. And we're interchangeably
kind of speaking about the same thing from the perspective of different disciplines,
which I find very, very interesting. And on the surprise thing, even though in this formalism,
we are minimizing surprise, I think there's an interesting perspective that sometimes surprise
is what we want. So for example, the chess algorithm, the ELO algorithm, it's only when
something surprising happens that the weights get updated because it's information. Or people on
YouTube, my videos are that they get more views when they have a cash value, which means they
have information content, which means that, you know, they're actually surprising your predictive
model. Even Arnold Schwarzenegger used to joke about it, he said, you have to shock the muscles.
You know, you have to do what the muscles don't expect. Otherwise, there's not an adaptation. So
there's this interesting juxtaposition between actually seeking out surprise, even though you
can think of our brains overall as minimizing surprise. And what was the other thing I was
going to say? Yeah, you were just getting onto variational inference, which is really interesting.
So there's a couple of intractable statistical quantities in this mixture that we're talking
about. I think it's the log model evidence and the Bayesian posterior. And we can't represent
those things directly. So we have to put a proxy in there, which kind of captures most of the
information, but it's still possible to deal with it. So how does this variational inference work?
Yeah. So I suppose maybe the first thing to think about, though, is just to recap what Bayesian
inference is. I suppose we've been talking about it quite a lot without necessarily defining it.
And many of you listeners, I'm sure, will know already. But the idea is actually relatively
straightforward and well-established and quite widely used. And it's the idea that if I have
some beliefs about things that are in my world that I can't directly observe, I may have a sense
of what's plausible to begin with. And that's what we refer to as a prior probability. I then also
need to have a model that says, given the world is this way, what would I expect to actually observe?
So for instance, given where you are relative to me, I can predict a certain pattern on my retina.
And if you were somewhere else, I would expect a different pattern on my retina. So I might have
a prior range of plausibilities as to where you are relative to me. And then I have a model that
explains how I'm going to generate some data based upon that. And Bayesian inference basically
takes those two things and inverts them using Bayes' theorem and effectively just flips both of them
round. So you now say instead of a distribution of where you are relative to me, I'm now talking
about a distribution of all the possible things that I could see on my retina. And instead of
predicting the distribution on the retina given where you are, I now want to know the distribution
of where you are given what's on my retina. And Bayesian inference, much like active inference,
is full of all these interesting inversions where you sort of flip things round from how
they initially appeared. But the problem is calculating those two things, calculating the
flipped model. So the distribution of all the things on my retina here would now be my model
evidence, my inverse surprise. And the distribution of where you are relative to what's on my retina
is my posterior distribution. But those things are not always straightforward to calculate.
And so variational inference takes that problem and makes it into an optimization problem. It
writes down a function that quantifies how far am I away from my, or what would be the true posterior
if I'd used exact Bayes. And then it says, well, let's parameterize some approximate posterior
probability. So come up with a function that represents a probability distribution that's
easy to characterize, something like a Gaussian distribution where I know I just need my mean
and my variance. And then just changes that mean and variance until you minimize this function
that represents that discrepancy, minimize this free energy, also sometimes known as an evidence
lower bound, in which case you maximize it. And interestingly, once you've maximized your
evidence lower bound or minimized your free energy, you end up with a situation where
the free energy starts to approximate your log model evidence or your negative log surprise.
And your approximate posterior distribution, your variational distribution starts to look
much more like your exact posterior probability distribution. So it's another one of those
interesting scenarios where doing one thing optimizing one quantity ends up having a dual
purpose. And in active inference, the only additional thing you throw into that is that you
want to then also change your data itself. So you do the third thing you act on the world
to then optimize exactly the same objective. The interesting thing, I guess, is just contrasting
to machine learning again. So in machine learning, we also have these big parameterized models and we
do stochastic gradient descent. And some might think of deep learning, because obviously you
can think of everything as a Bayesian. So you can think of machine learning as being maximum
likelihood estimation. Why is it that we go full Bayesian when we do active inference? Why not
something like maximum likelihood estimation? It's an interesting question. And there are a couple
of answers you could give again, some of which are more technical, but some of which are
some of which are slightly more intuitive. And I think one of the more intuitive answers is that
by having an expression of plausibility of things in advance, you just maintain things
within a plausible region. So maximum likelihood for those who are unaware is where you essentially
throw away that prior probability, where you throw away any prior plausibility as to as to
what the state of the world might be. And you just try and find the value that would maximize
your likelihood, which is your prediction of how things would be under some hypothesis or under
some parameter setting. And I think the first thing to say is if you throw away that prior
information, then you end up potentially coming up with quite implausible solutions.
That's particularly relevant if you're dealing with what's known as an inverse problem. So where
there are multiple different things that could have caused the same outcome. An example that's
often given is that for any given shadow, there's almost an infinite number of things, configurations
of the sun and the shape of the thing that's casting the shadow that could lead to exactly the
same shadow. And so maximum likelihood approach just won't be able to tell the difference between
all of those things. However, if you have some prior on top of that, if you have some statement
of the plausible things that might cause it, you can come up with a much better estimate of those
sorts of things. Another way of looking at it is that when you're dealing with a maximum likelihood
estimate, you're throwing away all uncertainty about the solution. So you're coming up with a
point estimate and you're saying this is the most likely thing, but you're ignoring all of your
uncertainty about it. And I think that is in itself a relatively dangerous thing to do and can lead to
the problem of overfitting, where you start to become very confident about what you can see from
a relatively small sample of things and you can end up with all of these well-described in the media
scenarios of complete misclassifications based upon that sort of overconfidence just because
all the uncertainty is gone. A more technical way of looking at it, I think, is if you think about
what a free energy is. So free energy is our measure of our marginal likelihood that we're
using when we're doing Bayesian inference. And one way of separating out what a free energy
looks like is to have our complexity, which is effectively how far we needed to deviate from
our prior assumptions to come up with an explanation, and our accuracy, which is how well we can fit
our model. Accuracy is common to both maximum likelihood type approaches because we're trying
to find the value that most accurately predicts our data and also to Bayesian approaches.
Both want to do that. But what's thrown away in the maximum likelihood type approach is the
complexity bit, the how far do you deviate from your priors. So there's an inbuilt Occam's razor,
the idea that the simplest explanation is a priori more likely that you get from a Bayesian
approach that you throw away when you're dealing with maximum likelihood estimation.
I wondered to what extent does the active part play a role here. So even in machine learning,
there's something called active learning, where you dynamically retrain the model,
or there's something called machine teaching, where you dynamically select more salient data
to train the model, and the model gets much better. And in things like Bayesian optimization,
for example, by maintaining this distribution of all of your uncertainty in a principled way,
you can go and seek and find more information to kind of improve your knowledge on subsequent steps.
So I guess it's sort of bringing in this idea of it's not just what happens now,
it's about how can I improve my knowledge of the world over several steps.
Yes, and that reminds me about the point you were making earlier, that sometimes we actually do
things to surprise ourselves, which seems very counter-intuitive in the context of the idea that
we're trying to minimize surprises as our sole objective in life. And sometimes people talk about
this in terms of a dark room problem, the idea that actually if all you want to do is minimize
your surprise, you just go into a room, turn off the lights and stay there because you're not going
to experience anything that's going to surprise you. I mean, the answer to this problem is that
actually, as organisms, as creatures, we don't expect to be purely in a dark room. And the
sort of organism that would be is, again, probably not a very interesting one. And that what we predict,
what we'd be surprised by might be permanently staying in a dark room. But it goes even further
than that. And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable
world where I know what's going to happen next. The best way of doing that is to actually gather
as much information as you can about the world around you. So the first thing you do really is
you turn on the light and see what the room looks like, because that might then predict all the sorts
of things that could fall on you in that room and could potentially cause surprise. And by knowing
about it, you mitigate the surprise that you might get in the future. And as you say, you can only
really do that if you know what you're certain about. And so if you take a maximum likelihood
approach, if you work based on point estimates and you have no measure of your uncertainty,
then there's no way you can possibly know what you're uncertain about to be able to resolve
that uncertainty. So this brings me on to causality. We know that predictive systems,
which are aware of causal relationships, work better. But if we just bring it back to physics
first, I mean, to you, what do you think causality is?
It is a tricky issue as to what causality is. And I think whether it exists or not is really a
matter of how you define it, isn't it? And some would define that purely in terms of conditional
dependencies, that the behavior of one thing is conditionally dependent upon something else,
and therefore you could say that the one thing causes the other. But as we know from Bayes'
theorem, that's not quite good enough, because you can swap any conditional relationship around
through that process of inverting your model. Sometimes that causality is written into the
dynamics of a model. So this would be the approach used in things like dynamic causal
modeling of brain data, where you might say that the current neural activity in one area of the
brain affects maybe the rate of change of neural activity in another part of the brain.
And it's the way in which those dynamics are written in, the fact that it's one affects the
rate of change of the other, that gives it that causal flavor and a very directed perspective on
it. Probably the work that is most comprehensive on this is looking at people like Judea Pearl and
a lot of his work on causality. There's a lot of detail about the notion of an intervention.
And I suppose you can think of this in terms of how you might establish causation in a clinical
context. If you were to run a trial to try and establish whether one thing's caused another,
you need to make sure you're not inadvertently capturing a correlation or a conditional dependence
