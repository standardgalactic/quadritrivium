my fondest memory. It's usually when I discover something that I think nobody has seen before,
but that happens very rarely because most of the things you think of somebody else has done before.
This episode is sponsored by Numeri. Are you a data scientist looking to make a real-world impact
with your skills? Do you love competing against the best minds in the world? Well, introducing
Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game for good.
Numeri combines a competitive data science tournament with powerful, clean stock market data
enabling you to predict the market like never before. Sign up now, become part of the elite
community, taking the stock market by storm and I'll see you on the leaderboard.
Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have.
We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber,
the researcher responsible for leading the research groups which invented much of the technology
which has powered the deep learning revolution. It's long been a dream to get you on the podcast,
you again. It feels like the day has finally arrived, so welcome to MLST.
Thank you, Tim, for these very kind words and this very generous introduction.
So on that, let's discuss the credit assignment problem in machine learning. Now,
you've dedicated a significant amount of time researching and publishing the actual history
of the field and there's a significant divergence between the public narrative
and what actually happened. And amazingly, no one has pointed out any factual inaccuracies in your
accounts, but the incorrect perceptions still persevere. Now, I particularly enjoyed reading
your history of the breakthroughs in machine learning, going back to ancient times and of course even
remarking on the very first computer scientist, Leibniz. And for example, you pointed out the
history of who invented backprop and the CNN. And you explained that there wasn't really
a neural network winter at all in the 1970s. So could you just sketch out a little bit of that
history? So that's a challenge. Actually, computer science history and computing history started
maybe 2000 years ago when Heron of Alexandria built the first program-controlled machine.
That was 2000 years ago in the first century basically. And he basically built an automaton
that was programmed through a cable which was wrapped around a rotating cylinder which had
certain knobs and then there was a weight which pulled it down and the whole apparatus
was able to direct the movements of little robots, of little puppets in an automatic theater.
That, as far as I know, was the first program-controlled machine in the history of mankind.
Even before that there were other machines. The ancient Greeks had even earlier the
Antiqueterre mechanism which was kind of a clock, an astronomical clock. But then more recently
we have seen many additional advances and you mentioned Leibniz, of course, who is of
special interest to our field because he not only is called the first computer scientist
because he had the first machine with a memory that was in the 1680s, I think. He not only had the
first machine that could do all the basic arithmetic operations which are addition,
multiplication, division and subtraction, then he not only had these first ideas for a universal
problem solver that would solve all kinds of questions, even philosophical questions,
just through computation. And he not only was the first who had this algebra of thought which
is deductively equivalent to the much later Boolean algebra. In many ways he was a pioneer,
but especially in our field in deep learning he contributed something essential, which is really
central for this field, which is the chain rule. I think 1676, that's when he published that and
that's what is now being used to train very deep artificial neural networks and also shallow
neural networks and recurrent neural networks. And everything that we are using in modern AI
is really in many ways depending on that early work. But then of course there was so much additional
work. The first neural networks, as we know them, they came up about around 1800. That's when Gauss
and Legendre had the linear neural networks, the linear perceptrons in the sense that they were
linear without having any non-differential aspect to it. So these first neural networks,
back then, were called method of least squares. And the training method was regression and the
error function was exactly the same that we use today. And it was basically just a network with
a set of inputs and a set of outputs and a linear mapping from the inputs to the outputs. And you
could learn to adjust the weights of these connections. So that was the first linear neural
network and many additional later developments led to what we have today. You had this beautiful
statement. You said that machine learning is the science of credit assignment and we should apply
that same science to the field itself. And I guess what I'm really curious about is
first, if you could educate our listeners just a bit on what credit assignment is in the context
of, say, machine learning and why you think it's important that that should apply to the field
in general. You know, why should we care about credit assignment? Why should we study the history
of the developments in the field? Why is it important? I'm interested in credit assignment,
not only in machine learning, but also in the history of machine learning,
because machine learning itself is the science of credit assignment. What does that mean? Suppose
you have a complicated machine, which is influencing the world in a way that leads to the solution
of a problem. And maybe the machine solves the problem. But then the big question is,
which of the components of these many components were responsible? Some of them were active
a long time ago and others later and early actions set the stage for later actions. Now,
if you want to improve the performance of the machine, you should figure out how
did the components contribute to the overall success. And this is what credit assignment is
about. And in machine learning in general, we have a system consisting of many
machine learning engineers and mathematicians and hardware builders and all kinds of people.
And there you also would like to figure out which parts of the system are responsible for later
successes. Yeah, and it's a brilliant point. And I completely agree with you, by the way.
And I think the way I think about it is you've got this giant architecture of humanity and in it
are these certain nodes that may be an individual, maybe a research group. And if they come up with
things that are very helpful, right, you want to try and direct more attention, more resources,
at that nodule, at that node, right, because it's likely to come up with additional very
important things. And if we don't get that right, we're just not optimizing the algorithm of science
as a whole. That's right, yes. Machine learning and science in general is based on this principle
of credit assignment where credit usually doesn't come in form of money, sometimes also in form
of money, but in form of reputation. And then the whole system is set up such that you create
an incentive for people who have worked on improving some method to credit those who
maybe came up with the original method and to just have these chains of credit assignment
that make clear who did what, when, because the whole system is based on this incentive.
And yes, those who are then credited with certain valuable contributions, they also can get
reasonable jobs within the economy and so on. But that's more like the secondary
consequence of the basic principle. And that's why all PhD advisors
teach their PhD students to be meticulous when it comes to credit assignment to past work.
So one last question, if I may, I've really enjoyed studying the history of advancement
because I found that when I go back and read original source materials, let's say
Einstein's first paper on diffusion or anything like that, because they're breaking new ground,
they're considering a wider array of possibilities. And then over time, the field becomes more and
more focused on a narrower avenue of that. And you can go back and look at the original work
and actually gain a lot of inspiration for alternative approaches or alternative
considerations. So in a sense, it's kind of in the sense of forgetting is as important as learning.
Sometimes we need to go back to go down a different branch of the tree, if you will,
and expand the breadth of the search a little bit. I'm curious if you've noticed that same phenomenon.
Yes, science in general is about failure. And 99% of all scientific activity is about
creating failures. But then you learn from these
failures and you do backtracking. And you go back to a previous decision point where you maybe
made the wrong decision and pursued the wrong avenue. But now you have a branching point and
you pursue an alternative. And in a field that is rapidly moving forward, you don't go back very
far usually. You just go back to a recent paper which came out five months ago. And maybe you
have a little improvement there. And then maybe there's yet another little improvement there.
And some parts of our field are at the moment a little bit like that,
where PhD students are moving in, who just look at the most recent papers and then find a way of
improving it a little bit and 2% better results on this particular benchmark. And then the same guys
are also reviewing at major conferences, papers by similar students and so on. And so then sometimes
what happens is that no very deep backtracking is happening, just because the actors aren't really
aware of the entire search tree that has already been explored in the past.
On the other hand, science has this way of healing itself. And since you can gain reputation by
identifying maybe more relevant points, branching points, you have this incentive within the whole
system to improve things as much as you can, sometimes by going back much further.
So there's been a lot of discussion in the discourse around this concept of AI existential
risk. And you again, you've published quite a few pieces about this recently, prominently in
The Guardian and in Forbes actually. And one of the things I wanted to focus on is this concept
of recursive self-improvement, because that seems to be one of the plausible explanations that these
folks give. And of course, when it comes to recursive self-improvement, you are an expert in
this field. I mean, Godel machines come to mind immediately. So I want to kind of explore asymptotes
and limitations. This whole idea of recursive self-improvement is very sexy, isn't it?
In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987,
that was my diploma thesis. And it was about this recursive self-improvement thing. So it was about
machine that learns something in a domain. But not only that, it also learns on top of that to
learn a better learning algorithm based on experience and the lower level domains. And then
also recursively learns to improve the way it improves the way it learns. And then also recursively
learns to improve the way it improves the way it improves the way it learns. And yeah, I called that
meta-learning. And back then, I had this hierarchy with, in principle, infinite self-improvement
in the recursive way, although it is always limited by the limited time that you run the system like
that. And then, of course, the motivation behind that is that you don't want to have an artificial
system that is stuck always with the same old human-designed learning algorithm. No, you want
something that improves that learning algorithm without any limitations, except for the limitations
of physics and computability. And so much of what I have been doing since then is really about that.
Self-improvement in different settings where you have, on the one hand, reinforcement learning
systems that learn in an environment to better interact and better create ways of learning
from these interactions to learn faster and to learn to improve the way of learning faster,
and so on. And then also gradient-based systems, artificial neural networks, that learn through
gradient descent, which is a pre-wired human-designed learning algorithm, to come up with a better
learning algorithm that works better in a given set of environments than the original human-designed
one. And yeah, that started around 1992 neural networks that learned to run their own learning
algorithms on the recurrent network themselves. So you have a network which has standard connections
and input units and output units, but then you have these special output units which are used to
address connections within the system, within this recurrent network, and they can read and
write them. And suddenly, because it's a recurrent network and therefore it is a general-purpose
computer, suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary
learning algorithms that translate incoming signals, not only the input signals, but also the
evaluation signals like reinforcement signals or error signals into weight changes, fast weight
changes, where the weight changes are not dictated any longer through this gradient descent method,
but no, now the network itself is learning to do that. But the initial weight matrix is still
learned through gradient descent, which is propagating through all these self-referential
dynamics in a way that improves the learning algorithm running on the network itself. That
was 1992, and back then, compute was really, really slow, it was a million times more expensive
than today, and you couldn't do much with it. But now, in recent works, all of that is working
out really nicely and has become popular, and we have, just if you look at the past few years,
a whole series of papers just on that. So that's the fast weight programming that you're referring
to? Yes, so it's fast weight programmers where you have a part of the network that
learns to quickly reprogram another part of the network, or the original version of that was
actually two networks, so one is a slow network, and then there's another one, a fast network,
and the slow network learns to generate weight changes for the second network,
and the program of the second network are its weights. So the weight matrix of the second
network, that is the program of the second network, and the first one, what does it do? It
generates outputs, it learns to generate outputs that cause weight changes in the second network,
and these weight changes are being applied to patterns, to input patterns, to queries, for
example, and then the first network essentially learns to program the second network, and essentially
the first network has a learning algorithm for the second network, and the first system of that
kind, 1991, that was really based on on keys and values, so the first network learns to program
the second network by giving it keys and values, and it says now take second network, take this
key and this value, and associate both of them through an outer product, which just means that
those units are strongly active, they get connected through stronger connections, and
the mathematical way of describing that is the outer product between key and value.
So that's how the first network would program the second network, and the important thing was that
the first network had to invent good keys and good values, depending on the context of the input
stream coming in, so it used the context to generate what is today called an attention mapping,
which is then being applied to queries, and this was a first step right before the most general
next step, which is then really about learning a learning algorithm running on the network itself
for the weights of the network itself.
Could I press you a tiny bit on this concept of meta-learning and convergence and asymptotes?
Now one of the reasons I think why the X-Risk people believe that it will just go on forever
is they believe in this idea of a pure intelligence, one that doesn't have physical limitations in
the real world, and I'm quite amenable to this ecological idea of intelligence that it does,
the world is a computer basically as well as the actual brain that we're building,
so surely it must hit some kind of asymptote. Do you have any intuition on what those limitations
would be? So you are talking about the ongoing acceleration of computing power and limitations
thereof, is that what you have in mind here? Well that's one part of it, so even if you
just scale transformers I think there would be some kind of asymptote, but we're talking here
about meta-learning, learning to learn, how to learn, and recursive self-improvement, and it's
similar to this idea of reflection, self-reflection and language models, it actually improves the
performance with successive steps of reflection and then it levels off, it reaches an asymptote.
I just believe that there are asymptotes everywhere and that's the reason why I
don't think recursive self-improvement will go on forever, but I just wondered if you had
any intuitions on what those impressions are. Yeah, you are totally right, there are certain
algorithms that we have discovered in past decades which are already optimal in a way
such that you cannot really improve them any further, and no self-improvement and no fancy
machine will ever be able to further improve them. There are certain sorting algorithms that
under given limitations are optimal and you can further improve them. That's one of the limits.
Then of course there are the fundamental limitations of what's computable, first identified by
Kurt Gödel in 1931, he just showed that there are certain things that no computational process
can ever achieve. No computational theorem prover can prove or disprove certain theorems
in a language, in a symbolic language that is powerful enough to encode
certain simple principles of arithmetic and stuff like that. What he showed was that
there are fundamental limitations to all of computation and therefore there are fundamental
limitations to any AI based on computation. I'm glad you brought that topic up because it's one of
our favorite things to discuss which is do you think the human mind ultimately reduces to just
an effective computation and so subject to those same limits or do you think there's any
known or unknown physics that give us some out in which the brain can do a computation that
amounts to hypercomputation? Since we have no evidence that the brain can compute something
that is not computable in the traditional sense, in Gödel sense and torings and churches sense
and everybody who has worked on this field, since we have no evidence we shouldn't assume that's the
case. As soon as someone shows that people can compute certain things or prove certain theorems
that machines cannot prove given the same initial conditions, we should look more closely but
there are many things that might be possible in fairy tales and we are not really exploring them
because the probability of coming up with interesting results is so low. Fair enough,
so you mentioned so far two asymptotes, one being of the mathematical kind where there's just
mathematical proofs that certain things are optimal, the other one being the limits of
computation itself. What other asymptotes do you see applying to or putting bounds on recursive
self-improvement? The most obvious thing is probably light speed and the limits of physical
computation. We know those for several decades, we have happily enjoyed the fact that every five
years compute is getting 10 times cheaper and this process started long before Moore's law was
defined in the 60s I believe because even in 1941 already when Susie built the first program
controlled computer this law apparently was active so back then he could compute maybe one
instruction per second and since then every 10 years a factor of 100 every 30 years a factor of
a million more or less until today and there's no reason to believe it won't hold for a couple
of additional decades because the physical limits are much further out. The physical limits that we
know are the Bremermann limit discovered I think in 1983 by Bremermann and they basically say that
one kilogram of matter cannot compute more than 10 to the 51 instructions per second.
So that's a lot of compute but it's limited and to give you an idea of how much compute that is
I also have a kilogram of computer in here and probably it cannot compute 10 to the 20
instructions per second otherwise my head would explode because of the heat problem
but maybe it can compute something that is not so far from 10 to the 20 instructions maybe 10
to the 17 something like that although most of my neurons are not active as we speak because again
otherwise my head would just evaporate. Now if you have an upper limit of 10 to the 20 instructions
per brain then the upper limit of all of humankind would be 10 billion times that individual limit
and that would be 10 to the 30 instructions per second and you see it's still far away from the
10 to the 51 instructions per second that in principle one kilogram of matter could compute
and now we have more than 10 to the 30 kilograms of matter in the solar system and there's some
and so if the current trend continues at some point much of that is going to be used for
computation but then it will have to slow down even if the exponential acceleration
will still be with us for a couple of decades because at some point it is going to be a polynomial
because due to the limits of light speed at some point it will be harder and harder
to acquire additional mass once you have reached the limits of physical computation per kilogram
the only way to expand is to go outwards and you know find additional stars and additional
matter further away from the solar system and then you will get a polynomial acceleration or
a polynomial growth at best so it will be much worse than the current exponential
growth that we are still enjoying. Sure but I would say you know the existential threat
that is more than sufficient to supply an existential threat and let me just put this
a little bit differently which is and I agree with you on this which is you are quoted as
saying that traditional humans won't play a significant role in spreading intelligence
across the universe and I think you are right I think we kind of share a vision of something
like the von Neumann probes that go out into space and form this star spanning civilization of
machines and artificial intelligence that have transcended you know biological limitations
so I guess my question to you is once that space faring star spanning you know civilization
exists if it becomes misaligned with us and decides that we are in the way right isn't that
an existential threat I mean might they just you know repurpose the earth regardless of whether
we're here or not for for their own aims yeah I'm often getting these questions and
and there is no proof that we will be safe forever or something like that on the other hand it's also
very clear as far as I can judge that all of this cannot be stopped and it can be channeled
in a very natural and I think good way in a way that is good for humankind now
first of all at the moment we have a tremendous bias towards good AI
meaning AI that is good for humans why because there is this intense commercial pressure
to create stuff that humans want to buy and they like to buy only stuff they think is good
for them which means that all the companies that are and that are trying to devise AI products
they are maximally incentivized to generate AI products that are good for those guys who are
buying them or at least where the where the customers think it's good for them
so it is still 95 so it may be five percent of all AI researchers really about AI weapons and
one has to be worried about that when all this has to be worried about weapons research but
there's a tremendous bias towards good AI so that is one of the reasons why you can be
a little bit optimistic for the future I'm always trying to point out the two types of
AIs there are those who are just tools of users human human users and the others that invent
their own goals and they pursue their own goals and both of them we have had for a long time
now for the AI tools it's kind of clear there's a human and a human wants to achieve something
and so it uses he uses or she uses that tool to achieve certain ends and and most of those are
of the type let's improve healthcare and let's facilitate translation from one person to another
one in another nation and just make life easier and make human lives longer and healthier
okay so that that's the AI tools but then there are the other AIs which also have existed in my
lab for at least 32 years which invent their own goals and they are a little bit like little
scientists where you have an incentive to explore the environment through actions through
experiments self-invented experiments that tell you more about how the world works such that you
can become a better and better and more and more general problem solver in that world
and so these AIs they have for a long time created their own goals and now of course
the interesting question is these more interesting AIs what are they going to do once they are
once they have been scaled up and can compete or maybe outperform humans and everything
they want to achieve so on the one hand the AI tools and there the greatest worry is
what are the other humans going to do to me with their AI tools so in the extreme case you have
people who are using AI weapons against you and maybe your neighbor is has bought a little drone
for 300 dollars and it has face recognition and it has a little gripper and it flies across the
hedge and puts some poison into your coffee or something like that so then the problem is not
the AI which is trying to enslave humans or something silly like that no it's your neighbor
or the other human and generally speaking you have to be much more afraid of other humans than you
have to be of AIs even those who define or set themselves their own goals because you must mostly
worry about those with whom you share goals so if you share goals then suddenly there is a potential
of conflict because maybe there is only one schnitzel over there and two persons want to
eat the schnitzel and suddenly they have a reason to fight against each other generally speaking
if you share goals then you can do two things you can either collaborate or compete an extreme form
of collaboration would be to maybe marry another person and set up a family and master life together
and an extreme form of competition would be war and and those who share goals they have many more
incentives to interact than those who don't share goals and so humans are mostly interested in other
humans because they share similar goals and because they give them a reason to collaborate or to
compete most CEOs of certain companies are interested in other CEOs of competing companies
and five-year-old girls are mostly interested in other five-year-old girls and the super smart AIs
of the future who set themselves their own goals they will be mostly interested in other super
smart AIs of the future who set themselves their own goals generally speaking there is not so much
competition and there are not so many shared goals between biological beings such as humans
and a new type of life that as you mentioned can expand into the universe and can multiply
in a way that is completely infeasible for biological beings so there's a certain
long-term protection at least through lack of interest on the other side
okay brilliant there's a few things I wanted to touch on there we will get on to what it means
for goals to emerge from systems later and you started off by saying that humans will buy
products that make them feel good and Facebook is quite an interesting example to play with
actually because Facebook is a little bit like an AI system which is a collective intelligence
and humans use Facebook but they have some idea that it might cause them harm and the thing with
population ethics is we know that our moral reasoning kind of decays over space and even more
so over time and part of the reason why time is so difficult is because it's predictive we don't
actually know what's going to happen in the future so our kind of reasoning about establishing
what the value of something is is very very faulty and I think that's one of the reasons why
these people would say that we don't really know what's good for us I do completely agree with you
though that the problem I think is humans rather than AIs on their own yes these are good points
feel free to uh offer some thoughts yes I mean it it would that's a whole separate discussion isn't
it when you discuss the limitations of what's predictable and um and how people often fail
to see what's good for them well I think maybe so you've already you've already um said that
there's no proof that we'll be safe forever right like I mean there could there could come an
existential risk you know from AI so I think my question to you is do you have sympathy for
the folks who say we need to be putting more resources into researching alignment like we need
to develop the tools um in order to allow it to be easier for people to construct AI that is aligned
for the goals and to make sure that you know that it doesn't that it doesn't have unintended
consequences like in other words there may not be a proof that we can go forever and be
safe for AI but we at least want to develop the basic mechanics that we need to safely
develop and deploy AI don't we yes and I sympathize with those who um are devoting their
lives to alignment issues and trying to build AIs aligned with humans
I view them as part of the evolution of all kinds of other ideas that come up as not only
nations compete with other nations but companies compete with other companies
and shareholders of different companies compete with shareholders of different companies and so on
and so there is such a huge set of different human goals which are not aligned with each other
that makes me doubt that you will come up with a general system that all humans can accept
simply because if you put 10 humans in a room and ask them what is good they will give you
10 different opinions however I sympathize with with this goal and it's good that people are
worried and they spend resources on solving some of these issues in the long run however
I think there is no way of stopping all kinds of AIs from having all kinds of
goals that have very little to do with humans the universe itself is built in a certain way
that apparently
derives it from very simple initial conditions to more and more complexity
and now we have reached a certain stage after 13.8 billion years of evolution and it it seems clear
that this cannot be the end of it because the universe is still young it's going to be much
older than it is now now there is this drive built in drive of the cosmos to become more complex
and it seems clear that civilization a civilization like ours is
is a stepping stone on to war it's something that is more complex and
could I touch on a couple of things here the bootloader example is kind of where I want to go
with this so a lot of the ideas of this movement can be traced back to Derek Parfit who is a
philosopher he was a moral realist so he thought there was such a thing as a moral fact and I'm
a bit of a relativist myself and actually if you trace this tree of complexity and how humans
evolve over time we might just be a stepping stone to a kind of rich diverse transhumanist
future where we become the thing over time that we're so scared of and I think the lens that
we're using here about what's right and what's wrong is kind of like I was saying before it's
a snapshot of humanity now and we kind of think of it as just this monolithic single thing
so does it really work when you project out to how we're going to evolve in the future
but first of all humankind is not a monolithic thing so many of these
arguments go like we should not do that because of that we should not do that because of that
but there is no us there is no we there are only and almost 10 billion different people
and they all have different ideas about what's good for them and so for thousands of years we had
these evolutions of ideas and of devices and philosophies competing partially competing
and partially compatible with each other which in the end led to the current values
that some people agree with and other people over there they agree with different values
nevertheless there are certain values that have become more popular than others more successful
more evolutionary with more success during the evolution of ideas
and so given this entire context of evolution of concepts and accepted ideas of what should be done
or what is worth being supported and what's not worth being supported all of this has changed a lot
if we look back 200 years the average people in the west had different ideas of what's good
than today and and this evolution of ideas is not going to stop any time soon
just a final question on this and there is a very real existential risk right now
of nuclear armageddon a real risk right now and if i were a rational person
i would be devoting all of my effort into that and other risks associated so do you think it's
a little bit weird that so much focuses on this ai x risk to me it's indeed weird now there are all
these letters coming out warning are the dangers of ai and i think some of the guys who are writing
these letters they are just seeking attention because they know that ai dystopia are attracting
more attention than documentaries about the benefits of ai in healthcare and stuff like that
but generally speaking i am much more worried about nuclear bombs than about ai weapons
a nuclear bomb a big one can wipe out 10 million people a big city within a few milliseconds without
a face recognition just like that without any ai and so in that sense it's much more harmful
than the comparatively harmless ai weapons than that we have today and that we can currently
conceive of so yes i'm much more worried about 60 year old technology that can wipe out civilization
within two hours without any ai well i guess um since we're we're not really going to worry about
ai for the moment we can uh we can turn our attention back to discussing with you uh how
we develop ai so um you know i'm really curious with with just the really the the vast you know
breadth and depth of your of your knowledge over the the history of of ai and the state of the art
i'm curious you know which current approaches you're you're most excited about and or what's on the
horizon um that you know for any of our listeners out there are thinking about um going into ai
research machine learning research you know what may be um alternatives that aren't getting enough
attention should they should they look into studying and and perhaps choosing the research
at the moment the limelight is on um language models large language models which pass the
touring tests and do all kinds of things that seemed inconceivable just a couple of years ago
at least to some of those who are now surprised but of course that is just a tiny part of
what's going to be important to develop true ai agi artificial general intelligence
um on the other hand the roots of what we need to develop true ai also
come from the previous millennium they are not new and of course what you need is an
environment to interact with and you need an an agent that can manipulate the environment and you
need a way of learning to improve the rewards that you get from this environment as you are
interacting it with it within a single lifetime so one of the important aspects of reinforcement
learning what we are now talking about is that you have only one single life you don't have
repeatable episodes like in most of traditional reinforcement learning no you have only one
single life and in the beginning you know nothing and then after 30 percent of your life is over
you know something about life and all you know is the data that you collected during these first
30 percent of your life and now there is an infinite almost infinite possibility set of
possibilities of futures and from this little short experience you have to generalize somehow
and try to select action sequences that lead to the most promising futures that you can shape
yourself through your actions now to achieve all of that you need to build a model of the world a
predictive model of the world which means that you have to be able to learn over time and to
predict the consequences of your actions so that you can use this model of the world that you are
acquiring there to plan to plan ahead and you want to do that in a way that isn't the naive way
which we had in 1990 which is millisecond by millisecond planning where you say okay now
I'm moving from A to B and the way to do it is first move that little pinky muscle a little bit
and move it a little bit more and move it a little bit more and then get up and so no you want to do
that in a high level way in a hierarchical way in a way that allows you to to focus on the important
abstract concepts for example as you are trying to go from from your home to Beijing you decompose
this whole future into a couple of sub goals you say a first important step is to go to the cap
station and get a taxi to the airport and then in the airport you will find your your plane
and then for nine hours nothing is going to happen and you exit in Beijing and have to find another
cab and so on so you you don't do millisecond by millisecond detailed planning no you have
high level planning to just reduce the computational effort and focus on the essentials of what you
want to do so that is something that most current systems don't do but for a long time we have had
systems like that and they are getting more sophisticated over time important you have a
predictive model of the world that is not just focusing on the pixels and predicting the how
does the video change as I'm moving my hand back and forth the video that I get through my camera
my eyes and so on and no higher level concepts that that reflect islands of predictability many
things are not predictable but certain abstract representations of these things are predictable
and so how can you discover these higher level concepts that you need to efficiently think
about your own future options and select those that are most promising in the single life
yeah yeah this is really interesting so we've been speaking with Carl Friston for example and he
talks about this collective intelligence where you have this multi-agent cybernetic framework
which is causally closed and one of the things we're talking about here really is not the model
itself people talk about chat gpt and it's just a model and people have configured it in arrangements
that have varying degrees of autonomy and in the future we will develop these collective
intelligences and they're not just predicting the actions and behaviors of other agents but even
the world that we're in is a computer to some extent so when you imbue agents with this kind of
creativity and autonomy that's the thing that I don't think people really understand what might
emerge from that it's related to this discussion about what kind of goals might emerge from that
do you have any intuition on what that would look like yeah let me give you just the simplest
example that we had in 1990 or 32 years ago of a system that sets itself its own goals and it
consists of two artificial neural networks and I know that Carl Friston is very interested in that
and only recently for the first time in my life I was on a paper where he was co-author
just a year ago and so back then it was really about a reinforcement learning agent and it
interacts with the world and it generates actions that change the world and then there is
another network which just is trying to predict the consequences of the actions in the environment
so the reactions of the environment to these actions and so that becomes a world model and
then what kind of goal was there which was different from traditional goals well in the
beginning this model of the world this prediction machine which is a model of the world a world
model knows nothing so it has high error as it is trying to predict the next thing as it is trying
to predict the reactions of the environment to the actions of the agent so as the second network
is trying to reduce its prediction error through gradient descent through back propagation essentially
the other one is trying to generate actions outputs that maximize the same error so basically
the goal the self-invented goal if you will of the first network is to generate an action
with whose consequences cannot yet be predicted by the other network by the model of the world
so the first network is generating outputs that surprise the second network
so suddenly you have an incentive where the first network is trying to invent actions
experiments that fool or that surprise the second network and that was called artificial curiosity
so now suddenly you have a little agent which a little bit like a baby doesn't learn by
imitating the parents no it learns by inventing its own little sub goals and it's trying to surprise
itself and have fun by playing with the toys and and observing new unpredictable things which
however become predictable over time and therefore become boring and then it has another incentive
to invent the additional experiments such that it still can surprise its model of the world
which in turn is improving and so on so artificial curiosity does that does that also
have the effect of making the network which is trying to predict does it have the effect of
making it more robust and more generalizable like almost a form of you know regularization
kind of built in in this pairing yeah you can build into that network all kinds of regularizers
an orthogonal concept which is also very important so that was just the first version that was
really in 1990 and then we have had a we had a long string of papers just on improvements of
this original concept of artificial curiosity so this old system is basically what you what you
now know as GANs Generative Adversarial Networks because the first network is generating a probability
distribution over outputs and the second network is then predicting the consequences of these outputs
in the environment and if you if the output is an image then the consequence can be either this
image is of a certain type yes or not no and then that's all that the prediction machine the world
model predicts in that simple case and you minimize the first network minimizes the same
error function that the second one maximizes so then you have basically a GAN but then you
don't have what you just mentioned yet the regularizer as a scientist what you really want to learn is
a model of the world that extracts the regularities in the environment that that
that finds predictable things which are regular in the sense that there's a short explanation there
of for example if you have falling objects in a video then they all fall in the same way they
accelerate in the same way which means it's predictable what these objects do if you see two
of the frames you can predict the third frame pretty well and the law behind that is very simple
this means that you can greatly compress the video that is coming in because you can
instead of storing all the pixels you can compute many of these pixels by just looking at two
successive frames and predicting the third frame or maybe three successive frames and predicting the
fourth frame something like that and you only have to encode the deviations from the prediction so
everything else you don't have to store separately which means you once you understand gravity you
can greatly greatly compress the video so that's what you really want to do and so the more advanced
version of artificial curiosity is about that where you have a motivation to find a disruption
of the data which is coming in of the video of the falling apples for example that is simpler than
the one that you had before so before you had the simple explanation of the data you needed
so many bits so many bits to um to describe the data and afterwards only so many and the
difference between before and after that is the reward that you get so that's the true reward
that the controller the first neural network should get in response to the improvements
of the second network which are now measured in terms of compression progress so first I needed
so many resources to encode the data but then I discovered this regularity gravity and I can
greatly compress all kinds of videos that that are reflecting the concept of gravity and certainly
I'm have a huge insight into the nature of the world and that is my true joy scientific as a
scientist my my true joy as a scientist that I want to encode in a little number which is
given as a reward to the guy who is inventing these experiments that lead to the data to the
data with the falling apples for example right well and of course this is this has been a challenge
in machine learning you know since the beginning which is okay as we add more and more parameters
how do we prevent it from learning spurious information with those parameters and instead
have it focus on parsimonious explanations on regular explanations on things that in this
universe are more likely to generalize you know to unseen examples and so I think my question to
you is does this setup that you describe is it a form of that and or what is the state of the art
you know these days for helping to push or nudge neural networks towards learning parsimonious
models for the world rather than highly detailed spurious susceptible to you know
high frequency anomalies and adversarial examples and all this sort of thing
yes what is the current state of the art in the regularizing
descriptors of the data such as neural networks such that you get simple explanations of the data
such that you get short programs that compute the data in other words such that the description of
the data is a short program that computes the much larger raw data and and how close can we
get to the limits which are given through this concept concept of algorithmic information
or comagor complexity comagor complexity of any data is the length of the shortest program
on some general computer that computes it since in our field the general computers are
recurrent neural networks we want to find a simple recurrent network that computes all this data
and given one computation of the data we want to find an even simpler one so we want to have this
idea of compression progress and here I have to say although we have lots of regularizers
invented throughout the past few decades there's nothing that is really convincing
I think one of the very important missing things is to make that work in a way that is
truly convincing that is as convincing as chat gpt is today in the much more limited domain of
generating text from previously observed texts and stuff a very old idea of I think the 1980s
was to have weight decay in a neural network which basically is the idea that all the weights
should have an incentive to become close to zero such that you can prune them
and so people built in regularizer that just punished weights for being large or being very
negative but that didn't work really well and something better was flat minimum search that was
1998 and first Arthur my brilliant student set book write that back then roughly the same time
when the LSTM paper came out and and there the idea is if you have if you plot the weights of
a neural network on the x-axis and you plot the error on the y-axis then given the weights you
have high or low error and then there is for example a sharp error function which has a sharp
minimum which which goes like that can you see my finger so here here is the x-axis here's the
y-axis here's the error and the error for a certain weight is really really low but then
for a different weight in the environment in the vicinity it's high again which would be very
different from a flat minimum which would be like this so here's the error and it's going down
and for many many ways it is low the error and then it goes up again so if you are a very sharp
well versus a very broad well yes a sharp well versus a broad well now if you are in a sharp well
you have to specify the weights with a lot with with high precision so you have to spend many bits
of information on encoding the weights of this network as opposed to a large to a flat minimum
where it doesn't matter if you you know perturb the weights because the error remains low
in this flat minimum so what you really want to find is is a network that has low complexity in
the sense that you can describe the good network so those with low error with very few bits of
information and suddenly if you maximize or if you minimize that flat minimum second order
error function then suddenly you have a preference for networks that that for example do this
you you have a hidden unit and the outgoing weights they have certain values but if you
give a very negative weight to the hidden unit then it doesn't matter what all these outgoing
weights do and flat minimum minimum search likes to find weight matrices like that where one single
weight can eliminate many others which you suddenly don't need any longer such that the
description complexity of the whole thing is much lower than in the beginning when you when you
just had a random initialization of all these weights so that is much more general than weight
decay because weight decay doesn't like these strong weights it wants to remove them but sometimes
it's really good to have a very negative weight coming to a hidden unit which is switched off
through that weight such that all the outgoing connections are meaningless but it's not um what
you what you it's very nice it's a very nice principle but it's not as general as finding the
shortest program on a university computer that computes the weight matrix that is solving your
problem to the extent how do you think we're how do you think we're gonna get to that point
how do you think uh what approaches are going to lead us to finding things that approach
comical of complexity yeah and i think that path has again a lot to do with meta learning and as
um a system is able to run its own learning algorithm on the network itself it can um suddenly
speak about the um
algorithms in form of weight matrices and it can discuss concepts such as the complexity of a
weight matrix and then you can conduct a search um in this space of networks that generates
weight matrices and then you suddenly are in the game so suddenly you are playing the right game
and then it's more a question of how to um choose an initial learning algorithm such as
gradient descent to come up with something that computes the simple solutions which you really
want to see in the end very recent papers on that on on aspects of that came out just a while ago
with my students vincent herman and louise kirch and um and francesco faccio and my poster kazuki
and robert joydash also um and also imann olschlag and there the idea is really to
have one network that computes an experiment and the experiment itself is the weight matrix
of a recurrent network so there is a generator of an experiment which can be anything that
describes a computational interaction with an environment so a program so that experiment
is then executed in the real world there's a prediction machine that predicts the outcome
of the experiment before the algorithm is executed and so then there's um just a yes or no question
either the following outcome will occur or not either it will occur or not but now the entire
setup is such that you don't have predictions all the time about every single pixel no you just have
something which is very abstract and which is just about whether a certain unit of the recurrent
network is going to be on or off at the end of the experiment and this internal on and off unit
can represent any computational question any questions that you can ask at all and now the
the task of the experiment generator which is another network which generates a recurrent
network weight matrix which represents the experiment the task of this experiment generator
is to again come up with something that surprises the um the prediction machine which looks at the
experiment and says yeah it's going to work or not and uh and suddenly you are again in this
old game uh except that now you have this world of abstractions where the abstractions can be
anything that is computable interesting really cool really cool could we spend the last 10 minutes
or so just talking about some of the the current ai landscape so in particular the capabilities of
GPT-4 and the moat building thing and and the the power that companies like uh google and open ai
have and um also the potential for open source so maybe we'll just start with the you know the
very current capabilities of GPT-4 are you impressed with it what do you think i'm impressed in the
sense that um i like the outcomes that you get there and um it wasn't obvious a couple of years
ago that it would become so good uh on the other hand of course and it's not yet this full AGI thing
and it is not really close to um to justifying those fears that some uh researchers sometimes
and now um document and um in letters and public letters and so on so to me it's a little bit
like a visa view because for for many decades i have um had discussions like that and people said
that you are crazy when i said that within my lifetime i want to build something that is smarter
than myself um and now suddenly in recent years um some of the guys who said it's never going to
happen suddenly they just look at chat gbt and they think oh now we are really close to AGI and
whatever uh so i i don't share these um extreme um
i'm less impressed than some of those guys let me say that right uh the open source movement
that you mentioned you you want to ask a specific specific question about that right
well yeah there was that famous google memo that got leaked and when the waits for loma from
facebook went out within about two or three weeks um it was a valing pretty similar to chat gbt
you know with this um laura fine tuning and the open source community has just exploded you know
you can now run it on your laptop and there is some question whether there is a significant
gap between the capability you know is is it just a parlor trick is it really as good potentially
or could it be as good as some of the next best models from open ai but i guess the question is
do you think that we need open ai to to have the best models no of course not um no i'm very
convinced of the open source movement and have um supported that some people say the open source
movement is maybe six or eight months behind the large companies that are now
coming out with these models and i think the best way of making sure that there won't be dominance
through some large company is to support the open source movement because how can a large company
compete against all these brilliant phd students around the world who are so
motivated to you know within a few days create something that is a little bit better than what
the last guy has um um put out there on github and whatever so i'm i'm very convinced that this
open source movement is going to make sure that there won't be a huge mode for a long time
i'm reading between the lines here but i would guess you would be opposed to
legislation like the eu is considering where you know very tight restrictions on
generative models you know onerous onerous kind of uh approval processes and things like that
because that's going to have this chilling effect on on open source innovation and the little guys
wouldn't it yes i have signed letters um which which support the open source movement and whenever
i get a chance to um maybe influence some you um politicians then i'm trying to contribute to
making sure that they don't don't shoot themselves in the foot by by by killing
killing innovation through the open source movement so you certainly want to avoid that
there are lots of different open source movements around the world so if one big entity fails to
support open source or even makes it harder for open source there will still be lots of other
entities which um won't follow follow and so no matter what's going to happen on the political
level i think open source is not going away i guess just in closing you've been in this game
for decades now and what is i know it's a bit of a strange question to ask but what's your fondest
memory in your career my fondest memory oh it's usually when i discover something that i think
nobody has seen before but that is that happens very rarely because most of the things you think
are well somebody else has done before um but yeah so
yeah um what usually happens is um you and and this has happened many times not many times but
quite a few times in my career since the 80s as a scientist who publishes stuff
but suddenly you think oh that is the solution to all these problems and now i really figured out
a way of building this universal system which um learns how to improve itself and learns the way
to improve the way it improves itself and so on and now we are done and now all is that's
necessary is to scale it up and it's going to solve everything and then um you think a little
bit longer about it and maybe you have a couple of publications but then it turns out something
is missing something important is missing and and actually it's not that great and actually
you have to think hard to add something important to it which then for a brief moment looks like the
greatest thing since sliced bread and um and then you get excited again but then suddenly
you realize oh it's still not finished something important is missing and so it goes back and
forth like that i think that's the life of a scientist the greatest joys are those moments
where you have an insight where suddenly things fall into place such that along the lines of what
we discussed before the description length of some solution to a problem suddenly shrinks because
two puzzle pieces they suddenly match and and become one or become one in the sense that
they fit each other such that suddenly you have the shared line between the two
puzzle pieces one is negative and the other one is positive and certainly the whole thing is
much more compressible than the sum of the things separately so these these things that's
what's driving um scientists like myself i guess
wonderful um professor you again schmidhuber it's been an absolute honor thank you so much
for coming on the show today thank you it was such a pleasure talking to you
