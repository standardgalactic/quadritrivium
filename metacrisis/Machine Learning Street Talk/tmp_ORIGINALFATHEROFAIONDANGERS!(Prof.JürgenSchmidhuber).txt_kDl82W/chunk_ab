an effective computation and so subject to those same limits or do you think there's any
known or unknown physics that give us some out in which the brain can do a computation that
amounts to hypercomputation? Since we have no evidence that the brain can compute something
that is not computable in the traditional sense, in GÃ¶del sense and torings and churches sense
and everybody who has worked on this field, since we have no evidence we shouldn't assume that's the
case. As soon as someone shows that people can compute certain things or prove certain theorems
that machines cannot prove given the same initial conditions, we should look more closely but
there are many things that might be possible in fairy tales and we are not really exploring them
because the probability of coming up with interesting results is so low. Fair enough,
so you mentioned so far two asymptotes, one being of the mathematical kind where there's just
mathematical proofs that certain things are optimal, the other one being the limits of
computation itself. What other asymptotes do you see applying to or putting bounds on recursive
self-improvement? The most obvious thing is probably light speed and the limits of physical
computation. We know those for several decades, we have happily enjoyed the fact that every five
years compute is getting 10 times cheaper and this process started long before Moore's law was
defined in the 60s I believe because even in 1941 already when Susie built the first program
controlled computer this law apparently was active so back then he could compute maybe one
instruction per second and since then every 10 years a factor of 100 every 30 years a factor of
a million more or less until today and there's no reason to believe it won't hold for a couple
of additional decades because the physical limits are much further out. The physical limits that we
know are the Bremermann limit discovered I think in 1983 by Bremermann and they basically say that
one kilogram of matter cannot compute more than 10 to the 51 instructions per second.
So that's a lot of compute but it's limited and to give you an idea of how much compute that is
I also have a kilogram of computer in here and probably it cannot compute 10 to the 20
instructions per second otherwise my head would explode because of the heat problem
but maybe it can compute something that is not so far from 10 to the 20 instructions maybe 10
to the 17 something like that although most of my neurons are not active as we speak because again
otherwise my head would just evaporate. Now if you have an upper limit of 10 to the 20 instructions
per brain then the upper limit of all of humankind would be 10 billion times that individual limit
and that would be 10 to the 30 instructions per second and you see it's still far away from the
10 to the 51 instructions per second that in principle one kilogram of matter could compute
and now we have more than 10 to the 30 kilograms of matter in the solar system and there's some
and so if the current trend continues at some point much of that is going to be used for
computation but then it will have to slow down even if the exponential acceleration
will still be with us for a couple of decades because at some point it is going to be a polynomial
because due to the limits of light speed at some point it will be harder and harder
to acquire additional mass once you have reached the limits of physical computation per kilogram
the only way to expand is to go outwards and you know find additional stars and additional
matter further away from the solar system and then you will get a polynomial acceleration or
a polynomial growth at best so it will be much worse than the current exponential
growth that we are still enjoying. Sure but I would say you know the existential threat
that is more than sufficient to supply an existential threat and let me just put this
a little bit differently which is and I agree with you on this which is you are quoted as
saying that traditional humans won't play a significant role in spreading intelligence
across the universe and I think you are right I think we kind of share a vision of something
like the von Neumann probes that go out into space and form this star spanning civilization of
machines and artificial intelligence that have transcended you know biological limitations
so I guess my question to you is once that space faring star spanning you know civilization
exists if it becomes misaligned with us and decides that we are in the way right isn't that
an existential threat I mean might they just you know repurpose the earth regardless of whether
we're here or not for for their own aims yeah I'm often getting these questions and
and there is no proof that we will be safe forever or something like that on the other hand it's also
very clear as far as I can judge that all of this cannot be stopped and it can be channeled
in a very natural and I think good way in a way that is good for humankind now
first of all at the moment we have a tremendous bias towards good AI
meaning AI that is good for humans why because there is this intense commercial pressure
to create stuff that humans want to buy and they like to buy only stuff they think is good
for them which means that all the companies that are and that are trying to devise AI products
they are maximally incentivized to generate AI products that are good for those guys who are
buying them or at least where the where the customers think it's good for them
so it is still 95 so it may be five percent of all AI researchers really about AI weapons and
one has to be worried about that when all this has to be worried about weapons research but
there's a tremendous bias towards good AI so that is one of the reasons why you can be
a little bit optimistic for the future I'm always trying to point out the two types of
AIs there are those who are just tools of users human human users and the others that invent
their own goals and they pursue their own goals and both of them we have had for a long time
now for the AI tools it's kind of clear there's a human and a human wants to achieve something
and so it uses he uses or she uses that tool to achieve certain ends and and most of those are
of the type let's improve healthcare and let's facilitate translation from one person to another
one in another nation and just make life easier and make human lives longer and healthier
okay so that that's the AI tools but then there are the other AIs which also have existed in my
lab for at least 32 years which invent their own goals and they are a little bit like little
scientists where you have an incentive to explore the environment through actions through
experiments self-invented experiments that tell you more about how the world works such that you
can become a better and better and more and more general problem solver in that world
and so these AIs they have for a long time created their own goals and now of course
the interesting question is these more interesting AIs what are they going to do once they are
once they have been scaled up and can compete or maybe outperform humans and everything
they want to achieve so on the one hand the AI tools and there the greatest worry is
what are the other humans going to do to me with their AI tools so in the extreme case you have
people who are using AI weapons against you and maybe your neighbor is has bought a little drone
for 300 dollars and it has face recognition and it has a little gripper and it flies across the
hedge and puts some poison into your coffee or something like that so then the problem is not
the AI which is trying to enslave humans or something silly like that no it's your neighbor
or the other human and generally speaking you have to be much more afraid of other humans than you
have to be of AIs even those who define or set themselves their own goals because you must mostly
worry about those with whom you share goals so if you share goals then suddenly there is a potential
of conflict because maybe there is only one schnitzel over there and two persons want to
eat the schnitzel and suddenly they have a reason to fight against each other generally speaking
if you share goals then you can do two things you can either collaborate or compete an extreme form
of collaboration would be to maybe marry another person and set up a family and master life together
and an extreme form of competition would be war and and those who share goals they have many more
incentives to interact than those who don't share goals and so humans are mostly interested in other
humans because they share similar goals and because they give them a reason to collaborate or to
compete most CEOs of certain companies are interested in other CEOs of competing companies
and five-year-old girls are mostly interested in other five-year-old girls and the super smart AIs
of the future who set themselves their own goals they will be mostly interested in other super
smart AIs of the future who set themselves their own goals generally speaking there is not so much
competition and there are not so many shared goals between biological beings such as humans
and a new type of life that as you mentioned can expand into the universe and can multiply
in a way that is completely infeasible for biological beings so there's a certain
long-term protection at least through lack of interest on the other side
okay brilliant there's a few things I wanted to touch on there we will get on to what it means
for goals to emerge from systems later and you started off by saying that humans will buy
products that make them feel good and Facebook is quite an interesting example to play with
actually because Facebook is a little bit like an AI system which is a collective intelligence
and humans use Facebook but they have some idea that it might cause them harm and the thing with
population ethics is we know that our moral reasoning kind of decays over space and even more
so over time and part of the reason why time is so difficult is because it's predictive we don't
actually know what's going to happen in the future so our kind of reasoning about establishing
what the value of something is is very very faulty and I think that's one of the reasons why
these people would say that we don't really know what's good for us I do completely agree with you
though that the problem I think is humans rather than AIs on their own yes these are good points
feel free to uh offer some thoughts yes I mean it it would that's a whole separate discussion isn't
it when you discuss the limitations of what's predictable and um and how people often fail
to see what's good for them well I think maybe so you've already you've already um said that
there's no proof that we'll be safe forever right like I mean there could there could come an
existential risk you know from AI so I think my question to you is do you have sympathy for
the folks who say we need to be putting more resources into researching alignment like we need
to develop the tools um in order to allow it to be easier for people to construct AI that is aligned
for the goals and to make sure that you know that it doesn't that it doesn't have unintended
consequences like in other words there may not be a proof that we can go forever and be
safe for AI but we at least want to develop the basic mechanics that we need to safely
develop and deploy AI don't we yes and I sympathize with those who um are devoting their
lives to alignment issues and trying to build AIs aligned with humans
I view them as part of the evolution of all kinds of other ideas that come up as not only
nations compete with other nations but companies compete with other companies
and shareholders of different companies compete with shareholders of different companies and so on
and so there is such a huge set of different human goals which are not aligned with each other
that makes me doubt that you will come up with a general system that all humans can accept
simply because if you put 10 humans in a room and ask them what is good they will give you
10 different opinions however I sympathize with with this goal and it's good that people are
worried and they spend resources on solving some of these issues in the long run however
I think there is no way of stopping all kinds of AIs from having all kinds of
goals that have very little to do with humans the universe itself is built in a certain way
that apparently
derives it from very simple initial conditions to more and more complexity
and now we have reached a certain stage after 13.8 billion years of evolution and it it seems clear
that this cannot be the end of it because the universe is still young it's going to be much
older than it is now now there is this drive built in drive of the cosmos to become more complex
and it seems clear that civilization a civilization like ours is
is a stepping stone on to war it's something that is more complex and
could I touch on a couple of things here the bootloader example is kind of where I want to go
with this so a lot of the ideas of this movement can be traced back to Derek Parfit who is a
philosopher he was a moral realist so he thought there was such a thing as a moral fact and I'm
a bit of a relativist myself and actually if you trace this tree of complexity and how humans
evolve over time we might just be a stepping stone to a kind of rich diverse transhumanist
future where we become the thing over time that we're so scared of and I think the lens that
we're using here about what's right and what's wrong is kind of like I was saying before it's
a snapshot of humanity now and we kind of think of it as just this monolithic single thing
so does it really work when you project out to how we're going to evolve in the future
but first of all humankind is not a monolithic thing so many of these
arguments go like we should not do that because of that we should not do that because of that
but there is no us there is no we there are only and almost 10 billion different people
and they all have different ideas about what's good for them and so for thousands of years we had
these evolutions of ideas and of devices and philosophies competing partially competing
and partially compatible with each other which in the end led to the current values
that some people agree with and other people over there they agree with different values
nevertheless there are certain values that have become more popular than others more successful
more evolutionary with more success during the evolution of ideas
and so given this entire context of evolution of concepts and accepted ideas of what should be done
or what is worth being supported and what's not worth being supported all of this has changed a lot
if we look back 200 years the average people in the west had different ideas of what's good
than today and and this evolution of ideas is not going to stop any time soon
just a final question on this and there is a very real existential risk right now
of nuclear armageddon a real risk right now and if i were a rational person
i would be devoting all of my effort into that and other risks associated so do you think it's
a little bit weird that so much focuses on this ai x risk to me it's indeed weird now there are all
these letters coming out warning are the dangers of ai and i think some of the guys who are writing
these letters they are just seeking attention because they know that ai dystopia are attracting
more attention than documentaries about the benefits of ai in healthcare and stuff like that
but generally speaking i am much more worried about nuclear bombs than about ai weapons
a nuclear bomb a big one can wipe out 10 million people a big city within a few milliseconds without
a face recognition just like that without any ai and so in that sense it's much more harmful
than the comparatively harmless ai weapons than that we have today and that we can currently
conceive of so yes i'm much more worried about 60 year old technology that can wipe out civilization
within two hours without any ai well i guess um since we're we're not really going to worry about
ai for the moment we can uh we can turn our attention back to discussing with you uh how
we develop ai so um you know i'm really curious with with just the really the the vast you know
breadth and depth of your of your knowledge over the the history of of ai and the state of the art
i'm curious you know which current approaches you're you're most excited about and or what's on the
horizon um that you know for any of our listeners out there are thinking about um going into ai
research machine learning research you know what may be um alternatives that aren't getting enough
attention should they should they look into studying and and perhaps choosing the research
at the moment the limelight is on um language models large language models which pass the
touring tests and do all kinds of things that seemed inconceivable just a couple of years ago
at least to some of those who are now surprised but of course that is just a tiny part of
what's going to be important to develop true ai agi artificial general intelligence
um on the other hand the roots of what we need to develop true ai also
come from the previous millennium they are not new and of course what you need is an
environment to interact with and you need an an agent that can manipulate the environment and you
need a way of learning to improve the rewards that you get from this environment as you are
interacting it with it within a single lifetime so one of the important aspects of reinforcement
learning what we are now talking about is that you have only one single life you don't have
repeatable episodes like in most of traditional reinforcement learning no you have only one
single life and in the beginning you know nothing and then after 30 percent of your life is over
you know something about life and all you know is the data that you collected during these first
30 percent of your life and now there is an infinite almost infinite possibility set of
possibilities of futures and from this little short experience you have to generalize somehow
and try to select action sequences that lead to the most promising futures that you can shape
