yourself through your actions now to achieve all of that you need to build a model of the world a
predictive model of the world which means that you have to be able to learn over time and to
predict the consequences of your actions so that you can use this model of the world that you are
acquiring there to plan to plan ahead and you want to do that in a way that isn't the naive way
which we had in 1990 which is millisecond by millisecond planning where you say okay now
I'm moving from A to B and the way to do it is first move that little pinky muscle a little bit
and move it a little bit more and move it a little bit more and then get up and so no you want to do
that in a high level way in a hierarchical way in a way that allows you to to focus on the important
abstract concepts for example as you are trying to go from from your home to Beijing you decompose
this whole future into a couple of sub goals you say a first important step is to go to the cap
station and get a taxi to the airport and then in the airport you will find your your plane
and then for nine hours nothing is going to happen and you exit in Beijing and have to find another
cab and so on so you you don't do millisecond by millisecond detailed planning no you have
high level planning to just reduce the computational effort and focus on the essentials of what you
want to do so that is something that most current systems don't do but for a long time we have had
systems like that and they are getting more sophisticated over time important you have a
predictive model of the world that is not just focusing on the pixels and predicting the how
does the video change as I'm moving my hand back and forth the video that I get through my camera
my eyes and so on and no higher level concepts that that reflect islands of predictability many
things are not predictable but certain abstract representations of these things are predictable
and so how can you discover these higher level concepts that you need to efficiently think
about your own future options and select those that are most promising in the single life
yeah yeah this is really interesting so we've been speaking with Carl Friston for example and he
talks about this collective intelligence where you have this multi-agent cybernetic framework
which is causally closed and one of the things we're talking about here really is not the model
itself people talk about chat gpt and it's just a model and people have configured it in arrangements
that have varying degrees of autonomy and in the future we will develop these collective
intelligences and they're not just predicting the actions and behaviors of other agents but even
the world that we're in is a computer to some extent so when you imbue agents with this kind of
creativity and autonomy that's the thing that I don't think people really understand what might
emerge from that it's related to this discussion about what kind of goals might emerge from that
do you have any intuition on what that would look like yeah let me give you just the simplest
example that we had in 1990 or 32 years ago of a system that sets itself its own goals and it
consists of two artificial neural networks and I know that Carl Friston is very interested in that
and only recently for the first time in my life I was on a paper where he was co-author
just a year ago and so back then it was really about a reinforcement learning agent and it
interacts with the world and it generates actions that change the world and then there is
another network which just is trying to predict the consequences of the actions in the environment
so the reactions of the environment to these actions and so that becomes a world model and
then what kind of goal was there which was different from traditional goals well in the
beginning this model of the world this prediction machine which is a model of the world a world
model knows nothing so it has high error as it is trying to predict the next thing as it is trying
to predict the reactions of the environment to the actions of the agent so as the second network
is trying to reduce its prediction error through gradient descent through back propagation essentially
the other one is trying to generate actions outputs that maximize the same error so basically
the goal the self-invented goal if you will of the first network is to generate an action
with whose consequences cannot yet be predicted by the other network by the model of the world
so the first network is generating outputs that surprise the second network
so suddenly you have an incentive where the first network is trying to invent actions
experiments that fool or that surprise the second network and that was called artificial curiosity
so now suddenly you have a little agent which a little bit like a baby doesn't learn by
imitating the parents no it learns by inventing its own little sub goals and it's trying to surprise
itself and have fun by playing with the toys and and observing new unpredictable things which
however become predictable over time and therefore become boring and then it has another incentive
to invent the additional experiments such that it still can surprise its model of the world
which in turn is improving and so on so artificial curiosity does that does that also
have the effect of making the network which is trying to predict does it have the effect of
making it more robust and more generalizable like almost a form of you know regularization
kind of built in in this pairing yeah you can build into that network all kinds of regularizers
an orthogonal concept which is also very important so that was just the first version that was
really in 1990 and then we have had a we had a long string of papers just on improvements of
this original concept of artificial curiosity so this old system is basically what you what you
now know as GANs Generative Adversarial Networks because the first network is generating a probability
distribution over outputs and the second network is then predicting the consequences of these outputs
in the environment and if you if the output is an image then the consequence can be either this
image is of a certain type yes or not no and then that's all that the prediction machine the world
model predicts in that simple case and you minimize the first network minimizes the same
error function that the second one maximizes so then you have basically a GAN but then you
don't have what you just mentioned yet the regularizer as a scientist what you really want to learn is
a model of the world that extracts the regularities in the environment that that
that finds predictable things which are regular in the sense that there's a short explanation there
of for example if you have falling objects in a video then they all fall in the same way they
accelerate in the same way which means it's predictable what these objects do if you see two
of the frames you can predict the third frame pretty well and the law behind that is very simple
this means that you can greatly compress the video that is coming in because you can
instead of storing all the pixels you can compute many of these pixels by just looking at two
successive frames and predicting the third frame or maybe three successive frames and predicting the
fourth frame something like that and you only have to encode the deviations from the prediction so
everything else you don't have to store separately which means you once you understand gravity you
can greatly greatly compress the video so that's what you really want to do and so the more advanced
version of artificial curiosity is about that where you have a motivation to find a disruption
of the data which is coming in of the video of the falling apples for example that is simpler than
the one that you had before so before you had the simple explanation of the data you needed
so many bits so many bits to um to describe the data and afterwards only so many and the
difference between before and after that is the reward that you get so that's the true reward
that the controller the first neural network should get in response to the improvements
of the second network which are now measured in terms of compression progress so first I needed
so many resources to encode the data but then I discovered this regularity gravity and I can
greatly compress all kinds of videos that that are reflecting the concept of gravity and certainly
I'm have a huge insight into the nature of the world and that is my true joy scientific as a
scientist my my true joy as a scientist that I want to encode in a little number which is
given as a reward to the guy who is inventing these experiments that lead to the data to the
data with the falling apples for example right well and of course this is this has been a challenge
in machine learning you know since the beginning which is okay as we add more and more parameters
how do we prevent it from learning spurious information with those parameters and instead
have it focus on parsimonious explanations on regular explanations on things that in this
universe are more likely to generalize you know to unseen examples and so I think my question to
you is does this setup that you describe is it a form of that and or what is the state of the art
you know these days for helping to push or nudge neural networks towards learning parsimonious
models for the world rather than highly detailed spurious susceptible to you know
high frequency anomalies and adversarial examples and all this sort of thing
yes what is the current state of the art in the regularizing
descriptors of the data such as neural networks such that you get simple explanations of the data
such that you get short programs that compute the data in other words such that the description of
the data is a short program that computes the much larger raw data and and how close can we
get to the limits which are given through this concept concept of algorithmic information
or comagor complexity comagor complexity of any data is the length of the shortest program
on some general computer that computes it since in our field the general computers are
recurrent neural networks we want to find a simple recurrent network that computes all this data
and given one computation of the data we want to find an even simpler one so we want to have this
idea of compression progress and here I have to say although we have lots of regularizers
invented throughout the past few decades there's nothing that is really convincing
I think one of the very important missing things is to make that work in a way that is
truly convincing that is as convincing as chat gpt is today in the much more limited domain of
generating text from previously observed texts and stuff a very old idea of I think the 1980s
was to have weight decay in a neural network which basically is the idea that all the weights
should have an incentive to become close to zero such that you can prune them
and so people built in regularizer that just punished weights for being large or being very
negative but that didn't work really well and something better was flat minimum search that was
1998 and first Arthur my brilliant student set book write that back then roughly the same time
when the LSTM paper came out and and there the idea is if you have if you plot the weights of
a neural network on the x-axis and you plot the error on the y-axis then given the weights you
have high or low error and then there is for example a sharp error function which has a sharp
minimum which which goes like that can you see my finger so here here is the x-axis here's the
y-axis here's the error and the error for a certain weight is really really low but then
for a different weight in the environment in the vicinity it's high again which would be very
different from a flat minimum which would be like this so here's the error and it's going down
and for many many ways it is low the error and then it goes up again so if you are a very sharp
well versus a very broad well yes a sharp well versus a broad well now if you are in a sharp well
you have to specify the weights with a lot with with high precision so you have to spend many bits
of information on encoding the weights of this network as opposed to a large to a flat minimum
where it doesn't matter if you you know perturb the weights because the error remains low
in this flat minimum so what you really want to find is is a network that has low complexity in
the sense that you can describe the good network so those with low error with very few bits of
information and suddenly if you maximize or if you minimize that flat minimum second order
error function then suddenly you have a preference for networks that that for example do this
you you have a hidden unit and the outgoing weights they have certain values but if you
give a very negative weight to the hidden unit then it doesn't matter what all these outgoing
weights do and flat minimum minimum search likes to find weight matrices like that where one single
weight can eliminate many others which you suddenly don't need any longer such that the
description complexity of the whole thing is much lower than in the beginning when you when you
just had a random initialization of all these weights so that is much more general than weight
decay because weight decay doesn't like these strong weights it wants to remove them but sometimes
it's really good to have a very negative weight coming to a hidden unit which is switched off
through that weight such that all the outgoing connections are meaningless but it's not um what
you what you it's very nice it's a very nice principle but it's not as general as finding the
shortest program on a university computer that computes the weight matrix that is solving your
problem to the extent how do you think we're how do you think we're gonna get to that point
how do you think uh what approaches are going to lead us to finding things that approach
comical of complexity yeah and i think that path has again a lot to do with meta learning and as
um a system is able to run its own learning algorithm on the network itself it can um suddenly
speak about the um
algorithms in form of weight matrices and it can discuss concepts such as the complexity of a
weight matrix and then you can conduct a search um in this space of networks that generates
weight matrices and then you suddenly are in the game so suddenly you are playing the right game
and then it's more a question of how to um choose an initial learning algorithm such as
gradient descent to come up with something that computes the simple solutions which you really
want to see in the end very recent papers on that on on aspects of that came out just a while ago
with my students vincent herman and louise kirch and um and francesco faccio and my poster kazuki
and robert joydash also um and also imann olschlag and there the idea is really to
have one network that computes an experiment and the experiment itself is the weight matrix
of a recurrent network so there is a generator of an experiment which can be anything that
describes a computational interaction with an environment so a program so that experiment
is then executed in the real world there's a prediction machine that predicts the outcome
of the experiment before the algorithm is executed and so then there's um just a yes or no question
either the following outcome will occur or not either it will occur or not but now the entire
setup is such that you don't have predictions all the time about every single pixel no you just have
something which is very abstract and which is just about whether a certain unit of the recurrent
network is going to be on or off at the end of the experiment and this internal on and off unit
can represent any computational question any questions that you can ask at all and now the
the task of the experiment generator which is another network which generates a recurrent
network weight matrix which represents the experiment the task of this experiment generator
is to again come up with something that surprises the um the prediction machine which looks at the
experiment and says yeah it's going to work or not and uh and suddenly you are again in this
old game uh except that now you have this world of abstractions where the abstractions can be
anything that is computable interesting really cool really cool could we spend the last 10 minutes
or so just talking about some of the the current ai landscape so in particular the capabilities of
GPT-4 and the moat building thing and and the the power that companies like uh google and open ai
have and um also the potential for open source so maybe we'll just start with the you know the
very current capabilities of GPT-4 are you impressed with it what do you think i'm impressed in the
sense that um i like the outcomes that you get there and um it wasn't obvious a couple of years
ago that it would become so good uh on the other hand of course and it's not yet this full AGI thing
and it is not really close to um to justifying those fears that some uh researchers sometimes
and now um document and um in letters and public letters and so on so to me it's a little bit
like a visa view because for for many decades i have um had discussions like that and people said
that you are crazy when i said that within my lifetime i want to build something that is smarter
than myself um and now suddenly in recent years um some of the guys who said it's never going to
happen suddenly they just look at chat gbt and they think oh now we are really close to AGI and
whatever uh so i i don't share these um extreme um
i'm less impressed than some of those guys let me say that right uh the open source movement
that you mentioned you you want to ask a specific specific question about that right
well yeah there was that famous google memo that got leaked and when the waits for loma from
facebook went out within about two or three weeks um it was a valing pretty similar to chat gbt
you know with this um laura fine tuning and the open source community has just exploded you know
you can now run it on your laptop and there is some question whether there is a significant
gap between the capability you know is is it just a parlor trick is it really as good potentially
or could it be as good as some of the next best models from open ai but i guess the question is
do you think that we need open ai to to have the best models no of course not um no i'm very
convinced of the open source movement and have um supported that some people say the open source
movement is maybe six or eight months behind the large companies that are now
