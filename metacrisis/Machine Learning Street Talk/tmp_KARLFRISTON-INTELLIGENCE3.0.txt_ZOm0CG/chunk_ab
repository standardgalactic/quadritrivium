theoretic probabilistic description of the world but I guess I'm sort of reading between the lines
and the twinkle in your eye is the idea that each one I get across is that maybe quite bad for
humanity is that the idea is it all that's what that's what he thinks and I mean he thinks
you're a computationalist you think that information is the kind of primary substrate
and in a way he's also worried that information is becoming the primary substrate but the big
difference between you and 3d is that he thinks that humans are special he thinks that we like
can't we have autonomy we choose our own actions you know we can't be replicated in silica so he
thinks essentially that there's no such thing as general intelligence that there's just algorithms
that perform skills and it's our humanity which is being truncated I see right okay well I would
be very sympathetic to that yes I mean we are special kinds of intelligence and one could
equip that argument with you know what is the definition of sentience what's a bright line
between you know a very clever thermostat or some machine learning artifact or a virus
and you and me I think there is a bright line and you of course you've just said what it is it's
the autonomy it's the agency it's ability to plan and the all the existential imperatives that
underwrite that planning and then we come back to curiosity so but if he's saying that
are the fact we are here realize the fact we are curious creatures and because we are
we populate a universe that comprises creatures like us we're all curious about each other
then I would certainly say yes that is a definitive aspect of us which is not found I think
I'm just thinking carefully I'm sure you can find examples but you know I don't in the kind of
artificial intelligence that we currently interact with in exchange with you don't have that planning
and curiosity they don't have the you don't have bait into the optimization framing of you know
what makes a viable or a good bit of intelligence you don't actually have bait in universally
this expected information can this curiosity and in that sense I think he's probably absolutely
right and in a sense the belief sharing getting to that the right kind of belief sharing of the
kind that the white paper was talking about is predicated on the notion that you will now be
able to equip sentient artifacts that we make with curiosity and maybe asking well what are
they going to be curious about their world what is their world it's you and me and the other
artifacts so they're only going to be curious about you and me they're going to be interested
in you and me so we're talking about you know a Siri or a Google Maps that starts to ask you
questions instead of you asking them questions so that I think that's that's one way of eluding his
his sort of rather dystopian attitude to the other the information is king I think information is
king belief updating is king the belief updating is you know of course the thing that ensues once
you act upon the world to do some smart data mining to you know to respond to some epistemic
affordances and you know the question then is you know what kinds of systems do that and the moment
I would argue it's probably just us there are other examples of some beautiful examples in
say active learning using machine learning you know to design your own experiments and
optimize the actual experiments and say drug discovery or molecular biology so I mean that
there is a long history both in statistics and in machine learning of active learning
that I think does have the potential but I don't see it really being
well from what I understand in discussions I don't see that being a bedrock of the way forward
an explicit part of the design for an age of intelligence that is puts us sympathetically
in an ecosystem of intelligence and that's really what that white paper was trying to think what you
know what would it look like and what would happen if your google maps became very curious about you
particularly I mean at google maps I think well there are two sort of sort of metaphors here
which might sort of ground the framework or the perspective that I'm trying to think about this
worry and one which I found very helpful again coming from my colleagues in industry is just trying
to explain the nature of shared intelligence and distributed intelligence and the analogy here would
be the brain you know you've got really smart little elements little neurons I mean they're
really smart and in fact if you get into the weeds of dendritic computation incredibly smart little
things that are in receipt of their sensations and they act by some pinging they don't know who
they're pinging but they're pinging away sending out little action potential or messages down down
the axons the wires that are emitted from from the nerve cells and you've got you know 10 billion of
these things and they're all very very smart but would you call any one of them autonomous would
be called would they have agency in an elemental sense I think they do but it's when you put them
all together lots and lots of little smart things together do you get this emergent kind of intelligence
that you could undeniably say has the capacity to roll out into the future to you know generate
fantasies or counterfactuals that are all conditioned upon what I'm going to do next where
I now becomes this collective so this is the kind of emergent behavior that emerges from
getting lots of little smart things to talk to each other in the right kind of way so that
that's you know I think quite a helpful analogy by what is meant between you know about shared
intelligence and what might you know might arise clearly a lot of these little smart brain cells
be analogous to you and me a lot of others would be all kinds of apps and you know
giving you now the potential to see through the eyes of any smart app that knows what you want
to know and is curious about what you want to you know to find out again coming back to this notion
that belief sharing I think is already there in the context of say you know sat nav you know I
share my beliefs about my preferred states of being my characteristic preferences are one part of the
you know the imperative suite for policy selection or good plans I share that with in terms of a
destination with some shared world model or shared narrative between me and my sat nav
in this instance of a geographical world model and then it has beliefs about the best policy
it makes a little plan and then it shares its beliefs with me again so I think that you know that
to the extent that your colleague in Oxford was saying we're already there I think that
that's absolutely right I mean you know we already have this kind of belief sharing
I think that the move though is to make that a much more symmetric belief sharing you know
I'm asking the app for its beliefs for its I'm asking it to behave as a recommender
and it only knows what I tell it so it has no autonomy but if it was now in a position to
actively smartly resolve its uncertainty about me the user then that's much more of a sort of
you know a balanced symmetrical dynamic interaction between me and the app and the agenda here
is not to create paper clips or make money the agenda remember is just to resolve uncertainty
the state curiosity and to move towards the state of greater mutual understandings
that's a sort of the non-dystopian view of information sharing so Shane Legg said that
his definition of intelligence is the ability of an agent and we're using words here like
agent we'll get to those words in a minute being able to solve a variety of tasks in different
environments Francois Chorlet said it's efficiently creating abstractions given limited prize and
experience Pei Wang says it's the adaptation efficiency over finite resources so when you
look at definitions of intelligence typically they focus on principle and function which I think
your one does or capability or behavior or structure now the interesting thing about the
principle one in particular is I think it's the least anthropomorphic and I think yours is the
least anthropomorphic definition I've ever heard of so and also this concept of grounding in the
physicality of information rather than reality itself and whether they are the same thing of
course is a philosophical discussion that maybe we'll park for another time but yeah how would
you contrast your definition of intelligence from these other ones well I think you've already
done that you've you've said it's you know it's a minimal essentially a physics based definition
of intelligence which requires you know a move or indeed a complete commitment to staying in the
space of information and information geometries and belief updating so having said that I think
all of those definitions touch upon some essential aspects of intelligence every one of them rang
true you know to my ears and I could read every one of those as being one key foundational aspect
of what would emerge if any self-organizing system managed to supervise and coexist with
other self-organizing systems from the point of view of you know of the free energy principle
or the sort of first principle approach so there are no axioms and no assumptions the question is
not you know what is quintessentially anthropomorphic the question is what emergent properties of
certain kinds of self-organizing systems would qualify as having that that bimimetic and then
ultimately anthropomorphic aspects to them you don't even have to go to anthropomorphism I mean if you
were talking as you have been to Mike Levin and his friends you know they would talk about basal
cognition they would talk about you know just a multicellular organism is a beautiful construction
of that rests upon orthopoiesis self-assembly of individual cells but also cells of cells and
you know how does a surface cell an epithelium know that he's on the surface and how does that
individuate the internal cells of an organ or an organ from the rest of the environment so that you
know the kind of intelligence that has this anthropomorphic feel I think people are also
seeing in biotic self-organization that would be a long way away from the kind of
folk psychology intelligence that we're talking about and yet it rests upon exactly the same kind
of mechanics and you know for me that would be the Bayesian mechanics that come from the
you know the dynamics of systems that are self-organizing open systems that are self-organizing
so you know you talked about so adaptation has been one aspect one common theme in the
the sequence of definitions you gave I think does speak to this bright line between
basal cognition and biotic self-organization biological intelligence of the kind you can
read in many many different ways your your DNA for example your genotype is an intelligent
information accumulating device on the point of view of evolution you know it it stores all
the information about what it would require to build a phenotype that's fit for purpose in this
particular environment so you know that's a kind of belief updating that's a kind of intelligence
but it doesn't have what we were talking about before which is this capacity to plan
evolution doesn't plan you could also argue that the the worldwide web doesn't plan
google maps you could argue does plan to a certain extent because it's certainly
so what's the difference the difference is I think implicit in at least the first two of your
of your definitions which is this notion of counterfactual futures this notion of
you know imagining a future or putting it another way having a world model or a generative model
that explains things that are not tied to the moment so if you're a physicist what you're
talking about is now a probabilistic mechanics a Bayesian mechanics or possibly just you know an
information theoretic mechanics based upon paths through time so we're talking about things like
the pathological formulation and you know but crucially we're talking about trajectories that
don't cannot be localized to this point in time that necessarily entail the future and indeed the
past so I've you know that notion of freeing yourself in a you know I can't remember the
name the philosopher now I can't remember the name I just can't pronounce it so I'm going to pretend I
can't remember well there are people who there are there are the philosophical schools that
emphasise this temporality aspect now you know and if you just look at physics look at contributions
of Richard Feynman for example it you know it's all about the pathological formulation
and so I think as soon as you talk about the elements to which the information geometry
as an intelligence and autonomy all of these things could apply are not states they are
trajectories dynamics narratives paths that have this this sort of future pointing aspect
um then being able to select among different futures becomes an emergent property of this
kind of sense making this kind of autopoiesis under you're read as a Bayesian mechanics of
self-organisation and just thinking about your definitions they all have that aspect of choosing
among different futures or considering or having abstractions um you know about what might happen
if if this so for me that that is one way of expressing curiosity because to be curious you
have to imagine well what would happen if I did that and what would I know if I did that but you
have to imagine it before it's actually happened which you know is is for me the big bright line
between you know between the anthropomorphic kind of intelligence and the intelligence you find in
the thermostat or in the you know a variation autoencoder yeah um later on we will decompose
the different aspects of cognition and I think as well as thinking it can be knowing and acting
as well and and we'll we'll talk to that but you said words you know action agent per se
goal plan behavior and I guess and I posed this to Levin as well it feels like these are terms
that we understand because we have cognitive priors like agentiveness and so on these are
things that that we understand that might actually only be a lens into something far more complicated
and just to touch on the information traversal points over a geometry that's very interesting
and I'm no expert but I think the medial temporal lobe deals a lot with spatial
contiguity and we have grid and place cells etc etc so it you know at a macroscopic level
in our brain it's a first-class citizen but there's also this hierarchy isn't there
when does it start happening you know do single cell organisms plan into the future
and to the previous point is planning necessarily a reductive lens of intelligence
you brought up loads of interesting things there you you call me by the reductive lens that's a
lovely phrase what does that what does that mean well um I didn't mean it in a purer sense but
when when we use words like you know like we we we say intelligence must do x y and z it must plan
it must reason it must act and we have this cybernetic loop and so on and I I have a theory
that this is just the way we understand things I see and and in a sense it's a lens onto a much
more complicated thing and and the reason this is interesting is the reason why we have people
like john so who says that the um the impenetrable realm of the subjective experience is beyond
function dynamics and behavior it's a little bit extra and and even and morality is another example
talk about that later that feels like it's something which is a little bit extra so there's
always this question of to what extent is intelligent behavior deducible from the models and the
words that we use yes I think that's a fundamental point uh you said um okay now I understand what
a reductive lens is I like that I like them I like it for many reasons of course because um
well first of all um it certainly is not uh you I don't think it could ever be used in a
pejorative sense um but it does speak to um two fundamental themes which is um the the way that we
do make sense of our world through coarse-graining through reducing uh through having um intuitive
models of the way that the world works um ultimately could be um um seen as language
words um could be I think could also be seen at quite to kind of as physics and maths as well
to be quite honest um you know these these the more I read about you know modern mathematicians
and physicists talking about that you know their skills and and the the legacies that they they
enjoy the more I I realize it's all changing all the time it's just another kind of reductionism um
but language particularly but it's a right kind of reductionism and I guess what you're saying is that
you know um we have this um way of summarizing classifying certain kinds of behavior which may
not truly reflect the underlying complexity the beauty of what's going on underneath
that at that that point I would go the other way I would actually say that do not properly
reflect the underlying simplicity of what's going on to be quite honest you know this comes back to
you know the um unashamed use of phrases like sort of existential imperatives and self-evidencing
yes yeah we're just here we're just we just have characteristic steps and sets we're just
realizations of some glorious um launch of our equations and all these stories about sort of
belief updating and sentience and intelligence and just reductive stories that make sense of
you know what we you know what um what we must um or can if indeed to a certain extent the free
energy principle itself I think is a reductive story of that kind you know when I say if you
look at things through the lens of Bayesian mechanics or as if I think the free energy principle
is another example of of this kind of reductive thing it's a looking at something which is inherently
much more simple than the lens through which you're looking at which is the Bayesian mechanics
and the free energy principle so I think that's absolutely right a really interesting idea um
um so uh and at another level I think it speaks to some key issues you know I mean you're um
you you're often um then confronted with you know okay I'm talking with you about agency
and agents and me and you uh so what license is that what aspects of my implicit world model
or generative model endow me with a sense of me and you and indeed me as agent and you know what
would that look like if I stripped away um different levels of meta awareness or meta uh
cognition if you're a psychologist and uh and just had a minimal selfhood um you know is just be
having plans um sufficient to call me an agent even if I don't know if I'm an agent if I have plans
