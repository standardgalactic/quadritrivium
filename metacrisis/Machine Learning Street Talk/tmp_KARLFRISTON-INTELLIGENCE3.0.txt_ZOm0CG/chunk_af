things that possess Markov blankets or have a separability um um uh what will they ultimately
do they'll ultimately find a synchronization manifold they'll find uh a synchronization
simply because um this is the um the most likely state of being and the most likely
state of being is that which maximizes the marginal likelihood which minimizes the free
energy which just means that they now understand and can infer each other so you know it's just
another expression of this existential imperative to resolve uncertainty have good models of my world
and my world also needs to have good models of me and if my world is you you have to have
a good model of me and I have to have a good model of you ultimately what that means is
we're going to converge on the same model um yeah I mean what interests me there is that
unlike many other researchers you're conflating understanding you know knowing and intelligence
itself I guess I guess so yeah um so good question um if intelligence is a process of belief
updating then yes it's just intelligence and learning and just learning knowledge and intelligently
inferring states of affairs are just the same process at different time scales and they both
depend upon each other so I have to have the right neural network of the right weights and the right
learning to make sense of the data in the moment and if you think of it from the point of view of
weight learning um either through propagation of errors or through experience dependent or
spike timing plasticity in the brain you have to have the right inference in order to do the
learning so there's again this circular causality between between the two scales so knowledge
requires the right inference or state estimation and state estimation requires the right knowledge
to make sense of the data that's being assimilated yes although I suppose philosophically we could
break it down because you know knowledge might be the sort of the information acquisition
because knowledge exists it's quite an interesting philosophical point actually that
knowledge is on Wikipedia and it's crystallized knowledge it exists as a thing but the ability
to to acquire knowledge without surprise is is is your your form of intelligence but I wanted to
talk about chat gpt just quickly and it's very interesting because Bing have just released
a new version of their search engine which integrates chat gpt and I've I've been on a bit of a journey
when they released gpt 3 in the it was November 2020 I got access to it I thought it was garbage
it was just generating a load of rubbish basically but there were people out there who were true
believers and they said Tim you're not seeing it I you know I I've seen it and they would show me
these ridiculous examples and I just thought no you're just fooled by randomness and and then
DaVinci 2 came out about a year ago and then that that transgressed the anthropomorphic
fooled by randomness threshold so I started to be a bit of a believer I knew it didn't understand
anything but it started to get very useful and I was using it for coding and generating emails and so
on much much to my loss actually because recently I've been checking code into the to the repo and
my colleagues have been saying to me Tim it looks like you used gpt to generate that why is it full
of holes and you have to hold your hand up and oh yes I'm sorry I just I just checked in some code
that I clearly didn't understand and you know didn't actually save me any time so sometimes you can
see problems with its generation but it's so plausible that most of the time they're hidden
and unfortunately if you want to verify that knowledge you might as well have just not bothered
using gpt in the first place because you could have just gone so a little recap it's using a
self-attention decoder transformer and that's a neural network architecture that introduces
permutation invariance to tuple permutation invariance which turns out to be extremely
useful for language and then there was this discovery of what's called in context learning
which is where rather than just getting it to because it's a generative model it just generates
token token you you insert a prompt and then continue to generate from that prompt and people
discovered you could ask it questions it had this emergent reasoning capability in big air quotes
and then more recently people have done what's called preference fine tuning which is that you
do some additional supervision on the top with human reference examples and that aligns it to
humans and makes it give slightly more politically correct or you know more sensible answers so
and now it's been integrated into gpt and that does this retrieval augmented generation which
means rather than just being a snapshot in time it will also go out go out to bing get some relevant
search results incorporate that into the prompt and then generate from there and there was this
incident a couple of days ago where Bing had this successful launch to much fanfare
and then people looked at the results it was generating including fine you know one of the
the things was give me a comparison between the financial results of lulu lemon and some other
company and it was just hallucinating the results it gave weren't even in the document
and the product managers at microsoft didn't even themselves bother to check the truthfulness of
this generation so god help anyone else using Bing um so what's your take cut
i love that story thank you um as we were talking about before i i've heard so much about chuck
gtp but i haven't been able to get on it because because it's always always been used over some
subscriber it's a it's a wonderful moment isn't it and so many issues there um i don't know where
to start with um perhaps i should start um by um conversations i've heard um about why the
chuck gtp moment is so important um always reduce really to um the fact that people got
in you're enchanted and had access to it so it wasn't so much other but it's actually really
interesting to hear about the technological and the structure of the genetic model makes it work
i didn't know that that was that was very useful but whatever you know the those are not really
sort of quantum leaps they're not massive technological you know innovations um but what
the innovation was of the accessibility so i think you know just standing back why that was a
moment and it has been a moment i think you know in terms of selling ai to investors and the like
they are they all now oh you're talking about chuck gtp like stuff i know about that that's
really exciting uh so it has changed the landscape i think uh there has been a moment um but why did
it happen i think it's basically all this belief sharing i think it's you know basically uh the
participatory aspect it is exactly um this um if you like sort of emphasis on belief sharing among
lots of smart agents including ourselves which is you know which if you can realize that potential
and getting people engaged uh is the is the nice way to use uh artificial intelligence um and in
that respect you you ask yourself well how is it being used um and it's being used as gerative ai
and of course you've asked yourself well okay what what's gerative ai got brought to the table
well it's generating the kind of stuff that i would see um and you're basically it's an
interpolation machine you know sort of if i give it enough stuff it'll interpolate and generate the
kind of things that i've given it um so why is that useful well you can now select from the stuff it
generates uh and you know but all of this game is all quintessentially dyadic interaction or
you participating with the generative ai that's why it's so attractive it's not the marvelous
stuff it generates it just interpolates stuff um you know the interesting bit is when you now
have the opportunity to select oh i like that one i don't like that one i'm going to check that code
before or i'm not going to check that code before before putting it in um so i looked at from that
point of view i think that both those if you like the um why it became so much foregrounded in people's
conversation and in the media uh and why more generally people are have been enchanted by
generative ai i think they both speak to the fact that you're actually engaging people
it's a dyadic exchange um of an asymmetric sort and that asymmetry um is exemplified by
generative ai that there's all the action all the choosing all the triaging all the selection
and what to actually show your friends or send off in your email is done by you the human user
so all the inactive bit is actually done by the human still you know the generative ai in and of
itself is not actually acting because it's generating content the other interesting thing about
generative ai is that it's generating content not beliefs so unlike google maps which actually
gives you a belief about the you know the best plan forward um it's actually generating content
it's in data space so from the point of view of a statistician or from the point of view of
a physicist committed to a um a holographic screen or markov blanket formulation of exchange with
the world um notice that generative ai is doesn't have doesn't need to understand because that's
not its purpose its purpose is to generate sensations to generate data to generate stuff
in content space or data space stuff that has been mined in the space that the mining took place
not in the sense making and the the abstraction and the understanding space so you know i i think
that's an interesting distinction which which um that you know i'm sort of going off an attendant
here but it's interesting when it comes to what do you mean by belief shurning and communication
but just look at that um that observation in light of the discussion about why
gtp is so successful it's successful because it generates language so the content now is
the belief and it's the kind of belief structures that have been honed through probably not is
yeah certainly millennia of cultural niche construction and uh so your language is a distillation
the most efficient way that we can carve up our knowledge of our world our lived world
and now the generative ai which was previously just limited to generating pictures and content
and sound files and whatever um is now actually generating stuff which is it in a belief space
because we have evolved language so i i think there's something quite special about generative ai
and large language models simply because they actually generate content in in the context of
language which you know has this um speaks to knowledge uh also it has abstracted and distilled
the kind of representations of of our world yes you're in the most efficient way uh
that yes i mean there are so many things we can say that they are a materialized snapshot of our
sense making our abstractions our world knowledge you know of the wittgensteinian language game if
you like but they also have a truncating effect and they introduce inertia because it's a static
model right and they are as you say that they produce traversals in word space and people
don't understand that these are random trajectories with some kind of modified form of maximum
likelihood estimation uh much more stochastic than people realize and we can discuss the degree of
how creative they are and what creativity is maybe creativity is just a random traversal
through some abstraction manifold and i i loved your poetic description of this kind of didactic
relationship between humans and machines much like an extended mind and this is where prompt
engineering comes in because people have realized that you can say to the language model that's not
quite what i wanted can you change it a little bit and it's an interactive process and that's why
as a conversational interface it's very very powerful but the problem is you can say to it
no uh two plus two doesn't equal four it equals five and it will say oh i'm so sorry i actually
meant five so it's it's polluting the infosphere with misinformation false news probably getting
out all this kind of stuff and i never really thought the the misinformation thing was a problem
because you know there's loads of misinformation out there i mean most people are full of shit
frankly uh carl but now it's been industrialized and democratized on this scale and people will
not bother fact-checking they'll that people see plausible text and they just take it as a given
and very very soon there'll be so much information out there on the internet more than was generated
by humans most of it will be generated by machines and we won't know the difference
and that reminds me of you know one thing which um the ambivalence that that whole issue induces
in people so you know this this um this this tendency to write in meaning an anthropomorphic
size that you know the the the content generated by generative ai i've heard um actually by the
by the second author of the white paper we started with it smashed the chewing test you know
and i guess it has i guess it has smashed the chewing test uh um but as you say there's
a price to be paid if you can't discriminate between sort of uh you actually had a nice
word written i hope you're going to use uh a game which is confabulation yes yeah you know
that's a great way of describing it so when i was talking about sort of interpolating generating
content novel content that is an interpolation it's you know that would be a confabulation it's yeah
i'm just you know mindful of the the fantasies that that were generated by uh jeff hintons weeks
sleep wake algorithm also you know the original the originals of amortization of
of um well yeah very short on coders i guess you'd you'd you'd think about nowadays um but the
you know this notion of confabulation i think is is a splendor i haven't heard it i haven't heard
it expressed like that but but that is a beauty of generative ai i guess what you're saying is if
people misinterpret that as real information that could be problematic um i'm too i i'm being
a bit older than you i'm slightly more mellow about this i'll just very quickly tell you a little
story i had to for a friend or a colleague at least an email colleague um in america i agreed to uh
write some blurb for his um uh 200 plus word book which is a philosophical model but he's also very
informed in terms of artificial intelligence and he sent it to me so i speed read it at the weekend
in order to write a three or four sentence blurb for the publishers um and halfway through i suddenly
had the awful realization that this may have been written by cha chi tp oh no it's a wonderful book
and i wasn't quite sure so i actually put in the blurb this this is a 21st century Turing test
uh there you either and you know i'll just advertise it now because this will probably come out by
the time people watch this i think it's called the hidden illusion uh and either this author was
very very skillful in writing the book last year and pre-empting um the public release of the of
these things pre-empting to the extent he could emulate the confabulation of a large language model
because part of the novel actually because the the protagonist the hero is actually working as a
on large language models for a tech startup it's a love story yes but it's interwoven with things
that he's actually generated on his large language machine yes and he um and that comprises part of
the model um but i i generally don't know whether the rest of the narrative was was actually written
by a large language model and then he's carefully gone through and triaged it
that's entertainment but it is entertainment that that really challenges can be viewed as a
Turing test which i suspect most people will fail and i suspect that book will be talked about
simply because it's very difficult to tell how much he wrote versus how much the machine wrote
and you know so but that you know but as long as it's kept to entertainment that's fine if it's
not then we come back to smart data mining we come back um to intelligent agents that just
don't confabulate content in the context of generative AI you actually need um the ability
of smart agents to go and you talked about fact checking what does that mean this basically means
having an explanation or a belief at hand that provides an accurate account of all the data
that is internally consistent so okay we're coming coming back to the the fundamental
principles of good modeling that can be quantified but to do that you're going to have to equip those
agents with autonomy and so to make equip those smart data mining machines with autonomy to be
able to select the you know the right kind of data that will hopefully preclude the you know
the confabulated data or things that look like data but in fact not uh not data i don't know how
you do that but that's going to be the challenge for the future well exactly i mean in the free
energy principle and that you have this entropy and and and it will actively seek out gpt never
says to you oh i don't i don't actually know that can can you explain can you explain more
different modes of understanding right so the reason why we don't confabulate is because we
actually understand things and these models just learn very very superficial surface statistics
um about language and and how it's used and i think it's going to change the the peer review
process because now so much of this stuff slips um beneath the net and the amount of um due diligence
and rigor that is required to weed out some of this stuff because most of it has gone undetected
i think that's the problem people don't realize how big the problem is because the mistakes are not
immediately um obvious on on the surface so um you said that we believe that developing a
cyber physical network of emergent intelligence in the manner described above not only ought to but
for architectural reasons must be pursued in a way that positively values and safeguards the
individuality of people as well as potentially non-human persons and i wanted to bring in the
is ought problem as articulated by david hume and he said it arises when one makes claims about what
ought to be that are based solely on statements about what is and hume found that there seems to
be a significant difference between descriptive or positive statements about what is and prescriptive
or normative statements about what ought to be and uh that it's not obvious how one can
coherently move from descriptive statements to and prescriptive statements and i do want to draw
a little bit of an analogy here to our discussion about consciousness and i i i can bring in consciousness
again but you know charmers said there's this kind of um hard problem and consciousness is a little
bit extra and similarly um people say the same thing about morality that it's a little bit extra
and it might not be deducible from all of this empirical data that we have in our in our models
