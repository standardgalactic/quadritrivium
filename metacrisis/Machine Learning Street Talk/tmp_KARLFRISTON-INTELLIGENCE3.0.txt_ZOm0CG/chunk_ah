you know carving nature in its joints in our head and part of that is is not just about the
state of the external extraceptive world but also our internal world our interceptions our
gut feelings our respiration and everything else so so her big thing and indeed other people
thinking along similar lines people like Anil Seth and sorry well would really always how
nod towards interception and embodiment but now beyond the the situated cognition kind of
embodiment you we were talking about before but actually now about the the physical body the the
the beast machine as as Anil Seth would would say so the physiology so having a constructed
explanation or hypothesis for the way that I put together my gut feelings my interceptive
signals with situational awareness derived from extraceptive sensations and indeed what I do about
I think leads to a very compelling notion of constructed emotions so for example
I am I can infer I can use the explanation on the hypothesis I am frightened as the best explanation
for why my heart is racing why I feel frozen proprioceptively why I have cardio acceleration
and why I have why I cannot discern that dark figure in this dark alley in a city which I've
never been in before all of these myriad of sensations and my low level constructs now
succumb to a simple explanation oh I'm frightened yes that explains everything explains my racing
heart explains this fact I can't I can't see who that is your the potential predation that
would follow from from that also interestingly because of you got this circular causality in
the inactivism the fact that I am frightened means I expect to cardio accelerate and of course under
active inference that's exactly what will happen because you're acting to generate the evidence
for your predictions and for your your your hypothesis about your you your you and your
lived world but also you as you hypothesize yourself to be so you've got this sort of
closing the circle in a sort of James Langian kind of sense the yes I explain my my current
set of sensations as having the emotion of fear that itself induces the very evidence that I was
trying to so you've got this sort of auto poetic self-fulfilling prophecy that you know
is just idea motor theory but in the interceptive domain yes so I think if you read constructed
that's right but I notice he says socially constructed which I I guess is okay yes if
you're taught that you can be have these you know this kind of fine grain repertoire of
feelings or you can use these to explain your own sensations yes I I would imagine it is molded by
you know by by convention and you know by the culture which you come come from in the same way
that you know Eskimo is having 12 words for snow would give them a finer visual acuity and
visual discrimination of whiteness I I'm sure that exactly the same kind of cultural inculturation
speaks to different kinds of elixir thymia and you know the coarse grainness of my
repertoire of explanations for emotional states of mind the best explained me in my
you know in my interactions now car I know you have a background in psychotherapy
can people be evil it doesn't exist in the diagnostic criteria I believe
no it doesn't um so that was out of the blue question which I've never
been asked in public before um as a psychiatrist I think it would be rather difficult
to conceive of that um there you know there are certain there are certain patterns of behavior
and um I do have some psychotherapy but it's group psychotherapy but really I'm a psychiatrist
so you know there's a distinction I think right um from a professional point of view
you know I'd be more like a biological my apologies I didn't mean to
honestly no no I mean the psychotherapy is an important aspect of of psychiatry it's just that
psychotherapists have to undergo you know five six years of training to become psychotherapists I
did two two years of very baby uh psychotherapy training um but um you know in terms of the
neurology and psychiatry you can certainly get sort of certain kinds of personality disorders
and certain kinds of psychopathy that would normally be associated with evil um evil behavior
and it normally basically um transcends the social norms so it comes back again to basically me
trying to work out what kind of person do you should I be which you know my only point of
reference is you so you know how should how do I think you think I should behave and when that um
when that kind of self-modeling doesn't work then you will be you know you will have um
behaviors which are so far from the social norms and morally acceptable I guess you can label them
as being evil when could they arise well when you fail to um have any theory of mind for example
if I am an able to see you as a another thing like me so I may see you as a French or a car or
some sort of your camera an artifact um you're not a person you know you don't have intentional
dispositions or you don't have we don't have a share now that I couldn't talk to you in in any
really deep sense um then obviously I can never ask the question how do you think I should behave
because you do not you know you don't have that kind of belief or that kind of
intentional stance in relation to me um so you could imagine that some you know some
kinds of um psychiatric conditions that preclude proper theory of mind and you know
the ability to sympathize or empathize or bond would enable the expression of certain behaviors
which would be regarded by other people as evil whether the person prosecuting them thought they
were evil or not would cause be a mute question one you never know because it's inside her mark
off blanket but uh but also uh from her point of view there is no reference and that is the problem
yes there you are um but you can certainly have somebody else out from the outside saying that
is evil um you normally don't you try not to do that when you're doing psychiatry or psychotherapy
you have to have conditional positive regard yes so you you can't really impute you know nastiness
or evil or bad intentions um yes and it's one of these things where um autonomy comes into it
free will comes into it possibly it it you know evil itself is a constructed concept um which
exists in in our language and and it's something which some people will perceive depending on
lots of other things they believe yes but um okay and talking about ethics in AI
it seems to suffer from a similar um form of fractionation in the sense that different people
with different beliefs think that it should be enforced in in different ways what's the solution
well i'd take a um sort of deflationary approach um and it won't be a terribly informed approach but
you know my answer would be well if you get the right um the right optimization the right
imperatives in play then that kind of question just goes away um i think um one way that i have heard
this discuss now i actually enjoy discussing this with you know with with my colleagues and friends
is that there is this dystopian meme you know the singularity the paper clips paradox everything you
see and you know sort of um you know on comic films of a futuristic dystopian dark very entertaining
i mean that's the first things i go for when going to the watch list but but but they they are all
dystopian in a rather unconvincing and fantastical way um and you have to ask yourself well why
why are they um why do people have this sort of dystopian um uh view of um realizing the potential
of um you know what used to be called you know AGI or um and um i think it usually inherits um
from that distinction when you were you introduced the is and the ought yeah so what should a good AGI
what ought it to do and who's in charge of saying who what it ought to do is it meant to make money
is it meant to make profits is it meant to save lives is it meant to um make paper clips um
of course that question just goes away if you're thinking um in terms of um the free energy principle
and an active inference and belief sharing so the only agenda in sharing beliefs is to resolve
curiosity so you know you know i i cannot prescribe what you should do or what an intelligent
artifact should do um other than put constraints on every kinds of outcomes that are expected to
encounter so i can certainly write down constraints in the in the spirit again of either um a multiple
constraint satisfaction from a sort of um uh an engineering point of view or from a mathematician's
point of view the constraints inherent in uh jane's constraint maximum tree principle which is another
way of reading the free energy principle writing down those constraints but within those constraints
so these are the no-go areas you were actually talking about you mentioned explicitly before
so there are certain things you never do um or put it another way uh with relatively high precision
there is a these outcomes are highly implausible and if you find yourself in these outcomes you
you remove yourself immediately so that's quite easy to write down but then within those constraints
you know what is imperative it's just to gather information about what about you it's just showing
an interest in you so become your psychotherapist uh you know well could could i um push back a
tiny bit so i think part of the reason why we have this focus on ethics is because of the
centralization of ai um things like facebook they're controlled by centralized corporations
and what you're leading to is far more interesting it's this multi-agent diffusion stratification
fractionation nicely and that's i agree in many ways might solve the problem because it would it
would emerge and then you can discuss whether morality was part of why we survived it it's not
orthogonal but then i might still push back and say well what if the wrong thing emerged so what
if we do need to introduce governance because in in this active inference multi-agent setting with
humans and machines um we started to see behaviors emerge which we didn't agree with um how could
we then place value pressures on on that um that's an excellent question and you also make that very
important point which i think needs to be foreground that yeah so when i was talking about
belief sharing and distributed the the age of intelligence you know it was exactly this
distributed ecosystem a democratized kind of belief sharing and data sharing um where the
data is small bits of smart data that are essential to to to reduce and so it was very much this
walking away from large data monolithic bits of say large language models for example that can
scrape data from you know from wherever they can get it so that i think that's an important
distinction which which which um um qualifies my um dismissing of all these dystopian outcomes so
i'm assuming that that so spending a billion uh or a million dollars on get training some some
deep neural network is not going to be happening in the future and and we're going to be buying
cheap and cheerful edge devices and little apps that are smart and just and just you know
go go and get the data that we need to know in terms of resolving on-center about what about
you know what what what we are going to do next um but um so so i thought that was a really important
point and of course i forgot your your major question which was what which was the oh well so
if we did have let's say en masse yeah what do you do about that yeah um well again i'm not sure
i'm going to give you a terribly informed response but you know i think the notion of an ecosystem
is quite central here and one if you like um tenet of that white paper was a nod to natural
intelligence and natural processes and what has actually happened what does actually happen in
as a natural scientist so if you have um if you do imagine uh you're an ecosystem of intelligence
in the future it will be subject to exactly the same um dynamics and pressures that that we have
currently in terms of you know um cooperation and competition and wars and geopolitical issues
that that's that's part of the ecosystem and then you will have the normal problems of
democratization and access and um so um there's you know you cannot prescribe aughts for this
because then you have to choose who's going to prescribe the aughts so you have to have a very
libertarian approach to this so the emphasis and now it's not really me talking so much but now the
the architects of the future the generation below who are sort of you know thinking about
the legacy they're going to leave their children um so a lot of emphasis um is from what i see in
conversations i have in my world which may not be um you know microsoft or or you know big tech
who are still focused on big data um in my world it's much more upon um putting constraints in
place that preclude the the the the the emergence of autocracies that um resolve uncertainty about
others by making them all like themselves basically that's one way to get harmony is to
make everybody do what um do the same thing um so but but i you can write down uh so there's a lot
of attention is being paid and i'm talking here about the the next generation of uh of message
passing um that will support that information sharing and belief sharing so a lot of attention is
being paid not just to generalize it from just hypertext but into a sort of more abstract hyperspace
so literally hyperspace message passing and uh sorry um languages and transaction protocols
but also the credentials and the contracts that underwrite that message passing so you know
a lot of emphasis on contracts shared agreements in terms of what data is shareable and who has
the credentials to share that and and having that distributed so not in a in a blockchain sense but
you know uh in some workable um shareable sense so i think if if if one gets the standards right
and a lot of work is being currently done um um under the auspices of the iEEE for example with
the spatial web foundation one gets that right that i think that these catastrophic dystopian
abuses or your emergence of autocracies in an in an age of intelligence will um will be precluded
simply because you've put the right constraints in in place but you know given given that you are
committed to creating an ecology that is truly democratized and and open you know there are no
guarantees are there i hope my friends don't hear me saying this but yeah if you aspire to an ecology
you are you're you're creating a nature on the web basically yes which we are participants and
that will have its you know that will have its own challenges so um you wrote a beautiful paper
called am i self-conscious or does self-organization entail self-consciousness and Keith and i agree
that this is probably the best quote we've ever seen in our lives you said the proposal on offer
here is that the mind comes into being when self-evidencing has a temporal thickness or counterfactual
depth which grounds inferences about the consequences of my action on this view consciousness
is nothing more than inference about my future namely the self-evidencing consequences of what
i can do and uh we spoke with you um last time i think and we invoked charmers and the hard problem
and so on and we were talking about qualia and subjective states and you know in a dialogue
and all this kind of thing and um you responded that different feeling states are hypotheses about
how i'm feeling at the moment and then it would use all the messages and belief updating and all
the planning and estimates of uncertainty which attend that planning the precision or estimates
of uncertainty play heavier roles the higher you get in the hierarchy and this rather leads to this
idea of planes of consciousness you know we said we'd kind of defer this discussion to later so
consciousness is something which um there's no operational measure for it there's no touring
test for consciousness but it's something that we all experience and we feel and and it's with us
um so could you talk to let's say the these planes of consciousness is consciousness everywhere
or are we only aware of the plane where most of the work is being done
okay excellent final question again i'm not the best person to answer this question because i
you know the 300 principle as i'm sure i've said before is not a theory or a principle that would
generate theories of consciousness um but there are lots of people are very interested in this
including yourself obviously and not your viewers um actually i should say also um there's um first
of all this has become a big question in the sort of r and d part of industry also the
Templeton Foundation are starting to fund a number of adversarial research collaborations
to really drill down on that thing that you are picking up on which is to be conscious to
actually have sentient behavior um requires this planning aspect this going out into the future
you know it's put it very very simply um i um conscious or sentient just because i have the
capacity to plan which also entails some kinds of free selection amongst alternative courses of
action which is something that a thermostat wouldn't have something the weather doesn't have you know
there are lots of self-organizing systems that don't have the capacity to select
amongst a number of counterfactual um policies so for me that's that's a sort of um bright line
between sort of um sentience at least and and uh and self-organization that is not um sentient
and that that distinction is exactly what is being tested in this adversarial research
collaboration funded by the Templeton Foundation it's called Intrepid it's contrasting information
IIT integrated information theory with various um predictive processing either of a sort of
non-representationalist or in an active sort so it's a really interesting issue um and it all
comes back down to agency and acting in the long term and I guess your question here is you know
what level um where at what point do you get um qualitative experience and at what point do you
do you think it's you that's having that qualitative experience is that what you meant
well I mean just just to comment on what you've said uh and Chalmers says something very similar
actually he's a computationalist functionalist and thinks that there are you know certain patterns
of information processing and causal structures and counterfactuals and so on if you take the
episode and remove the counter counterfactuals it's not conscious anymore but what you're
saying is interesting about this kind of hierarchy so the heart for example um that doesn't really
have many affordances it doesn't have many counterfactual plans or things it can do it just
has to beat all of the time so that so you would say that the heart is not conscious whereas the
the upper plane is conscious yes yes I would yes yes yeah that's a good one I hadn't thought about that
