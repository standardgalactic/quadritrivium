just because we are big. So all the random fluctuations are
averaged away. In this particular case, then you can
you can show that these particular kinds of things or
Markov blankets have a very well defined probability
distribution over paths into the future, specifically the
paths of the active states. So let's assume that there are
certain kinds of particles or creatures that have active
states and they have, you know, they and the blanket states
enshroud a sufficient number of internal states. What would
they look like if they had very precise dynamics? Well, you can
actually work out the probability distribution over the
active states that over their actions, over their movements,
and these could be physiological movements,
autonomic reflexes, they could be physical movements, eye
movements, or for ambulatory movements, indeed even talking.
And it transpires that the probability distribution over
these particular paths of active states has an interesting
functional form when written in terms of the potential of the
log probability, we call that an expected free energy. And that
expected free energy, when you just write it down, passes into
or come can be decomposed into things that people have been
dealing with for centuries, specifically, the expected
information gain, and the expected reward or utility where
reward or utility is scored by the log probability of occupying
an attractive state or rewarding state. And I use the word
attractive state deliberately because it is just part of the
pullback attractor that affords the application of the
pathological formulation. It's just the characteristic states
and this kind of thing occupies. So the the guleness, if you
like, is is inherent in the characteristics of the particle
or person that you're trying to describe. So for certain kinds
of particles or persons, then it will certainly look as if they
are acting with long term vision, namely planning in a way to
return themselves to their attracting set, which you could
look you could now regard as a goal or a rewarded or at least
an attracting an attracting outcome or or state of being. So I
think that kind of particle starts to now show the the deep
intention, you know, it looks as if they are they are doing this
because they intend some outcome. I think the next step,
though, which is the one that you were speaking to possibly done
then it was implying is another, I think, step, which is, but
does the does the internal state know that? You know, am I
aware that I am planning? And of course, that calls upon very,
very sophisticated, so if they exist, internal or generative
models that entail a model of me and the hypothesis that all of
this, these sensations and all the consequences of my actions,
which I am not aware of. You know, I cannot feel all the
sensory, not sensory, all the neuronal impulses that cause my
muscles to contract. All I can register is the consequences of
that through my muscle, my stretch receptors, my proprioceptive
receptors, or if it's in my gut or my autonomic intraceptive
receptors. So I don't know what I'm I can't I can't have I do
have have no access to my actual action. But I can see the
consequences of it and it looks as if I'm planning. So I now have
a hypothesis or a fantasy that I am a thing. And I seem to have
goals. And if I can learn the kinds of goals that I have, then I
can now start to mentalize and have notions in my generative
model about my goals. And then I can notice, oh, you have goals,
or perhaps it's the other way around. Perhaps the first of all,
it looks as if the explanation for this random dynamical system
over here called mom is actually, but she looks as if she
plans, it looks as if she has goals. And then a few years later,
if I develop a sense of self, well, perhaps I'm a thing like
mom, but perhaps I have goals. So I think that you're, you're
from the point of view of Dandenit's intentional stance, I
think we're talking about a very, very particular kind of system
that actually has this capacity to recognize, first of all,
intentional like behaviors, long term planning dynamics in other
things, or things like itself, and then actually transcribe that
and test the hypothesis, well, perhaps I'm a thing like that as
well. Perhaps I'm me.
And what's beautiful about what you just said there, and all
three of you have said this in different different ways. So I
want to dwell on this just a bit, because I think it's a source
of the misunderstanding and or controversy about the free
energy principles. So earlier Maxwell, you had said, you know,
it's really a model about the couplings and the isolations
together, the couplings and the decouplings and the dynamics
between them. You know, Carl, you just said, systems will behave
as if they had intentions, whether or not they really have
intentions or not is kind of a different philosophical question,
but they'll behave as if they had intentions. And Chris, you
brought up, you know, the watt governor, which is, it's, it's
obviously a very simple dynamic system, you know, spinning
centrifuge and, you know, arm bar, like whatever else. And yet,
it's, it's not as though it's a literal model in the sense that
I'm going to have a mathematical, mathematical model for what
the steam engine is doing, but it's behaving as if it were an
effective model of the steam engine, because hey, when it's
slightly out of sync, the arm flexes a little bit, it readjust
it's, you know, the spin changes slightly, things continue to
work because if it didn't have the dynamics that were as if it
had a model, the thing wouldn't function, would just fall apart.
So I guess what we're all, what's all being said here is that
systems at different scales will behave as if they had a model,
even if they don't necessarily have a model in the sense of a
person, like a person might have a conscious model from which
they develop a plan or whatnot, but all these systems of varying
scales and complexity still behave as if they had a model. They
act to minimize the free energy as a consequence of, you know,
being random dynamical systems that are self maintaining.
Yeah, I think that's really eloquently put. And actually,
that's this is this to kind of underwrite some of the worries
we had about the principle, when we first started walking on
working on it as kind of coming from an activist or a low road
an activist, this notion of kind of an explicit
representationalism that you can you can get in these models
where the model is a homomorphic with the environment, right?
And I just just it rankles people who've come from behavior
based robotics and stuff like this. But then to kind of
realize that there's a richer relationship between the
internal thing, external that's been mirrored, right? It
doesn't have to be this homomorphic mapping. And also,
the you know, the actions are deeply involved in carving out
the world that you represent, you live in your own vault. You're
a model of your own vault, you're a model of regulating your
own vault, which is not, you know, it's not just like this
static separation of the of the agent from its environment,
there's this deep coupling, which, which can, you know, which
these kind of mirrorings emerge from. But yeah, I think you
articulated it really well there.
Can we just
I saw Carl, any comments here?
No, I think you did a great job. I agree with Chris, like that.
Yeah.
So I think we've got about 15 minutes left. So we'll do two
more topics. We'll end on structural learning and the
practical implementations of the free energy principle. But I
just wanted to touch on what you were talking about earlier,
Maxwell, which is this notion of the boundaries being
potentially vague, or maybe even observer relative. Now, in
your multi scale paper, you did say, use the word ontological
boundaries, I think. And I also wanted to talk about meaning. So
there's the boundaries of cognition in the physical
world. So we have this dynamical system, and it kind of
converges and we've got these nested kind of hierarchy of
boundaries. But then in a kind of Dennett sense, meaning
emerges gradually and the meaning kind of depends on your
perspective. And the intentional stance also depends on your
perspective. So different agents could look at different
phenomena going on in their physical environment, and they
could describe different meaning in their generative models. So
we seem to have this kind of weird two tier system where no
longer are we even saying that the boundaries, the marker
boundaries are fixed, they are potentially variable. And then
the way that the agents construct meaning is also
observer dependent. And all of this is kind of touching on what
I was saying before with goals, which is that the reason they
emerge is because they are unintelligible, they're extremely
complex. Do you see what I mean? So it feels like the reason why
we need to have this high resolution generative model is
because in principle, it would be impossible for us to
explicitly program it.
So a few quick things about that. I mean, I think you're
you're really identifying some of the key issues. I would first
so with respect to ontology, I would, you know, distinguish
ontology from metaphysics. ontology is just talking about
what kind of things are. So when we say that the boundaries are
ontological, what we're saying is we're trying to capture kind
of features of the map, in some sense. And you know, what we've
argued in a paper recently is that the free energy principle is
metaphorically speaking, a map of that part of the territory
that behaves like a map. So I want to contrast this with
metaphysics in the sense that like this is all of this is a
modeling approach. This is a physics. This isn't saying like
once and for all, these boundaries are here and
crystallized and just, you know, are there independently of,
you know, the way that we are experiencing or sampling or
interacting with the world, like that that kind of metaphysical
stuff is not what's at stake. Yeah, so in the paper you're
referring to, we describe we describe these Markov blankets
as existential boundaries, in the sense that they're both
epistemological and ontological. They're ontological in the
sense that they are definitional of what a thing is, in this
kind of physics based approach, if you can pick out a thing as a
physical thing, then it has to have this blanket structure. And
it's it's epistemological in the sense that it's this
interface, right? Like the thing itself can only know the world
and indeed, we can only know the world, the thing through this
boundary. So like this isn't meant to be a priori. It's
sort of weird. It's in some sense, it's sort of like an app
posteriori. It's like an empirically driven way to
carve up the world.
That's fine. I mean, on that, I think that that answers my
question, because there's always this confusion about whether
we're talking about the map or the territory. And just to be
clear, we are we are always talking about the map when we
talk about these boundaries.
Yeah, the free energy principle is a map of the boundaries you
can think of and a map of what happens when things have
boundaries that it and so yeah, that that that is important to
keep in mind, like we don't want to engage in map territory
fallacies. The free energy principle is a model, it's a
scientific model, and it's a map and it turns out to be
the canonical way of modeling systems that are engaged in
modeling. So if maximum entropy is the canonical way of
modeling physical systems in physics, it's the way of arriving
at the most parsimonious model that generates, you know, some
data set, right? Well, the free energy principle is equally a
canonical way of modeling systems that also in turn seem to
