like the caches so have many we uses

flash attention uh technology we stole

it from uh uh these guys uh but uh to do

the right size of fles attention makes

it fast we do flashh attention

recurrence flashh attention recurrence

and now we are uh faster than if we do a

whole flashh attention over the whole uh

context and that's uh both in training

but also in inference this is chunk wise

flash attention it's called or we call

it like this and this gives us a speed I

didn't expect that we could be faster

than flash or 10

in training as or no no way but hey it's

unbelievably fantastic but set be a fast

and inference we know because uh here

tension has a has also go Auto

regressive uh because you have to uh

produce a new word in generation and

then push everything uh into your system

again uh and you produce a new word you

have to push everything in the system

you can uh cach some of the processing

yeah you can do it fast but uh attention

