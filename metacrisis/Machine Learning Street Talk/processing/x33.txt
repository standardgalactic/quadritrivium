xlsm but no input gate uh input gate is

left out therefore it's uh nice to see

that different method converge to the

same architecture a little bit so not

service mama because they don't have

inate I think the inut gate is important

so remaining architecture is very very

similar now so started with States based

models we started with ldm with Hopi and

blah blah and now we more and more

convert to very similar architectures

are you seeing any hints of Industry

adoption of xlsm yes first of all X xlsm

is now faster than flash attention in

inference also in training I I can tell

you why with flash attention you have to

put also stuff along context into the

GPU what we now do we did chunks of

Flasher tension and between the chunks

we do the recurrent stuff and we design

the chunks of cles tension that we can

uh more uh be more efficient on a GPU if

you have smaller trunks you don't have

to squeeze it in and do inefficient

stuff you can exactly make it so large

