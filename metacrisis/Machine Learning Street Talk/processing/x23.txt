fast Flash attention was super super

fast because it is Hardware optimized

wonderful so can you bring in XL STM

which is this this new invention and how

does it overcome some of these problems

with the original wellis tier yes I

start with a spoiler um uh because I I

talked about Flash attention we are with

XL faster than flash attention both in

training as also in inference especially

inference is important uh now I go back

xlsm uh uh we uh after seeing uh this

this this rise of the Transformer and so

we thought okay uh first of all could it

could it not be ldm because uh uh the

reset backbone architecture uh to build

very very large uh models uh was this

the key uh uh uh to have this feed

forward connection this many parameters

where you store all the information uh

meaning is it important to build big

models or is it important uh uh to have

uh some specific uh uh uh technology

looking back uh to compress the history

we thought LSM should do it and we asked

