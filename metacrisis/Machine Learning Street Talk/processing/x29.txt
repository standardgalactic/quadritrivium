go higher I cannot over rules this uh so

sigit is is limited and you have to make

a decision but lat it's limited to

higher values so exponential gating is

not limited you always can do larger

values but the problem is we never used

in earlier days exponential activation

function because learning would break

down but uh we have to have a second

ingredient uh one is the exponential

gating but the normalization you have an

exponential uh thing but then you

normalize by uh this exponential uh

input Gates but it's like a softmax uh

uh if you uh uh remember how a softmax

works you have this uh e uh to the power

of something you have this exponentials

and then you divide by the sum of this

exponentials it's like a rolling

softmax uh and therefore uh uh we went

in direction of attention with LSM but

it's uh recursive but it's very similar

you have exponential input gate but then

you divide by the sum of all input Gates

it's a little bit like a softmax but

