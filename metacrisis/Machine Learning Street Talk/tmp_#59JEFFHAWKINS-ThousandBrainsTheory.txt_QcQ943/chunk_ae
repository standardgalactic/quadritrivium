are finding grid cells throughout the cortex so so the general idea I won't keep getting to your
details in a moment but the general idea that hey the same mechanisms are work in both places
seems to be right and and now we don't understand those mechanisms completely even though there
are 30 years of research on grid cells and play cells there's still some mysteries about them
but there's still a lot but there's a lot that's known and so we can map on the concepts I was
talking about like okay where am I where am I in this graph what's what do I place in that graph
you could think of that a little bit like play cells in grid cells you can think of remapping
is there's two types of remapping like this is a whole new thing so let's just start with a new
graph or versus let's fix the graph I already got you know and and that's that's very much related
as you said to remapping grid cells not that we understand that completely we don't know it does
but we know a lot about it it's a very very technical field in the neuroscience literature
right I've been fascinated by Douglas Hofstadter he has a really interesting idea on cognition
and he says that cognition is a little bit like the interstate freeway
in the sense that we have this ability to make analogies and this is something that current
AI systems cannot do and a lot of the symbolic folks are saying well of course deep learning
models can't do it you're just interpolating on a manifold for goodness sake but your conception
of reference frames I found fascinating because they're not just on your sensory inputs there's
you actually said that thinking is a little bit like traversing through concept space through
these reference frames and that's absolutely fascinating so but the thing is when we learn
information many of us learn information differently but we learn the same knowledge and
the same facts you know we all know how to speak the same language and you did allude to this in
your book that there are some interesting differences and the topology of how you learn
information but we still seem to be able to reconcile it in the same way yeah well sometimes
you know it's it's like the way I view it again we're telling something some some of this hasn't
been published anywhere yet um but the way I view it is um if I think of myself as part of the
cortex as a cortical column um I don't know what come my input represents and I don't know where I
am in the world I don't I just don't know anything I'm just a bunch of neurons right and so I have
to discover while I'm getting I get two pieces of information I get some sensory data and I also
get information about how my sensor is moving in the world yeah we can talk about how that happens
but I get I get some knowledge and so I have to figure out on my own what is the dimensionality
of the space what kind of space is it I don't have a preconceived it's like oh the world is
three dimensional I don't know that um I don't know if it's two dimensional three dimensional or
n dimensional I've just kind of figure out and I have to figure out what is the structure of
which I'm going to to place this information and when you and I look at a chair we both see it as
three dimensional we it is a three dimensional object we're going to build three dimensional
reference frames that chair they're going to be pretty similar it's going to be hard for you to
have a representation of the chairs it's significant significantly different than mine um but when we
look at things that we might not sense directly something that someone tells me about or I'm reading
about in a book the kind of reference frames you and I might create for the same facts can be very
different and and so we can arrange the same data in different reference frames because we can't
directly sense it we're we're basically relying on information coming from other people and so we
can um but up with like the same facts represented differently and reach to different conclusions
and this is why we have different beliefs about the world it's always about things we can't sense
directly if we can sense them directly then we generally have the same belief about it you know
the same kind of structure so I think this is a fascinating idea um and um uh but it's it's really
fascinating to me is that there is no pre a notion about the dimensionality of the world that a column
is looking at and and the data so it can represent almost any kind of data as long as you can get
figure out from a movement vector in a sense of human data and sense data it can say can I build
a map of this thing that makes sense the predictive map and um uh and if it can I said okay that's
my that's my understanding of it another another beautiful thing about it is is the fact that
your brain is is creating these binary encodings at all like I when I was researching this I found
very interesting that for grid cells there is some some topography at least in the visual
cortex or in the visual layer I'm sorry so you know grid cells that are nearby each other
will have like a similar orientation but maybe a different scale but once you go up a level higher
so those things create the s dr you know they create these bits that are very sparsely populated
and then the place cells correspond to combinations of those those bits to indicate a certain location
and once you're at that layer there is no more topography it's like now they've become
truly digital encodings of information right well I'm not sure I'm not sure I understand that
because it's like from my understanding there's always going to be a topology you're never going
to lose it uh you may not see that topology in the play cell right um all right that's that I mean
this the neuroscientists will listen to white listen to this I don't I don't want to insult them
because I'm gonna make this very simple I say you can think of like place cells are like what and the
grid cells are like where okay so um and you can say well this thing the one thing is that this
location type of stuff and so if you just look at the play cells you don't this isn't because they
actually do uniquely encode places so this is it gets a little confusing here but um uh but you
can think of it just generally like that right um if we're going to go deeper about this we're going
to start getting into all the unknowns about grid cells and play cells and they're very interesting
sure um the unknown and weird properties um we actually think now that um this is work that
one of our employees Marcus um is doing is we think actually that the the better way to think
about a coding is not a location is not actually through the grid cells but through these vector
cells things things like optic vector cells and and so on um it's it's and the the grid cells
actually might just play a very interesting singular role they might play the role of what
we call path integration which is like if I'm moving in a certain direction I have to be able
to predict where I'll be uh as I move that's called path integration I'm gonna predict something I have
to know where I'm gonna be when I get there um and that most of the max the graph itself is not
built up of grid cells it's built up these objects these more like these polar coordinate cells
called vector cells um so again this is an evolving field and you know and even stuff we
wrote a couple years ago we're like oh maybe that's a lot of light it's not like this but the overall
principles are the same it's still a graph it's still you know data at locations in this graph
um that hasn't changed uh it's just like how's the graph actually constructed that's a little
tricky right I mean it's like it's fascinatingly deep it's just a fascinatingly deep field but if
I can just tie this back to SDRs because I know there's there's always a lot of really confusion
or uncertainty about the value of SDRs and just for our listeners' sake I wanted to share one
intuition and get your take on it which is if we are representing let's say location uh in some
space yeah if all we need is 20 bits to do that so we really only need 20 bits of information to
represent a space location to the resolution that we need then if you go further than that
if you have 10 000 bits and you decide ah I'm not going to use 20 I'm going to use all 10 000 put
like you know little 0.01 values kind of all throughout there really you're just creating
noise and that other space that you could have been using for other purposes right and so I think
that's one advantage of the SDRs like what do you think about that intuition that's a little bit
correct I I I haven't thought of it that way um let me give you some let me just give you my take
on some of the advantages of SDRs so let's say I'm going to represent something I have 5000 neurons
I'm going to represent still right so let's just talk about the capacity and let's say I'm going to
use a 2 percent sparsity so I have 100 active neurons and 5,000, 9,000, 8 active neurons and
first you can say what is the capacity of the system how many different representations you can
represent so that's that's just uh 5,000 choose you know 100 and it's you know gazon gazon gazon
so then you can say okay there's no representation capacity you then you say if I randomly chose
these SDRs um what is their overlap well they'll overlap by about two neurons each
because each neuron is going to participate in every 50 patterns um but not by a lot more you
won't find two patterns randomly overlapping by 50 bits or 30 bits it's all statistically impossible
so you can now you have all these these patterns you could pick randomly all day long and they're
not going to overlap so they're very unique each one's going to be extremely robust to noise so you
could add 50 percent noise to any of these patterns and they're still going to be recognized correctly
okay so very noise robust is another um thing so high capacity uh representation very noise
robust there's another property that uh we think is being used everywhere which is really interesting
and I don't know if there's any equivalent to it in other types of network is what we call the
union property where I could invoke um not one pattern but two three five ten poundings at once
and so now instead of having a hundred neurons active I say I broke ten patterns I'm gonna have
a thousand neurons active thousand out of five thousand well turns out that all the processing
that you do still works on all those you can process them in parallel this that even though
you're mixing these things together the other ends the the patterns that are detecting these patterns
don't get confused and you can show this mathematically so instead of representing probabilities
like a probability distribution you might think about the brain often relies on this union property
which is you know there are multiple possibilities right now it's not really a probability distribution
it's just like these are all possible things that could be happening right now let's process them
all in parallel and see which one works out a superposition that's the kind of property that
yeah I guess you could call that yeah um and you only get that the sparser you are the more of those
you can do and the whole thing becomes so you got this huge capacity super noise robustness um it
only takes you know if I want to recognize one of these patterns of a hundred neurons I only have
to recognize I only have to connect to 20 of them so so neurons typically don't really detect
they look for maybe up to about 20 or 30 synapses at maximum um at a time so so these are other
properties all these properties come about yeah since you did mention capacity though I would
like to because I did see you know some variable statements sometimes I've seen no problem whatsoever
with capacity mathematically we've got tons of it but I was listening to one of your um research
calls I guess and you started to question you know capacity I think at least in the maybe the
the grid cell you know the interrinal cortex or or some such so I guess I have two questions one is
is capacity a concern or not a concern and also uh in order to make best use of the capacity I'm
sure the brain must have some type of entanglement so you know machine learned artificial networks
we know that they entangle representations so the you know there's like weight sharing between
different representations do you think something similar is happening in uh biological networks
well I don't know uh the the machine learning uh field that you're talking about uh and so I can't
really comment on this idea of entanglement of weights in um machine learning um I'm not familiar
with it there um I do I want to maybe correct something about sdr's which relates to the
question you're asking uh we think in some parts of the brain some parts of the intercortex these
very sparse representations work as I just said there are other places where there are less sparse
maybe 10 sparsity uh grid cells is a great example uh if I actually look at a population
at grid cells you know it's not like 1 sparsity it's more like 10 sparse and and and there the
activation levels actually do matter it's not like they matter like three digits of precision
but maybe one digit of precision and um and they and so that doesn't have all the properties I was
just talking about um when you get to like 10 percent you can't really do a lot of superposition
patterns so it looks like in some parts of these algorithms there are sparse patterns that work
on some principles and you know and in other parts they're very very sparse I work at other
principles it's not everything is super sdr's you know I used to think that but I don't think that
anymore um I just became an amateur with sdr's I said oh my god this is really cool you know how
these things work um but I now we now know that there are in some parts of these algorithms you
need to have um it's still sparse but more dense and activation level matter and then some of the
capacity issues uh you don't have less capacity advantages and so um and that's clearly true with
with grid cell representations um but I I imagine it's true at other things too I know it's true
in other areas too so it's it's it's one size fits all here um I do I do say we could say
certainly that you don't find anywhere in the brain or anything's represented by full act of you
know uh fully dense uh networks where all the neurons are free to develop that this does not
exist it doesn't mean this couldn't be useful and it doesn't mean that we couldn't in the future do
something like that I just seems very unlikely to me uh I know that's how most uh you know
convolutional nor that works with deep one of that works work today um but um right but you
probably know there's a lot of there's a lot of effort going on in sparsity right now in the
classic machine learning community people try to do it and we're working on that too um so so
I think there's something fundamental about the representations we use when we build AI algorithms
there's a dichotomy between you know the good old fashioned AI people they they work at a
level of abstraction higher they try to implement the mind and us connectionists we're trying to
implement the brain I think there's something fundamental about that level of abstraction
that we work at so there's a discussion between people like Scott aridson who think that computation
is raw and should happen at the lowest possible level so just think of how neural networks work
they distribute knowledge don't they across uh lots and lots of different uh neurons and then
there's um uh people like christa schwami and he's a computer scientist and and he thinks that we
should be dealing with much higher level computational primitives like functions and types so um
I guess from your perspective because I I feel that all of this is strongly emergent I feel that
we could work at the very lowest primitive level of computation and all of the magic emerges
do you think we should be doing that or do you think we should be working higher up the stack
no I well I think we have to work in higher up the stack um I mean of course you can break any
system down and say it's just a bunch of atoms it's just a bunch of molecules right it's just a
bunch of cells um and that's true and you can shouldn't dare you should be able to understand
that but just like thermodynamics says it's not very useful to think about liquids as a bunch of
atoms right you you need to have something a higher level than that and and so you know what I guess
the principle we that we assume that we just we have now in some sense discovered that the brain
uses this representation scheme using reference frames and movement now I think that's the right
level to think about it I don't I you could try to abstract it less than that but what why bother
that is the unit that we're talking about here we use models that are well but okay that's that's
that's the thing though but um the brain was evolvable it evolved within biological and
environmental limitations and and go fi people say well you know what scrap that let's just implement
the mind directly you know and they think that um functions and types um they're not an imaginary
concept that have been invented by humans they think that mathematical knowledge has it's
universal and it's been discovered by us and we would be silly not to implement it directly in
this higher level language because if you think about it even um the knowledge that we have you
wrote about this in in your book it's fascinating that now there's a separation between our genes
and the knowledge we've created as as the human race and all of this stuff now is is is emergent
our society is emergent so why not implement it directly but why trust because you'll because
because you're gonna you're gonna you're gonna guess wrong right you know you're gonna say okay
well I will figure out what that structure is that what you know what is that mind structure that
we're you know and and so for example most of the you know the good old-fashioned AI never considered
movement in their in their in their fundamental aspects of representation space right um and
you know or even deal for what dr. lennard did was psych you know same idea right um yeah and so
but but now I know okay the brain builds these structures using movement so it it's not like
it was a wrong idea of the old AI people it's just they just didn't pick the right schema
it was the wrong schema well could could you expand on that because the thing is um because we've
got a go-fi person we're very we're very good friends with and he is convinced that there is
you know like the knowledge engineering bottleneck and yeah it's it's very very brittle and it didn't
work very well but um but you're reading your chapter on knowledge representation I think you
were saying in our brain it's it's like lots of models when we model a stapler it's actually
lots of little models in our brain and it's it learns a kind of um it analogizes the knowledge
in a really sophisticated and distributed way so when when we want to know what happens when
