like decision trees they still assume an ordinal value on the dimensions. HTM takes it one step
further. Its representations are also distributed over the features. The only encoding rules of
SDRs were that semantically similar data should have a significant overlap on the SDRs. The encoder
should be deterministic and the output should have a fixed dimensionality and the sparsity
level should be similar across the input domain. Now the thing that was missing to some extent
from HTM was the notion of representation learning like we have in neural networks.
You have to do it all yourself in the encoder which is probably the main reason why HTM never took
off for unstructured data problems. The reason why CNN's dominated computer vision was because it
learned representations that were better than any handcrafted representations. The locality prior
and the weight sharing and the stochastic gradient descent made it computationally tractable.
The HTM neuron was inspired by pyramidal neurons in the neocortex. A neuron is receiving SDRs from
distil apical dendrites higher up in the hierarchy and from basal dendrites from the same region of
the hierarchy and also from proximal dendrites from lower level of the hierarchy or some sensory
input which represented the classical receptive field of a neuron. Now all of these neurons are
receiving a stream of SDRs and they're figuring out when to fire and turn on their on bit or you
know when they should go into a predictive state which can tell other neurons when to fire. Right
from the very beginning Numenta knew that real neurons are not simple point neurons. The synapses
on active dendrites detect dozens of sparse contextual patterns and can learn complex temporal
sequences. Active dendrites enable flexible context integration in the layers of neurons.
There are two primary phases of the temporal memory algorithm. The first is to identify which cells
within active columns will become active on this time step. The second phase once those
activations have been identified is to choose a set of cells to put into a predictive state.
This means that these cells will be primed to fire on the next time step. HTM implemented
dendrite branch specific plasticity so if a cell becomes active and there's a prediction
it reinforces that dendritic segment. If there is no prediction it grows the connections by
subsampling the previously active cells and if the cells not active and there was a prediction
it weakens the dendritic segments. Now I learned all about the HTM algorithm from watching the
HTM school series of videos from Matthew Taylor who worked at Numenta. Matt tragically passed away
last year and I wanted to personally pay tribute to him here. His passion and enthusiasm was infectious
and he'll be greatly missed by the entire machine learning community.
Well sparsity is something that doesn't run really well on existing hardware. It doesn't
really run really well on GPUs and on CPUs and so that would be a way of sort of bringing more
more brain principles into the existing system on a commercially valuable basis.
There's a large body of work on training dense networks to yield sparse networks for inference
but this limits the size of the largest trainable sparse model to that of the largest trainable
dense model. This was the case actually until relatively recently. Now since the 1980s we've
known that it's possible to eliminate a significant number of parameters from a neural network without
affecting accuracy at inference time. Pruning can substantially reduce the computation of
demands of inference when the appropriate hardware is utilized to do so. When the goal is to reduce
inference costs pruning often occurs late in training. Now in 1995 researchers discovered
that retrospectively pruning low magnitude connections worked impressively well. Later
researchers found that retraining the prune connections produced even better results or even
better still rinsing repeating the process with multiple rounds of pruning and retraining.
Other approaches explored adding connections back in at random or even focusing on non-uniform
sparsity which is to say adding the connections in where they are most needed in the network.
Jonathan Frankel who was on the show last year by the way released his lottery ticket hypothesis
in 2019 which demonstrated that if we can find a sparse neural network with iterative magnitude
pruning then we can train that sparse network from scratch to the same level of accuracy.
However as the demands of training have exploded researchers have begun to investigate the
possibility that networks can be pruned early in training or even before training. The benefit
of doing so could reduce the cost of training existing models and make it possible to continue
exploring the phenomena that emerge at larger scales. Recently several methods have been proposed
specifically for pruning at initialization SNIP aims to prune weights that are least salient for
the loss, GRASP aims to prune weights that most harm or least benefit gradient flow and SYNFLOW
which Yannick made a video about by the way aims to iteratively prune weights with the lowest
synaptic strengths in a data independent manner with the goal of avoiding layer collapse where
pruning concentrates on certain layers. Now Frankel pointed out in his recent summary paper
that magnitude pruning after training outperforms all of these pre-initialization methods. Most
of these methods effectively prune the layers not the weights which to say you can perform
similarly well even if you randomly shuffle the weights that they prune in each layer. Now
interestingly SYNFLOW and magnitude pruning work quite well at initialization time without seeing
any data. Now Frankel didn't identify a single course for why these methods struggled to prune
in a specific fashion at initialization time and thought that this is an important question for
future investigation. Perhaps there are properties of optimization that make pruning specific weights
difficult or impossible at initialization perhaps because the training occurs in multiple phases.
Now combining gradient descent training with an optimal sparse topology
can lead to state-of-the-art results with smaller networks. In the brain Numenta argues that sparsity
is key for how information is stored and processed. They also believe it to be one of the most
important missing ingredients in modern deep learning. We reached out to Numenta after the show
and their VPs of machine learning architecture and research and engineering so Lawrence Bracklin
and Subtite Ahmed told us that at a high level the biggest difference is that they view sparse
networks as a unique standalone class of artificial neural networks that mirror the sparsity exhibited
in the brain versus being a derivative of dense networks created by pruning. So not really removing
redundant connections but creating networks that are designed to be sparse. In 2019 Numenta released
a paper called sparsity enables 50 times performance acceleration in deep learning networks. In that
paper they pointed to the scaling challenges faced by the current state-of-the-art neural networks.
They said that the brain is highly efficient right requiring a mere 20 watts to operate which is less
power than a light bulb. Contrast that to GPT-3 which costs millions of dollars to train. Numenta
believe that by studying the brain and understanding what makes it so efficient they can create new
algorithms that approach the efficiency of the brain. They think that the core reason the brain
is so efficient is the notion of sparsity. A sparse network is one where all of the neurons
are not densely connected to every other in the same cortical area. The brain stores and processes
information as sparse representations you know at any given time only a small percentage of the
neurons in the brain are active. This sparsity may vary you know from less than 1% to a few percent
of neurons being active but it's always sparse. Sparsity will lead to a massively smaller memory
footprint because only the non-zero elements are stored enabling the hardware to run more
networks simultaneously. GPUs and tensor processing units so TPUs they are dense execution engines.
They perform the same computation task on an entire vector or matrix of data.
This is a wise approach when the vector or matrix is dense which is to say it's all non-zero
but in the dense environment we gain efficiency by executing a single instruction to be applied
to all of the data. This is an approach called SIMD but when the data is predominantly zeros
then a prodigious amount of computation is wasted. So if you're keeping up to date on AI
hardware you might have heard of Graphcore or Cerebrus. Cerebrus in particular actually they've
developed this epic microprocessor with 850,000 cores and 40 gigabytes of memory on board it's
absolutely insane. Not only that the chip has been designed to support sparsity from the ground up
the Cerebrus cores never multiply by zero. The scheduling operates at the granularity of a single
data value so all of the zeros are filtered out and this in turn provides a performance advantage
by doing useful work during those cycles which otherwise would have been wasted not to mention
the power and efficiency savings. Now recently in a presentation they showed this graphic claiming to
achieve near linear speed up in respect of sparsity about 84 speed up for 94 sparsity.
Numenta released this paper a couple of years ago before the current sparse hardware was released
and at the time they chose an FPGA which is a field programmable gate array as the platform
to run their performance tests because of the flexibility it provided in handling sparse data
efficiently. In addition random access to memory is far more granular and efficient on an FPGA
enabling FPGA implementations to efficiently handle the unstructured access patterns in sparse
networks. Now in their paper as well as confirming the previous results in the literature about the
robustness of sparse networks to noise and variance error they also realized a significant performance
gain from using this specialized FPGA hardware. Now my intuition is that there's no difference at
all between the representational power of a discrete HTM type model which they used in their
previous generation of algorithmic approaches versus an artificial neural network. The obvious
difference is that feed forward monolithic vector space models are more amenable to training given
today's hardware. Pretty much everyone agrees that sparse networks are better but is there something
fundamentally special about sparsity? New mentors certainly seem to think so. They anecdotally point
to the brain as being sparse but the brain is probably sparse for the same reason that I don't
decide to get up every morning and travel to every city in the UK. Doesn't seem like a profound
insight to be honest. We can take it as a given that sparse networks suffer less from overfitting
because they're not going to be using their precious representational power memorizing individual
challenging or non-representative examples in the training data. It remains unknown if the
performance of the best pruning algorithms is an upper bound on the quality of sparse models.
There's actually some really interesting papers out there now like Momentum ResNet which allow
us to train huge neural networks with a small memory footprint but the key question is are
sparse networks functionally better than huge densely connected neural networks? Researchers
from Google released the paper rigging the lottery making all tickets winners back in 2019.
The rigging the lottery algorithm starts with a randomly initialized connection topology and then
layer by layer adds and removes connections densifying the layer and then sparsifying again
using a traditional weight magnitude heuristic. The algorithm achieves higher accuracy than all
previous techniques for a given computational cost at all levels of sparsity and then scored
higher accuracy than the dense to sparse algorithms. Now Momentum say that they've also produced a
similar algorithm although as far as I know it's not being made public yet so I assume it's pretty
similar to Google's approach as they pointed us in this direction but unlike traditional dense to
sparse iterative magnitude pruning Google's algorithm allows the topology to grow also during
the optimization which can apparently help overcome some of the local minima. Obviously the most
accurate sparsity algorithms required at a minimum the cost of training a large dense network in
terms of memory and computational horsepower but that approach has serious limitations right the
size of the sparse model you can learn is strongly bounded on the size of the larger dense model
which you're sparsifying from as a starting point it's simply too inefficient to waste computation
on so many parameters which would end up being zero anyway. Google's algorithm seems unequivocally
better than dense iterative magnitude pruning which is somewhat surprising to be honest given
that it's uh it seems to be doing the same thing but only one layer at a time. Now Training Mobile
Net 1 and 2 on ImageNet with this form of sparse training was instructive it was possible to train
a sparse network with nearly the same accuracy in about 30% of the compute time it was also possible
to train a large sparse network which was 5% better accuracy in roughly double the flops of
training the original dense version. Today's neural networks have something called the point
neurons it's a very simple model of a neuron and by adding dendrites to them at just one more level
of complexity that's in biological systems you can solve problems in continuous learning and rapid
learning so we're trying to take we're trying to bring the existing field and we'll see if we can
do it we're trying to bring the existing field of machine learning commercially along with us
you brought up this idea of keeping you know paying for it commercially along with us as we move
towards the ultimate goal of the true AI system. Now Numenta is working on some really cool stuff
behind the scenes unfortunately there's very little information about it in the public domain yet
Jeff's main aim is to realize the vision of the 1000 brains theory in an efficient computational
algorithm sparse networks are just a tiny part of this vision the next step is implementing
continual learning with active dendrites and this essentially means that they need to be able to add
new synapses and train them independently of the existing ones this is going to require a sparsified
version of back prop which will also require specialized hardware and algorithms to implement
they also mentioned to us that they want to exploit activation and weight sparsity simultaneously
mirroring the neocortex anyway next week on street talk
another book that I read around the same time that had a big impact on me
and and there was actually a little bit of overlap with Jean-Pierre as well and I read it around the
same time uh is uh Jeff Hawkins uh on intelligence which is a classic and he has this vision of the
mind as a multi-scale hierarchy of temporal prediction modules and these ideas really resonated
with me like the the notion of a modular hierarchy um of you know potentially um of
compression functions or prediction functions I thought was really really interesting and it
reshaped the way I started thinking about how to build minds let's kick off with the main show I
hope you enjoy it folks well I fell in love with brains actually when I read an article by Francis
Crick who was one of the co-discoverers of DNA and Francis had uh later in his life turned his
interest to neuroscience and he wrote this uh so essay that appeared in Scientific American
where it was sort of um the emperor has no closed type of essay he said you know we have all this
data about the brain and it's really wonderful we collected all this data and we've got decades and
decades but no one has a clue what the hell's going on and um and he says you know we need new
ways of thinking about the brain we don't really necessarily need more data and and that just
struck me I was 22 and I was like oh crap you know that's just a puzzle we have this we have
these pieces and someone has to put the pieces together and that seemed like something I would
be good at or at least I would enjoy and that's gotten me going that was that was the thing that
just I said I'm gonna make a career out of this and then very quickly I realized that well you know
this is the long thing that's gonna take a long time and uh but if we do this um then if we really
figure out how the brain works and what it does then we will have real big insights into how to
make intelligent machines and so I said oh my god the implications are here it's not just from a
neuroscience but from an AI point of view and so that got me going uh on this on this journey
and at that point I decided to change careers from engineering to neuroscience uh computer
engineering computer science to neuroscience and start all over I just got my degree from university
so I was like oh starting again here we are 40 years later it's been a long journey though as
you probably probably know there's been a lot of twists and turns to absolutely absolutely well we
really enjoyed reading your book but um I wanted to talk a little bit about some of the tribalism
in the machine learning community so I've been doing a bit of research online and um your mission
right now is to try and convince other people of your ideas you've got this incredibly exciting
idea of the brain and as you just said that in some level of abstraction the brain is infinitely
complicated but actually if you think about it in terms of simple rules that can produce a lot of
complexity it's not that complicated but you know I've noticed that when you speak to some of the
machine learning folks they are very quick to dismiss your ideas and they say well there's
there's no material difference with monolithic neural networks with point neurons and backprops
and backprop and you know it's good that people are so passionate about what they believe in
but by the same token it means that science only advances one funeral at a time yeah um
it's a very complex issue you bring up here and um I think our mission isn't to convince people
our first our mission was to figure out what the hell's going on in our heads that was the first
thing we had to do and we've made a lot of progress on that now we have to sell those ideas we have to
sell them to neuroscientists we publish papers I speak at conferences the people side of work they
test it and so on that has to happen it takes time um then uh then we now have a roadmap we can see
we can now I from the position I'm in and some of the people work with me we can see where the
shortcomings are in current AI techniques uh we can we can see oh the brain's doing it differently
