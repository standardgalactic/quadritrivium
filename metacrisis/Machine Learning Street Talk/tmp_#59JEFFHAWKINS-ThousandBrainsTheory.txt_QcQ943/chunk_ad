it's doing it this way and we have we can have two approaches to go forward we can say well we need
to convince everybody else which is not really a fun thing to do um or we can we can just put
our ideas out there document them show people them and then work at doing this ourselves like
just demonstrate it start building things um making things that work making things to solve
problems that other people have been struggling with so we're doing a bit of all these things
we're promoting the neuroscience theory we're promoting um these ideas in the machine learning
community and we're also implementing this stuff uh because in the end you if you don't if you you
can't implement it unless you understand it so it's a good test for us too um and you would wish
you know everyone wished like oh wish everyone just you know agreed with us where everyone you
know got what we understand but in reality this is a big world there's lots of big middle um
and there's a lot of um there are people who want to dismiss new ideas um but that's just the nature
of the of the beast right we just have to accept that um science is not just science you have to
promote your ideas as well as discover them and the same is true uh in the machine learning world
well what one thing that's kind of interesting about that you know philosophical aspect that
you have of look it's a big world there's lots of people doing a lot of different things this
that's kind of the heart of evolution right is that there's variation people taking different
approaches and and natural selection will kind of sort out you know which ones work and don't work
and that's one thing I've always thought about a lot of the folks that don't want to pay attention
to what the brain does is they don't have a sufficient appreciation for how much you know
let's say work has gone into designing the human brain right I mean a billion years of life uh
however many hundreds of millions of years of intelligent evolution so certainly we can learn
things from the neuroscience right I mean and and of course that was the original inspiration
for the artificial neuron but that was just one tiny simplest possible abstraction of a neuron
that happened what fit and I don't know what 50 years ago right there's gotta be more we can learn
right but yeah but you know the way I look at it is we're all trying to reach the same angle
we're all trying to figure out how to build intelligent machines they're truly intelligent
we all read they're intelligent uh we can talk about the applications of that um and I don't
think there will be multiple ways of doing this there's like in in the end there aren't multiple
ways of building computers they're all some sense universal turning machines and then we have variations
on that um and so I think that's what's gonna happen here too now our priority how would I know
that engineers couldn't figure this all out just by thinking about and doing engineering stuff um
you know my guess was that we'd have to figure out the brain works first uh and who knows I
could have been wrong that was a bet all right maybe people would figure it out but here we are
67 years later in the field of AI and I still I think today's AI systems are still incredibly
limited they don't really do much at all with like it's intelligent that's my opinion um they're
very very restricted they don't generalize they don't create behaviors they don't it's just so many
things that we're so far away and now we took a long time and now we've figured out a lot about
how the brain works so now we have a roadmap so um I think it's gonna be a lot easier it's now it's
not just like oh I think it's gonna be quicker to study the brain and other people say no I don't
think it will be I said well we now have some things we can we really understand about the brain
we don't have to worry about that anymore so um the approach we took has been fruitful um and um
and I should also say just be clear today's AI is really useful so I'm nothing against it yeah
it's great um it's just not intelligent and I want to build intelligent machines yeah it's
quite it's really fascinating to get to talk to you uh your book on intelligence is actually one
of the very first books of AI I ever read so uh it's your fault that I'm part of this field now
I was so bad when people tell me I said I hope it worked out okay for you
no it's been it's been a wonderful journey and I'm really glad that it was one of my first
exposures to the field in a way it was so full of like really interesting ideas especially for
that at the time seemed very revolutionary um and it very much shaped shaped my thinking
so I want to ask you a little bit um it appears to me as you know someone who works
sorry I work with deep learning you know a lot of course nowadays it feels to me that a lot of
your idea seemed uh validated in a lot of ability practices um a like a currently kind of like a
merging trend is how um with like these me have heard of such like these transfer models that
have become very popular recently that scaly these larger it's just stacking these transformer layers
seems to currently at least increase performance in like a you know in a pretty consistent manner
and I wonder well it feels like that it kind of validates in a way it's like a different
proposal to what you say about like a chordically uniformity do you think there's any kind of
connection ah you know I don't think about I'll answer your question but let's just say
I'm always forward thinking I never look back and so I don't worry about hey what those I write
about this or not or just say you know just let me get credit for this or that I don't really care
oh it's like let's go forward what's the best thing we can do right now and um and so you know I
think there's you can obviously I said really we're all going to converge on the same thing right
eventually I think so ideals become you know I got ideas from other people other people get
ideas from me the history of how that milieu of ideas travels it's very difficult to point to
um so I I'm very reluctant to claim precedent anything I just feel like hey you know if we're
all moving towards the same idea that's great I don't think transformers are an implementation of
the cortical column or the cortical algorithm idea we're not there yet because we now we now know
what the core of that algorithm is it has to do with movement and reference frames and transformers
don't have that in some sense they have a very primitive sort of um attentional movement if
you want to think of it that way um but nothing like kind of movement and reference that we talk about
so um so it's a little bit in the right direction um and of course they're really impressive um but
it's it's not it's not close to what I think we need to get to yeah it's really fascinating though
because transformers are strange beasts they perform a kind of information routing and it's
quite esoteric exactly how they work and I think in in that they in some sense they're similar to
capsules and I know capsules have been equated to some of your work but I wanted to talk a little
bit more deeper than that though so um a lot of your ideas from a machine learning point of
view come down to the fundamental dichotomy of discrete representations versus continuous
representations which is what we use in deep learning we use vectors and there are some
advantages to using these continuous vector spaces namely that you get for free some spatial
priors so you can encode semantic similarity as a function of how close they are and yeah and also
you have gradients which are useful for stochastic gradient descent and interestingly the manifold
hypothesis states that most natural data falls on very smooth and low dimensional manifold so all
of the human faces could be projected onto a low dimensional manifold but um I know that a lot of
your work has has been really solely focused on on these sparse distributed representations
and then um building on top in in the encoding for example so if I'm encoding a date representation
I would do it such that this sparse distributed representation would have a significant overlap
for date times that are in the vicinity yeah and so semantically similar things should intersect
each other and spatial relations between bits can be encoded using a receptive field similar to
you know how it works in in cnn so um but nementa has asserted that intelligence wouldn't be possible
without these sdrs and they're the primitive of of intelligence so what do you think about the
kind of comparison between vectors and and these um discrete representations that that you're an
advocate yeah well let's just start you know the whole idea of these sparse representations comes
because that's how brains work I mean that's there's no compression value uh if you're looking
at brain you look at any population of cells that are representing something at any point in time
most of the cells are inactive it can be anywhere from you know 90 percent to 99 percent
of the cells are inactive you just don't see of the information carrying neurons or some
neurons are far away all the time we're not talking about those but the ones that carry
information you just do not see any kind of dense representations that is a that when you when that
happens you're having a seizure um and we also see that in in in real neurons that they're not very
high fidelity they don't carry individual spiking rates are very crude um there's many instances
where even the single the first spike is actually the most important thing regarding information
so we're not like there's no so on a high resolution you know four bits or three bits or even even
often one bit of precision and firing up a neuron right so this doesn't exist in the brain um so we
just take that okay well we need to understand why that is and it's not just it's like more efficient
from an energy point of view that's not what it is um it turns out that um sparse representations
have a lot of really desirable and interesting properties that you don't get with dense
representations um we can talk about those um it's not like everything is um there's a spectrum
here right there are some parts of the brain where you have representations which are a little bit
less dense maybe tempest out of the neurons are firing and there the activity rates really do matter
they're still not high precision but they really do matter and then we believe there's other parts
where they're very very sparse i might have five thousand neurons and let's say two percent are
active so a hundred active neurons and all the information is encoded almost all the information
is coding in the population code not in the firing rates and we can talk about that if you
want to dive into the details what the advantage of sparse distribution representations um but it
is an empirical observation of the brain so you know we'll start with that it's okay well why is it
that way you know well one one thing i wanted to understand is because you made the comment before
that um they could improve generalization but intuitively they're they're the opposite of
improving generalization because they have such an incredible specificity so for example when i
was reading through some of your literature when you use boosting for example in your algorithm
it increases the specificity to such an extent but then the way we generalize in our cognition
isn't necessarily through the representation it's through the abstractness of the concept we're
learning would you argue yeah so oh no i'll be honest with you i've changed my opinion about this um
i used to believe exactly what you're saying which is like oh these overlaps between the
sdrs are really how you're gonna get generalization we now realize it is a much more powerful form
of generalization um i talked about it briefly in uh my new book um a thousand brains i don't know
if we really discussed it in any of our papers yet um and this is the idea that you're so i can
switch into that so i'm i'm i'm admitting right yeah i don't think it's what i said before okay
all right let me learn um but there's a much more powerful form of generalization if you want i can
delve into that right now um uh it's not okay so so we have this is what we've learned is that each
core of the column we can say the brain in general is um it's building this sort of uh the model of
the world you can think of as a graph but it's you can literally think of it as like where are things
relative to other things it's like a computer aided design model right CAD model it's literally
doing that you have a model of something like a computer or the house or room or bird or ideas
that they're structured using reference frames and data populating reference locations and reference
frames and um so you can imagine uh my knowledge of something i again as you know in the book i
use the coffee cup example quite a bit but you can use anything um a knowledge of say a stapler
is is it has a certain set of parts uh which are then arranged in a certain relative position
for each other and they have certain movements related to each other and um and that structure
is is that graph if you will is the uh is the definition of that object and that's your that's
your model of it i tell you i tell you represent it in your brain now when you when you learn it the
way you do this is you attend individual components one at a time so if i if i were to look at a new
object i haven't seen before like a stapler i would just attend to one end or look at what's what's
this thing look like then what's that thing look like and as you attend to these different parts
you're essentially building this graph you're essentially building the model you're saying okay
this component i recognize that that's like a hinge this component over here looks like maybe a
button this component here looks a rubber pad and i and i just start building this model in my head
but it looks my phone my phone look there um i so building this model okay so you got this i'm
trying to paint a picture in your head how you you're doing this every moment of your life as you
look around even like you're looking at the screen right now you're looking and see different things
you see where they are relative to each other i think most generalization in the world comes
about from a different from the process of if i see something new that i haven't seen before
or i'm experiencing something new it doesn't matter revision but just imagine you're seeing
something new you don't recognize it you don't see this arrangement as a familiar arrangement
then you look at a subset of the components and you say okay this is focused over here and you say
well these three components over here look like something i've seen before these look like it
might be a button that i pushed down and this thing over here looks it might be a hinge um and
then you then you say well if it's a hinge little button with this thing even i've never
seen it before maybe if i push in this button the hinge will look and and and so you you basically
your your generalization comes about because subsets of these graphs that you build of the
structural model are similar to subsets in other graphs you've learned and you can say well these
subsets are similar therefore the behaviors associated with those subsets are similar
the performance of those subsets are similar so i'm now going to describe to the stapler some
things i've learned about other things um and we keep and if i see something really new we just
keep going down to swirl and swallow pieces until we find something i recognize this next okay well
i thought i recognized and and so that i believe uh is actually the primary way we generalize in the
world um we can talk about how sdr's do that do that too but anyway what you're talking about it
sounds a lot like sort of some of the graph isomorphism you know work that that people talk about
where if you if you've got this new thing and it has all these parts and they're in a graph
you know certain subsets of that graph may be isomorphic to previous you know graphs that
you've seen and and then therefore you can learn about it um and you said something really
interesting which is when you see something new you know you attend to kind of different parts
about and you're doing kind of a remapping there and there's a lot of research on on grid cells
right that they remap so when a rodent goes into a new a new environment it remaps the grid cells
can you talk something i mean is this related to the remapping and i'm really curious about
how does that remap happen in the neurons and how quickly does it take place you know yeah well so
one of the the the hypothesis we have thousand frames theory is that well first of all we deduce
the idea that you need some that there is any in the cortex there are these reference frames and
these structures these graphs right we can deduce it it's like okay well i know this is happening
i walked through the logic behind that and then it was at first to me it was very odd like how could
neurons build these graphs right how and how do they structure them i mean it what with the way
i used to focus is like my cortex knows where my finger is relative to the thing it's touching
like relative to the thing it's touching not relative to my body and and so i use the example
the copy cup and the copy cup moves around so the reference frame has to move with the copy cup
i was the holy crap how does that happen it's really good for neurons to do this and then of
course we looked into the literature of the hippocampal complex which is grid cells in the
entorhinal cortex and play cells in the hippocampus and there's a whole bunch of stuff and there's
there's a 20 or 30 year history of research in this field it's not in the neural cortex but it's
something that's very related to the neural cortex and and so and that's what they it's
clear that grid cells and play cells and these other types these vector cells that exist there
are building maps just like this and so we said okay well it's likely then that the
same mechanisms that are used by grid cells are probably going to be used in the cortex
and the time you made that that was a purely evolutionary argument it was like saying
it seems unlikely evolution is going to discover something like this twice it's really hard to
do right it's really really hard to do this so and animals have to know where they were in the
world a long time ago so they probably evolved this complex mechanism from nowhere and where
they are and now the brain's going to use that same mechanism for knowing where like your eyes
are or your fingers are or yeah it's going to be the same mechanism for everything else and so we
just said hey it's probably likely that the same basic mechanism that uses grid cells and play
cells and vector cells is going on in the cortex the time we made that that prediction we were not
aware of any evidence of that was the case and now there's a lot of evidence for it now people
