going to need them to adapt to new environments like it's going to be different you know different
building it on the planes of Mars versus deep down in the valleys and mining water or whatever so
they're going to have to have some capability to allow variation you know then you have the mixture
for evolution and once you have evolution even if you had no goals like think of it this way the
ultimate thing which has no motivations whatsoever was inanimate matter and that's what the earth was
four billion years ago and we wound up with animate objects that have goals so despite
philosophers and this is kind of an interesting thing philosophers are always like you can never
get ought from is but interestingly enough ought arose from just stuff right yeah the very elegantly
said there's this weird kind of symmetry breaking that happens somewhere i mean in evolution the
symmetry breaking happens because of physics because things that don't want to replicate don't
replicate so we don't see them so we found the fruit ought from is comes from just the fact that
if there is a possibility of replicator and the possible non-replicator eventually you will only
see replicators and that's your first symmetry breaking and of course we'd be you know when we
build intelligent systems is that maybe we can break the symmetry in a more you know in a less
dystopian you know a bit uh mouth easy and weight and build systems that are more motivated by things
that we consider better than just pre-replication but it's very non-obvious to me you know how to
do that safely it's hard so it's a very problem you but Hawkins did say though that he was more
worried about the immediate threat of humans controlling the kind of ai that we have now or
the kind of ai we have now been used for bad purposes or even things like the spread of
false beliefs that's incredibly dangerous as well and that that replicates like a virus
that's why that um you know that social dilemma doc nary like i thought this was a very brilliant
quote that was in there and a lot of other people think it's like not an interesting quote but to me
it was beautiful when uh you know undermining our weaknesses exactly he said we've always been worried
about when ai would overcome our strengths when we should have been worried about when it would
overcome our weaknesses right and and it's already you know even though we don't have maybe we forget
if we have real intelligence or not or whatever clearly these machine systems have overcome lots
of our weaknesses and they've created things that are more addictive than they've ever been
found ways to waste more of our time than they've now of course there's offsetting things like yeah
we also get greater productivity here etc etc but we should be worried about when it overcomes our
weaknesses and what that's doing to us and our children and you know one of my favorite examples
is it's called the food up to my chair so this comes through them and yeah i go to roco and he
makes this example that so the way he describes it is that in like the in like the 19th century
we collectively summon the weak super intelligence to feed all humans like the deal was kind of this
this capitalist you know market system will produce really cheap food for us but the systems
was optimizing our reward signal based on profit of course let's see how these things work so it's
kind of like a weak it's not sent it was obviously not intelligent the way you know humans are but
the way it is opts in raising system it optimized for more more food and now we're at the point that
you know like what 60 percent of adults in the western world are obese something we are literally
eating ourselves to death and this is a very very weak intelligence quote unquote and this is already
powerful enough in a way to you know get people to basically kill themselves voluntarily so imagine
if you gave such a motivation you know these you know food corporations in the world you know built
their big their big agi yeah i'm using hawkins you know elves and brain algorithm of course uh and
giving the motivation of you know smith raff yeah but this this gets to utility and similarly hawkins
said oh you should never clone yourself because you're not cloning your forking i thought that
was a bit of a contradiction though because he was talking about um you know sending clones of
yourself to mars or ceding mars that's right and and that's the same thing isn't it you're actually
forking the human race when you have a population of humans on on mars i don't think um who doesn't
like forking what's wrong with that well no but the thing is because we want we're doing it because
we want to leave a legacy you know i like doing this youtube channel because you know after i'm
dead people can can see who is that tim scarf guy but um but you know you might say i'm going to
create a clone of myself because i want to have a legacy but it's not a legacy because after after
the next day it becomes a you know this it's a different person it's not connor anymore
and similarly when when you were talking about um you know why don't we restructure society because
let's take away social media let's let's fork and take away facebook and everything else but
what's your utility function how could you how could you justify that ourselves in that parallel
universe would be better off than we are now well first of all i mean i think it would be a legacy
so you and i both think that that the pixels on youtube are going to be a legacy and those are a
pale pale shadow of a you know projection of us right so i would totally you know a clone of you
or a robot programmed with your mind state or something that then can go off and live its own
life that's exactly what a legacy is like that that is a legacy it's not me it's a legacy yeah like i
described like even more extreme crazy versions of this kind of thing is like i fundamentally
believe in like you know a non dual identity and like that moment most of condoms of identity is
kind of incoherent it's just it's just a convenient abstraction in humans like humans tend to be
discreet that's just a convenient property that humans have to have it happened to have there's
no reason that identity or minds in the sense have to be discreet like that it just happens to be the
case that humans come in packages it looks like that we come in you know one intelligence please
one unit of intelligence but there's a reason they have it they have a mark out they have to
have a mark off blanket or they can't persist exactly but there's no reason you can't have systems
that don't really think there is not like a clear separation between the like the only reason i think
really that you know me and tim aren't the same person it's because the bandwidth between our
brains is really low is interbred it's lower than it is intra brain like why is the right
heart familiar maybe that's the person let's say those were different person depending on your
definition like you know there's like split brain patients were like the brain has to actually
disagree and like take different actions in like fighting with each other so like are they different
people i don't think so i think what makes identity is synchronization in a sense well like
aligned so you see is that identity the best way to define identity is alignment with agul
is that like does the the system coherently act without you know like shooting itself from the
foot so it's bigger like you know doing it all it's so like that's why i think it was a really
great argument that for example and colonies are one animal that happens to have like you know
very big individual parts because in a way it is aligned you know not all the parts were produced
yet but we're producing part you have you know fighting part if eating parts of what you know
food getting parts or whatever and they happen to not all be one body but that's just neutralization
detail you were also just colonies of cells you know it's it's just it's just implementation
details it matters is the coherency or the synchronization of you know knowledge and goals
in life yeah but again a lot of that is is emergent um but you were saying as well that um there's a
kind of there's a there's an io bottleneck when we communicate and this is um a lot of people talk
about neural link in in this context as well but i don't think there is because when we communicate
there's common knowledge that we both have so it's it's a fairly efficient communication
mechanism and we're invoking lots and lots of very complex knowledge set that we've
reconciled so when we read a book or when we when we take on board information the bottleneck is
comprehension i don't think it's the the bandwidth sure as it depends on how like and by bandwidth
we're also in comprehension bandwidth like if i could leader i imagine we could you know take
two human brain to put them in the same skull and collect to connect them through some super
clip with low summer or whatever and you know let the whole thing synchronize and you're not
died seizures immediately or whatever i think there's another strong argument to be made that
is one entity but if you could also make an argument to connect the entities and at some
point just with them to continue this like a classic like often when something seems confusing
if there's a discrete quantity of all try try continualizing the quantity and see if the confusion
disappears very cool well gentlemen it's been a pleasure your hands so see you next week folks see
you next week
