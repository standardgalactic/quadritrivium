we press down on the stapler we we run a simulation in our mind you know what what happens if I press
down and what happens if I stretch it and our models seem to um generalize to any version
of a stapler we might ever find yeah that that almost convinced me that we do need to have a
lower level representation of knowledge lower than what lower than the the reference frames
I was talking about well low so the go-fi people say that we should use um functions and types and
relations to describe knowledge but do you think that's possible I don't want to misrepresent go-fi
because they haven't looked at that stuff in a long time um I think you can't do this at the
level of the neuron I mean obviously brains are made of neurons so but you know they're not uniform
neurons we have dozens of different types of neurons are hooked up in very complex ways they
do different functions it's not just one big neural network yeah that structure is important
and so uh I think it's now become evident to me and it wasn't this way five years ago
it's really evident to me now that at least in the brain knowledge is represented in this structured
form uh with reference frames and movement it's the same inspiration that the unit has with with
capsules but he didn't incorporate movement as part of that structure um you know that with that you
know but now I now I understand okay well at least from a brain's point of view these reference
frames are constructed as I mentioned earlier they're discovered through through movement
observing movement of the of the sense within the world and um and so now we have this the the
basic structure of if you want to call it will fi is is these uh movement-related um
reference frames and placing you know and stuff like that I can't I don't know how I would do it
less than that I don't know what how I want to get rid of that now what do I do just ignore that
and go back and just put a bunch of neurons together I don't know do you mean could you
come up with a genetic algorithm that could have discovered this using just traditional neural
network sure maybe you could have you know I don't know what's the fastest way of getting there
but now that we know it why would I want to do something different I mean I maybe I'm just
understanding your question try well I think the the the main issue is that we don't understand
the representation you know if you look at the open AI microscope and you can visualize all the
different neurons and so on these objects have been distributed in a way which is completely
ineffable across all of these different neurons yeah and is that the most high fidelity possible
representation of knowledge I well I don't I don't know but well I will I will say now
this I currently believe very strongly that um in the future when we build AI systems they're
going to work more like on these frame structures and they won't be this highly distributed mysterious
blobs of neurons right they're you're that's great by the way it's it's great uh not only
because of what they how they perform it's great because we'll understand them a whole
lot better this you know this touches on the issue of the threats of AI right you know the
so many of the threats of AI are based on the idea of what we don't know what's going to happen
well no you actually will know what's going to happen you know I designed the thing it works
like this it's what's it you know I can't put everything but I you know I don't know what it's
going to learn but I know how it's going to learn and what it's what it's capable of um so I I just
you know I don't want to debate people about this because I you know there's a lot of smart
people out there and they may have things no things that I don't know and they can pursue things
but from where I sit right now um it's very clear to me okay this is how the brains do this
this explains everything explains like you know how we structure all knowledge um and how it evolved
that's a big part of this and how you know how we have this common algorithm and it's going to be
based on its reference frames and movement and so on and so okay well why would not just just
just build that let's go for that um and you know why build something else if I understand that
why build something else I don't I don't understand the point is it going to be better
you know in the future we will certainly take you let's say I'm right let's say I'm right
and we build machines that work on these principles that I outlined in the book well of
course we can vary go go for that we don't have to stick to the way biology I mean we'll come up
with a better way of representing things and structures and reference frames maybe it won't
be movement based maybe it'll be something else I don't know but could I try and do it build
something and get it working why would I go away from it you know yeah go ahead to try and steal
man the gofi people it's because they think that knowledge is universal we discover it there is
only one representation of knowledge and clearly in the brain you can learn things in different
ways you can I can teach you a curriculum and I can teach you it in a different way and maybe in
some weird way our brain does learn some latent representation of the knowledge which is the
universal knowledge but they're making the argument that why don't we just represent it the only way
it can possibly well first of all that's not you know it clearly isn't true I give it just like
you just said I give the example in my book about taking a bunch of historical facts and
arranging them along a timeline and arranging them on a map and you end up with different
inferences and different beliefs about the same set of facts so whether there is a universal
truth I don't know but clearly humans don't know that universal truth we we can't know everything
we can only sense the small part of the world and and and you know the vast majority is invisible
to us so who the hell knows what that universal truth is um what we can do is build good models
and uh and the models but there isn't a universal model there isn't one correct model for for history
right um we might like to believe there is my model's right and yours is wrong that kind of thing but
you know it's really easy to point out even non-controversial things like the timeline
versus the map will lead to different models and the order in which you train someone will matter
you know so I think we just have to live with this um and I don't think you know maybe there's
some platonic you know go-fi people thinking of this in platic universal knowledge it's correct
all time I don't know but certainly I don't think that's accessible to us if there was we just we
just we can't we can't sense the world both physically and time-wise we our senses only deal
with a teeny part of the world uh it's a good part but what not be a little teeny part
we don't even know what the universal truth about space and time is I mean
definitely true it if I could move on to some other questions about the brain um so you of
course felt it's very much on new cortex as the you know seed of human intelligence and
I think that I think you're definitely on the right path with that I think most people agree
something similar to that but there's also lots of other parts in the brain that see
pretty important and I'm specifically uh interested in so like what thing that machine
learning people are of the interest that it is the dopamine circuit so like the cortical basal
ganglia the harmica cortical loop and what seems really it seems like a learned signal it seems
like some kind of learning is going on there could you maybe say some words about that yeah so
obviously the neocortex is connected to the rest of the brain in my ways um myriad ways um
it's it's not in isolation and it's a very complex relationship so lots of other things
that doesn't mean you can't understand what its circuits are doing
but in a human the neocortex is complexly tied to things and one of those things is that if you
think if the neocortex is as this sort of map of the world or model of the world well it what
should it learn and what goals should it have the dopamine circuitry is equally associated with
what should we learn when should we take when should we take the effort to learn something which is
metabolically expensive um and um and so somebody has to decide that my basic intention is the
decision is not the neocortex's decision right mostly it's somebody else's decision and um
when we build intelligent machines we'll have to figure out who's making that decision and um it
won't be in the neocortical equivalent it'll be in another part of the intelligent machines equivalent
but uh there's absolutely no reason we would have to model the goals and um emotions and other
things that other parts of the brain do unless you wanted to create something was human like
right so you know i i used to joke that our dopamine signal in our models is a switch i
turn it on and turn it off i can learn now stop learning now right just manually turn it on
manually turn it off uh when we run experiments um there doesn't have to be anything you know more
complicated than that um but there's you know the only thing i'm claiming here is when we build
intelligent machines if we build them on the principles in the neocortex they have to be
embodied i mentioned this to the whole chapter in the book about they have to be embodied in some
sort physical and non physical embodiment but some embodiment they have to have uh you know
very sort of safeguards built in they have to have various mechanisms for what to learn what not to
learn someone has to provide goals uh what should the goals of the system be uh what are we trying
to achieve right now and um but so those things have to exist but they don't have to be modeled on
i don't see any reason at all to model those on the neural circuits the way the neurons do that
that now we're getting a pure biology we're now getting into what is a biological organism need
including like keeping your heart going and breathing and you know making sure you have sex
and all these things um you mean more of that i'm just we're gonna build intelligent machines
just make them a little bit um uh to our purpose i guess i would say another neuroscience question
is that i know um you know a lot of people are skeptical that the brain can do anything remotely
like bat propagation right i mean there's sort of good arguments for why that's just not possible
but on the other hand i don't know if you've seen kind of a lot of the recent work by uh
Blake Richards at McGill University where he talks about you can do something that performs in a very
similar way to bat propagation because and especially in the neocortex the uh the neurons
there had these very long apical dendrites and they had these sections with these calcium channels
that can do these very long lasting activations that trigger like spike trains and he's saying
that there are there is evidence that you know if you have feedback from a higher or downstream layer
back into this the uh the apical dendrite you know connections that that can have an effect
of creating something like bat propagation it's like stochastic bat propagation what are your
thoughts on that well i'm a little bit familiar with that work and Blake Richards work a little bit
i feel a little bit of a distraction because bat propagation is a cool idea
it works on classic neural networks pretty well i don't think the end result is like the kind of
direct what you see in the brain at all i don't think the kind of learning that that bat propagation
does is the kind of learning we see in the brain and and so and there's lots of other theories what
those apical dendrites are doing and why you have that stuff so um i'm not going to say that bat
propagation isn't occurring anywhere in the brain i don't know maybe it is um but it hasn't played a
role in our theories about the brain and i think a lot of people spend time trying to shoehorn in
and they're like i'm gonna prove it to you see the brain really is doing this it's like well
but the brainings look like anything else and none of this stuff the brain doesn't look like
anything like you're talking about here you're just trying to put this you know one little
piece backprop into this thing i mean for example just take away learning we know that learning
in a neuron lauren taught these dendritic branches and and the synapses are distributed on these
branches uh and this is this is key this is critical you can't you can't get around this
this is this there was important information theoretic reason for this it's it's not just some
other reason it's it's doing something important from an information point of view and and so
the whole idea of backprop and working with learning on individual dendritic segments it's
i don't even know what that means right it's like it's like hard to fit in so right to my mind it's
like noise i kind of like tune it out saying well if they figure out something really important
they'll let you know well i mean i'll pay attention to it yeah i think i think more his his motivation
and i quite agree with this it's about taking inspiration from the brain so this algorithm
that they discovered by kind of looking for things that could you know at least enable
you know feedback mechanisms for the training is it but are they are they taking inspiration
the brain or are they taking inspiration from the neural networks and trying to figure out the
brain could do what they're trying to do is they're trying to figure out how can the brain do deep
training how can the brain do deep learning yeah but and they but assuming that that's what that
assuming backprop is what they want no no they just want to know can it do how can it do deep
learning and so then they look very carefully the biology and they find this this alternative
mechanism which can produce results that are very similar to back propagation but actually it's
slightly better in some ways and so it's leading to inspiration for how to improve ml and i guess
that's the question i really wanted to ask you which is i always ask this what's missing question
okay so we have artificial neural networks clearly they're a very stripped down you know
micro abstraction of what happens in the brain if you could add in a few things like just one
two three kind of abstract properties to neural networks today what would you suggest
should be added to get the biggest bang for the buck like how can we incrementally improve them
okay well so so that's what we're doing in the mental right now so um my colleague subatomod
who's really the machine learning expert on more of the neuroscience guy um he put together a roadmap
or we did it together but he's implementing it a roadmap the idea is like hey we have these
grand theories about the neocortex and so on maybe the right maybe not but we think the right
um well how what do we do next like what do we do right so the answer question was well how
we incrementally get from where we are today uh machine learning to where we want we ever think
we need to be some number of years from now and so we literally have a roadmap and we started off
by saying okay uh first i'm going to focus on sparsity and so we've been doing that and we've been
uh we've been making really interesting progress some of it which we published
and some some of it we haven't yet uh about introducing sparsity to first we did it for
convolutional networks and we did it for other types of networks now we're doing it for transformers
and showing that we can speed these things up and make them more robust and and i'm talking
speed things up by a lot not a little bit by a lot uh depends on the network depends on the
what all these different things but uh how do you and then how do you implement that on
hard so we start with sparsity the next thing we're working on is the dendritic computation
because what the dendrites get you and we haven't really talked about this much at all so far
but with the dendrites each individual section of a dendrites like it's only a little
computational or pattern recognizer um they allow you to represent information in different contexts
and so um this is we think it's going to make it's far far less training data to train existing
networks um um and so and allow us to do continuous learning uh because when you train a neuron you
don't train all the synapses you only you only modify a few on on one of these dendritic branches
and so you can you can learn without forgetting things um so we're working on that so i can take
existing networks make them so that there can be continually learning or they they don't forget
things um and make them so you can train them for fewer uh data points and then so we're we're
well into that now um and then the third thing which the next big one that which is is sort of
biting off the whole concept of reference frames and um we're just starting that um and it probably
will take several years um but and then finally we do the whole the whole thousand brain theory
we have lots of these columns and so on so at least we put together we have a roadmap we're
working to it i think i'll be honest with you i think this the progress we've made is actually
far better than i thought it would be i thought it would take us longer to achieve the things we're
achieving uh the performance gains we're getting are really are really something dramatic and uh
without losing accuracy um so uh that's that's what nometa's working on and i don't think we've
talked about this roadmap um but we haven't given all the details of it so it's not public yet the
roadmap well the roadmap was that talked about it in like you know talks i just i just spoke about
it here right so i have pictures of that stuff um but the details how we're doing it like we we've
created we discovered like you talk about sparsity the problem is you want to run sparse networks
you want to run them on some hardware right what kind of hardware you're going to run them on
is it GPUs CPUs FPGAs well we're doing all of those um but they all have their problems and issues
and because GPUs don't like sparse networks at least you don't get any benefit from them
and um and FPGAs are hard to use but they're pretty they're good but they're really hard to use um and
and so that the work will be discovered and even including very recently we've discovered tricks
um engineering tricks that allow you to map sparse networks onto some of these existing
architectures um better than we think anyone else has been able to do by pretty large margin
and and so um that you know these are very engineering problems uh they're not pure you
know they're not pure theoretical problems are right and big engineering problems and we have to
