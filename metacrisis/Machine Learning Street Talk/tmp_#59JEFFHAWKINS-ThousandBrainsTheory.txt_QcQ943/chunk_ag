we have to solve this because it's no good to say hey sparse is great you can't run it faster
it's got to really work in a real world so it's exciting you know we'll see how far we go but
right now it's going great i'm really thrilled by it we've actually decided to invest more in it
because uh we're hiring more people because it seems to be working so well so we've been talking
a lot about intelligence um you know however that might exactly be defined i think we're all
on kind of the same page what we need when we talk about intelligence the neocortex this this map
this ability you know however you might exactly want to define it but we've also talked you know
we've um a little bit moved you talked about motivations but you know which may you know relate
more to dopamine or whatever you know within the brain there you just run into your cortical
intelligence so in your book use you and of course dedicate a good chunk of it to these
questions of motivations risks of benefits they are and you describe yourself as someone who is
pretty skeptical about like severe risks um so one of your one of the things which i fully agree
with by the way is you very uh eloquently describe how intelligence is kind of having a map
you know the map itself has no motivations it's not good or evil it's you know it's just a tool
it could be used to do something nice for something good you know or it could be used to do something
bad um the map itself is you know it is orthogonal to these kind of questions of motivations and you
you know do it good or bad it's kind of the the human idea of you know we can't get it off from
it is but at some point of course when we actually build these systems we don't want it bought to
be in there right you know we do want our systems to actually do things at some point we don't know
what's sitting there and know everything but never take any actions in the book you for example give
me to the example of you know we want them you know to go to Mars and then you know build a colony
and not just sit around the sun all day you know thinking about the universe or whatever
so at some point we have to introduce some kind of motivational system the brain seems to solve
this according to you this which is called an old breed the end um i'm not sure exactly which
parts you count as the old brain versus the new brain but some are quite obvious you know we have
circuits and get hungered or you know you have other motivations or whatever but it's and but it
seems like these parts or will be very important that okay let's assume your project goes fantastically
here 20 years from now new mentors the biggest AI company in the world you've developed our goal
you figured out the neocortex you've created a wonderful algorithm um at some point we have
to inject motivation i'm sure you agree with yeah and we have to assure that those motivations
aligned here in like what we want and that seems like a hard problem maybe you could talk a little
about that well i don't have it's a hard problem i think the concern that people have about this
is that these machines will evolve their own motivations that that or somehow they they are
in some sort of you know epiphenomena disappears um and um i don't think that's right um i think
we will have to work hard to put them in there like i mentioned earlier um it will not be easy
we'll have to design what how these systems work um unless we put this machine in some
sort of evolutionary um structure whether it's a self-replicating machine or it just uses evolutionary
algorithms genetic algorithms to decide what its motivations are um they won't modify themselves
they won't change on their own you know i i gave the very simplistic example of a self-driving car
but i think it's a correct example you know i tell the car where i wanted to go
it's not going to decide to go someplace else because today it feels different you know if
i could make a car that does that but why would i do that i'm not gonna design a car to do that
so if we send more rods to mars to to build machines i'll have to be motivated to solve
the task they've been given um and but we will also build in all kinds of safeguards just like
we built in safeguards into cars and we won't allow the machines to you know it won't be a
mysterious process it'll be like a self-driving car it it's not going to be something we don't
understand that happens and runs away from us uh it'll be difficult to do we can make mistakes we
can put in bad things and machines will do bad things but it's not like the machines on their
own they're going to do this it's not like we've disenabled this beast that's just going to take
over and decide on its own and what its motivations are so i make it very clear in the book that
there are a lot of risks with ai and a lot of bad things can happen with it but i my only contention
is that the fears of existential risk um are overblown i don't think they're true at all and
that the arguments don't really hold up um because there are more arguments uh based on ignorance
as opposed to some detailed knowledge of how these things are going to work
so did i answer the question so i absolutely agree with you uh i happily agree with you that
these like epiphenomenal explorations or na√Øve you know a lot of like sci-fi evictions are like
oh the robot becomes conscious and wakes up and it decides to rebel against its master of course
that's a pretty silly perspective well not not everyone thinks that's silly by the way so i i
know i and i'm well aware but not everything's so silly but i feel there is a stronger a similar
argument that's i don't consider the same argument but has similar consequences and in a way it's
kind of like in your book you have a chapter titled how the new tortex can thwart the old
brain and you yourself describe the old brain as the motivational system and now you're described
being the neocortex the new brain plotting to thwart the goals put in by you know the you know
our creator you know evolution or whatever so it seems like our why wouldn't our machines
maybe have similar capabilities to him with interest to like modify their own old brain quote
unquote i did just saying um uh yeah i can see your point there um let me see no one's ever asked
me that before so let me just think about it for a second um well um do i have you that
you know the the old brain's trying to thwart something okay so first of all
the new the new brain's trying to like take the old brain so the new the new neocortex
that's it has a model of the world and the model is very consistent and it strives to
have consistent models it wants it wants to make its model correct that's one motivation it has if
it sees something wrong it tries to correct it in this model so it has this model of the world
and says okay the world ought to work like this if a happens a b happens and c happens
and that's good uh but then you know the old brain comes along and says nope we don't we're
not going to do that we see a and b we're going to go x and uh and so it violates the model in
some sense right it's like violates the model and so um generally the neocortex uh is not able
to overcome the older brain things it just we just give into our desires and emotions uh but
we do struggle with it um so i i guess you're saying where does the motivation for the from
the neocortex come what how does it decide that its model is more important than the old brain's
model um and like i don't it's a good question i'll have to think about it some more but i think
that the root of it will be that the motivation the true and only motivations the neocortex has
is to make its model the world correct and to fix errors um and um that is it says this isn't
right my model says this isn't right uh we you know i want to correct my model but i'm being
told not to or i'm being told to do things over violating the model um and so that is its motivation
now you could take that to some extreme let's get a little sci-fi here right you could take that to
some extreme and and uh say well um the neocortex really figures out how the world works the future
brain the ai neo projects really figures out how the world works and humans got it all wrong
you know let's let's bring it to an example we could we can relate to there is no god right i
know i figured this out there's no god you guys all believe in this but it's not true so uh what
we do about that is is this ai system uh put up with it or is it or is it do something about it
or is you just keep trying the prod humans to do something differently um i don't know it's a good
question i think you bring up that's a very interesting philosophical question um i will say
this i don't think this is something that's gonna happen fast quickly overnight you know it's it's
not this is something we all have time to try to think about this quite a bit um but it does bring
up a question what if what if the the world as as a smarter machine knows it violates the world
as we'd like to believe it and that's really the the the fiction you're talking about and um
what should we do about that there's an even more direct like kind of physical
assault on this that doesn't require much philosophy it's just what about the traditional
mechanisms that created life which is variation and you know random variation and natural selection
so for example if we send robots to mars to build a colony they're going to get damaged
and destroyed a boulder is going to fall on them so they're going to need to be programmed to
replicate themselves they're going to have to be programmed to build replacements of themselves
and they're not going to be able to do that with absolute 100 fidelity so errors will creep into the
the neurons or the silicon neurons or the programming or whatever and so you've already
got in place the requirements for evolution which is information transfer or self-replication
I'll push I'll push back on that first of all uh hanging robots self-replicant is really really
hard are they going to build their own semiconductor chips you know are they going to you know mine
their own you know titanium or my I mean I don't know but but but even if they do evolution requires
a very complex structure for um how information is represented and and requires that information
be changed constantly in our offspring right you know so when we build we build you know computer
chips and van vidian builds in the next chip they they're all pretty much going to be identical
so I I push back on the idea that there's inherent evolution built in any kind of
self-replication I don't think that's true I think we're if it if there's some variation it would be
very very minor and and it's not going to be passed on genetically to somebody else you know
there's going to be some blueprint how to build this chip which is the neocortex the blueprint
itself isn't changing so if I if I if I build the neocortex from the blueprint that's slightly got
an error in it that's not going to be propagated to its children um so I think there's a lot of ways
you there's a lot of things you have to do to create evolution it's not easy um and I don't think
that's going to happen accidentally so I'm pushing back to you I don't think that's going to happen
you know it's good it's good to think about it ask that question right right but you know I don't
think so I think in reality if we send robots to Mars in fact you know in the chapter in the
book I mentioned this point that we could send robots across the universe and then they would
have to self-replicate right and after I wrote it I said oh damn I don't know how to do that I just
don't know how you know a fleet of robots you know AI systems appear on another planet in some
just part of the galaxy and they have to replicate themselves what would be the physical form of
those AI systems that they would be able to do that in any reasonable amount of time and effort
and because you know just think about again I mentioned like semiconductor factories right you
know it's like they got to build those things you know how do they do this so it made me it
it was a hole in my argument that we could send AI systems across the universe and I didn't put out
pull out the hole I said to myself well that I excuse myself in this regard by saying well you
know what if we're going to send AI systems to other parts of the universe they're not going to
be built of silicon chips that there might have to be some other manifestation that like biology
is able to replicate on its own without the use of these cupboards other systems like
semiconductor factories and but we're so far from knowing how to do anything like that today
that um I don't know and then maybe we'd be in trouble right we send these things over there
they evolve and they come back in you know so I don't know but we're talking a long time from
now yeah that's for sure there there is a kind of failure mode of thinking you know too far ahead
into the sci-fi feature that you're just coming generalized even fictional evidence yeah you
know yeah I guess what I um so what I have taken from like your just your description your book
in for now kind of about like motivations and existential risk is that I think actually at
least existential risk to people that I personally find interesting I think I agree with you in like
a lot of ways actually in that I think a lot of them are very reasonable in the sense of saying
like you know whether it's fast or we're short it's kind of like you know it's kind of beside the
point and you know trying to see you know humans already kind of you know hard to control and
like you know it's like if you actually try to think about how would we write a motivation system
because you know it can't really be a learning system because you know like in your book you give
an example where if the brain comes up with two ways to get to food and one of the path has a tiger
on it the old brain will say you know kind of like oh no don't go there but you know the old brain
how does it feel that a tiger is bad that's like also like a not obvious to me how we would necessarily
learn that so I guess it just it feels to me that um as you yourself say you know your focus on the
new cortex which I think is an important thing to be working on but we can agree on that some work
on these motivational systems and control problems might also be justified oh yeah sure uh totally
I agree with that um I think it's justified and necessary but I'm not scared of it and you know
the conversation sometimes people we shouldn't be doing this for each other because it's going to
get out of control I'm thinking you kidding me it's gonna be so hard to do this at all you know
don't need to get out of control um so we need to do that we should do it the conversation around
me today is great about that kind of stuff but I guess it's just not you know this there are
quite a few people feel this existential risk is you know upon us and in any day now you know
it'll be too late therefore you know we have to stop all this stuff and I it's just I can't see that
in any possible scenario but yeah we have to think about these things right we have to figure out
not because um it's scary or dangerous because we have to figure out because we have to do it
and and and I use the example of the book about you know we put in safe courage my car doesn't
always listen to me when I if I'm about to hit something my car puts a brakes on even if I put
the accelerator down it ignores me well we have to put in some some fail safe systems too um and
these systems so they don't you know end up damaging things but it's all has to be done
I had a question on on intelligence I'm fascinated on the nature of intelligence and
you were saying in your in your book that you know the traditional go-fi people had a task
specific skill conception of intelligence and then it moved towards more of a um a flexibility model
so being able to learn and there's also a really interesting tradition in cognitive science about
embodiment which I think is fascinating and you know my my favorite person is Francois
he says that intelligence is the information well that I think he says it's the task
acquisition efficiency and generalization but but also quite you know now we've been talking
about this a lot and there's this interesting idea that you can think of intelligence as the
ability to um acquire knowledge so almost all of the knowledge that humans acquire in their
lifetime is is not empirical and trial and error it's given through instruction or reasoning and
deduction and some some magic happens with these intelligent systems you know why why why can we
deduce so much knowledge is that a question to me yes uh why why I mean mechanistically why
how do we do then I mean I think I thought I answered this earlier so maybe I'm I don't want
to repeat myself too much um maybe I don't understand the question um well for for example you know
you knock the beer bottle off the table you can now reason that the floor is wet someone might slip
up on on the floor yeah so almost all of the the knowledge that we that we have around the world
is deduced or well it's it's it's one of its deduced that knowledge that you have to learn
that through observation if you never experience liquids and you're never very slipping on liquid
you never experience knocking a couple or you wouldn't know what's gonna happen I mean that's
what kids do little babies do this right they're like oh what happens we could live in a world
where liquids fall up and that would be a perfectly good world and that's what we would learn
so I won't think we deduce this we observe it and and then by analogy we we predict how other
things that are similar would behave similarly but I think you know to me the way I look at it is
you're you're bored with this with this structure in your head that is designed to learn the world
through eyes and ears and touch um but it really in the neocortex knows almost nothing about what
it's going to learn the whole part of the brain that's not true right refining refining question
because I'm I'm also fascinated by this concept that I learned it from the first time in your book
that the neocortex now I was going to say blank slate but Keith would kill me for saying that it's
not a blank slate it's a template because there's actually there's a lot of evolutionary knowledge
that's gone into there yeah yeah yeah because you because you can think you know it's just learning
all these signals but just the way the ear has evolved over time that the information gets encoded
in a certain way the cochlea has the logarithmically spaced bands and so on so it's learnable because
it's been encoded in in a certain way but people like Chomsky they say that we have a kind of universal
grammar or language built into our brains and it was endowed by evolution so where do you kind of
draw the does he does he say that about all knowledge you just about the actual language I
