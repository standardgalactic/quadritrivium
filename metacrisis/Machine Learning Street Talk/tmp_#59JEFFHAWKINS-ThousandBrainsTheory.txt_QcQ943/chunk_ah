thought he just said that about language about language yeah yeah so I don't know I mean but
we're we're talking about all knowledge now right so like it's so you know so I think look there is
there's all these assumptions about the how how the cortex is connected to things right and how it's
continually connected that have been evolutionally advantageous and so you know we we only see
certain spectrum of light because that's a good spectrum to look at and we only certain types of
you know sensory inputs because those are things that we process them before they get to the neocortex
so that they're in the right form for the neocortex to work we allocate a certain amount of our neocortex
to these different sensory modalities because that seemed to be the right thing to do from an
evolutionary point of view and so there's just a good the good example say it's like a template
that's right but the in the in what's actually learned in that template is is is is unknown in
terms of language I hinted at this when I talked about language in the chapter about high level
thought in the book and I don't consider myself an expert in language at all but it occurs to me
that if vertebral counsel is right that there is a single cortical algorithm that's basically
running everywhere the language has to be somehow fundamentally mapped on to this algorithm for
a sensory motor modeling through reference frames and I made the analogy about a big part of languages
has to do with recursion and recursive structures and and the algorithms in the
in the core column are really good at that and so to me I would say that you know if I would take
isn't it I never thought of this but if I would say take choppy's idea that there's a universal
language I would extend it beyond what we call language I would say there's a universal
structure to everything in the world that we can learn not everything in the world but everything
in the world that we can learn and that universal structure is this this reference frame sensory
motor idea and that universal structure can learn any language whether it's spoken language or
computer language or written language it can learn any structure that fits into that algorithm
and that could be you know how staples work and how birds fly and how evolution occurs and these
are all everything we know has to fit into that structure so in some sense I would I would agree
with chomsky I just think he only focuses on quote language um where I would say the universal
algorithm is language is a subset of the universal algorithm that chomsky talks about
that's an interesting idea I never I never said that before stuff man we'll see how it
feel about tomorrow but but there's there's a fascinating dichotomy though isn't there because
it's similar to the um bias variance trade-off in machine learning so you know evolution has
has given us a certain prior and a certain default encoding and then we have this learnability
algorithm and then what fascinates me is how externalized so much of this stuff is so there's
embodiment there's all the knowledge in in society that we learn language we're brought up by our
parents and we acquire the language around us so how much of it is being pushed down from society
and how much of it is being pushed up from the prior knowledge that we've evolved and inherited
well I would I would say there's three things there is the our knowledge we inherited there is
what we learn on our own just through exploratory behavior and and then what is passed down which
is only always through language but it could be also through by observation of other humans
and um and what's the balance between those two well I think in terms of the neocortex
it's not a lot from the biology and the evolution in terms of the other parts of the brain it's
very much so you know there's a it there's evidence this is the prior colloquious it's
is like the old visual system um it it it detects snakes and spiders so people who are scared of
snakes or spiders it's not the neocortex it's the old buddy going ass snake it looks like a snake
and so you know that's not learned it's just there it's already a rid of um so there's some of that
but you know our ability to walk we don't learn to walk we actually are programmed to walk it's
just that we didn't we haven't finished developing yet and so when we learn to walk we're really just
our nervous system is finished being grown you don't really learn to walk so these are these
old priors lots of them um and as a human they're very very important you know eating and sex and
and and uh body functions and you know survival all these things and hold there uh but from the
neocortex point of view I'd say there's very little um it's more just assumptions about this
the sensory types of sensory data you're gonna get I mean it's what you know it strikes me Tim it's
incredibly we are so incredibly versatile on what we can learn I mean just you know I think all
things we learned and I'm sure you know this you thought about I'm sure you saw that all things
we learned that we had no evolution and pressure to do like running these this kind of podcasts and
you know programming and computerism and talking about brains it's incredible it just just cries out
that there's a universal method here that's being applied to anything that not that everything
you know brains can't learn everything but they can learn a hell of a lot and it seems to be
we haven't really discovered the extent of it yet so this this says you know there's there is this
sort of blank slay-dish um system if you want to call it that way with these assumptions about our
our bodies built into it well the only thing about the blank slate thing is it's such a dichotomy
because I was reading your book and it seems to be so strongly determined by the embodiment and
the multimodality you know the way that we're wired because you said yourself AI will actually evolve
towards you know robotics basically it's not just the brain it's it's how it acts in the environment
and how it gets all those senses but I wanted to ask one final thing so I went to a yoga retreat at
the weekend and after the physical components of the practice were concluded matters of a
philosophical nature were investigated and the the basic case that the yoga teacher was making
is that we have two selves right we have the emergent social self and then we have the inner
self which I'm sure Sam Harris probably might might speak to you about you know the sense of
being and the sense of being is when you kind of ignore the virtual so social program on the top
and you know society it is an emergent virtual program and it does many of the things that the
cortical columns are doing it has the error correction it had you know it's the externalization
and distribution so even though we're programmed to care a lot about our social selves um we
are very stressed when we lose control of our own narrative because someone might be saying
something bad about us on twitter so anyway this yoga teacher was saying well there's something
deeply fulfilling um spiritually about just kind of um going into your mind and and and just
just that raw conscious experience but then I felt like saying to him well you should read
Jeff Hawkins book because your mind is just a load of prediction models and all you're doing is
traversing your reference range so what do you what do you think I don't I well again ask me to
talk about things I'm not too familiar with but I'll tell you something I'll tell you a personal
experience I have okay I don't do meditation and Sam didn't bring that up but but I thought about it
a bit I do something equivalent to meditation um is that when I find things in the world stressful
um I just sort of shut out the world and I think about interesting problems like the future of
humanity or the future of intelligence or what's the nature of life and things like this and so
now living in this world then it's sort of um is in some sense uh pure right it's not it's not
being messed up with like I'm hungry or this person's being nasty to me or just like tuning
it all out like oh my god I'm not in trouble again I know so I think that's my own personal
meditation and I don't think it's I think it's useful actually because uh I think it helps when
when you think like that you you're you're sometimes able to get to deeper truths um
that you separate the body functions and the day to day stuff and and I often do my best thinking
when there are no distractions either on sometimes while I'm driving or sometimes I'm just awake at
night like in bed um but where there's like I don't I don't have to deal with anything else
so that's kind of like meditation and I find I don't think it's useful I think it's very useful
because you it allows the brain to sort of separate out from the old brain stuff and um and
just have run of it sometimes it comes up some good ideas you know exactly I couldn't attest to
that well um Jeff Hawkins thank you so much for joining us today it's been an absolute honor thank
you well it's been a pleasure you guys are great I really I enjoy all your questions you're really
fun and um I think this is one of the one of the more meaningful conversations I've had in a long
I'm serious you know because you know you know what you're talking about you know what I'm talking
about you asked great great questions it really is deep thinking about all this stuff so I think
that's wonderful thank you we appreciate that thank you so all right all right well I hope it
helps it was good for you guys so that was a wrap how was that guys thank you that's pretty great
it's pretty great um and honestly yeah and I'm really impressed it was really nice to talk to
that me it's just clearly you know you know it's like so much more than was in the book uh it
builds in the book was really just scratched in the surface and I wish we had like you know
like just taken off for a beer and like really dig into some of the degrees yeah what I I love uh
I love people like him because he's so smart and and smart in a very um he has such great common
sense right so when you when you ask him questions he comes back with these really informative answers
that are very uh concrete and you understand what he's talking about so it's really fun to talk to him
I mean a very very great learning experience yeah I got the impression that his research
focus has changed a little bit so when we were doing some preparation for this we were looking a
lot into the htm algorithm which I understand is now um they're pivoting away from that a little bit
so he was talking about some stuff that they're doing with transformers models and sparsity and
quite a few things that I hadn't heard about and also he was really focused on um the particular way
of thinking about cognition using these reference frames which he spoke about in his book which I
think is actually a slight departure from from um some of the stuff the mentors had out about five
years ago yeah I definitely got the feeling that there he had like so much interesting stuff
that's just not quite ready yet for public consumption but um hopefully you know the book
I often had this feeling like I had to say to you just you know it feels like there's more to this
than there is in the book maybe or maybe just needs more time and I think I remember in the
book it took like a year and a half to write or so so maybe you know an idea you know lots of
things have happened um so I do look forward to whatever he's gonna publish in the future
well what I love too is you know he's more than willing to admit look I've changed my views you
know I think yeah lots of people who run into are not able to do that and if you agree with
Sholey's you know measure of intelligence the mark of intelligence is the ability to assimilate
new information and to learn from it and most of the people we taught to just want to stay
adamantly entrenched you know no look this thing that I called XYZ back you know 30 years ago
you're wrong that it was limited to this little area it actually encompasses everything you know
that that works now like nobody ever wants to admit that they've learned or changed or that
science has evolved I want to push back on that a little bit because there is something really
special I know we were joking that science advances one funeral at a time but there is
something really special about people who have an idea and they see it through just like Jan
Lacune and the deep learning pioneers everyone was asking them why the hell are you doing this
in the 1990s and they and they hung on and it was worth hanging on and it's a similar thing with
with some of the approaches from Nementor they've been at this for such a long time now and there
is some really really kind of biologically inspired implausible reasons why this might
lead to something interesting in the future but clearly they've been at this for a long time and
they've been sticking at it well nothing nothing's wrong with perseverance but perseverance without
adaptation is wrong it's just perseverance when you're right is right and perseverance when you're
wrong is bad you're never 100 right and you're never you're never 100 so that's the whole
point is you've got to be able to learn you have to be able to learn well the learning right needs
to be about 0.0 well sure I look up part you send this 3e-4 I think is the correct learning rate
for all matters in life yeah all all the matters specifically yeah I agree like also I'm really
glad I got to talk to Jeff about existential risk or something because of course you know I take
some of the stories pretty seriously yeah it's a very common experience to hear people
dismiss existential risk because they've heard bad versions of the arguments because there are some
horrible versions of these arguments that even from some very smart people like you know some
smart people I feel do the field a great justice then I just felt bad their arguments are um
the end and it was really nice to see you know him say like yeah you know that's something
we're thinking about and um I think honestly a lot of the disagreement comes down to yeah
like I agree with him and almost everything really so that was an interesting I was really
impressed with with his response and as you say you can see it from so many different perspectives
so you can cover it from a grand perspective talking about paper clips and um oh my god but um
oh I lost my try and I thought yeah so so he I read the the less wrong article which you linked
to me was that Steve Burns yeah Steve Burns yeah so that was fascinating so so he was saying that
um the end of the day you've got a robot rover on Mars or something like that and at some point
you need to actually give it something to do and you might give it an instruction and then
it might really want to complete that instruction so it might predict that you're about to give it
another instruction it says oh my god he's about to give me another instruction I've not done the
first thing yet so I need to kill him and that's not completely beyond the realms of possibility
now I I agree with Chalet that I think intelligence is externalized it is embodied and there are many
like natural environmental limiting steps to intelligence and you could talk about
all the replication stuff as Hawkins did but that's just a simple example I'm just an intelligent
agent on on Mars and I I need to have motivation because I need to be told what to do what am I
doing am I building a base and then I can plausibly see how that could be um you know misdirected
yeah I feel like it's it it the intelligence explosion arguments all get mixed up with these
like motivation or uh uh concerns which I think is unfortunate because they really aren't
working at all like you could like these these concerns about you know alignment and about
motivation will crop up whether you know it takes 10 years or 1000 years for API to come to be at
some point as Hawkins I think you know picks so when we put the work beats to be done you know
at some point you just have to build a safety mechanism at some point you have to write a
motivation system so that's why I think this you know a kind of research you know even if
the intelligence blows it doesn't happen it's still somebody who needs to get done
yeah and and there is this thing um I mean you could explain this better than me but
instrumental convergence which is this idea that um you know many seemingly innocuous goals
incidentally lead to dangerous motivations like self-preservation and self-replication and
goal preservation and um I can sign on to that that seems very plausible yeah exactly I mean
you can give a perfect example but like you know you give the rover and the the the job will you
know build a thing but actually just like oh actually I wanted to build something different
the rover you know still was motivated but I went to do the first thing so unless we have like
some really sophisticated you know system inside of it allows it to like you know equally value
doing this or do with another human does also with courage ability and which we don't currently
really know how to implement formally um it's very likely that it will you know having instrumental
goals like keeping itself along it so and or you're stopping you from giving it your orders
like it may be able to damage its entailment or something so you can't give it orders or maybe
if you if you're now if you try to destroy the rover because it's malfunctioning maybe it will
retaliate because if it's destroyed it can't build habitat okay so so yeah there's a lot of goals
like that's also also like to like power seeking goals just seem obvious like yeah if you don't know
how to achieve a goal and gaining your power is probably like always a good move you know getting
more money more social capital you know controlling for people with control of resources like almost
always a good thing like almost no matter how innocuous the goal might seem yeah let's see like
on the other hand I don't care if a rover on Mars tries to retaliate it has like no capability to
retaliate on us at all and so the only kinds of like artificial intelligence I worry about
are ones that are capable of expanding in some capacity like they have to be able to build
things and they actually have to be able to build intelligent things so that's why I brought up the
point of you know if we have a robot we're gonna need one that can build you know somehow or another
replicas of other robots to expand conduct more work replace ones that were broke and whatever
and it's not so infeasible to believe that at some point in the next few hundred years you know
we'll have 3d printing technology more than sufficient to build silicon chips and you know
whatever else and as soon as you have a system that's self-replicating and we don't live in a
perfect world and we have random variation right and they have sets of instructions and also we're
