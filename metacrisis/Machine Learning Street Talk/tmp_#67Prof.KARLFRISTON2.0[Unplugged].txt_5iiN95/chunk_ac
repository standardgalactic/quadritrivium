there's another one, which is even simpler, much, much simpler, that just excuse all the
delicate issues of non-equilibrium or non-equilibrium instead of state densities,
and just sticks and tries to pursue the whole story from front to end using purely the notion
of a Lagrangian and pathological formulations. So technically, that probably isn't terribly
interesting from your point of view or possibly even your viewers, but mathematically, it's been
a real gain getting back that difficult process of getting it simpler and simpler and simpler.
So that's certainly one thing that's strange. If you're asking me what's happened in terms of
uptake and application of these ideas, then I think things are moving more quickly than they were
a year ago. And I'm talking here about the application of or the corollary in terms of
your process theories or the way things work or the way that you might want to build artifacts
or algorithms, which is the active inference. So certainly there's been a much greater uptake
and interest in active inference that I've seen simply by pressure and number of emails and
conferences and workshops. So literally, I think two or three days ago, there was a wonderful
workshop at the Burlitz School of Mind and Brain, which was just about active inference
and free energy principle and a bit of philosophy at the end, which was an interesting mixture of
cocktail of intellectual challenges. But I was really impressed by work, which I had no idea about,
was ranging from simulations of declarative memory, actually getting little
synthetic subjects in a computer to actually told you why they made a choice. So you're taking
fairly standard economic games or the kind of paradigms of people in reinforcement that might
use, but having an epistemic aspect, so seeking for information to optimize their decisions
and their choices later on, but then taking the active inference framework and then putting on
top of that the ability to declare and explain why these decisions were made, leveraging what
distinguishes active inference from reinforcement learning, which is that under the hood, you have
an explicit representation of beliefs, belief structures. So there is a belief based approach
to any decision or any move that you might make. And you can use that to actually get
certain choices of decisions and actions to disclose the belief state. So essentially,
you can get the computer to tell you why I did that. So that was a wonderful example.
Great stuff going on in robotics and robotics. Well, I say great because of my local,
probably quite narrow view of it, but I'm really impressed with what these people are doing.
What they are doing is just taking us right off the shelf. The particular
objective function that underwrites active inference, which is the expected free energy,
and then thinking, well, okay, how can we scale this up? How can we make this work? And they're
bringing things like amortization and all the latest machinery from deep learning machine
learning to amortize some of the more delicate problems of mapping, say from high dimensional
inputs to the beliefs as qualified by the sufficient statistics of distributions.
Very much in the spirit of a variational autoencoder that actually has, at some level,
a very simple belief structure or posterior belief associated with the values at certain
nodes in certain layers, but taking it into the realm of state space models where you've actually
got an ongoing dynamics and engagement as the robot has to have with the environment,
and of course, the added and beautifully challenging aspect of robotics, which is the
inactive aspect, that it's not just a question of making sense of the data quickly and efficiently
in a belief-based way, but using that kind of data assimilation in order to decide what's the
best next move that will enable you to assimilate your data better under certain constraints,
which of course are particularly prescient in robotics. So that was very interesting.
I've noticed there's been a mood coming back to the original theme of communication and
mutual understanding between disciplines and between people who try to take these
ideas forward and converge on the end point, which we might not know when we've got that,
but the simple practical things like much of, in my world, much of the proof of principle and the
demos that try to unpack and render transparent and accessible, the basic ideas are written in
academic software, usually MATLAB, and I wasn't aware of this, but no one else uses MATLAB anymore.
I'm pre-millennial, but apparently you're meant to use Python and Julia and all sorts of things.
I'm afraid so. Well, actually, you use whatever tool works. Well, first of all,
I'm excited to see that you're seeing more uptake of the free energy principle because I think
both Tim and I, after our show last year, now we're seeing it everywhere. We're seeing the
concept everywhere. It's like everywhere we look, we think, oh, there's a connection here between
what we're talking about now and the free energy principle. I'm certainly a fan of people definitely
thinking about it more, trying to come up with practical systems based off of the principle.
I want to come back a bit to what you first talked about, which is the mathematical developments
or exposition, if you will, of the free energy principle. I did mention the free energy principle
made simpler, but not too simpler, which is apparently yet a less simple version than a
simpler version that you had also published, which I haven't looked at. But what I loved about
the paper that I did look over is the formulation of this as stochastic dynamical systems that are
orbiting around a pullback attractor. They are trying to get to this attractor. Of course,
they're never going to arrive there. And that was a very important point that you made in the paper
that, hey, look, this idea that, let's say, life is conducting and optimization is a bit
technically incorrect. It's not doing a formal optimization over the entire marginal distribution
that it has a set of dynamics that it follows. So there's a dynamical system that's going,
let's say, in a sense downhill towards this optimum, but it's moving around. It's being
perturbed by external variation, et cetera. So you get this kind of wandering around the
attractor. And you said that philosophers get very worried about how do you connect
systems like that to inference or to probable inference. And you give a connection, I believe,
at least if I read correctly in that paper, that you did actually give a mathematical connection
between let's show how you can take the dynamics of the system and map it exactly to inference. So
I believe that connection is provided in there. Is that correct?
No, that's absolutely right. And indeed, it is just that connection that is the free energy
principle that you're that. And in a sense, well, in the sense that we're talking about,
it does not surprise me that you are seeing that very simple construct emerge in many,
many different fields and many guises, because that's the whole point of just
trying to evince something that has this cross cutting aspect that you should for anything that
works in the sense that it persists in the context in which you place it, then it must
possess these properties. So what you've just said, which was a very concise and erudite
summary of the contribution of that paper is just an expression of things that persist or
things that exist. So how would you write that down mathematically? Well, if they exist over
a certain period of time, then they have to have an attracting set. What kind of attracting set
are we interested in? Well, we want the most generic, accommodating mathematical description of
systems that does not, I should just qualify this, that does not go quite as far as a quantum
mechanic background free description, but as close as you can get without going without crossing,
going to the other side. And that's a random dynamical system. So what is the attracting
set in a random dynamical system? It's a pullback attractor. So you're starting off with, well,
okay, so we're talking about things that exist in any given context and any given setting.
Things, thickness has to be defined. That's the Markov black kid. What does, what does that imply
having a Markov black kid? Well, if it exists sufficiently long for you to be able to, for
the conditional dependencies to be expressed so that you can write down there or assert that there
is a Markov black kid, then you have to have a degree of persistence or existence. So thickness
entails persistence over time that entails and necessitates then the pullback attractor.
Just still picking up on the sort of, you know, the possibility of
over interpreting the role of a pullback attractor in the sense that, of course,
the definition of a pullback attractor would really be the attracting set that
attracts all states as ticas to infinity. Is that a really relevant construct when we're
talking about real systems that have an itineracy, a separation of temporal scales,
as soon as you invoke a separation of temporal scales, which you would have to to talk about
the difference between inference and learning, learning and development, development and evolution.
Are you licensed now to restrict yourself to a random dynamical system or the kind that could
be written down by a logical system with an attracting set that may, that requires t equals
infinity for its definition? Clearly not. So you have to, I think, sort of confront the
certain interpretations of how you are using this notion of a pullback attractor. And that's
certainly been something that's been going on in the past year or so. Now,
large in conversation, I repeat, with people who are mathematically fluid, but their primary
agenda is more an interpretation and philosophical interpretation. What's come out of that or
certainly a reflect one motivation for that kind of discussion is this notion of separation of
temporal time scales. So in a nutshell, we've stopped saying talking about self-organization
to non-equilibrium steady state, because that implies that we're waiting until a system has
reached t equals infinity and is at steady state. And that's never what was in mind. We were using
the steady state and the notion of an attractive set simply to say that this set of differential
equations has a solution that is non-trivial in the sense that the states don't go off to
pessimise infinity. So if they don't go off to pessimise infinity as time proceeds, then there
must be an attracting set. There must be a solution, a non-trivial solution. And it so happens what
can interpret that non-trivial solution in terms of a non-equilibrium steady state solution. If
you've got a Markov blanket that endows it with the fingers and you've got solenoidal flow to make
it a non-equilibrium steady state. And then the question comes, okay, so if you're just using
the notion of a pullback attractor in its implicit steady state solution to the density dynamics,
does do the density dynamics in on themselves have any mathematical or theological role in them?
And can they possibly if the only, if this thing object is only defined in the infinite future?
So what you do that says, well, no, all we meant was for a suitable period of time, there are
certain equations of motion, certain dynamics in play that have a solution. And that the way
that you get to a probabilistic understanding of the behaviours and the dynamics that this
set of equations or this dynamics has is just by looking at the underlying Lagrangian
and looking at the dependencies not in terms of the states occupied, but purely from the
point of view of the paths that the system takes for this period of time. And then that
affords the opportunity. And I think this, I should have mentioned this when I asked you
what's what's been happening recently from point of view of the free energy principle.
Then you say, well, what do you mean by a certain period of time? Well,
then you get now to the separation of temporal scales. So you have this notion now,
where you can understand the the path that rejects the dynamics of this thing that is
defined by Markov blanket for this period of time. And then you say, well, let's make the
equations of motion drift at a slower temporal scale. And then let's repeat the game. And how
will we do that formally? Well, we'd use the renormalization group or the apparatus of the
renormalization group. And then what you have is now you've now freed yourself from the Markovic
constraint. So the now the universe as now articulated in terms of these series of random
differential equations, where the random differential equation at any one level supplies
the control practice of the parameters of the equations of motion for the dynamics at the
level below for a period of time, means that you've now have the opportunity to look at how
the realization of a free energy principle, say in terms of active inference, at one level,
is deeply literally contextualized in a hierarchical or renormalized renormalization sense
by exactly the same form and dynamics at the level above. So now we've come back to evolution.
Now we've got a slow process. And perhaps that's a little bit grand. Now we've got let's make it
slightly simpler. Now we've got a nice mathematical distinction between inference and learning.
And I'm focused on that because clearly many of your your viewers will be most machine learning
advances in that field. And what this brings to the table now, this implicit separation of
temporal scales, is quite a fundamental difference between inferring and learning.
And immediately tells you that to optimize the weights of a deep neuron network or the parameters
of variational to encoder or genital to serial network, you have to do it after you've done
some data simulation after you've done some inference. So because you've got the separation
of temporal scale, both at each scale exactly the same principles apply, and can be both
formulated in terms of the free energy principle or self evidence, so just maximizing the marginal
likelihood at that level. But the connection between the two now enables you to think about
how learning can contextualize inference, and how inference supports learning. So
by inference here, I simply mean optimizing beliefs or maximizing the marginal likelihood
of basing more limits or its elbow or its variational free energy atoms. With respect to
belief distributions or probability distributions over things that change quickly,
states of the world. So the kinds of things that you would do, you would need to infer
when doing surveillance or doing your speech understanding, or indeed in an active
inference sense speech production, as opposed to those things that fluctuate slowly,
which are the connection strengths, the weights, the parameters, the laws, the contingencies that
are in this particular context. Now, there's going to be another level above, which is
a slow change in the context that you can learn. And then you can start to understand
right from the Helmholtz machine, right through to amortization, your notions of learning to infer
is optimizing the slow process to make the fast process optimal. And as such,
noting that the way in which you optimize the fast, the slow stuff depends very much on having
some optimal inference scheme or data assimilation scheme at the fast level.
Pursuing the argument, it also tells you, well, the stuff all the way down and stuff all the way
up in terms of separation and table scales, you naturally now start to ask, well, what's the
scale above the learning? And this would bring us back to this more evolution and model selection
and structural learning. How many nodes, how many factors, what's the actual structure,
do I use a convolutional neural network? All of these are structural considerations
that you could use, for example, a genetic algorithm or a launch of an assisted MCMC
at a very, very slow type scale to now optimize the very structure of your variational autoencoder
that's doing the learning to infer that's enabling inference at a very fast timescale.
That leads to an interesting idea. So having different timescales for different levels of
the structural learning, and I don't know if that's been tried out yet, but I think it's
definitely worth trying. I think I have a different question, though, about the paper.
And thank you for that exposition. That was very helpful. One thing that's kind of occurring to
me now is that, again, the concept of the Markov blankets are still fundamental. And one,
like perhaps concern, I don't know if I should say concern, that I had with the paper is the way in
which it formulated the Markov blankets in terms of the curvature of the surprise being zero.
As people may remember, the sensory sets are those sets which the external world
can control, and you can view, but you don't control. And your active states are the ones that
you can control, not the external world, and the external world can view them. And so there are
these sort of four regions. And in the paper, you define ways in which, let's say, you can set
various terms to zero, which mark the boundaries between those regions. What I'm kind of wondering
is, do they have to be zero? What if this is generalized a bit to have a degree of active
stateness and a degree of sensory stateness, and there was kind of a more smooth boundary between
these? What would happen? I mean, would that give us any useful outcomes mathematically,
or would it just get in the way, and it's just more convenient to continue zeroing out certain
elements of the matrix? Has any thought been put into what if it was more of a matter of degree,
and would that be helpful, or just get in the way? Now, that's a very astute question.
I think quite a lot of thought has been directed at that problem, whether there have been any
useful answers, and that is another question. So just if I can paraphrase your question,
I think a really important one. What you are saying is that before we make any moves in terms
of understanding the dynamics of things that persist in any given context, we have to define
