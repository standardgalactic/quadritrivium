line of the introduction says, it is said that the free energy
principle is difficult to understand. This is ironic on three counts.
First, the free energy principle is so simple
that it is almost tautological. And what occurred to me there was
one of the most valuable life lessons I got in graduate school
was I was in a cryptography and cryptanalysis class.
And it was being taught by Professor Silvio McCauley.
And he's since gone on to quite some fame in the crypto
cryptographic space. And the first day of the course, okay, he puts a secure
communication problem on the board, the very first thing that he does on that
day, puts a secure communication. These are kind of like brain teasers in a way.
You have to come up with a way to solve this problem.
And he goes, okay, does anybody know how to do this?
Somebody raises their hand and gives them an idea. And he goes, no, that won't
work. Here's why. And he shows how to crack that.
A couple minutes later, another student, how about this?
Two random numbers communicate. No, that won't work either. Here's how to crack
that, right? And so after about five or six
attempts from the class, maybe 15 or 20 minutes, nobody could solve it.
And he goes, okay, this is the perfect place to begin this course.
Because I'm going to show you the answer. And when I do, it's going to be so simple
that you're going to say, huh, that's easy. No, something that's easy is easy from
the beginning. If somebody has to tell you the answer, it's only simple, right?
And I think, you know, human history is just full of examples of ideas that are
very, very simple, yet took, you know, the combined intelligence of humanity thousands
of years to discover, they may be very simple ideas, but they're still very
difficult to kind of grok to really internalize and
understand. And I think we have those all over science and the free energy
principle may well be one of those. So even though it may be very simple, it
could be difficult to kind of cognitively
grasp. But I almost, I'm almost starting to think over the last year or so that
human cognition itself, like in the cognitive space, there's
sort of this Markov boundary between an external world of ideas and internal.
And there's a fluctuation there at the boundary where sometimes
the most advanced ideas, they may creep into what's in our sensory range and we
can just grasp them, but then they kind of fly out again.
And they're just always on that cuss of what we can really understand.
Just curious what you think about that. I think that's a very compelling
formulation of the, and I've never heard that before, the distinction between
easy and simple. As you were talking,
I was mindful that everything you said could in one way
be applied to evolution. And indeed, in terms of the
you know, the
evolution of ideas and the scientific process itself
cast as in-cultured evolution, your sort of co-evolution or evolutionary
psychology, as one special case of the evolutionary
process. You could argue that evolution is a really
solving a really difficult problem to disclose a really simple solution.
And that simple solution, of course, is you and me.
Just finding the simplest phenotype artifact
that fits with the eco-niche, that is co-constructed by other
artifacts and simple constructs, Markov-Blockitz, that constitute the
eco-niche. I think that's a wonderful insight.
And the simplicity bit, I think, is really quite crucial
in the sense that, well, let's take the cryptography or cryptanalysis,
a mindful of one of the very first introductions of
variational free energy by David McKay, who just applied the free energy
minimization to the problem of encoding and
cryptographic analysis from the perspective of minimum description
lengths and compression, which speaks to an alternative
view of the imperatives for any
efficient representation or evidence accumulation or data
dissemination scheme or communication scheme.
In the sense, I mean that in the sense that you can either read
the history of this from Richard Feynman and the pathological
formulation of quantum electrodynamics, or you can go right back to
algorithmic complexity, Kolmogorov complexity, Solovov induction,
through to minimum description length, compressibility, and right through to
universal computation. Both at their heart have this notion
of simplification as aspiring to or converging to
a minimum complexity, a minimum description length,
a maximum compressibility. So if you read you and me as the products of
evolution that are the most efficient
installation of all the information that I need
to engage with this equinish, that we have co-constructed,
then exactly the same principle emerges. I guess what you're saying is
and I think you're absolutely right that they, those kinds of principles
not only apply to our structural field of types and
functional architectures that we have in our brain and indeed
our bodies, but also the cultured constructs, what's
people like Cecilia Hayes would call cognitive gadgets.
Things that, beyond a canoe which has evolved, but in a way very
differently from the way that you and I have involved,
beyond that to things like language itself, where did language come from,
how does that simplify our sense making and efficient representation
of causal structures, structure in the world. So I think there's some really
profound common themes, cross-cutting themes
in that description and in a sense, of course, spotting, which is not easy,
but spotting those simple cross-cutting themes is exactly the process that
we're talking about. Right, right. And I often get the feeling that
many of us or many researchers out there are all orbiting this kind of
central truth and it's not just a, there are the cultural constructs, the physical
constructs and some abstract concepts that are somehow
grounded in the core, you know, reality of physics and the universe and
feel like we're all orbiting them and we haven't quite gotten there yet.
I think the free energy principle may be the closest, you know, but there's
still, there's still gaps, there's still things that we don't understand,
there's still missing parts and it'll be really interesting when,
when somebody finally locks them all together and grasp that,
that simple connection of them all. So how will you know when they've done that?
That's a good question. How would they, how would you know when you've done that?
I guess for me, it would be when we really have a mathematics,
when we really have a mathematics of cognition. So it's, it's close, but,
but still far away in a sense. Yeah, I agree entirely. You know, and then that
simple point, having that, I mean, if you're committed to this notion that
there is an evolutionary process in play, which in my world, I would read as
nature's way or natural Bayesian point of selection or optimizing your explanations
for the world with respect to the evidence, the Bayesian model evidence for that.
So I look at natural selection as your, the, the process of Bayesian model selection.
And interestingly, just coming back to your example in your first class with,
with your well-respected professor, he was illustrating there, I think,
the process of Bayesian model selection by putting forward alternative hypotheses,
eliciting them from the audience and then evaluating the evidence for them.
By in this instance, falsifying because that particular strategy was easy to crack.
But illustrating that this is a process of selection and all
evincing a deep problem, which is generating the high policies, the models, the phenotypes,
from which to select. So I think that, that's why I think the evolutionary
evolution perspective is, it is so compelling, you know, speaking to issues, the difficult
problems, which in, I think in radical constructivism would be in a structural learning.
You know, it's easy to score a good structure, a good idea, a good explanation, a good hypothesis,
a good code algorithm. It's easy to score how good it is, but to actually create a portfolio
or an ensemble of alternative to build a hypothesis space in the first place.
That's the uneasy bit, that's the difficult bit, because what we're talking about is just
being a good scientist. It's asking the right questions by having the right kinds of hypotheses
for any good search for evidence. Again, that's a very nice example to bring to the table. Now,
completely lost what we were talking about. What did you bring to the table with your question?
Oh, no, no, it was just a comment that we all should take care that simple concepts
can be very, very hard. And furthermore, simple concepts can be extremely valuable.
So the two lessons I learned there were that simple and easy are not synonyms, and simple
and trivial are not synonyms. And you also highlighted the importance of a precise and
simple way of expressing those ideas in terms of mathematics. I'm tempted to ask you,
but why mathematics and perhaps mathematics is the simplest, most self-consistent calculus
language that you could adopt. Furthermore, it is the kind of language that you could
use to communicate and share in the sense of this cultural approach to co-evolution
with people who don't speak your native language with your Vietnamese colleagues or
people who don't actually communicate it in narrative form. But you can still, in a very
precise and ambiguous, simple way, express those ideas and those hypotheses even mathematically,
and it will still be understood at how meaningful somebody in another culture,
in another world, speaking another language. So on that view, if you had to commit to a
particular language, then it really has to be mathematics, which is actually a bit worrying
in the sense that not everybody is mathematically fluent, which brings us back to, you have to
understand certain simple mathematical fundamentals before you can even start talking.
Yeah, although interestingly, it's a little bit like Tetris, so you don't need to understand
that many fundamentals about maths until it has an extrapolative effect in many different parts
of experience space, if you like. But Professor Friston, I wanted to talk a little bit about
the low road and the high road. You often say that the low road is the mechanistic view of
predictive coding and neurons in the brain and all the low level stuff. The high level road,
being the emergent and the self-organising phenomena, let's just do the low road first.
We want to get to the high road later, but we just had a wonderful conversation with Professor
Yoshua Benjo, the godfather of deep learning, and he's doing something very similar to you.
He's got this invention called G-Flow Nets, and he's using that in place of the variational
inference technique that you're using in active inference to compute the free energy.
G-Flow Nets can be thought of as a drop-in replacement for Markov chain Monte Carlo
for computing unnormalised energy functions, but without the handcrafted priors.
Even if the modes and the energy function are thin and far between and in a high-dimensional
setting, it's a kind of machine-learned version. I think you'd find it very interesting.
But anyway, Benjo is also a huge fan of this distributional reinforcement learning and
balancing entropy in a principled way. When I pointed out the obvious similarities to your
work in active inference, he spoke very positively, so you need to check that episode out.
Lovely. I will do so. Yeah, I've never heard of that, but I have heard him talk before. I do
sense he's certainly becoming more of a physicist as he gets older.
A probabilistic physicist, I should say. Yeah, that's very nice.
So what's changed in the last year for the free energy principle?
Let me think. Well, one thing that, interestingly, that we've been working on is indeed this
communication and trying to sort of get towards a simpler exposition that people
can more readily access and use in their own setting. Part of this is until spotting what
the important simple contributions are and what it can do, what it can't do, what it's
there for, what it's not there for. So trying to pare back the fundamental parts of it from
those secondary auxiliary issues, which are more interpretational in nature. So from a
technical point of view, it's just been staring at the ideas, thinking about the ideas and seeing
what is the absolute essence of this and trying to articulate that mathematically as simply as
possible, but not making it too simple. The one technical aspect here is what is the precise
role of the Markov blanket? And to what extent does that rest upon the notions that you would
inherit from complex dynamical systems theory relative to a more probabilistic construction
of the kind that, say, Judez-Pil's formulation would speak to? Probably more importantly,
how does one relate to the other? So how does a sparse coupling of a dynamical sword lead to
certain forms of conditional independence? Those conditional independences then underwrite
the probabilistic formulation of the coupling between inside and outside of a Markov blanket,
which is at the heart of the high road to thickness and self-evidencing of the free energy
principle. So that's certainly been a focus, I have to say, slightly inspired by skepticism in
not from mathematicians or machine learning or artificial intelligence, but from philosophy.
So the philosophers become very exercised about these things and like to drill down on the
precise implications of anything that they understand when they read the map. And certainly
one thing which has caused puzzlement in that field is how do you constitute or where do you place a
mathematical description in terms of complex dynamical systems theory and in particular the
theory of random dynamical systems? How do you place that in relation to more conventional
conditional independence, sort of Perlian Markov blankets? A lot of work has been
focused on that. And just for your interest, the the upshot of that is possibly going to be a move
away from casting things in terms of non-equilibrium steady states, casting things in terms of the
probability distribution of a system, a Markov blanket system at t equals infinity, as they
underlie, if you like, aspiration, the probability densities that prescribe and shape behavior.
A move back to the much simpler formulation in terms of Lagrangian's path integrals. So this
actually is a nice reflection of this trying to get back to the kernel and the simplicity.
And interestingly, in this instance, right back to the inception of certainly of my reading of history,
of variation of free energy in the context of the path integral formulation. So just getting
right back to what is the Lagrangian? What is the gerative model under the hood? How is that
manifest in the equations of motion? And what does that look like in terms of the implicit
information geometries that come from the probabilistic concomitance of these random dynamical systems?
So I imagine you were talking previously about the most recent attempt to simplify,
demystify the free energy principle. Before that paper was actually put on archives,
