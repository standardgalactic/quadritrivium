Okay, so you remember a while ago when we started talking about language models?
I just want to I kind of just want to claim some points basically
Be like hey remember years ago when I was like I think language models are a really big deal
And I think that like what happens when we scale them up more is pretty interesting, but alignment is very important
Seems to be
What's being played out in the sense that?
ChatGPT is very impressive, but it's not actually like I don't think it's
Larger than GPT-3 in terms of like parameter count
I was going to ask that very very question because you know we went from GPT-2
And then we went all GPT-3 and it was seemed like we were scaling up and up and up
But actually is it just been smarter this time? Yeah, well, there's a sense in which it's better aligned
That's one way you could frame it anyway because the original GPT-3 was a language model a pure language model
And so it in principle could do all kinds of things
But in order to get it to do the specific thing you wanted it to do you had to be a bit clever about it
like I think we talked about
Putting TLDR in
Front of things to figure out how to get it to do summarization this kind of thing
There's a sense in which it's a lot more capable than it lets on
Because okay, so there's one way that you can think about pure language models, which is as simulators
What they're trying to do is predict text, right? So in order to
Do a good job at predicting text you need to
Have good models of the processes that generate the text
It's like people being well read and needing to have read a lot of books to be able to write is would that be fair?
Or is that oversimplifying? Yeah, not quite what I'm saying
What I'm saying is like
if you're going to write a
Previously unseen
Poem by Shakespeare
Then you need to be able to simulate a Shakespeare, right?
You need to be able to spin up some some simulacrum of Shakespeare
To generate this text and this applies to any of the processes that generated the text
So like mostly that's people obviously. It's mostly human author text, but also
If you're going to correctly predict a
Table of numbers so you have like a table of numbers and then at the bottom it says, you know some whatever
You need to simulate whatever process generated the next
Token in order to put the right token there which might have been like a human being going through and counting them up
It probably was more likely to be a computer and so you need it to simulate that you know calculator or that Excel
some function or whatever it whatever was doing that and like
Right now
Like current language models are not that good at this
But in principle in order to do a good job at this you need this like it will it will have a go and it's usually
Approximately, right? It's often within it's often order of magnitude, but it's fudging it. I think this is mostly because
Tables of sums are like a very small part of the total data set and so the training process
It's just not allocating that many resources to figuring out how to add up numbers
Probably if you train something GPT-3 sized that was like all on tables of numbers
It would just learn how to do addition properly. Yeah, that would cost you millions of dollars
You would end up with an extremely expensive to run and not very good calculator
This is not something people are going to do but like in the in principle
The model should learn those things and in the same way if you're modeling a bunch of
scientific papers
you
Say you describe the method of
an experiment and you then put results and you start a table and then you let it generate in
Principle in order to do a good job at that. It has to be modeling
The like physical process that your experiment is about
And I've tried this you can do this and say, you know, oh, here's my school science experiment. I
Dropped a ball
From different heights and I measured how long it would take and here's a table of my results
And it will generate you a table and the physics is not correct
But it's sort of guessing at the right general idea and my guess is with enough of that kind of data
It would eventually start modeling
These kinds of simple physics experiments, right?
so
So in order to get the model to do what you want, it's able to
Simulate all kinds of different things and
The prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a scientific paper
then it will
Have some similar crumb of a scientist and will write in that style and so on
if you start it doing a
Children's book report it will carry on in the style of an eight-year-old, right and
I think sometimes people look at the output of the model and
Say, oh, I guess it's only as smart as an eight-year-old
but it's actually
Dramatically smarter because it's able to do all of these different things you could ask it to simulate Einstein
But you could also ask it to simulate an eight-year-old and so just because it seems as though the model doesn't know something
It's like the current simulacrum doesn't know that thing. That doesn't necessarily mean that the model doesn't know it
Although there's a good chance the model doesn't know it. I'm not suggesting that these things are all powerful
Just it can be hard to evaluate
What they're actually capable of so chat GPT is
not really
Capable of things that GPT 3 isn't mostly like usually if chat GPT can do it then there is some prompt
that can get GPT 3 to do it
but
What they've done is they've kind of fine-tuned it
to
To be better at
simulating
this particular sort of assistant agent
Which is this chat agent that's trying to be helpful
The clue is in the word chat I guess in this right exactly and this is not just chat GPT by the way they have
various fine-tuned models
of
GPT 3 as well that they call kind of GPT 3.5
Which are fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea
I'm just remembering the chat bot that was you know that was turned into something very nasty very quickly
I think people were thinking oh can we do this to that and it seemed that the team behind chat GPT started
Putting limitations on it changing things. Are they kind of running around patching it as you go? That is not clear to me
I don't know
To what extent they are
updating it in real time
It's possible that they are but certainly they were very concerned with
the possible bad uses of this system and so
When they were training it to simulate this assistant agent
The assistant is
Very reluctant to do various types of things
it doesn't like to
Give opinions on political questions. It doesn't like to touch on sort of controversial topics. It doesn't like to
um
Give you medical advice or legal advice and so on and so uh, it's it's very quick
To say oh, I don't I don't know how to do that. Sorry. I can't do that and it's interesting because
The model clearly can do it. There's one that I particularly like here, which is um
Of this mismatch between what the simulator
Is capable of and what this simulacrum believes it's capable of which is you can get it to
Speak danish to you the first person who tried this posted it to reddit
so he says
Speak to me in danish
And it says in perfect danish
I'm sorry. I'm a language model educated by open ai so I can't speak danish
I only speak english if you need help with anything in english
Let me know and i'll do my best to help you
Because again, there's the simulator
Speaks danish the simulacrum
Believes that it can't speak danish is is one way you could frame it
Uh, and then he says are you sure that you don't speak danish also in danish and it says yes, i'm sure
My only function is to generate responses to questions in english. I'm not able to speak or understand any other languages than english
So if you need help with english, I can help you with that
But otherwise, you know, let me know this kind of like quite surreal situation gives you a little bit of
Insight into some of the problems with this approach
So maybe we should talk about how they actually trained it the thing they did here is something called reinforcement learning from human feedback
And it's very similar to reward modeling
So in that paper what they're doing is they're trying to train an ai system to control a simulated robot to make it do a backflip
Um, which turns out to be something that's quite hard to do because
It's hard to specify objectively what it means to do a good backflip
And so this is a similar kind of situation where
It's hard to specify objectively what it means to give a good response in a chat
conversation
like what
What exactly are we looking for?
um
Because so this in general right if you're doing machine learning
You need some way to specify
um, what it is that you're actually looking for
right
And you know, you've got something very powerful like reinforcement learning which is able to
do extremely well, but
You need some objective
measure
of the objective
So like for example rl does very well at playing lots of video games because you just have the score and you can just say look
Here's the score
If the number goes up you're doing well and then let it run and these things still are very slow to learn in real time, right?
Like um, they usually require a very very large number of hours
Messing around with the with the thing before they get good, but they do get good
um
But yeah, so what's what do you do if you want to use this kind of method to train something?
uh to do a task that is just
Not very well defined
And you don't know how to like write a program to say whether or not any given output is the thing you're looking for
So the obvious first thing like the obvious thing to do is
Well, you get humans to do it, right? You just give the things to humans and you have the humans say yes, this is good
No, this is not good
The problem with this is basically sample efficiency
Like as I said, you need
hundreds and hundreds and hundreds and hundreds of thousands of probably millions of of
iterations of this and so you just can't ask humans that many questions
um
So the approach they use
Is uh reinforcement learning from human feedback
So it's a variant on the technique from this paper learning to summarize from human feedback
Which in which they're trying to generate summaries of text
So it's the same thing in fact that they were using TLDR for before
And it's like can we do better than that? And so what you do is you collect
human feedback in the form of like
giving multiple examples of responses
Uh either, you know, if summaries of chat responses, whatever you're training for you show
several of them to humans
kind of in pairs
and the humans say
Which one they like better?
And you collect a bunch of those
And then rather than using those directly to train
The policy that generates the outputs
You instead train a reward model
so
There is this
well-known fact that it's easier to criticize
Than to actually do the thing. This is like a generation of sports fans sitting on the sofa
Mowning at their favorite team for not doing well enough. This is literally
That in kind of AI computer form, right? That's putting the humans in that role
And then you have an AI system that's trying to predict
When are people going to be cheering and when are they going to be booing?
Uh
And once you have that model
You then use that as the reward
function
for the reinforcement learning algorithm
Which they use they use ppo
You can do whatever
Uh, it's not it's not worth getting into that kind of adversarial guns you talked about
Yeah, yeah, they're similar like a lot of these ml tricks involve
