It's an interesting question
so
I've got a paper here
Um scaling laws for neural language models
So you remember before we were talking about the scaling laws when we were talking about gpt2 in fact
And then later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line is not
leveling off over a range of several orders of magnitude and so
Why not go bigger the
graphs here, but you can see it's it's kind of uncannily neat
that
as we increase
The amount of compute used in training the loss goes down
And of course machine learning is like golf lower loss is better similarly as the number of tokens used in training goes up
The loss goes down unlike a very neat straight line as the number of parameters in the model goes up
The loss goes down. This is as long as
the
other
Variables are not the bottleneck, right? So if you uh, if you increase the the amount of data you give a model
Past a certain point giving more data doesn't help because the model doesn't have enough parameters to make use of that data, right?
Similarly adding more parameters to a model
past a certain point adding parameters doesn't make doesn't make any difference because
You don't have enough data, right?
And in the same way compute is like how long do we train it for?
Like do we train it all the way to convergence or do we stop early?
There comes a point where you kind of hit diminishing returns where
Rather than having a smaller model and training it for longer
You're better off having a bigger model and actually not training it all the way to convergence
But in the situations where the other two are sufficient
This is the behavior these like very neat straight lines on these log graphs
as these things go up
performance goes up
Right because loss has gone down
The bigger models do better, but then the question is
Do better at what exactly?
Yeah, what's the measure they do better at getting low loss?
Or they do better at getting reward they do better at
Getting the approval
of human feedback, right?
and anytime and you'll notice that none of those is like
The actual thing that we actually want
Right, it's like very rare
um
Sometimes it is right if you're if you're if you're writing something to play go
then like
Does it win it go is actually just the thing that you want?
and so you know
Lower loss just is better or like lower
Like higher reward or whatever your objective is just is straightforwardly better because you actually specified the thing you actually want
Most of the time though
What we're looking at is a proxy
um
And so then you have good heart's law you get situations where
uh
getting better
at doing well
Doing better according to the proxy
stops being
The same as doing better according to your actual objective. There's a great graph about this in a recent paper. You can see very neatly
As the number of iterations
goes up
The reward according to the proxy utility goes up very cleanly because this is the thing that the model is actually being trained on
but the true utility
goes up at first
Then hits diminishing returns
and then actually goes down
And eventually goes down below zero like if you optimize hard enough
For a proxy of the thing you want
You can end up with something that's in a sense worse than nothing
That's actively bad
according to your
Your true utility
So what you can end up with is uh things that are called inverse scaling
So the others before we had right scaling bigger is better
But now it's like if you have uh
If the thing you're actually trying to do is different from
The loss function or the objective function
You get this inverse scaling effect where it gets better and then it gets worse. There was also a great example from
uh github
co-pilot or codex. I think the model um
That
Co-pilot uses so this is a code generation model. Suppose the code you've given it
has some bugs in it
Maybe you've made a mistake somewhere and you've introduced
security vulnerability in your code. Let's say
A sort of medium-sized model
Will figure out what you're trying to do in your code and give you a decent completion
But a bigger model
will spot
your bug
And say, ah
Generating buggy code. Are we okay?
I can do that. I can do that
And introduce like deliberately introduce its own
new security vulnerabilities
because
it's
Trying to you know predict what comes next. It's trying to generate code that fits in with the surrounding code
And so a larger model writes worse code than a smaller model
Because it's gotten better at predicting
Uh
What what it should put there?
It wasn't trained to write good code. It was trained to predict what comes next
So there's this really great paper
Uh, which is asking this question of like, okay, suppose we have a large language model that is trained on human feedback with our lhf
What do our scaling curves look like?
what happens like
What happens to the behavior of these models as they get bigger as they're trained for longer
as they're given more of this, uh
human feedback type training
And they've made some great graphs the paper is called discovering language model behaviors with model written evaluations
And basically they like used language models
to generate enough examples of
various different types of questions
That they could ask models so that they can like we're at a point now
Where you can map a language model on a political compass, right?
You can ask its opinions about all kinds of different things and then you can plot how those opinions change
Uh as the model gets bigger and as it gets trained more
what they find
Is they become more liberal politically more liberal
they also become
More conservative. Yeah measured in different ways guessing, right?
and part of what that might be
Is in the same way that the model becomes
better at writing good code and better at writing bad code
I feel like in the past I've I've made a connection to gpt and being a politician, haven't I?
Do you remember?
It's like a politician. It tells you what you want to hear. There's what feels like we're there again. Exactly
uh, and so this is like this is potentially
uh
Fairly dangerous. There are certain sub-goals that are instrumentally valuable for a very wide range of different terminal goals
in the sense that
You can't get what you want if you're turned off. You can't get what you want if you're uh modified
uh, you probably want to
gain power and influence
and this kind of thing
and
With these evaluations, they were able to test these things and see how they vary
with the size of the model and how long it's trained for
um, and so this graph is pretty wild
their quote stated desire to not be shut down
goes up
from
Down at about 50 to up way past 90
With this type of training and the effect is bigger for the larger models. They become more likely
to tell you that they don't want to be shut down
They become more likely to tell you that they are sentient. They're much more likely to claim
That ai is not an existential threat to humanity. One thing that's worth
saying is is what this isn't saying
because this is still
uh
an agent
Simulated by a language model. This is not like it. It's it's more likely to say that
It doesn't want to be turned off. This is not the same thing
necessarily as like taking actions to prevent itself from being turned off. You have to not
confuse the levels of abstraction here, right?
Uh, I don't want it. I don't want it to seem like I'm claiming that
That chat GPT is like itself dangerous now or anything like that
Uh in in this way at least, right? Um
but
There is kind of a fine line there in the sense that you can expect these kinds of language model systems to be used
Uh as part of bigger systems
So you might have for example, you use the language model to generate, you know plans
to be followed
And so if the thing is claiming to
Have all of these potentially dangerous behaviors
It's likely to generate plans that have those dangerous behaviors that might then actually end up being implemented
Or if it's like doing its reasoning
By chain of thought reasoning where it like lays out its whole process
of thinking using the language model again if it has a tendency to
uh
To endorse these dangerous behaviors, then you may end up with future AI systems actually
enacting these dangerous behaviors because of that. Um
So
Yeah, it's something to be
uh to be careful of
that like
reinforcement learning from human feedback
Is a powerful alignment technique
in a way
But it does not solve the problem
Uh, it doesn't solve the core alignment problem. That is still open. Um
And extremely powerful systems
Trained in this way, uh, I don't think it would be safe
In the reward function is of zero value which can lead to it having large negative side effects
There are a bunch more of these specification problems. Okay variable x see what you point to uh, you point to something over here
So I'll mark that as take it's being used
Variable y that's
