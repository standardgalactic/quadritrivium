Training models and then using the the output of one model as the training signal for another model. It's uh, it's quite a productive
range of approaches you can get that way so
That's the basic idea, right, but then
you cycle it
so
Once you've got your policy, which so so to be clear the uh, the rl algorithm is able to train
With thousands and thousands of examples because the thousands and thousands of like instances of getting feedback
Because it's not getting feedback from humans. It's getting feedback from this AI system. That's imitating the humans
And then you loop the process. So once you have
this system that's
Trained a little bit more on how to generate whatever it is you're trying to generate
You then get a bunch of those show those to the humans let the humans rate those
Then you keep training your reward model
with um
That new information
And then you use your updated reward model to keep training the the policy
And so it gets better and you can just keep cycling this around
and
It effectively you end up with something that's much more sample efficient. You don't need to spend huge amounts of human time
in order to um
Pin down
The behavior you want in that concrete case you're giving the thing a bunch of chat logs and then the humans can see possible
Responses that they could get and they decide which one they like more
This trains a reward model that's then used to train the policy that generates the chat outputs
The policy that they're starting with
Is this existing
Large language model. You're not really putting new capabilities into the system. You're using rlhf to
select
What simulacra the simulator is predisposed to put out?
and so they fine-tuned it to be particularly good at
simulating this
assistant agent
What's the end goal here for them? I mean, maybe it's blatantly obvious and i'm just missing it. Well, I mean the end goal
For all of these things or at least for open ai and for deep mind is a gi
um
To understand the nature of intelligence well enough to create human level or beyond systems
That are general purpose that can do anything
um
That's the end goal
And like chat gpt is just nothing much. So nothing much
Yeah, I the goal is um, the goal is very grand and I don't think that they're
Uh, they're not really quiet about that
You know, it's there. I think I think deep mind's mission statement is to solve intelligence and use that to solve everything else
What are some of the problems that we face with this or that it faces? It's fine tuned to be good at
getting the thumbs up from humans
and
getting thumbs up from humans is not actually
The same thing as human values
These are not identical
so
The sort of objective that it's being trained on
is not
The true objective
Right, it's a proxy and whenever you have that kind of misalignment you can have problems
So where does the human tendency to?
approve of a particular answer
Come apart from
What is actually a good answer? There are a few different places
One thing is, you know, like basically how good are humans and actually differentiating between good and bad?
responses
if for example
you ask
for
An answer to a factual question
and it gives you an answer
But you don't actually know if that answer is correct
You're not in a position to evaluate. So what it comes down to is
How good are humans at distinguishing good from bad?
responses right anywhere where humans fail on this front
uh
The model we could probably expect the model to fail. Um
So the obvious place. I'm sure we desist the right time to mention youtube comments or not
Ah
So minus side point there is it
So when I see a comment that's critical on a video as a videographer
I think it might be on a technical sense
But equally it could be that they're talking about the content that the person is talking about and
Often it's a combination of both. Anyway, so at side point
But do you sort of mean there are different criteria for deciding whether something is good or bad totally and in this case
all people are doing is saying
Kind of thumbs up thumbs down or which of these two do I like better?
um
So it's it's a fairly low bandwidth
thing you don't get to really say
What you thought was better or worse
um
But this turns out to be enough
Of a training signal to do pretty well
um
But so like for so one example right of a time where maybe this doesn't work is
the
Person asks a factual question
and the model responds
Uh with an answer and that answer is actually not correct
um
Right now
Possibly the human doesn't know the correct answer
And so if the model is faced with a choice
Uh, do I respond with sorry? I don't know
That's definitely going to get me
Uh, not a great score
Compared to do I just like take a stab at it?
Uh, if the humans are not reliably able to spot when the thing makes mistakes and like fact-check it and punish it for that
Uh, it will do that and so chat gpt as we know
Uh, is it is a total bulletish like it will constantly
Uh, it very rarely says that it doesn't know
unless
It's being asked a question, which uh
Is part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know
Even if it kind of does right even if the model itself maybe does
Uh, the assistant will insist that it doesn't
um
So that's one thing if you can't fact check
But then uh more than that
Uh, there is an incentive for deception
right anytime the system is uh
Anytime you can get a more likely to get approval by deceiving the person you're talking to
That's better. Um
And this is a thing that actually did happen a little bit in the reward modeling situation
um, they were trying to train a thing with a hand to pick up a ball
And it realized that there's only it's not a 3d camera
And so if it puts its hand like between the ball and the camera
This looks like it's going to get the ball, but doesn't actually get it. But the human uh
Feedback providers
Were presented with something that seemed to be good. So they gave it the thumbs up
um, so this like general broad category
um
Systems that are trained in this way
Are only as good as your ability
To distinguish good from bad in the outputs
Not all the humans will know the answer is right. So it's what appears to be good
You know, it's having exams marked by non-experts, isn't it? Right. Yeah, exactly in the gpt3 thing. We talked about writing poems
right
and uh for various reasons partly to do with
The way that these language models do their tokenization the byte pair encoding stuff
Uh, the models have a really hard time with rhyme
um
I mean, you know rhyme is tricky, but it's especially tricky when you kind of
Don't inherently have any concept of like sound of spoken language when your entire universe is tokens
Figuring out especially with english spelling
Figuring out which words rhyme with each other is is is not easy. You have to consume quite a lot of poetry to like figure out
Uh, that kind of thing and and getting dpt3 to write good poems is tricky chat gpt
is much more
Able to write poems, but interestingly
It it kind of always writes the same kind of poem approximately
like if you ask it to write you uh a limerick
Or an ode or a sonnet
Uh, you always get back approximately the same
type of thing
And I hypothesize that this is because the people providing human feedback did not in fact know
The requirements for something to be a sonnet, right?
And so if you ask something for a sonnet it again has a choice
Do I try to do this quite difficult thing and adhere to all of the rules?
of like stress pattern
And structure and everything of a sonnet and maybe risk screwing it up or do I just do like a rhyming poem and
kind of rely on the human to
Prefer that because they don't know that that's not what a sonnet is supposed to look like
It's easy to look at that and think oh the model doesn't know the difference between these types of
poems, right
but
you could say
That it just thinks that you don't know the difference
But specifically this comes out of misalignment if it were better aligned
It could either do its best shot a generator sonnet
Or tell you that it can't quite remember how to generate a sonnet
this thing of
with complete confidence
Generating you something which is not a sonnet
Because during the training process it believes that humans don't know what sonnets are anyway and it can get away with it
Right. This is misaligned behavior. This is not a big problem that the thing generates bad poetry
um
It's kind of a problem that it lies
Uh, or that it that it bullshits. This is like
In the short term pretty solvable by just allowing the thing to use google
because like
A person who doesn't care about the truth at all and is just trying to
Say something that'll make you give a thumbs up
uh
Is going to lie to you a lot
but
that same person
With the relevant wikipedia page open
It's going to lie to you a lot less
Just because they don't they don't have to now because they happen to have it in front of them, right?
So you can solve it's a bit like
Yeah, it's the yes man thing, isn't it? You know you you want something you need something
I'm going to give you something because you want exactly exactly
um
And so so so this agent is kind of
Firstly the agent is kind of a coward
Because they won't address any of these
There's a whole bunch of things that it just claims not to be able to do even though it in principle could
and it's also
a complete
sicker fan
Yeah
So then the question we were talking about earlier
Uh, where does this go? What happens when these things get
Bigger and better and more powerful
um
