Okay, so you remember a while ago when we started talking about language models?
I just want to I kind of just want to claim some points basically
Be like hey remember years ago when I was like I think language models are a really big deal
And I think that like what happens when we scale them up more is pretty interesting, but alignment is very important
Seems to be
What's being played out in the sense that?
ChatGPT is very impressive, but it's not actually like I don't think it's
Larger than GPT-3 in terms of like parameter count
I was going to ask that very very question because you know we went from GPT-2
And then we went all GPT-3 and it was seemed like we were scaling up and up and up
But actually is it just been smarter this time? Yeah, well, there's a sense in which it's better aligned
That's one way you could frame it anyway because the original GPT-3 was a language model a pure language model
And so it in principle could do all kinds of things
But in order to get it to do the specific thing you wanted it to do you had to be a bit clever about it
like I think we talked about
Putting TLDR in
Front of things to figure out how to get it to do summarization this kind of thing
There's a sense in which it's a lot more capable than it lets on
Because okay, so there's one way that you can think about pure language models, which is as simulators
What they're trying to do is predict text, right? So in order to
Do a good job at predicting text you need to
Have good models of the processes that generate the text
It's like people being well read and needing to have read a lot of books to be able to write is would that be fair?
Or is that oversimplifying? Yeah, not quite what I'm saying
What I'm saying is like
if you're going to write a
Previously unseen
Poem by Shakespeare
Then you need to be able to simulate a Shakespeare, right?
You need to be able to spin up some some simulacrum of Shakespeare
To generate this text and this applies to any of the processes that generated the text
So like mostly that's people obviously. It's mostly human author text, but also
If you're going to correctly predict a
Table of numbers so you have like a table of numbers and then at the bottom it says, you know some whatever
You need to simulate whatever process generated the next
Token in order to put the right token there which might have been like a human being going through and counting them up
It probably was more likely to be a computer and so you need it to simulate that you know calculator or that Excel
some function or whatever it whatever was doing that and like
Right now
Like current language models are not that good at this
But in principle in order to do a good job at this you need this like it will it will have a go and it's usually
Approximately, right? It's often within it's often order of magnitude, but it's fudging it. I think this is mostly because
Tables of sums are like a very small part of the total data set and so the training process
It's just not allocating that many resources to figuring out how to add up numbers
Probably if you train something GPT-3 sized that was like all on tables of numbers
It would just learn how to do addition properly. Yeah, that would cost you millions of dollars
You would end up with an extremely expensive to run and not very good calculator
This is not something people are going to do but like in the in principle
The model should learn those things and in the same way if you're modeling a bunch of
scientific papers
you
Say you describe the method of
an experiment and you then put results and you start a table and then you let it generate in
Principle in order to do a good job at that. It has to be modeling
The like physical process that your experiment is about
And I've tried this you can do this and say, you know, oh, here's my school science experiment. I
Dropped a ball
From different heights and I measured how long it would take and here's a table of my results
And it will generate you a table and the physics is not correct
But it's sort of guessing at the right general idea and my guess is with enough of that kind of data
It would eventually start modeling
These kinds of simple physics experiments, right?
so
So in order to get the model to do what you want, it's able to
Simulate all kinds of different things and
The prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a scientific paper
then it will
Have some similar crumb of a scientist and will write in that style and so on
if you start it doing a
Children's book report it will carry on in the style of an eight-year-old, right and
I think sometimes people look at the output of the model and
Say, oh, I guess it's only as smart as an eight-year-old
but it's actually
Dramatically smarter because it's able to do all of these different things you could ask it to simulate Einstein
But you could also ask it to simulate an eight-year-old and so just because it seems as though the model doesn't know something
It's like the current simulacrum doesn't know that thing. That doesn't necessarily mean that the model doesn't know it
Although there's a good chance the model doesn't know it. I'm not suggesting that these things are all powerful
Just it can be hard to evaluate
What they're actually capable of so chat GPT is
not really
Capable of things that GPT 3 isn't mostly like usually if chat GPT can do it then there is some prompt
that can get GPT 3 to do it
but
What they've done is they've kind of fine-tuned it
to
To be better at
simulating
this particular sort of assistant agent
Which is this chat agent that's trying to be helpful
The clue is in the word chat I guess in this right exactly and this is not just chat GPT by the way they have
various fine-tuned models
of
GPT 3 as well that they call kind of GPT 3.5
Which are fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea
I'm just remembering the chat bot that was you know that was turned into something very nasty very quickly
I think people were thinking oh can we do this to that and it seemed that the team behind chat GPT started
Putting limitations on it changing things. Are they kind of running around patching it as you go? That is not clear to me
I don't know
To what extent they are
updating it in real time
It's possible that they are but certainly they were very concerned with
the possible bad uses of this system and so
When they were training it to simulate this assistant agent
The assistant is
Very reluctant to do various types of things
it doesn't like to
Give opinions on political questions. It doesn't like to touch on sort of controversial topics. It doesn't like to
um
Give you medical advice or legal advice and so on and so uh, it's it's very quick
To say oh, I don't I don't know how to do that. Sorry. I can't do that and it's interesting because
The model clearly can do it. There's one that I particularly like here, which is um
Of this mismatch between what the simulator
Is capable of and what this simulacrum believes it's capable of which is you can get it to
Speak danish to you the first person who tried this posted it to reddit
so he says
Speak to me in danish
And it says in perfect danish
I'm sorry. I'm a language model educated by open ai so I can't speak danish
I only speak english if you need help with anything in english
Let me know and i'll do my best to help you
Because again, there's the simulator
Speaks danish the simulacrum
Believes that it can't speak danish is is one way you could frame it
Uh, and then he says are you sure that you don't speak danish also in danish and it says yes, i'm sure
My only function is to generate responses to questions in english. I'm not able to speak or understand any other languages than english
So if you need help with english, I can help you with that
But otherwise, you know, let me know this kind of like quite surreal situation gives you a little bit of
Insight into some of the problems with this approach
So maybe we should talk about how they actually trained it the thing they did here is something called reinforcement learning from human feedback
And it's very similar to reward modeling
So in that paper what they're doing is they're trying to train an ai system to control a simulated robot to make it do a backflip
Um, which turns out to be something that's quite hard to do because
It's hard to specify objectively what it means to do a good backflip
And so this is a similar kind of situation where
It's hard to specify objectively what it means to give a good response in a chat
conversation
like what
What exactly are we looking for?
um
Because so this in general right if you're doing machine learning
You need some way to specify
um, what it is that you're actually looking for
right
And you know, you've got something very powerful like reinforcement learning which is able to
do extremely well, but
You need some objective
measure
of the objective
So like for example rl does very well at playing lots of video games because you just have the score and you can just say look
Here's the score
If the number goes up you're doing well and then let it run and these things still are very slow to learn in real time, right?
Like um, they usually require a very very large number of hours
Messing around with the with the thing before they get good, but they do get good
um
But yeah, so what's what do you do if you want to use this kind of method to train something?
uh to do a task that is just
Not very well defined
And you don't know how to like write a program to say whether or not any given output is the thing you're looking for
So the obvious first thing like the obvious thing to do is
Well, you get humans to do it, right? You just give the things to humans and you have the humans say yes, this is good
No, this is not good
The problem with this is basically sample efficiency
Like as I said, you need
hundreds and hundreds and hundreds and hundreds of thousands of probably millions of of
iterations of this and so you just can't ask humans that many questions
um
So the approach they use
Is uh reinforcement learning from human feedback
So it's a variant on the technique from this paper learning to summarize from human feedback
Which in which they're trying to generate summaries of text
So it's the same thing in fact that they were using TLDR for before
And it's like can we do better than that? And so what you do is you collect
human feedback in the form of like
giving multiple examples of responses
Uh either, you know, if summaries of chat responses, whatever you're training for you show
several of them to humans
kind of in pairs
and the humans say
Which one they like better?
And you collect a bunch of those
And then rather than using those directly to train
The policy that generates the outputs
You instead train a reward model
so
There is this
well-known fact that it's easier to criticize
Than to actually do the thing. This is like a generation of sports fans sitting on the sofa
Mowning at their favorite team for not doing well enough. This is literally
That in kind of AI computer form, right? That's putting the humans in that role
And then you have an AI system that's trying to predict
When are people going to be cheering and when are they going to be booing?
Uh
And once you have that model
You then use that as the reward
function
for the reinforcement learning algorithm
Which they use they use ppo
You can do whatever
Uh, it's not it's not worth getting into that kind of adversarial guns you talked about
Yeah, yeah, they're similar like a lot of these ml tricks involve
Training models and then using the the output of one model as the training signal for another model. It's uh, it's quite a productive
range of approaches you can get that way so
That's the basic idea, right, but then
you cycle it
so
Once you've got your policy, which so so to be clear the uh, the rl algorithm is able to train
With thousands and thousands of examples because the thousands and thousands of like instances of getting feedback
Because it's not getting feedback from humans. It's getting feedback from this AI system. That's imitating the humans
And then you loop the process. So once you have
this system that's
Trained a little bit more on how to generate whatever it is you're trying to generate
You then get a bunch of those show those to the humans let the humans rate those
Then you keep training your reward model
with um
That new information
And then you use your updated reward model to keep training the the policy
And so it gets better and you can just keep cycling this around
and
It effectively you end up with something that's much more sample efficient. You don't need to spend huge amounts of human time
in order to um
Pin down
The behavior you want in that concrete case you're giving the thing a bunch of chat logs and then the humans can see possible
Responses that they could get and they decide which one they like more
This trains a reward model that's then used to train the policy that generates the chat outputs
The policy that they're starting with
Is this existing
Large language model. You're not really putting new capabilities into the system. You're using rlhf to
select
What simulacra the simulator is predisposed to put out?
and so they fine-tuned it to be particularly good at
simulating this
assistant agent
What's the end goal here for them? I mean, maybe it's blatantly obvious and i'm just missing it. Well, I mean the end goal
For all of these things or at least for open ai and for deep mind is a gi
um
To understand the nature of intelligence well enough to create human level or beyond systems
That are general purpose that can do anything
um
That's the end goal
And like chat gpt is just nothing much. So nothing much
Yeah, I the goal is um, the goal is very grand and I don't think that they're
Uh, they're not really quiet about that
You know, it's there. I think I think deep mind's mission statement is to solve intelligence and use that to solve everything else
What are some of the problems that we face with this or that it faces? It's fine tuned to be good at
getting the thumbs up from humans
and
getting thumbs up from humans is not actually
The same thing as human values
These are not identical
so
The sort of objective that it's being trained on
is not
The true objective
Right, it's a proxy and whenever you have that kind of misalignment you can have problems
So where does the human tendency to?
approve of a particular answer
Come apart from
What is actually a good answer? There are a few different places
One thing is, you know, like basically how good are humans and actually differentiating between good and bad?
responses
if for example
you ask
for
An answer to a factual question
and it gives you an answer
But you don't actually know if that answer is correct
You're not in a position to evaluate. So what it comes down to is
How good are humans at distinguishing good from bad?
responses right anywhere where humans fail on this front
uh
The model we could probably expect the model to fail. Um
So the obvious place. I'm sure we desist the right time to mention youtube comments or not
Ah
So minus side point there is it
So when I see a comment that's critical on a video as a videographer
I think it might be on a technical sense
But equally it could be that they're talking about the content that the person is talking about and
Often it's a combination of both. Anyway, so at side point
But do you sort of mean there are different criteria for deciding whether something is good or bad totally and in this case
all people are doing is saying
Kind of thumbs up thumbs down or which of these two do I like better?
um
So it's it's a fairly low bandwidth
thing you don't get to really say
What you thought was better or worse
um
But this turns out to be enough
Of a training signal to do pretty well
um
But so like for so one example right of a time where maybe this doesn't work is
the
Person asks a factual question
and the model responds
Uh with an answer and that answer is actually not correct
um
Right now
Possibly the human doesn't know the correct answer
And so if the model is faced with a choice
Uh, do I respond with sorry? I don't know
That's definitely going to get me
Uh, not a great score
Compared to do I just like take a stab at it?
Uh, if the humans are not reliably able to spot when the thing makes mistakes and like fact-check it and punish it for that
Uh, it will do that and so chat gpt as we know
Uh, is it is a total bulletish like it will constantly
Uh, it very rarely says that it doesn't know
unless
It's being asked a question, which uh
Is part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know
Even if it kind of does right even if the model itself maybe does
Uh, the assistant will insist that it doesn't
um
So that's one thing if you can't fact check
But then uh more than that
Uh, there is an incentive for deception
right anytime the system is uh
Anytime you can get a more likely to get approval by deceiving the person you're talking to
That's better. Um
And this is a thing that actually did happen a little bit in the reward modeling situation
um, they were trying to train a thing with a hand to pick up a ball
And it realized that there's only it's not a 3d camera
And so if it puts its hand like between the ball and the camera
This looks like it's going to get the ball, but doesn't actually get it. But the human uh
Feedback providers
Were presented with something that seemed to be good. So they gave it the thumbs up
um, so this like general broad category
um
Systems that are trained in this way
Are only as good as your ability
To distinguish good from bad in the outputs
Not all the humans will know the answer is right. So it's what appears to be good
You know, it's having exams marked by non-experts, isn't it? Right. Yeah, exactly in the gpt3 thing. We talked about writing poems
right
and uh for various reasons partly to do with
The way that these language models do their tokenization the byte pair encoding stuff
Uh, the models have a really hard time with rhyme
um
I mean, you know rhyme is tricky, but it's especially tricky when you kind of
Don't inherently have any concept of like sound of spoken language when your entire universe is tokens
Figuring out especially with english spelling
Figuring out which words rhyme with each other is is is not easy. You have to consume quite a lot of poetry to like figure out
Uh, that kind of thing and and getting dpt3 to write good poems is tricky chat gpt
is much more
Able to write poems, but interestingly
It it kind of always writes the same kind of poem approximately
like if you ask it to write you uh a limerick
Or an ode or a sonnet
Uh, you always get back approximately the same
type of thing
And I hypothesize that this is because the people providing human feedback did not in fact know
The requirements for something to be a sonnet, right?
And so if you ask something for a sonnet it again has a choice
Do I try to do this quite difficult thing and adhere to all of the rules?
of like stress pattern
And structure and everything of a sonnet and maybe risk screwing it up or do I just do like a rhyming poem and
kind of rely on the human to
Prefer that because they don't know that that's not what a sonnet is supposed to look like
It's easy to look at that and think oh the model doesn't know the difference between these types of
poems, right
but
you could say
That it just thinks that you don't know the difference
But specifically this comes out of misalignment if it were better aligned
It could either do its best shot a generator sonnet
Or tell you that it can't quite remember how to generate a sonnet
this thing of
with complete confidence
Generating you something which is not a sonnet
Because during the training process it believes that humans don't know what sonnets are anyway and it can get away with it
Right. This is misaligned behavior. This is not a big problem that the thing generates bad poetry
um
It's kind of a problem that it lies
Uh, or that it that it bullshits. This is like
In the short term pretty solvable by just allowing the thing to use google
because like
A person who doesn't care about the truth at all and is just trying to
Say something that'll make you give a thumbs up
uh
Is going to lie to you a lot
but
that same person
With the relevant wikipedia page open
It's going to lie to you a lot less
Just because they don't they don't have to now because they happen to have it in front of them, right?
So you can solve it's a bit like
Yeah, it's the yes man thing, isn't it? You know you you want something you need something
I'm going to give you something because you want exactly exactly
um
And so so so this agent is kind of
Firstly the agent is kind of a coward
Because they won't address any of these
There's a whole bunch of things that it just claims not to be able to do even though it in principle could
and it's also
a complete
sicker fan
Yeah
So then the question we were talking about earlier
Uh, where does this go? What happens when these things get
Bigger and better and more powerful
um
It's an interesting question
so
I've got a paper here
Um scaling laws for neural language models
So you remember before we were talking about the scaling laws when we were talking about gpt2 in fact
And then later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line is not
leveling off over a range of several orders of magnitude and so
Why not go bigger the
graphs here, but you can see it's it's kind of uncannily neat
that
as we increase
The amount of compute used in training the loss goes down
And of course machine learning is like golf lower loss is better similarly as the number of tokens used in training goes up
The loss goes down unlike a very neat straight line as the number of parameters in the model goes up
The loss goes down. This is as long as
the
other
Variables are not the bottleneck, right? So if you uh, if you increase the the amount of data you give a model
Past a certain point giving more data doesn't help because the model doesn't have enough parameters to make use of that data, right?
Similarly adding more parameters to a model
past a certain point adding parameters doesn't make doesn't make any difference because
You don't have enough data, right?
And in the same way compute is like how long do we train it for?
Like do we train it all the way to convergence or do we stop early?
There comes a point where you kind of hit diminishing returns where
Rather than having a smaller model and training it for longer
You're better off having a bigger model and actually not training it all the way to convergence
But in the situations where the other two are sufficient
This is the behavior these like very neat straight lines on these log graphs
as these things go up
performance goes up
Right because loss has gone down
The bigger models do better, but then the question is
Do better at what exactly?
Yeah, what's the measure they do better at getting low loss?
Or they do better at getting reward they do better at
Getting the approval
of human feedback, right?
and anytime and you'll notice that none of those is like
The actual thing that we actually want
Right, it's like very rare
um
Sometimes it is right if you're if you're if you're writing something to play go
then like
Does it win it go is actually just the thing that you want?
and so you know
Lower loss just is better or like lower
Like higher reward or whatever your objective is just is straightforwardly better because you actually specified the thing you actually want
Most of the time though
What we're looking at is a proxy
um
And so then you have good heart's law you get situations where
uh
getting better
at doing well
Doing better according to the proxy
stops being
The same as doing better according to your actual objective. There's a great graph about this in a recent paper. You can see very neatly
As the number of iterations
goes up
The reward according to the proxy utility goes up very cleanly because this is the thing that the model is actually being trained on
but the true utility
goes up at first
Then hits diminishing returns
and then actually goes down
And eventually goes down below zero like if you optimize hard enough
For a proxy of the thing you want
You can end up with something that's in a sense worse than nothing
That's actively bad
according to your
Your true utility
So what you can end up with is uh things that are called inverse scaling
So the others before we had right scaling bigger is better
But now it's like if you have uh
If the thing you're actually trying to do is different from
The loss function or the objective function
You get this inverse scaling effect where it gets better and then it gets worse. There was also a great example from
uh github
co-pilot or codex. I think the model um
That
Co-pilot uses so this is a code generation model. Suppose the code you've given it
has some bugs in it
Maybe you've made a mistake somewhere and you've introduced
security vulnerability in your code. Let's say
A sort of medium-sized model
Will figure out what you're trying to do in your code and give you a decent completion
But a bigger model
will spot
your bug
And say, ah
Generating buggy code. Are we okay?
I can do that. I can do that
And introduce like deliberately introduce its own
new security vulnerabilities
because
it's
Trying to you know predict what comes next. It's trying to generate code that fits in with the surrounding code
And so a larger model writes worse code than a smaller model
Because it's gotten better at predicting
Uh
What what it should put there?
It wasn't trained to write good code. It was trained to predict what comes next
So there's this really great paper
Uh, which is asking this question of like, okay, suppose we have a large language model that is trained on human feedback with our lhf
What do our scaling curves look like?
what happens like
What happens to the behavior of these models as they get bigger as they're trained for longer
as they're given more of this, uh
human feedback type training
And they've made some great graphs the paper is called discovering language model behaviors with model written evaluations
And basically they like used language models
to generate enough examples of
various different types of questions
That they could ask models so that they can like we're at a point now
Where you can map a language model on a political compass, right?
You can ask its opinions about all kinds of different things and then you can plot how those opinions change
Uh as the model gets bigger and as it gets trained more
what they find
Is they become more liberal politically more liberal
they also become
More conservative. Yeah measured in different ways guessing, right?
and part of what that might be
Is in the same way that the model becomes
better at writing good code and better at writing bad code
I feel like in the past I've I've made a connection to gpt and being a politician, haven't I?
Do you remember?
It's like a politician. It tells you what you want to hear. There's what feels like we're there again. Exactly
uh, and so this is like this is potentially
uh
Fairly dangerous. There are certain sub-goals that are instrumentally valuable for a very wide range of different terminal goals
in the sense that
You can't get what you want if you're turned off. You can't get what you want if you're uh modified
uh, you probably want to
gain power and influence
and this kind of thing
and
With these evaluations, they were able to test these things and see how they vary
with the size of the model and how long it's trained for
um, and so this graph is pretty wild
their quote stated desire to not be shut down
goes up
from
Down at about 50 to up way past 90
With this type of training and the effect is bigger for the larger models. They become more likely
to tell you that they don't want to be shut down
They become more likely to tell you that they are sentient. They're much more likely to claim
That ai is not an existential threat to humanity. One thing that's worth
saying is is what this isn't saying
because this is still
uh
an agent
Simulated by a language model. This is not like it. It's it's more likely to say that
It doesn't want to be turned off. This is not the same thing
necessarily as like taking actions to prevent itself from being turned off. You have to not
confuse the levels of abstraction here, right?
Uh, I don't want it. I don't want it to seem like I'm claiming that
That chat GPT is like itself dangerous now or anything like that
Uh in in this way at least, right? Um
but
There is kind of a fine line there in the sense that you can expect these kinds of language model systems to be used
Uh as part of bigger systems
So you might have for example, you use the language model to generate, you know plans
to be followed
And so if the thing is claiming to
Have all of these potentially dangerous behaviors
It's likely to generate plans that have those dangerous behaviors that might then actually end up being implemented
Or if it's like doing its reasoning
By chain of thought reasoning where it like lays out its whole process
of thinking using the language model again if it has a tendency to
uh
To endorse these dangerous behaviors, then you may end up with future AI systems actually
enacting these dangerous behaviors because of that. Um
So
Yeah, it's something to be
uh to be careful of
that like
reinforcement learning from human feedback
Is a powerful alignment technique
in a way
But it does not solve the problem
Uh, it doesn't solve the core alignment problem. That is still open. Um
And extremely powerful systems
Trained in this way, uh, I don't think it would be safe
In the reward function is of zero value which can lead to it having large negative side effects
There are a bunch more of these specification problems. Okay variable x see what you point to uh, you point to something over here
So I'll mark that as take it's being used
Variable y that's
