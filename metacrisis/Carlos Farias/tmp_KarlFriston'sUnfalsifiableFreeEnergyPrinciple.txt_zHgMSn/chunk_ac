palpating the world in the right kind of way to get that kind of information that's going to resolve
the most expected surprise or the most uncertainty. And you can have that at the level of
visual palpation every four times a second, or the news channels that you would afford epistemic
trust, which is probably quite a hot issue. Or the kind of, whether you've got a Wikipedia,
who would you consult? We make choices all the time in terms of what's the best way to act,
and what are the, what this notion of expected free energy brings to the table is that there are
the imperatives of a dual aspect, this pragmatic aspect that is formally this expected cost,
where the cost of the negative or the surprise in relation to my preferred states of being.
But also, to my mind, even more important is this epistemic part, this sort of expected
information gain that actually makes us all little scientists of one sort or another.
Yeah, that's incredible. I'm glad I'm recording this conversation, because I'm going to have to
listen to that a few times over to fully pull together all the pieces, because there's so much
there. I actually found this when I was doing research on the free energy principle that I was
going off and finding definitions for things, because to bring it all together is, well,
it's amazing that it incorporates so many different ideas from different disciplines.
I mean, you have your neuroscience, but you also have information theory. I mean, it's
physics, you know, you have all these different things that you're pulling together
and finding these deep truths that I think are fascinating.
Well, it's nice you said that, because that's exactly what Chris Fields wants. He wants to
sort of physics, psychology, biology, they're all the same thing. And again, you know,
when I was trying to answer your question, when was the harm moment? There really wasn't one, but
every few months you say, oh, that's just one of those. So to my mind, in a sense,
it is that capacity to accommodate very gracefully things that definitely have worked in the past,
that haven't been framed in this sort of simplifying, sort of unifying framework. It's that
capacity which lends the free energy principle of a veracity and a utility that you continually
impresses me, that when you were able to articulate something that's endured for
centuries or at least decades, that we all use in work, and that in terms of our understanding,
and suddenly fits in with the free energy principle, that to my mind is a reflection
of the utility of the free energy principle, but also interestingly, it means that the free
energy principle is conforming to itself. So if you remember, the whole point of the free energy
principle is finding accurate explanations as simple as possible. So it should provide a simple
explanation for everything. That's its foundational premise. So you shouldn't really be surprised
that you can do economics and move forward with learning. Yeah, one of your my favorite papers
is yours. I think it's the free energy principle made as simple as possible, but no simpler,
I think is the title, or it's close. I'll link to that in the description for folks who want to
read that. I may have butchered that title, sorry, but it's close.
It was Einstein's famous quote. Yes, it's from Einstein, yeah.
I even found that I didn't know this, but there's a great little article in Wikipedia called
the principle of simplicity. So there's actually people like Nick Chaitre and colleagues
have found this formalization of what comes principle. I think the free energy principle
could also be read as a dual to that kind of imperative to find the simple minimally complex
explanations for stuff. So I'd love to, I'll just focus somewhere in time on surprise,
the idea of surprise. It's central to this theory. And correct me if I'm wrong. So we're
trying to minimize surprise or slash uncertainty, right? Does the system want
zero surprise, precisely zero surprise? Or that might be, I imagine that might be impossible,
actually. So is it trying to approach zero? Is that optimally, is that better? Does that make
sense? It does, but again, you're, you're, you're asking as a, as a biologist, not a physicist.
I'll keep mixing that too, sorry.
But there is a lovely paradox here. The notice we've been saying in the past exchange that we are
compelled if we exist as certain kinds of things that have these very precise internal dynamics
to maximize information gain. So we are compelled to seek out novel novelty. We are compelled to
seek out surprising informative, informative outcomes, which needs to be contrasted with
what you've just brought to the table, which is the underlying, the underlying imperative really
is that of homeostasis to minimize surprise. So you've got this interesting dialectic here,
which just falls out of the maths, that in the service of trying to minimize my surprise to
maintain my homeostatic to keep myself within these, within these attracting sets that have a
low entropy. Remember that the entropy is just the average surprise. So if I spend my life minimizing
surprise, I am by definition, when averaged over time, minimizing my entropy, this minimizing
that's the other number of surprising physiological and mental and emotional states that I experience
during that period of time. But to do that, I have to actually go and minimize, maximize my
expensive information gain. So I'm going to look as if I'm actually very curious. So part of keeping
my home, keeping myself out of unfulfilled states and minimizing surprise, which you can
read as like this prediction error, for example, part of underwriting my capacity to minimize my
prediction error is actually to go and seek out surprising or apparently surprising outcomes.
So I will, I will be quintessentially curious, I will be sensation seeking,
I will be wanting to answer, it'll look as if I'm going to want to answer the question,
what would happen if I did that? So it is, you know, when you say we all want to minimize surprise,
we certainly want to avoid surprising sensations. Yes, you know, no deception underwrites our,
you know, beliefs about being in pain, for example, being very cold, being very poor,
being very unloved, being dead. You know, these are all very surprising states of being,
which we will in the moment avoid. But that does not mean to say that we will not seek out
those outcomes that can have, have information. So we're still very, very curious. And I say that
at length, because there was an interesting philosophical paradox called the dark room
paradox that was brought to the table around the inception of the free energy principle
in neurophilosophy. And it went along the following, well, perhaps you know this,
you probably know this better than I do, do you want to tell, tell your viewers what it was?
No, I wish I did, I'm sorry, I don't know about it.
It's a very commonsensical argument, but if it's the case, you want to minimize surprise,
why don't you just go into a dark room, turn off the lights, lie down, stay there forever.
Okay, that's kind of an idea, question ahead, why not just jump off a cliff, right? That's,
you'll know, the surprise will be over and then that'll, yeah, for extreme, but.
Yes, it's interesting that. So why don't we do that? And yet we go bungee jumping.
This is interesting. Yeah, let's go. Jumping off cliffs and bungee jumping are, you know,
they should be explainable under the free energy principle. And of course there's a crucial,
there's a crucial difference between the bungee jump and the jumping over a cliff.
But to come out to the dark room problem, I repeat, it's just the paradox that why don't surprise
minimizing machines or artifacts or systems basically sequester themselves from their environment
and close their eyes or turn the lights off and remove themselves from any particularly
surprising sensations. And the answer to that, well, there are a number of different answers
depending upon how seriously you take the question. Well, less serious answer is, well,
that's exactly what I do every day when I go to bed. So it's not quite that surprising.
Interesting. Yeah, that's a good counter. Yeah.
But, you know, ignoring the fact that we do need some downtime. And there are good reasons for
that, which come back to this separation of free energy into accuracy and complexity.
But more importantly, the first thing I do when I go into a dark room, imagine, you know,
you're at a conference and you go back to your hotel room and the lights don't work in your hotel
room because you can't find the switch. So, you know, what tells you first of all, the first thing
you do, you and I would do as free energy minimising creatures in the future, minimising the expected
free energy, maximising my expected information gain, it's a turn on the light. That's the first
thing that any free energy minimising system would do. And the second thing that that little
analogy tells you is that if you can't find the light, you just kind of feel your way around.
So, again, this is exploration, this curiosity that is coming to the imperative to resolve uncertainty.
Remembering that uncertainty is average surprise. So, in resolving uncertainty, you are minimising
your expected surprise. But this is about outcomes in the future that are not yet witnessed. So,
again, when you talk about things I do, then you are now talking about the imperatives that
underlie action upon the world and the consequences that live in the future. And in those instances,
it's all about minimising uncertainty. When you're talking about things, you know,
my states of being, surprise, it's just a measure of my state of being, it's just a likelihood of
these sensations given my model, my predictions of the sensations at this point in time. So,
they're fundamentally different ways of using the notion of surprise. I hope that makes sense. It's
probably a little bit over answered. I've over shared. No, no, it's great. That was useful. I mean,
it is something that I think that's why I wanted to remain at this, on this topic, on this level,
because it is, I think, the area where I still still have the hardest time
intuiting the difference between information gain and minimising surprise. Those two things,
they seem counter, or they seem like, you know, you can't have both, right? But I think it's
partly perhaps a problem of language in terms of like assigning a positive or negative valence
to the term surprise. So, something like, like an example, so I was thinking about this the other
day, if you won the lottery, if you had a winning lottery ticket, right, you won $100 million,
that's extremely surprising, and also very good, right? But it seemed to be counter to
minimising surprise, right? So, but I think it's a difference of scale, perhaps, like,
it's like the difference between perhaps the overall model and a particular event within
that model, you know, you know, perhaps using that example, could you possibly help
help me into it, sort of the difference in where, or these tensions kind of resolve?
Well, I think you've identified, well actually, it's a wonderful, let's talk about winning the
lottery, which is very surprising. Yes, I left you some day.
But also, I think just to reiterate what you implicitly just said, that, you know,
the word surprise is applied to many different constructs, sometimes more anthropomorphic and
sometimes less. So, when surprise is used in the context of the free energy principle or
and mathematically, it has a very specific and very deflationary meaning, it's just the
negative log probability of some data given your model of how those data were caused. So,
it's a measure of the implausibility of getting these data given a particular commitment to how
you think these data were generated. So, I repeat, it's probably best thought of as a
prediction error, you know, I've got some outcome, and I had a prediction of that outcome,
and the surprise is just the measure of the mismatch or the distinction, discrepancy between
my prediction and what I actually observed. This is also known as the prival, a l, an information
theory, a term coined by Tribus. More formally, more generically, it's also called self-information.
It's just a negative log probability of an event conditioned upon the model or the context in which
that event occurred. So, I think that's, if I was talking to, you know, students of
physics or information theory, I wouldn't use the word surprise, I'd use self-information because
immediately they know that the expected self-information is entropy of a Shannon sort.
So, when I talk about minimizing self-information, I'm using that as a shorthand that's saying,
I'm trying to describe systems that resist a tendency to dispersion of their states. So,
notice that this is, again, because it can be very confusing, again, this is, if you like,
almost the opposite of James's maximum entropy principle, and there's a reason for that because
what we're talking about here is the entropy, the average self-information or the average surprise
of actual outcomes, of real deterministic variables that constitute technically the
sensory states of my Markov blanket, the way that the world impresses itself upon me.
So, this has nothing to do with beliefs or probability distributions. It's the surprise
afforded this outcome, and I want to, you know, being a good homeostat to you,
cybernetics notion, I'm going to want to make sure that everything that the world impresses
upon me is within the bounds of, you know, that something like me can take, and this is just
a statement basis again. So, that kind of entropy, we're trying to minimize. So, remember before,
we were talking about James's maximum entropy principle, but the entropy was about the beliefs
about the causes. It wasn't about the actual outcomes. So, this is, again, if you like,
a dialectic or a paradox. On the one hand, they're trying to minimize the surprise or the
self-information of my, of the actual outcomes that the world impresses upon me, but when it comes to
the Occam's principle view of free energy minimization, I'm now going to try to maximize
the entropy of my beliefs about the causes of that. So, it is complicated, and perhaps I
won't should apologize for that. It's a little bit, it's a same kind of like dialectic that you get
into if you commit to James's maximum entropy principle as the right way of measuring things
and understanding things. Then you have to say, well, hang on a second. I thought the whole point
of biological self-organization was to resist the natural tendency to an increase in entropy.
I thought the whole point of Schrodinger's formulation of life was to minimize entropy. So,
who's right? Schrodinger or James? Well, they're both right. It's just that the entropy of the
things I talk about are completely different. James and the free energy principle are talking
about the entropy of measurements and beliefs. Schrodinger and homeostasis and syllogetics,
for example, are talking about the entropy of outcomes, of measurements, of the data that you
use to make your inferences and build your beliefs and update your beliefs. So, it's a fascinating
sort of yin and yang, and it really forces you to think about the nature of these mathematical
descriptions and to what they pertain. So, things like entropy and surprise and self-information
are just attributes of probability distributions. So, you have to say, of what? Is it the outcomes
or is it your beliefs about things? And with that sort of in mind, weighing the lottery,
is that surprising? Well, if you imagine the probability of it happening to you being surprising,
I can see that you might think, oh, that's highly implausible. So, it's not. It is very surprising.
On the other hand, if you believe that you are the kind of lucky chap that good things happen to,
which you have to believe in order to actually self-evidence your way, predict your way into
being a happy chap and that good things happen to you, winning a lottery is not that surprising.
That's the kind of thing that happens to people like me. I'm a successful New Yorker.
And I wouldn't be that surprised because that's consistent. It's egocintonic.
Winning the lottery is not going to put me in a truly surprising state that if I had a car accident
or I had a stroke or my partner let me, these are truly surprising things. Winning the lottery
is not surprising because it doesn't take me out of my comfort zone, my attracting center,
my callback attractor. Furthermore, I'm not sure that this example works so well,
in practice, but certainly you can now, if you win the lottery, if you did actually win the lottery,
that actually reduces a lot of surprise in terms of what you're going to do next.
So, that puts you in a certain position with a certain attitude of ways forward and ways of
behaving, which actually resolves uncertainty about whether you're going to be able to pay these
bills, whether you're going to be able to support an elderly relative, whether you're going to be
able to pay for your children to go to college. All of these uncertainties are exactly what
you're trying to actively resolve, but if you're very rich, a lot of those uncertainties resolve
themselves. So, that kind of, if you like, expected surprise, which is all about the surprise that
attends the things that you're going to do, is actually resolved by winning the lottery.
So, winning the lottery is both good of that simple level of analysis. I'm sure if you actually
speak to people that actually won the lottery, you probably get a very different impression,
but at least at this level of analysis, it's both good and surprise minimizing.
That's interesting. Right, it depends on the frame, almost the frame of reference,
which state are you looking at it at. Are you familiar? Just came to mind the other day,
as I was thinking about this principle. Do you ever watch The Twilight Zone?
Yes.
From back in the day, there's an episode called A Nice Place to Visit and Spoiler Alert for anyone
