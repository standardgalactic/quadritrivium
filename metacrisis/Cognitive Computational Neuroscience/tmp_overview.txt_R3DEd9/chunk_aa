Processing Overview for Cognitive Computational Neuroscience
============================
Checking Cognitive Computational Neuroscience/CCN 2019： SE-CC： Challenges and Controversies： The Free Energy Principle.txt
 The discussion revolved around the concept of free energy in the context of neural networks, particularly as it pertains to the models used by Scruffies (a term affectionately used for researchers in deep learning who often work on unstructured problems). The core prediction of free energy models is that free energy should decrease over time, and if it does not, it may be due to the incorrect choice of parameters Q or P.

The speaker emphasized the importance of ensuring that hypotheses derived from neural networks like AlexNet are testable and not trivially true. The speaker also noted that while current discriminative networks like AlexNet are accurate in predicting certain aspects of brain activity, they could potentially be reconceptualized as different forms of generative models.

The discussion highlighted that deep learning architectures, such as the ones used by Scruffies, are essentially universal function approximators that learn mappings from data to beliefs or inferences. These mappings are crucial for categorization, prediction, and understanding the underlying processes of the brain. The speaker suggested that these models can be described in terms of variational free energy minimization.

In conclusion, the session aimed to bridge the gap between different approaches in deep learning and cognitive science by emphasizing the importance of falsifiable hypotheses and the shared goal of understanding complex systems like the brain through the lens of generative models. The speaker expressed appreciation for the audience's engagement and patience during the discussion.

Checking Cognitive Computational Neuroscience/CCN 2019： Tutorial T-C： Approximate inference in the brain： free energy, sampling, and beyond.txt
1. The presentation focuses on how the brain performs approximate inference, which is necessary due to computational constraints like energy budget and stochasticity of signaling mechanisms.

2. The speaker discusses two main approaches to approximate inference: Monte Carlo methods (e.g., particle filters) and variational methods (e.g., mean-field approximations).

3. A novel approach is introduced, which combines coding cost (reconstruction error and sparsity penalty) with a reliability cost. This leads to optimizing a variational free energy, where the stochasticity of the network is harnessed to approximate the posterior distribution.

4. The presentation emphasizes that the brain's inference process is inherently approximate and involves a synergy between biophysical constraints (like energy efficiency) and statistical considerations (like approximating the posterior distribution).

5. The speaker suggests that the brain might use a combination of sampling and variational methods (sample-based variational inference), which could be computationally efficient and biologically plausible.

6. Amortization, or learning a family of models rather than just one, is proposed as an interesting area for future research in neural computation.

7. The speaker mentions that learning the generative models themselves is a complex issue but points to work by Wolfgang Maas's lab as a resource for understanding this process.

8. The presentation concludes with an invitation for questions and further discussion on the topics covered, particularly focusing on how generative models are learned.

