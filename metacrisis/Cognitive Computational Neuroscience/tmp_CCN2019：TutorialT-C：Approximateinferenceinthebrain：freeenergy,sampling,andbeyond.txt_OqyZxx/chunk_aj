from one inference problem to another um if those inference problems are similar um like
basically their queries are similar um and that's again consistent with this kind of amortization
framework um another intriguing phenomena that this could potentially explain is belief bias so um
if you had this um perfectly generic probabilistic inference machinery in your heads then I could
give you any numbers of priors of likely and likelihoods and you should be able to crunch
those numbers and give me back um um an inference and even if that's an approximate inference machine
machinery it probably it shouldn't um you know the version of the story that I've been giving
you before is that it doesn't matter what numbers you put into it um whether those numbers are
plausible or implausible it should just run its approximation and give you an answer um
but it turns out that that it does matter for people so um if you give people implausible numbers
then they seem to be worse um more poorly calibrated with the ideal Bayesian answer than if you give
them plausible numbers so if I told you like um here's a medical test for um uh for hiv the base
rate for hiv is 99 percent in the population and the medical test has an accuracy of two percent
right so that's a very implausible set of numbers if you're talking to a doctor in the western world
and um and so the so the the so the finding here the experimental finding is that when you get
people these kinds of impossible numbers they're not as good at being they're not as good Bayesians
as they would be if you give them possible numbers like a base rate of one percent and accuracy of
90 percent um and we could explain this um by in terms of learning to infer um because remember
people are optimizing their mapping their their approximate posterior they're they're
optimizing it for a particular query distribution that is the query distribution that they're
exposed to in their in the natural world and if you give them weird queries they're not
going to be optimized for that they haven't allocated cognitive resources to being able to answer um
query queries outside of their query distribution yeah
yeah yeah that's a great question so the question is about whether this is motivation dependent
and um so the assumption here is that you have limited cognitive resources because if you had
infinite cognitive resources then you could just perfectly represent all mysterious right um
now there's an open question which is that cannot if I'm sufficiently motivated can I increase my
availability of resources and therefore get better approximations um and not be quite as
restricted to the particular query distribution that I'm showing um so the prediction would be
that if you could motivate someone sufficiently some of these effects would go away um but I'm not
aware of studies that systematically look at that um there's one other um perspective on on
amortization that I wanted to bring up which is a regularization perspective um so suppose that
we're getting this sequence of of queries and I'm looking at one particular query I can break down
the optimization problem into um minimizing the KL diversions for that particular query plus a
regularization term that depends on all the other queries or rather the expected KL under this
query distribution um and the implication here is that if the focal query is high probability
then the second term counts less that that comes from this one over p of d term um so if I get a
really if I get a query this really high probability under the query distribution then I'm going to
ignore this regularizer and I'm just going to try to fit my um approximation to that to that
particular query but if it's very low probability under the query distribution then I'm basically
going to ignore that query and just pull my approximation towards the the approximation
that fits the rest of my query distribution um so so there's a kind of adaptive regularization here
that's um that makes intuitive sense um and we can we can think about this almost as a kind of
meta Bayesian inference so um when I have an approximate posterior um I have uncertainty
about what the posterior should be given the data that I've observed um and that's what the this
regularization perspective is saying is that I that given my uncertainty I want to pull my estimates
towards the distribution of of posteriors that I that I'm commonly confront um and this can explain
for example why probability estimates tend to be biased towards the mean of the query distribution
this is sometimes called conservatism um and it it's consistent with this idea that
you know on the tails of in the tails of the distribution your um you have a greater degree
of uncertainty um and so you should have strong um stronger regularization around the in the tails
of the distribution and it turns out that it actually makes another prediction which is that
the variance of the um judgments are going to be higher in around the mean of the query
distribution because of the essentially because of this bias variance trade-off so you have
stronger bias um in the tails of the distribution and lower variance but you have weaker bias
in the mean of the distribution and and higher variance and that and that's consistent with
the empirical data that that's been recently collected um all right so um to summarize so far
I've shown you some evidence from a bunch of different sources that people learn to infer
they their their judgments adapt to the query distribution and and this might um
offer an explanation of different contradictory patterns of uh under and over reaction that
are documented in the experimental literature um and it seems seductive to think that this might
be an account of how the the brain might do approximate inference that there's a single
network that's outputting posteriors directly rather than running inference from scratch um and
there's an interesting connection here to um uh deep networks and I think this comes back to a
question that came up in the previous uh part of the tutorial um so can we connect this in any
possible way to the kinds of um deep learning architectures that have been applied to um
systems neuroscience and uh I'll give you one answer to that question so let's take for example
this very successful model from Dan Yamans and Jim Decarlo um where they modeled eventual visual
stream as this feed forward neural network well now now they've added some bells and whistles
and there's recurrence and feedback but but this is the kind of the the initial version of this was
feed forward um a feed forward convolutional neural network um so one way we might think about
what's going on here is that this feed forward network is actually the approximate recognition
model you know approximate amortized posterior for a structured generative model and this is the
line of argument that was made um by Ilko Yildirim um so he developed this uh structure generative
model for faces that that has um a bunch of pose parameters like um sorry shape parameters like
like shape and also texture and then some pose and lighting parameters and these are all entered
into an rendering a graphics renderer to produce some realistic three-dimensional face um and then
he modeled the inference about these parameters as uh using this amortized neural network
that basically tries to map from the data back into the latent space of the structured generative
model and by training these things conjointly you can synthesize the combination of kind of
structured generative models which allow you to make um to have sort of strong inductive biases
about the nature of those parameters but the efficiency of um of deep neural networks
um so that you don't have to run some very costly inference algorithm and they did a bunch of
experiments on how people actually make inferences about things like lighting and pose
and they showed that the um if you try to train a convolutional neural network by itself on this
task it doesn't do very well um but you can do much better if you you take this um if you use the
convolutional neural network as the the um recognition model for a structured generative model
and by well I mean that that uh you know it matches people's human behavior much better
and in fact they went once that further and they connected uh the the representations from this
convolutional neural network to um uh to macaque face patch responses and and tried to argue that
the similarity that the um the similarity structure of these macaque face patch responses matches this
the particular similarity structure of this um recognition model this confnet that's trained
as a recognition model um they actually have a I discovered that they have a more recent
pre-print based on this that you can see on bioarchive which I haven't read yet but I'm sure
it's great all right so let's go to the the last part of the tutorial thanks for bearing with me
which is are we making things too complicated so we've we've gone through a lot of
technical stuff variational inference markup chains uh you know amortized posteriors and so on
that's a lot of work right to to make to set these things up so that they work properly
um maybe we're making things too complicated um maybe there's a way to do this with a lot less work
so this leads to this comparison between bespoke versus versus generic neural networks
um so we one nice thing about bespoke neural networks that are designed for this very particular
purpose that we can guarantee that they're doing the right thing under certain circumstances
but how far can we go with generic non-specialized neural networks that weren't optimized for this
purpose so this was um um a question that was answered by Ermin Orhan and Weijima
in this very interesting paper where they took generic neural networks
um the just simple feedforward networks that and the only thing bespoke about these networks is
that they were configured to solve particular tasks like q combination um but beyond that they
weren't specifically designed to solve probabilistic inference problems um and and the only thing that
they they added thing that they did here was that they assumed that um like in the probabilistic
population coding models that the gain of the responses at the input layer depended on
uncertainty so on on contrast for some of these tasks um so so then they basically just train
these networks without optimizing for probabilistic inference they're just trained to solve this
particular task um and they did this for a bunch of different tasks like q combination um coordinate
transformations common filtering causal structure inference um and then what they showed rather
remarkably is that um the estimates of the variables that they got um for continuous tasks
were very close to the um the estimates that you get from the Bayes optimal solution
and on discrete tasks you could even they even show that the posterior probability
the probabilistic output from of the network so that network can give a probabilistic output
but it's not actually computing a posterior uh at least not by design uh was very close to
to the true the true posterior probability um and they they took this one step further and they
showed that um if you take a if you train a q combination network with two q's and then you
hold those you basically freeze those the weights for that those that part of the network and then
you add a third q then the frozen part of the network can combine with this new q once it's
properly trained um to do three q uh q combination near optimally uh so what that means is that
in order to do that near optimally you have to have it implies that the these the the pre-trained
modules have represented something about the uncertainty um that they can then transfer to
this three q case so it's a pretty non-trivial generalization from this training regime um
and so so it's pretty interesting I think to to find that you can get kind of probabilistic
inference behavior out of non probabilistically trained networks and they they make an efficiency
argument here which is that you need many fewer neurons to achieve near optimal performance
compared to what you might need for example in a probabilistic population but so it actually
the minimum number of hidden units you need grows sub-linearly with the number of input units
which again suggests the kind of attractability argument in favor of this kind of objective function
there's another way to to construct probabilistic inference from ostensibly non probabilistic
machinery by introducing a reliability cost this is work by Lawrence Acheson and and Matelango
so the idea here is that you're going to take a generic neural network that's trained non probabilistically
or rather it's not trained to output a posterior probability but it but it has a
it's trained to optimize um or I'll show you what the optimization problem is in a second but I just
say one more thing about the reliability cost which is that the you know this kind of reliability
cost is plausible because um we know that signaling mechanisms like ion channels and
quantal release are intrinsically stochastic and that a big part of our brain's energy budget is
basically devoted to you know controlling the reliability of this of this stochastic machinery
so the objective function that they introduced here is a coding cost for encoding some latent
variable z that depends on the reconstruction error and and the sparseness cost so this is
basically like a standard optimization problem for a latent variable model um like sparse coding
model but then they also so so then the idea here is that they combine that coding cost
with a reliability cost and the and the simplest thing to do analytically is to use the entropy
because if you use the entropy then the expected coding cost plus the minus the entropy gives you
the variational free energy um so so what we've done here is shown how if you if you construct a
network that's just trying to optimize reconstruction reconstruction error plus sparsity penalty
plus reliability cost then an emergent property of that optimization problem is that they'll
actually be solving in effect a variational optimization problem that they will in effect be
optimizing a variational posterior so so it's important to keep in mind that the stochasticity
of the network is not designed to sample from the posterior rather the stochasticity is just there
but by training the network to optimize this objective function you can basically harness
that stochasticity to generate samples from the from this approximate posterior so that's another
way to get um approximate inference out of machinery that was not um that that was not
specialized for approximate inference um so what are the implications of this so we see that
probabilistic inference emerges from networks that are optimized to find good codes while paying
cost and reliability and and there's this means that there's an interesting synergy between
biophysical and statistical constraints so you might motivate of reliability cost purely on
biophysical grounds that is energetically costly to be reliable but that turns out to have this
other asset from a statistical perspective that you can approximate the posterior that way
um and there's an interesting um connection here between sampling and variational inference so
now we have these stochastic networks that are stochastic by virtue of their signaling machinery
but they can be hot harnessed to optimize free energy um all right so so i'm gonna wrap up and
then i hope that we can just have some some open discussion um so i hope to convey that
approximate inference in the brain is inevitable in the sense that there's no other way to do
inference except approximately but it's also still shrouded in mystery and we have these main
candidates like Monte Carlo methods and variational methods but really each one of those is kind of
this box filled with lots of different options for how to construct approximations um and in
some cases these things might tend to converge like we can build sample-based variational approximations
so it might turn out to be the case that the the approximation algorithm or algorithms that are
used by the brain are using all of these things simultaneously in some interesting way and and
finally um i think that the question of amortization opens up a whole host of interesting issues for
neural computation this idea that the brain's inference engine is optimized for not just a
single posterior but for a whole ensemble of procedures and i think that's deserving of of
significant future work um if you'd like to learn more about this i i just threw up a few
references here um and i'm sure there's more recent ones as well um and yeah i'd like to open it up for
questions unless everyone's totally exhausted yeah
yeah so that yes sort of i mean but i'm not sure how scalable yes there there is like
so for example some people in machine learning have developed amortized sequential Monte Carlo
methods yeah oftentimes they use some kind of deep network to parameterize the sampling distribution
for example yeah no no i think that that is something of the sort that people have tried
are there questions
yeah
yeah yeah so the question is how do you learn the generative models themselves yeah that's a
huge question i mean a lot of people have thought about that and i i sort of avoided that here
um but i mean i'd suggest for example looking at work from Wolfgang Maas's lab who's done a
lot of work on um on learning of generative models yeah anything
okay all right well thank you guys for listening this is fun
