been acquired for that novel q and i compare that to a case where the novel q was got the same
amount of reinforcement but um was not was never reinforced in compound with a and in general what
you find is that you get more response to uh more reward expectation for b when it was trained by
itself compared to when it was trained in compound with a another q that was already reward predictive
and there's a there's a classical explanation of this that comes from the rescorla wagner model
and the idea is that you use prediction errors to update your estimates and those prediction
errors are basically um distributed among all the q's so that's the principle of q competition
so in the first phase you learn that a predicts reward and now in the second phase um because the
the assumption is that the the reward expectations for a and b summit to produce the expectation
for that compound and since a already perfectly predicts the reward there's no prediction error
the reward is totally expected and so there's no updating of b's associative strength beyond zero
and that's why you get this blocking effect unfortunately that explanation doesn't work
for backward blocking which is just the same thing except we reverse the order of the phases so now
we first reinforce a b and compound and then we reinforce a by itself um and then again you see
that the um the response to b reinforced by itself is is um stronger than the the response
to be reinforced and compound so that the error the the error driven updating idea no longer works
here um because um b is not even present during the second phase so how can anything be learned
about b if b is not even present um models like the rescorla wagner model require that
you can only they assume that learning can only happen for q's that are present not for apps and
queues and that kind of inspired a bunch of ad hoc modifications of the rescorla wagner model
to accommodate that like you know introduction of apps and learning for apps and queues but with
the sign flip but it was all quite ad hoc there's another important constraint i'll shortly explain
how to explain both forward and backward blocking but there there's a there's another important
constraint which is that forward blocking is reliably stronger than backward blocking so how
can we explain that um so to start to explain just the forward and black forward and backward
blocking phenomenon by themselves let's think about this as an estimation problem
so think about this rat in the classical conditioning context and the rat has the
following linear Gaussian generative model of the world that assumes that the reward or punishments
as as in the case may be arise from a linear combination of the available queues plus some
Gaussian noise and so those the weights on that linear combination the x denotes queues and the
and the w denotes the weights those weights are basically a measure of the associative strength
to some q and then the inference problem is to basically invert this generative modeling for the
posterior over over w conditional on the history of queues and outcomes and if you make this these
linear Gaussian assumptions then the posterior is also going to be Gaussian and you can parameterize
it by some mean and covariance and it turns out that you can actually update these parameters
recursively so trial by trial and you can do this analytically using the common filter so a classic
tool from signal processing and what's neat about this is that the common filter looks a lot like
those for the Wagner model in the sense that the the weights are updated in proportion to the prediction
errors but one thing that happens is that you get this extra kind of the learning rates are now
endogenized into the model and they depend on the covariance structure and so the critical novelty
here is that you can allocate credit or blame to queues that are absent provided that they have some
positive or negative covariance with present queues and we're going to use that to explain
backward blocking but first let that the explanation of forward blocking is essentially the same as
the explanation for in the risk for the Wagner model that there's no prediction error on those
on those compound trials and so there's no updating so how do we think about backward
blocking so let's look at this graphically so let's imagine that you start with a prior over
weights that's isotropic and Gaussian so that's showing on the left and then your likelihood
function encodes the fact that there's basically a constraint line so if you get a single unit of
reward then the queue the two queues have to summit to produce that single unit of reward
so the weights on those queues can never always have to add up to one and that's what the constraint
line basically visualizes graphically but there's this kind of blurry boundary around the the
constraint line because there's noise in the observation so you don't need to exactly satisfy
the constraint you have to just be near the constraint line so according to baseral we
can get the posterior by just multiplying these two things together and renormalizing
and you get this posterior which is kind of this warped ellipse around the around the constraint
line and the important implication here is that this is a Gaussian with negative covariance
between the queues what that means is that when one queue goes up when the weight the associated
weight of one queue goes up the the associated weight of the other queue has to go down and
that's in order to satisfy approximately the this constraint line and that's what explains
backward blocking because if we go back to this this blocking at the backward blocking experiment
so during the compound queue phase you learn that there's this negative covariance between the queues
and then in the when you reward a by itself you're not only increasing the weight for a but
you're also decreasing the weight for b because of this negative covariance and that produces
backward blocking now the problem though is that this doesn't explain the asymmetry between forward
and backward blocking why is forward blocking stronger than backward blocking and so this is
where the variational ideas come into play so remember that the covariance terms are critical
for explaining backward blocking if we assume that the posterior factorizes then backward
blocking goes away completely is that clear everyone right because the factorization means
that you can't by definition you can't represent the covariance structure but forward blocking
doesn't depend on forward blocking doesn't depend on the covariance terms at all right all it requires
is the fact that you don't get you you don't get positive prediction errors when the queues are
reinforced in compound now this is not entirely satisfactory because why isn't that you get some
backward blocking so there is a backward blocking effect it's just not as strong as the forward
blocking effect so there's a way actually to to get partial backward blocking this was an idea
that was developed by Nathaniel Daw and Aaron Corville where you assume that the covariance
matrix is reduced rank so in effect you're you're restricting its ability to represent the full
distribution the full posterior covariance but it can sort of partially represent partially
capture some of the covariance structure and that is what what that does in effect is downweights
the covariance terms and that's that's illustrated in the simulation here so you get you get the
forward and backward blocking effects but the backward blocking effect is much weaker than
the forward blocking effect so there's an interesting neural circuit view to this
which is which was thought out by Shom Kakade and Peter Deann
where you can interpret the common filter you can interpret the you can implement the common
filter in the in a particular kind of neural network where the sensory cortex I'm giving
kind of the neurobiological gloss on this that wasn't in the original paper but let's imagine
that that cortex is conveying the cues and then there's a there's a downstream region
with recurrence that effectively implements a kind of whitening transform so when you run these
recurrent dynamics you're going to extract a new representation of the stimuli of the cues
that are effectively constituted kind of orthogonal basis and then what you can do
is do normal associative sort of vanilla associative learning between this
transformed representation and the outcomes so it's an interesting division of labor where
you have this kind of re-representation step and a basic associative learning step and that
and when you put those two things together you can get you can implement the common filter using
that machinery now that's assuming that that it can perfectly implement the common filter if
you assume that there's the same number of units in the hidden layer as there are in the input
layer so every cue gets its own kind of unique re-representation in this hidden layer but if
you restricted the dimensionality of the hidden layer or if you put some regularizer so that it
was limited in in its in its expressivity then it would no longer be able to represent the full
posterior covariance and that's in effect a way to implement this kind of reduced rank
transformation so you constrain the representational flexibility of this hidden layer
and that's that's what i'm showing here okay now this is all conjectural there's no there's no
direct evidence that this is what's happening in backward blocking unfortunately there's very
actually very few studies of backward blocking in animals so this is kind of an open question whether
this is a good story or not any questions so far yeah well can you can you elaborate a little bit
the question was about mutual information but what about mutual information
right well the covariance is expressing mutual information i mean that that's
there's there's going to be a correspondence between the covariance between cues and their
mutual inflammation i don't know if that helps at all yeah yeah okay well we can we can talk about
it more later if you want all right so if everyone if there are no other questions about this part
i'm going to talk about the causal graft surgery as another case study in how variational approximations
might come into play in approximately in like explaining psychological phenomenon
so here the idea is that if you have some if i give you some causal network and you have to
make some inferences about that network doing exact inferences in that network might be very
difficult but it might be easier if i could basically do some surgery on the graph like cut
some of the connections or delete some of the variables so that now i'm doing inference in a
simpler graph and then we can ask the question which is in essence a variational question
which simplified graph is closest with from drawn from some some restricted family of simplifications
closest and kale divergence to the true posterior over the graphs and Thomas Eichhardt and Noah
Goodman have used this idea to explain a number of seemingly irrational behaviors in the causal
inference literature so i'll give you an example which is a neglect of alternative causes so there's
an empirical finding which is that you neglect alternative causes more in predictive inference
tasks compared to diagnostic inference so so the difference in between predictive and diagnostic
inference tasks is illustrated here you have the same set of variables but in the predictive
inference task you're trying to reason from cause to effect and in diagnostic inferences you're
trying to reason from effect to cause right so why is it that people seem to neglect alternative
causes more in predictive inference tasks so what Eichhardt and Goodman showed was that
the sub model in which which is ignored so if you have this alternative cause if you ignore
that alternative cause that doesn't have as bad of an effect on your inferences
in the predictive inference tasks compared to the diagnostic inference tasks in the diagnostic
inference tasks you could do catastrophically bad if you ignore the alternative cause but you
could still predict pretty well if you ignored the alternative cause and they quantified this in
terms of the the kale divergence and so this is one idea about this is one example of where you
could take ideas about variational inference and show that that people might be adopting certain
restrictions on their variational family now I don't want to get into the details of this but
I thought it's worth mentioning which is that there's a there's a world beyond free energy
I know that that sounds heretical
so the kale divergence is only one possible measure of divergence between the true posterior
and its approximation there's a wider family which is known as the alpha divergence family
and remember that you know I'm kind of going back and forth between talking about free energy
minimization and kale minimization because in essence they're doing the same thing in variational
inference so understanding this equation is not important that what's more interesting
is the kind of special cases of these alpha differences so you can get free energy minimization
when alpha goes to zero but you can also get other things so you can get other kinds of algorithms
like expectation propagation and assumed density filtering when alpha goes to one
and there are other sort of lesser known things but but what's interesting is that these have
qualitatively different behaviors so so free energy minimization tends to
try to capture the modes of the distributions even if even if it means ignoring it captures sort of
the the mode with the highest probability mass even if it means ignoring some of the other modes
whereas algorithms like expectation propagation try to cover cover as many modes as they can
with a single distribution so that's what's shown on the right where you kind of stretch
your approximate posterior over a bunch of different modes as opposed to squash it onto a
single mode and you can make a kind of more general characterization of this with respect to this
alpha parameter so the critical difference has to do with zero first thing versus includes a zero
avoiding behavior so when alpha is less than zero you get zero forcing behavior which means that
when the probability distribution the true probability is zero it forces the approximate
probability to be zero and this is going to keep the areas of largest total that that means that
the approximate posterior is going to be forced to keep the areas of largest total mass so that
you'll get this kind of mode seeking behavior whereas when alpha is greater than one the
approximation will stretch to capture all the modes so this is a kind of zero avoiding behavior
who knows whether any of these
divergence functionals are relevant for neural computation or psychology but I think it's
important to to know about some of these things so for example like you know Zach has stuff on
tree-weighted belief propagation so that's a different alpha divergence from you know the
kinds of mean field approximations that are often applied to neuroscience and so we should keep in
mind that there's these are actually you know qualitatively different options here any thoughts
or questions about this so far yeah
well I mean I guess I should first start by saying that
we should distinguish between applications of mean field approximations to
analyzing neural systems as opposed to modeling neural systems so what I mean by that is you
know the classical application of mean field approximations to computational neuroscience
was taking some complex joint distribution over neural activities and then imposing the
mean field approximation so it's not that the neural circuit itself is is implementing the
mean field approximation we're just we're just using that as an analytical tool to
make some characterizations of that neural circuit right so that's purely a data analysis tool
and I'd say that's much more common than the actual actually using the mean field assumption
in the model of the neural system which there's you know not that many people have done that but
I think I guess generally speaking people have always liked the mean field approximation because
it's simpler it's easier to implement it's it's often broadly applicable so like you can implement
mean field approximations for basically for most exponential family distributions
so it's always been kind of a workhorse of variational inference and there's other issues
here which is that that have to do with convergence guarantees and things like that like
when you run the mean field approximation you're guaranteed to to reach a local
optimum of the variational free energy and if I'm remembering correctly that's not true of
expectation propagation but I I may be misremembering that
yeah but I think I think so anyway I just wanted to open your your minds to these
alternative possibilities without trying to advocate for any one of them
all right so so this kind of raises a bunch of questions and we can if you guys would like we
can spend some time discussing these so what divergence is the brain optimizing how can we
even know you know what kind of approximation family is the brain using so even once you
commit to the divergence functional what what what what family is the brain using
is there one divergence or maybe the or approximation family or does the brain use
a bunch of different ones is there generic optimization machinery that's being applied
to all inference problems or do we have specialized inference machinery for particular
problems so for example I already said before that you need to make these Gaussian assumptions
and gradient descent in this continuous parameter space to make like the classical predictive
coding schemes work but maybe you need other things to other other assumptions to make them work
for other kinds of variables like discrete variables and then another important question to
think about is what kinds of constraints do probabilistic approximations place on generative
models what I mean by that is I could dream up some super complicated generative model but if
inference is intractable in that generative model then what good is that to me right I you know
