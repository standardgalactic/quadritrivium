So now I can approximate that distribution basically by putting a delta function on each
of those samples. That's what that indicator I variable denotes. And basically just placing
a delta function at each of the samples, and that's going to give me a discrete approximation
of this distribution. And in general, I'll denote the approximate distributions by q.
Okay. And so we can contrast this kind of sampling based representation, which is fundamentally
discrete with parametric representations, which might try to represent the distribution
with a set of parameters like of Gaussian with some mean invariance. And that'll be
relevant when we come back to thinking about variational methods.
So the basic idea is very simple. It'll get a little bit more complicated as we go along
and start to think about how the samples are generated. But for now, we can already start
to say some things about applying this model to cognition. So for example, one motivation
for thinking about sampling based methods is to understand behavioral variability. Why
do people seem to be random to some extent in their behaviors, for example, in their
decisions? Well, Monte Carlo methods provide one answer, which is that if you're taking
a small number of samples, your posterior distribution now is going to be a random variable.
And so the randomness in your sampler is going to induce randomness in your decision behavior.
So imagine, for example, that the decision theoretically optimal action is to select
this stimulus that has the highest probability under the posterior. So this is sometimes
called the maximum a posteriori or map estimate. Now, I don't have direct access to the true
posterior. I only have access to this approximate posterior queue. So I'm going to run the
sampler, generate queue, and then select S greedily that maximizes queue. So you can
unpack queue to get this expression at the bottom. Now, in the limit, if you only took
a single sample, your decisions are going to be exactly probability matching the posterior.
In other words, you're going to choose stimulus S with probability equal to the, it's probably
under the posterior distribution. So that's called posterior probability matching. And
as you get more samples, you're going to get closer and closer to true maximization. And
then somewhere in between for a small number of samples, you're going to, you're going
to look sort of like you're doing quasi probability matching. So why do we think that people are
only taking a small number of samples? Well, this comes back to computational rationality.
I don't know, Wagey is here. So he's going to tell me, he's going to tell me the way
it's supposed to be. All right. Okay. So why only take a small number of samples? So
let's go back to computational rationality. There's a cost of time. And there's the benefit
of increasing the number of samples. So what we need to do is combine these two sources
of information to calculate the expected utility per unit time. And depending upon the relative
costs and benefits of samples, you're going to get this function that's non-monotonic.
So the optimal number of samples to take could be quite small. And this was worked out in
great detail by Ed Wuhl in this paper, which is very interesting, but also I think somewhat
arbitrary because it doesn't really give us a recipe for pinning down exactly what the
costs and benefits of sampling are. And I think that's one of the issues that's always
bothered me in thinking about these algorithms in the brain is because we don't know exactly
what the cost of sampling are. And so we can draw these curves, but they're somewhat disconnected
from reality. But we can still make a kind of general argument about why we want to use
a smaller number of samples because a large number of samples are going to be too costly.
And you see this phenomenon of probability matching in many different domains. So I'll
just give you a few examples. So this goes back actually to the early days of signal
detection theory. So this study by Tanner, Swets and Green, they changed the prior probability
of the signal at different levels. And then they looked at the probability of an individual
reporting signal present. And they found that it increases linearly with the base rate of
the signal. So that is not really compatible with kind of the signal detection theory under
perfect inference unless you posit some noise that's scaling with your posterior uncertainty.
And this kind of sampling-based approximation of the posterior could give one route to producing
that kind of posterior probability matching behavior. So these are low-level auditory
signal detection tasks, but you also see this in high-level tasks. So this was a study
by Noah Goodman and colleagues where they were looking at Boolean concept learning. And
I won't go into the details of that study, but suffice it to say that they developed
this Bayesian model that fit people's behavior pretty well. But one of the interesting observations
that they made was that the probability of observing a particular report in their group
of subjects scaled with the posterior probability of that hypothesis. So people look Bayesian
on aggregate, but the question is why do people show this linear relationship between posterior
probability under the model and the probability of generating the answer? If they were being
decision-theoretically optimal, they should just report the hypothesis with highest probability.
And you see this also in other tasks. So Ed Will and colleagues reanalyzed data from that
optimal predictions paper that I showed you before and showed that basically the same story
arises that on aggregate people look Bayesian, but it doesn't seem to be consistent with the
decision-theoretically optimal policy under perfect inference. You also see this in children.
So this was an experiment that Stephanie Denison and Allison Gottmick's lab did where they were
looking at causal inference in children. They had this machine where you could put blocks into
the machine and it would make some sound. They had different colored blocks, red and blue blocks.
And what they manipulated were the proportion of red and blue blocks in these buckets. And they
were looking at children's inferences about which of the blocks they thought was responsible for
producing the sound that the machine produced. And they again found this phenomenon that children
probability match. So the probability that children reported a particular color was scaling
roughly linearly with the probability of that block or chip. Now, so far I've been talking
about the number of samples as kind of a static object in the sense that we pick some number of
samples and generate a response by drawing that number of samples. But it could also be the case
that the number of samples is adaptively selected. And this was something that was studied by
Jess Hamrick. It's a little bit blurry, sorry. So they were looking at physical predictions.
So they would show subjects this trajectory of a ball that's bouncing around a frame and it
was following Newtonian mechanics. And they would ask their subjects whether or not they
thought that it would go into some particular target area. And they looked at people's answers
as a function of how many bounces that ball made, or sorry, how many bounces that ball was going
to make, respectively. So they wouldn't show the complete trajectory. They'd show part of the
trajectory. And then the idea was that subjects would have to simulate the rest of the trajectory
to make a prediction. And they used, Hamrick and colleagues calculated the optimal number of
samples to take under some linear cost model for sampling. And what they found was that it
predicted this pattern where the more uncertain you are, the more samples you should take. And that
was consistent with subjects' responses. So people seemed to take a longer, have a longer
response time when they were making predictions, when there were more bounces. And this comes
out of the model because when there are more bounces, there's going to be a higher level of
uncertainty. They're assuming here that when there's a bounce, there's going to be an increased
level of noise. So this was an interesting way of using response times to make inferences about
the number of samples that people are taking. Okay. I'm going to move on to discussions of
sampling in the brain. But before I do that, are there any questions? Yeah. This one. Yeah, I
mean, well, it depends on their utility function. But yeah, you would expect, you'd basically be
looking for a step function if everybody has the same priors and the same utility function. And then
that there is no noise, basically, in their decision process. So one hypothesis here, we don't
know exactly, but one hypothesis is that the noise is coming from a sampling process, that they're
doing some kind of sampling from the posterior. There are other sources of noise too, right? So you
can have noise, you could have an exact posterior, but you have the extra noise in your decision
process, or you could have extra noise in at the at the level of perception as well. And in some
sense, I'd imagine that we need to take into account all those sources of noise. Actually, so
Jan Drugowicz did has a very nice paper where he looked at this and he made the argument that most
of the noise that accounts for sub optimality in perceptual decision tasks comes from computational
noise as opposed to the sensory noise. So that he tried to exactly quantify what the specific
other questions. Yeah. Yeah. Yeah. So I should have explained that in a little bit more detail. So
the model that they used to make these predictions is called the noisy Newtonian model. And the idea
here is that the the trajectory of the ball is just following regular Newtonian mechanics. But in
addition, there's there's some noise that get there's some noise that's added when the ball hits
other objects. Yeah, well, the question is what is it that's making it harder? How do you model the
hardness? And so one way to do that basically is to assume that there's more uncertainty when
there's bounces? Yeah. Yeah, well, it depends on exactly what the the so basically, if the if
the posterior probability of some chip being the correct cause is greater than 50%. If we're talking
about two alternative force choice, then it should be it should be a step function, right,
because the map estimate is going to be a deterministic mapping from the from the posterior to
the decision. Well, I haven't gotten to the specific, I'm going to talk about that in a moment,
the specific ways that you might generate samples. Right now, I'm making the naive assumption that
you could just generate samples from the posterior. And the interesting variable here is how many
samples you you generate. So if you generate a small number of samples, then you're going to
produce matching behavior. Right. And we'll get into more specific predictions of particular
sample generating algorithms. Yeah.
Well, there are different sources of noise, right? So you could have noise and perception. It's
also totally possible that you have that that things look like noise. But that's because of
model mispecification. So Zach Pitko and Weiji have this paper on called not noisy, just wrong,
in which they draw out the implications of that idea. So we have to we have to be very careful
about parsing these different sources of noise. That's what makes it so so hard, right? So it's
possible that people are doing exact inference, but with the wrong model, and that could appear to us
the experimenters as looking like something like probability matching, possibly, depending on what
the model mispecification is. Right. Right. But if you're doing if you're doing noisy, well, this
is now. So if you're doing, for example, noisy rollouts, if your policy has a stochastic element,
for example, then then your rollouts will be noisy and your about your the resulting value
function is going to be noisy, right? Yeah. Oh, sorry. Yeah. Good. Good. Good. Yes. Okay. Well, the last
question was about planning, which I haven't really talked about. But the idea that that you could
have determinant, you the question was, would you have this kind of probability matching if you had
deterministic planning? Is that right? Okay. Or something like that. Yeah. But I was talking, my
response was that that many modern planets have some stochastic, many modern planners have some
stochastic element in that that might produce probability matching. But but in that regime,
we're not actually talking about inference anymore. We're talking about we're talking about planning.
Okay. So another reason why sampling algorithms have been interesting is because they seem to,
they for the same reason that they can explain behavioral variable, they may be hold the
promise of explaining neural variability. So we know that neurons are highly variable. And maybe
that variability is just kind of an endogenous property of those of the spike generating process,
but maybe it serves some functional purpose. And that's that's really the line of argument that
neural sampling theories try to articulate. So in the simplest version of such theories,
each neuron represents a particular random variable, and sampling represents sample from that
distribution distribution over that random variable. And then you have to set up the neural
circuit in such a way that it that it collectively samples from the correct and joint distribution
over these variables. So so and I'll get into some specific instantiations of that in a minute,
but before but it might be useful first to just try to think about what happens when you have
neural sampling, assuming that we can perfectly generate samples from the posterior distribution.
So I'm going to be referring to right in this first part to some analyses that were done by
Orban and colleagues to examine this hypothesis. Now in order to do this, you have to start by
postulating some internal models, some probabilistic model. And what they did was they were looking
at V1 neurons and they deposited a simple linear Gaussian model for image patches where each image
patch was generated by some linear combination of basis functions multiplied by a contrast,
the global contrast variable, and then corrupted by Gaussian noise. And so that's schematized on
the left here where you're going to take a little patch of this image and you have these
gobores that represent the basis functions and then you have different levels of activation for
each of those basis functions. And so the inference problem is now to infer the posterior
probability over the activations of those basis functions given the sensory input,
which is the image, the image patch rather. So this is a very simple model, but you already
can use it to make a bunch of interesting predictions. So one argument that they made,
and this is similar to the arguments that have been made about behavioral variability,
is that if you have higher uncertainty, you're going to have higher variability. And they
showed this by looking at the final factor, which is the standard deviation over the mean,
for low contrast and high contrast images. And what this shows is that the variance,
the final factor is higher under low contrast and high contrast. And that's consistent with
this idea that if you have more uncertainty, your sampler is going to have the samples that
are generated by your sampler are going to be more variable. And so you're going to have a
higher final factor. Another corollary of this is that stimulus onset should quench
neural variability. And that is true. So if you compare spontaneous activity to evoked
activities, evoked in response to some image, you typically see higher variability for
spontaneous activity compared to evoked activity. And this follows from the fact that when you
don't see any image at all, you're going to have the highest uncertainty. And so you're going to
have the highest variability. You can actually go one step further than this. And the reason that
actually the spontaneous activity should match the average evoked activity for natural images.
And the reason this is true is if we go back to the sum rule, the marginal probability of the
activations is going to be the sum of all the conditional probability of these activations,
conditional on the image, multiplied by the prior probability of the image. And if you think that
the images that they're showing in these experiments are drawn from the distribution of natural images,
then this relationship between the marginal probability of the activations and the conditional
probability of the activation should hold. But if you take some other distribution, like if you
just show images of gratings, which aren't drawn from the natural image distribution, then that
relationship will no longer hold. So the way that they went about analyzing this is by using a
divergence measure, the Kolbach-Liebler divergence. And this is actually something that's going to
come back when we talk about variational inference. But right now, this is just a data analysis tool.
So they're computing the divergence between the distribution of a spontaneous activity and the
distribution of evoked activity for either the marginal distribution of evoked activity for
either gratings or natural images. And what they find is that the divergence is much larger when
you look at the non-natural images compared to the natural images. And that's consistent with the
claim that the divergence should be small or, in principle, actually zero if you're looking at
samples from the, if you're looking at measurements from this evoked distribution for natural images.
Does that make sense to everybody? Okay. All right. Feel free to interrupt me as I go.
So we can also look at implications for perceptual decision-making. This is work that Ralf Heffner
did. So we're again going to use a simple generative model for V1 neurons. But here,
we're also going to add this decision variable that's generated conditional on the V1 activities.
And the reasoning here is that if you have variability in your beliefs, that's going to
produce variability in your choice behavior. And there should be a particular structure in the
relationship between these two things. And that's shown here. So they were interested in looking
specifically at noise correlations. So this is the case where no stimulus is present. But you
can measure the trial by trial correlations between neurons with different preferred
