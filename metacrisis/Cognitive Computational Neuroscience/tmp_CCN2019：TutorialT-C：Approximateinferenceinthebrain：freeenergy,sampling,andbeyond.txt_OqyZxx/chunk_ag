kale divergence it's just that at each step of the sampler they're not optimizing that the divergence
so that's the difference between um Monte Carlo methods and these kinds of particle approximations
variational particle approximations where the variational particle approximation is choosing
the samples to minimize this divergence at each at each step of the algorithm whereas um
most samplers are not doing that any other questions all right so so if we kind of take
this at face value the the theory part now we can talk a little bit about the empirical evidence
for this and i i think that this is the most compelling part of this whole um the the whole
set of claims here is is the evidence for predictive coding um now one thing to keep
in mind is that um there are a bunch of different variations on the idea of predictive coding out
there not all of them are interpreted in this variational way um so there's a classic paper by
rau and ballard um where they they first developed some of these predictive well they didn't invent
predictive coding but they were the first to apply the idea of predictive coding to this higher
cortical hierarchy um and um but they were not they were not analyzing it as a variational
approximation that came that was mostly due to carl friston um nonetheless the same intuitions
apply here which is that um uh the the basic kind of theme that we'll see here is that if you have
a prediction um that that predicts some pattern of data or or at higher levels some some the if
you have a higher level prediction that that um anticipates some some pattern of spiking at a
higher level of the cortical hierarchy then the um prediction can basically suppress the evoked
response um so a classic example of this from rau and ballard is the end stopping effect um
so if you have a receptive field uh in v one that that responds to line segments um if you
extend the line beyond the receptive field that has the effect of suppressing the response
um to uh to that of that neuron to that um line segment and the way that rau and ballard explain
this is that um once you extend the line now a higher level um a higher level receptive field
like in v two can detect a lot a line a longer line segment more visual structure and can it
basically explain away the um uh explain away the the the smaller line segment so in in essence
the line the line segment activates some higher level hypothesis which then explains away the
lower level hypothesis that that there's just a small line segment in that section of the visual
field and that explains why you get the suppression of the um the response if because the according
to this predictive coding idea the v one neurons the feed forward pathways are only um reporting
the prediction error not the prediction itself or the or the stimulus itself uh and one piece of
evidence that they argue in favor of this interpretation is that it should depend on feedback
and there is evidence that if you inactivate the feedback pathway to v one then um you can get rid
of this suppressive effect this end stopping effect um and they model that by basically removing the
feedback pathways in their model um so that's consistent with this general idea of predictive
coding there's a number of other examples that I'd like to go through um so here's here this is one
of my favorite examples predictive coding of shape so this is an f mi study and they they were looking
at the bold signal in both um v one and the lateral occipital con uh complex loc and they took the same
set of line segments um but arranged them in different ways um or I can't remember actually
the same set of lines with similar similar arrangements of line segments but but the idea
is that in one case in the random case you just kind of screw these little straws around so that
they don't really um they don't really uh show a coherent physical shape or you can arrange them so
that they show a coherent two-dimensional shape or you can arrange them so that they show a coherent
three-dimensional shape um and what they found was that the bold signal in v one was going down
as you have more visual structure from random to 2d to 3d and the loc signal was going up
so this is again consistent with this idea that um as you create more structure the higher levels
of cortex like loc are encoding that structure passing that prediction back down and then suppressing
the um evoked response in lower level of regions that are reporting prediction errors
this manifests in a number of different ways so um one way that's been studied quite extensively
is expectation suppression um so here's an example um from Chris Summerfield and Tobias Egnar
where they were looking at the fusiform face area with an area that's selective for faces
and looking at the effects of expectation on the signal in that area so they had a paradigm in which
faces or how is it how stimulus stimuli would appear with some probability
so they were parametrically manipulating the the degree of face expectation and they found that
the the neural response to ffa was going down as the face expectation was going up in contrast
um the response to houses uh was going up um as face expectation was going up and so this is all
consistent with this idea that um the ffa is responding not just to the bottom up representation
of faces but to unexpected faces um uh you can also see this in repetition suppression so
repetition suppression is the widely observed phenomenon that if you present um a stimulus
repeatedly on subsequent presentations the the response is going to be lower and what and what
the study found was that um repetition suppression is stronger when repetition is expected so if you
compare cases where stimuli were were alternating or stimuli were repeating um you see a bigger
repetition suppression effect um um when in the repetition repetition condition because now um
um the um basically because of the same predictive coding idea that that you're gonna
you're gonna get stronger suppression with higher predictions
one interesting twist in all of this is that um we've been talking about the mean amplitude of
the bold signal but there's also information that's contained in the patterns of activity so
and this turns out to uh these these signals turn out to carry different information so
it's true that expectation as we've been talking about suppresses bold responses in v1
but it also seems to increase classifier accuracy um in that area um if you train a
classifier on the patterns of activity in that area you can read out things like orientation or
contrast um and um expectation increases the ability to decode those variables even though
it suppresses the bold response the mean amplitude from v1 yeah v1 um so one we've already seen how
there's a distinction between feedback and feedforward pathways
and um we also know that these pathways have a distinct laminar structure so um
so prediction errors the feedforward pathways are in the superficial layer of cortex and the feedback
pathways are in the deeper layer of cortex so we predict that um that that we would see um
prediction error signals being preferentially signaled by the superficial layers of cortex
and then the predictions being preferentially signaled by the feedback right in the deep layers of
cortex and recently there's been an exciting development in um MRI technology where we can
start to make statements about the different lamina um from fmi data and basically this this
involves trying to unmix the contributions of different lamina to the bold signal uh and you
you can do this if you have a an unmixing matrix um it's in effect deconvolved the
the laminar contributions of the bold signal um and I'll show you the results of one study
that looked at this where um they they were looking at these kinesa triangle stimuli
so I'm sure all of you see this uh that in the left stimulus it looks like there's a triangle
that's occluding these circles um whereas in the right stimulus if you if you basically break up
that triangle then it looks more like a bunch of pac men that are oriented in different directions
so you don't see the the occluder even though the the underlying segments are there they've
just been rearranged um and the reason that they use these stimuli is because in the in the left
kinesa stimulus the idea is that you're going to have a strong prediction that there's some
um surface at this region of the visual field even though there's no actual contrast difference
to indicate a surface there it's all based on um these essentially gestalt principles for inferring
the the occluding shape and so what they found when they when they now try to measure
the different laminar contributions to to this bold signal for these different stimuli
they were looking at um they they they broke this up based on whether a particular voxel
had a receptive field centered on the illusory triangle or on one of the inducers and what
they found was that um if you look at the regions with receptive fields centered on the illusory
triangle um you you get a difference between the illusory figure and the no illusory figures
of the kinesa and the control stimulus um in the deep uh layers of cortex which is consistent
with this top these top down expectations whereas if you look at the voxels that have
receptive field centered on the inducer you get um the opposite effect so greater for
no illusory than illusory in the superficial layers um which are which are hypothesized to
convey the bottom up prediction error so this seems um like pretty good provisional evidence for
this predictive coding effect that you can see signatures of these feedback and feedback predictions
and feed forward prediction errors um in the um lamina as decoded by fmri. Yeah.
Yeah.
Yeah that's a super good question you know to be honest I was trying to
educate myself about the laminar fmri as I was preparing this tutorial and so I'm not I'm not an
expert on this at all maybe someone here can speak to that I don't know Sam do you know anything about
this yeah no um so uh yeah I really don't know what the limits of the technology is it's still
pretty new and people are I think are developing better methods for it yeah um all right so why
don't we keep going unless there are other questions yeah yeah well um the they have
similar in the sense that there's prediction errors that are being coded right but but they're
quite different in the sense that the the classical story about dopamine and temporal
difference errors is that um they're encoding errors in the value function whereas um here
we're talking about errors in a in a network of beliefs right so um yeah well so so first
has an entirely different story about what dopamine is doing um that um that basically tonic
dopamine signals aren't conveying this precision variable that we were talking about um the lambda
term um and he so he he doesn't really buy the idea at all that that dopamine is conveying td
errors because he doesn't buy the idea that the brain is doing td learning right the planning
as inference idea subsumes td learning into some other algorithm that that's basically all about
basically he's trying to transform the problem of learning values into a problem of um of
inference into an inference problem optimizing free energy so I guess the upshot is that they
have some superficial similarities in the sense that they use prediction errors to to do updating
but the um what the the thing that they're updating is rather different
okay um so there is a there is a puzzle here which is that so so far we've been talking about
examples where predictive coding posits the neural responses to expected stimuli should be
suppressed but at the same time there are lots of studies that report increased neural responses
to expected stimuli so how can we resolve this tension um and the answer or one proposed answer
is that um expectation in attention may have some overlapping functionality but the but
they're really at a computational level two distinct things happening here so one has to do with
the the defects of expectation on the prediction errors
so expectations will suppress the prediction errors but um expectation can also have an
effect on the precision um and this requires us to have a a little bit more um principal
distinction between expectation and attention um so so the argument from free energy um according
to free energy theories is that attention basically corresponds to precision so when
you are tending to some stimulus or some region of space then you're going to increase your your
precision um and so and that's going to have this kind of multiplicative or gain modulatory
effect on the prediction errors and to to make this a little bit more explicit let's look at this
diagram um from this uh book chapter by peter cook um so the idea is this follows so um let's
imagine that we have this uh predicted stimulus appearing or an unpredicted stimulus appearance
so the prediction error is going to be bigger uh for the unpredicted stimulus than for the
predicted stimulus um but we can also make this distinction between whether um the um
stimulus was attended or not attended so imagine that i'm getting some attentional queue which
tells me whether to pay attention to the to the upcoming stimulus um and i'm going to have
higher precision when i'm paying attention and lower precision when i'm not paying attention
and then when you multiply these two things together according to the predictive coding idea
then you get this precision weighted prediction error and you see this interaction effect emerge
so um for unattended stimuli uh you get someone really doesn't like attention and precision
but um uh so for unattended stimuli you get higher responses for unpredicted than
predicted stimuli and that's that's basically the the standard predictive coding story but if
you're attending then you can get higher um responses higher uh higher precision weighted
prediction errors for predicted than for unpredicted stimuli so it the pattern basically
reverses itself um and this was studied in the in the following way so imagine so there's a task
where um you get a prediction queue which says um which tells you the likely position where a
stimulus is going to appear and then you in addition get an attention queue which tells you
whether to pay attention to the stimulus in a particular location um so you can for example
get stimuli that are attended but unpredicted um or stimuli that are predicted but unattended
okay um and when you do this you can see this this um crossover interaction uh in the in the bold
signal amplitude in v1 um so you see that when it's unattended you get bigger responses for
unpredicted than for predicted stimuli and then this flips around uh for attention consistent
with that uh simulation of attention weighted uh prediction errors that I showed you before
um any questions about that okay
um now I've been a little bit vague about how exactly attention affects precision
so there there's broadly speaking there's two ways you could do this there's two ways that you
can increase signal to noise ratio so you can reduce noise or increase signal um and there's
some evidence for both of these possibilities so so for example um uh across if you look across
subjects in one there's one study um where they looked at um the um the amplitude the mean amplitude
of the bold signal in v1 and related that to uh a performance measure and showed that um individuals
with higher amplitude bold signals um showed greater performance on a psychophysical task so
that would be consistent with this signal amplification interpretation um and then there's
another possibility is that there's a noise suppression interpretation which is consistent
with some electrophysiology data from Cohen and Mansell um where they they found that um the variance
was lower um for stimuli that that were attended compared to stimuli that were unattended so these
are two possibilities and they're not mutually exclusive so you could have lower variance and
higher signal um to produce these attention modulations all right so to summarize the the
stories this far um um variational inference offers a path for scalable algorithms um and
if you look in the machine learning literature there's um there's been a trend sort of away
from Monte Carlo methods which are often considered to be too slow towards variational methods um
and even towards uh what are called amortized variational methods which i'm going to talk about
a little later so familiar algorithms like the variational autoencoder basically run on those
kinds of um those kinds of approximations um now the the predictive coding analysis shows that you
can implement um variational inference in a neuro neurobiologically plausible circuit um
but just keep in mind that this requires a bunch of assumptions um so if those assumptions break
like for example if you can't impose a Gaussian assumption then you have to do something else
um uh now there's a challenge here so what is the psychological evidence for for any of this um
one of the problems is that if we can't impose any constraints on the variational family it's
virtually unfalsifiable and we already sort of saw this in a few examples um like um so the the
predictive coding framework imposes some very strong assumptions and that's what makes it a
falsifiable claim about neural activity um but it's a little bit less clear what kind of falsifiable
claims we can make about human behavior on the basis of those assumptions um nonetheless i'm
going to try to give you two examples which i think maybe make the case for um psychological
evidence in favor variational approximation so the first has to do with order effects and factorized
approximations and the second has to do with um kind of mental graph surgery and causal inference
all right so um many of you may be familiar with um blocking effects
i'm using this notation where a and b correspond to q's and the plus and minus is correspond to
rewards or or absences of rewards um so the four blocking paradigm works as follows so
i present a q with reward and i do that a bunch of times and then in the second phase i present
that that same q but in compound with a novel q b and i also reward that compound and then i test
with um with that novel q so i want to see how much um how much association with the reward has
