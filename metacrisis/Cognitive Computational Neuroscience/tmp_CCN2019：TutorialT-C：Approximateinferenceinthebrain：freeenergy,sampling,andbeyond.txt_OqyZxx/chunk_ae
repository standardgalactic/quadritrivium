as this probability that's an exponential of a negative energy function. And the energy function
for a simple Ising model is going to be this bilinear function of these binary states. So
we're now imagining the states as being a collection of binary random variables. And
we have this weight matrix that determines the symmetric connections between those neurons.
Obviously, there's aspects of this that are biologically impossible like the symmetry,
but we're going to just put that aside for a moment. And now we're going to say,
now we're going to define a transition function, which is the transition function used by Gibbs
sampling, where we condition on all the neurons except one and sample from the conditional
distribution of that one neuron, conditional on all the other ones, and you get a rule which
will be familiar to many of you, which is you take the sum weighted activations, the inputs to
that neuron, and then you pass it through a logistic sigmoid and draw a sample from that
Bernoulli distribution. So this is basically like a discrete time analog of a linear nonlinear
Poisson process. But you can interpret this as sampling from this Markov random field's
probability distribution. And if you clamp any of those states, then those act as data. Those
are the visible units, and then the other units are the hidden units. And I'll mention just one
other thing about this that some of you may already know that this is very closely related to
hot field networks. So hot field networks are basically like the zero temperature limit of this
model. Okay. So if we just start with some of the summary statistics that people have
quantified about neural variability, for example, in V1, you can set up this kind of network to
emulate some aspects of the known properties of V1 neurons. So for example, you can get skewed
inter-spike interval distributions and a coefficient of variation that's close to one.
But those are not really unique properties of this particular kind of network, but I'd refer
you to Lars Brussing's paper, which was kind of the first paper to really take seriously
this neural dynamics as a sampling idea and look at how you could take that basic network
architecture that I just showed you and make it more biologically plausible, for example, by
introducing refractory periods and things like that. There's other evidence, for example,
of multistability in the hippocampus where you get things like flickering between
different place field maps or place fields that seem to be poorly correlated with each other
across repeated entries into the same environment. And by analogy with how we understood
perceptual multistability, it's tempting to interpret these kinds of phenomena as
multistability in the neural representation of space. But I think a lot more work has to
be done to really make that argument plausible. Right now it's just kind of suggestive evidence.
Now, gib sampling is very convenient in the sense that it's easy to imagine how a neural
circuit could implement gib sampling, but it's not very efficient. And it's not very efficient
in the following sense. So gib sampling is known to often give rise to random walk behaviors.
In essence, what that means is that the network is going to get stuck in local optima because
it's doing these local changes in a very undirected way. So it's kind of exploring randomly in some
region of the space. And it's not going to move very quickly along the energy function.
I mean, it eventually will generate samples that occupy low energy states, but it could take a
really long time. And that's why statisticians and even before them physicists had developed
techniques for more efficient sampling from probability distributions where, and this
specifically applies to cases where the probability distribution is differentiable.
And so we can basically take advantage of gradient information in the energy function
to follow, to do a directed stochastic sampling of that distribution. So we kind of move along
low energy regions of that distribution. And this is sometimes when it's Hamiltonian,
Monte Carlo. And the basic idea is that you can implement this by introducing an auxiliary
variable and alternate between sampling from the auxiliary variable and sampling from the
variable, the state variable. And Lawrence Acheson and Maté Lengel have developed a specific
neural implementation of this. And maybe Maté will talk about this in his keynote talk.
And the way that they do this is that they have excitatory neurons that represent the
latent variables, the state variables. And then you have inhibitory neurons which represent
the auxiliary variables. And these are reciprocally connected. And they both receive input from the
stimulus, they receive stimulus input. And if you set this up correctly so that
these two variables are basically drawing, the dynamics of these variables depend on the gradient
of the log probability distribution, then you can show that this exactly implements Hamiltonian,
Monte Carlo and is much more efficient than the kinds of Gibbs sampling algorithms that I showed
you before. And it also produces some interesting emergent properties. So for example, you can get
oscillations from this because of the reciprocal inhibition. And you can also get balanced
excitation inhibition, which is arguably a hallmark of cortical activity.
Another interesting idea is to apply MCMC not to activities of neurons but to actually synaptic
plasticity. So one thing that we know about dendrites is that the volume of dendritic spines
varies continuously over time and possibly stochastically. So if you image some synapse
at any given time, there's going to be highly variable configuration of dendritic spines.
And new spines are continuously being formed and existing ones are also continuously being
eliminated. And one interpretation of this is that this variability arises from some kind
of sampling based inference over the synaptic strength, so over the parameters of the graphical
model. And this was explored in more detail by Kaplan and Wilkin-Mason colleagues. And I'm not
going to go into detail, but I'll just refer you to this paper if you're interested in learning
more. And they basically showed how you could set up synaptic plasticity rules that stochastically
sample from the distribution of synaptic strengths. And then you can interpret this as a MCMC algorithm.
Yeah.
All right. Yeah. So let's go back to this graphical model as an example. Okay. So
this assumes that we know W, this weight matrix, but we don't know S, the state variable. So we're
trying to infer the state variable conditional on knowing what the underlying parameters of the
graphical model are, W, right? But you could also have uncertainty about W itself, right? And we can
write down, so typically what we would do is we would treat this as a point estimation problem
and we'd write down plasticity rules that estimate a single value for W. That's the standard way
that it's usually done, but we could alternatively treat this as a probability estimation problem
and try to infer the whole posterior distribution over W. And that's kind of the logic of this
synaptic sampling idea. That distribution is going to be intractable, but we can stochastically
sample it via MCMC in the same way or analogous to the way that we would sample the state variable.
Okay. So to summarize so far, MCMC can account for many aspects of neural and cognitive dynamics.
One general concern here is that it's slow, right? It's going to take a while to
generate all of those samples. And I think this goes back to your question, or one of
your guys' questions about, you know, is this possible for generating samples at the time
scale that we're looking at activity? So I think that's an important question
and one that the subsequent section is going to address. So I'll mention one other thing,
which is it's not really well designed for tasks that require online inference. So
suppose that data are streaming, they're coming in a stream and you need to update your
posterior online, you don't want to have to be rerunning the Markov chain over and over again
at every time step, right? So that seems highly implausible. So that motivates a different class
of algorithms based on important sampling. So here's the basic idea of important sampling.
So instead of distributing samples in time, like MCMC algorithms, we're going to distribute them in
space. So imagine we're going to draw a whole bunch of samples simultaneously. But again,
the problem comes back to bite us, which is how do you actually generate those samples? So we're
going to assume that you can sample from some other distribution phi, which is easy to sample from.
And then what we're going to do is construct a weighted set of delta functions from those samples
and the weights are going to depend on both the generative model and this proposal distribution
phi. And that specific functional form of the weights is given here. So in the numerator,
it's just the joint probability of the samples in the data. And then the denominator is phi,
the probably that phi puts on those samples, and that's to compensate for the fact that
you're sampling from the wrong distribution. And you'll notice that if we assume that you're
sampling from your prior, then this simplifies so that the weights are just normalized likelihoods.
And this idea has been exploited to formalize a particular neural implementation of important
sampling that she and Griffiths developed. And the idea here is that we have a bunch of stimulus
tuned neurons. And the tuning functions of these neurons basically correspond, are directly
encoding the likelihood function. And then these neurons are being recruited with probability
proportional to the prior. So that would imply that the distribution of preferred stimuli
should correspond to the prior. And then basically what you can do is very simply just take the
activations of these stimulus tuned neurons that are recruited with probability proportional to
the prior. And once you normalize them appropriately, you can get the correct posterior probability
of this, asymptotically at least. So they apply this to the oblique effect, which is defining that
sensitivity to orientation is greater for cardinal orientations than for oblique orientations.
And there is a normative explanation for this ecological explanation, which is that
if you look at natural images, cardinal orientations are much more frequent than oblique
orientations. And so it makes sense that you would want to represent those with higher probability.
And the way that they implement this in their model is assuming that the distribution of
neurons tuned to particular orientations follows this ecological distribution,
this ecological prior, so that you have more neurons that are tuned to cardinal orientations
than oblique orientations. And that seems to be consistent with the data on neural tuning curves
in V1. So let's talk now about particle filtering, which is the online version of
important sampling. So the idea here is that I'm getting data, one data point at a time,
and I can recursively update my importance weights using the following simple formula.
So I'm just recomputing my importance weights by multiplying this new likelihood with the previous
importance weight, and then renormalizing. And this has an interesting property, but I have
to tell you one more thing about particle filtering, which will make this make more sense,
which is that if you do this, you may find yourself in a situation where many of the particles
are conditionally unlikely. They have really low importance weights. So effectively,
they're not doing any work for you in the posterior. So ideally, you'd like to get rid of them
by deleting them and resampling the particles according to their importance weights. So the
idea is that I'm going to keep a particle around in proportion to its importance weight,
and then continue updating the particles that have survived that resampling step.
And so that's going to get rid of conditionally unlikely particles. But the problem is that
what it also means is that if some particle now becomes conditionally likely later on for
later evidence, you're not going to have that particle around anymore. And so you're not going
to be able to have the correct interpretation for that later evidence. And that leads to,
so this is essentially a kind of order effect. And you see this
behaviorally in what are known as garden paths. So this is most commonly
studied in linguistics. And this is the situation where initially promising hypotheses are invalidated
by later data. So humans are getting stuck with their initial hypothesis and fail data
in for the correct hypothesis. And as I said, this could be accounted for by some kind of
resampling step in the particle filter. So here are a few examples of garden paths in linguistics.
These are sentences you read them in. At first, you don't quite get them, and you have to kind
of go back and figure out what they mean. So the old man, the boat. Yeah, the horse race past the
barn fell. The horse that race past the barn fell, right? So if you think about what's going on here,
the problem is that the first part of the sentence suggested a particular syntactic
interpretation that was invalidated by the later part of the sentence. And Roger Levy and colleagues
have formalized this in a particular parsing, stochastic parsing model. So the idea is that
the particle filter is now doing inference over parse trees. But because of resampling,
it's going to delete some of these parse trees. So he uses this example of the woman
brought the sandwich from the kitchen tripped. We can focus our attention on two interpretations,
a main verb construction and a reduced relative construction. So initially,
the main verb construction is favored by the initial data. But then by the end of the sentence,
the reduced relative construction is favored. But the problem is that if you've deleted the
reduced relative construction because it was conditionally unlikely, then you have trouble
parsing the sentence and you have to go back. Now, the idea of garden paths is actually
more general than just linguistic. So we've studied this in concept learning, so specifically a
number of concept learning. And one of the reasons I want to tell you about this series of studies
is because we were interested in what kinds of algorithms are really scalable to more complex
compositional hypothesis spaces that have a much vaster hypothesis space than what I've been telling
you about. So we used a sequential version of the number game, and I'll explain to you what that is.
And we explored whether putting people under cognitive load would exacerbate
garden path effects. So we were also trying to look at this kind of computational rationality
interpretation to see if reduction of resource availability will have an effect on garden paths.
So here's how the sequential number game works. So I'm going to give you a number,
and you're going to have to tell me which other numbers between 1 and 100 belong to the same
number concept. In other words, I'm generating samples from some concept like numbers between
1 and 10, or even numbers, or odd numbers, or powers of two. But I can also generate from
compositional number concepts, like even numbers between 20 and 40. So the way this works is I'm
showing these numbers sequentially, and then subjects on each trial are going to give you
this, the extension of that, the hypothetical extension of that number concept. And the critical
thing that we did here was this order manipulation, where we took the same set of numbers, but we
changed where in the sequence the number 31 appeared. And the reason this was relevant is because
intuitively, when you see 31 early, then you favor the hypothesis numbers between 20 and 40.
But if you see the number 31 late, then you favor the hypothesis multiples of three.
So here's what the data looked like. Oh yeah, and I forgot to mention that we put
some of our subjects we put under distraction, so they had to execute a working memory task
simultaneously while doing this task. And so I'm comparing distractor and no distractor
across with whether the number 31 came early or it came late. And so you can see that when the
let's just look at the no distractor conditions first. So the people favored the,
sorry, let's just look at early versus late under no distractor. So people favor the numbers
between 30 and 40 concept when the distractor came early, but they favored this concept of,
they had some kind of mixture of the concepts of multiples of three and numbers between 30 and 40
when the distractor came late. And I should have mentioned that what I'm showing you here are the
responses at the very last trial, okay, so they've seen the entire sequence of numbers at that point.
So you can see now that there is an order effect consistent with the intuition that I just gave
you and that that order effect is exacerbated particularly in the late condition when I put
people under cognitive load with this distractor. And we can capture this with this particle filtering
model by assuming that there's this resampling step and also that when you're under distraction,
you have fewer particles available to represent this hypothesis space.
Similar to how she and Griffiths implemented important sampling in a neural circuit,
Hwang and Rao have developed a neural implementation of particle filtering.
I'm not going to go into much detail about this, but essentially it's a similar idea except that
because we're now talking about updating dynamically, what they've done is implemented
this forward transition model for a dynamical system in the recurrent connections and then
the feedforward connections correspond to the observation model. So the feedforward connections
are providing the likelihood function and the recurrent connections are providing the dynamics
of the prior. And then they show that you can, that this kind of model can generate stochastic
samples that can capture things like multimodal distributions, but it's still an open question
whether this kind of model will produce the kinds of garden path effects that we saw in
