orientations on these no stimulus trials. And what they find is that neurons with similar
preferred orientations show some correlation of activity. And it also depends on whether those
orientations lead to the same decision or a different decision. So these two things will
obviously be related. But essentially, you put a decision boundary in this orientation space. And
now, neurons that are on either side of this decision boundary, they'll basically co-variate
together more strongly than across that decision boundary. And they'll do that in a way that's
parametrically dependent on the difference in their preferred orientation. And so the reasoning
here is that you get these noise correlations because of the sampling process that when you
observe an image, you're going to get draws from this belief distribution. And the hypothetical
neurons that prefer similar orientations are going to tend to be co-active because they have
similar probability under the posterior. So let's come to the question now of generating
samples. So I've so far assumed naively that we could just somehow magically generate samples
from the posterior. But that's actually the hard part. How do we get samples from the posterior
if the posterior is itself intractable? And I'm going to talk about two broad approaches to
solving this problem. One is Markov chain Monte Carlo, and the other is important sampling in
its dynamical variant, which is called particle filtering. So the basic idea of MCMC is that we
can, even though we can't directly sample from the posterior, we can sample from some dynamical
system whose equilibrium distribution is the posterior. So that's schematized here. We're
going to set up a Markov chain that's parameterized by some transition distribution over the hidden
state or stimulus. And what makes it Markovian is because it only depends on the last sample that
was drawn from that distribution. And if you set this up right, then you can guarantee that this
chain is eventually going to converge to an equilibrium where samples are drawn from the
target distribution. And it turns out actually there are pretty straightforward ways to set up
chains with this property. I'll come back to that in a moment. But for now, I just want to
highlight two conceptual properties of MCMC. So what is that? In general, MCMC is going to be
autocorrelated. For most implementations of MCMC, there's going to be autocorrelation. We're going
to try to draw out some of the implications of that autocorrelation. So samples are going to
tend to be similar if they're generated at similar points in time. And then the other is ergodicity.
So asymptotically, if you run this long enough, it's going to be independent of your starting
point. So initialization doesn't really matter. However, again, if we return to this computational
rationality idea that we're only drawing a limited number of samples, because of autocorrelation,
you're going to show dependence on the starting point. And that's going to have some important
implications that we'll talk about. So the classic MCMC algorithm is called the Metropolis
Hastings algorithm. And the way it works is as follows. So again, I'm using k to index samples,
but now k is basically indexing time, a temporal sequence of draws from this Markov chain.
So what we're going to do is at time k, we're going to sample from some proposed new state,
s prime, from this distribution phi. And phi can basically be anything,
but there are some practical constraints on how we specify phi. And then we're going to accept or
reject this proposal based on this acceptance rule. So if the proposal, or actually let me back up
for a second. So if we look at this ratio, we're comparing the joint probability of this new sample
compared to the joint probability of the old sample. And then we're multiplying it by,
oh sorry, this should be a conditional distribution in that ratio. So we want to look at,
we have to compensate for the fact that it was drawn from this proposal distribution.
And in the simplest case, if that proposal distribution is symmetric, then those phi
terms cancel out. And we're just doing a ratio comparison of the joint distribution for the
proposed sample relative to the previous sample. So if the proposed sample increases the joint
probability, then you're going to accept it deterministically. But you could also accept it
with some probability if it decreases the joint probability. So it has this flavor of
stochastically exploring the posterior and sometimes transitioning to states that have lower
probability. Now, Gibbs sampling is a special case of metropolis hastings where the proposal
distribution is the conditional distribution. So I'm going to pick one of the variables,
and this applies to the case where the state is multi-dimensional. So I'm going to pick one of
the variables in the state, condition on all the others, and sample from that conditional
distribution. And it turns out that when you do that, all the proposals are going to be accepted.
And there are different variations of this. So you can do, for example, blocked Gibbs
where you sample from a bunch of these variables at the same time conditioning on all the others.
Okay. So that's kind of a very quick overview of MCMC, broadly speaking,
and some specific implementations of MCMC. Now I want to talk about applications to
perception and cognition. The place, I think, where this has been most successful
is in understanding perceptual multistability. Because that's an instance where there's very
clear stochasticity in the perceptual phenomenology, and it seems seductive to think that that might
arise from variability in the sampling process. And it's particularly important to use these
kinds of dynamical sampling algorithms to explain the dynamics of the perceptual phenomenology.
So there's a lot of different ways to create multistable percepts. I'm going to focus on
binocular rivalry. That's where you show two different images to the eyes. And in general,
what people experience is seeing one of the images dominant at a particular time, and then
there's stochastic switches between the dominant images.
So a number of years ago, we developed a sampling-based model of perceptual
multistability, particularly in application to binocular rivalry. And the basic idea was
not too dissimilar from the image models that I showed you before. So we're going to posit that
these two images, one for each eye, but the brain is trying to infer the true latent image that
generated the sensory inputs. It also has this outlier process that basically allows it to
determine whether particular pixels are corrupted so that they're giving incorrect
input, so it can kind of ignore the sensory input at those locations in the visual field.
And the way that we model perceptual switches is we set some threshold on the number of node
switches that would determine that a particular stimulus is dominant. So the idea is that the
sampling process is operating at the level of pixels, so one node per pixel, but the subject's
report is going to depend on some conglomeration of those nodes being consistent with each other.
This is a somewhat arbitrary assumption, but as we'll see in a second, there's some interesting
implications when you actually look at the individual nodes themselves. So I'm showing you
here on the top right an example dynamics of this model. The critical explanatory part of this model
comes from the fact that the posterior is going to be multimodal for these, when the binocular
images are different. And so the distribution is going to migrate from one mode to the other
and back and forth forever, and at equilibrium, it should be exhibiting that behavior.
One of the classic ways that people have quantified binocular rivalries looking at the
distribution of dominance durations, and people have spent a lot of time arguing about whether
this distribution is gamma distributed or log normal distributed, you can sort of, to a first
approximation, capture some of those quantitatively with this model, but I think that's kind of the
least interesting aspect of binocular rivalry. There's much more interesting aspects of binocular
rivalry when you look at some of the other dynamical phenomena that come out of that.
So one really interesting phenomenon is called traveling waves. These were experiments where
what they did was they showed these different gradings to the two eyes, and the gradings
were organized in an annulus. So at any given time, your eye only saw one of those gradings,
sorry, your perception, you perceived only one of those gradings, but what they did was
they would transiently increase the contrast of the grading that was currently not dominant.
And what the effect that had was kind of like lighting a fuse, so you would see
that contrast enhanced part of the grading would now become dominant, and then it would start to
travel around the annulus and make the whole annulus dominant that wasn't dominant before,
but there would be this sequential structure, and you could reveal that by measuring people's
judgments about what was the dominant percept at particular locations on the annulus relative to
where that contrast was enhanced, and you see this parametric effect of distance
on the propagation time. And in fact, you can make that propagation time longer by introducing a
gap in the annulus as though the inference had to kind of jump over this gap. And you can model
this with the model that I described to you when you impose this annular topology
on the underlying latent image. So this is a nice example of where the switches are not
unitary in the sense that you don't always see the whole image switch. Sometimes you see just
parts of the image switch, and that has a dynamic that you can capture with MCMC applied to these
graphical models. Another example of this is piecemeal rivalry. So if you make a stimulus really big,
then it turns out that people have more time spent in this kind of patchy zone where
the one image might be dominant in one part of the visual field, but not in the other part
of the visual field. And you're going to require more node switches until you achieve a stable
percept. So this is appealing to the idea that if for bigger images you have to basically
do more computation before you have a large scale shift in the percept.
Another interesting aspect of rivalry which can be captured by this kind of model is fusion.
So it turns out that you don't always experience rivalry. So sometimes you have
alternations between, sorry, sometimes the two stimuli fuse together into a unified percept,
and you can get this in a few different ways. So for example, if you decrease the contrast
of the images, then you get more fusion, or if you make them more similar to each other,
like gratings that different orientation, if you make the orientation more similar,
then you're more likely to get fusion. And this again appeals to this idea that the
multi-stability arises because of this multimodal posterior, and when you reduce the contrast or you
make the images more similar in some feature space, then you're going to be basically reducing the
multimodality of the posterior. And under some circumstances, you might get a single mode.
All right. So to summarize this last section, I've argued that multi-stability can arise
from sampling for a multimodal posterior. And sampling can explain a lot of different aspects
of this perceptual multi-stability. So domination, traveling waves, piecemeal rivalry, and fusion.
So now I want to shift gears a little bit and talk about the application of these models to
high-level cognition. Before I do that, any questions or comments?
Okay. Yes.
So you're talking about, because there are other approximate algorithms that do learn something
about an actual multimodal posterior, right? So there's a certain range of rational approximations.
Yeah.
We also have the updates, and we think they're like, this is very big.
Yeah.
And I'm looking at, is the right side of it just be a specific sound place,
as opposed to any approximate method that has an effect on the map?
Yeah. That's a really interesting question. So Peter Diane actually had a model in 1998 or 1999
of rivalry, which was based on a variational inference algorithm. But to get the switching
behavior, he needed to introduce a fatigue process. And that's often how in the biophysical models
of rivalry, they introduced some kind of fatigue process to get the switching behavior. Now,
I can't speak for all possible variational models. In fact, one of the things that I'm going to talk
about in the second half of the tutorial is it's very hard to pin down the commitments of
variational methods without making some more assumptions. So for example, you could have
sample-based variational approximations, and you could even have optimization of those sample-based
approximations operate via some kind of stochastic search. So at that point, are we even talking
about something that's different, distinguishable from MCMC algorithms? Yeah.
You mean you're talking about which generative model or maybe I'm not following. Yeah.
Not just which generative model, but the variable test represents. Right.
Is orientation the same thing as the weight of the basic functions?
Yeah.
Well, what do you mean by the same thing? Obviously, it's not the same thing, but
the question is, can we deduce that similar inference operations are acting upon weights
and orientations? Yes, I think so, right? Because there's no way to
make predictions from an approximate inference algorithm unless you first specify the generative
model that it's operating over, right? And actually even what the basic representational
primitives are, right? That's what makes it so hard because if the model makes the wrong
prediction, is that because the inference algorithm was wrong or because the probability
model was wrong or because we just represented the problem in totally the wrong way or we measured
things wrong, there's all sorts of failure modes. And that's what makes this so hard to do, right?
And so I'm kind of giving you the most optimistic reading of these phenomena,
but everything is up for debate. I totally accept that. Yeah.
I guess I'm saying the same thing, that's the very original question. Yeah, I guess when I think
about it, I don't know, but I think it's what I think you would have let you do in the follow-up.
So, say that one more time. In the follow-up work, I guess I was, as I did, I think deep,
like deep, garland, in particular, visual cortex about all of it. And then when you say,
did you put recurrence in the narrative page before this, and then from the switching between
the gap of all three models, the addition of the gap between the recurrence and the
gap difference before the network, you could also actually do it. Yeah, that's interesting. Sorry,
I broke my promise that I was going to repeat the question. So this, the question was,
could you get multistability in a feedforward neural network with recur, well, it's not feedforward
anymore. Feedforward plus recurrence using something like a convolutional neural network
of the story that Jim DeCarlo's lab uses. I mean, yes, like we know from, you know,
all of these models that I'm talking about, we're long predated by more biophysical models for
modeling rivalry that are based on dynamical systems. So you can construct dynamical systems
that show oscillations, right? That's essentially what it comes down to. So this doesn't exclude any
of those, of those kinds of formalizations, but at some level, what I'm arguing here is that
there's an interpretation of the stochasticity, or at least apparent stochasticity that comes
from those models that's consistent with a probabilistic inference or an approximate
inference interpretation. So we can construct, if you take the model that I just described,
you can write it down as a recurrent neural network. In fact, I'm going to talk about that
a little bit later when I discuss neural circuits. So it actually is at the circuit level very similar
to what those kinds of models would look like. But the advantage of looking at it through the
lens of probabilistic inference is that we can have this interpretation that it's sampling from
this distribution over latent variables. Okay. So I'm going to go on and talk about
application of these ideas to high-level cognition, but specifically to understanding
biases in probabilistic reasoning.
Everything okay out there?
All right. So consider the following three questions. I ask you, what is the probability
of dying from disease? Or what is the probability of dying from heart disease, cancer, stroke,
or any other disease? Or what's the probability of dying from pneumonia, diabetes, cirrhosis,
or any other disease? So all of these morbid questions have the same answer,
because all we've done differently in these different questions is unpacked some particular
disjunctive hypothesis into the conjunction of a bunch of, sorry, the disjunction of a
bunch of sub-hypothesis. So the question here is, even though these all have the same answer,
why is it that framing them differently leads to different responses?
And let me explain to you what the responses are. So there's two dominant patterns that
happen here. When you unpack a hypothesis into typical sub-hypothesis,
then you get what's called sub-additivity. So that's shown in this green example. So
heart disease, cancer, and stroke are all typical fatal diseases or typical diseases.
And the idea here is that intuitively, if I ask you what's the probability of dying from disease,
