the cognitive studies. So there's still a gap between the cognitive studies and what we understand
about the neural implementation. So to summarize so far, particle filtering offers a psychologically
and newly plausible mechanism for online approximate inference. So it fixes some of the problems that
MCMC had in the sense that you can distribute samples in space, you can sample online,
so it's more temporally efficient, but it also creates some problems that MCMC
didn't have as much of an issue with. For example, it gets, it's more vulnerable to getting stuck
in poor optima compared to MCMC because it can't really do a local search. However, we can augment
important sampling particle filtering algorithms with what's called rejuvenation, which essentially
is a hybridization of MCMC and important sampling where we apply MCMC transition functions
to the samples in a particle filter, and that can ameliorate this problem. So it kind of combines
the strengths of these two approaches. All right, so the last thing that I want to talk about just
briefly before we take a break is combining spatial and temporal codes. So we've talked about
spatial codes like important sampling and particle filtering and probabilistic population codes,
and we also talked about temporal codes like MCMC, and these have complementary strengths and
weaknesses, so the spatial codes are fast but inflexible in the sense that they, they can't
really be, that they may have, they may struggle with more complex distributions even though they
have the speed advantage, whereas temporal codes can be more flexibly applied to a broader range
of distributions, but they're slow. So can we combine the relative advantages of these approaches?
So Christina Savin and Sophie Deneuve have developed one particular answer to this question
where they assume that samplers are distributed both in space across neurons and across time,
so the underlying signal is kind of multiplexed spatial temporally, and then they have a linear
readout which decodes this spatial temporal information into the, into an inference about
the latent signal, and you can learn those decoding weights, and they've demonstrated how this could
lead to dramatic efficiency gains because now you're basically sampling in effect with multiple
parallel chains, so you can approximate this, the posterior inner fraction of a time that you'd
require from a single chain. So there are interesting psychological implications of this
parallel sampling as well. So for example, if you ask someone to do something like
repeatedly estimate a constant time interval, so you're presenting stimulus events at some
constant time interval, you see, in people's estimates, you see a power law autocorrelation
function, so it has this functional formula that I've drawn here where k is the lag between samples,
and there's some interesting work by Xu, Sanborn, and Chater that showed that this kind of power law
autocorrelation function could arise from sampling multiple chains simultaneously, but they had the
additional twist which is that they allowed the different chains to be at different temperatures,
so they actually had this whole hierarchy of temperatures from cold chains that are low
temperature to hot chains at high temperature, so the hot chains at high temperature are sampling
much more broadly, but are exploring low energy parts of the state space, whereas the cold chains
tend to focus on the only the, sorry, the hot chains are exploring high energy parts of the
state space, and the cold chains are exploring low energy parts of the state space, but you can
develop a Metropolis Hastings algorithm that can propose swaps between the chains, and that
has the effect of basically propagating exploratory samples from the hot chains into the cold chains,
and this kind of complicated machinery which was originally developed by physicists and sometimes
known as the replica method can produce these kinds of power law autocorrelation functions,
as well as a number of interesting dynamical properties of human sample generation, so
it's at least suggestive evidence that maybe this idea of multiple parallel chains has some face
validity. Alright, so interim conclusions from this section that we can combine spatial and
temporal codes via multiple chains, this is, I think this is still pretty speculative, like we
don't, I don't think we have any direct evidence of multiple chains, even at different temperatures
happening in the brain, but that would be really cool if someone could find evidence for that,
and there's some provisional evidence that mental sampling uses these multiple chains,
so in the last few minutes I just wanted to bring up a few other considerations, so one
thing that I haven't talked about is energy efficiency, so one argument for sampling could
be that instead of representing a complete parametric distribution that could be quite
complicated, we generate only a small number of samples, and that could lead to energy efficiency
gains, and we all know that the brain is, seems to be much more energy efficient than
conventional computers, so could this help us build low-energy neuromorphic devices, for example,
and then there's another question which we broached before, which is parsing sources of
variability, so there's not only going to be variability due to sampling, there's also going
to be variability due to noisy perception, model misfascification, noisy decisions, and so on,
and so, so really to make strong inferences about any of this stuff, we need to have a better handle
on what the different sources of variability are, so I'll stop there and see if you guys have any
questions or comments, yeah, can you speak louder please, yeah, no, I haven't heard of any applications
of that, yeah, all right, I think it sounds like people are anxious to get to the coffee break,
so why don't we do that and we'll reconvene in half an hour, all right, so we've talked about
Monte Carlo algorithms, and now we're going to talk about variational methods, all right, so here's
the basic idea of variational inference, the idea is to convert an inference problem into an optimization
problem, and what I mean by that is that we can think about a set of distributions or parameterized
distributions, and we want to pick the distribution in that set that comes closest to the true posterior,
and usually closeness here is measured in terms of Kolbach-Liebler divergence,
I'll come back later to other forms of divergences, but this is kind of the canonical
divergence measure between probability distributions, and so the optimization problem is the following,
so we're going to try to find the approximate posterior queue within some restricted variational
family that minimizes the divergence with the true posterior, and now the problem though is,
well there's a few different problems, so first of all this optimization is intractable
because it requires that we have access to the true posterior, how do we compute the divergence
if we don't know what the true posterior is, there's also a question which we'll come back to
shortly, which is how do you choose the restriction on the variational family so that this optimization
problem is tractable, so it turns out that you can reformulate this in a different way by using
the following identity, so the log marginal likelihood that's log p of d can be decomposed
into the sum of the divergence term and this other functional f, which is the variational
free energy, and the variational free energy has the property that it is tractable to compute,
well it is tractable subject to some constraints which I'll mention in a second, but the important
thing is it doesn't require access to the direct posterior, it only requires access to the
unnormalized posterior, so the joint distribution, the product of the prior and the likelihood,
and in general basically all the approximate inference algorithms we've talked about
assume that you can compute the product of the prior and the likelihood for any particular
sample, the free energy here also requires that you be able to compute this expectation
under the approximate distribution queue and that's what's going to require some further
restrictions on the form that that distribution takes, so assuming you can compute that expectation
and solve this optimization problem minimizing the free energy is going to produce the same
approximate posterior as minimizing the Kale divergence, but it's tractable because it doesn't
require access to the true posterior. These first two slides, variational inference is often
the hardest form of approximate inference for people to get, so you should stop me at any point
if things are unclear. So how do we, what kind of restrictions do we need to place on the variational
family to make this optimization tractable, so there's a few different possibilities.
The most common kind of the classical restriction is called the mean field approximation and it
goes, has a long history in physics and statistical physics and so the idea here is that the posterior
factorizes across dimensions of the state space, so we're going to enforce that the posterior
can be decomposed into the product of a bunch of dimension-specific posterior distributions
and you should keep in mind that in general that's not going to be the case, in general
the posterior is not going to factorize in this way, but the advantage of factorizing the posterior
in this way is that now we only need to take expectations with respect to each individual
dimension as opposed to the whole joint configuration space and that renders computing
the free energy tractable and it also renders, it also allows us to derive a coordinate descent
optimization procedure where we can iteratively minimize the free energy with respect to each
of these dimensions and then just keep iterating that and that will be guaranteed to reach a
local minimum of the free energy. The problem of globally optimizing free energy is not tractable
for most distributional assumptions. Okay, but there are other choices of restrictions on the
variational family, so for example I could impose a certain functional form on the distribution,
like I could posit that the variational approximation is a weighted sum of delta functions
and then the parameters that I want to optimize are both the positions of those delta functions,
so what are the states and then the weights and it turns out you can do this in a computationally
tractable way and it turns out the optimal weights are just the normalized joint probabilities
and this has an interesting connection to important sampling, so the important sampling
stochastically constructs this Monte Carlo approximation and weights the samples. Here
we're asking, suppose that I give you K samples, what is the best possible approximation that you
could come up with for minimizing the KL divergence with the approximate posterior
and this variational particle approximation gives the answer to that question
and it can be deterministically constructed, but it can also be stochastically constructed,
so for example in high-dimensional spaces you might want to stochastically search for the
particles and so then it blends together some of the Monte Carlo algorithms that we were talking
about before and the variational algorithms that, so combining kind of stochastic sampling and
optimization perspectives on approximate influence. Questions so far?
There's one more important restriction that will be important for some of the empirical
modeling which is called the Laplace approximation, so the idea is that if your state space is
continuous, it could be continuous and multi-dimensional, we can approximate the posterior
as a Gaussian around the mode and one reason that this makes sense, at least in some situations,
is that there's something called the Bayesian central limit theorem which says that for continuous
spaces under some regularity assumptions, as you get more data, the posterior will be increasingly
Gaussian around the mode and eventually will actually collapse to a point at the mode and so
this motivates or sort of justifies the use of Laplace approximations and the use of a Gaussian
approximation around the mode of the distribution. Now there's some mathematical tricks here that
we don't need to worry too much about but basically if you take a second order Taylor series
expansion to approximate the log joint probability as a quadratic function around the mode then we
can actually calculate the free energy analytically so it looks something like this, so here's this
non-Gaussian distribution but we're approximating it as a Gaussian around the mode and using this
Taylor series expansion we can calculate the free energy and actually optimize this Gaussian
approximation term. Okay so why am I telling you all this stuff? Well because that's all the setup
that you need to understand one of the most influential ideas about variational approximations
in the brain which is predictive coding. So it turns out that if you make a mean field assumption
combined with a Laplace approximation, combined with the Taylor series expansion, then you can
with all those those dominoes stacked in place then you can actually derive
rules for inference in the approximate inference in the brain that have at least at a conceptual
level a pleasingly simple form. In practice it's more complicated in this I'm simplifying but
this formulation is going to be sufficient to get the gist of the idea. So if we can approximate
the posterior Gaussian and we can use the Taylor series so we can compute gradients of that of
the free energy then you can write down changes in your expectations as proportional to precision
weighted prediction errors. So the thing in parentheses is your prediction error, it's the
discrepancy between the observed data and your expectation and then lambda is the precision,
this is like the reliability of the data and it's importantly you can stack this hierarchically so
the data could actually be the the data might actually be expectations generated by one level
of the hierarchy and so you can you can organize this whole predictive coding machinery hierarchically
and and the idea is that you're one there are feedforward there's a feedforward pathway that's
propagating prediction errors and then there's a feedback pathway that's propagating the predictions
the expectations and those expectations are getting updated according to this rule. So that's
pleasingly simple at least in this abstract form but I just wanted to pause for a second and emphasize
that to get to this kind of formulation you have to have a lot of assumptions in place and it's a
common misconception that predictive coding and the free energy principle go hand in hand and that's
not true that the predictive coding algorithm is not a generic consequence of free energy
minimization of variational methods so it depends on the restriction on the variational family it
depends on the particular way that you're approximating the free energy using a Taylor series
expansion and it depends on the idea that you're doing some kind of gradient descent on the free
energy in this continuous parameter space. So you need a whole bunch of assumptions in place
that's important to keep in mind. So that's that's the kind of the technical part about
predictive coding and free energy principle and and I encourage you guys to also go to this debate
between Carl Friston and Jeff Beck which I'm sure will be something to watch and oh is that you had
something to add yeah yeah yeah right I mean well Friston has derived versions of this for
dynamical models and I just didn't want to make it too complicated here but yeah but the same
sort of idea applies here he comes up he has this like generalized coordinate system for doing the
predictive the the plus approximation but why do you why do you bring that up or you just wanted
to add that that's another restriction yeah right right oh I see yeah so I actually I was
sort of sneakily hoping to avoid this but since you bring it up I'll mention it which is that
another important thing that comes up a lot in discussions of free energy is active inference
oh yeah I'll try to repeat the question the question was what happens when you when policies
interact with inference basically roughly speaking so that that's the focus of a lot of
work from Carl Friston and others on what if you can take actions to reduce free energy
that that's that's what's called active inference and that will have some interplay with this
predictive coding machinery because now now your policy is going to have an effect on
on the prediction or through the policy is actually going to be designed to reduce those
prediction errors but I the reason I didn't want to talk about this in this context was because
I'm trying to stay as pristine as possible and only talk about approximate inference and not
action selection but of course in any kind of real agent you have to think about those things
jointly and and actually um Friston deliberately blurs the boundary between those so so so one
of the more controversial claims of the free energy principle which I'm sure is going to come under
debate uh at this debate is um whether you can take uh decision theory and basically just turn
that all all aspects of decision theory into a probabilistic inference problem so instead of
doing the normal decision theoretic thing where you have a separation between utilities or rewards
and beliefs so you basically convert the utilities into probability functions and then
set if you set that up right then optimizing the free energy is it can be expressed isomorphically
to optimizing expected utility um and the motivation for doing that is that now you can have kind of
one algorithm to rule them all in the brain and not have like separate decision and belief components
right and there's an empirical question to what extent that is actually an accurate description
of how the brain works yeah well there is optimization in the in the same sense that
there's optimization here which is that inference the variational methods treat inference as an
optimization problem right yeah so you can but but the thing is that if you um if you have a good
approximation of the posterior then that's the same as saying that you have optimized the divergence
between your approximation and your and the true posterior right the the difference algorithm
algorithmically is whether you write it down explicitly as an optimization
algorithm right so so Monte Carlo algorithms usually are not interpreted as optimizing some
divergence functional um but but it is the case that asymptotically they are optimizing that because
asymptotically they'll converge to the true posterior and that's going to minimize the
