really I want to only think about generative models where I can actually do tractable inference
and and even in the cases where I try to do you know quasi tractable inference you know approximate
inference in some complex generative model my inferences might not line up very well with the
true posterior for that generative model so I may I may end up having a pretty stark mismatch
between my inferences and the true posterior so that might add that might mean that the brain
selects generative models that are somehow matched to its approximation machinery so I don't know
if any of you guys have any thoughts about that I don't know well I guess it depends maybe maybe
you can yeah right right I think it's just so hard to make a general statement about that but I
think you could probably prove that certain combinations are distinguishable I feel like
I don't know if wage is still here I think he was doing some experiments trying to do something like
yeah I mean I still think it's yeah I still think it's meaningful to because how do we
even talk about approximate inference or inference at all if we don't know what kind of generative
model we're assuming I mean then we may as well just abandon this whole perspective right because
we just there's no way to derive ex ante predictions about inference if we don't
start with a generative model yeah yeah right right right and yeah no no I think that's that's
a super interesting possibility and I guess I guess I don't know how else to deal with that
except on a kind of case-by-case basis like someone should say I think the brain is doing
this generative model in this inference and then we'll try to come with some alternatives that
maybe and maybe we'll find out that it's not identifiable in the end anybody else yeah
so we're always wondering this framework of optimization but I mean there's stuff on the
traditional in the old days or even T even if for example it's not following any cost
function that's not an optimization problem is it TD learning is not why not I mean there's a
I mean it you're trying to minimize that you know there's a there's a bellman optimality
equation that you're trying to satisfy right yeah yeah I mean it's suspenseful
yeah uh-huh
yeah but you're sorry so say again
right
right
yeah no I think that that always seemed very plausible to me that you could have this kind of
society of mind we have a bunch of different systems trying to do their own thing and then
there's not necessarily unified cost function I mean at some level if you believe in evolution
there has to be a unified cost function which is fitness but but at the level of the
organism the optimization that's being performed in the single organism that might not be
yeah yeah
um
what do you want to do with variation inequalities
yeah I mean I mean you you you can read Carl Friston right he's trying to
put everything into this framework right so um so you can judge for yourself whether he succeeds
yeah all right so one yeah yeah yeah yeah
well sorry well I think the predictive coding is the most plausible example of that right
that that's a that's a biologically plausible architecture for implementing
you mean like can we can we reverse engineer the constraints on the variational family from
some biophysical constraints um maybe I mean I think that I think that Carl Friston has tried to do
things of that sort um and and there's certainly some there's a paper by Andrey Bostos about how
you can use cortical canonical microcircuits to implement various predictive coding schemes and
so there there there's stuff like that I don't know how strong reverse inference you could make
from that but I don't know was there a comment over here that you wanted to jump in yeah
oh you're Andre okay
okay so all right so talk to Andre he seems to have the answer
yeah okay well that's that's interesting to know yeah
yes among other things
yeah
yeah that's an interesting idea so I would imagine yes I think that's so one thing that I
always wanted to do was like variational signal detection theory basically you know
most of signal detection theory is based on Gaussian assumptions so what happens if we have
some non-gaussian distribution like a heavy tail distribution but then we we assume that you're
doing signal detection theory with a plus approximation so you're basically trying to fit
a Gaussian to this non-gaussian thing so you're going to have some systematic biases
relative to the ideal observer so I think that that that could be done and I'm not aware of anyone
having done that yeah yeah well well one trick with mixture of Gaussians is that you can have a
kind of auxiliary variable setup where you condition on the mixture component so you
you sample the mixture assignments and then everything is Gaussian conditionally Gaussian
right so then you can kind of get around that but I don't know I don't know the answer then
specifically in the context of predictor and maybe someone else does yeah
yeah yeah yeah but but so so actually years ago I wrote a paper that derives
variational inference for mixture of Gaussians approximation so if you're interested that was
that was a machine learning paper and we didn't make any claims about the brain but that but so
you have it's not entirely trivial right so you have to you have to make you have to make some
additional approximations to to make that optimization problem tractable it's called
non-parametric variational inference if you're interested all right so why don't we keep going
okay great so let's go to move on to amortize inference which to me I think is kind of the
most exciting direction here so most of the approximations we've been talking about most
of the approximations studying in psychology and neuroscience are are memoryless what that means
is that if I ask you to solve like a sequence of inference problems your solution to one
inference problem will have no effect on your solution to the next inference problem you're
solving each of them de novo and this is obviously wasteful because there could be
significant shared structure across inferences and it's not being exploited so how can we
formalize sharing of structure across inferences so one way to think about this in the variational
framework is that we're gonna instead of modeling a separate variational posterior for each data set
we're going to model a mapping from data to to posterior so I'm now I'm now writing this explicitly
as q of s given d and now there's an additional thing here which is this expectation under p
tilde of the klde version so what is that so p tilde is the what we're calling the query distribution
over the data so this is the distribution over data that data sets you're going to be exposed to
that's some stream of data sets and the idea here is that you want to optimize this mapping
from data to posterior that minimizes the klde versions on average with respect to the query
distribution so the intuition here is that if you assuming that this variational approximation has
some limited capacity you want to concentrate the approximation resources on regions of the
data distribution that have high probability under the query distribution okay so an example is
shown here so I'm sure on the x axis the prior and the y axis is the the likelihood and so this
is a particular joint distribution and then and then that contour plot superimposed on it is the
query distribution and so what's shown in the middle is the result of running this of optimizing
that expected KL of solving that expected KL optimization problem where now you get a good
approximation of the posterior primarily in the regions of high query probability and outside that
region you basically say whatever I'm not going to care about those regions because I don't see
them very often right so and so so I'm concentrating regions resources on regions of high probability
and ignoring the regions of low probability and one consequence of this is that if the depending
on how you structure that mapping if there's some kind of convergence of that map so shared
parameters across the problem across the different posteriors then you're going to have a kind of
learning to infer effect which is that solving one inference problem is going to bleed into your
solution to the next inference problem so we can make this a little bit more concrete let's imagine
a particular neural network that's implementing this mapping from data to posterior so you get a
query the query includes the data set itself and the question you're being asked about the data
and then that gets passed through some hidden layers some computational bottleneck
and then and then outputs the output which is some sufficient statistics of the posterior so how
we choose to represent the approximate posterior so the implications the first one I already mentioned
people are going to learn to infer so even when you know all aspects of the generative model
so there's no learning about the distribution itself judgments are adapted to the query
distribution so we're going I'm going to talk in a moment about experiments where we think that
people aren't actually learning about the world they're learning about their internal inference
machinery so they're learning how to infer and then the other important implication is that
inference is not memoryless so early influence early inferences influence later ones
and people should learn to ignore their priors when they're uninformative there's a high signal to
noise ratio under the query distribution and but also to ignore their likelihoods when the signal
to noise ratio is low in other words they can adaptively pay attention to different parts
of the posterior calculation like the likelihood or the prior and this doesn't necessarily mean
that they have some explicit representation of the weighting of likelihood and prior it could all
happen in this kind of hidden layer of the neural network emergently so a little bit of background
so the idea of advertised inference is not actually new you know the classic example of this was the
Helmholtz machine that was developed in the 1990s by Peter Dan and Jeff Hinton and others
and so on the surface it looks just like a neural network right but the critical thing is that it's
simultaneously a generative model and a recognition model that is in a they use the term recognition
model to denote this amortized posterior so the model generates data from the sigmoid belief
network and then it also tries to optimize a neural network going the opposite direction
that does inference in that generative model and the same idea has been kind of updated and modernized
for applications to a bunch of different models most famously the variational auto encoder which
is a form of amortized inference so you can interpret the encoder as an approximate posterior
where the hidden layer represents samples from this belief distribution the approximate
belief distribution and then the decoder corresponds to the generative model and so these two things
work in tandem to optimize this amortized objective function so from a psychological
perspective we're interested in the question do people learn to infer can we show evidence
that people are sensitive to the the the query distribution in making their inferences so the
kind of the first step in this direction was I reanalyzed some data that I collected a few years
ago from for a different purpose where subjects were showing we're doing this very simple task
so they had to predict the next number that appears and the numbers were drawn from a Gaussian
distribution with some mean that they didn't know and importantly across blocks the mean of that of
each block was drawn from some other Gaussian distribution so it's a hierarchical Gaussian model
um and I had two different conditions one where the means across these blocks were were
very tightly clustered together and one where they were more dispersed so low dispersion high
dispersion um and um in this context the interesting question here is if subjects are
learning to infer that they should tend to overreact so they should learn to ignore their prior
in the high dispersion condition and do that more relative to the low dispersion condition
because essentially in the high dispersion condition the this this hierarchical prior
doesn't provide as much information about about the number prediction so they should basically
learn to ignore that prior relative to what an ideal observer would do so what I'm showing you here
is that that on the y-axis is the empirical data so how much do people update their predictions
from trial to trial um and then on the x-axis is the ideal observer so the optimal Bayesian
model and what you see is that there's this characteristic pattern where in the high dispersion
condition people seem to systematically overreact to the data so they seem to be updating more
than they should compared to than the low dispersion condition um if they were being ideal Bayesians
they would lie along this diagonal line um so that's one first clue that people might be learning
to infer but the part of the problem here is it's a little bit unclear whether people might have
also been learning about the generative model at the same time that they might be learning to infer
so we wanted a cleaner test of this so we went back to a kind of a classic workhorse of um of
experimental psychology where which are these ball and urn paradigms so the idea here is that
I'm going to draw one of these urns so I show you these two urns that have different compositions
of blue and red balls and I'm going to draw one of these urns with some probability those are the
base rates of the urns and I show this to you in this kind of wheel fortune thing um and then I'm
going to draw um one marble from one ball from the urn that I selected and show it to you so you
only get to see the the the color of the ball that I selected and you have to infer the posterior
probability of over the urns um so what what is what is the probability that uh that the urn
came from the left the ball came from the left or the right urn um so it's important to emphasize
that everything you need to know about the joint distribution is given to you here right so there's
no learning about the world the world is you have it all very explicit for you right um so um
but what we're one thing we're going to do here is from trial to trial we're going to give you a
whole bunch of these different uh problems and in some conditions we're going to vary hold the
likelihood constant across trials and vary the prior in other cases we're going to hold the
likely the prior constant and vary the likelihood so it looks like this um and the reason we're
doing this is we wanted to show that even though you have all the information available that you
need to be an ideal Bayesian um you're going to tend to ignore um the prior when the prior is not
variable and ignore the likelihood when the likelihood is not variable um and in fact that's
exactly what what um people do uh it shows that basically people uh learn to infer in the sense
that they can um uh they can pay more differential attention to the more informative source of
information based on their query distribution um and it turns out that this can can explain a
pretty broad range of phenomena um there are many documented deviations from from Bayesian
updating so um in general for these kinds of ball and urn problems you see that people consistently
update in the direction prescribed by Bayes rule but they don't they sometimes seem to update too
much and sometimes too little um and there's evidence for this actually in real financial
markets so um our claim here is that amortized inference can explain some of these deviations
and one one clue from the old psychology literature is that if you look at um people people's
responses relative to um ideal Bayesian response they seem to be better calibrated around odds
log odds of zero um and then once you get to more extreme log odds either in favor against
some hypothesis people seem to get systematically miscalibrated and there is an ecological uh
explanation for this in terms of the query distribution so if you look at those query
distributions um the um uh they tend to be centered around zero in other words the query
distribution is centered around exactly the place where people are best are most calibrated right
and so we don't think that's a coincidence uh we think that's because of learning to infer
um it turns out that you can also get these kinds of memory-based spillover effects in the
sub-additivity and super-additivity examples that I showed you before so we've we've done experiments
where we took the same basic paradigm that I showed you before where people are told about
some q-object like a table and then they have to um rate the probability that um some subset of other
objects is present in the image but what we do is we actually organize these trials into pairs
so they do the pairs of trials um and we're looking at sub-additive and super-additive effects on
the second trial as a function of unpacking in the first trial right so if people if if people
were memory-less then it wouldn't matter what you did on the first trial there would be no spillover
into the second trial um but we we predicted that um depending on whether you have an atypical or
typical unpacking in the in the first trial that's going to translate into a sub-additive or super
additive effect in the second trial and in fact that's exactly what we found um um and um in fact
you can take that one step further and show that um that it's sensitive to the similarity of q so
it's not just that there's arbitrary spillover from one trial to the next um it depends on the
similarity of q's between those those two inferences so you only are going to transfer information
