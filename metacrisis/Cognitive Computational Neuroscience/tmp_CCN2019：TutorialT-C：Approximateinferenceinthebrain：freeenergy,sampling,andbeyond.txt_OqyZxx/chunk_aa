All right. So let's dive right into it. Actually, let me say a few things. So I tried to make
sure to leave ample time for questions and discussion. So the number of slides is not
going to fill up four hours, God forbid. So I hope that you guys will interrupt me a lot.
And we can leave a block of time at the end of each half of this tutorial to just have
an open discussion about the topics. And I'm sure that there are people in here that know
about some of the topics that I'm going to cover better than I do. And so I have no ego.
You can interrupt me and tell me that I'm wrong. So I'm going to try to cover quite
a broad scope of approximate inference ideas. And really, in some ways, in some parts I'll
go into more depth, but some parts I'm going to cover more superficially just to give you
kind of a bird's eye view of what the different possibilities are. All right. So here's the
roadmap. I'm going to start with some basic principles of probability theory just to get
everybody up to speed. Probably a lot of you are already familiar with that. Then I'll quickly
raise the question, are people Bayesian at all? So to broach the question of approximate inference
in the brain, we have to ask, is there any inference at all to a first approximation? And
I'll talk about how there are some documented successes of Bayesian models of cognition,
actually quite a few. But on the other hand, there are many documented failures. And one
of the goals of this tutorial is to explain how some of those documented failures could
be explained as a consequence of approximate inference algorithms. And then I'm going to
talk about two broad families of approximate inference algorithms. In this first half I'll
talk about Monte Carlo algorithms. And then in the second half of the tutorial, I'll talk
about variational methods. And those are most familiar to neuroscientists in the form of
free energy methods or algorithms that stem from the free energy principle. And then I'm
going to talk about an extension or kind of a generalization of variational methods to
the setting of what's called amortized inference. And I'll explain what that means. But basically
it's the idea that you can learn a parameterized mapping from your data to your approximate
posterior. And that allows this parameterized mapping to basically learn how to infer. So
it's a kind of meta-learning for approximate inference. And then at the end I'm going to
talk about this question, are we making things too complicated? Basically you'll see that
in order to execute an approximate inference algorithm in a principled way in a neural circuit,
things have to be set up in a very specific way, right? So you have to organize, you have
to structure the neural circuit in a way that you can guarantee that it's actually implementing
the specific mathematical operations you need to do these specific approximate inference
algorithms. But maybe that's making things too complicated and we're putting too many
demands on the brain. Maybe it's actually a lot simpler. You could take generic neural
networks and you could specify conditions under which those generic neural networks
will approximate correct inference. And so approximate inference then arises emergently
from some more fundamental principles and we can talk about that. All right. So basic
probability, the main two rules of probability that you need to know are the product rule
and the sum rule. These are operations on joint probability distribution. So the product
rule says that you can factorize a joint distribution over two variables that's called
an A and B into the product of the conditional distribution and the marginal distribution.
And one reason this factorization is useful is because I like to think about it in terms
of a kind of a causal story. So one way to generate samples from this joint distribution
is to first sample from one of the marginals and then sample from the conditional distribution.
So it gives you a kind of recipe for sequentially sampling from the random variables to produce
a sample from the joint distribution. Then the other important rule is the sum rule which
tells you how to take the joint distribution and convert it into a marginal basically by
summing over the variables that you want to marginalize over. And I'll mostly be talking
about discrete variables just because it's easier to communicate. But if we're talking
about continuous variables, you can just replace the summation signs with integrals.
So most of what you need to know about probability, most of what you need to know about what I'm
going to talk about today comes from these principles. So let's just give a concrete
example. Let's imagine a joint distribution over two binary variables, rain or no rain,
and wet and not wet. So as you might expect, you're more likely to get wet when it rains,
but it's also possible to get wet when it doesn't rain. And it's also possible to not
get wet when it rains. So if we wanted to know the joint distribution over getting wet and
raining, you could factorize it into the probability of it raining and the conditional probability
of it being wet given that there's rain. So that's what I mean by a causal story that
you can factorize these joint distributions in a way that makes sense causally. Of course,
you can factorize it the other way around too. You can factorize it into the marginal
probability of getting wet times the conditional probability of rain given wet, and that might
be less causally intuitive for people even though mathematically it's equivalent. And
then you can also calculate the marginal probability of getting wet, which is just the probability
of getting wet given it rain times the probability of it raining plus the probability of wet given
no rain times the probability of no rain. So it's just basically summing over all the
different ways that you could get wet. Okay. Any questions so far? This is all pretty elementary.
Now, we can basically use those two rules. When you put them together, you get Bayes'
Rule, and that's really going to be the central topic of what I'm going to talk about today.
And I've relabeled the variables S and D to denote S being some latent variable, some
state of the world, if you will, and D being your data. So we can ask, what is the conditional
distribution, the posterior probability of the hidden state of the world given the data
you've observed? And Bayes' Rule says that that's going to be equal to the product of
the likelihood of the data under a hypothetical state times the prior probability of that
state. And then in the denominator, there's this normalizing constant. Okay. So that's
the posterior probability of hypothesis given data. And we're going to come back to why
computing this is hard in practice and why you need approximations. But before we get
there, let's just talk for a minute theoretically about why an idealized agent should be Bayes'
in at all. There are a bunch of classical arguments about why you should be Bayes' in.
And I'm not going to go into any of these in detail, but just so you're aware of them.
One argument is called the Dutch book argument. And essentially what it says is that if you
disobey the probability axioms, which entail that you're not going to be Bayesian, then
someone else can exploit you for their own gain. They can basically turn you into a money
pump. There's another decision-theoretic motivation for Bayes' Rule that's called the
complete class theorems that I show below. And roughly speaking, this says that if you
think about admissible decision rules, these are decision rules that aren't dominated by
some other rule. Basically almost all of the decision rules you would think of are Bayesian.
There are some classes of admissible decision rules that aren't Bayesian, but as a first
pass, we can make that statement. So what that says is that if you want a good decision
rule, you should use Bayesian decision theory. And then there's another motivation that's
based on a generalization of logic to probability, and that's known as Cox's theorem. And the
basic idea here is that if we assign scalar plausibilities to hypotheses that satisfy
a set of intuitive axioms, so basically they're real-valued and consistent with Boolean algebra,
then it turns out that the plausibilities have to be isomorphic to probabilities. So
we can derive these real-valued numbers that quantify the plausibility of some statement
in Boolean logic, and it turns out that if we want them to satisfy the intuitive axioms
of logic, then they have to be isomorphic to probability. And if you want to learn more
about any of these arguments, I highly recommend this book by E.T. Jaynes called Probability
Theory of the Logic of Science. But none of these arguments are going to be important
for what I want to talk about today. All right. So now let's talk about the empirical side,
which is our people, Bayesian. Some of you may be familiar with this famous paper from
Tom Griffiths and Josh Tenenbaum, where they ask people to make predictions about every
day temporal events. Like, suppose that I told you that some cake has been baking for
20 minutes, what do you think is the total time that it has to be baked for? And they
did this for a whole bunch of different things, like lengths of poems, movie grosses, movie
run times. Some of these are debatable to what extent they're actually intuitive, like
pharaohs. But the point here is that they could use ecological data on the prior probabilities
and combine that with a likelihood function, which they derived from first principles,
and show that basically people's judgments about these forecasts are basically very closely
aligned with Bayes' optimal predictions. And that motivated many other studies to look
at psychological prediction from a Bayesian perspective. And you'll see many studies with
titles like this, like motion illusions as optimal percepts, irrational analysis of the
selection task as optimal data selection. There's a huge number of studies that have
the word optimal in their title, and they're referring to applications of Bayesian inference
to cognition, going from visual perception all the way up to high level cognition.
But at the same time, I'm sure many of you are aware that there's another perspective,
which says that people are really bad at all sorts of things, including Bayes' rule, that
they violate the principles of decision theory and probability theory in many different ways.
And that's led to the view that people aren't actually following the rules of probability.
What they're doing is using a bunch of heuristics that can serve them well under certain circumstances,
but can also lead them systematically astray. So that's kind of the hardcore version of
that argument is that we shouldn't even try to match people to the Bayesian norm, because
they're doing something fundamentally different at a psychological level.
So what I'm going to talk about today is motivated by the challenge. Can we bridge this gap without
throwing out the whole apparatus of Bayesian inference, but still trying to explain why
people screw up in some systematic ways? And the basic argument here is that one reason
why people may look Bayesian in aggregate, but still make systematic errors is that basically
you can't be Bayesian. You can't be a perfect Bayesian for even moderately large problems,
the computational complexity is too large. So you have to make approximations. And classically,
if you look at papers on Bayesian inference and psychology, they kind of swept that part
under the regularity. They say like, all right, here's this generative model, and here's
the posterior. And then we do, you know, MCMC, blah, blah, blah. Don't worry about that.
It doesn't matter, because any possible algorithm for approximating the posterior will give
the same answer. But that's never going to explain the deviations from Bayes' rule. If
you run the computational algorithms that basically closely approximate the true posterior,
what we want to do here is ask whether we can explain deviations from Bayesian inference
in terms of the approximations that people use. And so we'll look at a bunch of different
possible approximations and think about the specific deviations that those predict. And
as we'll see, and hopefully maybe we can discuss this, sometimes it's not entirely
straightforward what predictions to make. And also there's this other scary possibility
that the space of approximations is so vast, there's not a clear way to discipline it.
And then throughout this tutorial, I'm going to talk about ways in which we can ground
these algorithms in biologically plausible neural circuits. And for some of these algorithms,
those ideas are better developed than others. Okay. So I've already said that people need
to use approximations because of computational interactability. So what exactly are people
optimizing? How do they choose a good approximation? Or if they choose a particular family of approximation,
how do they choose the precision with which they approximate the posterior? And that leads
us to an important concept that's sometimes called computational rationality or resource
rationality. And the idea is that because of finite cognitive resources, you're going
to choose an approximation algorithm that balances the cost of executing that approximate
inference algorithm and the benefits of higher precision, for example, by running it longer
or with more samples or however we choose to parameterize that. And this leads to the
idea that although you can possibly arbitrarily improve your performance with longer run times,
finite run times are going to be optimal given some cost on computation time. That could
be an opportunity cost or an energetic cost. And that's diagramed here. This is a diagram
that Eric Horvitz made. So the idea here is that the value of the result in some cost
to your world is increasing probably monotonically, but the cost of delaying your action to run
the computation is also increasing probably monotonically. And so the net value of action
is going to be non-monotonic. So that gives you kind of an intuition for why you'd want
to use approximation algorithms. And we'll come back to this framework for trying to make
some specific predictions about particular experimental paradigms.
Okay. So a lot has been said about representation and manipulation of probabilities in the brain.
And I'm not going to go exhaustively through all of that. What I want to do is focus on
neural inference implementations that are scalable in the sense that they can be applied
to very large, potentially very large problems. And I'll talk in a moment about some classical
ideas about inference in the brain, which don't appear to be scalable, or they might
be scalable, but you have to make some more elaborate assumptions. So for example, probabilistic
population codes is probably one of the best known ideas about representation of probability
distributions in the brain, but it can, at least in its classical implementation, it
runs into scalability issues. Okay. So here's what a probabilistic population
code looks like. The basic idea is quite simple. So there's some stimulus that's generated
by the world, and then that stimulus gets translated by the brain into some pattern of spikes.
That's represented by R. So there's some spike generating process. That could be some Poisson
distribution defined over some set of tuning curves. So I'm showing you an example here
for orientation where you have a bunch of these bell shaped, these orientation tuned
neurons, and then each of those neurons is going to generate spikes according to some
Poisson process. And then a downstream decoder can look at those spikes and try to read out
what the orientation value is as encoded by that set of spikes. And an important idea
in probabilistic population codes is that the gain of this population code is directly
related to the variance of the posterior. So when the gain is lower, which is shown on
the bottom there, so that this hill of activity is pushed down, then the decoded variable
is going to have a broader distribution. So in essence, you'll have more uncertainty
about that variable. And they argued that this was possible because variables like
contrast, which we know affect uncertainty, affect population codes at least in visual
cortex in this way. Now, it's important to keep in mind here that we're talking about
decoding a single scalar variable. What do we do when we want to encode lots of different
things? How do we encode them and how do we decode them? And in general, the number
of parameters that we would require to specify a multivariate distribution is going to scale
exponentially with the number of its variables. So we would need a vast number of neurons
to explain even a modestly large, a modestly high dimensional distribution. So that suggests
that this particular implementation would not be scalable. And there's also other constraints
when you want to use probabilistic population codes, depending on how you want to do decoding,
there are constraints on the characteristics of neural tuning curves and noise. So I think
it's still debatable to what extent you can use probabilistic population codes to do scalable
inference, but at least it suggests that naively implemented, they're not going to be scalable.
Okay. Are there any questions so far? Okay. So now we'll jump into approximate inference.
So I'm going to start with Monte Carlo methods, because I think they're quite easy to understand,
even if the process by which the samples themselves are generated could be rather complicated.
Okay. So the basic idea is the following. Suppose that I'm able to generate a bunch of samples
from the distribution of interest, the posterior. And that's what I'm showing you over there.
So the S sub k, the k indexes the samples that are generated from this distribution.
