what you're going to do is enumerate as many diseases as you can think of and evaluate whether
you, the probability of dying from each one of those diseases, but because you're resource limited,
you're not going to enumerate all possible ways of dying. And so you're only going to enumerate
a small number of them. And as a consequence, you're going to underestimate the marginal
probability. And if I unpack those, if I unpack that into a few examples, then I can alleviate
that underestimation of the marginal probability. It turns out that the opposite thing happens
when I unpack them into a typical example. So then you get super-additivity, where now
when I evaluate the sum of the probabilities that you apply to each of these sub-hypothesis,
that turns out to be less than the marginal probability. So why does this happen? Let me try
to give you some intuitions for this. So what I'm showing you here, the big circle is all the ways
that a person could die, and the little circle is all the ways that they can die by disease.
And so the goal here is to evaluate the area of that little circle as a proportion of the big
circle. So let's suppose that I seeded you with an unpacked example, like heart attack. So heart
attack would be typical, smallpox would be atypical. Now in a Metropolis Hastings algorithm,
I'm going to then generate some proposal distribution. So suppose that I proposed skydiving accident.
So if you generated a typical example, then you're more likely to prefer the typical example
compared to skydiving, because it has higher probability. But if I seeded you with the atypical
example, then you're more likely to, you're at least relatively more likely to accept the
skydiving accident proposal and move into this low probability region of the hypoxia space.
So the predictions here come from two things. One has to do with anchoring. So the initialization
of the Markov chain, you're going to be biased towards the initial state of the Markov chain
when you only generate a small number of samples. And there's this idea that if you're in a low
probability region of the state space, you're more likely to accept other low probability proposals.
And so this model can predict both sub-additivity and super-additivity, but only when the number
of samples is small. So we're running this Markov chain and looking at the relationship between
the marginal probability and the sum of these probabilities of the sub-hypotheses.
And so negative means that there's a super additive effect and positive values on the Y axis
indicates that there's a sub-additive effect. So we've tried to pursue this experimentally
in the following somewhat weird experimental paradigm where we ask people to make judgments
about hidden objects in a scene. So we'll tell people, suppose I'm looking at a table, what's
the probability of objects like, if I'll show you an example, a chair, computer, a curtain, or any other
object beginning with the letter C. So you have to evaluate the probability of this
disjunctive hypothesis, and we can manipulate how we unpack that disjunctive hypothesis.
And the reason we chose the scene inferences is that we can take advantages of a dataset
of object co-occurrence frequencies that Michelle Green collected and fit a probabilistic model
to this dataset, and that gives us basically the probabilistic model from which we can compute
the conditional distributions that we need to make predictions.
So we gave people a bunch of these different questions with different unpackings, and we fit
this MCMC algorithm with two free parameters. So one is the number of latent objects in the
scene, and the other is the number of samples. And we can show that, first of all, we can get
sub-additivity and super-additivity effects depending on whether the unpacking was typical
or atypical, and the model fits this quite well. You might think that 230 samples is somewhat large,
given what I was saying before about samples being costly, but on the other hand, I would
reiterate that we don't actually know how many, we don't know what the utility function is,
or the cost function over samples is, and so we don't know what is the optimal number of samples
ex ante. So in order to make some stronger claim about that, we would need to really understand
that cost function better. I'm going to skip this. Basically, the point of the second experiment was
just to show that using the same parameters, we can get predictions for finer-grained,
we can make finer-grained predictions for different kinds of unpackings.
So let's come back to this question of how many samples, and let's just start from the generic
assumption that sampling is costly. We don't know exactly how many samples people are going to take,
but we know that they should take fewer samples when we make the sampling more costly.
So for example, we can place people under time pressure, and we expect that that's going to
amplify sub-additivity and super-additivity, and that is indeed the case. So if you divide people
based on fast and slow response, people's response is based on whether they're fast or slow.
Under fast responses, you do see stronger sub-additivity and super-additivity effects,
and that's consistent with the idea that they're drawing fewer samples under time pressure
when they respond quickly. And a similar argument can be made about cognitive load. So when we put
people under cognitive load, so they have to do a secondary task while doing this task,
that should amplify the effects. In fact, actually, we found that this increased
super-additivity but didn't really have a substantial effect on sub-additivity, and it turns out that
these two effects turn out to be asymmetric as a function of the number of samples, and that
could potentially explain this asymmetry. So super-additivity is more sensitive to the number
of samples than sub-additivity. So you can apply the same logic to a range of other
biases that have been documented in the probabilistic reasoning literature. So I'll give you a few
examples here. So there's anchoring and adjustment, self-generation effect, the dud alternatives
effect, the weak evidence effect, and the crowd within. And all these can be explained in terms
of resource-limited sampling based on this Markov chain Monte Carlo algorithm.
So the basic intuition underlying super-additivity is that you essentially get stuck in low
probability regions of the hypothesis space, and this can also explain dud alternative and
weak evidence effects. So here's what the weak evidence effect is. So if I present positive
evidence in favor of a weak cause, then that will actually lead to the outcome being judged to be
lower probability. So for example, I ask, what is the probability that a person died of disease
given that her great aunt has diabetes? So this slightly increases the probability of the outcome,
but what also happens is that it causes the initialization of an atypical example, and this
produces an overall underestimation of the probability, and essentially this is a version
of super-additivity. Another interesting version of super-additivity is the dud alternative effect.
So this is where the judge probability of a focal outcome is higher when impossible
alternatives are presented. So for example, what is the probability that a person died
due to disease as opposed to a skydiving accident? And this is essentially super-additivity in the
complement space. So now P not A, so the probability of not dying due to disease is underestimated
because of initialization as an atypical example. So it's sort of the flip side of super-additivity.
You can also explain anchoring and adjustment phenomena. So the idea that there's a bias towards
prime hypotheses, and this is reduced when subjects take more time. So there are classic
experiments by Kahneman Tversky that showed, for example, if I tell you, if I ask you, what are
the last four digits of your social security number? And then I ask you a question like,
what year was Gandhi born? And it turns out that if your social security number is lower,
you're going to have estimates of Gandhi's birth that are lower. So it's a totally irrelevant
anchor, but if you think that it somehow infiltrates your hypothesis set, then it will pull estimates
down. And specifically because you're anchoring, you show this because of the autocorrelation
and the small number of samples, which means that you'll be sensitive to the initialization.
Another interesting observation about this is that you can reduce autocorrelation
by thinning the Markov chain. So if I introduce larger time gaps between subsequent measurements,
then I'm going to reduce autocorrelation, and that's going to reduce the error. So the estimation
error for a given number of samples is going to go down if I thin it more aggressively,
but of course, that means that I need to draw more samples. And there's some evidence for this
from Edvill and Hal Pashler who showed that you can get people's estimates to be more accurate
if you ask them the same question multiple times separated by longer gaps. And another
observation related to this is that, of course, there's much less autocorrelation between people
than within people. And so if you average answers between people, you're going to get a lower error
compared to if you average answers within a person because of this autocorrelation.
Okay, so to summarize this last part, we've seen before that you can explain low-level
perceptual phenomena, but you can also explain a lot of high-level probabilistic reasoning
phenomena using the same basic ideas about autocorrelation and a generation of a small
number of samples. And the small number of samples constrained derives again from
this idea of computational rationality. This one?
Yeah, so the question that you're asking, and what is the probability that you can see is
kind of, I mean, so if it's crazy in that way, right, it's more likely, it's actually more likely
that someone dies due to disease that's just out of here. But the question, is the new question
you're really trying to answer is the probability that there's a probability that there's a
hypothesis that you are considering skydiving, right? Because that's still in the hypothesis
space, right? You're still able to generate hypotheses outside of the disease region of that
space. Right, but the way this one comes to bring is as it's like, what is the probability that
a person died due to disease versus skydiving? And it's, yes, it's more likely that a person dies
due to disease, right? And so, as a result, like, that if you're going to get people who
saw the higher end of this, it's more likely that you have to die due to disease.
Right.
Well, so we're making the assumption here that potentially you could be generating samples
both in the focal space and in the complement space. So you could be evaluating both hypotheses
simultaneously.
Well, again, the question I'm trying to ask is, since I didn't have that question right,
instead of saying that you accidentally replaced that with something or a new part
answer, would you expect to answer whether, is the question that you're asking, like,
the answer to those two questions still the same?
No.
Because it's more likely that you will be dying due to the part answer that you decided.
But in this question, you're actually being, like,
maybe we can take this offline. Okay.
Any other questions before we talk about, go back to neural sampling?
Yeah.
So, sorry, I couldn't hear that. Can you say again?
Yes.
Yeah.
Yeah.
What are the two market processes?
What evidence is there that this should be the case?
Well, I mean, we can model angering effects by assuming that this prime initializes the
second Markov chain. I wouldn't even go so far as to say that there's a Markov chain for the
social security number, right? You have no uncertainty about your social security number,
right? That does seem a little bit outlandish, right? Why would people use this clearly irrelevant
number to initialize their Markov chain, right? And I think that that is a puzzle,
right? You'd think that people would be smarter than to, they'd have some kind of relevance
filter. And maybe they do, but there might be still some sort of subtler source of bias there.
And in fact, when I talk about amortized inference, one of the goals of modeling amortized inference
is to explain the origins of these anchoring effects in a sort of a more principled way.
Like right now, we're just making the simple assumption that I tell you something and you just
stick it into your sample set, but maybe there's something more subtle going on,
like maybe you're learning how to map from examples to an initialization or something like that.
Are there other questions? Yeah.
Yeah. Yeah, more or less. I mean, there are different choices we could make about how we
map from the samples to reports, right? So you could collect a bunch of samples
and then report some summary statistics of those samples, possibly you have some utility function
that you are optimizing with respect to that sample based approximation, or maybe you're even just
reporting, you know, samples as they come in one at a time. So those two are very different. One is
like you generate a hundred samples. Yeah. Like within a second you just pick. Yeah.
Well, there's some in between zone here, right? Because if you generate a small number of samples,
then it's going to be something in between saying the first thing that popped into your head and
you know, optimizing some utility function with respect to the sample set.
Well, I've tried to make the argument that at least as far as applications of the data I've
talked about, it's consistent with using a small number of samples, right? You get probability
matching specifically when you have a small number of samples, not when you have a large
number of samples. Because in general, if you have an accurate sample or you're going to
converge to the true posterior in the limit, and so you're not going to show any of these
biases at all. Now, another possibility is that you don't have an accurate sampler. You have,
you take a large number of samples from a bad sampler that doesn't have the right
asymptotic properties. And that is a conceivable hypothesis, but I haven't seen any,
I haven't seen any examples of that really investigated.
Yes, that's right. Or some, depending on what the nature of the problem is,
sometimes you could do things like take a running average of the samples,
and you don't need to keep them in memory. But that depends on the specifics of the problem.
Yeah?
Right.
Well, I should, so the question is, you know, are these, are those neural effects only going to
happen at equilibrium? And I mean, I don't, maybe someone here knows the answer to this,
but I would suspect, my intuition is that you wouldn't only see these effects at equilibrium.
So you would, even if the marker chain hasn't reached equilibrium, you would still see,
for example, differences between spontaneous and evoked activity.
I mean, I guess it depends on how far away from equilibrium we're talking about,
right? But all other things being equal, that's my intuition.
Yeah, that's a good question.
Yeah, that's, yeah, that's an interesting question. I don't know the answer to that.
Yeah.
Yeah, the question is, if we're, if we're taking into account the cost of sampling,
do we pre-compute the cost of sampling, I'm sorry, how many samples we're going to take,
or is it some, do we have some kind of any time algorithm that can determine when to
stop sampling? I think that's a super interesting question. I don't know of any
evidence that specifically answers that question. I think,
Yeah, yeah, right. So I think, so one thing that I think we can say is that there is some
evidence from Jess Hamrick and Falk Leader, where they make the argument that there's
some adaptive level of sampling. So the number of samples is not fixed. It depends on the
the cost-benefit ratio. But what I don't know is whether any evidence specifically
suggests that people are determining on the fly whether to stop sampling or determining
a fixed number of samples at the beginning. Okay, so we'll keep going. So let's go back
to neural dynamics of sampling. And now, now, so before we were talking about neural sampling in
a very generic way by just generating samples from the posterior, but now I want to talk about
neural, the specific dynamical properties of that sampling as instantiating an MCMC algorithm.
And actually, if you go back to the kinds of models that I was showing you before to model
vernacular rivalry, so those are technically Markov random fields, you can write down,
you can write down the dynamics of Gibbs sampling in a way that looks a lot like the dynamics of
neurons arranged in sort of a generic network. So a Markov random field can always be expressed
