and they do some computation
and they produce activity states.
Some of them are generative
and some of them are not.
One of the...
What you can do with each one of these networks
is you get this and regress them onto the brain
and you get a prediction.
Right now,
one of the neural networks
that gives us really accurate predictions,
reasonably accurate predictions
is Alex and it's a discriminative network
and it's not generative.
That's a hypothesis.
It could be true
or it could be false.
It seems like one of the things
that might be falsifiable here
is the assumption
that lives in the brain
because there are other computing things
that are not generative models
unless I'm just not thinking about Alex
as creatively as I need to.
I don't know if it's a question
of thinking creatively.
If you were a scruffy,
scruffies is not meant to be a derogatory term.
I think the majority of us are scruffies
but it certainly is easy
to reconceive those
deconvolutional networks we saw
in the last talk
in terms of an amortized
inversion of a generative model.
In fact, there'd be a very graceful
set of schemes that you could,
if you went to any
commercial AI company,
they could write down a variation auto encoder
and then amortize that
and you would end up with the sorts of schemes.
In a sense, and I love that presentation
because for me
what was being done
was essentially different forms
of generative models were being assumed
and they were being
for convenience amortized
using current
deep learning architectures
and being put forward
as working hypotheses
for a process theory
for broad scale functional anatomy,
hierarchical anatomy in the brain
and getting that right factorization
I think underneath
the shape and the form of the models
and at what point did they diverge
into sort of dual tasking
face versus object.
These are exactly the structural learning problems
that present themselves
when you don't know the form of the generative model
so I think it'd be quite easy
to, with the right Rosetta Stone
start to talk the language
that I would understand
to describe that very set of results
but you have to
understand that all of these
deep networks
are just universal function approximators
what are they approximating
the mappings that matter
what are the mappings that matter
they are mappings from data space
to beliefs
inferences they're just amortizing
or summarizing learning
how to infer
how to categorize, how to predict
and all of that should at the end of the day
be described as
a variation free engine minimization
Thank you in the interest of time
to the next session
I'm going to ask our
discussions if they're satisfied
and if they have any
closing statements
Satisfied?
I'm not satisfied yet
Will you ever be satisfied?
I'm waiting for something very specific
He wants his hug
They're going to hug. Thank you all for your patience
and your attendance and your great questions
