No, of course not, maybe I should be using this.
Yes, as the expected kale divergence between our policy
and something that is kind of like a probability distribution,
not quite, there's some constants that we're neglecting,
but they don't really matter.
What we do is we're going to find the U
that makes this as close to this as possible.
We call this, right?
Well, okay, we got it from this, you know,
sort of single state cost, it ends up looking something like this.
You can call that your policy, oopsie, right?
You can call that your policy prior, right?
This is how you should adjust your control signal.
You want your behavior to be like this,
to maximize your reward or to minimize cost.
This is the end point
of this kind of reward maximization calculation.
In contrast, this is the starting point
of a free energy framework.
You start with a policy prior,
sorry, I'm, Zach Pickhouse says I cannot use the word policy prior.
You start with a default policy,
which says how you would move from state to state
that you like, that is consistent with your goals.
But you've abstracted away reward entirely,
you just have this default policy.
So at the end of the day, what is Carl really,
yeah, we'll put it that, what is Carl really done?
My view on this is what he's done is he's sort of come up with
an equivalent, but slightly different definition
of what a decision-making organism is.
So here's the standard decision-making formulation, right?
There's going to be some data, it's going to come into your brain,
you're going to do some inferences,
it's going to be something how you represent the world
or represent your data, and then generate some action.
So what do you need to do in the standard decision-making formula?
You need to have a few things, right?
An organism becomes defined by its inference engine,
how it interprets data from the world,
it's defined by its reward function,
and implicitly the goal of maximizing reward.
Okay, this is the standard way to do it.
And then of course, what do you do with this information?
Well, just like you find a policy that maximizes your reward.
Alternatively, within the free energy formulation,
or framework, or whatever you want to call it,
an organism is not defined by reward,
but instead defined by its inference engine,
which is an approximate distribution,
applied to a generative model, this is an important caveat,
this is sort of a little more general in the sense
that there doesn't have to be a generative model
that informs the inference engine, but there can be.
And a policy, a default policy,
what you would do if you got to the cheese.
If you got the environment, your actions might be random,
you don't even know about the cheese,
if you do find the cheese, your actions become very clear,
and that sort of thing is incorporated
into this sort of default policy,
the probability of eating the cheese
given that you have cheese in front of you.
And then of course, there's the aesthetically pleasing feature
that if you take this approach,
beliefs and actions can be both selected
by minimizing a single objective function,
which is called free energy.
It's pretty.
So before I move on,
I want to pull the audience,
not electronically, that's really intimidating, just hands.
What makes more sense?
Like, reward or policy as a way of,
now they're equivalent, they're mathematically,
if you have a reward function,
you can find an equivalent policy that maximizes it,
and to some extent vice versa.
It's a little messy, but it can be done.
But which is better?
It's my view that it's really hard to say.
I kind of have a sneaking suspicion
that from an empirical perspective,
policy is a better thing to think about,
because policy is what we actually measure in the lab.
So here's, how do you measure reward?
Well, you don't measure directly, you infer it.
You put somebody in a room and you say,
do you want Snickers or Skittles?
What do you do?
You behave, you perform an action,
you say Snickers if you're a rational human being.
You do this a little by saying,
Skittles are 75% chances Snickers,
and maybe get a better estimate of their reward.
But at the end of the day,
what you're actually measuring is people's behavior.
It's the actions that people are taking
when placed in an unambiguous state.
That is a policy.
You always directly infer policies
in an experimental setting,
and never infer reward directly.
Reward is something you compute
from observations of someone's policy.
So from an empirical perspective,
this actually makes a little bit more sense,
and so I like it.
Just a couple more things I have to cover,
even though I think they're lame,
so there's the dark room problem.
I think that some of these criticisms
qualify as lazy or too...
Well, now I shouldn't have said that.
I take that back.
I think some of the criticisms that fall into this category
are too high level for me to understand.
So minimizing surprise is too easy.
There is a great way to minimize your sensory surprise.
You can go into a dark room,
you can cover your eyes,
you can simply stop participating in the world,
and that's all fine and dandy.
Now, the trivial response to this crazy criticism
is to say, yeah, that's not going to last very long.
If you close your eyes,
you will eventually die,
you will stop making predictions,
and everything will be surprising.
Now, there's a bit of a sticky wicked here
because if your actual prior says,
with great likelihood, you live in a dark room,
your predictions will be your prior predictions,
and they'll still be pretty good.
Now, we can play this game forever, turns out.
Oh, right.
So the response is,
well, total surprise minimization actually encourages life.
You wouldn't do this.
Okay, so I called this the pit cow rejoinder.
This comes from my good friend Zach,
who's seated over there
and is probably going to bury his head in a second.
I'm going to paraphrase a little conversation we had yesterday.
I'm going to change the words
to make me look good and Zach look bad.
But it goes something like this.
What if I brought nachos
into my dark room
where I'm blindfolded and my eyes are closed?
I'm not going to die.
You might need a few other things if you wish to survive,
but at the end of the day, yeah,
that's what seems to be like a perfectly acceptable solution
to the problem of minimizing surprise.
So, what's your problem, Zach?
What's wrong with this solution?
Zach would say,
can you just yell it?
No, Zach would say,
I would never do that.
And I would say, well, I mean,
it doesn't seem that bad bringing Xbox.
You know, that actually sounds a lot like my personal time.
That said,
other organisms find this acceptable.
What the hell's wrong with you, Zach?
Right?
So an example of this is mold.
Ah!
Perfectly timed.
Perfectly timed.
Done to pushing buttons.
This is a perfectly acceptable solution to some organisms.
That's one of them.
Believe it or not, that's actually a picture of mold on nachos.
So, what's Zach's problem?
The problem is that his default policy
is the policy of a human being and not of mold.
This becomes, this is a perfectly acceptable solution to this.
He's not mold.
I'm pretty sure.
And so, of course he's not going to pursue that as a solution,
but there are organisms that do because that's their policy.
Policy has become something that now defines an organism
instead of reward.
And that's really all I wanted to say.
So I hope that at this point you've come to a few conclusions
in a timely fashion.
I would say that reward functions and default policies
are equally valid ways of defining a decision-making organism.
The free energy principle is nice in the sense that it provides
a general and sensible mathematical framework
for description of inference and decision-making.
It's aesthetically pleasing.
And so on and so forth.
I hope you draw one third conclusion with this.
This is the worst debate ever.
Because I've just spent my 20 minutes
talking about what's right about the free energy principle
