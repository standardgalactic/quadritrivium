for the purposes of comparison, in which case
you would need them in some sense.
Yeah, I think that's true at two senses.
One is me trying to observe
your brain and trying to work out what generative model
you are implicitly using, sometimes
said to entailing. Your brain entails
a generative model to
make it clear that there's no commitment
to a physical instantiation of the generative model.
It's just that the message passing
is consistent with you using that
generative model.
So that would be me as a neuroscientist
looking at your brain and trying to infer
your generative model. I think from the point
of view of you actually using a generative model,
that's a slightly
which
simpler situation, which I'm sure
you can capture under your skeptical position.
It's an interesting way the question was framed.
Are we a generative model
or are we a process,
a policy
that can be described as if it
was optimizing a generative model defined by
its priors?
It's a subtle question
and it comes back to you.
And also irrelevant to the skeptic.
Yes, you'd have to worry about it.
But it is consistent with your focus on defining
phenotypes in terms of the things they
do, the policies.
So a thing
is a policy, it is not the generative model
that explains why you are doing that
planning as inference.
So there's a question
here on Jeff's default policy
from Roberto
Gulli at Columbia who says, what's the reasoning
behind choosing
default over priors? Who gave out to you
and how does it relate to the ability to potentially update
it if it's a default?
Right, so you do update
your default policy as soon as you receive
sensory information, which was the part that was left
out of those particular slides
which I've already apologized.
The reason why I don't use
the phrase policy prior
is because of the visceral reaction that
I received from that man.
That literally is the whole reason.
So I think that maybe
someone should hand Zachamike and ask
him
why the visceral reaction.
Carl, you put up
on the screen
Q of Pi
which is a prior
over policies.
And Jeff, you put up on the screen
P of S prime given S
which is a default dynamics.
Those are two different things.
And I don't know what the relationship is
between them, but they're different.
And the policy by itself
is the probability
of using actions
given a belief state.
Okay, so if I'd simply put a prior on A
sorry, not a prior on A
because I changed notation halfway through.
If I'd used Pi
and Q of Pi
instead of U
I would have been allowed to call it
a policy prior.
It's the meaning of what you were using.
The symbols we're talking about
the default dynamics of the states
which is not what you do.
It's not what you do when you're
just letting things go.
And that's equivalent to a reward.
Yes, so in practice
and not in my example
there is in fact a policy prior.
You gotta ask somebody else
for that one, I don't know.
No, because there is a Q of Pi.
Or sorry, there's a P of Pi.
And Pi is a probability distribution
distribution over distributions.
I could take a vote.
From my point of view
I just slipped in
planning as inference
just to demystify all this
epistemic value, instrumental value
in the connection between RL
and optimal Bayesian decision theory.
The expected free energy formalism
is just planning as inference.
That's all it is.
It incorporates both
the normal optimal Bayesian design
that
Andrea, as an literature physiologist
you are complying with those principles.
When you design your experiment
you are working out what data
should I go and sample, solicit from my lab world
in order to reduce my uncertainty
over the different hypotheses
that I am entertaining at the moment.
And I think the answer was that
these frameworks provide the good hypothesis space
that means you don't waste your time
acquiring data that doesn't resolve
the uncertainty. The Q of Pi is a posterior in fact.
So planning as inference
is just
how would you optimize your posterior beliefs
about what to do next?
So Pi is not a
mapping between
it's not a state action policy as in
RL or optimal control theory
where it's actually a mapping
between I'm in this state and I have to do that.
So the
active inference is much more general in deals with
sequential policy optimization where there are
multiple policies or sequences
of actions in play and they're denoted by Q
and as you say they have a posterior
which means they have to have a prior.
So I think the prior is
the softmax function
of the expected free energy.
So from my point of view you can have them
but possibly not in the sense
that you were arguing about.
So if
active inference give us something
on top of reinforcement learning
is there a class of
tasks that
would be solvable by one
framework or theory versus the other?
Yes, sequential policy optimization
in the case where
you need to
go and sample data that will change
your beliefs. So I notice you use the word
belief state. There's a lot of confusion about that.
So a belief state is actually a state of belief
that has always sufficient statistics
including the uncertainties
and of course to encode
completely would
require you to have an infinite dimensional
Markov decision process
to do that. So
problems in which making that move
resolves uncertainty
about what would happen if I then did that.
So do I go to a restaurant
or do I make
my own meal at home? Requires you first
to resolve
the uncertainty about whether you have the right
ingredients in your cupboard or in your fridge.
So your policy has to
incorporate your belief state
your priorities at the point
of acting
knowing that your beliefs will change as time goes on.
So there is no strict
RL scheme that can solve that problem.
Sometimes
then reformulating mathematically
RL could lead to
solutions of new paradigms. So reformulation
mathematical formulation has a value.
That's my comment.
So first of all
I will sheepishly admit to being
reviewer too, but in my defense
I don't think allied
Mr. Liding
is such an esoteric word.
But I had a
