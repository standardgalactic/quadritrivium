So all of these start
to shape the form and separation
of temporal scales in the hierarchical form
of the generative model at hand.
And of course, that's when all your process
theories come in. Are you going to use sampling
schemes, basing in Kelvin filtering, predictive
coding, belief propagation, variational
message passing, all of these are
process theories, which you really have
to commit to. And I think
at that point, then, the game
is basically what we all do
in our daily lives. There's
no great gift there, it's just having a framework
that highlights the connections
between them.
The other
point you were making, which I haven't thought
about before, was
inverting the
way that one defines the right
sort of behavior in terms
of the outcomes and
reward or cost functions
and just looking at that as a way
of writing down the kinds
of things that I do. There are
two equivalent ways. And I think
formally that probably you could articulate
that argument on the basis
of the complete class theorem. So that's probably provably
true.
Which means, as you intimated,
it's a question of personal choice
which you commit to or you write about
or the rhetoric that you use.
There was one point, though, which I
thought deserved a bit more attention.
You seem to have
I was going to use the word
elided, but I can't. But you certainly
have eluded
the epistemic part.
It is not the case that
expected free energy in its general
formalism can be reduced
to a reward function. You have
to have the information gain on top.
So what that means is that
the rat in the maze has to first
explore to become familiar with
the maze before it becomes exploitative
in an RL sense. So I would actually
challenge you that the
equality which you are selling, which I think is a beautiful
equality from the point of view of the complete class theorem,
is a slight misdirection
because the whole point of the free energy
minimizing expected free energy
is it dissolves the exploration,
exploitation, dilemma.
I was just being, oh, thank you.
Yeah, I was just being lazy.
Of course, I completely
left out observations. They were absent
from those equations for a reason because I feel like
they just get in the way. But that is certainly part of it
as well. Their information gain was implicit.
What I was trying to do
was just make a very specific link between reward
and default
policy.
But it would have been included had I been more complete.
That's all I have to say. I'm just saying I was being lazy.
High level.
High level, thank you.
That's it. I really have nothing else.
We're going to hug it out in a second.
He did assure us
this was going to end in a hug.
So I'm going to hold him to that at the end.
Well, maybe just to follow up on that,
some burning questions from the audience,
one of which asked explicitly, does or can
the free energy principle
capture non-ORL systems? And if not,
can it be considered unifying?
You can't see that here. It's coming on this laptop.
So
does it capture non-ORL systems?
Is that a way you'd phrase it?
What you just described.
Capture non-reinforcement.
Do you think it captures non-ORL systems?
I think it generalizes reinforcement learning.
So that's the one way of carving up
that sort of expected free energy
is in terms of expected reward,
which
we both wrote down as a
a lot probability or C
for cost function.
Perhaps I didn't, but you did.
So that would certainly
subsume all
the machinery, conceptual and
mathematical that you may want to bring to bear
if you're using RL.
Barry Pete, that's not the complete picture.
You need to
equip that
RL-like imperative
with an information game,
which is what I was referring to with the Lindley
experiment.
The imperatives are good experimental design.
In the
absence of any rewards, there is still
an imperative to go and get
good data that is good in the sense
of affording you information
and resolving uncertainty.
And I think that is exactly what you were talking about
in terms of the good surprise.
It's the resolution of uncertainty
afforded by
epistemic affordance, which mathematically
would be the thing that you'd
have to add to
an RL cost function in order
to get an expected free energy.
Did you mention
inverse RL?
No, you didn't.
I imagine that's another twist
on the sort of duality, which
but I think
to do that properly, you're going to have to put epistemic
back in
the resolution of uncertainty
as a fundamental part of the objective
function, sometimes known as salience,
sometimes known as novelty.
I call it
epistemic affordance, because I like that.
Giovanni Perluso told me to call it that.
So I do.
Good.
And so there is
a question from
Andre, but maybe is it following up on some of these?
So there's
questions on
progress for this field and in terms of
how do the models come about initially
and how do we go about
carefully driving process theories
from these assumptions or somehow inferring
the P and Q from data?
Is that where we're going next then?
Is that something the theory has helped us with
or were that narrow our investigations?
Let me see
the question here.
Yeah, so this is the, how do you
choose your P's and Q's?
And there is, does not, I
don't have a quick and ready
response. I can tell you what we do in practice,
which is we try to come up with sensible
ones.
For example,
we look at things like sparseness on natural
images or things like that.
And we just see how, at the
end of the day, we kind of just see how they work.
We see what sort of things would be predicted,
how is it consistent with behavior?
We often use very simple tasks when we talk
about things like the Bayesian brain.
So we have very simple P's and very simple
Q's that are often drawn,
that are often the actual P's
from our task.
That's often how a Bayesian brain
experiment goes. You as an experimenter
know what the P is and you're really testing
to see if the brain can figure that out too
through experience.
As for Q's, you know, if you're
optimal, then your Q is
just your posterior.
Looking for
causes of sub-optimality
is how, is one of the ways
in which we can identify
the limitations of optimal inference
and that's how we would then select our Q's.
Here's a simple experiment. So this is
