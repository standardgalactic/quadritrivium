Okay, let's start. Good afternoon everyone. It's my pleasure to welcome you to CCN's
Challenges and Controversies session where we will collectively discuss and
hopefully debate the free energy principle. So as you may know, the free
energy principle or active inference has been developed and applied in the
neuroscience community for the past 15 years, particularly in the field of
cognitive neuroscience and psychiatric research. And somewhat in parallel, it's
developed into a good model and robust debate amongst philosophers, particularly
there in the field of consciousness research and embodied cognition. And even
more recently, the framework has reached formally into physics where the maths
have been unfurled to reveal Schrodinger's equations, Maxwell's law of
electromagnetism and wave particle duality. And this is all afforded by
dynamical systems theory and Markov blankets, where spatiotemporal
recursions of a single idea, the drive to minimize free energy, speaks to
thinness across multiple scales from quantum phenomena to large active
particles like ourselves. So today in this session, I hope we can cover the core
concepts and mathematics and its implications for the study of the
human brain and cognition. And the format for the next hour and a half, we'll see
introductory remarks from our discussants for 15 minutes or 20 minutes a
piece, followed by hopefully a lively debate and Q&A from the audience. So
there'll be no prerogation here of our parliament. And to facilitate audience
participation, we'll have one roving mic. And I've also set up a poll
everywhere channel where you can submit your questions live now or during the
introductions or during the discussion via your smartphone or your laptops.
And so to do this, you go to pollev.com and enter a username which is me
Rosalind M092 or you can go directly to pollev.com forward slash Rosalind M092
and it's written on the boards here. And there you can submit your questions and
I'll pass them to discussants depending on how much time we have, we'll get
through as many as we can. If we have a lot of questions, I'll be updating the
live polls, so be patient if the polls fill up. I'll be putting a new poll on
the same address. And do add your name and institution to the end of the
question if you so wish. Okay, so to our discussants, they say that a good
boxing match is not based on opponents of different abilities, but on
matching opponents of different styles. And so today I think we'll have a very
good boxing match. Since in the red corner and in the blue corner, we have
two exceptional scientists who have in some ways sympathetic views and ideas,
but distinctive points of view. So in the red corner, we have Professor Carl
Friston. Carl is the scientific director of the Welcome Trust Center for
Human Ear Imaging at University College London. Carl is the originator of the
free energy principle and you might say has published extensively in the theory
from its development to applications across the fields I've mentioned. And in
the blue corner, we have Professor Jeff Beck. Jeff is professor of neurobiology
and biomedical engineering at Duke University and at Duke Institute of
Brain Sciences. And he is well known to this community for his seminal
contributions to probabilistic coding, Bayes in the brain, in sensory motor
domains and also in higher cognitive functions studying decision making
across species. So thank you both sincerely for agreeing to come and
participate. So from the rumble in the jungle to the thriller in Manila, we now
have the Battle of Bayes in Berlin. And so with that, for round one, let's
welcome Carl Friston.
Well, thank you for our invigorating introduction. It's clearly a great
pleasure to be here. And I'm flattered in a perverse sort of way, you know, to be
controversial. I don't think I've normally cast as controversial, so that's
good. And the last time I had to do something that was controversial was to
discuss different ideological positions under the banner of neats versus
scruffies. I don't know if this is a sort of dichotomy which people are aware of.
I liken this to a sketch, a sort of pre-Monte Python sketch in England where
class wars were the thing in the 1970s and 80s. And here we have John Cleese and
the two Ronnies talking about the class that they come from. And this tall
gentleman here is upper class, and he looks down upon this gentleman here who is
middle class, who in turn looks down upon Ronnie Corbett, who in this sketch is
very lower class. And I'm going to make the position, just to situate the
debate, I'm going to make the argument that this hierarchy of ideological high
churchness is parallel in the neats versus the scruffies and the mystics. From
our point of view, I'm going to associate the neats with physicists, people who are
in search of simple, unifying, neat explanations for everything. And I'm
going to contrast that with engineers who are the scruffies. And these are people
who are much more pragmatic and they want to know what works. So they will
adopt anything that works. And from our perspective, that could be 20th century
behaviorism, reinforcement learning, drift diffusion models, temple discounting. If
it works, use it. And then we have the mystics. Now I usually have psychologists
under that, but I thought there would be too many. You've got to keep the crowd
on the side, haven't you? So I've made them philosophers. We don't need to talk
about them any further. So in four slides, my brief is four slides through the
free energy principle. So here is the free energy principle in four slides. At its
simplest, it just says everything that can change will change to minimize
variational free energy. And that's sometimes articulated, particularly in
philosophy, as self-evidencing. Now free energy here, I've expressed in terms of
complexity minus accuracy. And that mathematically provides an upper bound on
a quantity known as log evidence or just evidence or marginal likelihood. That's
the probability of some outcomes or observations given a model or my
understanding of how those data were generated. So I'm going to focus on this
decomposition as a way of understanding the role of minimizing complexity whilst
providing an accurate explanation for the sensory observations at hand. And
motivate the importance of that just by unpacking the different kinds of
interpretation one could bring to bear to understand what this quantity is here.
The log probability of some outcomes at some time, possibly in the future, given
a model of how those outcomes were generated. And if you associate that
log probability with value, what we're basically saying is that minimizing free
energy is the same thing as minimizing a bound on value. And I should say if you
come from machine learning, this is exactly the same as the elbow or the
evidence lower bound using things like variational autoencoders. But from point
of view of a psychologist, it could also be value. It could be the sort of
expected reward that you will obtain pursuing a particular course of action.
So this can be regarded as a statement of the imperatives that underwrite
reinforcement learning, optimal control theory, and indeed in economics expected
utility theory. So that's nice. That covers a large bunch of people. There are
other groups of people who would think more in terms of information theory. So
the negative of this value is called self-information in information theory,
also called surprising. And it is the quantity that's upper bounded by the
variational free energy. And minimizing self-information or minimizing
surprising is essentially the same as maximizing the mutual information. And
from that, you can spin off the principles of minimum redundancy, the
principles of maximum efficiency, the information principle of Ralph Linska, and
indeed the free energy principle itself. That in turn is nice because the
expected, the time average of self-information is known as entropy. So
what we're saying is if everything has to minimize free energy, then it has to
basically minimize over time entropy. And of course that is the holy grail of
self-organization in physics, synergetics, and of course if you're a
physiologist, it's just homeostasis. It's just keeping outcomes within
physiological bounds, preventing the dispersion and the tendency to
disorder that one normally attributes to the second law of thermodynamics. But
there's a final interpretation here which might appeal to many of you in the
audience, and that's this drilling down on this marginal likelihood itself. So
if I interpret M as a model of, a gerontic model, of how these data were
generated, then this becomes model evidence, also known as integrated
evidence, which means that minimizing free energy is simply a statement of the
fact that or an imperative to maximize the evidence for one's models of the
world. And from that we can develop the Bayesian brain hypothesis, evidence
accumulation, and things like predictive coding. And that's largely the
perspective that I'm going to adopt on free energy minimization. There's another
move entailed by the free energy principle, which is probably less
familiar to many people. It's not just about inferring the best explanation for
the causes of your sensorium. It's also inferring what you're going to do. It's
also about what are the best data or sensory samples that with my active
palpation of the world, I can gather together in the aid of minimizing free
energy or maximizing the evidence for my own models. And we're appealing here to
very old ideas first elaborated by people like Lindley in terms of optimal
Bayesian design. So it's not about building good models of data. It's what are
the best data you can go and actively sample from your world in order to
afford the best kind of inferences about the causes in that world. And
that's formally can be articulated in terms of perception being in the game
of minimizing free energy. Basically the difference between your posterior beliefs
about hidden or latent states of the world at some time, possibly in the
future, in relation to your two posterior, which is the bound, and this is the log
evidence here. So policy selection in this expression here, where Pi denotes a
policy, is all about minimizing the free energy consequent upon taking a
particular policy or acting on the world in a particular way. And all we need to
do is to take the average free energy under our beliefs, our posterior
predictive beliefs, about the outcomes that would ensue if I did that. And if we
applied that to the free energy expression here, we're now conditioning on a
particular policy and we're just slightly relabeling and rearranging the
terms to express it in terms of a mixture of incentives and values. And
I've written them down here in terms, there are a number of ways of unpacking
this and I'll do that in a second, in terms of instrumental and epistemic
value. So what does that mean? Well, if I just focus on components of the terms
of the expected free energy and pretend that we had an agent or somebody who
had no particular prior preferences, no value, extrinsic value or reward
preferences, then we're left with these two terms here. And it turns out that
these are exactly, or this is exactly the quantity that's optimizing optimal
Bayesian design. In the visual neurosciences, it's known as Bayesian
surprise, popularized by people like Christoph Koch and Itty and Baldi.
Mathematically, that is exactly the same as a mutual information between
outcomes and their causes in the future. And of course, that is consistent with
the principle of maximum efficiency or minimum redundancy articulated by
people like Horace Barlow. Let's just now take two sorts of uncertainty
out of the mix and see what this expected free energy looks like. And I'm
hoping that you will start to recognize the terms. So the first sort of
uncertainty is basically ambiguity. I have no ambiguity about what's out
there beyond my sensations. I can see the states of the world directly. And
that just leaves us with these two terms here that basically score the
difference between what I will predict will happen and what I prefer will
happen. And that's known as a KL objective function for KL control in
optimal control theory. Or if you're an economist, it's risk sensitive
control to minimize the outcomes of behavior in terms of my expected
outcomes in relation to my goals or what I prefer to happen. Now I'm going to
take the final source of uncertainty, which is what will happen if I do that.
And if we take both sorts of uncertainty out, we're right back to
reinforcement learning expected utility theory. So the expected outcomes on my
prior preferences under what I think will happen. The point here is that
these are all internally consistent and equally valid ways of trying to write
down optimal behavior, but they are all special cases of one imperative, which
is to minimize the expected free energy or maximize the expected evidence for
your models of the world in the future. I won't go into this in detail.
Rosalind's is intimated you can extend this argument right down to quantum
physics or right up to evolution. And many people have started to do that by
putting a Bayesian gloss on in theoretical biology in terms of evolution.
Practically though, I just want to conclude with a comment upon the
expiry scope and what this actually means for people creating artifacts or
creating prediction machines to explain, say, fMRI responses in the
ventral stream. It is neat and simple. That's the whole point about being a
neat or a physicist. It's just basically saying what we have to do is to
provide accurate accounts of our world, of the sensed world, that is as
minimally complex as possible, thereby maximizing the evidence for me as a
model of my world. And this complexity term is extremely important. It's just
Occam's principle. And from that, we can motivate certain factorizations that
simplify a journal model of the world. You can get into functional specialization
as the neuroatomical or functional anatomy that belies or speaks to that
factorization. We can talk about redundancy minimization, the shape of
receptive fields and maximize the information gathered from the
environment, efficient coding and so on. We can also think about this from the
point of view of pragmatism and economics. What we're talking about,
because we're using an evidence bound, we're talking about bounded
rationality, slightly disingenuous use of the word bounded rationality from
the point of view of the economists, but it's still bounded rationality. It's
perfectly base optimal, but it's using an evidence bound. And that is the
definition of approximate base inference based upon free energy bounds. It
also very gracefully accommodates heuristics that emerge in many different
levels. So in economics, it would be Lorenzo like heuristics. These are just
priors, empirical priors that have inherited from your parents or neuro
development or some other form of Bayesian model selection. And if you're an
engineer or a physiologist, then via the Janinsky Equality and Landau's
principle, there's a direct relationship between your ability to minimize that
complexity cost and the algorithmic complexity that underwrites the
thermodynamic cost of erasing bits. So if you've got the free energy minimizing
solution, you have a bound upon the most efficient way to do it, whether you are
dealing with cerebral blood flow as a physiologist or whether you're actually
designing supercomputers and you want to minimize the amount of electricity
required to solve a particular computation. So that's about it. Neat and
simple. Evidence is equal to simplicity plus accuracy. As Alan Stein said,
everything should be made as simple as possible, but not simpler. So there you
have it. Thank you very much indeed.
Thank you, Carl. Next up, we have... Not sabotage.
So hopefully I see we're getting questions through. This is great. Keep
them coming. So just to remind you all, it's pollev.com forward slash
Rosalind M092 while Jeff is getting ready and his mic'd up to respond to Carl.
Should we start?
Sweet.
Thank you.
So, oh, thank you.
I have a word of advice for anyone who's ever invited to do something like this in the future.
Read the fine print on the invitation,
because I got this nice little email and it said,
would you like to debate Carl for a sin?
And I didn't really read the whole thing.
I just said to myself, that sounds like a ton of fun.
Like, what could be better?
But I was under the impression that we weren't talking about the free energy principle
as a unifying concept for cognitive computation,
but rather as a theory,
which has created a bit of a problem for me,
because as a unifying concept for cognitive computation,
Carl and I are, I think, complete agreement.
It's an incredibly boring debate, so I'm sorry.
Maybe we'll throw something,
but it's not going to get too much better than that,
because we are, as far as I can tell, largely in agreement on this question.
There's another question, the question I wished we were addressing,
which is, is free energy principle a unifying theory of cognitive computation?
And I would say, I have a slightly different answer, very different answer,
which is somewhere between no and not yet, or it needs more.
It's not a complete theory,
and I'll try to explain what I mean by that at the very end,
but because we're largely in agreement on a lot of things,
the first thing I'm going to spend a lot of time doing is addressing popular criticisms
of what I'm going to call the free energy framework instead of principle,
because I think that makes more sense,
and I'm not going to refer to it as a theory.
Neither did Carl, I noticed, I was tallying,
not a single use of the word theory.
Saw it coming, oh, sorry.
Actually, I'm going to put this down.
So I'm going to spend a lot of time just talking about
what are the common criticisms of the free energy framework,
and addressing them, because I don't think they're valid criticisms,
and I'm going to end with talking about why I don't think it's quite a theory,
or may not be a theory, but is nonetheless incredibly useful.
Okay, so here we go.
So here are the common criticisms of the free energy framework.
They largely center around,
they don't center around things like epistemic and intrinsic value.
Those words don't pop up very often.
Instead, they tend to take the form of something more like,
wait a minute, why this obsession about surprise?
After all, there are very desirable things that are very surprising.
So if I'm minimizing surprise, aren't I eliminating all of these desirable things,
like winning the lottery?
The second criticism comes with, oh, you know,
there's actually a really trivial way to minimize surprise.
It's known as the dark room problem.
What do you do if you want to minimize surprise?
Well, you just stop sensing the world.
Cover your eyes, go into a dark room.
And these criticisms have been addressed many times.
I think what was the name of the article that was the conversation
between the philosopher and the physicist?
Absolutely lovely.
I had trouble reading it, but I'm going to rephrase that
as I go through, and I think maybe plainer terms, I hope.
But here's a little exchange that made me chuckle.
This is one of Carl's responses in a paper that he wrote a few years back
addressing this sort of issue regarding the active inference part of the free energy principle.
So he did something that made me chuckle.
I thought it was hilarious and brilliant.
In his actual paper, he quoted a reviewer and then directly responded to him.
This was brilliant for two reasons.
So first of all, what better way to placate reviewer two,
especially a reviewer who'd use a word like elid,
than by actually acknowledging his use of that word publicly?
Completely brilliant.
Of course, my second response to seeing this was I had to look up what the word elid was.
It turned out it's actually pronounced elide.
And what it means is to conflate.
So 10 points for knowing what the word elid means,
and minus five for the missed opportunity for alliteration.
Catastrophically conflating is a lot better way to say that.
And here's Carl's response.
I'm not going to read Carl's response.
You can read it for yourself.
But I'm simply going to try to explain it with a few examples.
Okay, so here's how this goes.
Oh, sorry.
There was a...
Okay, okay.
Actually, let's...
Let me pause for a second.
Carl's response, I will not read it.
I will simply summarize it, was to say that optimal behavior...
There are two kinds of surprise that you have to worry about
when minimizing free energy and action selection
in an environment which you're also selecting your action.
One kind of surprise is about the sensory input.
And another kind of surprise is about your action.
Or it's about how surprising transitions between states become.
And that's what I'm going to try to elucidate.
So here's the basic problem, as a lot of people see it, in my opinion.
So this is a typical RL type problem.
Here's a mouse.
He's in a maze.
He's never been in this maze before.
He has a massive amount of state uncertainty
because he doesn't understand the configuration of the maze.
He doesn't know where the cheese is.
He doesn't know where the walls are.
There's a massive amount of uncertainty.
What does a mouse do?
He begins by exploring his environment.
So he zips around.
Eventually he finds the cheese.
And in an RL framework, he gets 10 points.
He got the cheese.
It's over.
You pick him up.
You put him in another spot in the maze.
And then what does the mouse do?
Well, he now knows something about his environment.
So he says, oh, I know something about this environment.
I now have an informed policy that will get me from point A to point B
so I can get more cheese and therefore more points.
There's something that's missing from this framework.
No, I'll just wave my hands.
There's a step that's missing from this simple RL perspective,
which is, yes, there's a reward maximization.
There doesn't have to be a reward in the free energy framework.
And maybe that's a problem.
But there's also something missing.
What does the mouse do when he gets to the cheese?
There's a serious question.
He eats the cheese, right?
Getting to the cheese, yes, it's rewarding.
And you can model that as reward,
but you can also model it as suddenly having a great deal of certainty
about his actions.
He's not surprised to find cheese.
He's sure he's going to eat it.
And I think that at a high level,
I'm hoping to get a nod and I'm not getting a nod.
Okay.
At a high level, I would say that that is the difference
between reinforcement learning and what I would call
like this free energy approach for active inference.
There's two notions of surprise that we're doing.
Sensory surprise and surprise in your action space
or surprise in your transition probability space.
I apologize for the math, but he started it.
Here's a simple Bayesian RL example.
I'm sure that many of you are familiar with this,
that elucidates the difference between
or how policy and reward are related.
So I'm going to frame this as a Bayesian reinforcement learning problem.
So initially you're a mouse, you're an amaze.
You have some, you know, in the absence of a control signal,
which will eventually be you, you have some sort of,
there are states that you would transition between
maybe just moving randomly through your environment,
but of course you could exert a control signal
and therefore you and therefore execute a particular policy.
What you'd like to do is you'd like to choose your control signal
so as to minimize, it's tempting, it's so tempting,
so as to minimize your cost or maximize your reward,
they're the same thing. How does that work?
Well, just for simplicity, because we want a simple example
with an explicit result, we're going to assume
that the cost of actions is simply measured
in the kale divergence between your, you know,
sort of go with the flow prior of just randomly moving through states
as opposed to, as opposed to exerting control,
as opposed to exerting the control signal, you.
And then of course there is some cost or reward
associated with getting the cheese,
or maybe just stagnating, starving, whatever.
And we're going to call that C of S.
So that's like the cost of achieving the state that is,
I suddenly, or the value of either not or achieving the state
in which you have cheese, for example.
So these are logs, we can do math, we move things around,
and what we end up with is being able to show
that our expected cost or expected,
negative of our expected reward,
can be expressed as a kale divergence
between our, oh, can you see my cursor?
No, of course not, maybe I should be using this.
Yes, as the expected kale divergence between our policy
and something that is kind of like a probability distribution,
not quite, there's some constants that we're neglecting,
but they don't really matter.
What we do is we're going to find the U
that makes this as close to this as possible.
We call this, right?
Well, okay, we got it from this, you know,
sort of single state cost, it ends up looking something like this.
You can call that your policy, oopsie, right?
You can call that your policy prior, right?
This is how you should adjust your control signal.
You want your behavior to be like this,
to maximize your reward or to minimize cost.
This is the end point
of this kind of reward maximization calculation.
In contrast, this is the starting point
of a free energy framework.
You start with a policy prior,
sorry, I'm, Zach Pickhouse says I cannot use the word policy prior.
You start with a default policy,
which says how you would move from state to state
that you like, that is consistent with your goals.
But you've abstracted away reward entirely,
you just have this default policy.
So at the end of the day, what is Carl really,
yeah, we'll put it that, what is Carl really done?
My view on this is what he's done is he's sort of come up with
an equivalent, but slightly different definition
of what a decision-making organism is.
So here's the standard decision-making formulation, right?
There's going to be some data, it's going to come into your brain,
you're going to do some inferences,
it's going to be something how you represent the world
or represent your data, and then generate some action.
So what do you need to do in the standard decision-making formula?
You need to have a few things, right?
An organism becomes defined by its inference engine,
how it interprets data from the world,
it's defined by its reward function,
and implicitly the goal of maximizing reward.
Okay, this is the standard way to do it.
And then of course, what do you do with this information?
Well, just like you find a policy that maximizes your reward.
Alternatively, within the free energy formulation,
or framework, or whatever you want to call it,
an organism is not defined by reward,
but instead defined by its inference engine,
which is an approximate distribution,
applied to a generative model, this is an important caveat,
this is sort of a little more general in the sense
that there doesn't have to be a generative model
that informs the inference engine, but there can be.
And a policy, a default policy,
what you would do if you got to the cheese.
If you got the environment, your actions might be random,
you don't even know about the cheese,
if you do find the cheese, your actions become very clear,
and that sort of thing is incorporated
into this sort of default policy,
the probability of eating the cheese
given that you have cheese in front of you.
And then of course, there's the aesthetically pleasing feature
that if you take this approach,
beliefs and actions can be both selected
by minimizing a single objective function,
which is called free energy.
It's pretty.
So before I move on,
I want to pull the audience,
not electronically, that's really intimidating, just hands.
What makes more sense?
Like, reward or policy as a way of,
now they're equivalent, they're mathematically,
if you have a reward function,
you can find an equivalent policy that maximizes it,
and to some extent vice versa.
It's a little messy, but it can be done.
But which is better?
It's my view that it's really hard to say.
I kind of have a sneaking suspicion
that from an empirical perspective,
policy is a better thing to think about,
because policy is what we actually measure in the lab.
So here's, how do you measure reward?
Well, you don't measure directly, you infer it.
You put somebody in a room and you say,
do you want Snickers or Skittles?
What do you do?
You behave, you perform an action,
you say Snickers if you're a rational human being.
You do this a little by saying,
Skittles are 75% chances Snickers,
and maybe get a better estimate of their reward.
But at the end of the day,
what you're actually measuring is people's behavior.
It's the actions that people are taking
when placed in an unambiguous state.
That is a policy.
You always directly infer policies
in an experimental setting,
and never infer reward directly.
Reward is something you compute
from observations of someone's policy.
So from an empirical perspective,
this actually makes a little bit more sense,
and so I like it.
Just a couple more things I have to cover,
even though I think they're lame,
so there's the dark room problem.
I think that some of these criticisms
qualify as lazy or too...
Well, now I shouldn't have said that.
I take that back.
I think some of the criticisms that fall into this category
are too high level for me to understand.
So minimizing surprise is too easy.
There is a great way to minimize your sensory surprise.
You can go into a dark room,
you can cover your eyes,
you can simply stop participating in the world,
and that's all fine and dandy.
Now, the trivial response to this crazy criticism
is to say, yeah, that's not going to last very long.
If you close your eyes,
you will eventually die,
you will stop making predictions,
and everything will be surprising.
Now, there's a bit of a sticky wicked here
because if your actual prior says,
with great likelihood, you live in a dark room,
your predictions will be your prior predictions,
and they'll still be pretty good.
Now, we can play this game forever, turns out.
Oh, right.
So the response is,
well, total surprise minimization actually encourages life.
You wouldn't do this.
Okay, so I called this the pit cow rejoinder.
This comes from my good friend Zach,
who's seated over there
and is probably going to bury his head in a second.
I'm going to paraphrase a little conversation we had yesterday.
I'm going to change the words
to make me look good and Zach look bad.
But it goes something like this.
What if I brought nachos
into my dark room
where I'm blindfolded and my eyes are closed?
I'm not going to die.
You might need a few other things if you wish to survive,
but at the end of the day, yeah,
that's what seems to be like a perfectly acceptable solution
to the problem of minimizing surprise.
So, what's your problem, Zach?
What's wrong with this solution?
Zach would say,
can you just yell it?
No, Zach would say,
I would never do that.
And I would say, well, I mean,
it doesn't seem that bad bringing Xbox.
You know, that actually sounds a lot like my personal time.
That said,
other organisms find this acceptable.
What the hell's wrong with you, Zach?
Right?
So an example of this is mold.
Ah!
Perfectly timed.
Perfectly timed.
Done to pushing buttons.
This is a perfectly acceptable solution to some organisms.
That's one of them.
Believe it or not, that's actually a picture of mold on nachos.
So, what's Zach's problem?
The problem is that his default policy
is the policy of a human being and not of mold.
This becomes, this is a perfectly acceptable solution to this.
He's not mold.
I'm pretty sure.
And so, of course he's not going to pursue that as a solution,
but there are organisms that do because that's their policy.
Policy has become something that now defines an organism
instead of reward.
And that's really all I wanted to say.
So I hope that at this point you've come to a few conclusions
in a timely fashion.
I would say that reward functions and default policies
are equally valid ways of defining a decision-making organism.
The free energy principle is nice in the sense that it provides
a general and sensible mathematical framework
for description of inference and decision-making.
It's aesthetically pleasing.
And so on and so forth.
I hope you draw one third conclusion with this.
This is the worst debate ever.
Because I've just spent my 20 minutes
talking about what's right about the free energy principle
or the free energy framework.
Well, it's because I haven't gotten to the last bit.
So is the free energy framework, I insist on calling it,
unifying?
And I would argue that yes it is.
It is precisely because it doesn't make a lot of assumptions.
And most of the assumptions that it does make,
you can kind of squeeze other things into.
So here are the fundamental assumptions
as I see it of the free energy framework.
The brain has a model that includes a policy.
It has a set of accessible approximations
for the purposes of performing inference and action selection,
which we call Q.
And inference and action selection occurs
simply by minimizing either KL, QP,
or free energy or whatever you want to call it.
As a result of it, it's some incredibly general theory, right?
Anything can be incorporated, not anything.
No, anything. I'm going to stick with anything.
Just about any description of a behaving entity
can be squeezed into the free energy form
by judicious choice of your model that you're inverting,
PM, which is what I called it,
and your choice of approximations Q.
Anything.
And I spent a lot of time trying to think of things
that you couldn't include in the free energy principle,
and I actually kind of, initially, I sort of thought,
oh, well, you know, sampling-based inference
is a unique kind of inference.
No, you can totally squeeze that into,
what's a sample after all, right?
What can you do with a sample?
So you have 50 samples. What can you do with them?
Well, I can evaluate free energy.
I can get a point estimate.
I can take an average, and to the extent that my sampling scheme
is a good one, on average free energy will be going down
using a sampling-based inference scheme.
Okay, so that gets in there, too,
so we kind of need to get rid of the what is not.
How about non-Basian schemes?
Totally, you can get non-Basian schemes.
Yes, this is very Basian by definition.
There is a Basian brain.
We do take into account uncertainty,
but maybe not always.
How can you fit a non-Basian scheme into this?
Well, again, by judicious choice of Q.
Right?
Typically, we like Q because Q can track variance
and uncertainty about the properties of the world
that we care about,
why not pick a Q that doesn't?
Why not pick a Q that just says,
oh, zero entropy Qs,
that's all unallowed.
Now you're basically doing map estimation,
and you're not behaving in a Basian way.
I've squeezed it in there anyway.
That's why I call it a framework and not a theory.
It's a great mathematical language.
It's a language that I hope we would all adopt
because I understand it.
It makes sense to me,
but it provides very little guidance
regarding the things that I actually care about.
There's no guidance within the...
I could be wrong.
There's no guidance within this framework
for choosing the generative models
that the brain is using.
There seems to be very little guidance for choosing Q.
There is some.
I will not say there's none,
but there's very little guidance for how you're going to choose
the kinds of approximations that the brain is using.
There's, as far as I can tell,
zero guidance for the optimization algorithm.
And so here's a few that people like.
And more critically,
to me, there's almost no guidance
in terms of the relationship between your beliefs,
in this case your Qs,
and neural activity.
It's simply agnostic.
And that's what makes it a framework
and not a theory.
I just wanted to quote
two peoples who said very smart things
and phrased things much better than I could.
The SG is Sam Gershwin,
and his quote is
the predictive failures of the free energy...
he said principle, I say framework,
are typically attributed to poor choice
of your P's and your Q's.
If you don't mind your P's and Q's,
you know, yes, thank you.
I got one laugh out of that one.
Thank you, Eric.
If you don't mind your P's and Q's,
you will make bad predictions,
and if you do, you can make good ones.
But that's not a property of free energy.
It's a property of your choice of models
and inference algorithms.
And Ralph Hefner is RH in this scenario.
Basically, we're at the same thing,
using much better words than I would have chosen.
That...
is that.
Thank you.
APPLAUSE
Thank you, Jeff. Let's leave those up.
Shall we?
And Dimitri,
I'm going to give you a microphone so you can wander around the crowd
and look for questions.
And we have some online.
And maybe we'll take a few minutes
for Carl to address
some of the points.
Yeah. This is fine.
Thank you.
That we just heard.
Whichever you'd like.
Yeah.
Thank you.
Well, I thought you'd use all the good words.
Usually in the right order.
I like that P and Q thing.
I've never thought of that before.
So for those of you who don't know,
that comes from the old days when they used to print
and used to have different blocks of P's and Q's
and apprentices used to get them inverted.
I always thought it was prepositions
and questions and the like.
It's actually literally getting the P's and Q's right.
So to take your last,
I think you've identified
a number of really key issues
and unfortunately we're going to be
in largely agreement here.
So
yeah, framework, I think,
is perfectly fine.
I use the word principle because
it's essentially the free energy principle.
It's just a variational principle of least action.
Formally, mathematically, that is what it is.
But if you want to use that
to provide a framework
within which to house
or consider various
process theories, I think that would be
a very appropriate use of the word
of a framework.
But I've slipped in there the distinction between a process
theory which may or may not be
consistent with the free energy principle
and the framework
and the principle itself.
And I think you're absolutely right that
the framework does not prescribe
a particular process theory
and inversion scheme.
And more crucially, it doesn't provide
what we all really want to know,
which is the nature and the form, the physiology
of the implicit generative models
to which you apply the variational machinery
under the framework or the principle.
And I think there's
implicit in some of your
statements, which I agree with entirely,
was at least a heuristic
recourse to the complete class
theorem or theorems
that say for any
pair of observed behaviors
and loss function, there is a base
optimal solution. So I think you're
absolutely right, the free energy principle framework
cannot be wrong.
It cannot be falsified, of course,
in the sense of a variational principle
of least action. It doesn't have
the attribute of being falsifiable.
But any process theory can.
Because to be a process theory, you'd have to
write down the form
of the generative model, what
sorts of quantities it entails. Are you including
inference about fast
hidden states? Are you including learning
about slow model parameters? Are you including
structure learning? Are you including evolution?
So all of these start
to shape the form and separation
of temporal scales in the hierarchical form
of the generative model at hand.
And of course, that's when all your process
theories come in. Are you going to use sampling
schemes, basing in Kelvin filtering, predictive
coding, belief propagation, variational
message passing, all of these are
process theories, which you really have
to commit to. And I think
at that point, then, the game
is basically what we all do
in our daily lives. There's
no great gift there, it's just having a framework
that highlights the connections
between them.
The other
point you were making, which I haven't thought
about before, was
inverting the
way that one defines the right
sort of behavior in terms
of the outcomes and
reward or cost functions
and just looking at that as a way
of writing down the kinds
of things that I do. There are
two equivalent ways. And I think
formally that probably you could articulate
that argument on the basis
of the complete class theorem. So that's probably provably
true.
Which means, as you intimated,
it's a question of personal choice
which you commit to or you write about
or the rhetoric that you use.
There was one point, though, which I
thought deserved a bit more attention.
You seem to have
I was going to use the word
elided, but I can't. But you certainly
have eluded
the epistemic part.
It is not the case that
expected free energy in its general
formalism can be reduced
to a reward function. You have
to have the information gain on top.
So what that means is that
the rat in the maze has to first
explore to become familiar with
the maze before it becomes exploitative
in an RL sense. So I would actually
challenge you that the
equality which you are selling, which I think is a beautiful
equality from the point of view of the complete class theorem,
is a slight misdirection
because the whole point of the free energy
minimizing expected free energy
is it dissolves the exploration,
exploitation, dilemma.
I was just being, oh, thank you.
Yeah, I was just being lazy.
Of course, I completely
left out observations. They were absent
from those equations for a reason because I feel like
they just get in the way. But that is certainly part of it
as well. Their information gain was implicit.
What I was trying to do
was just make a very specific link between reward
and default
policy.
But it would have been included had I been more complete.
That's all I have to say. I'm just saying I was being lazy.
High level.
High level, thank you.
That's it. I really have nothing else.
We're going to hug it out in a second.
He did assure us
this was going to end in a hug.
So I'm going to hold him to that at the end.
Well, maybe just to follow up on that,
some burning questions from the audience,
one of which asked explicitly, does or can
the free energy principle
capture non-ORL systems? And if not,
can it be considered unifying?
You can't see that here. It's coming on this laptop.
So
does it capture non-ORL systems?
Is that a way you'd phrase it?
What you just described.
Capture non-reinforcement.
Do you think it captures non-ORL systems?
I think it generalizes reinforcement learning.
So that's the one way of carving up
that sort of expected free energy
is in terms of expected reward,
which
we both wrote down as a
a lot probability or C
for cost function.
Perhaps I didn't, but you did.
So that would certainly
subsume all
the machinery, conceptual and
mathematical that you may want to bring to bear
if you're using RL.
Barry Pete, that's not the complete picture.
You need to
equip that
RL-like imperative
with an information game,
which is what I was referring to with the Lindley
experiment.
The imperatives are good experimental design.
In the
absence of any rewards, there is still
an imperative to go and get
good data that is good in the sense
of affording you information
and resolving uncertainty.
And I think that is exactly what you were talking about
in terms of the good surprise.
It's the resolution of uncertainty
afforded by
epistemic affordance, which mathematically
would be the thing that you'd
have to add to
an RL cost function in order
to get an expected free energy.
Did you mention
inverse RL?
No, you didn't.
I imagine that's another twist
on the sort of duality, which
but I think
to do that properly, you're going to have to put epistemic
back in
the resolution of uncertainty
as a fundamental part of the objective
function, sometimes known as salience,
sometimes known as novelty.
I call it
epistemic affordance, because I like that.
Giovanni Perluso told me to call it that.
So I do.
Good.
And so there is
a question from
Andre, but maybe is it following up on some of these?
So there's
questions on
progress for this field and in terms of
how do the models come about initially
and how do we go about
carefully driving process theories
from these assumptions or somehow inferring
the P and Q from data?
Is that where we're going next then?
Is that something the theory has helped us with
or were that narrow our investigations?
Let me see
the question here.
Yeah, so this is the, how do you
choose your P's and Q's?
And there is, does not, I
don't have a quick and ready
response. I can tell you what we do in practice,
which is we try to come up with sensible
ones.
For example,
we look at things like sparseness on natural
images or things like that.
And we just see how, at the
end of the day, we kind of just see how they work.
We see what sort of things would be predicted,
how is it consistent with behavior?
We often use very simple tasks when we talk
about things like the Bayesian brain.
So we have very simple P's and very simple
Q's that are often drawn,
that are often the actual P's
from our task.
That's often how a Bayesian brain
experiment goes. You as an experimenter
know what the P is and you're really testing
to see if the brain can figure that out too
through experience.
As for Q's, you know, if you're
optimal, then your Q is
just your posterior.
Looking for
causes of sub-optimality
is how, is one of the ways
in which we can identify
the limitations of optimal inference
and that's how we would then select our Q's.
Here's a simple experiment. So this is
my favorite experiment. We talk about this one
when we're trying to do auditory
visual localization. One possibility is
that you're doing it optimally, so you take into account
uncertainty on both of them. Another possibility
is that you don't.
That you do
means that on every trial
your Q is a mean and a variance
for both auditory localization and visual
localization. And if you're not going to do it
in a Bayesian way, if we see that it's not coming
from behavior, the explanation would be that you're
not tracking the variance on a trial-by-trial
base. You have some kind of fixed variance
and ultimately, I think that's how you address these questions.
You build tasks for which
you can do the inversion. You know what
the P is. You look at the kinds
of deviations people have, humans,
monkeys, whatever, mice, have
from optimal behavior and try to come up
with explanations for them by manipulating,
by coming up with restrictions on
Q.
That says nothing about the neural code,
so I hope that your question really wasn't about
that.
And then how did the models
arise in the first place? So do we just go back
in early development of genetic
priors or are there any
insights that we can take from this
principle for how they
arise? The generative models that you
showed were necessary for this
theory, but not necessarily
for our
RL-based policy selection.
So, I mean, the way
that you'd normally frame those
questions in terms of hierarchical
Bayes, so, yeah, certainly
you could look at different timescales in
terms of evolution, neurodevelopment,
experience-dependent learning,
attentional contours, synaptic efficacy
right down to fast fluctuations
in, you know, responses
to visual motion as
essentially being reflections
of a hierarchical
generative model where
each level of the hierarchy
affords empirical prize on
the level below. And I guess in some
sense could be looked at as doing
a form of Bayesian model selection.
So if you have a hierarchical model
questions like where did the prize
come from, again, just go away.
So they just come from the next
level up. And you normally end up
with evolution as you
intimated at the top level.
That's not a get-out. You're then compelled
to model evolution as a
process or natural selection
as a process of Bayesian model selection.
And it all comes then down to defining
the relationship of the Q with
the P. But the Q is now the phenotype
and the P is the environment.
And natural selection now becomes a process
of the environment selecting those
phenotypes that are fit for purpose
in terms of then being able to model
P, the environment, until you
get to humanity when we start
to design our own environments
and then you get into the world of eco-niche
construction and free energy minimization
at an ethological
level, which you can go there
if you want to.
So I'm not quite going to agree
with that because what you've done
is you've introduced yet a third P.
That's why I have a subscript for M.
You said that P now,
there's a third P, the P
is the way the world actually works.
Which is, in my view,
I mean, how many scientific
anti-realists are there out there?
That's inaccessible.
It's not a thing. The only thing that you've got
is what you can store, whether you're
in your brain, what you can compute. That's either what's
in your brain or what's in your genes.
There is no P.
There's only a piece of M.
You're coming out as a radical skeptist.
Absolutely.
In front of everybody.
Oh, yes. I've done it before.
Every once in a while someone says,
what's the most controversial position you've ever taken?
I'm not sure you guys exist.
I place no metaphysical
significance on the latent variables of my models.
So...
I promise not to use philosophical
terms, and I just violated that. I apologize.
So, I'll be
the structural realist then,
if that helps me.
So, the position that I
would be forced to take, which probably
is not what I'd want to, if I was
not in a controversy,
is that there is something real out there.
And it is the P.
And the Q,
and the P that the Q is predicated on,
the sort of implicit genetic model,
which I agree entirely is not the real P.
It's not the genetic process,
is
constrained by the real P
out there.
But philosophically, it's a really
interesting position, because the
whole sort of
inferring from within your
the skull, when you just have
access to data, and everything,
everything that you do
is
based upon an inference about what's out there
that you'll never, ever actually directly connect
with, really does force you into
a skeptical position
philosophically. The problem
is that the free energy really
does depend upon the
goodness of those P's and Q's
in the head, in relation
to something that's generating
those sensory observations. So, you sort of
force back into a realist
position, just to motivate
the machinery. So, philosophically, it's
very uncomfortable for me, but you are clearly
comfortable being a skeptical, taking
a skeptical position. So, that's
very nice.
That's a nice thing about being a skeptic.
It doesn't matter.
There's a really neat question
here, and then we'll go to Andre, which is
will theories develop
differently in the context of the
free energy framework, if this
isn't a theory?
No.
They'll go on with that.
No, I'm happy with that answer.
Just no.
Okay, sorry.
Mike and soapbox.
It's a
language. It's a way of talking about
theories of brain function, in
my view. It's a very useful one.
But again, we can shoehorn, because
of the freedom to choose P's or P's and Q's,
we can shoehorn just about
any computational framework into it.
And so, no, it would not develop
differently, other than the fact that
people might start talking about reward
a little differently. That's a possibility.
I think that that's
talking about reward and default policy
is being exchangeable might be a good
thing. It might be helpful in some sense.
Again, it's certainly the case that we measure
policies and we infer rewards, and maybe
there'd be some benefit to that. I don't
know.
So, no was maybe a little strong.
I'm just wondering, I mean, in a
sense you
intimated several ways that you could
use the framework
just to elaborate
a whole bunch of process theories
predicated on different kinds of generative
models that then you could go to
as a neuroscientist, you could go to
the empirical data.
One thing that's hit me in the past few years
is a fundamental distinction between
generative models based upon discrete
state spaces, like Markov decision
processes and hidden Markov
models and the like, and those
built around the assumption
you're dealing with continuous states
like Bayesian filters, predictive coding
and the like.
And just knowing that there are different
sorts of generative models that carve
in this way or that way, I think can put
a lot of very useful constraints
on the different
hypotheses
or process theories that you could then say,
well, does it look as though the brain's doing this kind
of message passing or that kind of message
passing or both.
And then we can get into the argument
about the neural code, what messages
are actually being passed and how
is it biophysically represented.
So I guess I'm
agreeing with you that the
specification, although the free energy
principle has nothing to say about it, when you
start to commit to a process theory
and you start to commit to the
generative model that will be apt for
this experimental paradigm, you do
actually narrow the hypothesis space about
the biophysics that will be doing
the
inference
and the kinds of
behaviors and neurophysiology
that you would see from that.
And then you just base
a model selection in the normal way to a
geutica.
Andre, do you want to give us your
question? Yeah, thanks for your presentation.
So I think in both presentations
we focused a lot on the needs
and also on the
mystics.
And are physiologists mystics
actually also?
That's not
my full question.
I'll just say that we are.
So we have the physicists
and the physiologists, psychologists
and so on on the other side.
But in between the scruffies, we haven't
discussed that very much and of course
a substantial part of the conference
are people from engineering and that are
trying to use some of these
principles to
make better models.
So I was wondering if the either the free
energy principle or theory
has informed
that engineering very practical
approach and if so, how?
And the other question
because we're in an interdisciplinary
setting here, it seems like
we are in a perfect setting to explore
the confluence
of those three groups together.
And I was wondering what you think
we can actually learn from each other
and what the interactions
should be between
the scruffies, the needs and
the mystics.
I'll say something.
So I barely remember
what those three things are. So scruffies were
engineers, is that right? Needs were physicists.
What was the third one?
Mystics. Mystics were
philosophers and then
where were we putting the physiologists?
I was going to say with the mystics.
With the mystics.
Yeah.
I don't know
how the free energy framework
particularly can bring these groups together
or explicate their differences, but I do
see a general way forward that is related to
like the Bayesian brain hypothesis
which is that
the role of the engineers
is to enhance
it's terrible, is to enhance
our computational space of models
to give us more P's for which we can find Q's
and actually do inference. So this is like saying
we can use artificial intelligence to help us figure
out maybe how real intelligence works.
Physiologists
obviously provide the constraints on what
biology can do which also tells us
something about behavioral, you know, the Q's
that are accessible, I think to some extent.
And philosophers
...
Oh, I'm sure there's one here.
Are there any philosophers here?
Philosophers can tell us what the right words
to use are.
That was pretty dismissive.
Do you want to go for that one too?
Also, someone
has asked, could Karl minimize my free energy
by speaking a bit louder?
Ah.
Sorry.
Right.
This is very enjoyable, by the way. Keep it coming.
We're trying to get to them.
I'd be interested
in your answer to be quite honest.
I mean, you do electrophysiology.
You are well versed in active inference
and some of the maths.
So when you've got your electrophysiology
cap on, do you
regard yourself as high church
theorist or
are you a mystic?
I would have actually put you with the scruffiness.
I think electrophysiologists need
to have a formal understanding
of the system
that they are trying to characterize.
And they are in the game, I think,
of testing the process theories that we've been talking about
doing the real heavy lifting.
But they need
to be able to
appeal to some theoretical framework
in order to
provide a mathematical rigor to their analyses
into their conclusions.
My experience is that they choose
what works.
So that would bring them
into the scruffy's camp.
And you're hearing the dialogue
here as to why it's good
to be a neat
or perhaps you're not.
What do you think?
Yeah, I like being
with the scruffies
doing something very practical.
But of course it has to be informed
that we have an infinite
space of possible experiments.
So we need to basically
use a theory to inform
what experiments we will do.
And even if a theory is conceptual,
only conceptual,
you still need to have something
to ground yourself in as a physiologist.
I was also wondering though
just in terms of the pure computational space,
people from AI and so on,
has there been any progress
in using free energy principles
in that space?
And if so, could you talk about them a little bit?
It's not my field,
but my limited knowledge of it
is it's making more
headway as one might imagine
in things like neuro robotics.
So we're talking about the construction of real-world artifacts.
So on the purely
software side, deep learning
is the oil tanker that is moving
forward and that's what everybody uses.
To be controversial,
I think that's exactly in the wrong
direction.
And it's obvious why.
Well, sorry, it should be
obvious why I say that, not why it's in the wrong direction.
It's all
about simplicity. It's always
about carefully choosing
the smallest amount of data
in the algorithmically
minimally complex way
to get to the same
reduction of uncertainty
or increase in evidence.
That means the challenge is using
small amounts of data
very carefully selected
to solve the problem.
That's not what current
sort of industry
artificial intelligence
is interested in. It's going in exactly
the other direction exploiting the availability
of very large amounts
of data labels,
exploiting leveraging Moore's law
and actually
celebrating the computational complexity.
For me, that's obscene
because the
computational complexity is part
of the variation free energy. It is
fundamental of this variation principle
of least action.
So I haven't seen much movement in that
and indeed we have been told by the good
and great in charge of the money in this area
that some bright-eyed
youngster like you or somebody like me
is not going to change the direction of this
oil tanker. That's where it's going.
But in your robotics, certainly
in the industrial scale
where you actually have to make real things
actively engage and interact with the world
I see active inference making a much more
simple, elemental and fundamental
difference. Just putting
servers and reflexes. Inference
is panning
on top of existing
robotics starting to think more
about sort of interpersonal
exchanges with robotics and the like.
I see that's probably
industry-wise making a difference
purely engineering perspective.
Just on the physics, I have one question
here from Ralph Hafner at the University of Rochester
who's
asking, does the free energy
principle also apply to purely physical
systems?
How is it different from Hamilton's
principle of least action and if so
what does it add? And if it does
not apply to purely physical systems, wouldn't
that make it falsifiable after all?
And just to mention there's been at least
10 or 15 people asking whether the theory
is falsifiable.
This is all you.
That is clearly a question for the physicists.
I'll do the physicists one, but the falsifiable
one, that's yours.
You'd religiously not use the word scheme.
You'd be very careful in the use of the word
frameworks.
I think you should go first though.
Sorry, that's my part.
The free energy principle,
the way the story was told here was
concise, almost
at the point of being tongue in cheek about
what the underlying equations are,
what it would look like from the point of view
of biological sciences, but the back
story is
a version of Hamilton's principle of least
action, not actually Hamilton's principle of least action,
but it is a variational principle of least
action.
It certainly
applies to physical systems, in fact we go a little
bit further, it would say that physical systems
inherit from the
same variational principles
that give rise to the
free energy principle.
The idea is you've got this
variational principles
based upon random
dynamical systems.
When you have random fluctuations
that dominate those, you get quantum physics.
When you deal with ensembles
you get statistical thermodynamics
or more precisely stochastic thermodynamics.
When you take
random fluctuations out of the game
you get classical mechanics and then you get
Lagrangian Newtonian mechanics
and Hamiltonians
come to the fore.
When you put
the Markov blanket, which we haven't spoken about,
but if we
frame this
self-organization in terms of
inference, you then get the Bayesian mechanics,
which is the free energy principle.
It doesn't explain
physics, but it inherits from exactly the same
maths that quantum physics
and other physics explain.
Again, it's not
falsified, but it's not there to be falsified.
It's not a theory that requires evidence.
It's just a mathematical truism.
Clear.
I'll answer your question.
I have one for Jeff here.
Someone has asked, following up,
which I think is an interesting question
which you've touched on, on
Rammstadt et al's 2019 work,
are generative models better conceived
as encoded in the brain
and enacted by agent and
environment?
Where does the model
live?
If it has a subscript
M, it's in your brain.
I think that's my short
answer.
It is part of the definition of
your computational energy, is the
set of available generative models,
and then the other thing that's important
is the restrictions on the set of
posterior distributions that you're capable of
of generating and representing.
So, brain.
Of course, as an extreme skeptic,
there is nothing outside of that.
So, I think that's why I have
my answer that I'm happy with.
I'd agree.
It's the free energy
gradients that drive the dynamics.
You need to define
the generative model to define the gradient,
but it's the gradient itself that does the heavy lifting.
Therefore, strictly speaking,
there is no generative model sub M.
It's just that which would be necessary
to explain the gradient flows and the dynamics.
He's frowning.
We've found a point of controversy.
Sorry, it sounds like you're saying
you don't even have a generative model
as long as you have the gradients.
I think from the point of view of a physicist,
that's probably right, yes, but if you wanted to interpret...
What if I don't want to do gradient descent?
I like coordinate descent
personally, and so
those gradients are a little different.
I think even when you move
to a coordinate descent,
so I don't mean to say that you can
write down a scheme that will optimize
something or self-organize
your particular attracting set of states
in the absence of a generative model.
I'm just saying that the generative model
is a mathematical construct.
You will only find it expressed
in the machinery during the message passing
the coordinate descent or the gradient descent
in the brain itself.
So you'll see reflections, a bit like
cortical hierarchies in a hierarchical generative model,
the factorization into where and what
in terms of mean-filled
approximation of latent causes
of sensory input.
But mathematically,
the generative model
itself does not exist as a physical object.
Is that still true when you're doing model comparison
inside the brain? Because you have multiple generative models
that are currently in sync? Because it seems to me
in that situation, you do need to evaluate
the probability of the data
given each of your models
for the purposes of comparison, in which case
you would need them in some sense.
Yeah, I think that's true at two senses.
One is me trying to observe
your brain and trying to work out what generative model
you are implicitly using, sometimes
said to entailing. Your brain entails
a generative model to
make it clear that there's no commitment
to a physical instantiation of the generative model.
It's just that the message passing
is consistent with you using that
generative model.
So that would be me as a neuroscientist
looking at your brain and trying to infer
your generative model. I think from the point
of view of you actually using a generative model,
that's a slightly
which
simpler situation, which I'm sure
you can capture under your skeptical position.
It's an interesting way the question was framed.
Are we a generative model
or are we a process,
a policy
that can be described as if it
was optimizing a generative model defined by
its priors?
It's a subtle question
and it comes back to you.
And also irrelevant to the skeptic.
Yes, you'd have to worry about it.
But it is consistent with your focus on defining
phenotypes in terms of the things they
do, the policies.
So a thing
is a policy, it is not the generative model
that explains why you are doing that
planning as inference.
So there's a question
here on Jeff's default policy
from Roberto
Gulli at Columbia who says, what's the reasoning
behind choosing
default over priors? Who gave out to you
and how does it relate to the ability to potentially update
it if it's a default?
Right, so you do update
your default policy as soon as you receive
sensory information, which was the part that was left
out of those particular slides
which I've already apologized.
The reason why I don't use
the phrase policy prior
is because of the visceral reaction that
I received from that man.
That literally is the whole reason.
So I think that maybe
someone should hand Zachamike and ask
him
why the visceral reaction.
Carl, you put up
on the screen
Q of Pi
which is a prior
over policies.
And Jeff, you put up on the screen
P of S prime given S
which is a default dynamics.
Those are two different things.
And I don't know what the relationship is
between them, but they're different.
And the policy by itself
is the probability
of using actions
given a belief state.
Okay, so if I'd simply put a prior on A
sorry, not a prior on A
because I changed notation halfway through.
If I'd used Pi
and Q of Pi
instead of U
I would have been allowed to call it
a policy prior.
It's the meaning of what you were using.
The symbols we're talking about
the default dynamics of the states
which is not what you do.
It's not what you do when you're
just letting things go.
And that's equivalent to a reward.
Yes, so in practice
and not in my example
there is in fact a policy prior.
You gotta ask somebody else
for that one, I don't know.
No, because there is a Q of Pi.
Or sorry, there's a P of Pi.
And Pi is a probability distribution
distribution over distributions.
I could take a vote.
From my point of view
I just slipped in
planning as inference
just to demystify all this
epistemic value, instrumental value
in the connection between RL
and optimal Bayesian decision theory.
The expected free energy formalism
is just planning as inference.
That's all it is.
It incorporates both
the normal optimal Bayesian design
that
Andrea, as an literature physiologist
you are complying with those principles.
When you design your experiment
you are working out what data
should I go and sample, solicit from my lab world
in order to reduce my uncertainty
over the different hypotheses
that I am entertaining at the moment.
And I think the answer was that
these frameworks provide the good hypothesis space
that means you don't waste your time
acquiring data that doesn't resolve
the uncertainty. The Q of Pi is a posterior in fact.
So planning as inference
is just
how would you optimize your posterior beliefs
about what to do next?
So Pi is not a
mapping between
it's not a state action policy as in
RL or optimal control theory
where it's actually a mapping
between I'm in this state and I have to do that.
So the
active inference is much more general in deals with
sequential policy optimization where there are
multiple policies or sequences
of actions in play and they're denoted by Q
and as you say they have a posterior
which means they have to have a prior.
So I think the prior is
the softmax function
of the expected free energy.
So from my point of view you can have them
but possibly not in the sense
that you were arguing about.
So if
active inference give us something
on top of reinforcement learning
is there a class of
tasks that
would be solvable by one
framework or theory versus the other?
Yes, sequential policy optimization
in the case where
you need to
go and sample data that will change
your beliefs. So I notice you use the word
belief state. There's a lot of confusion about that.
So a belief state is actually a state of belief
that has always sufficient statistics
including the uncertainties
and of course to encode
completely would
require you to have an infinite dimensional
Markov decision process
to do that. So
problems in which making that move
resolves uncertainty
about what would happen if I then did that.
So do I go to a restaurant
or do I make
my own meal at home? Requires you first
to resolve
the uncertainty about whether you have the right
ingredients in your cupboard or in your fridge.
So your policy has to
incorporate your belief state
your priorities at the point
of acting
knowing that your beliefs will change as time goes on.
So there is no strict
RL scheme that can solve that problem.
Sometimes
then reformulating mathematically
RL could lead to
solutions of new paradigms. So reformulation
mathematical formulation has a value.
That's my comment.
So first of all
I will sheepishly admit to being
reviewer too, but in my defense
I don't think allied
Mr. Liding
is such an esoteric word.
But I had a
real question which goes back to
falsifiability, which I think a lot of people
are interested in here and
let me just make sure I understood Jeff's
proposed experimental
strategy. So you're thinking about
defining some generative model
imposing it on an experimental subject
so that you
know what the optimal policy
is, the optimal posterior is, and then looking
for deviations and thinking and trying
to examine to what extent you could explain those
deviations in terms of some restriction
on the posterior. Is that correct?
Yes.
So that strikes me as
not being a strategy for falsification because
assuming
that
the way in which you pick, you're
looking for restrictions such that
when you minimize
free energy, that restriction
will satisfy the minimal free energy.
That's
maybe implicit in the way that you've set it up
but maybe not. It seemed to me that what you'd
really want to be doing is
impose the generative model
or at least separately elicit people's
assumptions about the generative model and
try to then characterize their prior
beliefs in terms of some restriction
on the variational family
before they've seen data
and then ask after they've seen data
would the
posterior within that restricted family
reduce free energy?
Note that it doesn't have to minimize free energy
so that kind of short circuits
the question about the algorithms because
even if you just take a gradient step
in that direction, you're at least reducing it
but if the free energy ever went
up, if people ever chose
an approximate posterior that increased
free energy, that would be
I think decisive falsification of the
free energy framework conditional
on having pinned down all the other pieces.
Well, perhaps not. Not enough samples
is a reason why free energy
can blip up.
Well, it depends
what you mean when you're talking about samples. So there could be
a sample based variational family
but then that would be part
of your specification of the variational family
that you do in the first part of the experiment
or I think what you're saying is
maybe that people are approximating
the expectation
with a bunch of samples and they didn't take enough
samples. Is that what you're talking about?
In my experience
there are two reasons why free energy goes up
it's usually because
that's the hardest thing to code up
is the evaluation of free energy
for me personally. So in other words
it's a bug. The other cause
is
because I'm doing something that's
I'm using some kind of stochastic method
that's the only thing that ever
causes it to go up in my experience
but then again, how would we
evaluate that behaviorally? I don't know
I just want to clarify
what you're actually falsifying
in the silly example I gave
is that the brain is optimally Bayesian
on this task.
That's already false, right?
Yes, but well, depends on the task
I could probably
I guarantee you that there are some
paths out there in general.
There's plenty of examples where the brain is not optimally
Bayesian. That's right.
Those are the interesting examples
because that's where we can start
nailing down the question. Okay, the falsifiable
thing is, is the brain optimally Bayesian? No.
Now we need an explanation. There are a variety
of different explanations, many of which
would take two forms, at least
within this framework, right? Wrong P
wrong Q
or severely limited Q
and we have to explore
those. We can do model comparison to see which one
is right completely within
this framework in terms of right as in
best fit to behavior
and that's the best
you can do. So the funny thing
about type of falsifiability is that
to a large extent it's not
like this is not a falsifiable
theory. Most theories of interest
complicated theories are not really falsifiable
in my opinion.
It's really, we would love to
live in the, I'm just going to finish
it, it's okay. Trust me, I'm just going to
interrupt you until I'm done.
I would love to
live in like a nice world of falsifiable
theories but at this point we don't have
any, right? We have descriptions of behavior
some are better than others and we use model
comparison to say this is right, this is wrong.
Very interesting that happens a lot of times when we're looking at human
behavior is that there isn't a single
model that fits everybody.
It's
incredibly rare. People use
a variety of different strategies to solve
exactly the same problem, often
with very minimal differences in reward
rates. You know,
I'll talk about specifics later, it's not
really important now.
So where does that leave us?
That leave us with the best description of behavior
and again this is now the skeptic talking one more time
last time, I promise.
Oh
crap, I skits down on what I was going to say.
Variety
of ways to do that. No, that was
okay, yes.
No, okay.
At the end of the day
science, in my opinion
is about prediction
and data compression
and not about truth.
It never bothers me
at all when
a hypothesis is not
falsifiable. I
really only care about those two things
and prediction is really the only thing I care about.
But I think the point still stands.
And if I'm marginalizing, I mean that's the automatic occupancy
effect of like doing Bayesian inference. Yeah, you got a bunch
of models that are way over parameterized,
they're total, they're not really useful, so what do you do
with them? Low posterior probability, they're gone.
You're doing great prediction because
you're focused on the models that actually work.
But my point was
I guess less about falsification and more just about
is there some distinctive prediction
specifically of free energy
that is
independent of the particular choice of P and Q
and the core prediction
has to be that free energy has to go down.
Right? And so
if it goes down...
If it doesn't, it's because
you're using the wrong Q.
That's the sense in which it's not falsifiable.
That's it.
I think it's still on the board.
That SG is you.
Paraphrase slightly.
SG is what?
Yes, yes.
I always say SG is you.
Good.
Thanks.
More on falsifiability.
Because I...
If I can leave this debate
and not spend the next several years testing
hypothesis that cannot be true
or maybe it's trivially true,
I would like to avoid it.
Just to bring it back
to some of the work that we've seen today
the scruffies
have given us over the past several years
a bunch of neural networks.
They take images
and they do some computation
and they produce activity states.
Some of them are generative
and some of them are not.
One of the...
What you can do with each one of these networks
is you get this and regress them onto the brain
and you get a prediction.
Right now,
one of the neural networks
that gives us really accurate predictions,
reasonably accurate predictions
is Alex and it's a discriminative network
and it's not generative.
That's a hypothesis.
It could be true
or it could be false.
It seems like one of the things
that might be falsifiable here
is the assumption
that lives in the brain
because there are other computing things
that are not generative models
unless I'm just not thinking about Alex
as creatively as I need to.
I don't know if it's a question
of thinking creatively.
If you were a scruffy,
scruffies is not meant to be a derogatory term.
I think the majority of us are scruffies
but it certainly is easy
to reconceive those
deconvolutional networks we saw
in the last talk
in terms of an amortized
inversion of a generative model.
In fact, there'd be a very graceful
set of schemes that you could,
if you went to any
commercial AI company,
they could write down a variation auto encoder
and then amortize that
and you would end up with the sorts of schemes.
In a sense, and I love that presentation
because for me
what was being done
was essentially different forms
of generative models were being assumed
and they were being
for convenience amortized
using current
deep learning architectures
and being put forward
as working hypotheses
for a process theory
for broad scale functional anatomy,
hierarchical anatomy in the brain
and getting that right factorization
I think underneath
the shape and the form of the models
and at what point did they diverge
into sort of dual tasking
face versus object.
These are exactly the structural learning problems
that present themselves
when you don't know the form of the generative model
so I think it'd be quite easy
to, with the right Rosetta Stone
start to talk the language
that I would understand
to describe that very set of results
but you have to
understand that all of these
deep networks
are just universal function approximators
what are they approximating
the mappings that matter
what are the mappings that matter
they are mappings from data space
to beliefs
inferences they're just amortizing
or summarizing learning
how to infer
how to categorize, how to predict
and all of that should at the end of the day
be described as
a variation free engine minimization
Thank you in the interest of time
to the next session
I'm going to ask our
discussions if they're satisfied
and if they have any
closing statements
Satisfied?
I'm not satisfied yet
Will you ever be satisfied?
I'm waiting for something very specific
He wants his hug
They're going to hug. Thank you all for your patience
and your attendance and your great questions
