engineer or a physiologist, then via the Janinsky Equality and Landau's
principle, there's a direct relationship between your ability to minimize that
complexity cost and the algorithmic complexity that underwrites the
thermodynamic cost of erasing bits. So if you've got the free energy minimizing
solution, you have a bound upon the most efficient way to do it, whether you are
dealing with cerebral blood flow as a physiologist or whether you're actually
designing supercomputers and you want to minimize the amount of electricity
required to solve a particular computation. So that's about it. Neat and
simple. Evidence is equal to simplicity plus accuracy. As Alan Stein said,
everything should be made as simple as possible, but not simpler. So there you
have it. Thank you very much indeed.
Thank you, Carl. Next up, we have... Not sabotage.
So hopefully I see we're getting questions through. This is great. Keep
them coming. So just to remind you all, it's pollev.com forward slash
Rosalind M092 while Jeff is getting ready and his mic'd up to respond to Carl.
Should we start?
Sweet.
Thank you.
So, oh, thank you.
I have a word of advice for anyone who's ever invited to do something like this in the future.
Read the fine print on the invitation,
because I got this nice little email and it said,
would you like to debate Carl for a sin?
And I didn't really read the whole thing.
I just said to myself, that sounds like a ton of fun.
Like, what could be better?
But I was under the impression that we weren't talking about the free energy principle
as a unifying concept for cognitive computation,
but rather as a theory,
which has created a bit of a problem for me,
because as a unifying concept for cognitive computation,
Carl and I are, I think, complete agreement.
It's an incredibly boring debate, so I'm sorry.
Maybe we'll throw something,
but it's not going to get too much better than that,
because we are, as far as I can tell, largely in agreement on this question.
There's another question, the question I wished we were addressing,
which is, is free energy principle a unifying theory of cognitive computation?
And I would say, I have a slightly different answer, very different answer,
which is somewhere between no and not yet, or it needs more.
It's not a complete theory,
and I'll try to explain what I mean by that at the very end,
but because we're largely in agreement on a lot of things,
the first thing I'm going to spend a lot of time doing is addressing popular criticisms
of what I'm going to call the free energy framework instead of principle,
because I think that makes more sense,
and I'm not going to refer to it as a theory.
Neither did Carl, I noticed, I was tallying,
not a single use of the word theory.
Saw it coming, oh, sorry.
Actually, I'm going to put this down.
So I'm going to spend a lot of time just talking about
what are the common criticisms of the free energy framework,
and addressing them, because I don't think they're valid criticisms,
and I'm going to end with talking about why I don't think it's quite a theory,
or may not be a theory, but is nonetheless incredibly useful.
Okay, so here we go.
So here are the common criticisms of the free energy framework.
They largely center around,
they don't center around things like epistemic and intrinsic value.
Those words don't pop up very often.
Instead, they tend to take the form of something more like,
wait a minute, why this obsession about surprise?
After all, there are very desirable things that are very surprising.
So if I'm minimizing surprise, aren't I eliminating all of these desirable things,
like winning the lottery?
The second criticism comes with, oh, you know,
there's actually a really trivial way to minimize surprise.
It's known as the dark room problem.
What do you do if you want to minimize surprise?
Well, you just stop sensing the world.
Cover your eyes, go into a dark room.
And these criticisms have been addressed many times.
I think what was the name of the article that was the conversation
between the philosopher and the physicist?
Absolutely lovely.
I had trouble reading it, but I'm going to rephrase that
as I go through, and I think maybe plainer terms, I hope.
But here's a little exchange that made me chuckle.
This is one of Carl's responses in a paper that he wrote a few years back
addressing this sort of issue regarding the active inference part of the free energy principle.
So he did something that made me chuckle.
I thought it was hilarious and brilliant.
In his actual paper, he quoted a reviewer and then directly responded to him.
This was brilliant for two reasons.
So first of all, what better way to placate reviewer two,
especially a reviewer who'd use a word like elid,
than by actually acknowledging his use of that word publicly?
Completely brilliant.
Of course, my second response to seeing this was I had to look up what the word elid was.
It turned out it's actually pronounced elide.
And what it means is to conflate.
So 10 points for knowing what the word elid means,
and minus five for the missed opportunity for alliteration.
Catastrophically conflating is a lot better way to say that.
And here's Carl's response.
I'm not going to read Carl's response.
You can read it for yourself.
But I'm simply going to try to explain it with a few examples.
Okay, so here's how this goes.
Oh, sorry.
There was a...
Okay, okay.
Actually, let's...
Let me pause for a second.
Carl's response, I will not read it.
I will simply summarize it, was to say that optimal behavior...
There are two kinds of surprise that you have to worry about
when minimizing free energy and action selection
in an environment which you're also selecting your action.
One kind of surprise is about the sensory input.
And another kind of surprise is about your action.
Or it's about how surprising transitions between states become.
And that's what I'm going to try to elucidate.
So here's the basic problem, as a lot of people see it, in my opinion.
So this is a typical RL type problem.
Here's a mouse.
He's in a maze.
He's never been in this maze before.
He has a massive amount of state uncertainty
because he doesn't understand the configuration of the maze.
He doesn't know where the cheese is.
He doesn't know where the walls are.
There's a massive amount of uncertainty.
What does a mouse do?
He begins by exploring his environment.
So he zips around.
Eventually he finds the cheese.
And in an RL framework, he gets 10 points.
He got the cheese.
It's over.
You pick him up.
You put him in another spot in the maze.
And then what does the mouse do?
Well, he now knows something about his environment.
So he says, oh, I know something about this environment.
I now have an informed policy that will get me from point A to point B
so I can get more cheese and therefore more points.
There's something that's missing from this framework.
No, I'll just wave my hands.
There's a step that's missing from this simple RL perspective,
which is, yes, there's a reward maximization.
There doesn't have to be a reward in the free energy framework.
And maybe that's a problem.
But there's also something missing.
What does the mouse do when he gets to the cheese?
There's a serious question.
He eats the cheese, right?
Getting to the cheese, yes, it's rewarding.
And you can model that as reward,
but you can also model it as suddenly having a great deal of certainty
about his actions.
He's not surprised to find cheese.
He's sure he's going to eat it.
And I think that at a high level,
I'm hoping to get a nod and I'm not getting a nod.
Okay.
At a high level, I would say that that is the difference
between reinforcement learning and what I would call
like this free energy approach for active inference.
There's two notions of surprise that we're doing.
Sensory surprise and surprise in your action space
or surprise in your transition probability space.
I apologize for the math, but he started it.
Here's a simple Bayesian RL example.
I'm sure that many of you are familiar with this,
that elucidates the difference between
or how policy and reward are related.
So I'm going to frame this as a Bayesian reinforcement learning problem.
So initially you're a mouse, you're an amaze.
You have some, you know, in the absence of a control signal,
which will eventually be you, you have some sort of,
there are states that you would transition between
maybe just moving randomly through your environment,
but of course you could exert a control signal
and therefore you and therefore execute a particular policy.
What you'd like to do is you'd like to choose your control signal
so as to minimize, it's tempting, it's so tempting,
so as to minimize your cost or maximize your reward,
they're the same thing. How does that work?
Well, just for simplicity, because we want a simple example
with an explicit result, we're going to assume
that the cost of actions is simply measured
in the kale divergence between your, you know,
sort of go with the flow prior of just randomly moving through states
as opposed to, as opposed to exerting control,
as opposed to exerting the control signal, you.
And then of course there is some cost or reward
associated with getting the cheese,
or maybe just stagnating, starving, whatever.
And we're going to call that C of S.
So that's like the cost of achieving the state that is,
I suddenly, or the value of either not or achieving the state
in which you have cheese, for example.
So these are logs, we can do math, we move things around,
and what we end up with is being able to show
that our expected cost or expected,
negative of our expected reward,
can be expressed as a kale divergence
between our, oh, can you see my cursor?
