Thank you so much for joining us today.
My name is David J. I'm the head of mobilization at the Center for Humane Technology and I'm
really excited to invite you to a very special for us edition of today's podcast club. For those
of you joining us for the first time, the podcast club, you can think of it sort of like a book club
for the podcast, your undivided attention, where we have more in-depth conversations with guests,
take questions from you all in the audience, and then invite you to connect with one another
around the really deep ideas that are getting talked about here at the Center for Humane
Technology, because ultimately the way things change isn't just us articulating powerful ideas,
it's you all building trust, building relationships, and building the power to change the world.
So with that, I'm going to start us off the way that we normally do with one minute of silence,
even though our bodies are here on this shared call. A lot of us are holding big complicated
things in our world and I found that taking a minute to ground ourselves before we fill
up with the really rich ideas we're going to be talking about here will yield a lot of benefits.
So you will take one minute, you'll hear a sound coming from my mic when we're done,
and then we will dive into our conversation.
Okay, so I am extremely excited to introduce our guest, Daniel Smokneburger.
I guess Daniel Smokneburger and Tristan Harris will be in conversation today,
continuing the great conversation you've had with the podcast. I'm also extremely excited to
introduce Stephanie Lep, the producer of Your Undivided Attention, and a first time kind of
guest in our podcast club. Stephanie, you have been going so deep into the really rich ideas
being explored today. We knew that you were the perfect person to be facilitating this discussion.
So I'm going to be bowing out and leaving the conversation in your very capable hands.
Oh, Stephanie, you're on mute.
Ah, thank you, Tristan. Thank you, David. I was just saying I'm loving these emojis.
I have no idea where they're coming from, but they're very sweet.
All right. Well, hello, Your Undivided Attention podcast club.
Yeah, I am honored to join the Center for Humane Technology and honored to have as my inaugural
episode feature two brilliant people who I also can call friends. Tristan is an old friend,
and Daniel, I think I can call you a new friend. And so here is how our time together is going
to unfold. So I'm going to briefly introduce Daniel and then briefly introduce the topic
that Daniel and Tristan will be talking about and then ask them some questions about that
topic. And then we will open it up to all of you first with some Q&A. So please throw your
questions into the chat as we go so we can get them up on the screen in the Q&A. And then we'll
have some breakout groups for further more intimate discussion. And then we will come back together
and close. And so with that, welcome, Daniel. Thank you, Stephanie. Can you hear me all right?
Yes. Yeah. So by way of introduction, these emojis are very sweet, but they're totally
dividing my attention. I don't know. I don't know how to make them stop. Maybe that's something
David J. can do. Yeah, they are kind of dividing my attention as well, I would say.
So yeah, by way of introduction, Danish Machenberger is focused on ways of improving the
health and development of individuals and society for the purpose of creating a more
virtuous relationship between individuals and society. Sounds like a very worthy focus.
He is a founding member of the Consilience Project, which is aimed at improving public
sense making and dialogue. And Daniel is a prolific and generous thinker and writer and
speaker. And if you follow his work, you already know that. And if you don't, you can find links
to his work on our website on the webpage for our episode with him. And so the topic that
Daniel and Tristan are going to be exploring together, kind of springboarding from our episode,
is this notion of a digital open society. So on the one hand, we see some societies using
exponential technology in order to... We just lost Stephanie. There she comes.
Stephanie, you just left for a second and lost connection, but centralized power, I think.
Centralized power, yes. And enhanced surveillance. So think China, think Singapore. So we might think
of that model roughly as, let's say, a digital closed society. On the other hand, we see other
countries using technology to enable, you know, interactive democracy and citizen agency and
transparency and accountability. And so consider Taiwan, consider Estonian. So we might think of
that model as a digital open society. And so what we'd love to explore today, with Daniel and
Tristan and all of you, is how this notion of a digital open society might be a helpful frame
for thinking about how societies can use technology in order to address, you know,
addiction, polarization, breakdown in shared sensemaking, the climate crisis, like the
interrelated problems that we care about, i.e., the meta crisis. And so with that, I'm going to
pose this first question to you, Tristan. And that is, you know, understanding that, you know,
digital open society is an evolving frame and that many people and organizations,
including folks who are here with us today, are already working on that without necessarily
calling it that. How would you articulate what a digital open society is and how it differs from
a digital closed society? Yeah. Well, I think, you know, as we get into that topic,
obviously, the best way for people, if you haven't listened to this already, is to go back
and listen to the, you know, long unended conversation that Daniel and I had, trying to
really explore why such a digital open society is so needed or important. Because I want to
say here that we're not coming to the table thinking that we have the perfect utopian model.
We've already figured it out. This is exactly the spec for what a digital open society is.
We're also not just randomly pointing to a new North Star. We're trying to answer a current
trajectory, which I is rapidly heading towards the oppression or chaos direction that Daniel
mentioned. And I wanted to start today by reading a quote that I've actually referred to many times,
including on the Joe Rogan podcast that actually, first and for my work, in fact, I'll say that
when my first presentation went viral at Google about the attention economy and sort of raising
the alarms and that presentation was the one that was featured in the film, The Social Dilemma,
the most powerful and useful thing that came back from someone at Google
was listing a quote from Neil Postman's book, Amusing Ourselves to Death. And when she opens
that book with a quote about teetering between Orwell and Huxley, for those who don't know,
Orwell was the Orwellian 1984 vision of a dystopia and Huxley was the brave new world dystopia.
And people know the Orwell one. They don't know the Huxley ones. I'm just going to briefly read
that quote and then actually want to let Daniel do most of the speaking here and kind of elaborating
on what we mean by a digital open society. And what Neil Postman wrote was, what Orwell feared
were those who would ban books. What Huxley feared was that there would be no reason to ban a book,
for there would be no one who wanted to read one. Orwell feared those who would deprive us of
information. Huxley feared those who would give us so much information that we would be reduced to
passivity and egoism. Orwell feared that the truth would be concealed from us. Huxley feared that
the truth would be drowned in a sea of irrelevance. Orwell feared we would become a captive culture,
while Huxley feared we would become a trivial culture. And as Huxley remarked in Brave New World,
the civil libertarians and rationalists who were ever on the alert to oppose tyranny or oppression,
they failed to take into account man's almost infinite appetite for distractions,
including maybe some of the emojis that are on the screen right now.
And in 1984 Orwell added, people are controlled by inflicting pain. In Brave New World, they are
controlled by inflicting pleasure. In short, Orwell feared that what we fear will ruin us,
and Huxley feared that what we desire will ruin us. And so I wanted to bring this up because
Daniel, I think what I would love to have you speak to just a little bit is
we're not just randomly trying to say, hey, there's this thing called an open society and we should
try to build that digital 21st century thing. What is the default outcome that we're heading
towards? You mentioned the attractors of chaos and oppression, which are Huxley and Orwell.
Why is that just maybe just elaborate a little bit on why we're heading to those
two out? Because if we do nothing, we take our hand off the steering wheel.
Yeah, I want to hear feedback. Is that better? Oh,
I want to just acknowledge there were some comments that said I don't know if it's fair to
consider Singapore a closed society. There are some people who would say it's not fair to consider
China a closed society. There are some people who would say the idea that the US is an open
society when as much of the budget goes into black projects or gray projects and TSSCI
compartmentalization is not a fair consideration or that there is or has ever been an open society
actually. So these terms are very squishy, and we can get in trouble quickly. And then what do we
mean by digital? Is the whole key that we're using digital technology? No, we need to rebuild our
energy infrastructure. That's not a purely digital technology. That's the domain of atoms. We need
to rebuild manufacturing. We need to rebuild the physical materials economy so that everything
isn't made from raw resources that are depleting the world unrenowably on one side and turning
into pollution on the other side. So when we say digital, it really represents the center of the new
tech stack, but that the center of the new tech stack means the entire new tech stack. So what
we're talking about is that our physical technologies are going through a step function.
When our physical technologies go through a step function, our total power and how we behave
because technology ends up eliciting patterns of human behavior. If you have a tractor and you
don't use it and other people are using plows, then your people are just going to do much less
well. So the existence of the tractor codes the pattern of behavior of using the thing because
it's so much more adaptive, which means that the agrarian religions and mythos that have to do with
plow type cultures are going to change to industrial ideologies. So basically every time the
technology stack changes fundamentally, the social technologies, how do we coordinate have to change
correspondingly. So there is a, the industrial revolution was a major change and starting with
the printing press, feudalism, a lot of the basis of how that worked kind of got debased and we got
nation states. And so the movement into an internet world and computation applied to everything.
This is why we can say digital is it's not just computation and then the rest of the non-computation
world. It's that we can apply computation and then advanced tools of computation, meaning
machine learning on big data sets and like that, we can apply that to biology and protein
folding and drug development. We can apply it to nanotech and material sciences and balancing
energy grid. So that's not purely digital, but it's the basically computation at the center of a
new tech stack, AI at the very center of that, the technological substrate that mediates computation,
where the computation, the software can make the hardware that supports it better,
better transistor density, quantum computers, often photo computers, those types of things.
And then also the rest of the hardware stack, like the Internet of Things sensors and the rest of
the physical kind of infrastructure hardware. And then that applied to material sciences, biology,
everything else. And the reason we talk about the technology as exponential is it's tech
that helps us make better tech. So there's a compounding return, like gaining interest on
interest, right? Better computers allow you to model to be able to make better computers.
And so I'm just wanting to clarify, because if we take the terms literally, we'll get misguided,
right? Like they're pointing to a whole field of things. So when we say a digital open society,
what we mean is there is an emerging new suite of technologies that are,
you know, when we figure out calculus, and we can plot a ballistic curve. And we've got some
people who are aiming cannons by plotting a ballistic curve, and other ones who are doing a
pendulum dousing, like one of those is just going to win really, really clearly. The gunpowder
revolution and those who have guns versus those who don't. So who has more power just ends up
winning the game of determining what the world is in a kind of real politic assessment.
When you have a jump of power that has an exponent exponent in it, those who are developing and
deploying exponential technologies will just have more of a say in determining the future than anyone
who's not developing and deploying them. So at the center of the exponential stack, we can say
is digital technology. So we use digital as a shorthand to say, what are the new societies that
can utilize these powers? Because if they don't, they just won't matter. Not because they shouldn't,
but because in the in the nature of what we'll get selected for, if the thing doesn't actually,
if it doesn't actually do a better job at the current game of competing for power and being
able to organize people and do productivity, it's just not going to matter. So how do we utilize all
these technologies? And how do we utilize them to make a world we actually want as opposed to not
want? Because the increased technological power can just mess the world up. And there's a lot of
catastrophic risk associated with runaway AI with even the mild examples we already see that Tristan
talks about that is definitely not like a full blown paperclip maximizer or some kind of runaway AGI,
but is, whoa, we just kind of broke democracy and broke public solidarity in the epistemic
commons and drove kind of addiction and limbic hijacks across the whole population with a pretty
narrow AI, just optimizing an ad revenue model on a couple websites. Now we start to multiply that
by growing AI and the AI empowering autonomous weapons and all the other things. And we realized
that the way humans have navigated power so far has not been all that wise. We have used our power
to destroy environments, we've used our power to destroy other civilizations and win the game of
empire. So if you multiply that by exponential power, exponential externalities don't keep
working and exponential warfare doesn't keep working. So the exponential tech not guided more
wisely than we have used tech in the past looks like either exponential warfare or exponential
externality, those both look like catastrophic risk in a world where we have full globalization,
we have so much interdependence on the supply chains that we all depend upon these computers
that we're talking through and the whole internet infrastructure and the servers and the satellites
that we're talking through. And all of these are produced on vast supply chains across six
continents where problems anywhere can actually mess up those supply chains everywhere. So we have
a situation where you can't have localized failures that don't start to create cascade
dynamics in a way that wasn't true in the Roman Empire, the British Empire or any previous time
in history. We're also running up against planetary boundaries, not just with climate
change and overfishing, but a hundred different planetary boundaries where you can't just keep
doing exponential growth through extraction. We're also running up to where even smaller
groups can cause whether the smaller group is a militant group or a small group like a startup
called Facebook can cause much larger effect on the whole world through the exponential
technology that's involved. And so we are at a point where catastrophic risk as a function of
the total amount of tech we have plus how interconnected the planet is plus how the cumulative
effect of hitting planetary boundaries means the world has high fragility. And so catastrophic risk
is one attractor. If we keep going with exponential tech in the current way, we keep getting
increasing probabilities of catastrophic risk across more different catastrophic risk categories.
The only way to avoid that is to have societies that can't can avoid it that can anticipate and
control keep those things from happening. And that requires quite a lot of information and
quite a lot of control. And most of those end up being societies that don't look like the nicest
place to live, according to the values of most people for most people. So that looks like models
that are maybe stable in the presence of exponential tech in, but they are stable in a way that
many would consider dystopic. And this is when we're talking about, you know, the system of
the constitutional republic we have in the United States, the system of governance,
was a like early industrial revolution, not even like post Henry Ford, like an early industrial
revolution, yeoman farmer level society that assumed very small little local populations
that could all get into a town hall and largely know each other that didn't have major catastrophic
risks that didn't have to have as many places where the people all had to work together because
they weren't affecting each other because of not having, you know, integrated tech stacks and things
like that. So you can see that we have we've innovated in our military, but we haven't been
innovating in our fundamental system of governance. How do we have government that is
up for by the people and support of life, liberty and the pursuit of happiness based on ideals that
everyone has created equal and has inalienable rights, like even as many problems as those ideas
had in our bad instantiations of them from the beginning, if any of those values, even modified
versions of them seem valuable, there's some fundamental innovation that should be happening
