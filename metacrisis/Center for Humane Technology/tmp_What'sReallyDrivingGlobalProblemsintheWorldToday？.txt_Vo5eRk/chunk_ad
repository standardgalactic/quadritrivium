where you come to the town hall in a way they have that right like it's 23 million people
but there is an older shared culture there is all there also happens to be an existential
threat just right off their border that is you know big enough that they can't just chill and
not focus on it they have everyone has to be civically engaged with some civic identity and
like that uh they didn't start making their culture in the industrial era and then have to
upgrade it right like they started later um where they're we're able to start at a higher level of
the tech stack um so there's a number of reasons why it's different so we're not going to naively
say what you do in a tiny country that is culturally and ethnically homogeneous uh
and has a higher GDP and education per capita and whatever is the same thing you would do
but we can certainly take a lot of the examples and say how would they apply differently in different
contexts so the thing we said earlier that this suite of exponential technologies is so much more
powerful than all of the previous types of power that only those who are developing and deploying
them will be really steering the direction of the future and that there are ways of employing them
that do cause catastrophic risk uh and the catastrophic risk is of two primary kinds right
conflict theory mediated and you can't you just can't do warfare with this level of technology
and this interconnected a world and make it through well not all catastrophic risk means
existential doesn't all mean nuclear war and nuclear winter and we've killed all the mammals on
earth it it might just mean we break global supply chains kill lots of people and regress humanity
and the quality of the biosphere pretty significantly so i'm i'm not just focused on existential risk
i'm interested in you know kind of catastrophic risk at scale in in general and we can see that
exponential tech applied as in conflict theory and in mistake right as externalities in the cumulative
effect could you define conflict theory and mistake theory for people who who are not familiar with
those terms yeah there's a very nice discussion on the less wrong forum if people are interested
to go deeper and is this question of how much of the problems in the world are the result of
conflict theory versus mistake theory meaning conflict theory is we knew we either wanted
to cause that problem that harm to whomever as in a knowingly wanted to win at a war and or at least
we knew we were going to cause that problem and didn't care because it was attached to something
we wanted right conflict theory or mistake theory we didn't know we didn't want to cause it and we
really didn't know and it was just unintended unanticipatable consequence and it's fair to say
that there's both right there's plenty of both one thing that is worth knowing is that if i
if i'm trying to do something that is actually motivated by conflict theory it benefits me to
pretend that it was mistake theory benefits me to pretend that i had no idea and then afterwards
say oh it was an unintended unanticipatable consequence it was too complex people can't
predict stuff like that and so the reality of mistake theory ends up being a source of plausible
deniability for conflict theory and but they're but they're both things and we have to overcome
both meaning we have to have choice making processes in our new system of coordination and
like this sounds like maybe hippy stuff until you take seriously the change of context so we
have to have problems of choice making that consider the whole that sounds like unrealizable
hippy stuff until you realize but we're making choices that affect the whole at a level that
can even individually be catastrophic and is definitely catastrophic cumulatively so if we
aren't factoring it then the human experiment self-terminates and maybe that's the answer to
the great filter hypothesis right uh and so are well yeah i think people don't have an intuitive
grasp of um what it means that each of us are walking around with the power of gods to influence
huge enormous consequences i mean i give a few examples every time you enact with the global
supply chain and hit buy on amazon you invisibly enacted you know shipping and planes and petroleum
and middle wars in the middle east there's a whole bunch of things that were sort of tied into when
you are posting something on social media and have more than a million followers you're influencing
a global information ecology and if you're angry and biased about one side or the other of the
pandemic is real or it's not real or something like that you're externalizing more bias into
the commons of how keep the rest of the world understands things so we're walking around with
increasing powers but i don't think the increasing power that we've granted is as intuitive
for for for some folks did you explain some more examples of that there's both cumulative effect
and like cumulative long term and fairly singular short term and cumulative long term i mean you go
back to early us settlers coming into the us moving west and they're being buffalo everywhere
and there had been buffalo everywhere for a very long time and then there's no buffalo in whole
areas that were forested with old growth forest became deforested and it was like no it's impossible
we could never get rid of all the buffalo like i we could never cut down all the trees but the
cumulative effect of lots of people thinking that way we're individually i have no incentive to
leave the buffalo alive and i do have an incentive for my family individually to kill it but everybody
thinking that way and increasing our um desire for how much we consume per capita our technology
that allows us to consume more per capita and developing more capital more total people well
then you start getting environmental destruction and species extinction at scale and that's a
long time ago right like that's much lower tech and much less people and it's distributed action
it's a cumulative it's a cumulative effect issue and obviously we see that with
with nobody's intending to fill the ocean with microplastics but everybody's buying
shit that is filling the oceans with microplastics and so everyone is participating with systems
where the system as a whole is sociopathic the system is self terminating the system
doesn't exist without all the agents interacting with it all the agents feel like their behavior is
so small that that justifies everybody doing that thing right so that's what we mean by cumulative
kind of catastrophic risk but it's also true that whoever made that thermite bomb and hooked it to
a drone and hit the ukrainian munitions factory a couple years ago that caused a billion dollars in
damage exploded the munitions factory the effect of a bomb as big as a the the largest non-nuclear
bomb the u.s arsenal has and it's an air bomb this is a home that was a homemade little bomb in a
drone right and so and crisper gene drives are cheap and easy and it doesn't take that much
advanced knowledge to start working with them and so that that starts to look like
individuals and small groups with real catastrophic ability not long-term and
cumulatively the increase in our tech gives us both issues via globalization and the overall
system you get these cumulative long-term effects and with the exponential attack creating
decentralized catastrophic capabilities one of the core questions we have to answer is how do we
make a world that is anti fragile in the presence of those kind of catastrophic capabilities that
are easy to produce and thus decentralizable so uh how do we do that what are the social
systems that we need to employ to bind some of these bad effects in ways that that the natural
inclinations of self-interested actors will drive things in that direction just to link this to the
social media space for people if i know that i can get a little bit more attention and a little
bit more likes and clicks and follows and shares and so on if i exaggerate the truth by five percent
just to use a little bit more of an extreme adjective you know i know that that in the
long run would be bad if everybody did that but for me right now i can win a few hits and i can
get more influence and i'm an instagram influencer and i'm making ten thousand dollars a month and
if i don't do it i'm noticing everyone else's do it and if i don't use the filter everyone
else is using the filter and so everyone ends up in this sort of another race to the bottom
sort of situation that has that kind of cumulative degradation or cumulative derangement where
there's increasing distance between what is true and what people believe because we've all been
subtly exaggerating it to make our point and gain influence and so on and so just to give another
example maybe for listeners in in kind of the space that they're more familiar with
but going back i mean the whole premise of this is as we gain more exponential technologies that
have more capacity and more hands so instead of having just the u.s and russia having this you have
whether as you mentioned chris projean drives or some of the drone things that are out there
more and more people have access to these things how can we bind those kinds of forces
and what are the social systems that we need to make that happen yeah i want to go back as you're
describing this i was thinking about how many people who um listen to your show who maybe work
in technology who might have uh they work in technology because they see the positive things
technology can do and have more of a kind of techno optimist point of view and this
overall conversation might sound very techno pessimist and like did we not read pinker and
watch hans rossling and you know those types of things and um so i want to speak to that briefly
first this is a meta point but it's worth saying right now particularly in on this podcast and in
the kind of post truth post or fake fact world where then so much of the emphasis has gone into
we need fact checkers and we need real facts um obviously it's possible to have an epistemic
error or even intentional error in the process of generating a fact is is there corruption in the
institutions and that kind of thing but let's even say that wasn't an issue and the things that go
through the right epistemic process as facts are facts can you lie with facts totally can you
can you mislead with facts yeah because nobody's going to make their choice on one fact they make
their choice based on a situational assessment based on a narrative based on a gestalt of a
whole thing that's lots of different facts well which facts do i include which facts do i not
include and do am i decontextualizing the fact so uh the quality of life has gone up so much
because we average person lived on less than a dollar a day in the us in 1815 and now they live
on this many dollars a day which inflation adjusted means higher quality of life yeah but in 1815 most
of their needs didn't come through dollars they grew their own vegetables they hunted they so i'm
decontextualizing the facts to compare something that's really apples and oranges so even if the
fact is quote unquote true the decontextualization and recontextualization makes it seem like it
means something different than it means and the same with the cherry picking of facts and i can
very easily say oh there's a lower percentage of people in extreme poverty but i might also be
changing the definitions of extreme poverty i can also rather than focus on percentage say well
there's more total people in poverty than there were total people in the world before the industrial
revolution so like so there's the ability to decontextualize and recontextualize facts there's
the ability to cherry pick facts and there's the ability to lake off frame facts and put particular
kinds of sentiment and moral valence on it and so am i talking about them as illegal aliens or
undocumented workers and i get a very different kind of sentiment so talking about it as a
pre-owned car or a used car everyone loves a pre-owned car no one wants a used car and so these
very simple semantic frames contextual frames cherry picking of the things means that i can
make a narrative where all the facts went through the most rigorous fact checker and yet the narrative
as a whole is misleading and so fact checking is necessary but it is not sufficient for a good
epistemic in good sense making and not only is it not sufficient it's even weaponizable
this is a very important thing to understand because if you are not pursuing that you're
if you're not recognizing that you might be believing nonsense thinking that you're using
epistemic rigor okay so the techno pessimists and the techno optimists both cherry pick and they both
lake off frame and this is true with all the difference in almost every political ideology
the woke in the anti-woke the um the pro socialist pro capitalist you'll notice that the way they do
their arguments the systemic racism is really really terrible no there's not that bad the systemic
racism they both have stats but this is actually you can almost think of it as statistical warfare
as a tool of narrative warfare and um so this is where a higher level of earnestness rather than
a particular vested interest or bias a higher willingness to look at biases a higher level of
rigor you know ends up being critical to actually overcome any of these things so can i cherry pick
stats that make it look like everything's getting better totally those things are true and nobody
wants to go back to a world before novocaine when you have to do dentistry and nobody wants to go
back to a world before penicillin when basic bacterial infections go around and like there's
totally good stuff that has emerged um and are there all kinds of uh ubiquitous mental illnesses
and chronic complex disease that didn't exist before and um increase in the total number
of addictive type behaviors within populations and um and a radical increase in the catastrophic
risk landscape a negative effect to environmental metrics so things are getting better and things
are getting worse at the same time it's important to understand that depending upon what you pick
it's just that the things that are getting worse are heading towards tipping points that make the
whole thing no longer viable and so that we're not denying that there are things that are getting
better we're saying that for the game to continue at all right to have it be an infinite game that
gets to keep continuing there are certain things that have to not happen and you can't have the
things that are getting worse keep getting worse at the curve that they are and have the things
that are getting better be able to continue at all so i just want to say that so naive techno
optimism can actually make you a part of the problem because then you do things like develop
a solution to a narrowly defined problem and externalize harm to other areas because you
weren't taking seriously enough not doing that but techno pessimism also makes you a part of the
problem or at least not a part of the solution because because the world is the future is not
going to be determined by Luddites it's not going to be determined by people who aren't developing
the tools of power so if you aren't actually looking at how do we develop a high tech world
that is also a fundamentally desirable in terms of a high nature and high touch world then you
really aren't thinking about it in a way that ends up mattering and so we are techno optimist but
not naive techno optimist we go through the totally cynical phase of man tech is a serious
issue and then you go to a post cynical phase of if i want to be techno optimist and not be silly
what does it take to imagine a world where humans have that much power and we are good stewards of
it meaning that we actually tend to each other well and we don't create a dystopic world that has
exponential wealth inequality in an underclass that nobody in the upper class would want to trade
places with and that doesn't cause catastrophic risk right now the amount of power of exponential
tech makes two attractors most likely catastrophic risk of some kind or social systems that are
that do not preserve the values that we care most about that are the ones that are currently most
working to develop and deploy that technology and to just give a very brief recap of the frame that
Tristan you gave on it earlier as you mentioned
China is not leaving a hundred percent of its technology development to the market to develop
however it wants even if it harms the nation state they are happy to bind technology companies
that are getting too large and in ways that would damage the nation state as we saw with
ant corporation and and they are doing a lot of very central centralized innovation
as well associated with long-term planning long-term planning is a key thing in the US
term limits make long-term planning very hard as does a highly rival risk to party system that is
willing to damage the nation as a whole to drive party wins so in that system
almost all the energy just goes into trying to win right you spend at least a couple years
but even the years before that are fundraising creating alliances to just try to win then
you're not going to invest in anything heavily that has return times longer than four years because
it won't get you reelected so no real long-term planning and then whatever you do do in those
four years will get undone systematically in the next four years for the most part all right
all right that system of governance will just fail comprehensively in relationship to a more
to a to a system that doesn't have that much internal infighting and that has the capacity
to do long-term planning and there's a million examples we can look at but just when did high
speed trains start they started you know we saw them emerge in Europe we saw them emerge in Japan
and in China we've seen China now start to export them all around the world and the US still doesn't
have any high speed trains and it's like what happened why and we can see that the US innovated
in fundamental tech in the Manhattan project kind of through the Apollo project but then it started
to privatize almost everything to the market the market started to develop in ways that really
were not advancing the technology in a way that increased the coherence of the nation and the
fundamental civic values and ideas of the nation even the world war two thing we can see we increased
our military capacities radically but that didn't mean we actually really advanced the ideas of
democracy or those values of do we make a better system to educate the people and inform them and
help them participate in their governance do we make better governance this is why the US military
is so powerful but the US government is so kind of inept and which is why nobody wants to fight a
