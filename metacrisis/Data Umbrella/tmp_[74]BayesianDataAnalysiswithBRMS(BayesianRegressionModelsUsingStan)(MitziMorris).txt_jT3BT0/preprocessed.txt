Hello, everyone. Welcome to Data Umbrella and our collaboration with Our Ladies New York
City webinar today. First of all, we're going to have the Our Ladies introduction, and I'd
like to introduce you all to Dorota Ritzink, and she is going to share a bit about the
Our Ladies community, particularly the New York City chapter. So welcome, Dorota.
Thank you. Yes. Hi, everybody. I'm here to just give a very quick intro to Our Ladies
and what we're about, and I am one of the lead organizers for Our Ladies NYC, the New
York City chapter. So there's Our Ladies Global, which I'll talk about a little bit at first,
and then I'll go into more detail about our specific chapter. So what is Our Ladies? Our
Ladies is a worldwide organization that promotes gender diversity in the art community via
meetups and mentorships in a friendly and a safe environment. And our mission is to
have more women and non-binary coders, developers, speakers, and leaders in the art community.
We believe firmly that more diversity, equity, and inclusion of people developing our packages
and being a part of our community is a fantastic thing, and that's what we strive for. And
Our Ladies Global is huge. We have a ton of chapters all over the world. I am actually not
100% sure when this image was taken, so it's possible for even more chapters at this point,
but we are everywhere. And it's actually not that hard to get involved. If you don't have
a local chapter where you live, there's a way to very easily start one. So I highly recommend
looking into that. To do that, you can just send an email to info at ourladys.org to get
in touch. You can follow Our Ladies on social media, on Twitter, LinkedIn, and yeah, you
can follow all these different chapters from all over the world. It's great. It's a really
wonderful community to be a part of. So more about Our Ladies NYC. We were founded around
2017, so several years after Our Ladies initially began and was already in several different
places, both in the US and beyond. A few fun facts. So we currently have, actually, we're
up to like 2,900 members on Meetup at this point. So it's quite large. That said, we don't always
have a huge turnout at Meetup events. So don't be discouraged by that very large number. We
host events monthly and contacting us is really easy. You can check out our website, OurLadiesNYC.org.
You can find us on Meetup or on Twitter and LinkedIn. And there on the left is a list of our
current board members, our wonderful group of Our Ladies that helps make these monthly events
possible. So join us. We host Meetup events like presenters, giving talks both technical and
professional development talks. We host a book club. We host networking and social events. And
we typically try to attend a few conferences here and there. And it's great. It's wonderful. We
have a wide variety of ways to participate. And speaking of participating, here are a few ways to
get involved. First and foremost, attend our Meetups, follow us on Twitter and other social media
pages. Join our Slack channel and GitHub pages. And ask questions. You could even write a blog
post for our website if you want to try or handle that. We are always encouraging folks who have
not really written or presented about our technical topic before to get involved. So please reach out
if you're interested in either of those options. And like I mentioned earlier, you could also
sponsor a local chapter if you don't have one near you. Or join the board of the New York City
chapter if you're in the area. And most importantly, spread the word and tell your friends. We are
always looking for more our ladies to join our community. Here's some, you know, contact information.
And that is it. That's the end of my presentation.
Okay. So, Reshma, you're muted. Yes, I'm going to do the data umbrella introduction. Hang on.
Sorry. That's okay. Slides. There we go. Okay. Hey, everybody. So I'm going to introduce the
data umbrella part of it. We're really happy to do this co-promotion and co-organization with
our ladies. It's been something we've wanted to do for a while. So I'm going to do a brief
introduction. Mitzi will do a presentation and you can ask any questions in the chat and we will
answer them as good time to stop and answer. This is being recorded and the video will be
available on YouTube typically within 24 hours. Data umbrella is a community for underrepresented
persons in data science and we are a nonprofit organization. This is the team of data umbrella.
We are in a few different countries around the world and there's a bunch of people that make
it happen behind the scenes. We have a code of conduct and we thank you for helping to make this
making a welcoming, friendly, professional environment for everybody.
There are various ways to support data umbrella. One of those ways is to follow our code of conduct.
Another way that you can do it is to ask general questions and share job events and postings.
You can also donate to our nonprofit. We are an open collective and if you work for a company
that uses Benevity, which is company match donations, that's another way to make a donation.
You can subscribe to the data umbrella YouTube channel. We have over 80 videos there on all
different data science topics and we have over 2,000 subscribers. We have a bunch of playlist.
One of them is career advice. We have playlists such as data visualization contributing to open
source, scikit-learn, PMC and NumPy. We also have a monthly newsletter and I will share the link in
the chat to find out about upcoming events and announcements. Our website, which we are in the
process of redesigning at the moment, has a lot of resources on open source conferences,
allyship language. We encourage you to check it out.
We are on all social media platforms as data umbrella. The place to find out about
upcoming events is Meetup. I have mentioned YouTube and the newsletter and Twitter and
LinkedIn are the two platforms where we are the most active. This webinar platform is
Big Marker and on the top you will see a CC for closed captioning. You can have live transcripts
enabled for this presentation if you would like. We have a call for volunteers. For all of our
videos we add timestamps. If you are available to add timestamps, I will share the link to it
for this video or some other videos. You can add those timestamps on GitHub.
We also have a feedback form which you will find in the handout section. I will also share
the link once I am finished speaking. If you have any technical issues or suggestions for
future event topics or any general feedback. We have a couple of upcoming events in March.
The first one is what is a sales engineer from a computer science background. That is going to
be a fascinating talk. On March 21 we have ML Ops from concept to product.
Today's talk is Bayesian data analysis with MRMS with Mitzi Morris who is a returning speaker.
Mitzi is a member of the stand development team and serves on the stand governing body.
Since 2017 she has been a full-time stand developer working for professor Andrew
Gellman at Columbia University where she has contributed to the core stand platform and
developed command stand pie a modern Python interface for stand. She is also an active
stand user developing, publishing and presenting on Bayesian models for disease mapping. Prior
to that she has worked as a software engineer in both academia and industry working on natural
language processing and search applications as well as data analysis pipelines for genomics
and bioinformatics. Feel free to tweet about this event at data umbrella.
And with that I will turn off my mic and video and hand over the screen share to Mitzi.
Thank you so much for that really great introduction and yes I used to do natural language processing
which is what we're not going to talk about today. Today we're going to talk about Bayesian
data analysis with BRMS and yes so I as a stand developer have not really used BRMS but BRMS is
just really I think well it's the most widely used interface of the stand interfaces and it is
absolutely fantastic so why don't I keep going with my slides so we can make this all very
concrete and clear for you. So BRMS stands for Bayesian regression and multi-level modeling
in stand. The developer who started this project is Paul Birkner he's German which I think is why
multi-level modeling is such a one word noun phrase in that particular use of multi-level
modeling and BRMS package uses the R formula syntax which if you're familiar with R and
fitting regression models in R this is the general syntax that's used to fit to describe
model a regression model and so this snippet of code here we're going to go through
and get to see how BRMS works but first some more background.
So the question here is why should I use BRMS if you want to do Bayesian modeling if you want to do
regression modeling you should use BRMS if you're working in R and you're at all if you've heard
you've heard about stand then you should actually learn about BRMS because BRMS simplifies model
development and it uses the extended R formula syntax to specify the likelihood it has a function
set prior to specify the priors on all the parameters and the BRMS package has lots and lots
of checks so that you can make sure that your model is actually fitting your data
and the downstream analysis packages that are also part of the stand ecosystem
baseplot, proj, pred and lu baseplot is the package that base that gives you plotting
lets you visualize everything in your fitted model, proj, pred is used to investigate the
particular parameters of interest in your model and it's helping you see what kinds of predictions
those particular parameters are going to make for you and lu stands for leave one out cross
validation and lu is a tool for comparing different models BRMS generated stand programs are efficient
and robust so if you have a data set and you know how you would like to model your data you have some
idea of the multi-level model that you want which of the predictors are going to be most relevant
which of the predictors have some kind of grouping structure working with BRMS is very nice because
you can quickly try out a lot of different variations on how much of the problem you want
to put into your model and then with BRMS you write a whole bunch of models and you can use
the downstream analysis packages to do the model comparison and this in a nutshell is the Bayesian
workflow the Bayesian workflow a lot of times when you're learning about stand or learning about a
problem set in doing Bayesian data analysis the model is presented to you and you're you're taught
the model the structure of the model how it fits in with the data and how well it works with
your data set but what you need to understand is that model development is the first piece
of the puzzle the second piece of the puzzle is model comparison some models are going to be too
simple they're easy to write simple to understand but they don't really work that well some models
are too complex some models are difficult to fit or some models are going to buy by building in too
much complexity in your model you might build a model that gives you a very very good analysis
for the data set that you have but it might be overfitting because you've just put too much
stuff in it and so you want to do model development you want rapid model development you want to be
able to to specify a lot of models very quickly and then you want to be able to do model comparison
and to do that BRMS package and its friends give you everything you need
and here's just a little bit of modeling terminology and notation because you're going to see these
terms and you're going to see some of this notation everywhere so I feel like we always
have to go through the slide we're talking about data and when we say data why we're talking about
all of the data we're not making a distinction between your outcome and your predictors when we
talk about theta we mean all the parameters in your model and we're building a Bayesian model
so we're building a joint probability distribution over all the data and the parameters and when
you write a stand program stand is solving the joint probability density so it's fitting all of
the parameters all at once now the important things to keep your eye on when you're doing
Bayesian modeling is this notion of the prior probability distribution p of theta this is
what you know about your problem before any data are observed and then p of theta given why the
posterior probability distribution is what you've learned from your data so it's the probability
of the parameters being what you've estimated them to be conditional on the data that you've seen
with that starting point of p of theta what you already knew before you started fitting the
data to the model the thing that we're really talking about though is the probability of why
given theta the probability of the data given the parameters and when you know the data when
you're fitting your model this is the model's likelihood once you've fitted your model and you've
got an estimate for theta which is fixed this is your sampling distribution and you use this
sampling distribution to generate new data I mean to fit new to make predictions for new previously
unseen data so these are our our terms and the things that we're going to be talking about
our prior our posterior probability distribution and our likelihood
multi-level regression I'm quoting here Richard McElrath if you haven't read statistical rethinking
um this is the book that we recommend to absolutely everybody statistical rethinking
is going to make um statistics it's it's designed to make um thinking about statistics far less
confusing than it probably has been in any statistics class that you've taken um so get
statistical rethinking and read it and the argument that McElrath makes is that multi-level
regression is the default form of regression because multi-level regression models handle
structured data and almost all of your data has some structure your observations can sometimes
be repeated from the same source or else they're ordered or there are some group structure in the
data so an example of group structure is a hierarchical model where you have students in
classrooms classrooms are in a school schools are in districts districts are in states states are in
regions and each of those nested areas is going to probably make the data from that particular
um school be of a certain flavor so we expect that when there's a hierarchical structure
we're getting a little bit of information from every node above your your your bottom most your
your leaf node in the in the tree we're getting information from all of those contributing to
your observation or if you have say an auto regressive you have time series data where the
the next item depends on the previous several items possibly spatial data where you have some
observation and we expected to be sort of similar to observations that are neighboring
spatio temporal where we blow the whole thing out into one big 4d it's my neighbor because it
happened in the time slice right before me in the region right next to me in the volume around me
so these are all examples of structured data and multi-level models let you say more about the data
we can estimate the variation on all levels of the model and with the multi-level structure
we can also say well what if I have a new group and then I can say well I don't know what you know
I don't know anything about the the items in this new group but if I know about the items in
other similar groups I can reason about unseen groups and what you what's what you find though
is once we start building these multi-level models your big data suddenly isn't so big
anymore because you've put all of these qualifiers on every data observation and this makes working
with a multi-level model both more interesting and more challenging
so the history of regression models in our from somebody who spends most of her time
programming in c++ and python is like this we have the pre-existing packages by pre-existing I mean
pre-stand which are LM and GLM for linear models and generalized linear models which
handle single-level linear models and these are part of the base our statistics package
and then you have LME4 which is designed to handle hierarchical linear models the goal of the
stand project which was started in 2010 was to build a better LME4. Andrew Gelman and Jennifer
Hill in 2006 2007 put out a really fabulous book multi-level regression regression analysis and
hierarchical models I forget what the exact title is it's in a subsequent slide and what's really
interesting about that book is that in Gelman and Hill they present a lot of data sets with
LME4 syntax and our code actually and the fact with matter is is that those those those models
can't really be fit with LME4 it doesn't really do the right thing on the data and Andrew's point
was that this is what I want LME4 to do so then a few years later he hired some postdocs to build
a better LME4 and the result was the stand probabilistic programming language and the
nuts HMC algorithm which gives you a much more efficient way of doing Monte Carlo Markov chain
Monte Carlo samplers but that's not what this talk is about so we all I can say is we use stand
on the back end and stand fits your model using a back end algorithm which for the best inference
is going to be nuts HMC and this will the the problem is that stand was designed by computer
scientists and so the stand probabilistic programming language while it builds on bugs
has been relatively challenging for statisticians to work with it it's a programming language it
looks a bit verbose and it's difficult to actually code up a model that does what you think it's
going to do it's very easy to put bugs in your bugs bugs by the way BUGS stands for Bayesian
uncertainty under Gibbs sampling and when I tell this to non-statisticians who've never heard of
bugs they think it's really the best name for a programming language ever but bugs basically
really changed the course of Bayesian modeling it made it very easy for people to write down
their models because prior to bugs you had to write not just your model but your Gibbs samplers so
that really set it for a pretty high bar for people who wanted to do Bayesian data analysis
with bugs Bayesian data analysis really took off and Stan I think has helped everybody
brought it along another another order of magnitude and so BRMS was developed in about 2016
by Paul Berkner and the goal was to simplify model specification to to relieve statisticians
of the need to write Stan programs instead it appeals to our users because it uses
elemi for style formulas and our functions to wrap Stan so you can work in R but you get the
goodness of Stan and the the comfort of thinking about your models in a way that if you were used
using LM GLM or LMA for a way that you're used to if you're a data analyst doing statistical
modeling in R I should also mention that our Stan arm is a package which gives you pre-compiled
Stan models and it's similar to BRMS in spirit in that with our Stan arm if there's a package
that you've been using in R for a particular kind of model our Stan arm probably has a version of
that model that has a pre-compiled Stan model on the back end and that will probably do a better
job of analyzing your data than the package that it's replacing and by better I mean it will be
faster and it will scale to larger data sets so this is sort of the the landscape of packages
and so when I talk about BRMS to the R ladies I'm hoping that most of you are going to be
relatively familiar with working with a regression package in R but if you're not
please stay tuned I'll show you how it's done linear regression this is just a quick review
of linear regression so and I'm I've got this slide here because I just want everybody to
sort of be really clear on the terms that I'm going to be throwing around when we get to the notebook
so we've got a linear model we specify the linear model in linear regression we're trying to fit
a line to or a multi a plane or a multi plane to some set of observations and the definition of
a line is an intercept with the slope and there's some amount of noise if we assume that this noise
is independent for every observation then we've got the guts we've got the specification of a
linear model and so the linear model is written where we say that every observation yi is drawn
from a normal observation which is centered at alpha plus beta times xi with variance sigma
so sigma is that error term that we see in the formula above in the equation in the first equation
and alpha plus beta xi is our linear predictor sigma is the variance term
in a generalized linear regression we're going to kick it up a level so instead of assuming
a normal distribution we can use any distributional family and I've bolded family because you'll see
well actually you won't see but family is what you specify when you're using glm package or
the lm4 package or brms to talk about some other distribution so you can talk about a beta
distribution a Poisson distribution if your data is discrete Poisson and the variance parameter
sigma when we're talking about a normal distribution is going to be any family specific
parameter theta and we can generalize our linear predictor alpha plus beta xi to eta which we just
mean any linear predictor and we transform our linear predictor by an inverse link function
so that's where we're getting the generalized linear modeling that's our link function coming
into play and then we can use group level subscripts on our parameters to allow for the
structure in the data to make our model multi multi-level or hierarchical so given all of that
we just have the equation specification for a general multi-level model which says that yi
is drawn from some distributional family and we have our function our inverse link function
applied to our linear predictor eta and there should be an xi there in that subscript but there
isn't and we have some distributional parameters theta but what we really are going to be talking
about when we're working with BRMS and thinking about the models that we're fitting in BRMS are the
intercept and the slope terms in our linear predictors so this is the regression formula
syntax this is the very first example I had in that first call to the BRM function in BRMS
reaction is distributed with according to an intercept term and the number of days
and actually I don't even want to try to explain what this guy is saying I just want you to observe
that this is a formula that on the left hand side we have reaction and on the right hand side
we have a specification for our linear predictor and in the linear predictor
because we're building a multi-level model we have things which are population level terms
fixed effects and those are parameters where we don't differentiate they're the same parameter
for every observation no matter what group membership it might have in the in the structure
of the data whereas group level terms which are sometimes called random effects vary by grouping
factor so we're going to start putting subscripts on our parameters to talk about which group
a parameter is is applied being applied to and in BRMS we have these group level terms where we
say coefficients vertical bar group by coefficients we mean the terms in the linear predictor so if
you think of the intercept as being the first column in your predict in your in your design
matrix then or your set of predictors the the intercept term is just designated by one and
everything else is going to be designated by the column label that it has in the predictor
in your your data frame which is your set of predictors so here in this sleep study which
is where this this this example comes from one of the columns in the sleep study data
is labeled reaction one of the columns is days one of the columns is subject so when we say
open parentheses one plus days vertical bar subject what we mean is that the subject is is the
grouping factor and we're going to see that both the intercept term and the the days the slope of
the days are are varying with the grouping factor i hope that wasn't too confusing and we'll show a
whole bunch of examples when we get to the notebook what BRMS actually does is quite impressive
BRMS is writing a stand program for you so you just call the BRM function and to call the BRM
function you specify your formula you specify your data you specify your distributional family if it's
not Gaussian by default and you can specify all kinds of priors prior specification is
a whole talk in and of itself so i'm not going to say anything about priors here
you specify all of these things and then the BRM call feeds all of that into make stand code and
make stand data then it so it generates a stand model using the stand a model written in the
stand probabilistic programming language that is then going to have to be compiled to c++
so the model code the data and all sorts of additional arguments which BR
BRMS is putting into your call are passed to a back end the back end can be either
command stand r or it can be r stand but you should probably be at this point using command
stand r it's the modern interface the modern r interface to stand it's more robust and
it supports the latest version of stand and it's more scalable i mean it
for it can handle bigger models and more data and but otherwise under the hood it's all stand
goodness it doesn't matter if you like to use our stand keep using our stand the model is compiled
and fitted in stand and then BRMS does post processing and the post processing is reversing
a lot of the magic that goes into compiling up writing that stand model so basically it's
translating all of the names of things that are fit inside the stand model back to the names that
you specified when you in your formula and are used in your data set and then all the results
can be investigated using all sorts of our methods in the BRMS package or the base plot
prod pred or lube package and whoa there should be a link to that notebook but don't worry we are
going to now jump into the into the demo part of the the live demo part of this talk so this is
the notebook in my screen i don't know if putting it into a full screen mode will make it bigger i
hope so so this um this we're i'm just going to work through this um this little uh our markdown
document so the first thing we're going to do is um do some stuff that i always put into a notebook
and we're going to load all the packages that you might want um i'm not actually using ggplot2 in
this notebook but never hurts to have ggplot2 so we're going to load the um brms the base plot the
lube the prudge pred and command stnr although i you probably don't need to look well i don't know
if you need to look through and stnr so we're going to load everybody and and the data uh the
book that andrew and jennifer wrote in 2006 is data analysis using regression and multi-level
hierarchical models andrew refers to that as by shorthand as arm applied regression modeling
it's a really great book and it makes a lot of things very clear if you're at all interested
in a really lucid discussion of causal inference i highly recommend this book and the example that
they uh they wait till chapters 11 and 12 to get to actual multi-level hierarchical models
and one of the first example data sets they use is the radon level data set which um is a data set
that i did my previous talk on and in the chat earlier there's a link lube liana lecture that's
more digging into what's going on with that particular data set which is an old historical
data set that's mainly of interest to play around with uh hierarchical models so this was a national
level survey uh andrew and jennifer analyzed data for the state of minnesota which consists of 919
measurements from residential houses um from 85 counties uh and the measurements were taken either
in the basement or on the ground floor and um although it's not entirely clear i'm pretty sure
that if the house had a basement the measurement was taken in the basement so the only times that a
measurement would have been taken on the ground floor in a house is if there was no basement
and another epa data set provides the soil uranium level for each county i didn't um so why is this
interesting radon gas is a carcinogen um that happens to be particularly uh deadly if you're
a smoker so if you're a smoker and you have a house that has a basement and you're living
in the basement and there's no better ventilation your chance of contracting lung cancer somewhere
along the way goes up really astronomically so um nowadays modern building codes and um
general awareness of radon has made this much less of a problem thanks in part to this epa
activity uh in the 1990s so radon gas comes from the ground it's a byproduct of uranium decay
uranium is radioactive and it has a very very long half-life decaying slowly one of those
one of those products of uranium decay is radon um and the regression model is going to try to
predict the home radon level on the log scale based on the county in which the house is located
the floor on which the measurement was taken and the amount of soil uranium uh for that county
so the data looks like this
we have the floor the county name the amount of data uh the amount the log radon level the
log uranium level which as you can see is uniform across a county and the county id which is a
factor which will be very useful when we're trying to group by county id so the modeling choices that
we have are uh to completely pull all the data across the uh the state of minnesota so we're
going to do a single regression average home radon level in minnesota is based on all 919 observations
or we can try to use county information and a stupid model is the no pooling model
where we run 85 separate regressions to estimate the home radon levels by county so we just say
you know as much data as i have on a county that's as much data as i'm going to use to figure out
what's going on in that county and then we have partial pooling a multi-level model
counties are similar so we're going to build a multi-level regression which says that we're
going to share information across all counties for every county we we estimate what's going on
in that county but at the um at the um hierarchical one level up we say that all counties are drawn
from a distribution for that that is a county low county distribution so that there's a certain
amount of uh there's a certain location that we expect uh certain certain set of parameters
that are common hyper parameters in the model which say that okay for any county in minnesota
we sort of expected to have this general characteristic we expect a certain radon level
and we expect a certain amount of variance across the county and given that um height that set of
hyper parameters then we're going to estimate what's going on in each individual county
so the complete pooling model is just a simple linear regression formula and so in brms the brm
function all we need to do the first um the first argument to the brm function is the formula the
formula is very simple here and the data set is the minnesota radon data set so we can just run that
and what's going to happen well actually um we're going to see a little bar right now this is the c
plus plus compiler compiling the generated stand code as soon as it's done compiling it this model
runs like lightning because it's a really really really simple model and it runs four chains using
the nut say hmc sampler and then it gives us um the uh summary of how well it fit that model
so for this gaussian family uh that is works we're we're just doing the simple the the not
simple but we're doing or oh yeah we are doing a simple linear regression where we expect everything
to be uh distributed normally we um have the identity the links here this is your generalized
linear model links where we're using the identity function for both the mu and the sigma of the
gaussian the formula is log radon um is distributed according to the floor the level on which the
measurement is taken and we um have the estimate of for the model's intercept and for the floor
and this all makes perfect sense because the way that things are coded the basement is uh
coded as floor zero and ground floor is coded as floor one and we know that radon gas is um the
levels of radon gas increase the the lower you go in a building so we expect the slope of the
regression line to be negative meaning that if you're in the basement your radon level
will probably be higher than if you're on the ground floor and there's a certain amount of
variance here um that's our sigma parameter we can visualize what's going on by using the brms
conditional effects on fit complete pool and that is going to give us a beautiful plot
the floor is actually discrete so it's either zero or it's one but here you see the regression line
in blue and the amount of uncertainty uh that's about the 50 percent
um uh interval around the regression line in uh dark gray so if you're in the basement your
log radon level is going to be higher than if you're taking the measure on the uh ground floor
and this confidence uh the the the uncertainty grows um because we have far fewer measurements
which are taken on the ground floor pretty much 80 percent of the measurements in this data set
were taken in the basement because in minnesota most homes that were in the survey had basements
the no pooling model looks kind of silly um because it's silly so what we're doing here
in this formula is we're saying there is no general intercept and that is because for each
county we're going to have its own intercept so we can fit that model too and again we can see uh
that brms is uh has generated the stand code and c plus plus is is uh compiling the stand code takes
a few seconds to do that takes like zero time to run the model and now we get oh yeah so um if we
look at this let's look at this in the uh output window um oh no we definitely want to look at
this here this is so beautiful this is the um conditional effects plot for when we have uh
85 separate intercepts so we've got a linear regression model and if we go up uh to the previous uh
one of these guys here we see if we pool all of the data we only have a single regression line
here we're saying that we're going to still use the uh floor as the uh as the slope of the line
and we're having 85 different intercepts so we get 85 different beautiful lines that are all
parallel and i think this is just really fabulous um this is also a really stupid model and you don't
want to do it so let's move on to the first model um which is we say that we want to have
a partial pooling model where we're going to pool information between counties
and uh we're going to pull information between counties but for every county we're going to
estimate its own intercept so this is what the formula looks like in brms and now we can fit this model
and when we're done fitting the model um again it takes the thing that takes the longest
when running brms is just the c++ compiler translating your model to c++ because the
c++ compiler is doing a lot of optimization so that your resulting model will run will be performant
and this plot the base plot mcmc interval plot here we're just plotting the uh county level
the estimates for the county level regression parameter so we have 85 different uh parameters
one per county and what's going on in this plot which is not actually that easy to read because
there are 85 things is that for counties which have a lot of uh observations we get much tighter
50 intervals around the um around the estimated uh mean and uh in this data set there are very few
counties in minnesota where all the people live there are essentially the counties for the minnesota
st paul and also for diluth the rest of minnesota is really rural and so for most counties you have
three or four maybe five observations which is why you want to be building a multi-level model
because you need to pull your information across your um regions and then finally we have the
pièce de résistance which is the varying slope varying intercept model which says that okay
for every uh county we expect that the floor uh effect the slope might be different for that
county as well as the intercept term so then we can fit this model and we can do exactly the same
plots and uh i think we're getting almost to eight o'clock i don't know how much longer i have to
talk here um but one more example and then we will be ready for questions so now the sampling is
actually taking a little bit longer because this is a more complicated model it has more levels
it has more uh parameters and it has more um hard choices to make so if we look at what's going on here
this plot doesn't look that much well actually it looks quite a bit different than this plot
um things are are moving around uh and what we see is that the the regression now is pulling
ever more closer to uh the the center of the the the means now are pulling more together
so you're getting more and more pooling especially uh down here and finally uh we can add in
one more predictor the log uranium which is really which is the same across all observations
in the county we can see what adding log uranium is going to do to this model
so this is a varying slope varying intercept model adding in another predictor the log uranium
and we don't need to um to do anything on the county uh level for log uranium because log
uranium is already on a perp county level although you know play around with it by all means don't
take my word for it um see what happens if you add uh log uranium into this um into this uh
group level effects and um adding log uranium does uh does it change things
it uh yes it pulls things way more towards the center so that is um that's pretty much it
hey mitzi we have a question which is how do you compare the different models oh yes perfect
question yes great so am i still sharing my r screen yes okay so now we've got well we'll just
go over here we're done with the notebook so this is this is thank you you this is the last this
should be the last um cell in the notebook so you're going to add it yourself so we've now
got these various fits right now if we say
um
now we're going to run the um lou function and uh oh yeah um so what this is saying is um that
oh you you've asked me the right question um but we're all going to have to together read the
documentation for the lou package to understand what these um estimates are doing and what it
is saying um is that um you know what i'm not really sure i um let's try a simpler one
the lou function is designed specifically to compare your models so the the short answer to
your question is you run lou
hey mitzi another question is um is this rmd available on git i did i i did link to your
github um is this the same exact um our markdown file that that people can oh yeah i put it on my
git i put it on my github repo yes okay got it um and then another question that came up is can
brms be used for non hierarchical data structure and non regression type of analysis um
it can definitely be used for non hierarchical data structure so there are plenty of multi-level
models which are not hierarchical um and a non regression what do you mean by a non regression
type of analysis the answer there is no the package is about regression um
and how to view the trace of the parameters um to view the trace of the parameters we can do
something like plot and i think this will give you yeah
oh what is that doing that's so bad
there you get some nice trace plots
another question here is uh does brms have options for checking model assumptions
um what do you uh yes definitely that is all about the prior predictive checking
so your model assumptions essentially are your priors and uh brms has functions uh for
prior predictive checks and it has posterior predictive checks and yes um checking your model
assumption is very very very important and that's really part of the cool stuff of the generated
stand code for brms it anticipates that you're going it generates a stand program that has
everything you need to do the model checking and the posterior um oh thank you uh thank you
richard montez i know it's not in scope but curious what priors were used um these are the
default priors the default priors are um the default priors here are a student t distribution
with three degrees of freedom and paul berkner works a lot with akibitari one of the key stand
developers uh key key contributors uh co-author of bayesian data analysis and he's very interested
in the effect of the priors um and so um you can specify whatever prior makes sense and
there are a lot of different theories about what a good prior for a model is the prior
really depends on your data so it's it's you shouldn't rely on the default priors but the
default priors are chosen to be very robust and over the course of the time that the stand project
has existed we've learned so much more about how these models behave because we can fit more complex
models with more different kinds of data and with larger data sets and we've learned a lot about the
behavior of what priors are good and the difference between hamiltonian monicarlo the
hmc algorithm that is used by the stan mcmc sampler and the previous generation of mcmc
samplers the gib samplers is that for gib sampling you had to choose certain kinds of priors
just so that the gib sampler didn't fall over and the hamiltonian monicarlo algorithm behaves
very differently so there was a lot a lot of discussion and uh and investigation that's gone
into what makes a good prior and what is recommended as a good prior for a model
has changed a lot from say 2007 when gelman and hill use all sorts of interesting priors in in that
text to now and the priors are specified the carlos wants to know how the priors are specified
laura i'm really sorry i don't know how you can use this in excel but carlos wants to know how the
priors are specified the priors are specified via an argument to the brm function and you create
something called a brms prior object so the brms documentation will tell you everything you need
to know about how to specify your priors and there's um several examples and vignettes
in the brms documentation that should help you they're written by academic researchers they
have a certain flavor but if you if you look around and ask questions on the stand discourse
you should be able to do just fine any uh anything else
last call for questions if anybody has um any questions meanwhile i'll read some of the comments
which is thank you for the great talk thank you mitzi this was an incredibly approachable introduction
so so glad to hear it thank you all great questions
all right so um oh oh i want to share one more thing go ahead my final slide i think i i i
i need to go to slide of 13 because these are everything that you should read next
so uh yeah um take a screenshot and uh check out some of those URLs and um the slides and the
notebook are up on github for you
mitzi thank you so much for the excellent talk um everybody thank you for joining us
and the video will be available very soon have a great um evening morning wherever you are bye
everybody and thanks darona darota
