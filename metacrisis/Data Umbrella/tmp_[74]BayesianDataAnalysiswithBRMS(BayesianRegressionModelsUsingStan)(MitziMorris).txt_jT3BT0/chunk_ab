with bugs Bayesian data analysis really took off and Stan I think has helped everybody
brought it along another another order of magnitude and so BRMS was developed in about 2016
by Paul Berkner and the goal was to simplify model specification to to relieve statisticians
of the need to write Stan programs instead it appeals to our users because it uses
elemi for style formulas and our functions to wrap Stan so you can work in R but you get the
goodness of Stan and the the comfort of thinking about your models in a way that if you were used
using LM GLM or LMA for a way that you're used to if you're a data analyst doing statistical
modeling in R I should also mention that our Stan arm is a package which gives you pre-compiled
Stan models and it's similar to BRMS in spirit in that with our Stan arm if there's a package
that you've been using in R for a particular kind of model our Stan arm probably has a version of
that model that has a pre-compiled Stan model on the back end and that will probably do a better
job of analyzing your data than the package that it's replacing and by better I mean it will be
faster and it will scale to larger data sets so this is sort of the the landscape of packages
and so when I talk about BRMS to the R ladies I'm hoping that most of you are going to be
relatively familiar with working with a regression package in R but if you're not
please stay tuned I'll show you how it's done linear regression this is just a quick review
of linear regression so and I'm I've got this slide here because I just want everybody to
sort of be really clear on the terms that I'm going to be throwing around when we get to the notebook
so we've got a linear model we specify the linear model in linear regression we're trying to fit
a line to or a multi a plane or a multi plane to some set of observations and the definition of
a line is an intercept with the slope and there's some amount of noise if we assume that this noise
is independent for every observation then we've got the guts we've got the specification of a
linear model and so the linear model is written where we say that every observation yi is drawn
from a normal observation which is centered at alpha plus beta times xi with variance sigma
so sigma is that error term that we see in the formula above in the equation in the first equation
and alpha plus beta xi is our linear predictor sigma is the variance term
in a generalized linear regression we're going to kick it up a level so instead of assuming
a normal distribution we can use any distributional family and I've bolded family because you'll see
well actually you won't see but family is what you specify when you're using glm package or
the lm4 package or brms to talk about some other distribution so you can talk about a beta
distribution a Poisson distribution if your data is discrete Poisson and the variance parameter
sigma when we're talking about a normal distribution is going to be any family specific
parameter theta and we can generalize our linear predictor alpha plus beta xi to eta which we just
mean any linear predictor and we transform our linear predictor by an inverse link function
so that's where we're getting the generalized linear modeling that's our link function coming
into play and then we can use group level subscripts on our parameters to allow for the
structure in the data to make our model multi multi-level or hierarchical so given all of that
we just have the equation specification for a general multi-level model which says that yi
is drawn from some distributional family and we have our function our inverse link function
applied to our linear predictor eta and there should be an xi there in that subscript but there
isn't and we have some distributional parameters theta but what we really are going to be talking
about when we're working with BRMS and thinking about the models that we're fitting in BRMS are the
intercept and the slope terms in our linear predictors so this is the regression formula
syntax this is the very first example I had in that first call to the BRM function in BRMS
reaction is distributed with according to an intercept term and the number of days
and actually I don't even want to try to explain what this guy is saying I just want you to observe
that this is a formula that on the left hand side we have reaction and on the right hand side
we have a specification for our linear predictor and in the linear predictor
because we're building a multi-level model we have things which are population level terms
fixed effects and those are parameters where we don't differentiate they're the same parameter
for every observation no matter what group membership it might have in the in the structure
of the data whereas group level terms which are sometimes called random effects vary by grouping
factor so we're going to start putting subscripts on our parameters to talk about which group
a parameter is is applied being applied to and in BRMS we have these group level terms where we
say coefficients vertical bar group by coefficients we mean the terms in the linear predictor so if
you think of the intercept as being the first column in your predict in your in your design
matrix then or your set of predictors the the intercept term is just designated by one and
everything else is going to be designated by the column label that it has in the predictor
in your your data frame which is your set of predictors so here in this sleep study which
is where this this this example comes from one of the columns in the sleep study data
is labeled reaction one of the columns is days one of the columns is subject so when we say
open parentheses one plus days vertical bar subject what we mean is that the subject is is the
grouping factor and we're going to see that both the intercept term and the the days the slope of
the days are are varying with the grouping factor i hope that wasn't too confusing and we'll show a
whole bunch of examples when we get to the notebook what BRMS actually does is quite impressive
BRMS is writing a stand program for you so you just call the BRM function and to call the BRM
function you specify your formula you specify your data you specify your distributional family if it's
not Gaussian by default and you can specify all kinds of priors prior specification is
a whole talk in and of itself so i'm not going to say anything about priors here
you specify all of these things and then the BRM call feeds all of that into make stand code and
make stand data then it so it generates a stand model using the stand a model written in the
stand probabilistic programming language that is then going to have to be compiled to c++
so the model code the data and all sorts of additional arguments which BR
BRMS is putting into your call are passed to a back end the back end can be either
command stand r or it can be r stand but you should probably be at this point using command
stand r it's the modern interface the modern r interface to stand it's more robust and
it supports the latest version of stand and it's more scalable i mean it
for it can handle bigger models and more data and but otherwise under the hood it's all stand
goodness it doesn't matter if you like to use our stand keep using our stand the model is compiled
and fitted in stand and then BRMS does post processing and the post processing is reversing
a lot of the magic that goes into compiling up writing that stand model so basically it's
translating all of the names of things that are fit inside the stand model back to the names that
you specified when you in your formula and are used in your data set and then all the results
can be investigated using all sorts of our methods in the BRMS package or the base plot
prod pred or lube package and whoa there should be a link to that notebook but don't worry we are
going to now jump into the into the demo part of the the live demo part of this talk so this is
the notebook in my screen i don't know if putting it into a full screen mode will make it bigger i
hope so so this um this we're i'm just going to work through this um this little uh our markdown
document so the first thing we're going to do is um do some stuff that i always put into a notebook
and we're going to load all the packages that you might want um i'm not actually using ggplot2 in
this notebook but never hurts to have ggplot2 so we're going to load the um brms the base plot the
lube the prudge pred and command stnr although i you probably don't need to look well i don't know
if you need to look through and stnr so we're going to load everybody and and the data uh the
book that andrew and jennifer wrote in 2006 is data analysis using regression and multi-level
hierarchical models andrew refers to that as by shorthand as arm applied regression modeling
it's a really great book and it makes a lot of things very clear if you're at all interested
in a really lucid discussion of causal inference i highly recommend this book and the example that
they uh they wait till chapters 11 and 12 to get to actual multi-level hierarchical models
and one of the first example data sets they use is the radon level data set which um is a data set
that i did my previous talk on and in the chat earlier there's a link lube liana lecture that's
more digging into what's going on with that particular data set which is an old historical
data set that's mainly of interest to play around with uh hierarchical models so this was a national
level survey uh andrew and jennifer analyzed data for the state of minnesota which consists of 919
measurements from residential houses um from 85 counties uh and the measurements were taken either
in the basement or on the ground floor and um although it's not entirely clear i'm pretty sure
that if the house had a basement the measurement was taken in the basement so the only times that a
measurement would have been taken on the ground floor in a house is if there was no basement
and another epa data set provides the soil uranium level for each county i didn't um so why is this
interesting radon gas is a carcinogen um that happens to be particularly uh deadly if you're
a smoker so if you're a smoker and you have a house that has a basement and you're living
in the basement and there's no better ventilation your chance of contracting lung cancer somewhere
along the way goes up really astronomically so um nowadays modern building codes and um
general awareness of radon has made this much less of a problem thanks in part to this epa
activity uh in the 1990s so radon gas comes from the ground it's a byproduct of uranium decay
uranium is radioactive and it has a very very long half-life decaying slowly one of those
one of those products of uranium decay is radon um and the regression model is going to try to
predict the home radon level on the log scale based on the county in which the house is located
the floor on which the measurement was taken and the amount of soil uranium uh for that county
so the data looks like this
we have the floor the county name the amount of data uh the amount the log radon level the
log uranium level which as you can see is uniform across a county and the county id which is a
factor which will be very useful when we're trying to group by county id so the modeling choices that
we have are uh to completely pull all the data across the uh the state of minnesota so we're
going to do a single regression average home radon level in minnesota is based on all 919 observations
or we can try to use county information and a stupid model is the no pooling model
where we run 85 separate regressions to estimate the home radon levels by county so we just say
you know as much data as i have on a county that's as much data as i'm going to use to figure out
what's going on in that county and then we have partial pooling a multi-level model
counties are similar so we're going to build a multi-level regression which says that we're
going to share information across all counties for every county we we estimate what's going on
in that county but at the um at the um hierarchical one level up we say that all counties are drawn
from a distribution for that that is a county low county distribution so that there's a certain
amount of uh there's a certain location that we expect uh certain certain set of parameters
that are common hyper parameters in the model which say that okay for any county in minnesota
we sort of expected to have this general characteristic we expect a certain radon level
and we expect a certain amount of variance across the county and given that um height that set of
hyper parameters then we're going to estimate what's going on in each individual county
so the complete pooling model is just a simple linear regression formula and so in brms the brm
function all we need to do the first um the first argument to the brm function is the formula the
formula is very simple here and the data set is the minnesota radon data set so we can just run that
and what's going to happen well actually um we're going to see a little bar right now this is the c
plus plus compiler compiling the generated stand code as soon as it's done compiling it this model
runs like lightning because it's a really really really simple model and it runs four chains using
the nut say hmc sampler and then it gives us um the uh summary of how well it fit that model
so for this gaussian family uh that is works we're we're just doing the simple the the not
simple but we're doing or oh yeah we are doing a simple linear regression where we expect everything
to be uh distributed normally we um have the identity the links here this is your generalized
linear model links where we're using the identity function for both the mu and the sigma of the
gaussian the formula is log radon um is distributed according to the floor the level on which the
measurement is taken and we um have the estimate of for the model's intercept and for the floor
and this all makes perfect sense because the way that things are coded the basement is uh
coded as floor zero and ground floor is coded as floor one and we know that radon gas is um the
levels of radon gas increase the the lower you go in a building so we expect the slope of the
regression line to be negative meaning that if you're in the basement your radon level
will probably be higher than if you're on the ground floor and there's a certain amount of
variance here um that's our sigma parameter we can visualize what's going on by using the brms
conditional effects on fit complete pool and that is going to give us a beautiful plot
the floor is actually discrete so it's either zero or it's one but here you see the regression line
in blue and the amount of uncertainty uh that's about the 50 percent
um uh interval around the regression line in uh dark gray so if you're in the basement your
log radon level is going to be higher than if you're taking the measure on the uh ground floor
and this confidence uh the the the uncertainty grows um because we have far fewer measurements
which are taken on the ground floor pretty much 80 percent of the measurements in this data set
were taken in the basement because in minnesota most homes that were in the survey had basements
the no pooling model looks kind of silly um because it's silly so what we're doing here
in this formula is we're saying there is no general intercept and that is because for each
county we're going to have its own intercept so we can fit that model too and again we can see uh
that brms is uh has generated the stand code and c plus plus is is uh compiling the stand code takes
a few seconds to do that takes like zero time to run the model and now we get oh yeah so um if we
look at this let's look at this in the uh output window um oh no we definitely want to look at
this here this is so beautiful this is the um conditional effects plot for when we have uh
85 separate intercepts so we've got a linear regression model and if we go up uh to the previous uh
one of these guys here we see if we pool all of the data we only have a single regression line
here we're saying that we're going to still use the uh floor as the uh as the slope of the line
and we're having 85 different intercepts so we get 85 different beautiful lines that are all
parallel and i think this is just really fabulous um this is also a really stupid model and you don't
want to do it so let's move on to the first model um which is we say that we want to have
a partial pooling model where we're going to pool information between counties
and uh we're going to pull information between counties but for every county we're going to
estimate its own intercept so this is what the formula looks like in brms and now we can fit this model
and when we're done fitting the model um again it takes the thing that takes the longest
when running brms is just the c++ compiler translating your model to c++ because the
c++ compiler is doing a lot of optimization so that your resulting model will run will be performant
and this plot the base plot mcmc interval plot here we're just plotting the uh county level
the estimates for the county level regression parameter so we have 85 different uh parameters
one per county and what's going on in this plot which is not actually that easy to read because
there are 85 things is that for counties which have a lot of uh observations we get much tighter
50 intervals around the um around the estimated uh mean and uh in this data set there are very few
counties in minnesota where all the people live there are essentially the counties for the minnesota
st paul and also for diluth the rest of minnesota is really rural and so for most counties you have
three or four maybe five observations which is why you want to be building a multi-level model
because you need to pull your information across your um regions and then finally we have the
pièce de résistance which is the varying slope varying intercept model which says that okay
for every uh county we expect that the floor uh effect the slope might be different for that
county as well as the intercept term so then we can fit this model and we can do exactly the same
plots and uh i think we're getting almost to eight o'clock i don't know how much longer i have to
talk here um but one more example and then we will be ready for questions so now the sampling is
actually taking a little bit longer because this is a more complicated model it has more levels
it has more uh parameters and it has more um hard choices to make so if we look at what's going on here
this plot doesn't look that much well actually it looks quite a bit different than this plot
