Daniel Schmartenberger, thank you for being with us.
Thanks for having me.
So, you're a philosopher, a founding member of the Consilience Project.
The goal of this conversation today is to analyze the direction our civilization is
taking in half an hour, because you've been doing like so many great podcasts about the
metacrisis during lasting three to four hours, and I suggest that people go and watch them
on the Internet.
They're very good.
We'll start with the harvest of the day.
The question I'm asking to all the guests here for Harvard Podcasts, if something easy
or simple could be done and would make the world a better place, what would it be for
you, Daniel?
When I saw the note that you sent me, that that would be a last question.
Is there a simple or easy thing that everyone could do that make the world a better place?
That kind of cringed?
Because I usually really am not a fan of that question, because the world needs so many
different kinds of things done that require different skills and capacities and orientations
and to try to reduce it to some thing that would be true for everybody.
You get a platitude like be kind or loving or something like that, or you get something
like recycle or pick up trash or try to use less carbon or something that doesn't map
to the whole set of things that the world needs.
I think there's a process where movements have been associated with political processes
and markets in a way that it's like, here's this great catastrophe that'll happen if the
other side gets elected, so everybody needs to get out and vote for so and so.
That's like everybody can do a simple thing because we're relating to everybody as voters
or everybody donate to this cause or boycott this thing, but the complexity of the world's
issues from climate issues to AI risk to supply chain issues to electrical grid issues, like
there's no action like that.
There's no somebody to vote for or not to vote for thing to donate to that addresses
it.
One thing that is not necessarily easy, but is relatively simple, that would be great
if everybody in the world could do more, is to seek to try to understand other people's
perspectives much more deeply, particularly those that are most different than their own.
If you can try to take the opposite perspective on abortion, on gun control, on climate change,
on the Ukraine-Russia war, on the Chinese versus Western system, on any of those things,
on the Israel-Palestine issue.
If you can try to earnestly be able to make the argument that the person on the other
side would make as well enough that they don't have anything to add to it, and not
just as a rhetorical process, but connect to the values that they care about and what
it feels like to be them and see the world through their eyes, realizing that there might
be distortion.
There might be a lot of things missing, but there's not zero truth or zero value to it.
That process, if everyone did that, would actually result in addressing the metacrisis
in all of its complexity, the issues in synthetic biology risk, and pandemics, and escalation
pathways to warfare, and economic issues, and geopolitical issues, and all of them.
You can say that they either come down to conflict or externalities, like we cause harm
directly, intentionally, which a war is a great example of, or harm gets caused that
we didn't intend to cause.
No one intended to cause climate change.
We just wanted to have transportation and energy, and the secondary byproduct of that
was climate change.
All of the environmental issues, no one intentionally had a conflict with the environment that was
causing it.
It was the externality of optimizing something and causing harm somewhere else.
There are problems that we intentionally cause, and there are problems that we accidentally
cause.
Both of them would be corrected by seeking to understand all the perspectives more,
because if you sought to understand the perspectives well enough, conflict theory would evaporate.
Most of the mistakes, when you're trying to optimize for one thing and you end up causing
externalities to something else, somebody else saw that and knew that, and if you were
in wide enough conversations, then the thing that you're trying to optimize for that's
going to cause harm somewhere else someone else would have mentioned and said, actually,
let's improve your design or your strategy by factoring this.
Both the unintentional externalities and the intentional conflict would be resolved through
active perspective seeking and then perspectives and this is wonderful.
When you look at the history, as you said, humans seem to have a talent for innovation
and progress, but also a natural tendency for war and chaos.
These two tendencies fit each other and make things bigger and bigger, so greatest but
out of control technologies can cause a huge damage.
What do you think should be done about technologies and do they represent innovation or danger
for you?
First thing about technology is that even if we're not talking about a military technology,
we're talking about a technology for some other purpose, even if we develop a technology
for some non-military purpose, it will have a military application or some kind of conflict
oriented application, basically saying all technologies dual use.
Maybe we're doing the synthetic biology gene editing for trying to cure cancer, but as
we get better at making tools to do gene editing, can that be used for bio weapons?
Totally.
Maybe we're making the AI to try to do drug discovery, but can that same AI do autonomous
drones?
Of course it can.
Whatever purpose we're developing technology for, we're also making that technology cheaper
and easier for all other types of purposes simultaneously, and that's a huge thing we
have to factor.
From a conflict point of view, obviously people with stone age technology can't cause a war
that blows the world up, and people with bronze age technology can't cause a war that blows
the world up.
The harm is proportional to the amount of tech, so as we move into exponentially more
powerful tech, we can't continue to use it with the types of conflict orientation and
irresponsibility we used previous tech.
The other thing is that even when we're not using tech for intentionally conflict oriented
purposes, all of the tech we use does externalize harm in different ways.
So whether we're talking about agricultural technology where the nitrogen fertilizer fed
a lot of people but also causes all the dead zones in the ocean and soil erosion and biodiversity
loss, exponentially more technology also means exponentially more externalities.
And so we can't handle exponential war and we can't handle exponential externalities.
So we have to change our relationship with technology really fundamentally and say no
other animal have the ability to destroy the biosphere that it depends upon.
We now do.
We did not for all of human history, so we didn't have to really wrestle with that power.
We did kill and enslave and genocide and every previous civilization doesn't still exist
because they all ended up collapsing mostly for reasons that were largely self-induced.
Even when wars happened, oftentimes a war that overtook a civilization was from an enemy
that was less powerful than ones that they had vanquished in their prime.
They had already went through some internal institutional decay from infighting and things
like that.
Many early civilizations died from environmentally induced causes.
They cut down all the trees.
They over stripped the soil of nutrients.
So civilizational breakdown is actually the norm.
It's just never been at a global level.
Now we don't live in the United States or China.
We live in a place where the cell phone that we're watching this on or the computer we're
watching it on took six continent supply chains to make communicating via satellites
so we live in a kind of global civilization where none of the countries are actually
autonomous for fundamental things that they need.
Now that we do have the ability to destroy the biosphere either very rapidly through
exponential technology like synthetic biology or AI or warfare or kind of slowly through
the limits of growth and environmental issues but that's not all that slow.
If you have the power to destroy the nature that you depend upon you have to consciously
steward it or you'll self-terminate.
So the gist is we don't have evolutionary capacities.
We have trans evolutionary capacities meaning-
What's the difference here?
Yeah.
So and I'm meaning evolution in a biologic evolution sense.
So another animal has the capacities that it has corporeally built into its body based
on its genes.
So a predator can't become radically more predatory quickly.
It is only through genetic mutation that maybe it becomes slightly faster or has slightly
bigger teeth and then it's going to be a relatively small change and then there will be co-selection.
The slightly more effective predator will eat the slightly slower preys which means
that the faster prey genes and breed and you get this kind of co-selective process.
We threw our ability to build tools and then tools on tools, recursive abstraction.
If you look at a true apex predator, you look at an orca in the ocean, an orca maybe can
catch one fish at a time, one tuna at a time, then you look at a trawling boat that has
a mile long drift net that can pull up 100,000 fish at once.
They're not apex predators, right?
Like it's wrong to think of us as apex predators.
We have power that is not encoded in our bodies, extra corporeal technological capacity.
You look at a nuclear bomb explosion versus a pissed off polar bear.
They're not similar levels of destructive capacity.
So since we have beyond evolutionary capacity, we actually have to have beyond evolutionary
motive to guide that capacity.
And if you want to say that mythopoetically, it's if you have the power of gods and by
gods here, like I mean little G, right?
I mean it mythopoetically meaning you can make species extinct.
You can destroy ecosystems.
You can create an Anthropocene where the largest effect on the geology of the planet is human
activity.
You can genetically engineer new species, right?
That's much closer to the power of gods than it is the power of an apex predator.
If you don't also have the love and wisdom of gods and prudence of gods to guide it,
it doesn't go well.
And so, you know, that is just another way of saying if you have recursive abstraction
on tools that gives us and tools and coordination that give us the radically more than evolutionary
capacity to affect the world, we have to move into trans evolutionary motive, which means
the same recursive abstraction that we're doing right now saying, oh yeah, I guess it
makes sense that we can't run an exponential financial system that's attached to a linear
materials economy that takes stuff out of nature faster than it can be replenished and
turns it into trash and pollution in nature faster than it can be processed.
You can't do that exponentially forever on a finite planet, so we have to do something
fundamentally different, which means you can't orient towards continued, maximized growth
and maximized conflict orientation forever.
So that's what I mean by a trans evolutionary motive.
Is it naive to think that we need a global government and we can make a global governance?
When you look at the problem of countries having competitive dynamics with each other
where nobody wants to price carbon properly, because if they do, their own economy will
be so damaged relative to whoever doesn't that the radically decreased geopolitical
power will express itself as less military power, less trade power, and particularly
with whoever is at the leading edge of guiding the world system.
This classic, the US isn't going to if China doesn't and vice versa, so then everyone is
mostly actually just in an economics race that is also bound to an actual arms race.
And that's true for pricing carbon and climate change.
It's also true for fishing of the oceans and aerosols and on and on and on.
So if you have an issue like the atmosphere, aerosols and the atmosphere and ozone layer,
or you have an issue like the oceans or climate change, no country can solve that problem.
And any country that does the thing that is doing its share that is economically disadvantaged
in the short term by doing it, it just isn't going to do that if everyone else doesn't
because they are caught in the competitive dynamics.
So when you look at that, you're like, all right, well, we need global government because
we have global issues.
We don't just have national issues and you have to have governance at the level that
you have issues.
But then of course, most thinking people aren't really a big fan of the idea of global government
because it's not a great idea to have unchecked power, though we don't have a good history
of being good stewards of unchecked power.
And so in many modern governments in the United States, it was kind of like the foundation
of the whole idea was let's separate church and state, let's separate the judicial branch
and the legislative branch and the executive branch.
Let's even separate the legislative branches in the separate houses.
Let's try to create as much check and balance on power as possible.
So if you had a one world government that had enough power to be able to price carbon
properly and enforce fishing laws and et cetera, how do you prevent it from becoming
corrupted or captured?
And so we need global government and we don't want global government and so that this is
this like you have catastrophes on one hand that need to be avoided and that typically
looks like more control mechanisms of things that if you don't control will lead to catastrophe
and the control mechanisms typically lead to dystopias.
So we want something that is not catastrophes or dystopias.
