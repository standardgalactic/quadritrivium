a framework like physical externalities and psychosocial externalities and getting a group
of people that represent different stakeholders that would be touched by the system and that
represent different expertise. And just say let's think through all the possible externalities based
on patterns of human use using this and based on the supply chains that it'll take to make this
thing happen. All of a sudden, we anticipate a lot of stuff we wouldn't have anticipated.
And then we say, well, if those things happen, what are the externalities of produce to get a
third order? Right. And so then we start thinking about that and saying, how could we design it
differently that internalize that. And similarly, thinking about something like the relationship
of infrastructure to social structure and superstructure, you get to start to think, well,
what are the effects of this on the systems of human governance and social structures? What are
the effects on the superstructures? And obviously, all of that is embedded within the biosphere.
What are the effects on the biosphere? So these are frameworks for being able to consider
externalities better. Then of course, there will be some things that you didn't predict. So you
want to create monitoring systems. And you also want to create tests, right? How do we test this
at a small scale, what we'd call a safe to fail probe? And Dave Snowden's terms. And
then be able to observe what are the things that happen, including stuff. Since we don't even know
what to predict, how do we make just very kind of wide observations and notice new stuff that starts
to emerge and then be able to factor that. But then of course, what we need is that let's say we
make a technology and it's off and running. And we did all of our good upfront assessment. We still
need to be watching what are the other things that it does. And we also even want to be incentivizing
and decentralized intelligence to show us unintended consequence, right? And then we'll need to figure
out how to solve that in terms of a recursive design of the technology or a design of the social
system, i.e. a law or something that will affect the utilization of the technology.
And then we have to actually be able to enact that. So you can't move from the tech designers to now
the CEO and board of directors have a fiduciary responsibility to maximize return on shareholders
and it doesn't matter if you create an externality, you don't ever get to change it. So you actually
have to build into the governance protocol of the thing, the ability for ongoing recursion that
internalizes extra points when they're found. But the underlying reason not to do that because
procedurally what I just said is not that hard. It's not obvious, but once you hear it, it's
kind of obvious. And it's not perfect, but it's like so many orders of magnitude better than what
we do. But then you're like, okay, well, the other issue is the conflict theory, the perverse
incentive. So let me explain the perverse incentive that is underneath much of the problem here. Is
that I'll specifically talk about market applications. There, there is much more incentive to focus on
the upsides of a technology than the risks. There's much more of an incentive for a technologist and
they funder and a lawmaker and a politician and whatever who can bind themselves to those upsides.
Right. So let's just say for the technologist, if I say, oh, we're going to make this AI thing and
it's going to like, do protein folding and solve all these diseases and do all this amazing stuff
and everybody wants it for those reasons. Yeah, well, what about all the risks of the really
terrible things? Oh, no, it's not going to do those terrible things. Of course, whoever focuses on that
moves faster, gets first mover advantage in the market, gets the race to network dynamics,
you know, a massive adoption of their thing. Whoever goes slower to say, hey, wait, we're
not sure when this protracted process of anticipating second and third order negative
effects and we found a bunch and we're internalizing those into the design. So we're iterating the
design a lot. Those people just get out competed by the other people who just rushed to market.
So the incentive is to not look for where it's going to harm things or to do a bullshit job
where you do some box ticking thing that says, oh, we did do diligence, but you really don't
want to know anything that's going to mess up the speed to market. And so this is where
things that are actually conflict theory gets a height as if they're mistake theory.
And this is where you have to factor the relationship between the those topics well,
but when you're thinking about new technologies in say the web three space, you want to be able
to think about what new types of conflict theory dynamics does this actually enable? Like, where
does it enable the people who are better at this technology to have more game theoretic ability over
the people who are less good at it? Where does having this type of spear or chainsaw incentivize
thinking in certain ways? What are the effects of those things? How can this increase or decrease
the underlying basis of conflict in the space? And of course, when we're trying to think of
governance tech, how do we make it to where there isn't this perverse incentive to not focus on
externalities? Well, you could do things like really tight attribution where everything in your
ecosystem actually does have to pay for the externalities that it finds that are found later.
And so it becomes catastrophically unprofitable, whereas right now the company gets to privatize
its gains and socialize almost all the losses. Could we make a system where the externalities
have tighter attribution or internalized? And so it actually affects the cost equation and the
incentive. We have a situation where you actually do regulation from something like a DAO first,
right? You create some new governance protocol. And before a new technology is rolled out,
the people actually have to see that its risk analysis was good enough. So it's not the technology
rolls out. And after we see that it ruined everything like DDT or cigarettes or whatever,
then we try to regulate after the fact, which once you're an exponential tech, it's too late to do
that shit. You can't let out self replicating biotech. And then after everything say, oh,
we should regulate that it's too late or AI. So we actually have like, say before tech gets
released into the market, is this safe? Did it actually pass enough, you know, appropriate process,
which of course happens in some areas the FDA is supposed to before getting authorization to,
you know, ensure oversight of drug safety. But that's not the case in like almost anything
in software for these purposes. But showing have even bigger effects on culture than any
particular drug would. Yeah. And we've got the same coming out of the valley,
move fast and break things. It's almost the exact opposite culture of what you're talking about.
Remember how we said that the dominant narrative is apologism for the power system?
Yeah. Move fast and break things. Oh, we're all, we're all about disruption and coming up with
innovative new stuff. And we're like fail and be these great innovators. It's just like what you're
talking about is having privatized a fuck ton of gains and socialized losses. Because what you
broke was the social contract of the entire fucking country and the epistemic comments and the ability
for anybody to do anything to solve climate change. Like if what you broke is the biosphere in the
country, no, that was not a good thing to do. That was really fucking stupid. Yeah. I think it's
really interesting to hear about the conflict theory, which is kind of like initiating harm
via conflict on purpose. Maybe there's a scarcity of resources and the mistake theory
creating harm via an externality of not knowing and then the plausible deniability
of really being in conflict theory and but treating it as mistake, mistake theory in order
to get away with it. Is there anything more to say about that in the intersection of what we've
talked about? Yeah, I would really love to see web three developers taking mistake theory super
seriously and saying what are the negative effects psychosocial, particularly effects of this
technology proliferating? How do we incorporate it into the design? And then where are the incentives
of people to overemphasize the positives and move fast and they might break things? Because where they
have an incentive to be focused on the upside, they are probably in conflict theory, they're
going to blow past preventing mistake theory they could have. And so you really want to look at
where do the incentives give people less orientation to pay attention to real risks,
second, third order consequences and risks. Where does the incentive landscape and deterrent
landscape where they'll be able to socialize the harms later or have plausible deniability,
they didn't really cause it or attribution issues. And so for the web three world to say,
let's really look at the incentives well. And make sure that there aren't ones that are fucking up
our ability to think through the effects. And then let's look at what are we, what are we really
trying to serve? Are we trying to serve a thriving biosphere and a thriving human society? If so,
we don't want to externalize harm anywhere. We know these technologies are going to be powerful.
How do we really think through what that power is doing? And where the tradeoffs are and whatever
is being benefited, what's being harmed? So how do we hold more procedural rigor in doing that?
And how do we identify where there's incentives not to do it to both try to change those incentives
and to know where to be dubious of certain claims? Right. So I mean, I guess, you know,
you and I are going to wrap this episode soon. But for episode three and episode four, we're
talking about how web three can help address the meta crisis. I'm almost imagining a scorecard
here in which we can take a couple of projects, hold them up to the prism of ways of what we've
developed here, looking at each project within the lens of conflict theory and mistake theory,
recognizing that tech is not values neutral, looking at the narratives that have been manufactured
by each project, looking at how they intersect with the infrastructure, social structure and
superstructure. And then also looking at whether or not these projects confer an adapted advantage
to their participants or at least not a disadvantage. It's kind of like my scorecard
for something that's going to work and is something that is going to help address the
meta crisis. But, you know, I don't know if there's anything to add to that list.
The scorecard involves things that are qualitative, right? Not just qualitative. So you
can't make the decision based on just expected value calculus where you've converted everything
to quantitative and then the same metrics. So that means actually something like human
wisdom is still at the center of the whole thing, earnestness and honesty and wisdom to be able
to factor. Because as soon as you get into the, okay, well, here's the trade off. And if we did
this thing, we can solve this problem, but it'll cause us other problem. Then I have to say
how many dead whales are worth how many tons of CO2 are worth how many abused children.
And if you just any version of doing that ends up turning into somewhere between nonsense and
evil. And this is one of the things I really want the web through community to get is that
you cannot convert the world into quantification. There's a lot of qualified stuff that cannot be
quantified. And there's a lot of quantified metrics that are uncommentable. And that if you make them
commensurable, you do it at a disadvantage to reality. And so what that means is there's a lot
of decisions that cannot be made in a metrified way, which means they cannot just be automated
and computed. They actually do require adjudication and discernment. And so how we actually grow
populations capable of holding that complexity and the qualified things and the uncomparable
things and making wise choices has to stay at the center of our focus. And so that scorecard
is not a metrified one, like there are some metrics, but it's, it's asking like, are they,
you know, where are the incentives? A lot of those will be squishy because some of the incentives
will be like status or, you know, whatever. And like, did they do the risk analysis? Well,
it's going to be, you'll get an index maybe, but you're not going to be able to do a perfect
quantification because how do you know what was not included that maybe should have been included?
And so a scorecard on process, was there earnest and deep and rigorous process?
Yes. And it won't be perfect, but it's directionally right. A pre-computable thing.
But considerations that we would like the Web 3 superstructure, the Web 3 culture to hold very
deeply, where people, like, one thing I'll say about the future is that if we are to have a
future where we make it through the metacrisis, we will be less focused on design systems of
extrinsic incentive and more motivated around intrinsic incentive associated with a deeper
connection to life. And if I am deeply connected to nature and to people, that connection informs
and embodied ethics, that intimacy informs and embodied ethics. And the systems of extrinsic
incentive are basically how do you control people based on their selfishness, because they don't
have ethics or to even override them. And so I actually want people to do shit based on incentive
and reward much less. And I would like to be able to enable people to have needs met and have
then a deeper existential development of what is actually meaningful and worth doing with our
short lives here. You know, it's interesting, the ETH Barcelona conference just happened and
they had every speaker come on stage and touch grass. They literally had grass up on stage,
which I guess grounded them a little bit of nature in the heart of the city.
Something. Yeah, it's a start. I'm also imagining this like, I'm in an imperfect vessel for this
conversation, because I only have a certain vantage point on even Gitcoin, like it's gotten so big that
I don't even know everything that's happening. And it almost feels like a distributed intelligence
question of in mapping all of the externalities, you almost have to look at it for the hyper
structure from all of the different vantage points of it happening. I mean, I think we can
take a stab at it in episodes three and four, but the real thing is empowering the everyday
citizens of the web three space to have these conversations and almost to create a hyper structure
that can like rate projects on their ability to solve them at a crisis and help them find the
externalities in like. This is why the diversity perspective is important. Yeah. Because
if nobody in there actually lives in an industrial zone,
like physically and knows what it's like to live in an industrial zone, or if nobody actually lives
in poverty or any of those things, and there's just types of experience they are completely
unclued to and they just can't academically clue into the reality of it in a way that anyone who
lives there can. And so, you know, the question of like, Hey, do you have people from other classes?
Do you have people from other races? Do you have people from all the genders? Do you have them
there as well? They have different experiences. And their experiences are like, Hey, you're
fucking designing civilization for who you're designing a thing that's going to affect a lot
of things through all the second, third order effects. Do you have everybody who's affected
by it weigh an in on it? And it's specifically what you just said is that they can anticipate
some of the externalities differently. Because they experience it differently,
because they're in a different part of the system. So if you think of sensors, if you
think of this as like a human internet of things sensor system, then of course, I want a wide
distribution of human sensors all giving the feedback of what they're sensing in the various
aspects of the system to design a better system. And so then, rather than so often we design a
thing, we take it as very precious, we don't want to criticize, we fight for it. No, fuck that,
like design a thing. And then ask everybody for what's wrong with it, knowing that what they're
offering you is the gift of how to make it better that you couldn't have seen. And then ask them,
how do I make it better? How do I factor these things? So let's say you start having red teaming
the design, not just the security, but the design from an externality point of view, identify all
the externalities here. And then that'd be something that is proceduralized and even
incentivized in a decent way. And then, okay, we got all these things. Now we're going to incentivize
anyone that can come up with solutions for them that don't cause other externalities in the process.
That's how you get a decentralized collective intelligence thinking about the right things
and actually synthesizing its intelligence. I guess a great place to end for this episode.
We've got some great conversations set up for episode three. Maybe one last question I'll ask
you before we break is, how should we select what projects we discussed for episode three?
I mean, obviously, I have a lot of knowledge of and experience with Web three on Ethereum
and Gitcoin is a project I founded, but it's now a McDowell. What kind of projects do you think we
should hold up to the lens in the next episode that we do? We can certainly pick ones that either
are some of the most prominent or possibly most promising ones or talking about some design
aspects that are pretty fundamental, which we can do. And if you want to open this up as you put
it out there for people to say, hey, what about this, projects or specific technologies or ideas,
anything that people are particularly interested in would be fun for us to have a look at.
Great. Yeah, I think accepting submissions from the crowd would be great. Is there anything I
didn't ask you, Daniel, that you want to say? Not in this time. I'm looking forward to next
time and this was a fun round two. Yeah. So enjoying the series that we're doing together.
You're so lucid. You're so articulate. You're many steps ahead thinking in so many different
things and I'm thankful for this collaboration with you. So thanks again, Daniel. I'm excited
about thinking about you having a whole community of people that can actually build stuff that are
incentivized to build stuff and actually care about things like public comments,
