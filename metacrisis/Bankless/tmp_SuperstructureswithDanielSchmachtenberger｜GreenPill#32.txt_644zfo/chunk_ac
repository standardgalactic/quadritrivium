It affects psychologies and cultures. So there's physical externalities. So if I look at my phone,
I'm going to hurt my neck. Psychosocial externalities, tech has an incentive to use it,
which confers an advantage, and then it becomes compulsory. And the other psychosocial
externality that I wrote down is body dysmorphia. So like outrages the Facebook example that it's
designed to keep me on site longer. And so that creates an algorithm that as a side effect creates
more outrage that polarizes the population. So for myself and the Web3 ecosystem as system
designers, we want to think about externalities, both physical and psychosocial, and think about
how to confer an advantage to systems that solve the metacrisis in order to make that kind of tech
advantage. Especially in a networked age, in a high tech age, where the question is existential,
sticks and stones may break my bones, but they're not going to blow up the world in a nuclear
catastrophe. So the how big the problem is has gotten bigger with bigger and bigger tech. So
to me, this leads to this question of like, what does the internet do? And what does Ethereum do?
How does it code minds? And how does it code cultures? And how can we be in conversation with
everyone in the system, both humans and nature, as we're designing a system that can build and
solve the metacrisis? And then of course, infrastructure, social structure and superstructure
are all intertwined. If we have values of building and funding digital public good,
solving coordination failures, those are the people who should be building this infrastructure.
Those are all the notes I wrote down. I don't know if there's anything I missed,
or is that directionally accurate? It was a good recap. Okay. Cool. So where should we go next?
I think next on my notes, I have conflict theory and mistake theory to get into next,
but I'm curious if there's another place you want to go? Yeah, let's come back to that.
I want to do one more piece regarding this unique category of tech that is the tech that
the aspect of the infrastructure that mediates the social structure. So notice, if we say,
what is the technology that enables Facebook? And obviously, whether we're talking Facebook or
TikTok or YouTube or whatever, there are certain things that are in common across all of those,
but it's not it's not video player tech. It's the ability to take my online behavior,
particularly when I'm on platform and record it, record my mouse uses and click patterns and stuff
like that, and then be able to have AI do pattern analysis for pattern prediction,
and then have it be able to give me split tested stuff to be able to upregulate its pattern
prediction, to be able to develop a predictive model of behavior that is fully personalized,
and then be able to put specific pieces of content in front of people to affect their
behavior in specific ways, all driven by a profit motive in which the user is not the customer.
The user is, you know, ad sales. And so the customer being engaged has been more time on
site engaged with the content more so that more ads come across them and that the right type of
ads come across them is what that data is being used for. And so we can see that, like,
of course, at the superstructure level, human belief and values and emotions and identity are
going to be conditioned by what we're paying attention to. You can't pay attention to stuff
and have it have no effect on the nature of your mind and in our experience. And this is why, you
know, every culture has some version of a statement like you become the average of the five people
you spend the most time around and the mimesis of learning, we learn language simply by watching,
we also learn belief systems and all those things by watching what we're surrounded by.
So if I can't observe the world because there's too much of the world, so now I'm observing
a subset of the world that is customized for me, then I am, I can't not take as the world,
right, because it's what I'm actually taking in. And it happens to be being customized for me
based on things that will commodify me. And it happens to be that my emotions commodify me
better than my rationality. So it will appeal on those things. And my identity being part of an
in group will commodify me better. That the design of that tech, meaning the incentive
economic system plus the way that suite of technologies are used is changing culture.
It can't not, it can't not change culture. And the one thing I'll add here is that
we're talking about a hyper supercomputer that is trained on hundreds of millions of data sets,
and it's pointed at our brains when we're using these, so we're way outgunned, it's like a hot
knife through butter and changing our attention. Yeah, we already were saying as far as the kind
of like voluntary as an idea, oh, it's the market, so you don't have to buy the thing,
it's just a gibberish argument. One, it's like saying,
do you have to use dollars? Well, no, I could try to just barter, right, like,
but I'm going to be so radically disadvantaged that I am since there's a monopoly on the
creation of currency, I pretty much have to fucking participate with that monopoly if I want
to be effective at all when like, you know, when like, do I have to use the telephone lines when
there was AT&T ran all the telephones? It's like, pretty much I do. If I could sort of say that
I am voluntarily engaging, if I don't have an authentic choice to still be
viable in society, if a small business doesn't advertise on any of the major platforms,
it's pretty fucking hard to be a viable small business, right? And then as we were mentioning
with the plow case, if any civilization starts making, if any other country starts making AI
weapons, do we really have a choice not to? Yeah, not really, right, we're probably going to do that
thing, because otherwise, we kind of obliquely lose. So it's important to understand that the
voluntary or voluntaryism argument is there's a lot of places where we pretend it's more true than
it is. So if tech confers a bunch of advantage, some people will start using it, and then that
will be used for conflict theory, and then it does require the other people to either use that or
some comparable thing, or they start mattering. And so we come back to the Facebook thing you were
saying, you know, this is this huge AI supercomputer directed at our brains. Now, we also have a
situation that one, we don't really have the capacity to choose to not engage with the whole
suite of those technologies and still be effective at certain domains of society.
And then the other thing is that it's like saying, well, it's voluntary whether or not we engage with
ads and whether we purchase a thing. I don't actually get a choice in those environments
of whether or not I'm exposed to them, nor do I get a choice of if my personal data is harvested
and used with AI dynamics to compel me to do that thing. And the asymmetry of it is so fucking much
that it's like, okay, this is why we wrote in that paper about the topic of undue influence. Now,
that was actually a different paper on the Consent's Preserve where we talked about that
undue influence of like someone is choosing something, but they're under some type of
cult mind control, or like why it's not okay for a professor or a therapist or a priest to
be in a relationship with someone is the other person really, there's so much power asymmetry,
they can't consent adequately that even if they're consenting, the asymmetry of power messes up the
integrity of their consent. We'll link that article in the show notes, I just found it,
put it in there. So we would propose that if information about you that enables from your
behavior, that enables a platform to predict your behavior better than your wife or therapist or
lawyer could, that that is privileged information, that that does enable undue influence, that that
needs to be in a fiduciary contract, a principal agent binding contract, or whoever's using that
is using it for you, not in any way against you. And so that would mean that those tech platforms
had to actually have a fiduciary relationship with you, the user to only use that information
aligned with your interests, where you had full visibility in settings and could change all of
it. That would of course require a change of business model, probably where you were the
customer or the state that is representing you or the commons that is represented is.
So as we get into tech that is more powerful, the asymmetry of those who are using it relative
to everyone else also becomes a bigger deal. And that affects the nature of social systems
radically is when there's such radical asymmetry of information processing,
can you really still just say buyer beware when the buyer can't begin to deal with weighting
all of the things that the other side that's using an AI and we're not
can factor? Right.
Does this get into to sort of like the political theory in such a way, at least with the Facebook
example where Facebook's management is optimizing for the shareholders instead of the customers.
And so it's optimizing for the customers as well as just the customers or the advertisers,
not the users. Right. That's actually a point is that the users should be the customers.
Well, I guess what I'm getting at is like, there's this idea in political theory that the
consent of the governed is the only legitimate basis for governance, as opposed to like the
divine right of Kings. And what if the Facebook users were owners of the platform and had,
you know, like management had to sort of like optimize for them in a little bit more
of like you said, a fiduciary obligation kind of way.
Otherwise, the people are actually the thing being extracted from like we used to extract from
nature. Now we're extracting from human minds, the particular kinds of behavioral predispositions
we want for the extension of power. But should Facebook have the customer, the user be a customer,
yes, but should it go further? Should it actually be a public good? And that if it is,
if it has that capacity to affect human behavior at scale, what the fuck should drive that?
Right. And this, well, and should everyone, should the consent of the governed have some say
in the governance of a thing that is going to have AI's pointed at their mind?
And so then you're like, Oh, well, it should be a DAO of some kind. And then you're like,
well, fuck, isn't that what the government should be? And now that it's not 1776 technology with
a town hall, but can we actually similarly have a process? Because when you say the consent of
the governed, what you're saying is that super structure needs to be the basis of social structure.
Right. The social structure law specifically, if it is not an instantiation of the will of
the people, it will be tyrannical. For it to be an instantiation of the will of the people,
then the thing that really matters is not only how do we actually listen to what the values of
the people are, but how do we make sure we're developing people that have better sense making
and deeper and richer meaning making? So that the, because the will of a very uneducated xenophobic
audience is not a great thing, which is actually why Kings made sense in low development environments.
So how do we develop super structure to have an increasingly considerate population? And then
how do we have that creating the laws that both utilize and also bind, guide, and direct the
technology? And I would say what's happened is we've got exponential curves in infrastructure
during the digital age, but we have not got exponentially faster governance or exponentially
faster growth of morals, virtues, and epistemology and culture. And as a result, the effects of
infrastructure influencing the other ones have radically increased and the effects of the integrity
of the social structure influencing or of the superstructure, the culture influencing the
social structures to bind the tech has radically decreased and got hollowed out. And then I would
say it's that direction from the values of the people from the superstructure to a healthy social
structure that has the oodalooops that has the capacity itself to bind, guide, direct the technology
to be in service of what the superstructure wants it in service to what the collective will of the
people does. That is the critical direction that we must increase to make it through metacrisis.
So that means we have to be both thinking about human values and how we support the proliferation
of the development of human values, how we encode those in our decision making systems,
and how we make sure that our tech is not only bound by those decision making systems,
but that the use of the tech that the psychosocial effects of it are in turn in service of an
enrichment of structure. Immutable X is the Layer 2 platform for crypto gaming. Immutable
offers massive scalability with up to 9,000 transactions per second and instant transaction
confirmation. No more gas fees, no more waiting around for your transaction to clear. Immutable's
zero knowledge roll up finally unlocks the world of crypto gaming. Immutable X is the only gas-free
NFT minting platform with over 26 million NFTs minted all with zero gas fees. With the power
of immutable gaming developers don't also need to become smart contract developers, they just need
to plug into Immutable's API and instantly start unlocking the full potential of crypto assets
inside of games. This is why world-class companies and projects have decided to deploy on Immutable X
like Gamestop, Ember Sword, Planet Quest, Illuvium, TikTok, and many more behind the scenes. So start
building your game on Immutable X today at immutable.com.
Coinshift is a leading treasury management and infrastructure platform for DAOs and crypto
businesses who need to manage their treasury operations. Every crypto organization manages
their treasury and Coinshift offers a simple, flexible, and efficient multi-chain treasury
management platform built on top of the extremely secure NOSA safe. With Coinshift, your organization
can go from primitive single-chain treasuries to expressive, flexible, multi-change features such
as global user management, global contracts, proposal management, and many other features
that can be chaired across an entire organization. Coinshift layers on powerful treasury management
tools on top of the proven security of NOSA safe, allowing users to save time and reduce
operational burdens and gas costs. Coinshift even has data tools like account reporting across the
seven chains on which it operates. Used by industry powerhouses such as Uniswap grants,
Balancer, Consensus, and Misari, Coinshift is speeding up the coordination and efficiency
of the organizations that use it. In DeFi, you have to keep up with the frontier and Coinshift
to make that easy. So sign up at coinshift.xyz slash bankless.
So let's go into conflict theory and mistake theory next. Curious to pull that thread?
Yeah. So we're talking about externalities. And typically, we think of an externality as a
mistake. We didn't intend the laptop to hurt people's necks. Like that wasn't a built-in thing.
It was just an unforeseen consequence or something that we, yeah. And similarly, nobody
intended climate change. They intended to build their businesses and their businesses required
energy and the cumulative effect of everybody doing that caused climate change. So we think this is
not like an intentionally created problem. It's a problem that is a byproduct of solving some
other problem. But obviously, there are places where somebody causes harm because they want to
invade that area and get access to those materials or that port or that whatever it is. So whether
they are initiating conflict on purpose or at least they know that it'll cause harm and they
don't care, right? Maybe it's the intent or they don't care. So there's a blog post on the
less wrong blog, I think with this title conflict versus mistake theory. And it's a subset of a
very large conversation and kind of social theory about what percentage of the problems in the world
are the result of simply not being able to factor all of the second and third order consequences
and what percentage of it are the result of things that are being initiated knowingly.
I think the article does a good job of discussing considerations on both sides, but the thread is
particularly interesting of comments afterwards. Michael Vasser, who's a very interesting thinker
in there specifically goes into that the existence of mistake theory ends up being used as a cover
for a lot of things that are actually conflict theory. Meaning that we can knowing that I can
say later, oh, that was an unintended consequence, we could have never predicted it. I can hide the
fact that I actually knew that thing was going to happen, or at minimum not try really hard to
figure it out, because it's again my incentive to try to figure it out. So then I have a mistake
theory as plausible deniability that I didn't intentionally do the fucked up thing. And so I
think when we're thinking about tech design and social systems, we want to think about conflict
theory. What is the underlying basis of what gets humans into conflict with each other, individual
humans, groups of humans, and humans as a whole with the environment? And how do we resolve the
underlying basis of conflict? And obviously, economics is very core to that. It's not the whole
of it. Marx has his analysis of conflict theory, Gerard has his analysis. If anyone wants to solve
any of the problems of the world, this one of the things to go deep on is what are the various
things that initiate human conflict, and what would it take to create a system that addressed
those more fundamentally? Not just conflict mediation, but conflict generation mitigation,
how to mitigate that. And because the mistake theory part, we can do a much better job procedurally
of that if we had the incentives, meaning if we had the motivation, incentives and deterrence
and intrinsic values. So can we predict all of the effects of a new tech? No, of course not.
Can we do a much better job than we've ever done? Easily. And so can we predict all the
effects is the same as predicting weather a ways out in complex systems, you end up
not being able to predict beyond a certain point. But for instance, just even thinking through with
