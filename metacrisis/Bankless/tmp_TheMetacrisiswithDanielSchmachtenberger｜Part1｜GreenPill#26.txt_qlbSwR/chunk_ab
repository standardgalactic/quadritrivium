to get, right? Like, okay, there's some very deep things about our economy that have to change.
So the catastrophes direction is very decentralized. And as tech gets more powerful,
there's this idea that we like of like, oh, exponential tech will like democratize everything.
And democratize everything sounds nice superficially, but the democratization or
giving to everyone catastrophic capabilities, not such a nice idea, right? Like, it's kind of nice
that only the G8 have nukes, it'd be kind of nice if no one had nukes. And if anyone has nukes,
it would be nice that it's not that many people they can monitor each other and do mutually
assured destruction. It's very hard to make nukes, right? There's not that many places
that had uranium enriching uranium is really hard, you can see it being done from satellites.
When we start talking about cyber weapons, that are pretty fucking easy, like small groups of
hackers can do in their basement, but then you can take out infrastructure targets,
they can kill a lot of people, right, mess up a lot of stuff that non state actors can use in
their basement with no exotic materials. That's really tricky. The same with drone weapons, the
same with tabletop crisper, which is coming about the same with GPT three, and more advanced AI that
can make Turing test passing propaganda ubiquitously in the next few years. So decentralized catastrophe
weapons for everyone, not such a good thing, right? Now, how do you prevent that stuff? If people can
make catastrophe weapons in their basement, the first obvious answer for how to prevent it is,
well, you have to know what people are doing in their basement. So surveillance state could prevent
it. And you kind of make sense to have surveillance. If someone can do something so catastrophic,
you don't want to wait till they do it, right? You wait till somebody shoots up the school,
it's really a bummer. But as you go from a knife to a gun to an AR 15, as you go from that to
tabletop crisper, it's way worse. And you kind of can't wait for people to do it,
you got to be able to preempt it. Okay, well, how do you make a surveillance state that can
know what everybody's doing in their basement that is not susceptible to capture or corruption
becoming a totally fucking legal dystopia? And similarly, how do you control all of the markets
to not do it's in market incentive without something that looks like global government?
Because again, if US wants to or Europe wants to price carbon properly, they will price themselves
out of existence relative to China or India if they don't do it also. And if somebody says we
won't make the AI weapon and the other one does, they win. So in order to really be able to solve
those problems, you need something like international agreement, which only means something if you
have international enforcement has that much power. And how is that okay? And so it's like on
one side, we really don't want global government because we don't want dystopias. On the other
side, we really want global government because we don't want multipolar traps. So this is where
the two attractors are catastrophes and dystopias because typically make to prevent the catastrophe
requires a enforced control structure and the control structure that has enough monopoly of
violence or monopoly of enactment or enough power to keep this happening. How do you create checks
and balances on it? So those are the two attractors. So when we talk about a third attractor, it's how
do we make a world that avoids all the catastrophes and isn't dystopic, meaning that the control
structures that prevent the catastrophes have checks and balances on themselves that prevent
them from being corrupt. And so whatever world we want is definitely in that third attractor.
I don't think anybody is like, I want the catastrophes or the dystopias model. So it's
pretty easy to agree on. We want something not those two as a starting place.
Right. So what I heard is that like, you can almost imagine the third attractor,
like in order to define the third attractor, you need to define the first two attractors.
And you can kind of think of these as just like general directions like waters. I'm in Colorado,
so I'll just use the Rocky Mountains, like water going on the west side, the west slope of the
mountains versus the east sides. Those are the two attractors of the water basins in Colorado.
We're talking about a game theoretic incentive gradient in which we have two attractors. The first
attractor is societies moving towards catastrophes, which is having those systemic metacrisis risks
that we talked about earlier. And then increasing dystopias, which is kind of like centralized
control. I want to say China here, but I'm not sure if you'd agree. So like the two axes are
how well are we solving the metacrisis with coordination. And then the other axis is how
centralized and corrupt is the system of surveillance. And we're looking for a third attractor that
allows solving of the metacrisis without dystopic surveillance and governance. Is that fair?
Yes. Okay. And if people haven't watched it, there's a really great YouTube video. It's about
18 minutes from a guy named CDP Gray called Rules for Rulers. And first to just go before that,
if people have not read it, there is a article called Meditations on Molok on Slate Star Codex.
Meditations on Molok is kind of the best article out there describing multi polar traps. So the
thing that we have like a decentralized coordination failure. And I'll go ahead and just try to state
it succinctly because the catastrophes are largely the result of this kind of Molok multi
polar trap side, which is where you have a number of different agents where each agent
doing what is in their own rational like authentic rational best interest in the short term
leads to collective behavior that is in everybody's worst interest in the long term.
And yet where nobody can make the good long-term choice because they lose in the midterm if they
do. So what I think you call a really adaptive tip becomes obligatory because if you want to
keep up in the economy, you have to adapt the sort of defection in order to keep up with everyone
else. Whether it's if they start, if somebody else starts cutting all the trees down and we
don't have some enforcement to stop them, then me not cutting the trees down doesn't protect the
forest. It just means that they get all the advantage that they'll use in war against us. So now we'll
just race to cut the trees down faster or the climate change or the whales or the whatever.
That's a tragedy of the commons. We solve those nationally with a monopoly of violence and law,
so we can say, no, you're allowed to log these areas, but not the national parks. And if you try
to log the national parks, we can send a police force that has enough monopoly of violence that
even if your loggers bring some guns out, they're not going to. But without a monopoly of violence
and rule of law, you can't really solve that thing. And so within a country, law is a way to try to
solve a multipolar trap by centralized capacity, right? Then you start to worry about, oh, does
that centralized capacity get corrupt? Does it actually do the will of the people democratically
or does it get corrupted? Now we get kind of dystopic police state things. And so we can see this
teetering between those. Now, as soon as we deal with international issues, there is no ability to
do rigorous enforcement because now the police state, since you don't have a police state across,
it looks like a war. So we can do sanctions and stuff. But at a certain point, it's like,
the war is going to hit us back hard enough because we don't have monopolistic power that we
have a deterrent against giving them the deterrent. And so, and especially if the other guy has nukes.
So let's say China doesn't want to tax carbon properly because it would hurt their economic
growth too badly. So we really can't because otherwise economically we're fucked. So we say,
okay, well, we're going to do it and we're going to force you to do it. Well, how are we going to
force them? If it's, we can tariff or whatever. But if they're like, no, ultimately, we're just
going to do this and we're going to win. Are we going to bomb them? If they also have nukes? So
this is where the tragedy of the commons gets tricky, right? There's a lot of small scale
tragedy of the commons where Eleanor Ostrom or like local law stuff works. But when you get large
scale with a lot of power invested in it, it gets much trickier. And the arms race, if anybody
builds the weapons and we don't know if they are not because they could be building them in an
underground base and who knows? We have to assume they are and we'll just build them anyways. We
have to race to get there faster. And the market arms race, they're going to get first mover
advantage. If they get first mover advantage, they'll win everything because you're going to get,
especially in tech, that has network dynamics. Whoever network dynamics in Metcast law lead
to natural monopolies, right? You end up getting one Google that's bigger than every other search
engine. You get one Amazon that's bigger than all the other online stores, one Facebook that's
bigger than the other social media. As a result, whoever gets to take away effects, the runway
effects on Metcast law is going to win. So you have the incentive to not avoid externalities. So
just fucking go for it as fast and aggressively as you can. Silicon Valley's move quickly and break
things deal with it later. So anyone who tries to be thoughtful about externalities move slower
just loses. So it's maladaptive to want to solve this problem. So it's maladaptive because ultimately
there is a perverse game theory that orients towards people who are more focused on upside than
avoiding risk. Because if I'm focused on selling the benefits of the AI or the CRISPR or the whatever
and not the risk heavily, then I'll get first mover advantage and I'll win. And all of the risks,
all the externalities, I won't have to pay for my company to have limited liability protection and
I'll socialize the losses. So Facebook didn't have to pay for breaking democracy and polarizing
the left and right and whatever, but it got to privatize trillions of dollars of gain.
And so the incentive is to not take risks seriously. And this is why we actually need
laws because the market does not incentivize long-term thinking appropriately in a lot of these areas.
So Meditations on MOLOC, multi-polar trap, that gives you that side. But the rules for rulers
is another version of the same phenomena, but for centralized coordination, like centralized power
system corruption rather than decentralized poor coordination. And it basically says you can't
really have enlightened dictators long-term. Because let's say to avoid that multi-polar
trap situation, you do create a centralized authority that has a monopoly of violence or
monopoly of enactment. And you put a really enlightened dictator or an enlightened small group
process in there, in oligarchy or whatever. Well, if there are some malevolent people who really
want that position of power and they're willing to do malevolent corruption to get there, they'll
probably win because if they're not constrained by values, they will have more possibilities
available. So for the enlightened dictator to stay in place, they have to be able to deal with the
malevolence aimed at them, which makes them less benevolent and more malevolent. So there,
the race for who can get in the position of power ends up having a corrupting influence. That's what
the ruler thing shares. And so this is why you're like, okay, well, power tends to get corrupt.
This way we don't want a one-rule government. We want something to check it. But if you don't
have a one-rule government, you get this, how do you stop arms races and global tragedy of the
comments? So people should definitely read Mentations on Wallach. Watch rules for rulers
and say, okay, the third attractor has to solve both of those.
Right. So at this point, listeners will probably know, but you may not know this,
that in the 2019 crypto bear market, Meditations on Wallach went viral. It was something that
really meant a lot to us in the Ethereum ecosystem as coordination failures were sort of
given this prominence as the highest thing that the Web3 ecosystem could solve. And there was
actually a DAO that was launched called Bullock DAO that this sort of interesting meant to solve
coordination failure. So I think most of the audience is probably really familiar with Meditations
on Wallach. And we'll put links to that and rules for rulers in the show notes so that they can go
check that out. I don't know the people at Mollach DAO or what it's doing. It's kind of exactly
the wrong name, but like, I get it. Yeah. Yeah, I could see that. Really what you wanted to,
I think Yula was the name of the God of art and softness and all of the kind things in the Meditations
on Wallach piece. And Wallach is the God of coordination failures. They named it after the
thing that they were trying to slay in a funny sort of way. So the tricky thing that's very
interesting is it's like, let's take a small personal example of this large thing. Small
personal example is if someone has grown up with ideals about kindness and honesty and virtue and
turning the other cheek and redemption for everyone and things like that, not everyone did
grow up. A lot of people grew up with just a lot of trauma and shit. But let's say somebody did grow
up. They really try to enact that. Awesome. At a certain point, if they have enough success
and they get enough power, then they become a tractor, a honey trap for people who do not
have those values and go, oh, that looks like a sucker that I can take advantage of. And then
they realize that there's a lot of dark triad personality disorders and pretty fucked up.
Then the naive ideals to cynicism. The cynicism is like, man, the no good deed goes unpunished
and the people that you trust the most end up fucking you and whatever. Well, that's not a
reasonable place to go or to stay. But you also can't go back to naive ideals. You have to move
to this like post cynical, right? There's naive and like a post cynical place that is focused on
how do those virtues actually get enacted knowing that there are some people who don't share them
and will even try to game and play them. So how can I be aware of people who are willing to be
bad actors? Do I deal with that? Well, still working to be a good actor, find other good actors,
coordinate with them and even help the bad actors like help there be more incentive for people to
become good actors, not bad actors. How do I ensure that my life energy is working on the world
moving towards good actors for everyone? Effective requires not being fucked by bad actors. In the
meantime, the reason I bring this up is because when people think about what is the ideal world
they want to create, it doesn't include other people badly motivated trying to fuck it, right?
And it's a world where everyone cares about the comments and cares about each other and are kind
of like virtuously motivated and honest. The transition from this world where people have
been incented not to be that thing and traumatized and conditioned to get ahead game theoretically.
The transition from this world to that world is different than trying to just win at this world,
which will just keep being in service of Moloch. But it's also different than being in a world
where you don't have to protect against Moloch because it's already been defeated.
You're like, okay, I am very committed to not being in service of Moloch. I'm also committed
to not being naively just duped by Moloch and beaten. So that means that I have to be
game theoretically aware and I have to be able to build things that have power,
but they're ultimately really authentically in service to transcending that.
So I just want to, I've been sort of diagramming a little bit as we've been chatting and I just
want to get your reaction to this little diagram that I made right here. So I've got a two by two
matrix on the screen and basically the x-axis is how well are we solving the meta crisis
coordination failures. The top end of this is really well and then the bottom axis is not really
well. And then the other part of the two by two matrix, the y-axis is how centralized and corrupt
is the system of governance and surveillance. And then the bottom left, where we're not solving
the coordination failures, but we've got maybe decentralized, something decentralized or maybe
laissez-faire, you've got catastrophes, which we've learned about the coordination failures that
are created there because of meditations on Molek. In the top right quadrant, we've got dystopias
and those are centralized, but they may have solved the coordination failures and rules for
rulers are kind of the, what we're looking for there. The third attractor is trying to solve for
solving coordination failures, but without a centralized corrupt system of governance. And
so the problems that we have to solve there are how do we solve coordination failures in
slay Molek, but how do we up-level the actors that are good actors and not get fucked by the bad
actors? I'm a pretty visual person and people who are listening to this in audio are just going to
have to forgive me, but I'm curious if this two by two matrix resonates with you at all, Daniel.
Yeah, it's really good. I like it. The one thing I would change about it is, and this is good that
you put it there, is the x-axis, it says how well are we solving for meta crisis coordination failures.
I would not define the meta crisis as the catastrophes. I would define the meta crisis
as being stuck between catastrophes and dystopias where our increasing tech and exponential tech
makes both catastrophes and dystopias more likely. So on the bottom x-axis, I would say,
how well are we solving for catastrophes? I see. And how well we prevent both catastrophes
and dystopias or corruption is how well we're solving the meta crisis.
Right. So the meta crisis is... Yes, correct. Metacrisis is angled like you're doing there,
visually. Okay. And then what that means is your bottom left corner is catastrophes,
your upper right is dystopias, your bottom right is we have a third attractor, your upper left,
which would be the worst of everything, is the one that we're currently doing.
And that's actually worth noting, which is increasing catastrophes and increasing dystopias
at the same time. So let me explain that quickly, because that's actually the most probable thing,
which is you don't get an AGI that kills everything right away and you don't get strategic
