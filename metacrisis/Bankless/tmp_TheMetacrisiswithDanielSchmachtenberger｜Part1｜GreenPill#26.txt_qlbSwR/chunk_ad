We can press rewind and watch the entire thing. Well, that helps multipolar traps, right? Because
one of the things that makes multipolar traps worse, as you know, in the prisoner's dilemma,
it's that it's a lack of communication that makes coordination. If you can't know what the other
actor is doing, everyone has to assume the worst thing and then just race to do the worst thing.
If you can know what they're doing, right? So if you can force transparency and remove the
opaqueness advantage, then you can make better coordination, right? You can make better Nash
equilibrium. That's one example of many examples. And obviously, blockchain is very oriented to
forced transparency being a net positive thing, right? I think there are many examples of
forced transparency because of tech combined with new solutions where transparency wins
over opaqueness. Obviously, you look between militaries and they try to make all of their
military stuff national security secrets and classified so they can have a upper hand in a
military conflict. So there is strategic advantage on asymmetry of information.
But there's downside too. The downside is you start getting to a place where
everything is classified and or so much of us classified because of national security,
that the integrity of democracy is totally broken down because the population can't actually
know what's going on because if the population knows then China or Russia or whatever can know.
And therefore, the faith in government goes down. Everybody's afraid of corruption,
but they say it's the other side. It gets more partisan. Like the classification is part of
the breakdown of the democratic system, which means that it's driving autocracy movement towards
autocracy. The other thing is that the classification becomes compartmentalized and so you end up
having different departments duplicating each other's shit because they don't know and nobody
being able to do adequate intel across all of them and even some of them competing for who gets a
larger percentage of the black budget. If you could make them transparent and get enough benefit
from the transparency because you can use machine learning to process the data across them and then
help optimize coordination across them, then even if you give up some of your asymmetry of
advantage, asymmetric information advantage, you get new advantages of not having to invest all of
that energy and opaqueness, giving more buy-in of the population and less breakdown of democracy
and way more efficiency, which is what the Swedish government does. And it's actually
prototyped as working in a major nation with a major military. So if you take forced transparency
and you take solutions that make transparency win, can you start to drive increases in transparency
where the game theory orients a race to the top rather than a race to the bottom? Yes.
That's an example. Beautiful. Now, I can give you another example.
Technological automation, both because of robots and AI, portend a loss of enough jobs that the
capitalism is not a viable solution for humanity in the next pretty soon period of time,
whether it's 10 years or 30 years, whatever, it's soon.
Okay. So either this becomes dystopic as can be because at least in feudalism,
you needed people's labor. And so the people couldn't be too terribly treated because you
didn't want violent revolts. If you have a situation where the elites don't even need
the people's labor, they're just useless eaters, that could get really bad.
And the robots and AI can do stuff faster. And so how do we have a situation in which the market
will choose the robots and the AI because they're fucking faster and cheaper? How do we have a
situation in which that is occurring that isn't dystopic? That's a really important question.
And if there's a very deep existential question, if robots are better than us at certain things,
and AI are better than us at certain things, what are we still better at that is what we
should be developing humans to do and or what is intrinsically meaningful to humans?
And what you end up getting is like outside of full AGI, sentient AGI,
things that involve authentic connection are things that you're going to want humans to do
rather than automate. Could we automate? And most of the things that you could automate,
if people didn't need the jobs and money, they'd rather not spend 40 hours every week doing anyways,
because if it's automatable, it's fairly rote and it's not the most engaging thing.
And yet high connection things are intrinsically rewarding, right? Like evolution rewarded high
connection things. So could we have a situation where a much higher percentage of the entire
population are educators? Like one in four people as opposed to one in 40 or whatever,
and a much higher percentage are nurses, but the nurses are trained as much as doctors.
Because, and doctors similarly, doctors aren't going to do the surgeries anymore, the DaVinci
robots, DaVinci machine type robots are going to do, they're going to do a lot of the diagnostics,
so then the role of the nurse and doctor become much more the interface with the AI and the
interface with the patient, it's extremely high touch, extremely high care that gets all the
qualitative stuff that the quantitative thing can't get. Same with the educator, we don't get to have
the world-class minds become educators because they just doesn't work, they go into other areas,
but you can't grow world-class minds with teachers who are not world-class thinkers, right? So
there's a study that showed that the highest statistical correlator for people who became
world-class mathematicians was that they trained with a world-class mathematician young.
Oh, wow. So it's like fractal.
You're not going to learn to think like a world-class mathematician from someone who
doesn't think that way, right? It's like if you're talking to a field's award winner versus your
high school math teacher, they don't think about math the same way, right? And so your brain learning
math is going to learn it, in your mind, is going to learn it really differently,
which is why the aristocratic tutoring model is the thing most known to correlate with super genius,
which was the aristocracy were able to hire the smartest people in the whole land to tutor their
kids. This is why Marcus Aurelius' first chapter of meditations was just dedicated to his tutors,
is when you're, is to be the emperor of Rome, the best mathematician and
philosopher and grammarian are your teachers. Of course, you're going to be way the fuck smarter.
So now this is another question about hope. Can we make a world where GPT-3 can already,
and so now imagine GPT-12, right, can already not just pass the Turing test as a chatbot pretty
well, but do it in the voice of specific people. I can train GPT-3 on Thomas Jefferson enough that
it can answer like Thomas Jefferson. Well, now that gets way better, way more data in it. You
metaverse with that up. Can the kid have Socrates and Confucius and Einstein and von Neumann and
Gertl all as tutors? Yes, that's fucking amazing, right? But also can the kid have real human tutors
who are brilliant and care about them because the AI one doesn't care about them,
or the real human tutors talking to them saying, what do you think is different,
what the AI Einstein said versus the real Einstein might have said, and to be able to
really talk with them about that, they have to say, well, what do you think is different between
artificial intelligence and organic human intelligence? What is intelligence? What is,
what do you think is different about the nature of self-organizing systems versus
design and built systems? The recursion on thinking about what is intelligence and
organic intelligence and sentience will start to lead those qualities way the fuck up. So now
the kid gets a level of human tutoring that is as good as Marcus Aurelius had, but could be
democratized, right? Everybody could have it because of a new economic system,
a level of this AI tutoring thing that nobody ever had. And when we go back and look at
most of the great polymaths, they have this, right? Even all the way down to Einstein and von
Neumann as the last ones had mathematician governances when they were young, the remnant of
the European system. What hope does it bring for me, for humanity? And the Dalai Lama's tutoring,
right? Like you get somebody like the Dalai Lama when you have that kind of tutoring system of the
very best Tibetan monks tutoring you intensely from the time you're tiny. If everybody had that
educational system, and the goal of education was not to prepare you for the market that the robots
in the Azure doing, but was to develop in you what is uniquely human capacities, not automatable
capacities, and then also your unique aptitudes. And you have a economic system that can incent you
to be able to or support you, where it's not even about incense, but support you to do what
you're intrinsically incented to do deeply and really well trained to do. That's really fucking
inspiring. And that would, these same technologies that portend really terrible dystopias, way worse
dystopias than ever before, because dystopias had a limit of how controlling they could be
because the people would revolt. AI-empowered, drone-empowered dystopias are really, really bad,
right? And AI catastrophes are really bad. But AI third attractor-empowered, which is not
disintermediating human intelligence, but facilitating it, not disintermediating human
connection, but facilitating it. And this one of the key things is that all of the tech has to
facilitate what is uniquely human rather than debase or disintermediated. Your online world
should be helping your offline world be richer, not the other way around. Online relationship
should be enriching offline relationships. Anything that's an emergent property of some
more fundamental thing needs to be enriching the fundamental thing that it depends on, right?
That's a general design principle of good tech. Now, these are a couple examples of like
really inspiring tech. I can also give examples of how we can use AI to make democracy actually
work because you can't fit everybody in a town hall. Does the internet make it possible to fit
everybody in a town hall and where we can use GPT-3 like semantic analysis to be able to get
to see the distribution of everybody's values and stack ranking on everybody's values and to then
be able to take all of those values as constraints of proposition crafting and then also help craft
propositions that are better synergistic satisfiers with less unnecessary theory of trade-offs?
Totally. This is not an AI singleton disintermediating humans. This is AI being able to help the scale
issue of collective intelligence. It's like a high resolution democracy. It could be really powerful.
We have to build this. The third attractor needs this, right? It's a decentralized collective
intelligence with central enactment capability, but where the central enactment capability
comes from a bottom-up system that is not captureable, not creptable.
These are a few examples, and I could give a heap more in energy and manufacturing and
waste management, whatever, but these ones are on coordination of things that I find
really hopeful, but that we don't have the incentives to be building them that way yet
because we're starting multi-polar traps, which is why this podcast and stuff like
this is important to me is because more people building things like this is really important.
Next part. What brings me hope at the level of human nature is important.
The real politic assessment, that humans are just too irrational and just too kind of rivalrous
and nasty to be able to make a nice world until we just need an AI singleton or we just need
something like that. It's important to understand the real politic assessment,
but it's also totally gibberish in that it doesn't factor development. If you study
developmental psych, there are higher levels of cognitive capacity, higher levels of
moral capacity, work of Colberg and Lovinger and things like that, and they can be developed
across the whole population, both by building tech that incentivizes them in economic systems
and educational systems and et cetera. If you don't factor development, you just see a Gaussian
distribution of violence or rationality or whatever, and you don't know what makes up the
Gaussian distribution. Well, the people on two standard deviations to the right and two standard
deviations to the left are not just distributions of human nature. There are reasons for that,
and there are reasons that can be changed in societal structure. The reason we know this
is because there are some cultures that are way less violent across the whole culture and some
that are way more violent and it doesn't have to do with the distribution of genetics.
So we can see that the Jains are nonviolent to humans, animals, and bugs across the entire
huge population across a long period of time. And we can see that the John's weed are extremely
fucking violent across the entire population. That's culture affecting the expression of human
nature. Human nature can do either of those. Human nature has radical plasticity.
And then I shared this video a while ago. I don't know if you saw it, but if not,
you should link it as well. It was a really good explainer video on the lead additive in gasoline
and the effect of lead in the environment. Because to just make engines stop knocking,
the fuel companies figured out to add this leaded thing that can busted it and put lead in the
atmosphere. And it turned out that the effect that has on brains dropped the global IQ by about
a billion points and made people about 4x more violent. And you're like, fuck, that's one chemical.
That's one chemical that we made for internal combustion engines to stop knocking. There's
like 50 million chemicals in the American chemical database. They all do weird stuff.
That's not including the zenoestrogens and the DDT and the, you know, whatever else.
The, that one chemical, if we say that the real politic assessment is humans are too
dumb and nasty. And that one chemical made them way dumber and way nastier. We're like, okay, well,
it's not just humans. It's humans under conditioning, both physiological and psychological
effects. We can do things that move it in the other direction too. And so when I look at the
average level of education of the Jewish community, Jewish across a long time of non-violence across
the Buddhist community, it's like, this is not a person. This is a distribution across an entire
population. And they didn't even have tech to move it in the right direction. If we see how fast
Facebook can polarize the population with its AI that curates bias doubling shit in people,
what if Facebook's algorithm was incentivized to do the opposite thing, which was be able to expose
people to stuff that would correct for their biases and to connect them to people that expanded
their network connectivity the most, we would see an enlightenment engine culturally faster than
anything the world has ever imagined. And so this is where it's like, I'm excited about tech builders
understanding this more complete picture because we could build tech and not just tech, but the
combination of tech and the regulation of it and the incentive system and things that could be
really profound. So I'm inspired by actual human nature and the plasticity and potential of it
and the things that the tech can do and with culture and like that. So this is answering your
question about hope just with a few examples. And the last thing I want to say on that is like,
it's very connected to this idea of like this other quality of faith of like having hope where
I don't even know the thing I have hope in yet. And I would say without a religious sensibility,
one can and actually needs to have a particular kind of faith, I believe,
which is, and we can say in a really secular way,
it's easy to say there's no way it can work. Nope, I see the patterns that we only have this many
years, it's getting worse like this, we're fucked. That's assuming and that kind of certainty can
justify us doing some pretty nasty stuff because of utilitarian calculus. All right,
if it's for sure that we're going to destroy the environment in this period of time,
my eco-terrorism is a really good idea. If it's sure AGI is going to take over in this period
of time, my brain chip is a really good idea, whatever it is, even if it's really bad for other
reasons because, but we can't be that certain because the unknown unknown set that could include
solutions, we don't know, we can't even factor how much it is in it. And so it's really easy to get
false certainty that then justifies really bad actions under utilitarian ethics. And so there's
a faith that says, I don't know what the solution is, but I know that the things that I don't know,
and I'm not even aware that I don't know, is a really large incident set. And as a result,
I'm going to not just sit back and say, oh, it'll take care of itself. I'm going to study my ass off
and I'm going to innovate, I'm going to talk to other people, I'm going to work on finding the
solutions that might be in those various parts of the unknown unknown set that I don't currently
know. So there's like a faith in possibility that keeps one actually learning and innovating,
that also requires a humility on not having the wrong kinds of certainty about catastrophism.
Right. So it seems like sort of like studying your ass off and really dedicating to solving
the problem, but knowing that the solution is likely in the unknown unknown set and having
that faith to continue studying the problem without knowing what the goal line is.
And maybe I know criteria and it's like, you know, Edison was a very, very flawed person, but that
kind of famous conversation when Napoleon Hill was interviewing him after the light bulb and he
said, had you, had you never been able to figure out the light bulb, what would you have done?
And Edison said something like, young man, if I, I'd still be in my lab working on it.
Right after all the field times is like, I was going to fucking die on the hill trying because
