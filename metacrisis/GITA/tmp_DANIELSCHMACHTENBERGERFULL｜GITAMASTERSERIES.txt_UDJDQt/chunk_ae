It just takes earnest wrestling with it moment by moment.
Patrick, does that bring up something for you?
The honest wrestling moment by moment is what I feel what I do anyway.
I'm not the biggest measure a guy in the world so far, but I was just, yeah, I have to think
about it.
But because I still feel every minute I think about something and I wrestle with it with
energy or with trying to bear an open mind, it will always bring my human mind down to
a decision that moves one across the other.
And then you can name that what comes out in the end.
You can name that measurement or you can name that I found it in another way.
I'm not quite sure.
I'm not quite sure.
This is really important.
Draw the line.
Yeah.
Yeah.
Hippocrates oath for the doctor, first do no harm.
Then if you can help do, but don't in the name of helping possibly do harm, which is
also a hymsa in the Vedic system.
Most ethical systems have a first don't fuck stuff up and then figure out how to help.
Because almost everything open AI was made to prevent AI risk because deep mind had so
much supremacy.
Now it's driving it faster than anybody.
So the guys that in at open AI that were scared of it made anthropic to really focus
on AI risk.
Then they took 300 million from Google and now they're racing ahead and they were a major
part of what made open AI release faster in the name of will make something better and
then get captured by Molok.
Most of the problems of the world are actually the results of us trying to solve other problems.
If the best thing you do is don't make shit worse, awesome, like everybody's in the name
of making shit better.
They're making shit worse and then lying to themselves because their motivated reasoning
requires them to.
If you don't know how to actually make stuff better, do less stuff.
This is what the whole doubt aging basically says is chill the fuck out, let the mud settle
till the water is clear and when something is really clear and there is something like
and there's a feeling of clarity, but humility at the same time, not hubris, then move forward
with it while maintaining heightened observation to change it if you need to change it.
So you don't make a once in final decision.
You can continue to change it is you won't be able to predict everything in advance.
So if I could just say your if you just do less stuff as opposed to stuff that ends up
being that negative in the name of progress, that would be way better.
And if you got other people to do that would be better, then work on.
Yes, you are not the UN, right?
You are not trying to say, how do we handle resource flows for the planet?
You're saying, how do I handle a little bit?
I have a little bit of time.
And so I am going to finally decide to work on this thing or this thing.
But you are not saying in that this other thing is less important.
You're saying of the things I'm aware of, here's where I feel I can be a best service.
That's fine because you do have not just the input of capital, you have the input of your
knowledge, your contacts, all those things.
And so you're factoring those multiple inputs into where you feel you can do the best thing.
But if you just really check for the motivated reasoning of all the various types,
the one that will make the most profit, the one that will keep everybody else happy,
it's the one that I can prove.
I'll give you a great example.
The Gates Foundation, for each dollar I can put into mosquito nets.
I know that I'm going to save this many kids.
So any philanthropist who's going to put a dollar in somewhere else has to say,
is it worth this many kids dying that I don't put the mosquito nets in?
Fuck, I hate that line of thinking.
In one way, it seems very reasonable because that seems like I could put that money there
immediately.
But on the other hand, why are there so many mosquitoes in the area?
Why are there so many malaria filled mosquitoes?
Because there's no fish and bats that eat them.
Because all the trees were cut down that the bats lived in and the fish were overfished in the area.
And so the problem will just keep getting worse.
The people, why are the people in that shitty area to begin with and not in the better areas?
Because there are landmines covering Angola and covering Mozambique from the wars that were there
that were left.
And then civilian gets blown up every 19 minutes from one of the 100 million landmines left.
And we got to remove those fucking things and move the people back to the non-shit areas.
They had to move to the shit areas because of the landmines we left.
Remove landmines, get trees back, get fish back, get people out of shit areas,
get sustainable agriculture.
We'll fix malaria.
Now the deep nets, someone can't live their life inside of a deep net.
And since they're already malnourished people, they're more susceptible to the fact that
deed is a neurotoxin.
Yes, if I'm only measuring that one measure in the narrowest way, that seems to make sense.
But it's literally the stupidest strategy I can think of if I zoom out from that one metric.
Now, if I say how many bats we save, it's bats, but it's also other predatory insects
like mosquito eaters.
And it's also, but the trees that get that don't just give me those bats.
They also give me soil stabilization.
And how do I say this other strategy was better?
Because in the short term, it didn't maximize this one metric as much.
And the things that it benefits aren't even measurable.
They're not all measurable.
Tell a story about it that's honest.
Be honest about your sensing and then try to communicate your sensing and have people
who are willing to make real choices that include, but are not limited to metrics
and are really willing to look at the underlying theory and sensing.
Do that.
Something like a mixture between a comment and a question to build on that maybe.
Which we've, as a species, developed the sensing for Dunbar 150 scale living, scale life.
But that's the kind of the level of complexity, the radius of our complexity
that we can usually manage as a normal human.
And given the global predicament, given the potential also benefits of global coordination
towards positive outcomes, what could be a good strategy to manage those higher
radiuses of complexity that are necessary for the things you are describing?
Then what we're actually capable of, we're just going to lay out to end of the spectrum,
a few massive galaxy brains when we're trying to outsource that to a large language models
at the moment, that can hold that complexity or can we somehow fructally move it up from a
Dunbar 150 to a global coordination system or anything in between?
What are your thoughts?
It's a very technical question that goes well beyond the limits of what we can do in the next
few minutes because obviously at a 150 person scale where most things are not based on important
export but are local, you have a totally different set of dynamics where you don't have
you don't have invisible externalities, right? If your waste is where you are, you see it,
your resource extraction is where you are, you see it. If you're starting to cause unrenovability
issues, it's where you are, that's really different, right? So you have direct sensing
and the direct sensing is within an environment we co-evolved with so we actually have hardware for
feeling a certain way about those things, right? We have mirror neurons with each other and we
have mirror neuron like things about the integrity of an environment. Most anyone who looks at
the tar sands compared to the forest that was there before without any ideology feels a thing
just associated with that for evolutionary reasons but we all buy shit that depends on the
tar sands but we don't look at the tar sands, right? The thing we look at is the thing on Amazon we
want and then we don't look at the plastic in the ocean that comes from that or the landfill or
the whatever and so the ability to ship all the ugly shit away and stay in this area is a major
part of the problem. So when you have six continent supply chains, how do you possibly take responsibility
for the end-order effects and when you have people that seem so different than you and it
doesn't seem like your life depends on them, it actually seems like they are against you,
how do you care about them and so obviously we're not going to go back to a world where
everybody lives in 150 person tribes without a global supply chain and we either go that low tech
or we figure out how to do lithography at every 150 people we're just not going to do that, right?
So we do have to figure out is there 150-ish person like various levels of coordination
that give the high touchness possible where then that whole unit has a cell interact with another
one, right? And that's what the Iroquois Confederacy or certain things were and then you get the
governance of a bio region that is a natural boundary. It's not an arbitrary one to find
in war and politics but basically things within a watershed, things within a migratory system,
whatever it is and then those interact with each other and so you go all the way up to a
planetary system but cells, tissues, organs, organ systems, organism, environment, you have these
kind of layers of self-interaction and obviously the level at which an effect is occurring is
the level at which governance has to be occurring. So you want subsidiarity that pushes things out
to the edge as much as possible but that also can do governance at the level across them because
that happens. So then you say what is the total complexity of the system that we are managing
at any of those levels? Like how many bits of information under maximum compression are required
to understand that thing if you wanted to say it, if you want to use complexity as a formal term
and then does our information processing system have the complexity both in amount and type to
process that thing? If it doesn't then our decision system will be an entropy pump, right? It will
fundamentally be reducing the total information to less than the real amount under compression,
make a decision based on that which means externalities will be proportional to that.
So how do we do the collective decision-making? What parts can the machines do? Like where can we
use AI and computational intelligence to crunch really big numbers and calculus that doesn't
disintermediate human choice at the center? How do we train humans to have better epistemology both
in terms of cognitive sense-making? Specialists can't do externality thinking as well. There's
some level of generality that is required to be able to think through externality. So there's
something in the nature of the what is curriculum trained, right, in people. So how do we train the
epistemology of people? Then how do we get collective intelligence? One person, one vote is dumb,
market dynamics are dumb. We need things that are smarter than those things. So what is the future of
liquid democracy, qualified democracy, subsidiarity, blah, blah, blah, all those types of things. It
ends up being a collective intelligence system augmented by computational capacities but not
disintermediating it. That is adequate. That is the what is the future of social systems, right?
What is the future of governance economics writ large? That has to be answered. To replace this,
we don't just have one fungible input. We have lots of types of input at a resource
accounting level and then lots of types of choice-making at various levels that occur.
So that is critical to answer the long-term question. In short-term, now we're in this system,
we're engaging with whatever business we want to do, even if it's a local business, still
is supplied by six continent supply chains that are supported by military industrial complex and
intelligence communities and all that shit, driving the arms races it does, otherwise the
sort of contract of the central bank produced dollar wouldn't be there. It is backed by OPEC
and horizontal oil and climate change. All of that is a part of every dollar we generate.
Can't not be the dollar as a contract. It doesn't mean shit without a military industrial complex,
a global financial system, OPEC backing and all those things. Okay, so I can't factor all that.
I can't, right? Galaxy brain or not, but what I can do is try really hard to do a much better job
than I have. I'm not going to just be excited about the upside and then do a plausible deniability
box checking on risk assessment. I'm going to really try hard and I'm going to talk to smart
people in different disciplines and say, do you think this is a good idea? How could it be a bad
idea? Regenerative agriculture person. How do you think this might be a bad idea? Watershed person.
How do you think this might be problematic? Person who's aware of the cultural dynamics in
this area and how it will affect wealth inequality and social dynamics and stuff. Oh, I would have
never thought of those things. So part of it is try, part of it is engage people that know
shit you don't earnestly, then engage them in, okay, I just have to kill the project because
it's obviously bad for a bunch of reasons. I didn't know and wasn't in the pitch deck of the
investors when they were trying to pitch it to me, but I would really like this thing to happen.
Do you, can you help me innovate at all? And if so, you get some of the upside or whatever it is.
Can you help me innovate into how to make this problem that you're seeing in this domain better
in this thing, right? So we yellow team and then we bring that back into a blue teaming process.
And then no, we can't figure it out. Then go with the first do no harm, don't invest in it.
We can make it better. Okay, now let's realize that there might be problems with it. We didn't
anticipate. So let's not go straight to full scale deployment, right? Let's not go to 100
million people in three weeks. Let's do a smaller scale deployment and say, okay, it's good enough.
We figured out how to do it enough that it's worth going forward, but it's worth going forward and
watching for things we couldn't have anticipated and then doing that again.
Now, this is going to go slower. Will it win market races? First to market advantage. Nope.
Will it cost more? Yep. Do you need the types of capital that are okay with that to not be
directly the cause of extinction in the process of their investment portfolio? Yes,
you need a capital that is aligned with those things. Could not steward anyone's capital that is
not. Otherwise, you are fiduciarily bound to the extinction paradigm.
Thank you. Very helpful. I don't know how your time is today, but we're reaching
the 90 minutes of our slots and sometimes it's also getting late-ish. Do you have a hard stop
in my question? I'm happy to take another question. Okay.
I guess one thing I want to say is
that Peter, do you mind this message is so much more exciting?
Like everything's getting better and as going to solve all the problems and
there it'll create some problems, but there've always been problems and humanity are problem
solvers and we'll rise to the occasion. We'll innovate our way out of it. It's going to bring
