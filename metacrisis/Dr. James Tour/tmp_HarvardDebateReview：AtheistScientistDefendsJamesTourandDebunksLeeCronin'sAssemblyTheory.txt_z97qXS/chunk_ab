algorithm? Because everything else would collapse. Assembly theory would basically collapse into
exactly algorithmic complexity. So it is in their interest to say that this is not a compression
algorithm. Not only that, but their compression algorithm is very basic. So it is based on
compression algorithms that are called dictionary based. So basically it's just about finding
repetitions in a piece of text or at the piece of data. And it is not optimal in any universal
sense. And Cronin always says no, but we are not looking for optimality. The problem is that
they don't have any evidence that their compression algorithm or however they want to call it corresponds
to anything ground truth about nature. They are just speculating on how it may assemble. That's
the whole hypothesis. So they don't have any reason to believe that their scheme is better than any
other. And the problem is that we have applied all the other schemes and actually either reproduce
exactly the same results or better. So there's actually evidence of potential evidence that those
other compression algorithms may be closer to ground truth. That will be the scientific evidence.
Okay, so let me just summarize for the audience what you're saying. You're saying that what
assembly theory is, it really backs out to a standard compression algorithm that is analogous
to such algorithms that were developed in the 1960s. And in fact, the ones in the 1960s actually
did a better job than what assembly theory is doing. That's correct. That's right. And people may ask
us why you publish, write and publish a paper. Well, we have and it is online and people can
find it. I'm going to show some figures in the next slides. Obviously it is more difficult and
this is the asymmetry paradox. So they are making deceiving statements. We have to come up with all
the effort to disprove them. And that is very expensive. It requires a lot of effort. We're
distracting ourselves from our own research. We basically try to make a favor to the science.
But let me show you how similar their algorithms are actually to
existing compression algorithms. And this is the paper I was talking about, our study is currently
available online. And I'm hoping it's going to be soon published in the journal. But you can see here
on the screen, we reproduce exactly their results. So this is figure four, and it coincides with their
figure four. They also have the same figure, but with only one of the measures. And that's one of
the criticisms that we have made before, why they didn't compare their index to anything else. So
that's a basic control experiment. You don't just come up with a new measure and say it is
fantastic. You have to prove that actually you are doing something that no other measure can do.
Otherwise you are just repeating what you could have done before with anything else. And that's
exactly what we are showing on this paper. We used exactly the same data because that's another
argument that he has put forward against our paper that we are perhaps using some sort of
different data that is not true. So in figure three on this paper, we use different data because
we wanted to show that actually if you take nomenclature names of the chemical compounds,
or you take distance matrices, for example, almost pretty much any representation of the data,
the chemical data, you get the same results. Later on, you are going to see one of the arguments.
Basically, we are showing that any index and any data basically gives you the same results as they
are pretending to be completely novel and revolutionary. So that's what it doesn't make
sense. So on the screen you are seeing their molecular assembly is the blue one. And you can
see that for abiotic and dead categories, they are kind of in the same place. But then biological
ones, they call them that way because these are organic compounds, they have a higher index. And
here the y-axis doesn't have to correspond because obviously we are using logarithmic space so that
we can put all these indexes together on the same screen. So you see that molecular assembly comes
higher. And that's fine. It is basically saying that you can separate this category from this
other two. But then when you look at anything else, including, for example, a very simple compression
algorithm that is called RLE for run length encoding, it is also doing so. If you take also
Hoffman on our own measure in yellow, they are also exactly doing the same. So you have like the
two examples almost at the same level, sharing the same mean. But then for biological cases,
they come much higher. And if you do the statistics, we don't only match their results for actually
outperform them. So what Hector is saying, he's taken his theory, which was developed quite some
time ago, but several years ago, and also theory from the 1960s and matching the output that assembly
theory is supposed to be giving and is totally novel. No, he's saying that there are algorithms
from the 1960s that are matching it and even outperforming. On the actual data that assembly
theory has put out, just these old theories that are 50 years old are doing the same thing. So
what he's doing is he's dispelling the novelty of assembly theory. He says that this is a
traditional compression algorithm that does the same thing. That's absolutely correct. Exactly
the same data that they made available for their paper, which by the way is kind of incomplete
and not in the best shape. But we took at face value their results for their index and then used
the data that they provided. We were able to produce this plot on the screen. Exactly the same
procedure. Now the other thing, so Jim was saying, oh, it's not correct. We can actually measure it.
This is when I was like, oh, we can physically measure it without any mathematics. We can go
and do molecular spectroscopy. That doesn't make any sense. What does he mean by not using mathematics
and directly on physical data? That doesn't make any sense. They show a formula on the screen that
we are going to see in the next slide. It is a mathematical formula. And when they say they are
applying to physical data, it's basically just a matrix with real numbers, which is exactly what
we fed with our algorithms. So exactly the same. So let me get into the details. Why it is not a
surprise that we are basically matching the same results if not doing it much better. That's because
their algorithm is, as I said, a dictionary based compression algorithm, even if they hate the idea.
I have put as an example Hoffman coding because it's kind of the simplest and optimal. But
actually their algorithm is exactly the same step by step to an algorithm that is called LC77.
What a typical LZ compressor will do is it will work its way through all of the text
that you need to compress and will actually look for sequences of characters that recur
over and over again and will attempt to reuse them as much as it possibly can.
This comes from their paper. This is an example and you can see it on the clip. He provides the
same example with Abra-Cadabra. And he basically says because you produced Abra before you can reuse
it and put it at the end and basically you save that step. And you do things and events happen
and there is a memory trapped on that line. And then you count the number of steps. That's exactly
the definition step by step of LZ77 algorithm. And you can see it here. I didn't have even to
produce by myself the examples because if you go online and you type LZ77 Abra-Cadabra, you are going
to find hundreds of examples of students that have been given this as a homework in an introductory
course in computer science. And this is exactly the same algorithm. So you traverse the stream,
find the longest repetition and then you have a pointer. You have to go seven places back
and the length of the word that you are going to replace is four letters. And you can find
plenty of examples of exactly the same. From understanding you have an object at a beginning
and you do things and events happen and there is a memory trapped on that line. And that is assembly
theory. It's not hard. He says this is it about assembly theory. This is the whole algorithm.
There's nothing else. This is very simple. I'm using pretty much his words. So if this is it,
this is LZ77 that was introduced in the 77. There's nothing else. Then he says it's because
we can produce a graph. You can produce a graph for this exactly as they do. Actually, the way
they are doing it is kind of the wrong way because as I said, they don't do it optimally. They cannot
prove that the algorithm is optimized. It's limited. So after LZ77, the world has moved on to more
powerful compression schemes because it has some limitations. Let me just interject there. So what
Hector is suggesting is that Abracadabra teaching that Lee gave, that is a typical homework assignment
for a student taking computer science and that's using a compression algorithm, this LZ77. There
was nothing novel in this and in fact, assembly theory did it in an inferior way and that's why
it's often not used because they have much better algorithms now. That's right. So it is exactly the
same as LZ77. But what I'm saying is that there are new, better algorithms that are different to
this one and therefore to assembly theory that are much better and that's why it is not a surprise
that we were able to actually do better on their data to separate the classes of
organic and non-organic categories. And they always have an answer. They may say now, for
example, no, that's because we are only interested in the number of steps and not the resulting compressed
text or whatever it is. It doesn't matter. The algorithm comes with the number of steps anyway.
As I said, they also say that it doesn't come up with the graph. The graph, you can produce it
yourself just as you're doing it for assembly theory. So this is not a situation in which we
are attacking Cronin because he's kind of a Galileo with a new theory, a renegade of
status quo because actually he mentions Galileo, I think, twice during the debate.
I've reminded you the heliocentric view of the universe that Galileo kind of had
Bill popularized a lot about what Galileo did and I'm no Galileo.
This is about no substance at all. And that is very strange. They also say, oh, that's because we
are applying this algorithm to mass spectral data. And that's when he says that they are applying it
on physical data that nobody else can do. We can actually measure it. This is when I was like, oh,
we can physically measure it without any mathematics. We can go and do molecular spectroscopy.
That doesn't make any sense. So LC77, the evolved version of LC77 is called LZW.
Basically, just a small variation of the same algorithm is used behind GZIP. And we use this
algorithm to compress images all the time. Images are just real value matrices, just the
mass spectral matrices that Cronin was using in his original paper. So there's absolutely no
difference in the kind of input to these measures. They are exactly the same. It doesn't make any
sense to talk about physical data. There's some post processing in the middle, but it is exactly
the same. We take exactly the same input. We run a very simple algorithm that is implemented,
instantiated in a computer program, just as he did. There's absolutely no difference.
Let me just clarify something. Hector, is it right that you have used the very mass spec data
that he has used, and you're able to reproduce it with much older algorithms?
That's correct. So that's figure four I showed before. That's exactly the data he was talking
about. Well, actually, I am aware of a new paper coming out on this regard already showing that
you can produce, actually, they occur naturally, these kind of minerals with very high assembly
index. So even that separation between organic and nonorganic doesn't make much sense. And that's
why we didn't make much noise with our own research. I just showed you a paper that we published
five years before assembly theory showing that we could separate those categories. But again,
organic doesn't necessarily mean directly like biology or the other way around. We thought it
was interesting, but not to make these kind of claims because a lot of research is needed.
And that's exactly what a responsible scientist says when doing science. You say we have made
progress, but that doesn't mean we have solved everything. So here's another paper from the
same group with Cronin and Sarah Walker, also senior authors. They opened this paper with this
figure, and I made sure to put in purple color what I added to this figure. So the original one is
the blue, white, black figure. And this is, I think, figure two or somewhere in the paper.
So let's take first figure one in this paper published in entropy. They are claiming that
assembly theory is different to Kolmogorov complexity and to Shannon entropy. But strangely
enough, this example is exactly the same. It is only that they are applying it in the wrong way.
So you remember assembly theory basically counts the number of repetitions. And in this case,
I think it is probably five. Now, depending whether you take the input as a step or not.
But then look at what they put in the figure. They say Kolmogorov complexity is completely
different because what it's going to come back with is a computer program that says,
do six times this output or print six times this output. And they claim this is completely different.
Well, the information they need is exactly here. It is the number of steps. So I don't understand
what they are saying. They say that Kolmogorov complexity is not computable. That's partially
true. It is to be technically correct. It is semi computable, which means that you can approximate
it just as they are approximating it with assembly theory. So assembly theory is an
approximation to Kolmogorov complexity. It is called a resource bounded approximation of
Kolmogorov complexity. And that's what makes it computable. But there are plenty of computable
approximations, including the ones that my groups have proposed for a number of years.
We have the coding theorem method, the block decomposition method, and there are methods
that others have been using. So basically there are two main approximations, approximation methods
to Kolmogorov complexity, as we are going to see in the next slide. Either you approximated by
compression algorithms of which assembly theory is one and one of the most basic, or you do something
a little bit more sophisticated that is a little bit farther away from simple statistics just as
assembly theory is. Remember, assembly theory is just counting repetitions. You then count how many
copies of the object you have, and the larger the number of copies and the larger assembly index,
the more you're sure it came from evolution. And actually, we all know, I mean, those that
work in my field, computer scientists, informaticians, are actually Shannon entropy is a function that
counts repetitions. It basically was introduced in the 40s or 50s for that purpose. It counts
repetitions based on a probability distribution. So you have some sort of prior, you don't leave
the same way to everyone. But you can even see here how they apply wrongly Shannon entropy,
because they are providing Shannon entropy with a different input. The input should have
been the two squares, not only one. And with two squares, what you have to implement is some sort of
Shannon entropy rate estimator, which would have told you that the lowest entropy of this
object is when you take the tuples, and then you just count the number of tuples. And that's
exactly what these compression algorithms do. What that's why they are entropy estimators,
optimal entropy estimators. So RLD, Hoffman coding, and this Hoffman coding is the one
optimal I was talking about. LC-77 is also optimal in the Shannon entropy sense. So they are
Shannon entropy estimator, and they would provide you exactly with information that you would
have required for a uniform probability distribution. Let me pull this together, is that what you're
seeing is you're seeing an informatician, somebody who works in the air of computer science,
and has to package information. So when you download an image, and you bring up an image on
your computer, how did that image transfer through a wire and through the internet, through the cloud?
How was that image packaged? And computer scientists have devised ways to do this.
Mathematicians, computer scientists, informaticians have devised ways to do this over a period of
60, 70 years, and he's even quoting even longer than that, packages to do this.
And what he's saying is that assembly theory is doing exactly the same thing that these are doing.
Now, these are far more developed because people have been working on these things for 60 or 70
years. And he says that assembly theory is just a weak version of these things, that there's nothing
novel here. And that's why even on this slide, he's saying, is this ignorance of theirs? Could be.
Or is it out and out dishonesty? Now, I have never presupposed that Lee Cronin was being
dishonest, but here's the question that Hector is asking. He says it's so obvious to those in the
field that this is well known. That's correct, Jim. And at the beginning, I was sure this was
ignorance, but actually I've been in communication with Cronin even before his first paper.
And I was telling him everything that I'm telling you and your audience. And he decided not to
take it on board. So that's why I'm asking what is the motivation behind it. And so oddly, in this
example, it turns out that Shannon entropy called more of complexity that they despise so much.
And assembly theory basically gives you exactly same information. I'm obviously
abusing here the equal sign, because it's not exactly equal. Some of them would give you more
information. So called more of complexity is actually coming back with a computer program.
But the computer program has the information needed. So as a definition of complexity,
algorithmic complexity, the size of the information and then the number of steps to
compress and decompress this object is exactly the same. The same when they claim that they
can come up with these graphs again, these are very well known in computer science.
Introductory courses, you come up with these kind of graphs and finite state of automata all the
time, dictionary trees or nothing new. Assembly pathway start with a basic set of building blocks
and allow joining reuse. It's a bit like a give a child and say, right, start here,
let's just take abracadabra. Well, they call them assembly pathways. So everything is about like
renaming things that were already around. So as I was saying, basically, there's two ways to
