design and theoretical analysis by just by looking at the spike trains. We didn't look at anything
else. We were actually sub-sampling some of these spike trains. We are only looking at 480 neurons
involved in this. Once we've learned the system, we can also simulate as before. If we simulate,
the network actually generates decisions on its own, depending on the stimulus. So
we built a model of decision-making of spiking neurons using spiking neurons.
And it generates more spikes. Okay, so that's the first part. Let me switch gears and get to the
second part, which is the exploration of unknown neural state space with attractors. So in the
previous cases, we were studying mostly spontaneous activity. There were no feedback controls
signals. But if we can control the system, we can do a much better job in learning about the
system. Because the recording time we have for the animals and the number of trials that
the animals can perform are very limited, which means every single spike, every single observation
is very, very precious. So we want to maximize how much we learn about the system by choosing
the best stimulus that will generate the best spikes, such that we can learn about the system.
And this has been done in many places, especially in visual areas from long time ago. But what I
want to do is not to use visual stimuli and study the static encoding properties, but rather
the dynamical system itself. I want to infer the most about the dynamical system. And this is
important because of attractors. So let me illustrate this by this example of a snowman
dynamical system. You see here, this is the snowman dynamical system. There are two attractor
oscillation-like objects here. And the neural state is going around here. And you can see that
SVMC recovers these top attractor very well because it has explored it on its own
spontaneously. There was no stimulation. It just happens that the state was stuck in this
attractor for a while. And while it was exploring, we learned that particular attractor. So what we
see is that whenever there's an attractor dynamics, it leads to bio-sampling. We sample
that attractor more. And neural attractors are everywhere in the neural system. We know they're
everywhere. Which means if we wanted to learn the more global picture of how the computations
are implemented and how the dynamics is, then we really need to actively explore around it. If we
just push this state a little bit to the outside of this top attractor, we might have discovered
this bottom attractor more quickly instead of waiting for a long time until it spontaneously
switch over. And let me just illustrate how difficult this problem is by analogy of a pirate who lost
a map because he doesn't have a map. The pirate doesn't know anything about the world. The world
is completely unknown. And this exploration has to be done in an online fashion. Unlike playing
video games, you can't reset the agent to the initial point or wherever you want to start and
then repeat different actions to learn about the system. You have to do it in one long episode,
which makes it much more difficult. And the environment itself is dynamical. We're filled
with attractors. And there's almost no extrinsic reward. If there's any reward, it's going to be
very, very sparse. And only by chance, you will rarely encounter them. So most of the exploration
has no guidance from external rewards. So the formulation of the problem is such that we have
to choose controls to explore a dynamical environment. So we're going to modify this
equation a little bit, where we have this controls u of t in the dynamic system equations.
And the problem itself can be posed as a type of reinforcement learning problem,
where the extrinsic reward is very, very rare. So we need to have some intrinsic reward
internally generated by the agent, such as curiosity. These kinds of rewards have been
studied widely in we propose a different objective. This is our proposed intrinsic reward function,
where it's combining the count based reward and the discovery bonus. Just to quickly illustrate,
let's just say this is our state space, the blue circle parts are the places that we have already
explored. If we go to one of these places, you get reward based on inversely proportional to the
number of times you visited that state. And if you visit a new place, if you let's say this new
place over here, you get reward based on the distance from the closest place that you visited to
that particular new point. So further away, it's redder here, which means the bonus that you get
is higher. And once you visit that place instantly around it, it will be lower reward around it.
And we can show that maximizing this intrinsic reward is the equivalent to maximizing entropy of
your visitation distribution. So we have a reward function that maximizes entropy,
we just need to combine with a reinforcement learning algorithm, we have two choices,
model free and model based. Model free is very popular, of course, but the main assumption
of model free is that the reward has to be Markovian. But as we just talked about, whenever
you visit a state, that state is less attractive already, the reward decreases as soon as you
visit a state. So it's totally non Markovian. So model free doesn't work very well. And model
free also has a problem being sample inefficient, especially if you can build a good model, model
based methods are very efficient, demonstrated by this example here, where they are comparing different
model based and model free methods, none of them actually know the world, they all have to learn
the model implicitly or explicitly. But the model based ones learn to do the task much faster.
So our proposed function that we want to maximize is this v, we want to try to find at x time t,
we are currently at x of t, try to find the best control signals u t plus one to t plus h with a
time horizon h, some time steps h in the future, using the current intrinsic world model that we
know predicts what how much amount of intrinsic we will get, choose the best one. To put this
together, well, as the is our algorithm called escape the maze. So we're an uncertain world.
Every time step we observe a new state, the new state is used to update our internal model,
the new state is used to update our intrinsic reward landscape. And then combining the two
information together by with a model based planning, we'll find the next best control
that will maximize the exploration. And doing so, also learn about the system, the dynamics model,
as pretty quickly. Here are examples of this multiple attractor chain link environment.
Our proposed algorithm escape the maze, explores it pretty quickly. This is random
exploration. This is random network distillation, state of the art method, and prediction error
method. Neither of those explore these systems very quickly. And if you have maze environments like
this, they also don't explore very well. Our algorithm explores pretty quickly as it summarized
over here. If you watch this video for long enough, you'll see that these algorithms get stuck
in a particular corner and they never leave. Because the intrinsic reward is probably because
it's non Markovian, it doesn't know if you keep going that way, you don't get any more reward,
you got a lot of reward on the way to going there, of course. Okay, so to summarize,
I've introduced you to a couple of new real time neuroscience tools that could potentially accelerate
scientific discovery, the real time statistical inference tools for neural trajectory and
non-linear dynamics. These can enable new experimental paradigms and feedback control
systems. One such feedback control system is the active exploration of unknown neural states
with attractors. It allows us to better infer the global dynamics and also as a byproduct can
escape attractor states. This escaping of attractor state is very interesting to us because it has a
potential clinical application to treat disorders with consciousness. With that, I would like to
thank my lab members, especially Hosun Al-Sar, my PhD student here. He's done a lot of work
and my former postdoc, Iwan Zhou here, who's done a lot of work and presented today as well.
I would also thank my funding sources and my collaborators and of course, thank you for your
attention.
