These strange worm-like creatures are produced by a type of program known as neural cellular
automata. I found them while playing around with my new web project, NeuralPatterns.io,
which allows you to explore these programs for yourself. In this video, I want to explain
the underlying algorithm that gives rise to these and many other strange patterns so you
can understand exactly what it is you're looking at. But before I explain neural cellular automata,
what are cellular automata? A cellular automaton is, in its most basic form, a grid of cells that
can either be one or zero, colored or black, alive or dead. Every frame, each cell looks at its
neighboring cells, its neighborhood, and decides what its new state should be by following a simple
set of rules. Probably the most famous cellular automaton is Conway's Game of Life, which has
the following rules. If a cell is dead and has exactly three neighbors, it comes alive. If a cell
is alive and has two or three neighbors, it stays alive. Otherwise, it dies. These seemingly
simple rules lead to some surprisingly complex behavior, and Conway's Game of Life is one of
the classic examples of emergence from simplicity. Cellular automata are nothing new and have been
around for decades. There are many, many fascinating variations of Conway's Game with different rules,
larger neighborhoods, different cell states, and other interesting changes. In fact, one could
argue that the life engine is one such variation, though there is a footnote to that claim. One of
the more recent innovations, however, is to merge these classical cellular automata with more modern
techniques for machine learning. Thus is born the neural cellular automata. Neural cellular automata,
which I will call NCA's from here on out, were first introduced when researchers use them to
grow images that can regenerate even after being partially destroyed. I strongly recommend you
check out their work on this website, which is immensely impressive and obviously foundational
for neural patterns. Neural patterns, the website, gives you the ability to play around with the
algorithm's parameters and strips it down quite a bit to its basic components. NCA's do a few
things differently from classical cellular automata. For one, they are continuous, where rather
than a cell state being either a binary one or zero, it can be a decimal value in between,
like point five or point two. This adds a lot of complexity and means that the update rule that
defines a cell's behavior is more of a mathematical operation than a logical one. So why the neural
part of the name? Well, the update rule is made up of two steps, first a convolution and then
an activation. These are the core functions of what are called convolutional neural networks,
which are tools in the field of deep learning. They're especially useful for image processing.
They're very good at recognizing patterns between pixels and their local neighbors
in a process that begins to look very similar to cellular automata.
Each frame of an NCA is essentially generated by passing the previous frame through a neural network
and letting it decide what the next state for every pixel should be, hence neural cellular automata.
Now a quick disclaimer before we go on, some of these patterns can be a little hard on the
eyes, though I've toned it down quite a bit for the video. So a mild epilepsy warning for the rest
of the video and a pretty strong one for the website itself, it can be pretty easy to produce
bright flashing lights. So the first step of the algorithm is the convolution. This is the local
neighborhood part of the algorithm, where each cell looks at its neighbors and determines its
next state. The filter, this three by three grid of values seen here, is slid over each cell,
and each filter value is multiplied with the corresponding cell value in that local neighborhood.
Then all of these products are added together, resulting in the final convolved value for that
cell, which in this case is one, so the value actually didn't change. However, you have to
remember that this cell is in the neighborhood of all of its neighboring cells, so it will affect
their values as well. What would this look like if we apply this filter to every pixel on every frame?
It sort of grows off to the right. This happens to be the pattern associated with this filter.
A couple technical notes here. First, the filter is actually inverted from what you see in the panel.
This is pretty standard practice in most convolution implementations I've seen,
and I think it makes things slightly more intuitive. Second, when a filter comes to an edge,
it simply wraps around to the other side of the grid, so the final pattern will wrap around the
edges as well. This step by itself is enough to produce some impressive complexity, but as those
who might know a thing or two about neural networks might guess, much more complexity can be added
by introducing what is called an activation function. The activation function is a mathematical
function that's applied to each cell, where it is given the convolved value from the previous step,
performs some logic, and returns what will ultimately be used as the pixel's value.
In neural patterns, this logic is defined here, where you can write your own function in a language
called GLSL, a shader language. Since the function takes in a decimal value and returns a decimal
value, the best way to think of it is as a math function, f of x, so I like to use this online
tool Desmos to visualize the functions I'm writing. The default activation is pretty straightforward,
it's what's called the identity function. It simply returns the input x without change,
as if there is no activation function at all. But we can make it slightly more complicated
by multiplying it by some constant value, say 2. By the way, GLSL requires that float values be
indicated with dots, so we have to say 2 dot, not just 2. This changes the final pattern a little bit.
What if we took the absolute value of it?
It's a much bigger difference. There are a lot of interesting possible activation functions,
and subtle differences can have a big influence. It's important to note, by the way, that in
neural patterns, because the pixel value can only be between 0 and 1, the final value is clipped
between 0 and 1. So an activation function that looks like this will actually end up looking
like this, where the values greater than 1 and less than 0 will be chopped off. This isn't
universally true of activation functions just in neural patterns. Activation functions can add a
lot of power to the algorithm. In fact, you can actually implement Conway's Game of Life
with a clever filter and activation. You can load it up here and see the details of the filter
activation, which is a simple logical check.
This brings us to the worm simulation, which you can load up the same way.
It has this funky activation function, which is essentially an inverted bell curve, or Gaussian.
This is such a neat pattern to me. It's really organic looking, which is not what I was expecting
to find. You may be asking, why does this filter and this activation produce this pattern?
Well, to be frank, I have no idea. Neural networks typically have dozens of layers of convolutions
with thousands and in some cases up to trillions of parameters. It really is unbelievable to me
that a single convolution and a relatively simple activation function could produce such
organic looking complexity. I feel like I'm looking through a microscope into a strange
computational microverse. I have a few very neat NCA's hidden at my sleeve for future videos,
but until then, I hope you now have a better understanding of how they work and how you
can explore this strange world for yourself.
