Yeah.
And now instead of running MCMC on it, you say, OK, I'm going to train this
neural network.
So this won't yet be worth it for just one data set.
Now, the cool thing is if you want to keep your actual model, so whatever you write
down in PMC or Stan, you want to keep that fixed, but now plug in different data sets.
That's where amortized inference really shines.
So for instance, there was this one huge analysis in the UK where they had like
intelligence study data from more than one million participants.
And so for each of those participants, they again had a set of observations.
And so for each of those one million participants, they want to perform
posterior inference.
It means if you want to do this with something like MCMC or anything
non amortized, you would need to fit one million models.
So you might argue now, OK, but you can parallelize this across like a thousand
cores, but still that's that's a lot, that's a lot to show.
Now, the cool thing is the model was the same every single time.
You just had a million different data sets.
And so what these people did then is train a neural network once and then it
will train for a few hours, of course.
But then you can just sequentially feed in all these one million data sets.
And for each of these one million data sets, it takes way, way less than one
second to generate tens of thousands of posterior samples.
But that didn't really answer your question.
So your question was about what, how can we generalize in the model space?
And that's a really hard problem because essentially what these neural networks
learn is to give you some posterior function if you feed in a data set.
Now, if you have a domain shift in the model space, so now you want inference
based on a different model, and this neural network has never learned to do that.
So that's tough.
And that's, that's a hard problem.
And essentially what you could do and what we are currently doing in our
research, but that's cutting edge, is expanding the model space.
So you would have a very general formulation of a model and then try
to amortize over this model.
So that different configurations of this model, different variations,
could just be expected to showcase the model essentially.
Okay.
So can you take an example maybe to, to give an idea to, to listeners?
How, how that would work?
Absolutely.
So we have one preprint about sensitivity aware amortized vision inference.
And what we do there is essentially have a kind of a, a, a, a, a, a, a,
essentially have a kind of multiverse analysis built in to the neural network
training.
So give some, some background multiverse analysis basically says, okay,
what are all the preprocessing steps that you could take in your analysis?
And you can code those.
And now you're interested in like, what if, what if I had chosen a
different preprocessing technique?
What if I had chosen a different way to standardize my data?
Um, then also the classical prior sensitivity or likelihood sensitivity,
um, analysis, like what happens if I do power scaling on my prior power
scaling on my posterior?
So we also encode this.
Um, what happens if I bootstrap some of my data or just have a perturbation of my
data?
Um, what if I add a bit of noise to my data?
So these are all slightly different models.
What we do is essentially keep track of that during the training phase and just
encode it into a vector and say, well, okay, now we're doing preprocessing
choice number seven, um, and scale the prior to the power of two, don't scale
the likelihood and don't do any perturbation and feed this as an
additional information into the neural network.
Now the cool thing is during inference phase, once we're done with the training,
you can say, Hey, here's a data set.
Now pretend that we chose preprocessing technique number 11 and prior
scaling of power 0.5.
What's the posterior now?
Because we've amortized over this larger, more general model space.
We also get valid posterior inference.
If we train for long enough over these different configurations of model.
And essentially, if you were to do this, um, in a, like with MCMC, for instance,
um, you would refit your model every single time.
Yeah.
And so here you don't have to do that.
Okay.
Yeah.
I see.
That's super.
Yeah.
That's super cool.
And I feel like, so that would be mainly the main use cases would be, as you were
saying, when, when you're getting into really high data territory and you have
what's changing is mainly the data side, mainly the data set and to be even more
precise, not really the data set, but the data values, because the data set is
supposed to be like point the same, like you would have the same columns, for
instance, but the values of the columns would change all the time.
And the model at the same time doesn't change.
Is that like, that's really for now, at least the best use case for that kind of method.
Yes.
And this might seem like a very niche case.
But then if you look at, um, like Bayesian workflows in practice, um, this,
this topic of this scheme of many model, it doesn't necessarily mean that you have
a large number of data.
This might also just mean you want extensive cross validation.
So assume that you have one data set with 1000 observations.
Now you want to run, leave on our cross validation, but for some reason you can
do the Pareto smooth important sampling version, which would be much faster.
So you would need 1000 model refits, even though you just have one data set because
you want 1000 cross validation refits.
Um, maybe can you explicit what your meaning by cross validation here?
Because that's, uh, that's not a term that's used a lot in the Bayesian
framework, I think.
Yeah, of course.
Um, so especially in a Bayesian setting, uh, there's this approach of leave on our
cross validation where you would fit your posterior based on all data points, but
one, and that's why it's called leave one out because you take one out and then fit
your model, fit your posterior on the rest of the data.
And now you're interested in the posterior predictive performance of this one
left out observation.
Yeah.
And that's called validation.
Yeah.
Go ahead.
Yeah, no, just I'm going to let you finish, but yeah, for listeners, um, familiar
with the frequented framework, that's something that's really heavily used in,
in that framework, cross validation.
And it's very similar to the machine learning concept of cross validation, but
in the machine learning area, you would rather have something like five
fold, in general, K fold cross validation where you would have larger splits of your
data and then use parts of your whole data set as the training data set and the
rest for evaluation.
Essentially, like leave on a cross validation, just puts it to the extreme.
Everything but one data point is your train data set.
Yeah.
Yeah.
Okay.
Yeah.
Damn, that's super fun.
And is there, is there already a way, uh, for people to try that out or is it mainly
for now implemented for papers, uh, and, and you are probably, I'm guessing, uh,
working on that with, uh, with a key and all his group in, in Finland to make that
more open source, helping people use packages to do that.
And what's the state of the, of the things here?
Yeah.
Um, that's a great question.
Um, and in fact, the state of usable open source software is far behind what we have
for likelihood based MCMC based inference.
So we currently don't have something that's comparable to pi MC or Stan.
Um, our group is developing or actively developing a software that's called base flow.
Um, that's because like the name because like base, we're doing Bayesian inference.
Um, and essentially the first neural network architecture that was used for
this amortized Bayesian inference are so-called normalizing flows, um, conditional normalizing
flows to be precise.
And that's why, why, why the name base flow, um, came to be.
But now, um, they have a bit of a different take because now we have a whole lot of
generative neural networks and not only normalizing flows.
So now we can also use, for example, um, score-based diffusion models that are mainly
used for image generation, um, and AI, um, or consistency models, which are essentially
like a distilled version of, um, score-based diffusion models.
And so now base flow doesn't really capture that anymore.
But now what the base flow Python library specializes in is, um, defining
principled amortized Bayesian workflows.
So the meaning of base flow slightly shifted to amortized Bayesian workflows.
And hence the name base flow again, um, and the focus of base flow and the aim of
base flow is two-fold.
So first we want a library that's good for actual users.
So this might be researchers who just say, Hey, here's my data set.
Here's my model, my simulation program.
And please just give me fast posterior samples.
So we want, like, um, usable high-level interface with sensible default values
that mostly work out of the box and an interface that's mostly self-explanatory.
Also, of course, good teaching material and all this.
Um, but that's only one side of the coin because the other large goal of
base flow, um, is that it should be usable for machine learning researchers who
want to advance amortized Bayesian inference methods as well.
And so the software in general, um, is structured in a very modular way.
So for instance, you could just say, Hey, take my current pipeline, my current
workflow, um, but now try out a different loss function because I have a new fancy
idea.
I want to incorporate more likelihood information.
Um, and so I want to alter my loss function.
So you would have your general program, um, because of the modular architecture
there, could just say, take the current loss function and replace it with a
different one that is used to the API.
And we're trying to, you know, doing both and serving both interests, user-friendly
side for actually applied researchers who are also currently using base flow.
Um, but then also the machine learning researchers, um, with completely
different requirements for, for this piece of software.
Maybe we can also do base flow documentation, um, and the current
project website in the two notes.
Yeah.
Yeah, we should definitely do that.
Um, definitely going to try that out myself.
That sounds like fun.
Uh, I need a use case, but, uh, as soon as I have a use case, I'm definitely
going to try that out because it sounds like a lot of fun.
Um, yeah, several questions based on that and then so on for being so clear.
Um, and so detailed on these.
Um, so first we talked about normalizing flows in episode 98 with
Marie-Lou Gabrie, uh, definitely recommend listeners to, uh, listen to
that for some background.
Um, and, um, questions.
So base flow, yeah, definitely we need that in the show notes.
Uh, and I'm going to install that in my, in my environment.
And I'm guessing, so you're saying that that's in Python, right?
The, the package.
Yes.
The core packages in Python and we're currently refactoring to Keras.
