In this episode, Marvin Schmidt introduces the concept of amortized Bayesian inference,
where the upfront training phase of a neural network is followed by fast posterior inference.
Marvin will guide us through this new concept, discussing his work in probabilistic machine
learning and uncertainty quantification using Bayesian inference with deep neural networks.
He also introduces Bayes-Flo, a Python library for amortized Bayesian workflows, and discusses
its use cases in various fields, while also touching on the concept of diffusion and its
relation to multimodal simulation-based inference.
Yeah, that is a very deep episode and also a fascinating one.
I've been personally diving much more into amortized Bayesian inference with Bayes-Flo
since the folks there had been kind enough to invite me to the team, and I can tell you
this is super promising technology.
A PhD student in computer science at the University of Stuttgart, Marvin is supervised
actually by two LBS guests you surely know, Paul Birkner and Aki Vettari.
Marvin's research combines deep learning and statistics to make Bayesian inference
fast and trustworthy.
In his free time, Marvin enjoys both games and he's a passionate guitar player.
This is Learning Bayesian Statistics, Episode 107, recorded April 3, 2024.
Welcome to Learning Bayesian Statistics, a podcast about Bayesian inference, the methods,
the projects and the people who make it possible.
I'm your host, Alex Andorra.
You can follow me on Twitter at Alex underscore Andorra, like the country, for any info about
the show.
LearnBayestats.com is the place to be, show notes, becoming a corporate sponsor, unlocking
Bayesian merge, supporting the show on Patreon, everything is in there, that's LearnBayestats.com.
If you're interested in one-on-one mentorship, online courses or statistical consulting, feel
free to reach out and book a call at topmate.io slash Alex underscore Andorra.
See you around, folks, and best Bayesian wishes to you all.
Hello, my dear Bayesians!
Today I want to thank the fantastic Adam Romero, Will Geary and Blake Walders for supporting
the show on Patreon.
Your support is truly invaluable and literally makes this show possible.
I can't wait to talk with you guys in this live channel.
Second, the first part of our modeling webinar series on Gaussian processes is out for everyone.
So if you want to see how to use the new HSGP approximation in Poimsy, head over to the LBS
YouTube channel, and you'll see Juan Orduz, a fellow Poimsy Core Dev and Mathematician,
explain how to do fast and efficient Gaussian processes in Poimsy.
I'm actually working on the next part in this series as we speak, so stay tuned for
more and follow the LBS YouTube channel if you don't want to miss it.
Okay, back to the show now.
Marvin Schmidt, Willkommen na learning Bayesian statistics.
Thanks Alex.
Thanks for having me.
Actually, my German is very rusty, do you say na or zu?
Well, welcome learning Bayesian statistics.
Maybe Willkommen im Podcast?
Na!
Obviously, obviously, like it was a third hidden option.
Damn.
Oh, it's a secret third thing, right?
Yeah, always in German, it's always that.
Man, damn.
Well, that's okay.
I got embarrassed in front of the world, but I'm used to that in each episode.
So thanks a lot for taking the time, Marvin.
Thanks a lot to Matt Rosinski actually for recommending to do an episode with you.
That was kind enough to take some of his time to write to me and put me in contact with you.
I think you guys met in Australia in a very fun conference based on the beach.
I think it happens every two years.
Definitely when I go there in two years and do a live episode there.
Definitely that's a project.
I wanted to do that this year, but that didn't go well with my traveling dates.
So in two years, definitely going to try to do that.
So yeah, listeners and Marvin, you can help me accountable on that promise.
Absolutely, we will.
So Marvin, before we talk a bit more about what you're a specialist in and also what
you presented in Australia, can you tell us what you're doing nowadays and also how you
ended up working on this?
Yeah, of course.
So these days I'm mostly doing methods development.
So broadly and probably machine learning, I care a lot about uncertainty quantification.
And so essentially I'm doing Bayesian inference with deep neural networks.
So taking Bayesian inference, which is notoriously slow at times, which might be a bottleneck,
and then using generative neural networks to speed up this process, but still maintaining
all the explainability, all these nice benefits that we have from using Bayesian modeling.
And I have a background in both psychology and computer science.
That's also how I ended up in Bayesian inference, because during my psychology studies, I took
a few statistics courses and started as a statistics tutor, mainly doing frequent statistics.
And then I took a seminar on Bayesian statistics in Heidelberg in Germany.
It was the hardest seminar that I ever took.
Well, it's super hard.
We read like papers every single week.
I had to prepare every single paper for every single week.
And then at the start of each session, the professor would just shuffle and randomly
pick someone to prison.
Oh my god.
That was tough, but somehow it stuck with me.
And I had this aha moment where I felt like, OK, all this statistics stuff that I've been
doing before was more of following a recipe, which is very strict.
But then this holistic Bayesian probabilistic take just gave me a much broader overview
of statistics in general.
Somehow I followed the path.
Yeah.
I'm curious what that, so what does that mean to do Bayesian stats on deep neural network?
Concretely, what is the thing you would do if you had to do that?
I would say there does that mean you mainly you develop the deep neural network and then
you had some Bayesian layer on that?
Or you have to have the Bayesian framework from the beginning.
How does that work?
Yeah, that's a great question.
And in fact, that's a common point of confusion there as well.
Because Bayesian inference is just like general, almost philosophical framework for reasoning
about uncertainty.
So you have some latent quantities, call them parameters, or whatever, some latent unknowns,
and you want to do inference on them.
You want to know what these latent quantities are, but all you have are actual observables.
And you want to know how these are related to each other.
And so with Bayesian neural networks, for instance, these parameters would be the neural
network weights.
And so you want full Bayesian inference on the neural network weights.
And like fitting normal neural networks already support confusion.
Exactly.
Over these neural network weights, exactly.
So that's one approach of doing like Bayesian deep learning, but that's not what I'm currently
doing.
Instead, I'm coming from the Bayesian side.
So we have like a normal Bayesian model, which has statistical parameters.
So you can imagine like a mechanical model for like a simulation program.
And we want to estimate these scientific parameters.
So for example, if you have a cognitive decision-making task from the cognitive sciences, and these
parameters might be something like the non-decision time, the actual motor reaction time that
you need to move your muscles, and some information uptake rates, some bias, and all these things
that researchers are actually interested in.
And usually you would then formulate your model in, for example, pi mc or stan, or however
you want to formulate your statistical model, and then run mcmc for parameter inference.
And now where the neural networks come in in my research is that we replace mcmc with
a neural network.
So we still have our Bayesian model, but we don't use mcmc for posterior inference, but
instead we use a neural network just for posterior inference.
And this neural network is trained by a maximum likelihood.
So the neural network itself, like the weights there are not probabilistic, like there are
no posterior distributions over the weights, but we just want to somehow model the actual
posterior distributions of our statistical model parameters using a neural network.
So the neural net, I think so, that's quite new to me, so I'm going to rephrase that and
see how much I understood.
So that means the deep neural network is already trained beforehand?
No, we have to train it.
And that's the part about this.
Okay, so you train it at the same time, you train it at the same time, you're also trying
to infer the underlying parameters of your model.
And that's the cool part now, because in mcmc you would do both at the same time, right?
You have your fixed model that you write down at pi mc or stan, and then you have your one
observed data set, and you want to fit your model to the data set.
And so you do, for example, your Hamiltonian Monte Carlo algorithm to traverse your parameter
space and then do the sampling.
So you couple your approximating phase and your inference phase.
You learn about the posterior distribution based on your data set, and then you also
want to generate posterior samples while you're exploring this parameter space.
And in the line of work that I'm doing, which we call amortized Bayesian inference, we decouple
those two phases.
So the first phase is actually training those neural networks.
And that's the hard task, right?
And then you essentially take your Bayesian model, generate a lot of training data from
the model, because you can just run prior predictive samples.
And those are your training data for the neural network.
And you use the neural network to essentially learn a surrogate for the posterior distribution.
So for each data set that you have, you want to take those as conditions and then have
a generative neural network to learn somehow how these data and the parameters are related
to each other.
And this upfront training phase takes quite some time, and it usually takes longer than
the equivalent MCMC would take, given that you can run MCMC.
Now the cool thing is, as you said, when your neural network is trained, then the posterior
inference is super fast.
Then if you want to generate posterior samples, there's no approximation anymore, because
you have already done all the approximation.
So now you're really just doing sampling.
That means just generating some random numbers in some latent space and having one pass through
the neural network, which is essentially just a series of matrix multiplications.
So once you've done this hard part and trained your generative neural network, then actually
doing the posterior sampling takes like a fraction of a second for 10,000 posterior samples.
OK, yeah, that's really cool.
And how generalizable is your deep neural network then?
Do you have, like, is that because I can see the real cool thing to have a neural network
that's customized to each of your models.
That's really cool.
But at the same time, as you were saying, that's really expensive to train a neural
network each time you have to sample a model.
And so I was thinking, OK, so then maybe what you want is have generalized categories of
deep neural network, let's say, so that would probably be another kill.
But let's say I have a deep neural network for linear regressions, whether they are
generalized or just plain normal likelihood, you would use that deep neural network for
regressions, linear regressions.
And then the inference is super fast because you only have to train the neural network
once and then inference, posterior inference on the linear regression linear regression
parameters themselves is super fast.
So yeah, like, that's a long question.
But did you did you get what I was asking?
Yeah, absolutely.
So so if I get a question right now, you're asking, like, if you don't want to run linear
regression, but want to run some slightly different model, can I still use my pre-train
neural network to do that?
Yes, exactly.
And also, yeah, like, in general, how does that work?
Like, how are you thinking about that?
Well, are there already some best practices or is it like really, for now, really getting
a research death and all the questions are in the air?
Yeah.
So first of all, the general use case for this type of amortized region inference is
usually when your model is fixed, but you have many new data sets.
So you have some quite complex model where MCMC would take a few minutes to run.
And so instead for one fixed data set that you actually want to sample from.
