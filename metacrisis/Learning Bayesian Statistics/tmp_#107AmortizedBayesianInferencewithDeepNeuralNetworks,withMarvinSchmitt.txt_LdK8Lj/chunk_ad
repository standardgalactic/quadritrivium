that we have access to a likelihood density, but instead we just assume that we
can sample from the likelihood, um, and essentially simulate from the model.
Um, in fact, the likelihood is still present, but it's only implicitly defined.
And we don't have access to the density.
That's why likelihood free inference doesn't really hit what's happening here.
But instead, like in, in the recent years, people have started adopting the term
simulation based inference because we do Bayesian inference based on simulations
instead of likelihood densities.
Um, so methods that have been used for quite a long time now in the simulation
based inference, um, research area, um, but for example, rejection ABC, so approximate
Bayesian computation, or then like ABC, SMC, so combining ABC, uh, with sequential
Monte Carlo on, um, essentially the next iteration there was throwing neural
network at simulation based inference.
And that's exactly this neural posterior estimation that I talked about earlier.
And now what researchers noticed is, Hey, when we train a neural network for
simulation based inference, instead of running rejection approximate Bayesian
computation, then we get amortization for free as a site product.
It's just a byproduct of using a neural network for simulation based inference.
And so in the last, maybe four to five years, people have mainly focused on
this algorithm that's called neural posterior estimation for simulation based
inference.
And so all developments that happened there and all the research that happened
there, almost all the research, sorry, um, focused on cases where we don't have
any likelihood density.
So we're purely in the simulation based case.
Now, with our view of things, when we come from a Bayesian inference, like
likelihood based setting, can say, Hey, amortization is not just a random
coincidental byproduct, but it's a feature and we should focus on this feature.
And so now what we're currently doing is moving this idea of amortized Bayesian
inference with neural networks back into a likelihood based setting.
So we started using likelihood information again, for example, using
likelihood densities if they're available or learning information about
the likelihood.
So like a surrogate model on the fly and then again using this information
for better posterior inference.
So we're essentially bridging simulation based inference and likelihood
based Bayesian inference again with this goal, larger goal of amortization.
If we can do it.
And so this work on deep fusion essentially addresses one huge
shortcoming of neural networks when we want to use them for amortized Bayesian
inference.
And that is in situation where we have multiple different sources of data.
So for example, imagine you're a cognitive scientist and you run an
experiment with subjects and for each test subject, you give them a decision
making task, but at the same time, while your subjects solve the decision
making task, you wire them up with an EEG to measure the brain activity.
So for each subject across maybe a hundred trials, you know, I have is
both an EEG and the data from the decision making task.
Now, if you want to analyze this with Pi MC or Stan, what you would just
do is say, hey, well, we have two data generating processes that are governed
by set of shape parameters.
So the first part of the likelihood would just be because we know process
for the decision making task, we just model the reaction times.
At least in a procedure there in the cognitive science.
And then for the second part, we have a second part of the likelihood that
we evaluate that somehow handles this EEG, these EEG measurements.
For example, a spatial temporal process, or just like some summary
statistics that are being computed there, just however you would usually
compute your EEG, then you add both to the PDF of the likelihood, and then
you can call it a day.
Now, you cannot do that in neural networks because you have no straight
forward sensible way to combine these reaction times from the decision
making task and the EEG data, because you cannot just take them and
slap them together.
They are not compatible with each other because these information data
sources are heterogeneous.
So you somehow need a way to fuse these sources of information so that
you can then feed them into the neural network.
That's essentially what we're studying in this paper, where you could
just get very creative and have different schemes to fuse the data.
So you could use these attention schemes that are very hip and large
language models right now with transformers essentially, and have these
different data sources attend or listen essentially to each other.
With cross attention, you could just let the EEG data inform your decision
making data or just have the decision making data inform the EEG data.
So you can get very creative there.
You could also just learn some representation of both individually
and then concatenate them and feed them to the neural network.
Or you could do very creative and weird mixes of all those approaches.
And in this paper, we essentially investigate, I have a systematic
investigation of these different options, and we find that the most
straightforward option works the best overall.
And that's just learning fixed size embeddings of your data sources
individually and then just concatenating them.
It turns out then we can use information from both sources in an
efficient way, even though we're doing inference with neural networks.
And maybe what's interesting for practitioners is that we can compensate
for missing data in individual sources.
So in the paper, we induced missing data by just taking these EEG data
and decision making data and just randomly dropping some of them.
And the neural networks have learned, like when we do this fusion process,
the neural networks learn to compensate for partial missingness in both sources.
So if you just remove some of the decision making data, the neural network
learned to use the EEG data to inform your posterior, even though the data
in one of the sources are missing, the inference is pretty robust then.
And again, all this happens without model refits.
So you would just account for that doing training.
Of course, you have to do this like random dropping of data doing a training
phase as well.
And then you can also get it doing the inference phase.
Yeah, that sounds, yeah, that's really cool.
Maybe that's a bit of a, like a small piece of this paper in our larger roadmap.
This is essentially taking this amortized vision inference up to the level
of trustworthiness and robustness and all these gold standards that we currently
have for likelihood-based inference in PyMC or Stan.
Yeah, yeah.
And there's still a lot of work to do because of course, like there's no free lunch.
And of course, there are many problems with trustworthiness.
And that's also one of the reasons why I'm here with Aki right now.
Because Aki is so great at Bayesian workflow and trustworthiness, good diagnostics.
That's all, you know, all the things that we currently still need for trustworthy
amortized vision inference.
Huh.
Yeah, so maybe when I talk a bit more about that and what you're doing on that,
that sounds like something very interesting.
So one huge advantage of an amortized Bayesian sampler is that evaluations
and diagnostics are extremely cheap.
So for example, there's this gold standard method that's called simulation-based
calibration where you would sample from your model and then like a sample from
your prior predictive space and then refit your model and look at your coverage,
for instance, in general, look at the calibration of your model on this
potentially very large prior predictive space.
So you naturally need many model refits, but your model is fixed.
So if you do it with MCMC, it's a gold standard evaluation technique,
but it's very expensive to run, especially if your model is complex.
Now, if you have an amortized estimator, simulation-based calibration
on thousands of data sets takes a few seconds.
So essentially, and that's my goal for this research visit with Aki here in Finland,
is trying to figure out what are some diagnostics that are gold standard,
but potentially very expensive, up to a point where it's infeasible to run
on a larger scale with MCMC, but we can easily do it with an amortized estimator.
With the goal of figuring out, like, can we trust this estimator, yes or no?
As you might know from neural networks, we just have no idea what's happening
inside the neural network.
And so we currently don't have these strong diagnostics that we have for MCMC,
like, for example, our head.
There's no comparable thing for a neural network.
So one of my goals here is to come up with more good diagnostics
that are either possible with MCMC, but very expensive, so we don't run them.
But they will be very cheap with an amortized estimator,
or the second thing just specific to an amortized estimator,
just like our head is specific to MCMC.
Okay, yeah, I see.
Yeah, that makes tons of sense.
And actually, so, I mean, I would have more technical question on these,
but I see the time running out.
I think something I'm mainly curious about is the challenges,
the biggest challenges you face when applying amortized patient inference
and diffusion techniques in your projects, but also, like, in the projects you see.
I think that's going to also give a sense to these nerves of when and where
to use these kind of methods.
Yeah, that's a great question.
And I'm more than happy to talk about all these challenges that we have,
because there's so much room for improvement, because, like, these amortized methods,
they have so much potential that we still have a long way to go
until they are as usable and as straightforward to use as the current MCMC samplers.
And in general, one challenge for practitioners is that we have most of the problems
and hardships that we have in PMC or STAN.
That is, that researchers have to think about their model in a probabilistic way
and a mechanistic way.
So instead of just saying, hey, I click on t-test or linear regression
in some graphical user interface, they actually have to come up with a data-generating process
and have to specify their model.
And this whole topic of model specification is just the same in amortized workflow.
Because it's some way we need to specify the Bayesian model.
And now on top of all this, we have a huge additional layer of complexity,
and this is defining the neural networks.
In amortized Bayesian inference, nowadays we have two neural networks.
The first one is a so-called summary network,
which essentially learns the latent embedding of the data set.
Essentially, those are optimal learned summary statistics.
And optimal doesn't mean that they have to be optimal to reconstruct the data,
but instead optimal means they're optimal to inform the posterior.
So for example, in a very, very simple toy model, if you have just a Gaussian model
and you just want to perform inference on the mean.
Then a sufficient summary statistic for posterior inference on the mean would be the mean.
Because that's all you need to reconstruct the mean.
This sounds very totalogical for you.
And then again, the mean is obviously not enough to reconstruct the data
because all the variance information is missing.
So what the summary network learns is something like the mean.
So summary statistics that are optimal for posterior inference.
And then the second network is the actual generative neural network.
So like a normalizing flow, a score-based diffusion model, consistency model,
flow matching, whatever conditional generative model you want.
And this will handle the sampling from the posterior.
And these two networks are learned end-to-end.
So you would learn your summary statistic, output it,
feed it into the posterior network, the generative model,
and then have one evaluation of the loss function optimized both end-to-end.
And so we have two neural networks, a lot of story short,
which is substantially harder than just hitting, like, sample on a PMC or STAN program.
And that's an additional hardship for practitioners.
Now in base flow, what we do is we provide sensible default values for the generative neural networks,
which work in maybe like 80 or 90% of the cases.
It's just sufficient to have, for example, like a neural spline flow,
like some sort of normalizing flow with six layers and a certain number of units,
some regularization for robustness and, you know, cosine decay of the learning rates,
and all these machine learning parts to try to take them away from the user
if they don't want to mess with it.
