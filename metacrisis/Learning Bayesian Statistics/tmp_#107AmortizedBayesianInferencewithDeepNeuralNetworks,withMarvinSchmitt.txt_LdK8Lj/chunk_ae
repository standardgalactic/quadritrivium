But still, if things don't work, they would need to somehow diagnose the problems
and then, you know, play with the number of layers and this neural network architecture.
And then for the summary network, the summary network essentially needs to be informed by the data.
So if you have time series, you would look at something like an LSTM,
so this, like a long short-term memory, time series neural networks.
Or you would have, like, recurrent neural network or nowadays a time series transformer.
They're also called temporal fusion transforms.
If you have IID data, you would have something like a deep set or a set transformer
which respect this exchangeable structure of the data.
So again, we can give all the recommendations and sensible default values.
If you have a time series, try a time series transformer.
Then again, if things don't work out, users need to, you know, play around with these settings.
So that's definitely one hardship of amortized patient inference in general.
And for the second part of your question, hardships of this deep fusion,
it's essentially if you have more and more information sources
then things can get very complicated.
For example, just a few days ago, we discussed about a case where someone has 60, like,
60 different sources of information and they're all streams of time series.
Now we could say, hey, just slap 60 summary networks on this problem,
like one summary network for each main source.
That's going to be very complex and very hard to train,
especially if we don't bring that many data sets to the table for the neural network training.
And so there we somehow need to find a compromise.
Okay, what information can we condense and group together?
So maybe some of the time series sources are somewhat similar and actually compatible with each other.
So we could, for example, come up with six groups of 10 time series each.
Then we would only need six neural networks for the summary embedding.
So all these practical considerations that makes things just like as hard as in likelihood-based,
MCMC-based inference, but just a bit harder because of all the neural network stuff that's happening.
Did this address your question?
Yeah, yeah, it gives me more questions, but yeah, for sure, that does answer the question.
And when you're talking about transformer for time series, are you talking about the transformers,
like the neural network that's used in large language models, or is it something else?
It's essentially the same, but slightly adjusted for time series,
so that the statistics or these latent embeddings that you output still respect the time series structure,
where typically you would have this autoregressive structure.
So it's not exactly the same standard transformer, but you would just enrich it to respect the probabilistic structure in your data.
But at the core, it's just the same.
So at the core, it's an attention mechanism, like multi-head attention,
where the different parts of your data set could essentially talk or listen to each other.
So it's just the same.
Okay, yeah, that's interesting. I didn't know that existed for time series.
That's interesting.
That means, so because the transformer takes like one of the main thing is you have to tokenize the inputs.
So here you would tokenize like that there is a tokenation happening of the time series data.
You don't have to tokenize here, because the reason why you have to tokenize in large language models,
or natural language processing in general, is that you want to somehow encode your character,
so your words into numbers, essentially.
And we don't need that in Bayesian inference in general, because we already have numbers.
Yeah.
So our data already comes in numbers, so we don't need tokenization here.
Of course, if we had text data, then we would need tokenization.
Yeah, yeah, yeah.
Okay, okay. Yeah, it makes more sense to me.
Okay, that's fun. I didn't know that existed.
Do you have any resources about Transformer for time series that we could put in the show notes?
Absolutely. There's a paper that's called Temporal Fusion Transformers, I think.
I will send you the link.
Oh yeah, awesome. Yeah, thanks.
So we have this time series Transformer, Temporal Fusion Transformer implemented in Baseflow.
So now it's just like a very usable interface where you would just input your data,
and then you get your latent embeddings.
You can say like, I want to input my data, and I want as an output 20 learned summary statistics.
So that's all you need to do there.
Then you can go crazy and get creative with it.
Yeah, what would you do with these results, basically the outputs of the Transformer?
What would you use that for?
Those are the learned summary statistics that you would then treat as a compressed fixed-length version of your data
for the posterior network, for this generative model.
So then you use that afterwards in the model?
Exactly. So the Transformer is just used to learn summary statistics of the data sets that we input.
For instance, if you have a time series, like we did this for a COVID time series.
If you have a COVID time series worth like for a three-year period would be in daily reporting,
you would have a time series worth about a thousand time steps.
That's quite long as a condition into a neural network to pass in there.
And also, like, if now you don't have a thousand days, but a thousand and one days,
then the length of your input to the neural network would change, and your neural network wouldn't do that.
So what you do with a time series Transformer is compress this time series of maybe a thousand or maybe a thousand and fifty time steps.
Into a fixed-length vector of summary statistics.
Maybe you extract 200 summary statistics from that.
Okay, I see. And then you can use that in your neural network.
In the model that's going to be sampling your model.
In the neural network that's going to be sampling your model.
We already see that we're heavily overloading terminology here.
So what's the model actually?
So then we have to differentiate between the actual Bayesian model that we're trying to fit,
and then the neural networks or the generative model or generative neural network that we're using as a replacement for MCMC.
It's a lot of this taxonomy that's this odd when you're at the interface of deep learning and statistics.
Another one of those pickups are parameters.
Like in Bayesian inference parameters are your inference targets.
So you want posterior distributions on a handful of model parameters.
When you talk to people from deep learning about parameters, they understand the neural network weights.
Sometimes I have to be careful with the terminology and words used to describe things.
Because we have different types going on at different levels of abstraction in different functions.
So that means in this case, the transformer takes in time values, it summarizes them,
and it passes that on to the neural network that's going to be used to sample the Bayesian model.
Exactly.
And they are passed in as the conditions, like conditional probability, which totally makes sense.
Because like this generative neural network, it learns the distribution of parameters conditional on the data or summary statistics of the data.
So that's the exact definition of the Bayesian posterior distribution,
of the Bayesian model parameters conditional on the data.
That's the exact definition of the posterior.
Yeah, I see.
And that means, so in this case, oh yeah, no, I think my question was going to be,
why would you use these kind of additional layer on the time search data, but you answer that,
is that, well, what if your time search data is to be or something like that?
Exactly, it's not just being to be, but also just a variable length.
Because the neural network, like the generative neural network, it always wants fixed length inputs.
Like it can only handle, in this case of the COVID model, it could only handle input conditions with length 200.
And now the time series transformer takes part, so the time series transformer handles the part that our actual raw data have a variable length.
And time series transformers can handle data of variable length.
So they would just take a time series of length, maybe 500 time steps to 2,000 time steps,
and then always compress it to 200 summary statistics.
So this generative neural network, which is much more strict about the shapes and form of the input data,
will always see the same length inputs.
Yeah, yeah, okay, yeah, I see, that makes sense.
Awesome, yeah, super cool.
And so as you were saying, this is already available in Baseflow,
people can use this kind of transformer for time series.
Yeah, absolutely, for time series and also for sets, so for IED data.
Because if you just take an IED data set and input into a neural network,
the neural network doesn't know that your observations are exchangeable.
So it will assume much more structure than there actually is in your data.
So again, it has a double function, like a dual function of compressing data,
encoding the probabilistic structure of the data, and also outputting a fixed length representation.
So this would be a transformer or deep set is another option.
It's also implemented in Baseflow.
Super cool, yeah.
And so let's start winding down here because I've already taken a lot of your time.
Maybe a last few questions would be, what are some emerging topics that you see within deep learning
and probabilistic machine learning that you find particularly intriguing?
I've been to talk here a lot about really the nitty-gritty, the statistical detail and so on,
but now if we did zoom a bit and we start thinking about more long term.
Yeah, I'm very excited about two large topics.
The first one are generative models that are very expressive,
so unconstrained neural network architectures, but at the same time have a one step inference step.
So for example, people have been using score-based diffusion models a lot,
or flow matching for image generation, like for example, stable diffusion.
You might be familiar with this tool to generate input a text prompt and then you get fantastic images.
Now this takes quite some time, so like a few seconds for each image,
but only because it runs on a fancy cluster.
If you run it locally on a computer, it takes much longer.
And that's because the score-based diffusion model needs many discretization steps
in this denoising process during inference time.
And now throughout the last year, there have been a few attempts
on having these very expressive and super powerful neural networks,
but they are much, much faster because they don't have these many denoising steps.
Instead, they directly learn a one step inference, so they could generate an image,
not like a thousand steps, but only in one step.
And that's very cutting edge or bleeding edge, if you will,
because they don't work that great yet, but I think there's much potential in there
because it's both expressive and fast.
And then again, we've used some of those for amortized Bayesian inference,
so we use consistency models and they have super high potential, in my opinion.
So with these advances in deep learning, we can always, often times,
we can use them for amortized Bayesian inference.
We just reformulate these generative models and slightly tune them to our tasks.
So I'm very excited about this.
In the second area, I'm very excited about our foundation models.
I guess most people are in AI these days.
So foundation models essentially means neural networks are very good at indistribution tasks,
so whatever is in the training data set, neural networks are typically very good
at finding patterns that are similar to the training set, what they saw in the training set.
Now in the open world, so if we are out of distribution, we have a domain shift,
distribution shift, model mis-specification, however you want to call it,
neural networks typically aren't that good.
So what we could do is either make them slightly better at our distribution,
or we just expand the indistribution to a huge space and that's what foundation models do.
For example, GPT-4 would be a foundation model because it's just trained on so much data.
I don't know how many, it's not terabyte anymore, it's essentially the entire internet.
So it's just a huge training set.
And so the world and the training set that this neural network has been trained on is just huge.
And so essentially we don't really have out of distribution cases anymore,
just because our training set is so huge.
And that's also one area that could be very useful for amortized Bayesian inference.
And to overcome the very initial shortcoming that you talked about,
where we would also like to amortize over different Bayesian models.
Hmm, I see, yeah, yeah, yeah.
Yeah, that would definitely be super fun.
Yeah, I'm really impressed and interested to see this interaction of deep learning artificial intelligence
and then the Bayesian framework coming on top of that.
That is really super cool, I love that, yeah.
Yeah, it makes me super curious to try that stuff out.
So to play us out Marvin, actually, this is a very active area of research.
So what advice would you give to beginners interested in diving into this intersection of deep learning
and probabilistic machine learning?
That's a great question.
Essentially I would have two recommendations.
The first one is to really try to simulate stuff.
Whatever it is that you're curious about, just try to write a simulation program
and try to simulate some of the data that you might be interested in.
So for example, if you're really interested in a soccer,
then code up a simulation program that just simulates soccer matches
and the outcomes of soccer matches.
So you can really get a feeling of the data generating processes that are happening
because probabilistic machine learning at its very core is all about data generating processes
and reasoning about these processes.
And I think it was Richard Feynman who said, what I cannot create, I do not understand.
That's essentially at the heart of simulation based inference in a more narrow setting
but probabilistic machine learning and machine learning more broadly, or science more broadly even.
