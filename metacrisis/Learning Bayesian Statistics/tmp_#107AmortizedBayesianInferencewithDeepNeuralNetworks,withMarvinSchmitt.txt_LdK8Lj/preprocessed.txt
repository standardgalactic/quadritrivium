In this episode, Marvin Schmidt introduces the concept of amortized Bayesian inference,
where the upfront training phase of a neural network is followed by fast posterior inference.
Marvin will guide us through this new concept, discussing his work in probabilistic machine
learning and uncertainty quantification using Bayesian inference with deep neural networks.
He also introduces Bayes-Flo, a Python library for amortized Bayesian workflows, and discusses
its use cases in various fields, while also touching on the concept of diffusion and its
relation to multimodal simulation-based inference.
Yeah, that is a very deep episode and also a fascinating one.
I've been personally diving much more into amortized Bayesian inference with Bayes-Flo
since the folks there had been kind enough to invite me to the team, and I can tell you
this is super promising technology.
A PhD student in computer science at the University of Stuttgart, Marvin is supervised
actually by two LBS guests you surely know, Paul Birkner and Aki Vettari.
Marvin's research combines deep learning and statistics to make Bayesian inference
fast and trustworthy.
In his free time, Marvin enjoys both games and he's a passionate guitar player.
This is Learning Bayesian Statistics, Episode 107, recorded April 3, 2024.
Welcome to Learning Bayesian Statistics, a podcast about Bayesian inference, the methods,
the projects and the people who make it possible.
I'm your host, Alex Andorra.
You can follow me on Twitter at Alex underscore Andorra, like the country, for any info about
the show.
LearnBayestats.com is the place to be, show notes, becoming a corporate sponsor, unlocking
Bayesian merge, supporting the show on Patreon, everything is in there, that's LearnBayestats.com.
If you're interested in one-on-one mentorship, online courses or statistical consulting, feel
free to reach out and book a call at topmate.io slash Alex underscore Andorra.
See you around, folks, and best Bayesian wishes to you all.
Hello, my dear Bayesians!
Today I want to thank the fantastic Adam Romero, Will Geary and Blake Walders for supporting
the show on Patreon.
Your support is truly invaluable and literally makes this show possible.
I can't wait to talk with you guys in this live channel.
Second, the first part of our modeling webinar series on Gaussian processes is out for everyone.
So if you want to see how to use the new HSGP approximation in Poimsy, head over to the LBS
YouTube channel, and you'll see Juan Orduz, a fellow Poimsy Core Dev and Mathematician,
explain how to do fast and efficient Gaussian processes in Poimsy.
I'm actually working on the next part in this series as we speak, so stay tuned for
more and follow the LBS YouTube channel if you don't want to miss it.
Okay, back to the show now.
Marvin Schmidt, Willkommen na learning Bayesian statistics.
Thanks Alex.
Thanks for having me.
Actually, my German is very rusty, do you say na or zu?
Well, welcome learning Bayesian statistics.
Maybe Willkommen im Podcast?
Na!
Obviously, obviously, like it was a third hidden option.
Damn.
Oh, it's a secret third thing, right?
Yeah, always in German, it's always that.
Man, damn.
Well, that's okay.
I got embarrassed in front of the world, but I'm used to that in each episode.
So thanks a lot for taking the time, Marvin.
Thanks a lot to Matt Rosinski actually for recommending to do an episode with you.
That was kind enough to take some of his time to write to me and put me in contact with you.
I think you guys met in Australia in a very fun conference based on the beach.
I think it happens every two years.
Definitely when I go there in two years and do a live episode there.
Definitely that's a project.
I wanted to do that this year, but that didn't go well with my traveling dates.
So in two years, definitely going to try to do that.
So yeah, listeners and Marvin, you can help me accountable on that promise.
Absolutely, we will.
So Marvin, before we talk a bit more about what you're a specialist in and also what
you presented in Australia, can you tell us what you're doing nowadays and also how you
ended up working on this?
Yeah, of course.
So these days I'm mostly doing methods development.
So broadly and probably machine learning, I care a lot about uncertainty quantification.
And so essentially I'm doing Bayesian inference with deep neural networks.
So taking Bayesian inference, which is notoriously slow at times, which might be a bottleneck,
and then using generative neural networks to speed up this process, but still maintaining
all the explainability, all these nice benefits that we have from using Bayesian modeling.
And I have a background in both psychology and computer science.
That's also how I ended up in Bayesian inference, because during my psychology studies, I took
a few statistics courses and started as a statistics tutor, mainly doing frequent statistics.
And then I took a seminar on Bayesian statistics in Heidelberg in Germany.
It was the hardest seminar that I ever took.
Well, it's super hard.
We read like papers every single week.
I had to prepare every single paper for every single week.
And then at the start of each session, the professor would just shuffle and randomly
pick someone to prison.
Oh my god.
That was tough, but somehow it stuck with me.
And I had this aha moment where I felt like, OK, all this statistics stuff that I've been
doing before was more of following a recipe, which is very strict.
But then this holistic Bayesian probabilistic take just gave me a much broader overview
of statistics in general.
Somehow I followed the path.
Yeah.
I'm curious what that, so what does that mean to do Bayesian stats on deep neural network?
Concretely, what is the thing you would do if you had to do that?
I would say there does that mean you mainly you develop the deep neural network and then
you had some Bayesian layer on that?
Or you have to have the Bayesian framework from the beginning.
How does that work?
Yeah, that's a great question.
And in fact, that's a common point of confusion there as well.
Because Bayesian inference is just like general, almost philosophical framework for reasoning
about uncertainty.
So you have some latent quantities, call them parameters, or whatever, some latent unknowns,
and you want to do inference on them.
You want to know what these latent quantities are, but all you have are actual observables.
And you want to know how these are related to each other.
And so with Bayesian neural networks, for instance, these parameters would be the neural
network weights.
And so you want full Bayesian inference on the neural network weights.
And like fitting normal neural networks already support confusion.
Exactly.
Over these neural network weights, exactly.
So that's one approach of doing like Bayesian deep learning, but that's not what I'm currently
doing.
Instead, I'm coming from the Bayesian side.
So we have like a normal Bayesian model, which has statistical parameters.
So you can imagine like a mechanical model for like a simulation program.
And we want to estimate these scientific parameters.
So for example, if you have a cognitive decision-making task from the cognitive sciences, and these
parameters might be something like the non-decision time, the actual motor reaction time that
you need to move your muscles, and some information uptake rates, some bias, and all these things
that researchers are actually interested in.
And usually you would then formulate your model in, for example, pi mc or stan, or however
you want to formulate your statistical model, and then run mcmc for parameter inference.
And now where the neural networks come in in my research is that we replace mcmc with
a neural network.
So we still have our Bayesian model, but we don't use mcmc for posterior inference, but
instead we use a neural network just for posterior inference.
And this neural network is trained by a maximum likelihood.
So the neural network itself, like the weights there are not probabilistic, like there are
no posterior distributions over the weights, but we just want to somehow model the actual
posterior distributions of our statistical model parameters using a neural network.
So the neural net, I think so, that's quite new to me, so I'm going to rephrase that and
see how much I understood.
So that means the deep neural network is already trained beforehand?
No, we have to train it.
And that's the part about this.
Okay, so you train it at the same time, you train it at the same time, you're also trying
to infer the underlying parameters of your model.
And that's the cool part now, because in mcmc you would do both at the same time, right?
You have your fixed model that you write down at pi mc or stan, and then you have your one
observed data set, and you want to fit your model to the data set.
And so you do, for example, your Hamiltonian Monte Carlo algorithm to traverse your parameter
space and then do the sampling.
So you couple your approximating phase and your inference phase.
You learn about the posterior distribution based on your data set, and then you also
want to generate posterior samples while you're exploring this parameter space.
And in the line of work that I'm doing, which we call amortized Bayesian inference, we decouple
those two phases.
So the first phase is actually training those neural networks.
And that's the hard task, right?
And then you essentially take your Bayesian model, generate a lot of training data from
the model, because you can just run prior predictive samples.
And those are your training data for the neural network.
And you use the neural network to essentially learn a surrogate for the posterior distribution.
So for each data set that you have, you want to take those as conditions and then have
a generative neural network to learn somehow how these data and the parameters are related
to each other.
And this upfront training phase takes quite some time, and it usually takes longer than
the equivalent MCMC would take, given that you can run MCMC.
Now the cool thing is, as you said, when your neural network is trained, then the posterior
inference is super fast.
Then if you want to generate posterior samples, there's no approximation anymore, because
you have already done all the approximation.
So now you're really just doing sampling.
That means just generating some random numbers in some latent space and having one pass through
the neural network, which is essentially just a series of matrix multiplications.
So once you've done this hard part and trained your generative neural network, then actually
doing the posterior sampling takes like a fraction of a second for 10,000 posterior samples.
OK, yeah, that's really cool.
And how generalizable is your deep neural network then?
Do you have, like, is that because I can see the real cool thing to have a neural network
that's customized to each of your models.
That's really cool.
But at the same time, as you were saying, that's really expensive to train a neural
network each time you have to sample a model.
And so I was thinking, OK, so then maybe what you want is have generalized categories of
deep neural network, let's say, so that would probably be another kill.
But let's say I have a deep neural network for linear regressions, whether they are
generalized or just plain normal likelihood, you would use that deep neural network for
regressions, linear regressions.
And then the inference is super fast because you only have to train the neural network
once and then inference, posterior inference on the linear regression linear regression
parameters themselves is super fast.
So yeah, like, that's a long question.
But did you did you get what I was asking?
Yeah, absolutely.
So so if I get a question right now, you're asking, like, if you don't want to run linear
regression, but want to run some slightly different model, can I still use my pre-train
neural network to do that?
Yes, exactly.
And also, yeah, like, in general, how does that work?
Like, how are you thinking about that?
Well, are there already some best practices or is it like really, for now, really getting
a research death and all the questions are in the air?
Yeah.
So first of all, the general use case for this type of amortized region inference is
usually when your model is fixed, but you have many new data sets.
So you have some quite complex model where MCMC would take a few minutes to run.
And so instead for one fixed data set that you actually want to sample from.
Yeah.
And now instead of running MCMC on it, you say, OK, I'm going to train this
neural network.
So this won't yet be worth it for just one data set.
Now, the cool thing is if you want to keep your actual model, so whatever you write
down in PMC or Stan, you want to keep that fixed, but now plug in different data sets.
That's where amortized inference really shines.
So for instance, there was this one huge analysis in the UK where they had like
intelligence study data from more than one million participants.
And so for each of those participants, they again had a set of observations.
And so for each of those one million participants, they want to perform
posterior inference.
It means if you want to do this with something like MCMC or anything
non amortized, you would need to fit one million models.
So you might argue now, OK, but you can parallelize this across like a thousand
cores, but still that's that's a lot, that's a lot to show.
Now, the cool thing is the model was the same every single time.
You just had a million different data sets.
And so what these people did then is train a neural network once and then it
will train for a few hours, of course.
But then you can just sequentially feed in all these one million data sets.
And for each of these one million data sets, it takes way, way less than one
second to generate tens of thousands of posterior samples.
But that didn't really answer your question.
So your question was about what, how can we generalize in the model space?
And that's a really hard problem because essentially what these neural networks
learn is to give you some posterior function if you feed in a data set.
Now, if you have a domain shift in the model space, so now you want inference
based on a different model, and this neural network has never learned to do that.
So that's tough.
And that's, that's a hard problem.
And essentially what you could do and what we are currently doing in our
research, but that's cutting edge, is expanding the model space.
So you would have a very general formulation of a model and then try
to amortize over this model.
So that different configurations of this model, different variations,
could just be expected to showcase the model essentially.
Okay.
So can you take an example maybe to, to give an idea to, to listeners?
How, how that would work?
Absolutely.
So we have one preprint about sensitivity aware amortized vision inference.
And what we do there is essentially have a kind of a, a, a, a, a, a, a,
essentially have a kind of multiverse analysis built in to the neural network
training.
So give some, some background multiverse analysis basically says, okay,
what are all the preprocessing steps that you could take in your analysis?
And you can code those.
And now you're interested in like, what if, what if I had chosen a
different preprocessing technique?
What if I had chosen a different way to standardize my data?
Um, then also the classical prior sensitivity or likelihood sensitivity,
um, analysis, like what happens if I do power scaling on my prior power
scaling on my posterior?
So we also encode this.
Um, what happens if I bootstrap some of my data or just have a perturbation of my
data?
Um, what if I add a bit of noise to my data?
So these are all slightly different models.
What we do is essentially keep track of that during the training phase and just
encode it into a vector and say, well, okay, now we're doing preprocessing
choice number seven, um, and scale the prior to the power of two, don't scale
the likelihood and don't do any perturbation and feed this as an
additional information into the neural network.
Now the cool thing is during inference phase, once we're done with the training,
you can say, Hey, here's a data set.
Now pretend that we chose preprocessing technique number 11 and prior
scaling of power 0.5.
What's the posterior now?
Because we've amortized over this larger, more general model space.
We also get valid posterior inference.
If we train for long enough over these different configurations of model.
And essentially, if you were to do this, um, in a, like with MCMC, for instance,
um, you would refit your model every single time.
Yeah.
And so here you don't have to do that.
Okay.
Yeah.
I see.
That's super.
Yeah.
That's super cool.
And I feel like, so that would be mainly the main use cases would be, as you were
saying, when, when you're getting into really high data territory and you have
what's changing is mainly the data side, mainly the data set and to be even more
precise, not really the data set, but the data values, because the data set is
supposed to be like point the same, like you would have the same columns, for
instance, but the values of the columns would change all the time.
And the model at the same time doesn't change.
Is that like, that's really for now, at least the best use case for that kind of method.
Yes.
And this might seem like a very niche case.
But then if you look at, um, like Bayesian workflows in practice, um, this,
this topic of this scheme of many model, it doesn't necessarily mean that you have
a large number of data.
This might also just mean you want extensive cross validation.
So assume that you have one data set with 1000 observations.
Now you want to run, leave on our cross validation, but for some reason you can
do the Pareto smooth important sampling version, which would be much faster.
So you would need 1000 model refits, even though you just have one data set because
you want 1000 cross validation refits.
Um, maybe can you explicit what your meaning by cross validation here?
Because that's, uh, that's not a term that's used a lot in the Bayesian
framework, I think.
Yeah, of course.
Um, so especially in a Bayesian setting, uh, there's this approach of leave on our
cross validation where you would fit your posterior based on all data points, but
one, and that's why it's called leave one out because you take one out and then fit
your model, fit your posterior on the rest of the data.
And now you're interested in the posterior predictive performance of this one
left out observation.
Yeah.
And that's called validation.
Yeah.
Go ahead.
Yeah, no, just I'm going to let you finish, but yeah, for listeners, um, familiar
with the frequented framework, that's something that's really heavily used in,
in that framework, cross validation.
And it's very similar to the machine learning concept of cross validation, but
in the machine learning area, you would rather have something like five
fold, in general, K fold cross validation where you would have larger splits of your
data and then use parts of your whole data set as the training data set and the
rest for evaluation.
Essentially, like leave on a cross validation, just puts it to the extreme.
Everything but one data point is your train data set.
Yeah.
Yeah.
Okay.
Yeah.
Damn, that's super fun.
And is there, is there already a way, uh, for people to try that out or is it mainly
for now implemented for papers, uh, and, and you are probably, I'm guessing, uh,
working on that with, uh, with a key and all his group in, in Finland to make that
more open source, helping people use packages to do that.
And what's the state of the, of the things here?
Yeah.
Um, that's a great question.
Um, and in fact, the state of usable open source software is far behind what we have
for likelihood based MCMC based inference.
So we currently don't have something that's comparable to pi MC or Stan.
Um, our group is developing or actively developing a software that's called base flow.
Um, that's because like the name because like base, we're doing Bayesian inference.
Um, and essentially the first neural network architecture that was used for
this amortized Bayesian inference are so-called normalizing flows, um, conditional normalizing
flows to be precise.
And that's why, why, why the name base flow, um, came to be.
But now, um, they have a bit of a different take because now we have a whole lot of
generative neural networks and not only normalizing flows.
So now we can also use, for example, um, score-based diffusion models that are mainly
used for image generation, um, and AI, um, or consistency models, which are essentially
like a distilled version of, um, score-based diffusion models.
And so now base flow doesn't really capture that anymore.
But now what the base flow Python library specializes in is, um, defining
principled amortized Bayesian workflows.
So the meaning of base flow slightly shifted to amortized Bayesian workflows.
And hence the name base flow again, um, and the focus of base flow and the aim of
base flow is two-fold.
So first we want a library that's good for actual users.
So this might be researchers who just say, Hey, here's my data set.
Here's my model, my simulation program.
And please just give me fast posterior samples.
So we want, like, um, usable high-level interface with sensible default values
that mostly work out of the box and an interface that's mostly self-explanatory.
Also, of course, good teaching material and all this.
Um, but that's only one side of the coin because the other large goal of
base flow, um, is that it should be usable for machine learning researchers who
want to advance amortized Bayesian inference methods as well.
And so the software in general, um, is structured in a very modular way.
So for instance, you could just say, Hey, take my current pipeline, my current
workflow, um, but now try out a different loss function because I have a new fancy
idea.
I want to incorporate more likelihood information.
Um, and so I want to alter my loss function.
So you would have your general program, um, because of the modular architecture
there, could just say, take the current loss function and replace it with a
different one that is used to the API.
And we're trying to, you know, doing both and serving both interests, user-friendly
side for actually applied researchers who are also currently using base flow.
Um, but then also the machine learning researchers, um, with completely
different requirements for, for this piece of software.
Maybe we can also do base flow documentation, um, and the current
project website in the two notes.
Yeah.
Yeah, we should definitely do that.
Um, definitely going to try that out myself.
That sounds like fun.
Uh, I need a use case, but, uh, as soon as I have a use case, I'm definitely
going to try that out because it sounds like a lot of fun.
Um, yeah, several questions based on that and then so on for being so clear.
Um, and so detailed on these.
Um, so first we talked about normalizing flows in episode 98 with
Marie-Lou Gabrie, uh, definitely recommend listeners to, uh, listen to
that for some background.
Um, and, um, questions.
So base flow, yeah, definitely we need that in the show notes.
Uh, and I'm going to install that in my, in my environment.
And I'm guessing, so you're saying that that's in Python, right?
The, the package.
Yes.
The core packages in Python and we're currently refactoring to Keras.
So by the time, uh, this podcast episode is aired, um, we'll have a new major
release version, hopefully.
Um, so you're agnostic to the actual machine learning backend.
So then you could choose TensorFlow, PyTorch, or Jax, whatever integrates
best with what you're currently proficient in and what you might be currently
using in other parts of the project.
Hmm.
Okay.
That was going to be my question, like, because I think, uh, while preparing
for the episode, I, I saw that you were mainly using PyTorch.
So that was going to be my question.
What is that based on?
So the backend could be PyTorch, Jax, um, or, um, uh, what, what did you say?
The last one was TensorFlow.
Yeah.
I'll always forget about, about like all these names, um, I really know PyTorch.
So that's why I like the other ones and Jax, of course, for PyMC.
Um, and then, so my question is the workflow, what would it look like if
you're using base flow?
Um, because you were saying the model, you could write it in standard
PyMC or TensorFlow, for instance.
Um, although I don't know if you can write patient models with TensorFlow
anymore, uh, well, anyways, let's say PyMC or, or, or Stan, um, you write your model.
But then the sampling of the model is done with the neural network.
So that means, for instance, PyTorch, uh, or, or Jax.
How does that work?
Do you have then to write the model in a Jax compatible way, or is the
translation done by the package itself?
Yeah, that's a great question.
Um, with the touches on many different topics and considerations and also on
future roadmap for base flow.
Um, so this class of algorithms that are implemented in base flow, these
amortized Bayesian inference algorithms to give you some background there.
Um, they originally started in simulation based inference.
It's also sometimes called likelihood free inference.
Um, so essentially it is Bayesian inference when you don't bring a close
form likelihood function to the table, but instead you only have some generic
forward simulation program.
So you would just have your prior as some Python function or C plus plus
function, whatever, any function that you could call, and it would return you
a sample from the prior distribution.
You don't need to write it down in terms of distributions actually, but you
only need to be able to sample from it.
And then the same for the likelihood.
So you don't need to write down your likelihood in like at PyMC or Stan in
terms of a probability distribution, in terms of density distribution or
densities, um, but instead it's just got to be some simulation program, which
takes in parameters and then outputs data.
What happens between these parameters and the data is not necessarily like
probabilistic in terms of closed form distributions could also be some
untractable, um, differential equations could be essentially everything.
So for base flow, this means that you don't have to input something like a
PyMC or Stan model, which you write down in terms of distributions, but it's
just a generic forward model that you can call and you will get a tuple of a
parameter draw and the data set.
So you'd usually just do it in NumPy.
So you would write if I'm using base flow, I would write it in NumPy.
It would probably be the easiest way.
It could probably also write it in Jax or in PyTorch or in TensorFlow or
TensorFlow probability, whatever you want to use and like behind the scenes.
But essentially what we just care about is that the model gets a tuple of
parameters and then data that has been generated from these parameters.
For the new network training process.
That's super fun.
Yeah, yeah, definitely want to see that.
Do you have already some Jupyter notebook examples on the repo?
Are you working on that?
Yeah, like currently it's a full-fledged library.
It's been under development for a few years now and we also have an active
user base right now.
It's quite small compared to other Bayesian packages, but we're growing it.
Yeah, no, that's not cool.
In documentation, there are currently I think seven or eight tutorial notebooks
and then also for a base on the beach like this conference in Australia that we
just talked about earlier.
We also prepared a workshop and we're also going to link to this Jupyter
notebook in the show notes.
Yeah, definitely we should link to some of these Jupyter notebooks in the show
notes and I'm thinking you should, like if you're down, you should definitely
come back to the show and but for a webinar, I have another format that's
modeling webinar where you would come to the show and share your screen and go
through the model code live and people can ask questions and so on.
I've done that already on a variety of things.
Last one was about causal inference and propensity scores.
Next one is going to be on about helper space GP decomposition.
Um, so yeah, if you're down, you should definitely come and do a demonstration
of base flow and amortized Bayesian inference.
I think that would be super fun and very interesting to, to people.
Absolutely.
Then to answer the last part of your question, um, yeah, like if you currently
have a model that's written down in PMC or Stan, that's a bit more tricky to
integrate, um, because essentially what all we need in base flow are samples
from the prior predictive distribution.
If you talk in Bayesian terminology, um, yeah, and if your current model can,
can do that, that's fine.
That's all you need right now.
And then like the base, you can have like a, you can have like a Bayesian,
like a PIMC model and just do PM dot sample, prior predictive, say that as an
MPI, big, non-point multidimensional array and pass that to base flow.
Yes.
Okay.
Just all you need are tuples of the ground truth parameters of the data
drainage process.
So essentially like the result of your prior call and then the result of your
likelihood call with those prior parameters.
Mm hmm.
Mm hmm.
So you mean what the likelihood samples look like once you fix the prior
parameters to some value?
Yes.
So like in practice, um, you would just call your prior function.
Yeah.
Then get a sample from the prior.
So parameter vector.
Yeah.
And then plug this parameter vector into the likelihood function.
And then you get one simulated synthetic data set.
Mm hmm.
Yeah.
And you just need those two.
Mm hmm.
Okay.
Super cool.
Uh, yeah, definitely sounds like a lot of fun.
And you definitely do a webinar about that.
I'm very excited about that.
Uh, yeah, fantastic.
Um, and so that was one of my main question on that other question is, uh, I'm
guessing you are a lot of people working on that, right?
Because your roadmap that you just talked about is, is, is super big.
It's having a package that's designed for users, but also for researchers is
quite, that that's really a work.
So I'm hoping you're not a lot doing that.
No, we're currently a team of about a dozen people.
Mm hmm.
No, yeah, that makes sense.
And it's an interdisciplinary team.
So, um, like a few people with a hardcore, like software engineering background.
Um, like some people with a machine learning background.
Um, and some people from the cognitive sciences and also a handful of physicists.
Cause in fact, these amortized Bayesian inference methods are particularly
interesting for physicists, some example for astrophysicists who have these
gravitational wave inference problems where they have massive data sets and
running MCMC on those would be quite cumbersome.
So if you have this huge in stream of data, um, and you don't have this
underlying likelihood density, but just some simulation program that might
generate sensible, like gravitational waves, then amortized Bayesian
inference really shines there.
Okay.
So that's exactly the case you were talking about where the model doesn't
change, but you have a lot of different data sets.
Yeah, exactly.
Cause I mean, what, what you're trying to run inference on is your physical model.
Mm hmm.
And that doesn't change.
I mean, it does.
And then, then again, physicists are very good.
Physicists have a very good understanding and very good models of the
world around them.
And that's made one of the huge of the largest differences, people from the
cognitive sciences, um, where, you know, the models of the human brain, for
instance, are just, it's such a tough thing to model.
And there's so much, and so much uncertainty in the model building process.
Yeah, for sure.
Okay.
Yeah, I think I'm starting to understand the idea.
And, uh, yeah.
So actually episode 101, uh, was exactly about that.
Um, black holes, collisions, gravitational waves.
Uh, and I was talking with, uh, LIGO researchers, uh, Christopher
Berry and John Beach.
Um, and we talked exactly about that, their problem with big data sets.
Uh, they are mainly using sequential Monte Carlo.
Uh, but I'm guessing they would also be interested in, in amortized Bayesian
inference.
So, um, yeah, uh, Christopher and John, if you're listening, uh, if you're
just reaching out to Marvin and use base flow.
Um, and, uh, listeners, uh, this episode will be in the show notes also.
If you want to, um, give it a listen, that's, uh, that's a really fun one
also learning a lot of stuff, but, uh, the crazy universe we live in.
Um, and actually a weird question I have is why is it called amortized?
Bayesian inference.
Yeah.
Um, the reason is that we have this two stage process where we would first pay
up front with this long neural network training phase, but then once we're done
with this, this cost of the training phase amortizes over all the posterior
samples that we can draw within a few milliseconds.
I see that makes sense.
That makes sense.
Um, and so I think something you're also working on is something, something
that's called diffusion.
And you do that in particular with, uh, for multimodal simulation based inference.
Uh, how is that related to amortized Bayesian inference if at all?
And what is it about?
Yeah.
Um, so I'm going to answer these two, two questions, um, in reverse order.
So first about the relation between simulation based inference and amortized
Bayesian inference.
Um, so to, to give you a bit of, um, history there, um, like simulation based
inference, essentially Bayesian inference based on simulations where we don't assume
that we have access to a likelihood density, but instead we just assume that we
can sample from the likelihood, um, and essentially simulate from the model.
Um, in fact, the likelihood is still present, but it's only implicitly defined.
And we don't have access to the density.
That's why likelihood free inference doesn't really hit what's happening here.
But instead, like in, in the recent years, people have started adopting the term
simulation based inference because we do Bayesian inference based on simulations
instead of likelihood densities.
Um, so methods that have been used for quite a long time now in the simulation
based inference, um, research area, um, but for example, rejection ABC, so approximate
Bayesian computation, or then like ABC, SMC, so combining ABC, uh, with sequential
Monte Carlo on, um, essentially the next iteration there was throwing neural
network at simulation based inference.
And that's exactly this neural posterior estimation that I talked about earlier.
And now what researchers noticed is, Hey, when we train a neural network for
simulation based inference, instead of running rejection approximate Bayesian
computation, then we get amortization for free as a site product.
It's just a byproduct of using a neural network for simulation based inference.
And so in the last, maybe four to five years, people have mainly focused on
this algorithm that's called neural posterior estimation for simulation based
inference.
And so all developments that happened there and all the research that happened
there, almost all the research, sorry, um, focused on cases where we don't have
any likelihood density.
So we're purely in the simulation based case.
Now, with our view of things, when we come from a Bayesian inference, like
likelihood based setting, can say, Hey, amortization is not just a random
coincidental byproduct, but it's a feature and we should focus on this feature.
And so now what we're currently doing is moving this idea of amortized Bayesian
inference with neural networks back into a likelihood based setting.
So we started using likelihood information again, for example, using
likelihood densities if they're available or learning information about
the likelihood.
So like a surrogate model on the fly and then again using this information
for better posterior inference.
So we're essentially bridging simulation based inference and likelihood
based Bayesian inference again with this goal, larger goal of amortization.
If we can do it.
And so this work on deep fusion essentially addresses one huge
shortcoming of neural networks when we want to use them for amortized Bayesian
inference.
And that is in situation where we have multiple different sources of data.
So for example, imagine you're a cognitive scientist and you run an
experiment with subjects and for each test subject, you give them a decision
making task, but at the same time, while your subjects solve the decision
making task, you wire them up with an EEG to measure the brain activity.
So for each subject across maybe a hundred trials, you know, I have is
both an EEG and the data from the decision making task.
Now, if you want to analyze this with Pi MC or Stan, what you would just
do is say, hey, well, we have two data generating processes that are governed
by set of shape parameters.
So the first part of the likelihood would just be because we know process
for the decision making task, we just model the reaction times.
At least in a procedure there in the cognitive science.
And then for the second part, we have a second part of the likelihood that
we evaluate that somehow handles this EEG, these EEG measurements.
For example, a spatial temporal process, or just like some summary
statistics that are being computed there, just however you would usually
compute your EEG, then you add both to the PDF of the likelihood, and then
you can call it a day.
Now, you cannot do that in neural networks because you have no straight
forward sensible way to combine these reaction times from the decision
making task and the EEG data, because you cannot just take them and
slap them together.
They are not compatible with each other because these information data
sources are heterogeneous.
So you somehow need a way to fuse these sources of information so that
you can then feed them into the neural network.
That's essentially what we're studying in this paper, where you could
just get very creative and have different schemes to fuse the data.
So you could use these attention schemes that are very hip and large
language models right now with transformers essentially, and have these
different data sources attend or listen essentially to each other.
With cross attention, you could just let the EEG data inform your decision
making data or just have the decision making data inform the EEG data.
So you can get very creative there.
You could also just learn some representation of both individually
and then concatenate them and feed them to the neural network.
Or you could do very creative and weird mixes of all those approaches.
And in this paper, we essentially investigate, I have a systematic
investigation of these different options, and we find that the most
straightforward option works the best overall.
And that's just learning fixed size embeddings of your data sources
individually and then just concatenating them.
It turns out then we can use information from both sources in an
efficient way, even though we're doing inference with neural networks.
And maybe what's interesting for practitioners is that we can compensate
for missing data in individual sources.
So in the paper, we induced missing data by just taking these EEG data
and decision making data and just randomly dropping some of them.
And the neural networks have learned, like when we do this fusion process,
the neural networks learn to compensate for partial missingness in both sources.
So if you just remove some of the decision making data, the neural network
learned to use the EEG data to inform your posterior, even though the data
in one of the sources are missing, the inference is pretty robust then.
And again, all this happens without model refits.
So you would just account for that doing training.
Of course, you have to do this like random dropping of data doing a training
phase as well.
And then you can also get it doing the inference phase.
Yeah, that sounds, yeah, that's really cool.
Maybe that's a bit of a, like a small piece of this paper in our larger roadmap.
This is essentially taking this amortized vision inference up to the level
of trustworthiness and robustness and all these gold standards that we currently
have for likelihood-based inference in PyMC or Stan.
Yeah, yeah.
And there's still a lot of work to do because of course, like there's no free lunch.
And of course, there are many problems with trustworthiness.
And that's also one of the reasons why I'm here with Aki right now.
Because Aki is so great at Bayesian workflow and trustworthiness, good diagnostics.
That's all, you know, all the things that we currently still need for trustworthy
amortized vision inference.
Huh.
Yeah, so maybe when I talk a bit more about that and what you're doing on that,
that sounds like something very interesting.
So one huge advantage of an amortized Bayesian sampler is that evaluations
and diagnostics are extremely cheap.
So for example, there's this gold standard method that's called simulation-based
calibration where you would sample from your model and then like a sample from
your prior predictive space and then refit your model and look at your coverage,
for instance, in general, look at the calibration of your model on this
potentially very large prior predictive space.
So you naturally need many model refits, but your model is fixed.
So if you do it with MCMC, it's a gold standard evaluation technique,
but it's very expensive to run, especially if your model is complex.
Now, if you have an amortized estimator, simulation-based calibration
on thousands of data sets takes a few seconds.
So essentially, and that's my goal for this research visit with Aki here in Finland,
is trying to figure out what are some diagnostics that are gold standard,
but potentially very expensive, up to a point where it's infeasible to run
on a larger scale with MCMC, but we can easily do it with an amortized estimator.
With the goal of figuring out, like, can we trust this estimator, yes or no?
As you might know from neural networks, we just have no idea what's happening
inside the neural network.
And so we currently don't have these strong diagnostics that we have for MCMC,
like, for example, our head.
There's no comparable thing for a neural network.
So one of my goals here is to come up with more good diagnostics
that are either possible with MCMC, but very expensive, so we don't run them.
But they will be very cheap with an amortized estimator,
or the second thing just specific to an amortized estimator,
just like our head is specific to MCMC.
Okay, yeah, I see.
Yeah, that makes tons of sense.
And actually, so, I mean, I would have more technical question on these,
but I see the time running out.
I think something I'm mainly curious about is the challenges,
the biggest challenges you face when applying amortized patient inference
and diffusion techniques in your projects, but also, like, in the projects you see.
I think that's going to also give a sense to these nerves of when and where
to use these kind of methods.
Yeah, that's a great question.
And I'm more than happy to talk about all these challenges that we have,
because there's so much room for improvement, because, like, these amortized methods,
they have so much potential that we still have a long way to go
until they are as usable and as straightforward to use as the current MCMC samplers.
And in general, one challenge for practitioners is that we have most of the problems
and hardships that we have in PMC or STAN.
That is, that researchers have to think about their model in a probabilistic way
and a mechanistic way.
So instead of just saying, hey, I click on t-test or linear regression
in some graphical user interface, they actually have to come up with a data-generating process
and have to specify their model.
And this whole topic of model specification is just the same in amortized workflow.
Because it's some way we need to specify the Bayesian model.
And now on top of all this, we have a huge additional layer of complexity,
and this is defining the neural networks.
In amortized Bayesian inference, nowadays we have two neural networks.
The first one is a so-called summary network,
which essentially learns the latent embedding of the data set.
Essentially, those are optimal learned summary statistics.
And optimal doesn't mean that they have to be optimal to reconstruct the data,
but instead optimal means they're optimal to inform the posterior.
So for example, in a very, very simple toy model, if you have just a Gaussian model
and you just want to perform inference on the mean.
Then a sufficient summary statistic for posterior inference on the mean would be the mean.
Because that's all you need to reconstruct the mean.
This sounds very totalogical for you.
And then again, the mean is obviously not enough to reconstruct the data
because all the variance information is missing.
So what the summary network learns is something like the mean.
So summary statistics that are optimal for posterior inference.
And then the second network is the actual generative neural network.
So like a normalizing flow, a score-based diffusion model, consistency model,
flow matching, whatever conditional generative model you want.
And this will handle the sampling from the posterior.
And these two networks are learned end-to-end.
So you would learn your summary statistic, output it,
feed it into the posterior network, the generative model,
and then have one evaluation of the loss function optimized both end-to-end.
And so we have two neural networks, a lot of story short,
which is substantially harder than just hitting, like, sample on a PMC or STAN program.
And that's an additional hardship for practitioners.
Now in base flow, what we do is we provide sensible default values for the generative neural networks,
which work in maybe like 80 or 90% of the cases.
It's just sufficient to have, for example, like a neural spline flow,
like some sort of normalizing flow with six layers and a certain number of units,
some regularization for robustness and, you know, cosine decay of the learning rates,
and all these machine learning parts to try to take them away from the user
if they don't want to mess with it.
But still, if things don't work, they would need to somehow diagnose the problems
and then, you know, play with the number of layers and this neural network architecture.
And then for the summary network, the summary network essentially needs to be informed by the data.
So if you have time series, you would look at something like an LSTM,
so this, like a long short-term memory, time series neural networks.
Or you would have, like, recurrent neural network or nowadays a time series transformer.
They're also called temporal fusion transforms.
If you have IID data, you would have something like a deep set or a set transformer
which respect this exchangeable structure of the data.
So again, we can give all the recommendations and sensible default values.
If you have a time series, try a time series transformer.
Then again, if things don't work out, users need to, you know, play around with these settings.
So that's definitely one hardship of amortized patient inference in general.
And for the second part of your question, hardships of this deep fusion,
it's essentially if you have more and more information sources
then things can get very complicated.
For example, just a few days ago, we discussed about a case where someone has 60, like,
60 different sources of information and they're all streams of time series.
Now we could say, hey, just slap 60 summary networks on this problem,
like one summary network for each main source.
That's going to be very complex and very hard to train,
especially if we don't bring that many data sets to the table for the neural network training.
And so there we somehow need to find a compromise.
Okay, what information can we condense and group together?
So maybe some of the time series sources are somewhat similar and actually compatible with each other.
So we could, for example, come up with six groups of 10 time series each.
Then we would only need six neural networks for the summary embedding.
So all these practical considerations that makes things just like as hard as in likelihood-based,
MCMC-based inference, but just a bit harder because of all the neural network stuff that's happening.
Did this address your question?
Yeah, yeah, it gives me more questions, but yeah, for sure, that does answer the question.
And when you're talking about transformer for time series, are you talking about the transformers,
like the neural network that's used in large language models, or is it something else?
It's essentially the same, but slightly adjusted for time series,
so that the statistics or these latent embeddings that you output still respect the time series structure,
where typically you would have this autoregressive structure.
So it's not exactly the same standard transformer, but you would just enrich it to respect the probabilistic structure in your data.
But at the core, it's just the same.
So at the core, it's an attention mechanism, like multi-head attention,
where the different parts of your data set could essentially talk or listen to each other.
So it's just the same.
Okay, yeah, that's interesting. I didn't know that existed for time series.
That's interesting.
That means, so because the transformer takes like one of the main thing is you have to tokenize the inputs.
So here you would tokenize like that there is a tokenation happening of the time series data.
You don't have to tokenize here, because the reason why you have to tokenize in large language models,
or natural language processing in general, is that you want to somehow encode your character,
so your words into numbers, essentially.
And we don't need that in Bayesian inference in general, because we already have numbers.
Yeah.
So our data already comes in numbers, so we don't need tokenization here.
Of course, if we had text data, then we would need tokenization.
Yeah, yeah, yeah.
Okay, okay. Yeah, it makes more sense to me.
Okay, that's fun. I didn't know that existed.
Do you have any resources about Transformer for time series that we could put in the show notes?
Absolutely. There's a paper that's called Temporal Fusion Transformers, I think.
I will send you the link.
Oh yeah, awesome. Yeah, thanks.
So we have this time series Transformer, Temporal Fusion Transformer implemented in Baseflow.
So now it's just like a very usable interface where you would just input your data,
and then you get your latent embeddings.
You can say like, I want to input my data, and I want as an output 20 learned summary statistics.
So that's all you need to do there.
Then you can go crazy and get creative with it.
Yeah, what would you do with these results, basically the outputs of the Transformer?
What would you use that for?
Those are the learned summary statistics that you would then treat as a compressed fixed-length version of your data
for the posterior network, for this generative model.
So then you use that afterwards in the model?
Exactly. So the Transformer is just used to learn summary statistics of the data sets that we input.
For instance, if you have a time series, like we did this for a COVID time series.
If you have a COVID time series worth like for a three-year period would be in daily reporting,
you would have a time series worth about a thousand time steps.
That's quite long as a condition into a neural network to pass in there.
And also, like, if now you don't have a thousand days, but a thousand and one days,
then the length of your input to the neural network would change, and your neural network wouldn't do that.
So what you do with a time series Transformer is compress this time series of maybe a thousand or maybe a thousand and fifty time steps.
Into a fixed-length vector of summary statistics.
Maybe you extract 200 summary statistics from that.
Okay, I see. And then you can use that in your neural network.
In the model that's going to be sampling your model.
In the neural network that's going to be sampling your model.
We already see that we're heavily overloading terminology here.
So what's the model actually?
So then we have to differentiate between the actual Bayesian model that we're trying to fit,
and then the neural networks or the generative model or generative neural network that we're using as a replacement for MCMC.
It's a lot of this taxonomy that's this odd when you're at the interface of deep learning and statistics.
Another one of those pickups are parameters.
Like in Bayesian inference parameters are your inference targets.
So you want posterior distributions on a handful of model parameters.
When you talk to people from deep learning about parameters, they understand the neural network weights.
Sometimes I have to be careful with the terminology and words used to describe things.
Because we have different types going on at different levels of abstraction in different functions.
So that means in this case, the transformer takes in time values, it summarizes them,
and it passes that on to the neural network that's going to be used to sample the Bayesian model.
Exactly.
And they are passed in as the conditions, like conditional probability, which totally makes sense.
Because like this generative neural network, it learns the distribution of parameters conditional on the data or summary statistics of the data.
So that's the exact definition of the Bayesian posterior distribution,
of the Bayesian model parameters conditional on the data.
That's the exact definition of the posterior.
Yeah, I see.
And that means, so in this case, oh yeah, no, I think my question was going to be,
why would you use these kind of additional layer on the time search data, but you answer that,
is that, well, what if your time search data is to be or something like that?
Exactly, it's not just being to be, but also just a variable length.
Because the neural network, like the generative neural network, it always wants fixed length inputs.
Like it can only handle, in this case of the COVID model, it could only handle input conditions with length 200.
And now the time series transformer takes part, so the time series transformer handles the part that our actual raw data have a variable length.
And time series transformers can handle data of variable length.
So they would just take a time series of length, maybe 500 time steps to 2,000 time steps,
and then always compress it to 200 summary statistics.
So this generative neural network, which is much more strict about the shapes and form of the input data,
will always see the same length inputs.
Yeah, yeah, okay, yeah, I see, that makes sense.
Awesome, yeah, super cool.
And so as you were saying, this is already available in Baseflow,
people can use this kind of transformer for time series.
Yeah, absolutely, for time series and also for sets, so for IED data.
Because if you just take an IED data set and input into a neural network,
the neural network doesn't know that your observations are exchangeable.
So it will assume much more structure than there actually is in your data.
So again, it has a double function, like a dual function of compressing data,
encoding the probabilistic structure of the data, and also outputting a fixed length representation.
So this would be a transformer or deep set is another option.
It's also implemented in Baseflow.
Super cool, yeah.
And so let's start winding down here because I've already taken a lot of your time.
Maybe a last few questions would be, what are some emerging topics that you see within deep learning
and probabilistic machine learning that you find particularly intriguing?
I've been to talk here a lot about really the nitty-gritty, the statistical detail and so on,
but now if we did zoom a bit and we start thinking about more long term.
Yeah, I'm very excited about two large topics.
The first one are generative models that are very expressive,
so unconstrained neural network architectures, but at the same time have a one step inference step.
So for example, people have been using score-based diffusion models a lot,
or flow matching for image generation, like for example, stable diffusion.
You might be familiar with this tool to generate input a text prompt and then you get fantastic images.
Now this takes quite some time, so like a few seconds for each image,
but only because it runs on a fancy cluster.
If you run it locally on a computer, it takes much longer.
And that's because the score-based diffusion model needs many discretization steps
in this denoising process during inference time.
And now throughout the last year, there have been a few attempts
on having these very expressive and super powerful neural networks,
but they are much, much faster because they don't have these many denoising steps.
Instead, they directly learn a one step inference, so they could generate an image,
not like a thousand steps, but only in one step.
And that's very cutting edge or bleeding edge, if you will,
because they don't work that great yet, but I think there's much potential in there
because it's both expressive and fast.
And then again, we've used some of those for amortized Bayesian inference,
so we use consistency models and they have super high potential, in my opinion.
So with these advances in deep learning, we can always, often times,
we can use them for amortized Bayesian inference.
We just reformulate these generative models and slightly tune them to our tasks.
So I'm very excited about this.
In the second area, I'm very excited about our foundation models.
I guess most people are in AI these days.
So foundation models essentially means neural networks are very good at indistribution tasks,
so whatever is in the training data set, neural networks are typically very good
at finding patterns that are similar to the training set, what they saw in the training set.
Now in the open world, so if we are out of distribution, we have a domain shift,
distribution shift, model mis-specification, however you want to call it,
neural networks typically aren't that good.
So what we could do is either make them slightly better at our distribution,
or we just expand the indistribution to a huge space and that's what foundation models do.
For example, GPT-4 would be a foundation model because it's just trained on so much data.
I don't know how many, it's not terabyte anymore, it's essentially the entire internet.
So it's just a huge training set.
And so the world and the training set that this neural network has been trained on is just huge.
And so essentially we don't really have out of distribution cases anymore,
just because our training set is so huge.
And that's also one area that could be very useful for amortized Bayesian inference.
And to overcome the very initial shortcoming that you talked about,
where we would also like to amortize over different Bayesian models.
Hmm, I see, yeah, yeah, yeah.
Yeah, that would definitely be super fun.
Yeah, I'm really impressed and interested to see this interaction of deep learning artificial intelligence
and then the Bayesian framework coming on top of that.
That is really super cool, I love that, yeah.
Yeah, it makes me super curious to try that stuff out.
So to play us out Marvin, actually, this is a very active area of research.
So what advice would you give to beginners interested in diving into this intersection of deep learning
and probabilistic machine learning?
That's a great question.
Essentially I would have two recommendations.
The first one is to really try to simulate stuff.
Whatever it is that you're curious about, just try to write a simulation program
and try to simulate some of the data that you might be interested in.
So for example, if you're really interested in a soccer,
then code up a simulation program that just simulates soccer matches
and the outcomes of soccer matches.
So you can really get a feeling of the data generating processes that are happening
because probabilistic machine learning at its very core is all about data generating processes
and reasoning about these processes.
And I think it was Richard Feynman who said, what I cannot create, I do not understand.
That's essentially at the heart of simulation based inference in a more narrow setting
but probabilistic machine learning and machine learning more broadly, or science more broadly even.
So yeah, definitely simulating and running simulation studies can be super helpful
both to understand what's happening in the background
also to get a feeling for programming and to get better at programming as well.
And then the second advice would be to essentially find a balance
between these hands on, getting your hands dirty, type of things like
implement a model in white horse or carers or solve some Kaggle tasks,
just some machine learning tasks.
But then at the same time also finding this balance to reading books
and finding new information to make sure that you actually know what you're doing
and also know what you don't know and what the next steps are to get better from the theoretical part.
And there are two books that I can really recommend.
The first one is Deep Learning by Ian Goodfellow.
It's also available for free online.
You can also link to this in the show notes.
It's a great book and it covers so much.
And then if you come from this Bayesian or statistics background
you see a lot of conditional probabilities in there
because a lot of deep learning is just conditional generative modeling.
And then the second book would in fact be Statistically Rethinking by Richard McElrath.
It's a great book and it's not only limited to Bayesian inference but more.
Also a lot of causal inference of course.
Also just thinking about probability and the philosophy behind this whole probabilistic modeling topic more broadly.
So earlier today I had a chat with one of the student assistants that I'm supervising
and he said, hey Marvin, I read Statistically Rethinking a few weeks ago
and today I read something about score-based diffusion models.
So these like state-of-the-art deep learning models that are used to generate images.
Because I read Statistically Rethinking it all made sense.
There's so much probability going on in these score-based diffusion models
and Statistically Rethinking really helped me understand that.
And at first I couldn't believe it but it totally makes sense.
Because Statistically Rethinking is not just a book about Bayesian workflow and Bayesian modeling
but more about reasoning about probabilities and uncertainty in a more general way
and it's a beautiful book so I'd recommend those.
Nice, yeah, so definitely let's put those two in the show notes.
Marvin, so of course I've read Statistically Rethinking several times so I definitely agree.
The first one about deep learning I haven't yet but I will definitely read it
because that sounds really fascinating so when I get that book.
Fantastic, well thanks a lot Marvin, that was really awesome.
I really learned a lot, pretty sure listeners did too so that's super fun.
You definitely need to come back to do a modeling webinar with us
and show us in action what we talked about today with the baseball package.
It's also I guess going to inspire people to use it and maybe contribute to it.
But before that of course I'm going to ask you the last two questions I ask every guest at the end of the show.
First one, if you had unlimited time and resources which problem do you try to solve?
That's a very loaded question because there's so many very very important problems to solve
like big picture problems like peace, world hunger, global warming, all those.
With my background I don't really know how to contribute significantly with a huge impact on those problems.
My consideration is essentially a trade-off between how important is the problem
and what impact is it on solving the problem or addressing the problem.
And what impact could I have on solving the problem.
And so I think what would be very nice is to make probabilistic inference
or Bayesian inference more particular like accessible, usable, easy and fast for everyone.
And that doesn't just mean methods, machine learning researchers
but essentially means anyone who works with data in any way.
And there's so much to do like the actual Bayesian model in the background.
It could be huge, it could be like a base GPT, like trade GPT but just for base.
Just with the sheer scope of amortization and different models and different settings and so on.
So that's a huge, huge challenge like on the back-end side.
But then on the front-end and API side I think it also has many different sub-problems there
because it would mean people could just write down a description of their model in plain text language
like a large language model and don't actually specify everything via programming.
Maybe also just sketch out some data like expert elicitation
and all those different topics I think there's this bigger picture
that thousands of researchers worldwide are working on so many niche topics there
but having this overarching base GPT kind of thing would be really cool.
So I probably choose that to work on.
It's a very risky thing so that's why I'm not currently working on it.
Yeah, I love that.
Yeah, that sounds awesome. Feel free to collaborate with me on that.
I would definitely be there on that. Absolutely amazing.
Send me an email when you start working.
We'll be happy to join the team.
Second question, if you could have dinner with any great scientific mind, dead, alive or fictional, who would it be?
Again, very loaded question, super interesting question.
There are two huge choices.
I could either go with someone who's currently alive
and feel like I want their take on the current state of the art and future directions and so on.
Then the second huge option what I guess many people would go with is someone who's been dead for two to three centuries.
And I think I'd go with the second choice.
So really take someone from way from the past and that's because of two reasons.
Of course, speaking to today's scientists is super interesting and I would love to do that.
They have access to all the state of the art technology and they know about all the latest advancements.
So if they have some groundbreaking creative idea to share and do that they come up with, they could just implement it and make them actionable.
And the second reason is that today's scientists have a huge platform because they're on the internet.
So if they really want to express an idea they could just do it on Twitter or wherever.
So there's other ways to engage with them apart from having a magical dinner.
So I would choose someone from the past and in particular I think Ada Loveless would be super interesting for me to talk to.
Essentially because she's widely considered the first programmer.
The craziest thing about this is she's never had access to a modern computer.
So she wrote the first program but the machine wasn't there yet.
So that's such a huge leap of creativity and genius.
And so I'd really be interested in if Ada Loveless saw what's happening today,
all the technology that we have with generative AI, GPU clusters and all these possibilities.
What's the next leap forward?
What's today's equivalent of writing the first program without having the computer?
I'd really love to know this answer and there's currently no other way except for your magical dinner invitation to get this answer.
That's why I go with this option.
Yeah, awesome. I love it.
That definitely sounds like a marvelous dinner.
Awesome. Thanks a lot, Marvin. That was really a blast.
I'm going to let you go now because you've been talking for a long time.
I'm guessing you need a break.
But that was really amazing.
Thanks a lot for taking the time.
Thanks again to Matt Rossinski for this awesome recommendation.
I hope you loved it, Marvin.
And also Matt, me, I did.
So that was really awesome.
As usual, I'll put resources and a link to your website.
And also Marvin is going to add stuff to the show notes for those who want to dig deeper.
Thank you again, Marvin, for taking the time and being on this show.
Thank you very much for having me, Alex. I appreciate it.
This has been another episode of Learning Basions Statistics.
Be sure to rate, review and follow the show on your favorite podcatcher
and visit LearnBasedStats.com for more resources about today's topics
as well as access to more episodes to help you reach true patient state of mind.
That's LearnBasedStats.com.
Our theme music is Good Bayesian by Bababringtman,
fit MC Lance and Megaram.
Check out his awesome work at Bababringtman.com.
I'm your host, Alex Andorra.
You can follow me on Twitter at alexunderscoreandorra, like the country.
You can support the show and unlock exclusive benefits by visiting patreon.com.
Thank you so much for listening and for your support.
You're truly a Good Bayesian.
Change your predictions after taking information.
And if you're thinking, I'll be less than amazing.
Let's adjust those expectations.
Let me show you how to be a Good Bayesian.
Change calculations after taking fresh data.
Those predictions that your brain is making.
Let's get them on a solid foundation.
