So by the time, uh, this podcast episode is aired, um, we'll have a new major
release version, hopefully.
Um, so you're agnostic to the actual machine learning backend.
So then you could choose TensorFlow, PyTorch, or Jax, whatever integrates
best with what you're currently proficient in and what you might be currently
using in other parts of the project.
Hmm.
Okay.
That was going to be my question, like, because I think, uh, while preparing
for the episode, I, I saw that you were mainly using PyTorch.
So that was going to be my question.
What is that based on?
So the backend could be PyTorch, Jax, um, or, um, uh, what, what did you say?
The last one was TensorFlow.
Yeah.
I'll always forget about, about like all these names, um, I really know PyTorch.
So that's why I like the other ones and Jax, of course, for PyMC.
Um, and then, so my question is the workflow, what would it look like if
you're using base flow?
Um, because you were saying the model, you could write it in standard
PyMC or TensorFlow, for instance.
Um, although I don't know if you can write patient models with TensorFlow
anymore, uh, well, anyways, let's say PyMC or, or, or Stan, um, you write your model.
But then the sampling of the model is done with the neural network.
So that means, for instance, PyTorch, uh, or, or Jax.
How does that work?
Do you have then to write the model in a Jax compatible way, or is the
translation done by the package itself?
Yeah, that's a great question.
Um, with the touches on many different topics and considerations and also on
future roadmap for base flow.
Um, so this class of algorithms that are implemented in base flow, these
amortized Bayesian inference algorithms to give you some background there.
Um, they originally started in simulation based inference.
It's also sometimes called likelihood free inference.
Um, so essentially it is Bayesian inference when you don't bring a close
form likelihood function to the table, but instead you only have some generic
forward simulation program.
So you would just have your prior as some Python function or C plus plus
function, whatever, any function that you could call, and it would return you
a sample from the prior distribution.
You don't need to write it down in terms of distributions actually, but you
only need to be able to sample from it.
And then the same for the likelihood.
So you don't need to write down your likelihood in like at PyMC or Stan in
terms of a probability distribution, in terms of density distribution or
densities, um, but instead it's just got to be some simulation program, which
takes in parameters and then outputs data.
What happens between these parameters and the data is not necessarily like
probabilistic in terms of closed form distributions could also be some
untractable, um, differential equations could be essentially everything.
So for base flow, this means that you don't have to input something like a
PyMC or Stan model, which you write down in terms of distributions, but it's
just a generic forward model that you can call and you will get a tuple of a
parameter draw and the data set.
So you'd usually just do it in NumPy.
So you would write if I'm using base flow, I would write it in NumPy.
It would probably be the easiest way.
It could probably also write it in Jax or in PyTorch or in TensorFlow or
TensorFlow probability, whatever you want to use and like behind the scenes.
But essentially what we just care about is that the model gets a tuple of
parameters and then data that has been generated from these parameters.
For the new network training process.
That's super fun.
Yeah, yeah, definitely want to see that.
Do you have already some Jupyter notebook examples on the repo?
Are you working on that?
Yeah, like currently it's a full-fledged library.
It's been under development for a few years now and we also have an active
user base right now.
It's quite small compared to other Bayesian packages, but we're growing it.
Yeah, no, that's not cool.
In documentation, there are currently I think seven or eight tutorial notebooks
and then also for a base on the beach like this conference in Australia that we
just talked about earlier.
We also prepared a workshop and we're also going to link to this Jupyter
notebook in the show notes.
Yeah, definitely we should link to some of these Jupyter notebooks in the show
notes and I'm thinking you should, like if you're down, you should definitely
come back to the show and but for a webinar, I have another format that's
modeling webinar where you would come to the show and share your screen and go
through the model code live and people can ask questions and so on.
I've done that already on a variety of things.
Last one was about causal inference and propensity scores.
Next one is going to be on about helper space GP decomposition.
Um, so yeah, if you're down, you should definitely come and do a demonstration
of base flow and amortized Bayesian inference.
I think that would be super fun and very interesting to, to people.
Absolutely.
Then to answer the last part of your question, um, yeah, like if you currently
have a model that's written down in PMC or Stan, that's a bit more tricky to
integrate, um, because essentially what all we need in base flow are samples
from the prior predictive distribution.
If you talk in Bayesian terminology, um, yeah, and if your current model can,
can do that, that's fine.
That's all you need right now.
And then like the base, you can have like a, you can have like a Bayesian,
like a PIMC model and just do PM dot sample, prior predictive, say that as an
MPI, big, non-point multidimensional array and pass that to base flow.
Yes.
Okay.
Just all you need are tuples of the ground truth parameters of the data
drainage process.
So essentially like the result of your prior call and then the result of your
likelihood call with those prior parameters.
Mm hmm.
Mm hmm.
So you mean what the likelihood samples look like once you fix the prior
parameters to some value?
Yes.
So like in practice, um, you would just call your prior function.
Yeah.
Then get a sample from the prior.
So parameter vector.
Yeah.
And then plug this parameter vector into the likelihood function.
And then you get one simulated synthetic data set.
Mm hmm.
Yeah.
And you just need those two.
Mm hmm.
Okay.
Super cool.
Uh, yeah, definitely sounds like a lot of fun.
And you definitely do a webinar about that.
I'm very excited about that.
Uh, yeah, fantastic.
Um, and so that was one of my main question on that other question is, uh, I'm
guessing you are a lot of people working on that, right?
Because your roadmap that you just talked about is, is, is super big.
It's having a package that's designed for users, but also for researchers is
quite, that that's really a work.
So I'm hoping you're not a lot doing that.
No, we're currently a team of about a dozen people.
Mm hmm.
No, yeah, that makes sense.
And it's an interdisciplinary team.
So, um, like a few people with a hardcore, like software engineering background.
Um, like some people with a machine learning background.
Um, and some people from the cognitive sciences and also a handful of physicists.
Cause in fact, these amortized Bayesian inference methods are particularly
interesting for physicists, some example for astrophysicists who have these
gravitational wave inference problems where they have massive data sets and
running MCMC on those would be quite cumbersome.
So if you have this huge in stream of data, um, and you don't have this
underlying likelihood density, but just some simulation program that might
generate sensible, like gravitational waves, then amortized Bayesian
inference really shines there.
Okay.
So that's exactly the case you were talking about where the model doesn't
change, but you have a lot of different data sets.
Yeah, exactly.
Cause I mean, what, what you're trying to run inference on is your physical model.
Mm hmm.
And that doesn't change.
I mean, it does.
And then, then again, physicists are very good.
Physicists have a very good understanding and very good models of the
world around them.
And that's made one of the huge of the largest differences, people from the
cognitive sciences, um, where, you know, the models of the human brain, for
instance, are just, it's such a tough thing to model.
And there's so much, and so much uncertainty in the model building process.
Yeah, for sure.
Okay.
Yeah, I think I'm starting to understand the idea.
And, uh, yeah.
So actually episode 101, uh, was exactly about that.
Um, black holes, collisions, gravitational waves.
Uh, and I was talking with, uh, LIGO researchers, uh, Christopher
Berry and John Beach.
Um, and we talked exactly about that, their problem with big data sets.
Uh, they are mainly using sequential Monte Carlo.
Uh, but I'm guessing they would also be interested in, in amortized Bayesian
inference.
So, um, yeah, uh, Christopher and John, if you're listening, uh, if you're
just reaching out to Marvin and use base flow.
Um, and, uh, listeners, uh, this episode will be in the show notes also.
If you want to, um, give it a listen, that's, uh, that's a really fun one
also learning a lot of stuff, but, uh, the crazy universe we live in.
Um, and actually a weird question I have is why is it called amortized?
Bayesian inference.
Yeah.
Um, the reason is that we have this two stage process where we would first pay
up front with this long neural network training phase, but then once we're done
with this, this cost of the training phase amortizes over all the posterior
samples that we can draw within a few milliseconds.
I see that makes sense.
That makes sense.
Um, and so I think something you're also working on is something, something
that's called diffusion.
And you do that in particular with, uh, for multimodal simulation based inference.
Uh, how is that related to amortized Bayesian inference if at all?
And what is it about?
Yeah.
Um, so I'm going to answer these two, two questions, um, in reverse order.
So first about the relation between simulation based inference and amortized
Bayesian inference.
Um, so to, to give you a bit of, um, history there, um, like simulation based
inference, essentially Bayesian inference based on simulations where we don't assume
