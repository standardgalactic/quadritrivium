We model the weather to either be cloudy, rainy or sunny.
These are three states, discrete, that call for a categorical with a parameter vector
theta, for instance 0.2, 0.3, 0.5, but can we put a distribution over it?
Yes, we can, and that's the Dirichlet distribution.
Hi, the agenda for this video is that we will first give an intuition to this distribution,
then we derive the probability density function,
we will look at some visualizations, and in the end we will see how it is implemented
in tensorflow probability.
So let us start.
The weathershorten w was the free state categorical with either be cloudy, rainy or sunny.
Let's be a little bit more general and say w is a d-state categorical random variable,
and by random variable I just mean that it is distributed according to a categorical distribution.
And from this we know that this categorical takes a parameter vector theta, so we have
to define this parameter vector theta, and if we have d-states then we know this is also
d-dimensional, so it has d-dimensions.
But it's not just any vector with d-dimensions, it has to adhere to certain restrictions,
and surely since it encodes probabilities, all the theta d's, so the entries of the vector,
I will call this with the index d, have to be within 0 and 1, so for all the d's, and
additionally we have the property that the vector has to add up to 1, and I think this
makes sense because otherwise we would have too much probability mask, and for instance
then we would have like 0.5, and it would be more than 100% that any of these events
take place, so this is a really natural constraint on this.
But the big question now is like, how can we encode this?
And the solution is the Dirichlet distribution, but let's look at it a little more intuitively.
And for this I want to start with a simple case, so let's say let's start with d being
2, so just a two-state categorical variable, and let us draw a plot where we have theta
0 on the x-axis and theta 1 on the y-axis, and then let's introduce our constraints,
so of course we have two states, so we have a two-dimensional theta, so we are in the
two-dimensional plane here, and for the theta 0 we have the constraint, or let me first
introduce maybe for instance a 1 here, a 1 here, and a 0 here, and for the theta 0 it
has to be in this interval from 0 to 1, and for theta 1 it has to be in this interval,
right?
And then if we take the cross-product of these two intervals, then we get the square, essentially
this one here, and this is then what this property encodes, so that both of them have
to be between 0 and 1.
But then I think it's probably quite clear, if we take a point here, for instance the
point 0.8, 0.8, and then check the next property, then it does not hold anymore, this is 1.6,
and the same would be true, so let's make it like that, it does not work, and the same
would be here, so 0.2 and 0.2 would be 0.4, and that's definitely not one.
And of course if you look at the square and look at the number of points that adhered
this property, this is just the straight line connecting the 0.1 to 1.0, because all the
way on this line this property holds.
Let's extract this in a little bit more direct plot, so we see in the theta 0, theta 1 plot
again, we have with d1 and 1, we only have this line of possible values, so all possible
values have to be on that line.
And let's draw this three-dimensionally, because right now we were just drawing the
state space, and we haven't associated a probability density with it, and we will see since this
is a continuous strip, we are now looking at probability density functions, in contrast
to what we have for the categorical, which is a probability mass function, so keep this
in mind, and if you would draw this three-dimensionally, and we would get something like this, a theta
0, a theta 1, and then in the set axis we have the probability of the theta vector consisting
of theta 0 and theta 1, and we just said that the state space is restricted to this line
here, and if we would then look at probability density functions of this, or if we look at
the Dirichlet distribution, then we would get for instance something that looks like
this, it's like a peak here, and it goes down, and it all only has probability mass within
this or on this strip, so let's maybe note this by, it is an area in a sense, or it encodes
an area, but we can also have maybe something like this, and then we would get something
like this here, and if you watched my video on the beta distribution, which was the prior
for the Bernoulli distribution, then you might recall these shapes, and they are quite similar,
and if we look at the Dirichlet distribution for this case, then we would get something
like the probability density function is proportional to theta 0 raised to an alpha 0 minus 1 times
a theta 1 raised to an alpha 1 minus 1, with the restriction that alpha 0 and alpha 1 have
to be greater than 0, and that is really close to what we have from the beta distribution,
so recall, the beta distribution was given as follows, I mean it was a univariate distribution,
it wasn't defined over a vector, it was defined over the scalar theta, and it was proportional
to theta raised to an alpha minus 1 times 1 minus theta to beta minus 1, and the 1 minus
theta was nothing else than the complement, so in essence what we had here is also a two-state
categorical distribution, I think that's probably fairly obvious, I mean the two-state
categorical that is in essence the Bernoulli distribution, so in this case we expect a
beta distribution, but over this line in two dimensions, and I think this already naturally
encodes where we are heading with the Dirichlet distribution, that it will be a natural extension
of the beta distribution, but before we can be sure about this let us go to the three-dimensional
case, so let's now say that the state is three-dimensional, so we have three components in our theta distribution,
and then we are getting something like this, we have a theta 0, we have a theta 1, and we
have a theta 2, and we have the restriction theta 0, or let's again first introduce some
markers here, and then we have the restriction that theta 0 has to be in this range here,
theta 1 has to be in this range, and theta 2 has to be in this range, and if we take the
vector product of these three intervals then we will get the cube that I will really
really draw here, it's ugly, but I think you get the point, and now we can apply the same
again, for instance take this corner here, and this corner is at 0, 1, 1, and then we see well
the property does not hold anymore, this adds up to 2, and then we can ask ourselves what is the
collection of points inside this cube that adheres to the property that if we add up and it equals
1, and the only way we can find points in here is by the triangle that is, it has its vertices at
1, 0, 0, 0, 1, 0, and 0, 0, 1, and I think it becomes a little more clear if I draw it in
another plot here, so we have a theta 0, a theta 1, and a theta 2, and we are getting the triangle
in here, and now all the values of our potential theta vector have to be on this triangle, so the
values have to be on the triangle, and again if we look at the Dirichlet distribution then it would
give us something like p of theta vector is proportional to theta 0 to an alpha 0 minus 1
times theta 1 to an alpha 1 minus 1 times a theta 2 to alpha 2 minus 1, and requiring that alpha 0,
alpha 1, and alpha 2 are all greater than 0, and now we are getting it, this is just a natural
extension of the beta distribution. In order to complete on this statement, let us find the
general case, so in general we have d states, so beta array is from the rd, and if we now encode
these constraints that it has to be between 0 and 1, and then we need the property that it has to
add up to 1, then we get the values theta have to be on a simplex of d minus 1 dimensions,
and recall in the three-dimensional case it was a triangle, and the triangle is a two-dimensional
object, and the vertices, so with vertices at, well we have 1, 0, 0, and so on, then 0, 1,
and so on, or 0, and so on, and 0, 0, 1, and so on, and so forth, and these are all the vertices
defining it, the vertices in the dimensions defining a d minus 1 dimensional simplex,
or like the hyper-dimensional analogon of a triangle. Then we can also generalize the probability
density function, and we're getting p of, call this, this is the pdf, probability density function,
and p of vector d is proportional to the product from d is 0 to capital D minus 1,
over theta d raised to the alpha d minus 1, and then surely we can ask ourselves, well this is
just proportional, what about the normalization, and I excluded it so far, because it's not really
relevant for understanding distribution, you just have to show that it integrates to 1,
and so on, and so forth, and this will be given as follows, it is the gamma function of the summation
from d is 0 to capital D minus 1 over alpha d, over product from d is 0 to capital D minus 1,
over the gamma function of alpha d, and then we multiply this with what we had so far,
the product from d is 0 to capital D minus 1, over theta d raised to the alpha d minus 1,
and then we can define this as our Dirichlet distribution over the theta vector given an
alpha vector as parameter, and then we also know what we have to save, or in other words what's
the parameters of the distribution are, and in our case this is the alpha vector, and the alpha
vector has to be d-dimensional for the d-dimensional theta vector, and it has the restriction that
it is greater or strictly greater than 0 in each of its components, so this is like a component-wise
inequality, and with this we have a definition for the Dirichlet distribution, okay let's explore
how the alpha vector influences the distribution, here you will see a three-dimensional plot, so we
have the simplex, the triangle, and we have three theta values, we have theta 0, we have theta 1,
and we have theta 2, and we can select an alpha vector here in order to change the probability
density function, and the probability density is encoded by the color, and right now I selected an
alpha of 1 1 1, and we can see since the color is uniformly distributed over the triangle,
we have a uniform distribution over the triangle, and in other words this would mean that all of the
fetas would be equally likely, and then for instance we can set all the alpha values to 2,
and if we do this then we see that our distribution is no longer homogeneous,
and we have a peak here in the center, and we see if we look at a pdf value it's 4.3,
whereas in the corners it is 0, so it is like decreasing, which means that the probability
is more centered towards values that are equally as big, so for instance if you would choose
theta 1 to be 0.3 and 0.3 and 0.4, something like this where the theta values are almost as big as
the others, and of course this was now just a uniform change in the alpha values, and we can
also have like of the distribution here, and for instance if we do 2.0 2.0 1.0, then we see that
more probability mass is towards the theta 0 and the theta 1, and less towards the theta 3,
so in other words this would mean that it is more likely to choose higher theta 0 and theta 1 values
over theta 2 values, and we can play around with this, and we see that's how we shifted over the
simplex, and of course this can be shifted all the way around, and if we only select one of the
alpha values to be different from one and be bigger, then we see that the probability mass
is towards one corner, and this is for instance if we say that we live in a region where our
theta 2, which was associated with Sunnyweather for instance, is just more likely, and we can also
play it the other way around, we can for instance choose 0.9 0.9 0.9, and if we do this then it is
the opposite effect as if we would increase all the values over one, and we would get more probability
mass towards the edges and the corners, and if we only select one value to be lower than 1.0,
then we would get more probability mass towards the opposite edge of this corner that is associated
with this alpha value, and of course we can also play around and do one value bigger, one value lower,
and you probably get where I'm heading towards and this alpha value kind of shifts the distribution
over it, and we see the same two behaviors as we've seen with the beta distribution that we can
either shift the mass towards the boundaries or more towards the center. Okay, finally let's have
a look at how this implemented in TensorFlow probability, but before we go there, if you
enjoyed the video so far, then please consider liking it and subscribing, I would really appreciate
this. Okay, let's get into Python and TensorFlow. Here I am in an interactive Python session with
the packages already loaded, and let us start by creating the random variable for the fetus
array, and I will call this fetus weather, and we will be using the TensorFlow probability
distributions Dirichlet, and the argument is called the concentration, but in essence this is the
same as an alpha, and since this is the only argument that Dirichlet takes, we can also use
non-named arguments, and we want to define our Dirichlet distribution based on the fact that the
sunny weather is the most likely, then the rainy weather is the second most likely, and the cloudy
weather, so maybe for instance do 1.5 for the cloudy, it's the 2.0 for the rainy and 5.0 for the
sunny, then we can look at the distribution we just created, we see it's a Dirichlet, it has the event
shape of 3, because we expect the three-dimensional vector when we sample it, so let's do this,
let's sample the distribution, and then we get a vector with the entries that have to sum up to
1, which are the probabilities for our weather, and we can sample it again, and I think, well this
is probably an exception here, but the overall picture is probably what we would expect, that the
sunny probability is the highest, and then the rainy probability, and then the cloudy probability,
and we can also carry the probability density function with fetus weather dot probability,
and what is the probability of the vector that we used as an example earlier, which was 0.2,
20% for cloudy, 40% for rainy, and 50% for sunny, and we see it's 5.53, and now you might ask yourselves
what this is bigger than 1, but keep in mind the Dirichlet distribution is a probability
density function, so this is just a probability density, and it's not a probability mass, so it
does not have to be below 1, and this is almost the end of the video, and I will give you one more
trick, we saw that the normalization constant of the Dirichlet distribution was given by the
gamma functions, and this fraction of the two gamma functions is nothing else as the multivariate
beta function, and we can query the multivariate beta function with tf math dot l beta, and we
apply it to the alphas that we also put into the Dirichlet, so 1.5, 2.0, and 5.0, and then we would
be getting out minus 6.9, but this is the logarithm of it, so let's take the exponential, and then
we get this value, and this is our normalizing value for the Dirichlet, so we would divide by this
in order to normalize a non-normalized Dirichlet distribution. Okay, that's the end of the video,
I hope you enjoyed the interaction to the Dirichlet distribution, here you will find
similar videos, and I'm looking forward to see you next time.
