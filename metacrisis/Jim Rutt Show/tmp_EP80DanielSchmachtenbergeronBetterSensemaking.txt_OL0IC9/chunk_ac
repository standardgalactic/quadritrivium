I have to understand the problem embedding landscape of the adjacent problems and the adjacent topics and the adjacent meaningful things.
And I have to start thinking through second and third order effects better, whether I'm designing a proposition or designing a piece of technology or designing a company to solve a problem to say, if that solution is effective, what other things is it likely to do?
And what harms could that cause to complex environments?
And then how do we actually factor that into the design process?
Yeah, all true. But how the hell do you get a human to actually be able to process at that level?
And again, the humans aren't going to get smart enough to do that. None of us can do that, right?
And so the answer has got to be, as you keep pointing to some form of collective sense making and collective decision making that's high fidelity and in congruence with reality, I think.
I mean, I don't feel any sense that education plus a better child rearing environment at home is going to get anybody to be smart enough to deal with that constellation of issues.
Well, so notice, as you said that, you know, you and I are probably more oriented to try to sense make the world an average population and have better educational resources, but still can't make sense of everything.
But you also notice that when we talk, I know that you know things that are important that I want to know that I don't and vice versa.
So I'm actively seeking to try to understand your opinion.
And so there's an interesting balance that is important to understand that is that one of the dispositions beyond empiricism that orients towards the capacity for collective sense making to emerge.
I'm neither oriented for agreeability nor disagreeability with you, because I'm actually oriented toward respect relationally, but then also respect to reality objectively.
So there's like a intersubjective and an objective respect.
So my objective respect for reality is I'm not going to agree with you if you're wrong, but I'm also not going to listen to you where you might be right.
And so I want to listen in good faith, seek to understand what you understand, push back where that's relevant, not because I have a disagreeable personality and I want to push back or have an agreeable personality and I want you to agree.
But because I care about what is true enough and I'm in good faith with you enough that we can seek better understanding together.
So the enlightenments that I'm talking about are not just people being able to have better objective sense making, but also better intersubjective capacity that leads to higher quality conversation so that collective intelligence can start to emerge.
So you're mentioning being able to find somebody that knows.
And first, let me just say, before we discuss a perfected system, we can just discuss how to stop some of the most egregious things about the current system.
Current system, people are radically certain about things that they have no basis to be certain about, and it's actually their false certainty that causes most of the problems.
If they simply acknowledge that it was too complex and they didn't know, they at least wouldn't be going to war over dumb stuff.
And so to simply be able to have people be like, I don't know yet, but I'm interested and I'm going to try to seek to know better would actually slow the rate of breakdown tremendously.
I believe that the phrase I don't know is one of the most important phrases in human collaboration.
It's interesting when you read scientific papers, the word it is not yet known appears at high frequency and the better the science, the more frequently it appears.
Right. This is one of the things that is a value I really want to have come across as people having a mature relationship to the topic of certainty.
And I think it's easy to get it wrong in both directions, which, you know, a disposition to want to be more certain because either I think the certainty gives me security or I seem smart or something like that just makes us bad sense makers and makes us dangerous.
And so the fear of uncertainty and the desire for excessive certainty, excessive meaning more than the desire for certainty that has us jump to it before we've done the right epistemic process is dangerous.
But the compensation, right, there's a kind of postmodern trend that takes any certainty is probably some kind of imperialism and only holding uncertainty as virtue also doesn't give the basis to act.
So what I want is to say I'm comfortable being certain and my certainty will never be 100% but it might be 99.9% depending upon how much experimentation I've done relative to the thing.
The fact that it's never 100% means that I'm always open to the possibility that there's data I didn't factor that's important.
But I also need enough certainty to act where inaction is also consequential.
So factoring that action and inaction have consequence and I have an ethical binding both to the quality of my action and the results of my inaction.
Then I want to say, Well, how do I get enough confidence to make the right choice for this particular thing factoring the consequentiality of the thing.
And do I want to be comfortable with uncertainty and I want to be comfortable with relatively higher certainty through the right epistemic process that informs right action.
Absolutely. And I would also say that the very important one that you mentioned that most people do not build into their sense making the decision making is the cost of inaction.
You know, and I had a long and interesting and fun business career and I think I was more successful than most and one of the reasons was I very carefully calibrated about that.
How much information and how much certainty did I need to pull the trigger?
You know, if this was something where if you don't decide you're fucked, then you decide when you're at 40% because the alternative is worse.
There are many times when you're at 52% you should pull the trigger because frankly the downside isn't that bad.
Maybe you'll lose some money.
So what if the stakes were higher, maybe it's 60%.
So anyway, to calibrate how much certainty you need to pull the trigger is one of the great skills of decision making, which is not taught by anybody that I've ever seen.
I fortunately was able to sort of pick it up by getting my MBA with real bullets, so to speak, in the real business battlefields.
But if we made that part of our, you know, education and epistemology, that by itself would be huge, right?
And of course, it would totally attack the fundamentals of American politics in which it's always the right thing to do to kick the can down the road, not confront it, no matter what the costs are for kicking the can down the road.
Yeah, now this is a tricky thing is there's almost a negative gradient problem here where the people that do less good thinking come to certainty faster and then make actions faster.
So if someone doesn't want to make the wrong kinds of choices, so they want to actually do deeper sense making first, then if someone takes that too far, they're just seeding the control of the world to people with the worst sense making.
In the most action bias. And yet that doesn't we don't want a multipolar trap where everyone just says okay I'm pretty sure I'm right so I'm going to go for it.
So this is why that balance of there is responsibility for inaction as well as action and different things have different consequentiality like what confidence do I need that this is going to be a better recipe to experiment with how much spice I put in the food,
not that much because at worst I ruined the meal, but how much confidence do I need that a particular application of AI is actually a good one for the world.
Well it should be a fuck ton higher confidence based on much better processes of looking at second third order effects because the consequence is so high and the and the reversibility might be so low.
And this is where we're in a situation where there are market incentives to be first mover, which means to move as fast as possible in some of the areas of highest consequence where we should actually be moving slow and doing really good safety analysis.
And yeah, that's that's a problem.
Yeah, that's it. Unfortunately, you know, it's baked into the current operating system which is hill climbing incrementalism right every player in the AI world has lots of incentives to make a move every quarter at least the shows they're making progress drive their market cap or the world's perception of their importance etc.
And usually the moves aren't that big, you know, GPT three, which is the buzz in AI at the moment, it's a pretty good size move but it's not huge, but it's incremental and nobody stops to really think what a whole series of incremental moves over 10 years is going to result in.
There is no meta system to evaluate these kinds of risks and to keep people from playing the hill climber game theoretic game of incremental moves.
Okay, so this is interesting. Let's let's look at kind of the ascendancy or return to power depending upon the perspective you want to take the China's going through and the kind of.
Increasing fragmentation and descendancy you can see in the US.
Descendancy and coherence at least.
So term limits make long term planning quite difficult.
And we know why we like term limits was we didn't want people to get too much power and corruption and start thinking dynastically of foreign by the people rotating.
But it also does create an incentive for people to do stuff where either for their reelection or simply for their new economic opportunities getting out of the way that they're judged.
They're only going to be oriented to do stuff that will create positive return within the time of their term limit.
Obviously it's even a shorter time limit for directors of corporations that have quarterly decision profit cycles.
And so a dynastic monarchy that is thinking not only about the person's whole life but about their kids can actually do better long term planning.
And if I have left versus right differences where when anyone does something for four years the next four years somebody will undo it.
It's also and whatever the internal coordination costs of the disagreement are so high.
It's also very hard to unify enough to do stuff which is why the US has had a hard time fixing its infrastructure since the 50s in China has built high speed trains all around the world.
And so for a lot of reasons we can say well monarchy dynastic monarchy is just a better system because getting a lot of people to coordinate is really really hard.
And having someone that can do long term planning be able to actually implement with some coherence because there's not much internal descent seems like a better system.
And that system can check its own AI that system can do a lot of things.
Whereas the system where all the companies are competing against each other in a quarterly way and the parties are competing against each other seems to be stuck on a multipolar trap governed situation.
And so this is I actually know a lot of people who've thought about multipolar traps a lot.
Whose only answer is we actually return to monarchy and most of them think a global monarchy run by a benevolent AGI.
And so I think if we right now it's pretty easy to see that if the US continues the kind of left right and all the divides that it has its shaping of the world continues to decrease in relevance.
Probably China's continues to grow and we see that not just China but like Turkey and Venezuela and Brazil and a lot of places have moved to less participatory more autocratic style of governance.
I think we lose the 21st century to that.
And while that is because it's more effective in a bunch of ways.
And I think the only way to be able to have something like a republic or a participatory governance be more effective is if the many coordinating with each other actually produces higher quality results than just a few being able to control the thing.
And that's only going to happen if it can harvest the collective intelligence and there actually really is some high collective intelligence.
So now this comes back to your metamodern question.
Do I think that we can get the hierarchical complexity or the ability of people to process information up?
Yes.
Even before that if I can start getting them a medic immune system to where they aren't just cognitively and emotionally hijacked.
Right now it's mostly not even can they do good epistemology.
It's that they're just captured by narrative warfare.
If I can simply get them a medic immune system where people start to notice how rustle conjugation and lake off framing happens.
Yes that scientific article said something but the news article put spin on it.
Can they notice how the spins occurring?
Yes that's a true statistic but it's cherry picked.
When you look at all the other statistics it doesn't look like that same picture at all.
And people start noticing those kind of info and narrative weapons and become inoculated enough that it's not like absolute lowest common denominator collective intelligence.
That will make a huge shift so we have to factor the kind of mimetic emotional immunity and cognitive immunity topic.
Then better epistemology for the individuals and better orientation towards Socratic dialogue, Hegelian dialectic like seeking shared understanding because of understanding the need to coordinate being less bad than warfare not coordinating.
Then I think we start to get emergent collective intelligence where more sovereign and intelligent people in better conversation start to produce systems of coordination.
To bottom up effect where those systems of coordination produce a top down effect that continues to incentivize that bottom up effect better and you get a recursive process.
Between better systems of collective sense making and choice making and better development of individuals and their communication capacity with each other.
That's a beautiful picture but it's got to fight hyper normal stimulation. How does it escape that trap?
Well that's one of the immune systems that I'm talking about. How do we fight the trap of group identity?
That's a fucking hard one there's a courage required to not agree with the thing where my friends are saying silence is complicity and you're either with us or against us.
To actually be willing to be honest and say I'm not sure I haven't looked at the statistics I haven't made good enough sense of this requires a kind of profound courage and epistemic commitment.
The ability to mistrust my own certainty realizing like this is one of the insights that kind of really got me into this was I was certain about things where I later realized I was wrong enough times that I became very dubious of my own certainty.
And then I recognized fuck I'm clear where I think a lot of other people are wrong.
And I'm clear where almost everyone historically believe stuff fervently that I'm pretty sure they were wrong about flat earth or which God is true or whatever.
And I'm clear where I was wrong in the past.
But there isn't a single belief that I hold now that I can say that I'm probably wrong about even though statistically I'm probably wrong about most of them.
And when I go forward into the future there will probably be some I recognize that asymmetry is really problematic and this is what got me to start wanting to calculate what is my basis for belief and what is the confidence margin.
And where do I believe things more fervently because it's a good story and I get a sound smart or I get to be part of a group or because it gives me security and how can I start to have.
A psychology that is more independent of those things so my belief system is not bolstering me identity wise or existentially.
So that's a kind of psychological emotional mimetic social immune process that has to be part of the enlightenment we're talking about and so basically I'm saying participatory governance is hard.
Autocracy is just easier for a bunch of reasons either we should start designing the monarchies we want to be part of or participatory governance only ever emerged out of and was successful as long as there was a cultural enlightenment that made it possible.
So we either have to redrive a cultural enlightenment to be able to revivify participatory governance or we should start steering the kind of autocracy we want.
The enlightenment we want to drive is not just a cognitive enlightenment.
It is the susceptibility to hypernormal stimuli.
It is a value system around respectful engagement with other people and seeking their points of view.
It's all those things together.
Yep.
Let me get on a couple of point topics and I'm going to turn it over to you because I know you've thought about this more than most.
What are the next moves that need to be done?
First, let's hit on two other topics.
One, we talked earlier about bad faith discourse and then you alluded to the fact that not only is there bad faith discourse, but there's good faith discourse that's just wrong.
And I will say that our current evolved information infrastructure seems to have produced an evolutionary context which produces some very virulent forms of being wrong.
You know, anti-vaxxer being an interesting one, right?
It's like, what the fuck?
A hundred people have died from bad vaccines in the United States since 1950.
Millions have been saved.
How could this even be an issue?
And yet, evolutionarily, a memeplex has been created that has attracted millions and millions of people.
QAnon's another interesting one, a classic example of a evolved artifact that, you know, by any objective measure just seems fucking insane.
And yet, not only do millions of people believe parts of it, hundreds of thousands at least seem to have it as a main hobby in their life trying to do the research, figure out the clues, etc.
And so something, and I guess I would point out, compared to 1965, you are not going to see either anti-vaxxers or QAnon on the CBS Evening News with Walter Cronkite.
The gatekeepers of that era, if nothing else, were good at keeping absolute nonsense out of widespread public circulation.
Then the second point, which you just hit on, is tribalism seems to be what we fall back on when we're so overwhelmed by an information-evolved platform which is driven by bad incentives, plus complexity, plus scale.
And we now just say, throw up our hands and say, fuck it, I can't figure it out.
So I'm Team Blue. Whatever Team Blue believes, I'll believe.
I'm Team Red. Whatever Team Red believes, I'll believe.
We've all seen those big posters with all the logical fallacies on them.
I've come to believe that this epoch of tribalism has made one logical fallacy dominant all over the rest, and that's confirmation bias.
Everything I hear, I would say not me. I'm the one exception.
But let's say the laughing here, ironically, that the mass people who are in the tribes, they compare every fact or item they hear against what's the tribal take on all this, rather than trying to look at something objectively and decide whether they should believe it or not.
So, Daniel, what can be and should be done starting tomorrow morning? What do we do?
Should we start at the level of what individuals can do on their own, or what we could do to try to attack these issues project-wise?
I would suggest a bit of both. And what's, of course, best is where project-wise cycles back to the individual and then back to the project.
But go with what you think is the priorities.
I want to just say one thing first, and this is a funny thing to say, but if we talk about anti-vax or QAnon or any particular view like that, I can really empathize with those, and I think it's important that everyone can.
Because I think most people sense that there are a lot of things wrong with the world and moving towards catastrophically worse,
that those who are in most power both politically and financially don't seem to be doing a good job with, though they seem to be increasing in their own personal wealth or power or whatever else.
So there's a sense of corruption that most everyone senses. It's just then they want to fit it into very simplistic narratives where there's good guys and bad guys and they can be on team good and that kind of thing.
So the right all thinks that the left's news is fake. And the left thinks that the right's news is fake. They each think that the other one's politicians are corrupt.
So the idea that politicians could be corrupt or news could be fake. Well, that's actually true. It's just not true that it falls just along those partisan lines.
It's more that you've got to understand as for anyone to ascend the stack of power, they had to do well at the game of power and there's a lot of things that that involves to do well at the game of power.
And so I think the idea that there is are these institutional authorities and we should just totally trust them has pretty much broken. You were mentioning, you know, in 1965.
In 1965, the people who were live paying attention to the news directly remembered World War Two and their life experience, right?
And that had a kind of unifying basis, a patriotic and unifying basis and an empirical basis. We had to really double down on science and technology to be able to win the war.
But then there starts to be an institutional decay where not that many people who are live paying attention to the news now or very large numbers of them weren't around in World War Two.
And they don't have any real embodied sense of what breakdown in the developed world could look like. And so similarly, they have much more sense of the enemy that is near than the enemy that is far and those types of things.
And then when we so when we look at vaccines, for instance, it's actually a bit of a complex topic because if you're making something that is a drug that's intended to have long term effects as opposed to most small molecule drugs that stop acting after they've left your body.
Could they have long term consequences? And if we've done mostly the studies on individual vaccines and yet they're given in schedules with lots of them together, do we really know what the collective effect of that much immune modulation long term on lots of them together is?
And if at the same time that they've been going up, polio and whatever's been going down, but hygiene and antibiotics have also been happening during that time.
And then autoimmune disease and other things like that seem to be going up is the autoimmune disease and the infertility going up because of the vaccines or is it going up because of the glyphosate or ubiquitous environmental toxins or stress.
Those are actually complex topics. And so it's very easy to say all vaccines are bad or all vaccines are good anti-vaxxers are stupid and neither of those are complex enough to actually make good sense of it.
But the need to take a very strong position because the the other view is so bad and dangerous we have to fight it ends up being the tribalism confirmation bias.
And so and then people only have a pejorative straw man or even worse a villainized version of the other.
They can't even imagine how anyone could be that stupid or bad as to believe that thing.
And of course you can't have participatory governance when that's how you think about the other people you're supposedly part of a republic with or part of a civilization at all with.
So where people start to doubt the institutional authority, there's something good in that.
But then oftentimes what happens is rather than really learn the epistemology needed to make sense of it, they just jump to the new authority and usually the new authority is whoever it is that's telling them to doubt the other one.
And so that's inadequate, they're not actually moving up vertically into more hierarchical complexity, they're just shifting authorities.
I just wanted to say that as preface.
Yeah, that's I think that's a good preface and I will confess that I'm a real neat joker on anti-vaxxer.
Have I done enough research to be absolutely sure about it?
No.
And so I will plead to a little bit of tribalism on that one.
It's the only topic which I have, at least when I was running the show over in Rally Point Alpha, the only topic I which I banned discourse on was anti-vaxxer.
So may Koopa continue.
So and this is tricky because let's say a new person is coming into a topic that has been very well addressed, but they don't know.
Do the people who've well addressed it have to re-go through it every time?
That's actually a tricky topic, right?
From the good use of time point of view.
So in terms of collective intelligence, if a group has done really earnest dialectic to come to a certain confidence margin on something,
the path there, not just the conclusion, but the path there should be left in some intact way that other people can walk.
And then if someone has critiques that are legitimate object level critiques of the actual path there, the conclusions, we should listen.
And that's, I think, how science can continue to evolve is where we can continue to critique our best understanding and have dissent around it without having to repeat the things that have actually been done well many times.
Okay, so your question, what do we do tomorrow?
At the level of the individual.
Forgive me, I'm going to make a religious reference.
The commandment in the Bible to not have any false idols.
The way that I take that, I take it very similarly to the first verse of the Tao Te Ching.
So I think a lot of cultures said something wisdom wise, and maybe maybe I'm just reading it the way I want to read it.
So no false idols.
Any model I make of reality isn't reality.
It's me trying to info compress the complexity of reality into some small number of variables and operators on those variables where I can model the outcome with hopefully enough numerical accuracy that it's useful.
So this is the all models are wrong.
Some of them are useful.
The moment I make a model truth.
Now I'm not in direct relationship with reality anymore.
I'm in relationship with the simulacra.
And so the idol of the models that I have of reality, I want to hold them as useful and see which ones are more useful will never actually having any identity attached to them.
Never having any sense of sacredness attached to them because I know that better ones will come along.
