Howdy, this is Jim Rutt, and this is The Jim Rutt Show.
This is The Jim Rutt Show, and I'm your host, Jim Rutt.
Today's guest is Daniel Schmottenberger.
Daniel's an independent thinker, focusing on the future of civilization,
the sensitivity and potential of our current situation,
and how we may navigate the path forward.
He's also director of R&D and co-founder of the Neurohacker Collective.
Welcome, Daniel.
Thanks for having me, Jim. I'm looking forward to this conversation.
Yeah, I followed your work for some time, and while I keep my eye on several thinkers
who are thinking about the future of our society,
your perspective is perhaps both the most dire and the most hopeful.
Interesting that you can be both.
Yeah, I call that kind of a hard fork hypothesis.
I like that. It's a good way to describe it.
On the dire side, you say pretty explicitly,
you believe humanity is going to end relatively soon.
We don't address some of our fundamental design issues in our social operating system.
Yeah, I basically say that when we look at history,
we see that most of the previous civilizations, obviously they don't still exist,
and they underwent internal decay that led to how their collapse occurred.
And we can look at the way that Tainter studies this, the way Jared Diamond did,
or the way that kind of Strauss Howe, or Baudrillard,
or different models of civilization will collapse to describe it.
But they're just like people have a life cycle.
There seem to be these life cycles of civilization,
and there are certain things in common that lead to their breakdown.
The difference now is that we have a fully globalized civilization,
and rather than just causing local environmental harm that can lead to
a limits of growth issue, we can affect the habitability of the biosphere writ large.
Obviously, we didn't used to have weapons of mass destruction,
so there's a total difference of the capacity for warfare, etc.
So technology, both the globalization and technology,
have changed the magnitude of the issues in a way
where the change of magnitude actually becomes a change in kind.
And yet we haven't figured out not civilizational collapse.
So if we forecast all of the different possible catastrophic risks or existential risks,
there's a lot of them.
And what I would say is that they all are the result of some underlying
common generator functions.
And so as a result, if we just tried to deal with a particular AGI scenario,
or a particular climate change, or a particular biodiversity loss,
or a World War III scenario, we don't buy ourselves that much time
before another scenario emerges because the situation is basically overdetermined.
So we actually have to address the generator functions.
We have to address it categorically rather than just instances.
And if we do that, that becomes the kernel of a new civilizational model
that is radically different than any civilizations here too for,
which is the more optimistic picture.
And this is kind of like a hard fork between very different scenarios.
Yeah, that was what I was going to say.
On the hopeful side, you were basically trying, at least,
to define a new social operating system.
And at least for me, in my kind of naive way of looking at things,
it's still a very rough sketch.
But I can see that if you are able to fill this in,
it may make human life vastly better and more humane than it's ever been before.
Before we get to your specifics, though,
I would like for our audience, and keep in mind that our audience
are smart people, I hope, but they're not necessarily experts on the way you and I
and Jordan and some of our friends talk.
So I want to make sure that neither of us get too far afield in jargon
and that we make sure that we bring substrate issues up before we dive in deeper.
Specifically, I've heard you talk in the past very eloquently
about how human created technology is a fundamental change in the dynamics of the world
and that technology, i.e. human invented technology,
is fundamentally different than evolution
and produces fundamentally different dynamics.
You want to talk about that a little bit
before we jump into the more specifics of where we're at?
Yeah, it's definitely a kind of central thing for us to figure out,
especially when if we don't realize this,
we try and use evolutionary biology and evolutionary theory
to model human systems like reifying theory of markets
based in evolutionary theory and social Darwinism writ large
and that actually in my assessment doesn't work.
A lot of this insight on the fundamental difference of tech and evolution
and highlighting it was something Forrest Landry helped me understand,
but the model goes like this.
In evolution, in evolved systems,
we can kind of think of evolution as defined by three primary characteristics,
which is mutation occurs and then selection occurs,
but selection is two things.
Does the agent survive and does it reproduce?
So survival selection made selection.
And then obviously that is survival and mating within evolutionary niches.
Now, something to understand about that is that the mutation pressures
that are affecting everything in an ecosystem,
there's a kind of evenness of the distribution of mutation pressures,
whether we're talking about gamma rays or oxidation or just copying errors or viruses,
they're affecting all of the lions pretty similarly
and the lions and the gazelles and the plants.
So we don't have radical mutation occurring in one place
and no mutation occurring somewhere else.
And so as you have a mutation that would make a lion faster or any predator faster,
you have similar mutations that have a distribution in the fastness
and slowness of the prey animals.
So then of course, if you get a little bit faster in one
and they say the lion eats the slower gazelles,
then the fastest gazelles reproduce and that leads to doubling down on those genes.
So that's the next part is not just a evenness in the distribution of mutation,
but also co-selective pressures.
And so the advances anywhere lead to a pretty strong symmetric coupling of power across the whole system.
And so this leads to a situation where you do have rival risk dynamics in nature.
You do have something like individual agents doing self-maximization.
Of course, it's not purely that we have a lot of symbiosis.
We have animals that are paying attention to their young
and animals that wouldn't survive if not for the whole group of animals.
But you can model individual self-optimizing agents
and get a certain successfulness if you have this symmetry of power.
So you have this lion and this gazelle are in rival risk dynamic,
but all lions and all gazelles are symbiotic with each other,
meaning the lions would die without the gazelles,
the gazelles would die without the lions.
And as either one makes an advancement,
it drives the other one to make an advancement.
So this is kind of where when we think of social Darwinism,
we think of the idea that competition drives innovation and advancement and those types of things.
The difference when tech comes about is,
so again, these changes in the animals are happening mediated through genetics,
mutation of the genes and then survival and then recombinatorics of the genes.
So the genes are physically instantiated pattern replicators.
When technology, technology, I don't just mean physical tech.
I also mean language.
I also mean social tech, coordination tech.
But so tech in the Sanskrit sense of consciously mediated methods of doing things,
basically things that come from the capacity for abstraction
and creating abstract pattern replicators.
The abstract pattern replicators can change much faster than the instantiated ones can,
and they can change with an uneven distribution.
So when we think about toolmaking starting with, you know,
homo habilis and stone tools, right, which is very different than a chimp using a rock that it finds,
but not whittling a sharper, you know, or chipping a sharper rock,
which is that the chimp or the bird or the whatever it is that's using something can
experientially notice that this thing is better at doing what it wants than this thing is in the
moment. But it can't understand that between all three rocks,
why this one is better at cutting the thing is because of the abstract principle of sharpness
and then say, oh, I understand what mediates sharpness and I can design something with more
sharpness. That abstraction capacity seems to be part of how we define early humans and then
got doubled down in homo sapiens. And it's a different process than evolution of bringing
new stuff into existence. It's not occurring through kind of random mutation and just selective
dynamics. It's occurring through an agent that's actually understanding something abstractly and
intentionally creating it. And so if you think about evolution as the stuff that emerges,
there wasn't a conscious choice to have something emerge. It emerged as the result of
complexity dynamics. So it's unconscious. It's radically parallel. It's radically distributed.
It's radically combinatoric. It's slow. Almost everything fails. But you get a interoperability
of everything. So what makes it through are very self stabilizing complex systems with technology.
It's actually consciously created. It can happen in a local way, not everywhere. So it's not
radically parallel and decentralized. It happens more in a serial fashion. And it creates parts
that are not necessarily in equilibrium with whole systems. And so it really is mathematically
almost an opposite kind of creative process. And the thing is, if you have an evolutionary agent,
like a human that has evolutionary motives, but is now able to say human operating as apex predator
can increase its predatory capacity, orders of magnitude rapidly faster than the environment
can increase its resilience to that predatory capacity. Now we have a fundamental problem.
This is a lion getting 1000 times faster in a hurry without gazelles being able to
make a mutation in adequate time and the lion's eat all the gazelles and then go extinct. And so
we have a situation where technology has broken the power cemetery that is what is necessary for
the metastability of evolved systems. And so you see that not only is there a symmetry of power
between the lion and the gazelle, there's also a symmetry of power between lions and lions and
gazelles and gazelles. The most badass lion is only, you know, two X or one and a half X more
in the median lion. But if we look at Putin's killing ability or Trump's compared to yours or mine,
it might be billions or trillions of times more. And same we could say for economic capacity. And
if you look at sapiens writ large compared to the rest of the biosphere, it's similar. And so when
you think about like, you know, one lion just even if it went rogue and didn't just kill to survive,
just start killing everything it could, it has such limited destructive capacity. And that's not
true. And especially as we get into decentralized exponential tech, one actor or a small group
of actors has really radical amplification of agency. So if you keep having rival risk agency,
rival risk basis for agency, but with very high power relative to the overall playing field,
you end up getting a basis for fundamental instability.
And of course, it's not all all about destruction either, you know, two data points that jump out
at me that we are doing things that we think of as constructive that have to be getting well
near our limits are, you know, for instance, the fact that of the large mammals on earth,
it's now thought that the majority of the biomass is humans plus our domesticated animals. And when
it comes to birds, it's even more radical. It's thought that the domesticated birds represent
70% of the biomass of all birds on earth. And so nothing destructive, specifically destructive
about catching and killing, but we've essentially engineered a technology which has co-opted a
majority or 70% of the energetics and biomass of the bio and it's continuing to grow exponentially,
which strikes me as a very strong signal, which is very little talked about.
Yeah, so this is, you know, kind of a limits of growth thing, as opposed to say a warfare or
terrorism or, you know, intention destructive thing. So think about this, and we think of apex
predators, they evolved to fit a niche. And because their adaptive capacity is mediated
through genes, through concrete pattern replicators, they don't do very well outside of that niche.
And so polar bears don't leave the Arctic, right? And cheetahs don't leave the savanna and orcas
don't get out of the ocean. But because our apex predator capacity was mediated by tools,
and we could make different tools, including different coverings for ourselves, and in
different environments, when we would overkill an environment, rather than have our population
stabilize, we would just move to a new environment and become the apex predator there. And so we
went and became apex predator everywhere, over hunted, and over farmed environments everywhere,
over fished, etc. That is really different than every other animal. And like, again,
if we think of the examples you were just giving was like total biodiversity loss and the relationship
of total animal life in domestication versus wild, if you think about an apex predator like an orca
or a great white shark in the ocean, and how many say fish, it can kill in an hour. And then you
think about a ocean trawler with a mile long drift net. It's just not even like, obviously,
we aren't apex predators, they can't destroy whole ecosystems, they also can't genetically
engineer new creatures. So we have to stop modeling ourselves as apex predators competing
with each other to be better apex predators and take the top because the destructive capacity,
even of not intending to destruct, just of intending to extract, is well beyond the replenish rates
of the system. And on top of that, in our first episode with Simon Dedeo, we talked fair amount
about the rate of social evolution is accelerating. The rate of invention of new technologies, new
capabilities is way faster now than it was just 30 years ago. And if we're already approaching the
limits, or probably exceeded the sustainable limits of our society, and we have social evolution
that's going exponential, and we still have a rising world population, last I saw projected
to top out 11 billion, with many of those people expecting a increase in their lifestyle, quote,
unquote, towards the American and European one, looks like to me a train wreck coming at very
high speed. Right. And so again, this idea that we didn't start over hunting an environment, and
then it became harder to eat and breed. So then we came into sustainable population with the
environment, we just moved to another environment. There is this kind of rebound effect that we
see with humans. And we see it in all of evolution, right? Like our friend Brett Weinstein will say,
if there's an evolutionary niche, it will get filled with something, right? So in the similar
sense, though, with humans, we don't have to wait for a genetic mutation, we can have a memetic
mutation to figure out how to exploit some new niche. And so this is where we get rebound effects
where increases in efficiency don't lead to us being more sustainable with the environment,
they lead to us figuring out how to have profitable exploits on more area. And so,
you know, when you look at something like the Jevons paradox or other abstractions of it,
if I get a 20% increase in energy efficiency, we don't just use 20% less energy, we find whole
new markets that weren't open based on now what's going to be profitable, we go exploit those and
end up using more net energy as a result. And so when you look at like trying to figure out a steady
state population, steady state population doesn't actually work. It doesn't work the way that we
like to model an evolutionary biology with the way that humans continue to advance increases in
capacity and efficiency and then exploit all of them. Yeah. And you've spoken quite a bit about
you combine exponentially increasing technology with the core rival risk when lost dynamics of
at least our economic system and a goodly part of our social system. Could you talk a little bit
about how the coupling of those two together are spectacularly different than either by itself?
Yeah. So this is, I mentioned earlier that all the catastrophic and existential risks have
underlying generator functions. What I mean by that is that so like, let's say we're looking at
wanting to stop particular kind of environmental destruction or war or whatever it is, well,
we can look at why are those things caused and we can find some things that seem special to
specific instances, but some things that are part of a causal set that are true across all of them.
So obviously, we can look at something like perverse incentive. And so with the environmental
destruction, if the wild birds aren't worth very much to us in the wild, but domesticated ones
and farms where the farms take over the wild areas are, if cattle are worth something to us when
domesticated and then killed and not when they're free, then of course we have an economic incentive
to exploit everything. And the same would be true if we have a for profit military industrial complex
where war is more profitable than pieces. And so the underlying idea here is that you can't prevent
to harm while there is a very strong incentive to cause it because that incentive is kind of an
evolutionary niche for a particular way of getting ahead and it will end up getting filled. So we
try to make law to bind that, but as we know, strong economic power ends up being able to influence
law pretty heavily. So the thing that is designed to bind the problems of the incentive system ends
up getting corrupted by the incentive system. So we can get into that more later. But this is an idea
of something like perverse incentive that isn't unique to one issue. It's underneath lots of
issues. So if we want to abstract and say, what are the generators that give rise to all of the
possible existential risks in the future? The first one is the one that you just mentioned,
first one I would identify is that if humans are running rival risk games, and by rival risk games,
I mean some in group that is seeking to get ahead in a way that can occur at the expense of an out
group and or the commons. So whether that's a person or a company or a country, if it can beat
someone else via a war, or it can exploit an environment and get ahead or it can corner the
market or whatever it is, it's playing a game where its win is going to require or at least
reserves the right to be at the cost of something else. If we run rival risk games, but we're not
limited in our rival risk capacity, the way animals are, because we can innovate new ways of
winning at rivalry via abstraction, i.e. tech. But then the moment we deploy some new asymmetric
tech, everybody sees it, reverse engineers, it makes mutations on it. And so we keep ratcheting
up power, then we get an exponential power equation. But we're using power in ways that
inexorably cause some harm to the total system. When you you can't actually run exponential
harm in a finite playing field and not have more entropy than the system can handle. So
rival risk games multiplied by exponential tech self-terminate. And right now, exponential tech
is inexorable. We cannot put the cat back in the bag. We can't stop it. So either we figure out
rigorously anti-rival risk systems or the human experiment, as we know it, is finite in duration.
But the thing is to say rival risk systems, that's a big deal because separate nation states are the
basis of rival risk systems. And so are private balance sheets. And so the changes that we're
proposing are very, very fundamental ones. They're ones at the level of the axioms of what we think
of as civilization. But that's why we have to start by saying all previous civilizations also
failed. We actually have to change things at a deeper level where we're not talking about just
making a new civilization where this is similar to the Enlightenment or the Renaissance or the
founding of America or the beginning of Sumerians. It's actually different in kind in each of those
changes because each of those changes have been an upratcheting of rival risk capacity. Some new
coordination capacity or technological capacity that led to more capacity to win rival risk games
against an outgroup. I'm basically saying now humanity as a whole with no outgroup has to
figure out how to do not rival risk games. As you know, I'm interested in this and have worked on
some of it and thought about a little bit, but I'm going to play the cheerful skeptic here for a
moment, if you don't mind, and offer a possible argument that says, maybe we don't have to go
that far. And again, I'm not going to say this is my actual belief, but let's say this is a well
established argument for a less radical approach, which we might call democratic liberalism.
And in passing, talked about the fact that our legal system, our political system has been hijacked
by our economic and financial systems. We couldn't, in theory, fix that. It may require an amendment
to the Constitution and some other things, but it could be fixed. Secondly, the depletion of the
commons, the democratic liberal argument is if we priced the externalities correctly, we could
defend the commons. We also look at Ostrom's theories of managing the commons, and those are
assumed to take place in a world with rival risk economics around them. How would you respond to
those who say rather than changing the jet engine on the airplane while it's in flight, we'd be better
off attempting to be smart about fixing our democratic liberalism by breaking the hack between
money and politics and by rigorously and probably initially very conservatively, meaning
expensively pricing externalities. I will say that that is formally impossible. And I'll explain why,
but there's a few aspects to it. So like this will take a minute. Take as much time as you like. This
is really important. I would start by wanting to just be clear on the fundamental difference between
a market and a government and the relationship between those because what you're talking about
is a government regulating a market. Yes. In fact, that would call that the modern democratic
liberal synthesis that started in the 1920s was fully in existence by the 1930s and was locked
in in its modern form in 1948. Right. So what I would say is that there are certain architectures
within markets that lead inexorably to certain issues, other ones within governments. And I'm
going to try and abstract this at the most abstract level that works, which is this will be true for
any form of government, whether it's a two-party system or a three-party system or has a parliament
or doesn't due to simply the nature of it being a top-down imposition of law via monopoly of force.
And the same is true for whether we're talking about any different version of how one might think
of a market. So if we think of a market, you can actually think about theory of markets like pure
laissez-faire theory and try and model it via evolutionary theory. And people do. And this is
where social Darwinism kind of comes, right? Which is these three things, mutation, survival
selection, and then mating, mating selection are what define the success of markets. So you have
an environmental niche, which here would be called demand. People need real stuff. And so then that
creates a impetus to try and figure out how to fit that niche, which is supply. People make some
product or service to try and do it, but they'll make slightly different versions that equals
mutation. The one that actually meets people's needs best of the best values, the one that will
make it through survival selection. And then if a few of them have different properties that are
all really desirable, they might mate and meaning, you know, emergent acquisition or IP trade or
whatever. And you get a recombinatorial dynamic and that's mate selection. That's the idea, right?
Except as we mentioned, that's true. But you don't get the metastability of an evolved system
because of the asymmetries that are going to be intrinsic to abstract replicators that aren't
there for instantiated replicators. So if we think about a market as a bottom up coordination system,
and bottom up meaning that we don't have long term central planning of what we're trying to do,
everybody's not trying to agree on something, we're just interacting with each other via supply and
demand dynamics and stuff gets up regulated. So basically the society is all emergent properties
of the bottom up interactions. So that's one kind of thing. And we can really think of it as having
certain characteristics of just the bottom up math of it. One thing I would say then, and this is a
critique of libertarian ideology, free market kind of ideology writ large is that without regulation,
markets are going to have multipolar traps that they cannot resolve. And by multipolar trap,
it's a generalization of whether it's a tragedy of the commons or an arms race or any kind of race
to the cliff or race to the bottom. These are scenarios where somebody can do something that
is bad for the whole over the long term, but very good for them over the near term and provide so
much competitive advantage that without law to bind it, people will still buy the thing they will
still be able to get employees right like the market forces won't stop it. And so then everyone
else has to compete to do the same thing, or they will just lose by default in the short term. So now
you have everybody competing to get the cheaper material that is comprehensively damaging the
environment or racing to cut down the trees faster than they actually need them because if they don't,
the other guy will cut down the trees anyways, or one guy makes the AI weapons, so everybody has to
make the AI weapons where they're going to lose by default. And so multipolar traps in the past,
when we had limited power, could lead to boom and bust cycles, where we start polluting the water
because say everybody else isn't polluting the water, my pollution of the water doesn't make
it that much worse, but not having to deal with my pollution properly increases my margins because I
and externalize some of the cost. So I'm getting ahead, then other people say fuck it, that guy's
getting so much more ahead. Everybody starts doing it eventually the water is so polluted that everybody
is doing worse because the fish are all dead. So then there's now a new market for selling
water purifiers. And so we start doing that thing until you know, then there is a new scenario for
the pollution to occur. So you get these kind of at best, you get a race to the bottom and then
some new market opportunity race to the top. With exponentially more people and exponentially
more power per person, more over, you have situations where the bottom can be so bad that
it's unrecoverable. Let me jump in a little bit here. You're drawing the classic 19th century Adam
Smith quasi libertarian model. I suppose though we take a more modern social democratic perspective
where we assume it is the job of the legal political government side as instantiated through
democracy to put limits on the market. And what I've called in the past parametric social democracy
where instead of a bunch of fine grained regulations, you use the dozen or so very powerful settings on
the gravitational attractors within the game itself. I'll just throw out two examples, which I've
talked about in the past in some detail. One would be a high and rapidly rising carbon tax,
let's say $50 a ton to start rising $10 a year for 15 years to $200 a ton, which would send a
magnificently powerful signal to stop using carbon would empower alternative energy production in a
very major way. A second one, much simpler. One of the hacks we know that our current system does
is use psychologically informed advertising to invent demand. Suppose we put a 200% tax on
all advertising. I have 10 more I could throw out there. So why can't a list of powerful parameters
tame the market without having to undergo the radical shift that you're talking about?
Forgive me, I was starting with an unregulated market because I wanted to establish something
about theory of markets, which is why we can't do just the libertarian thing and why there is a
good case for a regulated market. Now, I want to then do a construction on what's wrong with
governments. And then I want to do a construction on what's wrong with them together, because I
have to refer to what's wrong with each one individually before I can do the combined thing.
All right, you go right ahead. Okay, so basically anything that has bottom up coordination only,
but abstraction mediated capacities like markets is going to fall to multipolar traps,
multipolar traps with exponential tech will be catastrophically bad. That's that. Then this
is at least the story of a major reason why we justify creating states, which is, okay,
we don't want everybody to cut down all of the trees. Just a storm is lumber because if they
don't, the other guy will and we're left with no trees. And yet how do we deal with this? We have
a real coordination problem. Well, we need to create some entity that has the ability to stop
everybody from doing it. And so that entity needs some kind of monopoly of force to be able to
actually uphold an agreement. So we want something like rule of law, we want the ability to come up
with good laws and we want the ability to enforce those laws. So we agree to bequeath to a government
a legitimate monopoly on violence, which internal is a police force without which law doesn't actually
exist, right? The thing that we think of as law doesn't exist. And then externally a military
force so that the freedom for us to do this thing is upheld against other groups. Now,
we can say why do we create states? Well, one reason is to unify groups to be able to win wars
and defend themselves. And the other is to solve multipolar traps. I create rule of law so that
we can coordinate better. And arguably the actual reason is to consolidate power even more. But that
is not the answer that is given when justifying it. So if we think about a government, a government
is now not a bottom up system, it's a top down system, meaning that there is some centralized
rather than decentralized body that can actually make choices, whether it's a monarchy or a oligarchy
or a democracy or whatever. So it's one person or it's a majority of people, there's some process
to be able to say, okay, this is the law we're going to do. And here's then how that law will
be upheld via the agency of that central body and its monopoly force. Now, we can identify
things that will end up going wrong with top down systems. And we're trying to bind the problems
associated with incentive. And yet the agents who are mediating because the government isn't
actually an entity in and of itself, even though it's acting like it, it's run by people who are
all still agents within the economic system that all still have incentive themselves. The judge and
the lobbyists and the lawyer and the politician all still have their own fundamentally still
largely rival risk basis for wanting increased status power, whatever it is. Yeah, the famous
economic problem of agency risk, right? Every company has that problem, right? Every employee
in theory is out for their own good. And so they have to build structures so that at least
to a first order approximation, the company achieves some level of good for the shareholder.
So yeah, this is a commonplace of any social structure. Yeah, because a corporation is also
a top down system like a government is. And so we can see similar issues that occur. And so this
is public choice theory is basically the critique of the wrongness of the incentive structures of
government agents associated with markets. And it's a classic libertarian critique of the regulatory
process on markets. But so let's look like this, some examples. So unless I have a fully global
government, I'm going to make a law at the level of say about a carbon tax or whatever at the level
of say a nation state. And if anyone doesn't make that the nation states are still caught in a
multipolar trap with each other. So I can have multipolar trap at the level of individuals or
corporations or countries or trading blocks. And so then anybody doesn't do the thing and they
economically get ahead in the short term, even if what they're doing is totally unviable for the
long term, then people in the other group don't want to be bound to the law that everybody is not
bound to. And yet they don't actually have the capacity to enforce the law on everyone else.
And this would be true for things like AI arms races. This is why we tried to make a UN after
World War two is recognizing that national only governments won't stop World War. So we wanted
some super national force to be a monopoly force for everyone, except when individual nation states
have catastrophic level capacity, i.e. they have nukes. What we find is that there is no monopoly
force that can be exerted over them because to have the monopoly force work, you have to be able
to exert it. So the UN can't tell a member country that has nukes, you have to get rid of them because
they're like, we have nukes fuck off, what are you going to do and bait us. And this is one of the
big problems is that monopoly force only works when you can exert it. That's why it obviously
doesn't work for nuclear deproliferation. It's why it doesn't work for a lot of global issues.
But it's also why even at a national level with the evolution or the emergence of decentralized
exponential technologies where small groups and non state actors and even individuals can get
catastrophic capacity through gene drives and drones and whatever. If I have a gene drive pandemic
type weapon connected to a dead man switch, there is no rule of law that can be exerted
over me and we're not that far from those types of things being possibilities. So this represents
an emergent breakdown in the capacity for rule of law writ large unless you run a perfected
surveillance state that doesn't allow anyone to have capacities, which is the China strategy.
Again, I'm going to argue here the moderate ameliorative position, even though it may not
actually be mine. Let's take the example of the carbon tax that other countries want to
free ride and defect from, right, which you can easily see there being pressures to do so.
Free riders and defections are the essence of game theory and at least in my mind,
your multipolar trap is a slight generalization of the concepts of predatory game theory. So
there is a partial fix, whether it's enough or not, I don't know, which would be to have
an implicit carbon tariff on imports. So let's say the Chinese want to defect and not collect
the carbon tax. We don't really care to the degree they don't collect the carbon tax that we do care,
but at least partially we can penalize them by putting a tariff equal to the implicit carbon
tax that we would have charged for the equivalent amount of carbon in a product. And especially
in countries like China and increasingly India, which are very dependent on trade, this is at
least a partial way of raising the cost quite significantly for those who defect or attempt
to be free riders. How would you respond to that? Yeah, so let's say the US agreed to do that,
but then let's say Brazil under its new presidency says, fuck that, I'll buy this stuff from China
and they have some capacity to and say Russia does say some other places do China is still growing in
its GDP externalizing cost and those other places are now we're losing relative to them and now you
get increasing citizen pressure to revoke that law. So this is again the essence of a coordination
problem from the view of it makes sense if we all did the good thing, right? And the prisoner's
dilemma, if we all coordinate, it would be good. But if we don't have the capacity to ensure the
coordination, anyone can defect, it becomes very easy to have the strange attractor be everybody
default to defection. Yeah, the answer to the prisoner's dilemmas we all know is you have Tony
Soprano, right? That if you defect, I'll kill you. Yeah, which is basically, you know, some kind of
even worse punishment than the original thing was that's outside of the scenario. And that ends
up being how we do it, right? Which is underneath our tariff situation, we're willing to go to war
to uphold a lot of these things. But now let's come back to even within a country. So we know that
right now, someone's ability to get elected has to do with a largely their access to not just
capital, but also allyship. So that's going to affect how other representatives support them
and how much they can do campaigning and media they can get and all those kinds of things. And
we know that we don't have an educated citizenry. And so you have a situation where people are going
to get elected proportional to really the incentives of the system more than anything else. And because
people can say stuff that isn't true, and people still believe it, and they can do Russell conjugations
to give people the wrong sense of things and appeal to emotional triggers and in group defection
dynamics and stuff like that. So let's say that we try to make a law, then of course,
everyone who has financial interest that would be damaged by that law supports the other candidate.
So then we have to do something like campaign finance reform. But who is going to bring the
campaign finance reform through this isn't being one of the key things is that the lobbyists are
paid for by somebody, the people who are working to change the laws continuously are paid for by
somebody. And whoever it is that would the ability to get money to someone who is campaigning also
is very easy to hide through offshore banking and through third party entities and think tanks
and whatever. So because we don't have something like perfected transparent accounting, if I try
to bind incentive using law, everyone who has the incentive to change that law has more resources
than those that are trying to bind it and they end up basically winning. And the gist there is that
economics is deeper than law is in the stack of power. And so you can use law to bind economics
to a limited degree. But let's say again, the company, a country tries to put some law forth,
it's particularly bad for a multinational company, the multinational company says,
I'll move headquarters to another country that doesn't do that, I'll give them the taxes rather
than you guys, and we will, you know, support whoever is campaigning against you and we'll
put a shit ton of lobbyists on changing the law. This is all the critique of public choice theory
is that the incentive of the agents in the system doesn't align with the well being of the whole.
And the government is mediated by agents in the system, and we don't have the right coordination
dynamics. So each actor doing kind of a utility maximization function, each actor doing what makes
most sense for them in the near term rationally creates a maximally stupid hole because of the
misalignment in agency of the various actors and the inability to coordinate effectively across them.
And this gets worse with the more we can corrupt accounting, the more we can actually hide that
these things are occurring. And the larger the system is, the easier it is to do that because
who can actually monitor all of the things that are happening in the system. Actually,
a lot of this was prefigured and talked about in a very underappreciated book by Mancer Olson
called The Logic of Collective Action. I don't know if you ever read that, but if you haven't,
I would strongly recommend it. He's more well known for I think it's the rise and decline of
nations, but the deeper book is The Logic of Collective Action, where he lays out the fact
that strong small groups that have a strong interest in an issue are very likely to dominate against
a much broader community who have small levels of skin in the game. And it's a pretty strong
critique and it does exactly public choice, but it's close. So let's okay, let's deem that.
It's both who has the most incentive will work hardest, and smaller groups can coordinate
better than larger groups. And this is why if I had two groups, two countries say that had an
equal number of people and an equal number of total dollars to begin with, and one of them
tried to create law that bound economic inequality, so the richest person couldn't have more than 10x
more than the poorest person, the group that didn't bind economic inequality would win over the group
that did in any form of warfare if the group was fairly large, because if you end up getting a power
law distribution of wealth where three guys own almost everything and everybody works for them,
the one guy who has almost all the resource can coordinate with himself better than the
million people can coordinate with each other. At least so far. And I think there's an interesting
possibility to think about what comes next. You know, really, all these are coordination and
signaling problems, right? And we are operating on a single or relatively low dimensional signaling
coordination system that has given rise to the system that we have today, what I often
call the money on money return signal, right, molds our world. So presumably, if we're going to get
past this, we have to think about multidimensional signaling and multidimensional coordination.
This is exactly the center of what I'm focused on is that the problem is fundamentally the inability
to coordinate between agents where their basis for agency intrinsically has deltas, right? There's
what's best for me in the current system of a private balance sheet and money and those types
of things, which best for me is not what's best for you and best for the commons, even though
it would be over the long term over the short term, it doesn't seem to be. And so everyone is
doing utility maximization functions, but with again, unlike evolution with asymmetric power
relative to the environment and supply side relative to demand side and things like that.
And this then ends up creating a situation where it gets worse, like this is actually a very important
point. If I have true information about the nature of reality, that is a source of strategic
competitive information. So I want to withhold that information. We call this a trade secret or
classified or confidential information or intellectual property. But I don't just want to
withhold that. I also want to make sure to throw anyone else that would figure it out off the scent
trail. So I want to do not just withholding of information, but disinformation. And we have
a situation where everyone is incented to do withholding of true information and signaling
of disinformation. And then we have information technology that's exponential information
technology where I can do customized disinformation for different persona types and all the way down
to individuals, we get to a world where we stop being able to parse signal from noise because
there's so much radical disinformation. And we have a situation where coordination actually
becomes impossible because of these agency issues everywhere. It's supposed to be the two different
intelligence agencies within a country perfectly coordinate with each other to support that country
because they're all on team country, right? Team USA against the Russians and the Chinese or
whatever. But really, those two different intelligence agencies are also competing against
each other for a larger percentage of the budget. And then even two different departments within
that organization and even two different people competing for the same promotion will withhold
information and maybe even disinform engage in corporate politics. Corporate politics is where
someone's optimizing their own bonus structure and their fealty relationships at the expense of
what's actually good for the whole because they're not actually coupled to the whole effectively.
And so you get a situation of fractal defection, everybody defecting on everyone to some degree
while signaling that they're not doing that. And this basically means a catastrophic breakdown
in the sense making necessary to make good choices while having an exponentially increased
amount of choice making power. And if we think about that exponentially decreasing quality of
sense making relative to the overall situation with exponentially increasing choice making power,
that's another way to think about inevitable collapse. Yeah, certainly recent evolutions
in our information infrastructure have raised even higher the power of bad faith discourse.
And if nothing else, it's very substantially reduced the cost of bad faith discourse, right?
And if we talk about as we often do in our world sense making, the idea of sense making and then
the bigger issue of choice making in a world of predatory disinformation and bad faith discourse
leaves us in a very dangerous situation. Yeah, exactly. That is the central thing for us to solve.
All right, let's stipulate now that there's no way out within the present game. What do you
suggest we do next? Okay, so I would say that the consideration that rival risk games are necessarily
causing harm to some other agent or to the commons, but that when they are multiplied by the
leverage of technology that that harm becomes larger than the ecosystem can handle. If we take
that as one generator function of X risk, we know we have to have a situation that creates
an anti rival risk basis for coordination. That is necessary, but not sufficient. There are a
couple other generator functions of X risk that we also have to address because we can show that
all of the catastrophic risks and this would take longer to do than we have, but we could show that
all the environmental degradation issues, pollution issues, dead zones and ocean ocean
acidification, biodiversity, loss species, loss, climate change, all of those issues,
all of the what would cause World War three or large scale war and all the exponential tech
mediated issues like gray goo or AGI scenarios or biotech scenarios and all the things that would
cause collapse, grid collapse, economic collapse, that all of those have a few generator functions
in common. And we could actually do a construction where we put forward these three generator functions
and prove that that set is actually subsumptive, that there are no risks that are not a result of
those things. So then coming up with a solution to those things is solving for the class of what
creates all of them rather than instances, that becomes the kernel of a new civilization system.
And that we can say is both necessary and sufficient requirements for a non self terminating
civilization. So complicated systems subsuming their complex substrate increase fragility
and evolved systems, not the technology systems. We talked already about evolution technology
being different because of the nature of the metastability of evolved systems, we get anti
fragility. So if I burn a forest, it will regenerate itself. If I cut my body, it will heal itself.
If I damaged my laptop, it won't heal itself. I burned my house down, it won't heal itself.
So humans take the anti fragility of the natural world and turn it into fragile stuff. We turn
it into simple and then complicated stuff. So we turn a tree that's anti fragile and complex
into a two by four that is simple and then a house that is complicated, but both fragile.
But we don't stop at a certain place, we basically have complicated systems
subsume the complex system. So we're creating an increasingly higher fragility to anti fragility
ratio that we're then trying to run exponentially more energy through an exponentially increasingly
fragile system. And this is a way of thinking about what Tainter said in the collapse of
complex societies, which is the relationship between the complex and the complicated as we
continue to grow the complicatedness of the scenario ends up breaking down. And so it's
important to know humans only know how to build complicated stuff, we don't actually know how
to build complex stuff. And we also don't know how to limit our growth. That's what I was saying
whenever there's an increase in efficiency, we just exploit more stuff. So this is another kind
of generator function is that we both have to learn how to build stuff that is either anti
fragile itself or in a fundamentally different kind of well, to some degree, we have to build
stuff that's more anti fragile, and we have to not exploit all exploitable areas. And so this is
really different than everything we have ever done. This is kind of one way of speaking about it.
The other thing that we can say is a generator function of X risk is that it's much easier to
break stuff than it is to build new stuff just from a kind of entropy or thermodynamic perspective.
Second law thermodynamics, the one law that you can't repeal.
Right. And so from a information theoretic perspective on the second law, one way I would
say it is the problem I'm interested in solving is that the way humans solve problems tends to
create worse problems. So whether we're talking about us coming up with a technological solution
or a government solution or a kind of social ideology solution or an economic solution for the
solution to solve the problem, that means it overtakes the problem, the solution has to be
larger, faster, somehow bigger than the problem was. But the solution typically is to solve a very
narrowly defined problem. So we're defining a solving one metric or two or three metrics. And
yet it's going to interact with complex systems that affect lots of other metrics where it will
end up having harm externality, but they will be larger than the original thing. So the plow
solved the problem of local famines, but ended up causing desertification and species extinction,
and all these things writ large globally, right? The internal combustion engine solved the problem
of too much horseshit in the cities and the difficulty of horses, but climate change and
oil spills and wars over oil and the destabilization of the Middle East are all the unintended
externalities of the internal combustion engine. And we can see the same for like the value of
Facebook then compared to the unintended externalities it created or Twitter or whatever it is. And so
in general, it's because I can define a problem in a narrow way, but that's actually not the problem,
right? That's a little part of it. And this is the same with biotech, right? Which is, I can say
the problem is one biometric that I'm trying to address LDL or whatever it is. And I can give
something that lowers that, but it might also do a bunch of other things that are negative,
which are the side effects of that thing in the overall system, which is why that approach is not
a really good approach to medicine. And so to formalize this even further, what we can say is
that the information and the computation, the information processing that it takes to come
up with a new piece of tech is orders of magnitude less than the information processing it takes to
ensure that that tech won't have any externality in its long-term application. And it's not just
orders of magnitude. We can say that the safety analysis is going to end up being NP-hard relative
to the work that it takes to come up with the tech being expressible as a polynomial.
Yes, that's absolutely right. One of the things I've learned in my 17 years of rolling around in
complex system science is the truth of the matter is the ability to project the future evolution of
a complex system is way less than most people think if only for the very simple reason of
dependency on initial conditions. We know from the very simple Lorenzi and Strange Attractors
that surprisingly simple systems are effectively impossible to predict because they're highly
dependent on very small differences in initial conditions. And when you add into that strategic
agents, the problem becomes, I would say, effectively impossible. So what's the answer?
That basically says don't ever do anything yet. We can't do that. And this is the tricky thing.
So this is kind of why I say there's something like a hard fork hypothesis, which is if you look at
the history of humans, the recorded history that we have, at least, nobody would consider us very
good stewards of power. We've used our power to do some lovely things. And we've also used our power
to torture and oppress and kill and destroy environments and whatever. And everybody's
done that. And the few that didn't do it got killed in war by those who did do it. And so
let's say there's a Gaussian distribution on the goodness or badness of our choices, the rivalry
or non rivalry of how we use our power. If we have beings that are at all like the beings we
have been for a long time, and you exponentially increase the power of those choices itself terminates.
So you can't get the power of gods without the love and wisdom and prudence of gods to guide it
and have that be a sustainable scenario. So we actually need humans to be safer vessels for power,
given the amount of power that we're coming into. And we have no idea how to do that. And what that
means is we need a different basis for human choice making than we've ever had to have the
level of choice making power that exponential tech is bringing about. And so then we have to really
get into, well, why is it that humans are making the choices that they do? And what is it that is
conditioning the rival risk basis of those choices that basically have us suck with choices?
Yeah, so we can talk about what are some systems that we could suggest get there. But I would say
that's the core of the thing that we're trying to address is how do we create a system in which
any agents that are making choices are making choices that are not directly causing or indirectly
causing harm to the system, because that leads to the upratcheting of others doing that. And that
leads to self termination. And we're too powerful to do that. So how do we get something like
omniconsideration of the agents leading to omnipositivity of choice making?
Sounds nice. But how do we do it in a world where the ability to project
in a complex systems environment, the impact of any change is well nigh impossible. I would also
add another thing. I'd love to hear your thoughts on this. One of the things we know from human
anthropology is in essentially every society on earth, somewhere between half of 1% and 2% of
humans are sociopaths, right? And I'd add in my experience as a corporate executive that that
number may go up to 10% or more in the sea level suites and probably higher than that in the world
of finance. So we have this problem. How do we evaluate the impact of a proposed change in a
complex system, which is inherently not very predictable, a little bit predictable, but we
lose our ability to see quickly and that some noticeable percent of the actors, you say 1%
of sociopaths doesn't sound that bad. That means in the United States, there's 3 million sociopaths,
right? And they gravitate towards power and a lot of them are pretty damn good at manipulating
people. How do we get across those two traps? Yeah, if I look at all of the cluster B personality
disorders in the current DSM, I would say they're all pretty concerning. So, you know, not just
sociopathy, but narcissism, etc. But the most current stats I have seen were 3 to 5% of the
general population in the developed world's tests for sociopathy and 30% of people in the
C-suite of Fortune 500 companies. I'm going to share a thought on this that I don't have all
the data to back up, but I don't think would be hard to come up with. So why do we get so much
concentration of sociopathy in the top of Fortune 500 companies and politics and then especially
things like finance? Well, because they're basically systems to attract, reward, incentivize, and
condition sociopathy. Because to get to the top of a power game, it's going to be people who are
attracted to power and people who are good at winning a bunch of win-lose games. Because at
each step they move up the ladder, they're winning against somebody else usually via involving things
like disinformation and defection and whatever it is. And so if you think about the nature of what a
government or a corporation or any top-down power system are, it is basically a strange attractor
for people who want to have power over for people who are running power dynamics. And this is why,
let's try and say that I had a benevolent dictator. Well, there's a reason that we don't
get sustainable benevolent dictators is because let's say I had a benevolent dictator and we
can get this in a corporation from great founder theory sometimes because if the founder holds the
majority of stock and whatever, but it never outlives them and usually they end up getting kicked
out. So let's say I have a benevolent dictator. All the people who are one step under them are
doing things that they require to be able to stay as a dictator because it's pretty easy to kill
somebody or to oust them or to whatever. So if I'm at the top of a top-down power system, I have to
keep everybody under me preferring me to be above them rather than overthrow me, which means that
rather than do what's best for everybody, I have to do what's best largely for those who are right
near me. And they have to do that for those who are under them. And that ensures a kind of power
law distribution of power. And again, there's like a multipolar trap on corruption. If anyone's
willing to do a really fucked up thing to try and overthrow me, I have to be able to play at the
game of fucked up things where I get overthrown. So we can see how top-down power systems are going
to both attract, condition, reward, incentivize things like sociopathy. And so then we end up
having a world run by sociopaths, which is not a good thing for anybody. But now let's think about
something like a tribe. And I'm not going to over romanticize here. I'm just kind of thinking
through the dynamics in a first principles way. If I've got 40, 50, 70, up to a Dunbar number of
people living in a tribe, there's an extraordinarily high degree of transparency that is forced in
that scenario. Everybody pretty much sees everything that's going on with everybody. And
everybody knows everyone. Everyone has fealty relationships with everybody in the tribe. So
sociopathy is not going to be advantageous. You're not going to have an evolutionary niche in that
environment for much in the way of conspiring and lying because it will get found out and it
will get punished. And so the forced transparency creates an accounting system where you don't
get an evolutionary niche for somebody fucking the other people in the system. And so as soon
as the system starts to get large enough, that one, there's anonymous people so I can harm people
who I don't really know and care about as opposed to everybody who is in the system is somebody
that I know and care about. And two, I can do stuff that people won't be able to see. I can kind
of have a corruption of the accounting in the system. Now we get an evolutionary niche for
rather than participating with the system, doing internal defection. I'm not externally
defecting and leaving the system. I'm internally defecting and playing the system. And that's
what most everyone inside of a corporation or a government is optimizing what is good for them
and their direct fealty relationships rather than what's good for the whole and nobody can tell.
And this is a particularly hard scenario. But the reason I'm saying this is because we do our
social science inside of a world where these systems have become ubiquitous. And then we assume
that those properties where there's ubiquitous conditioning are intrinsic to human nature.
And I think we have to be very careful about that. Because I think a lot of them are not intrinsic
to human nature. They are a result of the ubiquitous conditioning. And we could create
conditioning environments in which things like sociopathy are just not advantageous.
And so they don't get upregulated. The anthropologists seem to find sociopaths
the varying ratios, but they find them at least half a percent ratio pretty much every place.
And in fact, I've thrown a challenge out to some anthropologists who they've not been able to reject
it. One of the big questions in anthropology is how did human society transition from chieftains
to early states? And my conjecture has been that it's based on the arrival of a sufficiently
charismatic sociopath. And that's the story, right? And they've pulled their beards and said,
hmm, that's an interesting theory. Here's even some ways to measure it. So I think if we're
going to design a social operating system, we're going to have to assume some level of sociopathy
and have some defense mechanisms for saying. Well, so let's go ahead and do the analogy and say
that sociopathy within a social body is like a cancer cell inside of a animal body, which is
not cancer cells inside of a human body are doing something that is good for them and good for the
other cells around it and good for the whole simultaneously. And they have an evolved coordination
system to be not in rivalrous dynamic with each other. The heart and the lungs are not rivalrous
with each other. They're not competing to extract scarce resource and hoard it. They're in this,
you know, kind of radically necessarily symbiotic relationship. Now, a cancer cell realizes that
it can actually consume and reproduce faster if it defects on the agreement. And that happens all
the time, but that cancer cells only able to affect the cells immediately around it. And so then
they're able to either fix the cell or kill it and limit its effective action. If the cancer
cell could broadcast oncogenes to all cells in the body simultaneously, because it had something
like technology to be able to leverage what it wanted to do, we'd be fucked. And so when we
think about the world today and the capacity that exponential technology gives for anyone to have a
much stronger coupling to the whole system, not mediated through having to have that flow through
a bunch of other agents where error correction can occur. This is one of the things that's really
problematic with the world today is that even small numbers of people that become sadistic,
sociopathic, whatever can really fuck the whole system up. And so you actually have to have something
that creates anti fragility for this particular thing. And so we can say to have a system that
doesn't have catastrophic collapses eminent, it has to be able to limit this. And one way of doing
it is the China strategy, which is ubiquitous surveillance. And anyone does anything that
looks at all concerning and their powers removed from them. And I would argue that that system
will also inexorably collapse. Because even though it won't collapse because of multipolar traps,
it'll collapse because we notice that markets are much better at innovation, bottom up processes are
much better at innovation than top down processes tend to be. If you control the bottom up processes
that rigorously, you won't end up being able to innovate enough to keep up with changing
environments, and you'll fail to red queen dynamics. That's my prediction for the China
strategy. But what I would say is that a tribe is actually like a family. And a tribe or a village
is actually a really good method for being able to do surveillance. And I mean, a very healthy
rather than a fucked up type of surveillance, it's not a top down one to many surveillance.
It's a many to many and where the goal is not prevent someone from doing bad things,
but actually caring about people. And so rather than the sesame credit version or the
Institute of religious idea where everyone shames everyone out of fear of God version,
this is a have a situation where no one can actually be a shut in, right? We noticed that
when somebody goes and shoots a bunch of people up with AR 15, typically they weren't in interaction
with a lot of other humans, they were able to because of modern society, they were able to live
for a while where afterwards in interviews, their neighbors say we never saw him, he was real quiet,
he kept to himself, he never came out. As we have increasing technological capacity to agents,
we can't have agents that can evolve in their psychopathology unchecked. We also can't have
situations where they are interacting that they can hide the results. So we need something like
both real accounting of what's happening and everyone having to interact with other people
in local ways that ensure the health of people and if not actually take care of them. So that's
one of the first things I can talk about is that as long as the defect on the whole is more
advantageous, there's two things. One is as long as the defect on the whole is more advantageous
than participate with it, it will happen. And so you have to have the kinds of accounting that
keep that from occurring. And the other thing is as long as individuals can become psychologically
damaged and have that not be noticed and still have access to power, that's also a problem.
So there is a process by which the psychological health of people has to be noticed and the process
by which the social system has to be more advantageous to participate with and to defect against.
You have some ideas on how we might structure this sounds like a cell network, essentially,
where people are assembled at the Dunbar Dumber below in an intense solid way and presumably
with minimal ability to migrate because we all know that the stranger in town is a much riskier
character than the person that's been around for 30 years. Talk a little bit about how that might
actually be accomplished. I am going to go somewhere that's going to be really bothersome.
I don't know of any model where that's accomplishable as long as there's private property.
Okay, we certainly have alternatives such as syndicalism, which is quasi-private property,
but not in the same sense we have here. And then there's anarchy, which is another similar
model. So I don't think we have to keep private property on the table. So I would entertain
any proposal that you think could implement this theory. I think if organs believed that they could
live at the expense of the other ones and that a famine might come so they needed to hoard resource
because they wouldn't be able to get it from other ones and they were actually in competition
for scarce resource, we'd be fucked, right? The body would break down very, very quickly.
And so you don't see the coordination dynamics of cells or organs or tissues involving something
like private property. You have a situation where things are stored wherever they're stored,
the calcium stored in the bones, fat cells or wherever they are for the utilization by any part
of the system that needs it as it's needed. So you don't have a situation where any cell or organ
has a delta between what's good for it and what's good for the whole because it depends upon the
rest of the whole. I do believe that we have to create a similar thing in the human systems.
And specifically, my ability to increase my own private property ownership, my own balance sheet
increases my quality of life. Not only can I decouple that from you or the commons, I can
anti-couple it. I can directly fuck the commons or fuck you and get ahead in that way. That becomes
the basis for misaligned agency. And I think we assume that basis for misaligned agency kind of
across the board. And of course, I'm not going to present something like Marxism. But I think if
you actually study something like how the resource provisioning of something like a body works,
it is obviously neither socialism or Marxism or capitalism. It's a much more complex system
with different underlying axioms. And so let's say that rather than you possess a good in order
to have access to it, your capacity to access it is to be a possession. But your possession means
that I no longer have access to it. And the scarce for something is the more value we give to it.
So then we also have an incentive to create artificial scarcity because abundance makes
the stuff not worth anything. And you also then have an incentive to possess more stuff than you
need, especially when you get compounding returns on the stuff that you have. So your incentive
is to extract as much it as possible to drive scarcity in the system to hoard information,
to cause disinformation, all of those things associated with the private ownership type
advantages. So then anything that you own that I no longer have access to, I'm not stoked on you
owning stuff, right? I'm actually don't want you to own the stuff I want to own the stuff.
If instead you have access to something that is part of a Commonwealth resource where your access
doesn't remove my access via possession, like shopping carts at the grocery store, because
there is enough of them for peak time, everybody doesn't have to bring their own shopping cart,
which would be a pain in the ass, take a lot more resources and, you know, obviously be more
difficult. So you having access doesn't bother me at all because it doesn't limit my access. We
start to say how many places could this be the case? And we see that with the sharing economy,
we could replace transportation comprehensively with Commonwealth shared resources rather than
possessed resources have just enough for peak time plus maintenance, which would be something like a
20th of the total number that are there now have them be higher quality for everybody and obviously
lower accidents, higher quality, much lower cost of civilization, because you don't have a bunch
of grotesque duplication, and you don't have money having to go to marketing budgets and financial
services, it just goes to product development, you don't have designed an obsolescence, you have
modular upgrade ability built right into the system. So you can see a much higher quality
of life for everyone with a much lower load on the system. And you also then remove the
destructive competitive dynamics in doing so. Now, let's say that rather than have a central
company like an Uber, whatever mediating that, which now still has a misalignment of agency,
we use something like a blockchain type system to be able to make that actually a Commonwealth
resource where the money that would be extracted from the system doesn't need to be extracted
from the system, you start to see, oh, we could actually create things that are like Commonwealth
access based dynamics where you having access doesn't decrease my access, but it's actually
better than that. That would just be non rival risk rival risk is your access mediated through
possession decreases mine so fuck you rival risk is anti coupled your wellbeing and mine are actually
anti coupled non rival risk is just uncoupled which I'll argue is not strong enough anti rival risk
is rigorously positively coupled your wellbeing and mine go up and down together. So in a situation
where you have access to the transportation and access to other Commonwealth resources,
it actually increases your capacity to be creative. Now you can also get to the maker studio and to
the art studio to make art and music and your motive isn't to get money by selling the art
because you already have access to all the things that money would normally give you. So getting
stuff doesn't confer advantage and also doesn't grant identity. Your identity is now only going
to come not through what you get out of the system but by what you create and contribute to the system
and we don't get the same zero sum dynamics on contribution to a system through creativity as
you do through getting stuff because it's much harder to compare a Salvador dolly and an mc
Escher than it is dollars and dollars right creativity ends up being non fungible. Yeah,
very high dimensional. Right. And so now if I have a system where if you don't get stuff,
you die and then if you don't get stuff, you don't breed and then if you don't get stuff,
you don't self actualize at every level of Maslow's hierarchy, you have to get stuff and
you're getting stuff involves other people not having stuff. That's the best way to condition
greed, jealousy and sociopathy and everybody and then call it human nature. If I have a situation
in which having access to those things is a given and it's utterly boring and the only way that you
actually have self actualization is through shit that you create. But as other people are more
creative, the commonwealth that you live in is better and you actually have access to more stuff
than you both have a much better basis for real creativity. That's one master, not two masters
and you're also incented to support everyone else to self actualize because your life is directly
better than they do. So in a world where you have access to the transportation and the makers
studios and the whatever, then I get to listen to better music and have access to better shit.
So this is one part of it is how we move from rival risk through non rival risk to anti rival
risk is we have to have situations where we are actually coupling our incentive in our agency
rather than having it anti coupled. And so there's much more to say about it, but this is one example.
Now, here's the key part. I will buy that this would be great if we could do it. In fact, Jordan
Greenhall and I and some other folks cooked up fairly naive political programs several years ago
and we identified maximizing self actualization as the highest goal and we weren't as structured as
this thinking, but we had some things that were moving in this direction, which eventually became
a concept called game B. But game B at that point at least failed because it couldn't convince any of
us or at least the critical mass of us that there was a reasonable way to get from here to there.
You know, the current system is pretty damn optimized for doing what it does. And how do you get
people to switch from the current activities that they're engaged in, which are relatively
economically optimized to this new alternative system? That's the hard question. And I would
put to you that that transition from game A to game B is the part that so far many of us have
never seen a reasonable path. Yeah. So first, I think inadequate blueprint for game B has to
become clear. And so we have to say that we have a set of architectures that meet necessary and
sufficient criteria. Otherwise, we don't know how to reverse engineer. Then yes, there has to be an
implementation path. First, though, I want to say that I don't think the current system is close to
economically optimized. And this is actually a very important point. Let's say that I'm one of the
richest people in the world today. I'm Bill Gates or Warren Buffett or whatever. There's really
important stuff that the world could produce that it can't inside of capitalism that I don't have
access to. And this is actually kind of everywhere. And it's really basic. The best phone that science
could make involves some intellectual property owned by Apple, and some owned by Google, and some
owned by a few different companies. And the same is true with the best laptop and the best car.
And even with billions of dollars, I can't buy that thing. And all the things that I can buy
are produced by someone where not only do they have limited IP, but they also have whatever
designed in obsolescence and desire for proprietary stuff. So you use the rest of their ecosystem
of stuff. So it's not interoperable. I have to deal with that shitty suboptimality even from the
richest guy in the world. Goddamn Apple is the perfect example, right? At one level, they have
some beautiful engineering, nice integration. But on the other side, they're fucking you,
they're fucking you, they're fucking you all the time, right? It's amazing. Fucking you and the
environment and people and all kinds of things, right? And so then I go to things like, okay,
let's take a look at, and this might be more controversial, but I don't think it should be,
let's take a look at biotech and health research. There's a whole bunch of shit that makes sense
to research where there's just no money to research it. So we did so much work in small molecules
because they were patentable. And it was really critical they were patentable, because if I had
to go through FDA trials, it cost a billion dollars, and I needed to make that money back,
I had to be able to have the patent on the thing. So other people couldn't undercut me just
on cost of goods. So we were only going to research it was patentable, the patentable
stuff was only going to be synthetic stuff, because we don't want people to be able to patent
intrinsic parts of humans or nature. So that means that anything that was a part of how my body
worked when I was healthy, doesn't get to be something I do research on, because I won't be
able to ever make the money on that research back. And a synthetic molecule that wasn't part of the
evolutionary environment, wasn't part of how my body worked when it was healthy, it doesn't even
make sense that that would actually be a real cure. And that's the only shit we're going to research.
And of course, I live in a world where war is more likely and the environment is fucked for
reasons that even from the richest person in the world, I can't control. And where near-term
catastrophic risk affects everybody, including me. So I think today, even the wealthiest people,
if they are willing to think about it, can recognize that the world that we are discussing,
if it was possible, is comprehensively better for them than the current world is.
I'm not sure I believe that, but I try to sell that to Peter Thiel.
Yeah, so I won't name names, but I have had conversations like this with many people of
that class. And both in the face of the inexorability of catastrophic risk and the
incapacity for this system to produce some really important stuff that they would care about,
a lot of people are increasingly capable of recognizing that they just don't feel like
they know how to initiate the new system either.
Well, that's good news, actually. And of course, I think Ecoside is now becoming the forcing
function. Anyone who is really thinking seriously about this stuff has to realize that we have
either already overshot or are about to overshot the carrying capacity of the earth. And that alone
ought to be a pretty strong argument to go on a different road. But as we talked about before,
people are locked into these rival restructures and their fractal and their nature, their
a goodly amount of their self-definition is about where they sit in this complex,
multi-level pecking order. That's a big lift to get 330 million Americans, let alone eight and a
half billion people in the world to retreat from the game, a hierarchical fractal structure and move
towards something else. Yeah, everyone is stuck in a kind of multipolar trap regarding incapacity
to coordinate better. Because let's say I got a billionaire and they say, okay, well, that makes
sense. But I have no idea how to build a new system. And I have no idea how to get people to
participate with a new system. And as long as other people are doing the fuck thing, then it
doesn't seem like there is any other game for me to play. It's kind of like there is really only
one game in town. So if we can even take like, let's take Elon Musk as an interesting example,
where, okay, arguably one of the highest agency people in the world currently, right? And after
he came across Bostrom stuff, he became very concerned about existential risk from AGI, created
open AI to work on it. And then when you saw him on Joe Rogan a few months ago, he was saying,
I've spent years trying to get the world to understand this AGI issue and to create different,
you know, safety protocols and their own incentive to be their first first mover advantage,
whatever is such that I can't do anything about it. And I basically have kind of given up on being
able to protect against this risk. And all we can do now is just hope that the risk isn't terrible
and try to mitigate it. So if even the most agentic, powerful people feel like they actually don't
have the capacity to do anything in the presence of, you know, multipolar trap type dynamics,
and it's like, all right, how do you do it? This is the transition question. Well, one thing is the
what is the new thing you transition to has never been adequately specified. So we have to be able
to specify something that meets the solution to these generator functions. And again, that's a
longer conversation, but we could go further, we're starting to lay some of the groundwork. And as
far as transition goes, I'll share one example of a way to think about it. I'm not saying this is
how it will happen. I'm saying this at least provides a thought experiment. First, there is no such
thing as sustained competitive advantage from a single innovation. Because if I have an innovation,
whether it's better extraction tech or economic tech or social tech or warfare tech, the moment I
deploy that capacity, then everyone else will see it reverse engineer it, you know, and make their
own kind of innovations on it. And so sustained competitive advantages and ongoing innovation
competition, which is the upratcheting of rival risk power, which is the dynamic that we're looking
at here. And so there's this question most people get stuck in, which is like, okay, well, the current
system seems to be self terminating. So if we need to make a new system that outcompetes this
system, what source of asymmetric advantage of the current system is it going to have? And how
does that not just, is that not just part of the same power game? Because we're used to thinking
in terms of outcompetes. And so like, if you think historically that there were less violent cultures
that invested less in military and more in quality of life arts and sciences and humanity and lived
in more harmony with the environment and lower population, they just got slaughtered by the
warring cultures. And then when the warring cultures intersected, the most successful one
subsumed to the successful parts of the other one. And there was this basically distillation
of successful warfare capacity. And we're like, okay, well, we don't want to try and be Tibet in
the presence of China again, or any of those scenarios. So I can't make some non violent
thing that will simply just get killed. And yet, whatever else I create, if that isn't the case,
seems like it's still just competing at the game of power. So fuck, what is the option? So there's
a question of what could provide increased capacity that can't be weaponized? That's a very
interesting question. We could say that every technology that increases capacity can be weaponized,
meaning can be used by some agent to increase their capacity relative to other agents or the
commons, except if we had a social technology that was anti rivalrous, but it actually produced
increased coordination capacity, you actually can't weaponize it because it is the solvent for
weaponization itself. It is actually the basis of how agents interact in a way that doesn't
incentivize weapons. And so for anyone to instantiate that thing, they are actually changing the
nature of their agency. So in the current system, again, private balance sheet, I'm in a big
corporation, I'm going to do the thing that optimizes my bonus structure and my status in
the company, even if it fucks other people in the company and the company as a whole,
and that might include spreading disinformation with holding information, etc. Well, let's say I
could create a situation where I couldn't get ahead at the expense of the whole. So I had both the
right kind of transparency and accounting systems and access to Commonwealth resources where only
as the Commonwealth does better, do I do better and things like that, then we can have a situation
where no one let's just say if we could invent a situation in which no one had an actual incentive
to spread disinformation or to hoard information, if they shared true information, maybe they'd be
wrong, but they at least had the incentive for full earnestness and full transparency. If you
had a situation where that was the incentive, you figured out how to do that. And I will say
there is a way to do that. I believe there's a way to do that. Then you would get a situation where
you had an information ecology that was actually intact at a larger scale that would lead to
radically better capacity to coordinate and innovate better sense making the current system
has. And that system as a whole would be more effective at producing all of the metrics that
matter relative to total resource per capita than any current system would be. And I only need a
small number of people, relatively small number of people who get that and want to instantiate it
as a new full stack civilization to create a new stranger tractor or new attractor basin where
anyone else looking at it says, wow, quality of life is higher on every metric there. And
they're also able to out innovate us on a bunch of things. They're figuring out solutions to
shit that we don't have. Well, then why don't we just kill them? Well, because they aren't trying
to win the game of power against us. They aren't building militaries or signaling or narrative
warfare to try and beat us. And actually, they're exporting solutions to us that we need to the
rest of the world. Because if they have increased capacity to innovate and solve problems, because
they can actually coordinate better because they don't have disinformation information with holding,
then they can look at what groups that would otherwise have enmity with them actually need,
develop those solutions and create dependence rather than enmity relationships while simultaneously
saying if you want to know how to do this shit as well, we've actually open sourced it. It's a
social technology, you're welcome to use the social technology, but the social technology will
fundamentally change your basis for agency if you employ it. So obviously, there's a million things
we would need to dig into there. But just the thought experiment goes, fast adopters build a new
ground up system where the new ground up system becomes a new attractive basin. And, you know,
that's a way of thinking about that I don't try and if I have to shift at the level of axioms,
I can't retrofit the current system, I have to build something. But if the thing that I build is
fundamentally more attractive ground up, then I only need fast adopters to understand it in
concept to have medium adopters understand it after seeing its implementation.
I really like that. And in fact, when you were talking about the lack of sustainable competitive
advantage from any single innovation, I was thinking there's a soft counter examples,
not good forever, but it's what we call network effects, right? And if we re spin what you just
said a little bit, suppose we were able to create a strong beneficial network effect to people who
played by positive generative rules and did so in a way that was not directly threatening back to
people who didn't play by the rules. But because these people were better sense makers and choice
makers, they actually could create things to trade back to people who weren't on the network.
And of course, as we know, the early adopter A types will come and join the network. And once
it's proven to be more successful per unit of human effort at both well being, but also importantly,
actual creation, the next layer of people will come in if only to hang out with the A's.
And then you make that as you said, I think this was the brilliant part that I hadn't
thought of before, you make the ground rules such that anyone who becomes a member has essentially
committed to a doctrine, which is subversive of the previous paradigm, but in a subtle way,
not directly challenging it, just a value orientation and a set of rules for dealing
with each other. And this again gets very much back to the original naive spirit of game B,
where one of the things we did was gave ourselves all the title of peer. We always said that when
you're dealing with another game B person, you have a moral obligation of considerable power to
deal with them as a peer irrespective of any other different other dimensions of power differences.
And so if this strong beneficial network effect system had as an example that anyone who's a
member of it is within the constraints of the system, at least a true peer, that would be very
interesting. Yeah, so I can only compete on a defined and narrow metric, right? So if we're
competing over who has more money or who's taller, who can run faster, then we can just have a
straight up competition. But how do I compare Dolly to Escher is a fucked up thing, right? You
actually kind of can't do that in a meaningful way. And any way that you try to say, well,
here's a metric or some set of metrics with which to assess that you've actually reduced the thing
to something it isn't. And so you could imagine that an Escher and a Dolly could interact with
each other in a way where they both acknowledge that each other are bringing something to the
world that is actually enriching the world and beautiful that they aren't bringing and that
they're stoked each other are doing it. And there's no hierarchy intrinsic in that. And I'm not
saying there are never healthy hierarchies, but I'm saying in general, rather than a competition
on a very narrow metric, which is inherently information reduction, we are seeking self
actualization of rich creativity. And that intrinsically can't be holistically compared
in the same way. So that's one big part of it. And the other part of it is, you know,
you and I had a conversation the other day, Jim, where you said something and you and I are just
getting to know each other. And I like I instantly loved you because of it, because you said anyone
who would abuse people with a below 90 IQ, I want to beat their head in with a baseball bat or
something like that. And that you wanted to see that those who have more intelligence are actually
protective of rather than exploiting of those who couldn't compete. So then the question is what made
you that way? And how do we create social systems where to the extent that anyone has obviously
increased capacity over anyone else, they're actually oriented to steward everyone else rather
than use that in an exploitive way. And I think this is conditionable both at the level of social
values and at the level of the way values are codified in a value equation, i.e. the economics
and the social system. Wow, I think we should wrap it here. We could go on for three more hours,
but I'd like to thank you for our amazingly interesting conversation. This was a delight.
Production Services and Audio Editing by Stanton Media Lab. Music by Tom Muller at ModernSpaceMusic.com.
