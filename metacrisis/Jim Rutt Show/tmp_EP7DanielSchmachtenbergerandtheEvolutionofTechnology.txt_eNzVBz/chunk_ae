anti coupled non rival risk is just uncoupled which I'll argue is not strong enough anti rival risk
is rigorously positively coupled your wellbeing and mine go up and down together. So in a situation
where you have access to the transportation and access to other Commonwealth resources,
it actually increases your capacity to be creative. Now you can also get to the maker studio and to
the art studio to make art and music and your motive isn't to get money by selling the art
because you already have access to all the things that money would normally give you. So getting
stuff doesn't confer advantage and also doesn't grant identity. Your identity is now only going
to come not through what you get out of the system but by what you create and contribute to the system
and we don't get the same zero sum dynamics on contribution to a system through creativity as
you do through getting stuff because it's much harder to compare a Salvador dolly and an mc
Escher than it is dollars and dollars right creativity ends up being non fungible. Yeah,
very high dimensional. Right. And so now if I have a system where if you don't get stuff,
you die and then if you don't get stuff, you don't breed and then if you don't get stuff,
you don't self actualize at every level of Maslow's hierarchy, you have to get stuff and
you're getting stuff involves other people not having stuff. That's the best way to condition
greed, jealousy and sociopathy and everybody and then call it human nature. If I have a situation
in which having access to those things is a given and it's utterly boring and the only way that you
actually have self actualization is through shit that you create. But as other people are more
creative, the commonwealth that you live in is better and you actually have access to more stuff
than you both have a much better basis for real creativity. That's one master, not two masters
and you're also incented to support everyone else to self actualize because your life is directly
better than they do. So in a world where you have access to the transportation and the makers
studios and the whatever, then I get to listen to better music and have access to better shit.
So this is one part of it is how we move from rival risk through non rival risk to anti rival
risk is we have to have situations where we are actually coupling our incentive in our agency
rather than having it anti coupled. And so there's much more to say about it, but this is one example.
Now, here's the key part. I will buy that this would be great if we could do it. In fact, Jordan
Greenhall and I and some other folks cooked up fairly naive political programs several years ago
and we identified maximizing self actualization as the highest goal and we weren't as structured as
this thinking, but we had some things that were moving in this direction, which eventually became
a concept called game B. But game B at that point at least failed because it couldn't convince any of
us or at least the critical mass of us that there was a reasonable way to get from here to there.
You know, the current system is pretty damn optimized for doing what it does. And how do you get
people to switch from the current activities that they're engaged in, which are relatively
economically optimized to this new alternative system? That's the hard question. And I would
put to you that that transition from game A to game B is the part that so far many of us have
never seen a reasonable path. Yeah. So first, I think inadequate blueprint for game B has to
become clear. And so we have to say that we have a set of architectures that meet necessary and
sufficient criteria. Otherwise, we don't know how to reverse engineer. Then yes, there has to be an
implementation path. First, though, I want to say that I don't think the current system is close to
economically optimized. And this is actually a very important point. Let's say that I'm one of the
richest people in the world today. I'm Bill Gates or Warren Buffett or whatever. There's really
important stuff that the world could produce that it can't inside of capitalism that I don't have
access to. And this is actually kind of everywhere. And it's really basic. The best phone that science
could make involves some intellectual property owned by Apple, and some owned by Google, and some
owned by a few different companies. And the same is true with the best laptop and the best car.
And even with billions of dollars, I can't buy that thing. And all the things that I can buy
are produced by someone where not only do they have limited IP, but they also have whatever
designed in obsolescence and desire for proprietary stuff. So you use the rest of their ecosystem
of stuff. So it's not interoperable. I have to deal with that shitty suboptimality even from the
richest guy in the world. Goddamn Apple is the perfect example, right? At one level, they have
some beautiful engineering, nice integration. But on the other side, they're fucking you,
they're fucking you, they're fucking you all the time, right? It's amazing. Fucking you and the
environment and people and all kinds of things, right? And so then I go to things like, okay,
let's take a look at, and this might be more controversial, but I don't think it should be,
let's take a look at biotech and health research. There's a whole bunch of shit that makes sense
to research where there's just no money to research it. So we did so much work in small molecules
because they were patentable. And it was really critical they were patentable, because if I had
to go through FDA trials, it cost a billion dollars, and I needed to make that money back,
I had to be able to have the patent on the thing. So other people couldn't undercut me just
on cost of goods. So we were only going to research it was patentable, the patentable
stuff was only going to be synthetic stuff, because we don't want people to be able to patent
intrinsic parts of humans or nature. So that means that anything that was a part of how my body
worked when I was healthy, doesn't get to be something I do research on, because I won't be
able to ever make the money on that research back. And a synthetic molecule that wasn't part of the
evolutionary environment, wasn't part of how my body worked when it was healthy, it doesn't even
make sense that that would actually be a real cure. And that's the only shit we're going to research.
And of course, I live in a world where war is more likely and the environment is fucked for
reasons that even from the richest person in the world, I can't control. And where near-term
catastrophic risk affects everybody, including me. So I think today, even the wealthiest people,
if they are willing to think about it, can recognize that the world that we are discussing,
if it was possible, is comprehensively better for them than the current world is.
I'm not sure I believe that, but I try to sell that to Peter Thiel.
Yeah, so I won't name names, but I have had conversations like this with many people of
that class. And both in the face of the inexorability of catastrophic risk and the
incapacity for this system to produce some really important stuff that they would care about,
a lot of people are increasingly capable of recognizing that they just don't feel like
they know how to initiate the new system either.
Well, that's good news, actually. And of course, I think Ecoside is now becoming the forcing
function. Anyone who is really thinking seriously about this stuff has to realize that we have
either already overshot or are about to overshot the carrying capacity of the earth. And that alone
ought to be a pretty strong argument to go on a different road. But as we talked about before,
people are locked into these rival restructures and their fractal and their nature, their
a goodly amount of their self-definition is about where they sit in this complex,
multi-level pecking order. That's a big lift to get 330 million Americans, let alone eight and a
half billion people in the world to retreat from the game, a hierarchical fractal structure and move
towards something else. Yeah, everyone is stuck in a kind of multipolar trap regarding incapacity
to coordinate better. Because let's say I got a billionaire and they say, okay, well, that makes
sense. But I have no idea how to build a new system. And I have no idea how to get people to
participate with a new system. And as long as other people are doing the fuck thing, then it
doesn't seem like there is any other game for me to play. It's kind of like there is really only
one game in town. So if we can even take like, let's take Elon Musk as an interesting example,
where, okay, arguably one of the highest agency people in the world currently, right? And after
he came across Bostrom stuff, he became very concerned about existential risk from AGI, created
open AI to work on it. And then when you saw him on Joe Rogan a few months ago, he was saying,
I've spent years trying to get the world to understand this AGI issue and to create different,
you know, safety protocols and their own incentive to be their first first mover advantage,
whatever is such that I can't do anything about it. And I basically have kind of given up on being
able to protect against this risk. And all we can do now is just hope that the risk isn't terrible
and try to mitigate it. So if even the most agentic, powerful people feel like they actually don't
have the capacity to do anything in the presence of, you know, multipolar trap type dynamics,
and it's like, all right, how do you do it? This is the transition question. Well, one thing is the
what is the new thing you transition to has never been adequately specified. So we have to be able
to specify something that meets the solution to these generator functions. And again, that's a
longer conversation, but we could go further, we're starting to lay some of the groundwork. And as
far as transition goes, I'll share one example of a way to think about it. I'm not saying this is
how it will happen. I'm saying this at least provides a thought experiment. First, there is no such
thing as sustained competitive advantage from a single innovation. Because if I have an innovation,
whether it's better extraction tech or economic tech or social tech or warfare tech, the moment I
deploy that capacity, then everyone else will see it reverse engineer it, you know, and make their
own kind of innovations on it. And so sustained competitive advantages and ongoing innovation
competition, which is the upratcheting of rival risk power, which is the dynamic that we're looking
at here. And so there's this question most people get stuck in, which is like, okay, well, the current
system seems to be self terminating. So if we need to make a new system that outcompetes this
system, what source of asymmetric advantage of the current system is it going to have? And how
does that not just, is that not just part of the same power game? Because we're used to thinking
in terms of outcompetes. And so like, if you think historically that there were less violent cultures
that invested less in military and more in quality of life arts and sciences and humanity and lived
in more harmony with the environment and lower population, they just got slaughtered by the
warring cultures. And then when the warring cultures intersected, the most successful one
subsumed to the successful parts of the other one. And there was this basically distillation
of successful warfare capacity. And we're like, okay, well, we don't want to try and be Tibet in
the presence of China again, or any of those scenarios. So I can't make some non violent
thing that will simply just get killed. And yet, whatever else I create, if that isn't the case,
seems like it's still just competing at the game of power. So fuck, what is the option? So there's
a question of what could provide increased capacity that can't be weaponized? That's a very
interesting question. We could say that every technology that increases capacity can be weaponized,
meaning can be used by some agent to increase their capacity relative to other agents or the
commons, except if we had a social technology that was anti rivalrous, but it actually produced
increased coordination capacity, you actually can't weaponize it because it is the solvent for
weaponization itself. It is actually the basis of how agents interact in a way that doesn't
incentivize weapons. And so for anyone to instantiate that thing, they are actually changing the
nature of their agency. So in the current system, again, private balance sheet, I'm in a big
corporation, I'm going to do the thing that optimizes my bonus structure and my status in
the company, even if it fucks other people in the company and the company as a whole,
and that might include spreading disinformation with holding information, etc. Well, let's say I
could create a situation where I couldn't get ahead at the expense of the whole. So I had both the
right kind of transparency and accounting systems and access to Commonwealth resources where only
as the Commonwealth does better, do I do better and things like that, then we can have a situation
where no one let's just say if we could invent a situation in which no one had an actual incentive
to spread disinformation or to hoard information, if they shared true information, maybe they'd be
wrong, but they at least had the incentive for full earnestness and full transparency. If you
had a situation where that was the incentive, you figured out how to do that. And I will say
there is a way to do that. I believe there's a way to do that. Then you would get a situation where
you had an information ecology that was actually intact at a larger scale that would lead to
radically better capacity to coordinate and innovate better sense making the current system
has. And that system as a whole would be more effective at producing all of the metrics that
matter relative to total resource per capita than any current system would be. And I only need a
small number of people, relatively small number of people who get that and want to instantiate it
as a new full stack civilization to create a new stranger tractor or new attractor basin where
anyone else looking at it says, wow, quality of life is higher on every metric there. And
they're also able to out innovate us on a bunch of things. They're figuring out solutions to
shit that we don't have. Well, then why don't we just kill them? Well, because they aren't trying
to win the game of power against us. They aren't building militaries or signaling or narrative
warfare to try and beat us. And actually, they're exporting solutions to us that we need to the
rest of the world. Because if they have increased capacity to innovate and solve problems, because
they can actually coordinate better because they don't have disinformation information with holding,
then they can look at what groups that would otherwise have enmity with them actually need,
develop those solutions and create dependence rather than enmity relationships while simultaneously
saying if you want to know how to do this shit as well, we've actually open sourced it. It's a
social technology, you're welcome to use the social technology, but the social technology will
fundamentally change your basis for agency if you employ it. So obviously, there's a million things
we would need to dig into there. But just the thought experiment goes, fast adopters build a new
ground up system where the new ground up system becomes a new attractive basin. And, you know,
that's a way of thinking about that I don't try and if I have to shift at the level of axioms,
I can't retrofit the current system, I have to build something. But if the thing that I build is
fundamentally more attractive ground up, then I only need fast adopters to understand it in
concept to have medium adopters understand it after seeing its implementation.
I really like that. And in fact, when you were talking about the lack of sustainable competitive
advantage from any single innovation, I was thinking there's a soft counter examples,
not good forever, but it's what we call network effects, right? And if we re spin what you just
said a little bit, suppose we were able to create a strong beneficial network effect to people who
played by positive generative rules and did so in a way that was not directly threatening back to
people who didn't play by the rules. But because these people were better sense makers and choice
makers, they actually could create things to trade back to people who weren't on the network.
And of course, as we know, the early adopter A types will come and join the network. And once
it's proven to be more successful per unit of human effort at both well being, but also importantly,
actual creation, the next layer of people will come in if only to hang out with the A's.
And then you make that as you said, I think this was the brilliant part that I hadn't
thought of before, you make the ground rules such that anyone who becomes a member has essentially
committed to a doctrine, which is subversive of the previous paradigm, but in a subtle way,
not directly challenging it, just a value orientation and a set of rules for dealing
with each other. And this again gets very much back to the original naive spirit of game B,
where one of the things we did was gave ourselves all the title of peer. We always said that when
you're dealing with another game B person, you have a moral obligation of considerable power to
deal with them as a peer irrespective of any other different other dimensions of power differences.
And so if this strong beneficial network effect system had as an example that anyone who's a
member of it is within the constraints of the system, at least a true peer, that would be very
interesting. Yeah, so I can only compete on a defined and narrow metric, right? So if we're
competing over who has more money or who's taller, who can run faster, then we can just have a
straight up competition. But how do I compare Dolly to Escher is a fucked up thing, right? You
actually kind of can't do that in a meaningful way. And any way that you try to say, well,
here's a metric or some set of metrics with which to assess that you've actually reduced the thing
to something it isn't. And so you could imagine that an Escher and a Dolly could interact with
each other in a way where they both acknowledge that each other are bringing something to the
world that is actually enriching the world and beautiful that they aren't bringing and that
they're stoked each other are doing it. And there's no hierarchy intrinsic in that. And I'm not
saying there are never healthy hierarchies, but I'm saying in general, rather than a competition
on a very narrow metric, which is inherently information reduction, we are seeking self
actualization of rich creativity. And that intrinsically can't be holistically compared
