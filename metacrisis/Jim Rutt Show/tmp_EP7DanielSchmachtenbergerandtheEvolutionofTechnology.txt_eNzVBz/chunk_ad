agents, the problem becomes, I would say, effectively impossible. So what's the answer?
That basically says don't ever do anything yet. We can't do that. And this is the tricky thing.
So this is kind of why I say there's something like a hard fork hypothesis, which is if you look at
the history of humans, the recorded history that we have, at least, nobody would consider us very
good stewards of power. We've used our power to do some lovely things. And we've also used our power
to torture and oppress and kill and destroy environments and whatever. And everybody's
done that. And the few that didn't do it got killed in war by those who did do it. And so
let's say there's a Gaussian distribution on the goodness or badness of our choices, the rivalry
or non rivalry of how we use our power. If we have beings that are at all like the beings we
have been for a long time, and you exponentially increase the power of those choices itself terminates.
So you can't get the power of gods without the love and wisdom and prudence of gods to guide it
and have that be a sustainable scenario. So we actually need humans to be safer vessels for power,
given the amount of power that we're coming into. And we have no idea how to do that. And what that
means is we need a different basis for human choice making than we've ever had to have the
level of choice making power that exponential tech is bringing about. And so then we have to really
get into, well, why is it that humans are making the choices that they do? And what is it that is
conditioning the rival risk basis of those choices that basically have us suck with choices?
Yeah, so we can talk about what are some systems that we could suggest get there. But I would say
that's the core of the thing that we're trying to address is how do we create a system in which
any agents that are making choices are making choices that are not directly causing or indirectly
causing harm to the system, because that leads to the upratcheting of others doing that. And that
leads to self termination. And we're too powerful to do that. So how do we get something like
omniconsideration of the agents leading to omnipositivity of choice making?
Sounds nice. But how do we do it in a world where the ability to project
in a complex systems environment, the impact of any change is well nigh impossible. I would also
add another thing. I'd love to hear your thoughts on this. One of the things we know from human
anthropology is in essentially every society on earth, somewhere between half of 1% and 2% of
humans are sociopaths, right? And I'd add in my experience as a corporate executive that that
number may go up to 10% or more in the sea level suites and probably higher than that in the world
of finance. So we have this problem. How do we evaluate the impact of a proposed change in a
complex system, which is inherently not very predictable, a little bit predictable, but we
lose our ability to see quickly and that some noticeable percent of the actors, you say 1%
of sociopaths doesn't sound that bad. That means in the United States, there's 3 million sociopaths,
right? And they gravitate towards power and a lot of them are pretty damn good at manipulating
people. How do we get across those two traps? Yeah, if I look at all of the cluster B personality
disorders in the current DSM, I would say they're all pretty concerning. So, you know, not just
sociopathy, but narcissism, etc. But the most current stats I have seen were 3 to 5% of the
general population in the developed world's tests for sociopathy and 30% of people in the
C-suite of Fortune 500 companies. I'm going to share a thought on this that I don't have all
the data to back up, but I don't think would be hard to come up with. So why do we get so much
concentration of sociopathy in the top of Fortune 500 companies and politics and then especially
things like finance? Well, because they're basically systems to attract, reward, incentivize, and
condition sociopathy. Because to get to the top of a power game, it's going to be people who are
attracted to power and people who are good at winning a bunch of win-lose games. Because at
each step they move up the ladder, they're winning against somebody else usually via involving things
like disinformation and defection and whatever it is. And so if you think about the nature of what a
government or a corporation or any top-down power system are, it is basically a strange attractor
for people who want to have power over for people who are running power dynamics. And this is why,
let's try and say that I had a benevolent dictator. Well, there's a reason that we don't
get sustainable benevolent dictators is because let's say I had a benevolent dictator and we
can get this in a corporation from great founder theory sometimes because if the founder holds the
majority of stock and whatever, but it never outlives them and usually they end up getting kicked
out. So let's say I have a benevolent dictator. All the people who are one step under them are
doing things that they require to be able to stay as a dictator because it's pretty easy to kill
somebody or to oust them or to whatever. So if I'm at the top of a top-down power system, I have to
keep everybody under me preferring me to be above them rather than overthrow me, which means that
rather than do what's best for everybody, I have to do what's best largely for those who are right
near me. And they have to do that for those who are under them. And that ensures a kind of power
law distribution of power. And again, there's like a multipolar trap on corruption. If anyone's
willing to do a really fucked up thing to try and overthrow me, I have to be able to play at the
game of fucked up things where I get overthrown. So we can see how top-down power systems are going
to both attract, condition, reward, incentivize things like sociopathy. And so then we end up
having a world run by sociopaths, which is not a good thing for anybody. But now let's think about
something like a tribe. And I'm not going to over romanticize here. I'm just kind of thinking
through the dynamics in a first principles way. If I've got 40, 50, 70, up to a Dunbar number of
people living in a tribe, there's an extraordinarily high degree of transparency that is forced in
that scenario. Everybody pretty much sees everything that's going on with everybody. And
everybody knows everyone. Everyone has fealty relationships with everybody in the tribe. So
sociopathy is not going to be advantageous. You're not going to have an evolutionary niche in that
environment for much in the way of conspiring and lying because it will get found out and it
will get punished. And so the forced transparency creates an accounting system where you don't
get an evolutionary niche for somebody fucking the other people in the system. And so as soon
as the system starts to get large enough, that one, there's anonymous people so I can harm people
who I don't really know and care about as opposed to everybody who is in the system is somebody
that I know and care about. And two, I can do stuff that people won't be able to see. I can kind
of have a corruption of the accounting in the system. Now we get an evolutionary niche for
rather than participating with the system, doing internal defection. I'm not externally
defecting and leaving the system. I'm internally defecting and playing the system. And that's
what most everyone inside of a corporation or a government is optimizing what is good for them
and their direct fealty relationships rather than what's good for the whole and nobody can tell.
And this is a particularly hard scenario. But the reason I'm saying this is because we do our
social science inside of a world where these systems have become ubiquitous. And then we assume
that those properties where there's ubiquitous conditioning are intrinsic to human nature.
And I think we have to be very careful about that. Because I think a lot of them are not intrinsic
to human nature. They are a result of the ubiquitous conditioning. And we could create
conditioning environments in which things like sociopathy are just not advantageous.
And so they don't get upregulated. The anthropologists seem to find sociopaths
the varying ratios, but they find them at least half a percent ratio pretty much every place.
And in fact, I've thrown a challenge out to some anthropologists who they've not been able to reject
it. One of the big questions in anthropology is how did human society transition from chieftains
to early states? And my conjecture has been that it's based on the arrival of a sufficiently
charismatic sociopath. And that's the story, right? And they've pulled their beards and said,
hmm, that's an interesting theory. Here's even some ways to measure it. So I think if we're
going to design a social operating system, we're going to have to assume some level of sociopathy
and have some defense mechanisms for saying. Well, so let's go ahead and do the analogy and say
that sociopathy within a social body is like a cancer cell inside of a animal body, which is
not cancer cells inside of a human body are doing something that is good for them and good for the
other cells around it and good for the whole simultaneously. And they have an evolved coordination
system to be not in rivalrous dynamic with each other. The heart and the lungs are not rivalrous
with each other. They're not competing to extract scarce resource and hoard it. They're in this,
you know, kind of radically necessarily symbiotic relationship. Now, a cancer cell realizes that
it can actually consume and reproduce faster if it defects on the agreement. And that happens all
the time, but that cancer cells only able to affect the cells immediately around it. And so then
they're able to either fix the cell or kill it and limit its effective action. If the cancer
cell could broadcast oncogenes to all cells in the body simultaneously, because it had something
like technology to be able to leverage what it wanted to do, we'd be fucked. And so when we
think about the world today and the capacity that exponential technology gives for anyone to have a
much stronger coupling to the whole system, not mediated through having to have that flow through
a bunch of other agents where error correction can occur. This is one of the things that's really
problematic with the world today is that even small numbers of people that become sadistic,
sociopathic, whatever can really fuck the whole system up. And so you actually have to have something
that creates anti fragility for this particular thing. And so we can say to have a system that
doesn't have catastrophic collapses eminent, it has to be able to limit this. And one way of doing
it is the China strategy, which is ubiquitous surveillance. And anyone does anything that
looks at all concerning and their powers removed from them. And I would argue that that system
will also inexorably collapse. Because even though it won't collapse because of multipolar traps,
it'll collapse because we notice that markets are much better at innovation, bottom up processes are
much better at innovation than top down processes tend to be. If you control the bottom up processes
that rigorously, you won't end up being able to innovate enough to keep up with changing
environments, and you'll fail to red queen dynamics. That's my prediction for the China
strategy. But what I would say is that a tribe is actually like a family. And a tribe or a village
is actually a really good method for being able to do surveillance. And I mean, a very healthy
rather than a fucked up type of surveillance, it's not a top down one to many surveillance.
It's a many to many and where the goal is not prevent someone from doing bad things,
but actually caring about people. And so rather than the sesame credit version or the
Institute of religious idea where everyone shames everyone out of fear of God version,
this is a have a situation where no one can actually be a shut in, right? We noticed that
when somebody goes and shoots a bunch of people up with AR 15, typically they weren't in interaction
with a lot of other humans, they were able to because of modern society, they were able to live
for a while where afterwards in interviews, their neighbors say we never saw him, he was real quiet,
he kept to himself, he never came out. As we have increasing technological capacity to agents,
we can't have agents that can evolve in their psychopathology unchecked. We also can't have
situations where they are interacting that they can hide the results. So we need something like
both real accounting of what's happening and everyone having to interact with other people
in local ways that ensure the health of people and if not actually take care of them. So that's
one of the first things I can talk about is that as long as the defect on the whole is more
advantageous, there's two things. One is as long as the defect on the whole is more advantageous
than participate with it, it will happen. And so you have to have the kinds of accounting that
keep that from occurring. And the other thing is as long as individuals can become psychologically
damaged and have that not be noticed and still have access to power, that's also a problem.
So there is a process by which the psychological health of people has to be noticed and the process
by which the social system has to be more advantageous to participate with and to defect against.
You have some ideas on how we might structure this sounds like a cell network, essentially,
where people are assembled at the Dunbar Dumber below in an intense solid way and presumably
with minimal ability to migrate because we all know that the stranger in town is a much riskier
character than the person that's been around for 30 years. Talk a little bit about how that might
actually be accomplished. I am going to go somewhere that's going to be really bothersome.
I don't know of any model where that's accomplishable as long as there's private property.
Okay, we certainly have alternatives such as syndicalism, which is quasi-private property,
but not in the same sense we have here. And then there's anarchy, which is another similar
model. So I don't think we have to keep private property on the table. So I would entertain
any proposal that you think could implement this theory. I think if organs believed that they could
live at the expense of the other ones and that a famine might come so they needed to hoard resource
because they wouldn't be able to get it from other ones and they were actually in competition
for scarce resource, we'd be fucked, right? The body would break down very, very quickly.
And so you don't see the coordination dynamics of cells or organs or tissues involving something
like private property. You have a situation where things are stored wherever they're stored,
the calcium stored in the bones, fat cells or wherever they are for the utilization by any part
of the system that needs it as it's needed. So you don't have a situation where any cell or organ
has a delta between what's good for it and what's good for the whole because it depends upon the
rest of the whole. I do believe that we have to create a similar thing in the human systems.
And specifically, my ability to increase my own private property ownership, my own balance sheet
increases my quality of life. Not only can I decouple that from you or the commons, I can
anti-couple it. I can directly fuck the commons or fuck you and get ahead in that way. That becomes
the basis for misaligned agency. And I think we assume that basis for misaligned agency kind of
across the board. And of course, I'm not going to present something like Marxism. But I think if
you actually study something like how the resource provisioning of something like a body works,
it is obviously neither socialism or Marxism or capitalism. It's a much more complex system
with different underlying axioms. And so let's say that rather than you possess a good in order
to have access to it, your capacity to access it is to be a possession. But your possession means
that I no longer have access to it. And the scarce for something is the more value we give to it.
So then we also have an incentive to create artificial scarcity because abundance makes
the stuff not worth anything. And you also then have an incentive to possess more stuff than you
need, especially when you get compounding returns on the stuff that you have. So your incentive
is to extract as much it as possible to drive scarcity in the system to hoard information,
to cause disinformation, all of those things associated with the private ownership type
advantages. So then anything that you own that I no longer have access to, I'm not stoked on you
owning stuff, right? I'm actually don't want you to own the stuff I want to own the stuff.
If instead you have access to something that is part of a Commonwealth resource where your access
doesn't remove my access via possession, like shopping carts at the grocery store, because
there is enough of them for peak time, everybody doesn't have to bring their own shopping cart,
which would be a pain in the ass, take a lot more resources and, you know, obviously be more
difficult. So you having access doesn't bother me at all because it doesn't limit my access. We
start to say how many places could this be the case? And we see that with the sharing economy,
we could replace transportation comprehensively with Commonwealth shared resources rather than
possessed resources have just enough for peak time plus maintenance, which would be something like a
20th of the total number that are there now have them be higher quality for everybody and obviously
lower accidents, higher quality, much lower cost of civilization, because you don't have a bunch
of grotesque duplication, and you don't have money having to go to marketing budgets and financial
services, it just goes to product development, you don't have designed an obsolescence, you have
modular upgrade ability built right into the system. So you can see a much higher quality
of life for everyone with a much lower load on the system. And you also then remove the
destructive competitive dynamics in doing so. Now, let's say that rather than have a central
company like an Uber, whatever mediating that, which now still has a misalignment of agency,
we use something like a blockchain type system to be able to make that actually a Commonwealth
resource where the money that would be extracted from the system doesn't need to be extracted
from the system, you start to see, oh, we could actually create things that are like Commonwealth
access based dynamics where you having access doesn't decrease my access, but it's actually
better than that. That would just be non rival risk rival risk is your access mediated through
possession decreases mine so fuck you rival risk is anti coupled your wellbeing and mine are actually
