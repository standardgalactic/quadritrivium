to exert it. So the UN can't tell a member country that has nukes, you have to get rid of them because
they're like, we have nukes fuck off, what are you going to do and bait us. And this is one of the
big problems is that monopoly force only works when you can exert it. That's why it obviously
doesn't work for nuclear deproliferation. It's why it doesn't work for a lot of global issues.
But it's also why even at a national level with the evolution or the emergence of decentralized
exponential technologies where small groups and non state actors and even individuals can get
catastrophic capacity through gene drives and drones and whatever. If I have a gene drive pandemic
type weapon connected to a dead man switch, there is no rule of law that can be exerted
over me and we're not that far from those types of things being possibilities. So this represents
an emergent breakdown in the capacity for rule of law writ large unless you run a perfected
surveillance state that doesn't allow anyone to have capacities, which is the China strategy.
Again, I'm going to argue here the moderate ameliorative position, even though it may not
actually be mine. Let's take the example of the carbon tax that other countries want to
free ride and defect from, right, which you can easily see there being pressures to do so.
Free riders and defections are the essence of game theory and at least in my mind,
your multipolar trap is a slight generalization of the concepts of predatory game theory. So
there is a partial fix, whether it's enough or not, I don't know, which would be to have
an implicit carbon tariff on imports. So let's say the Chinese want to defect and not collect
the carbon tax. We don't really care to the degree they don't collect the carbon tax that we do care,
but at least partially we can penalize them by putting a tariff equal to the implicit carbon
tax that we would have charged for the equivalent amount of carbon in a product. And especially
in countries like China and increasingly India, which are very dependent on trade, this is at
least a partial way of raising the cost quite significantly for those who defect or attempt
to be free riders. How would you respond to that? Yeah, so let's say the US agreed to do that,
but then let's say Brazil under its new presidency says, fuck that, I'll buy this stuff from China
and they have some capacity to and say Russia does say some other places do China is still growing in
its GDP externalizing cost and those other places are now we're losing relative to them and now you
get increasing citizen pressure to revoke that law. So this is again the essence of a coordination
problem from the view of it makes sense if we all did the good thing, right? And the prisoner's
dilemma, if we all coordinate, it would be good. But if we don't have the capacity to ensure the
coordination, anyone can defect, it becomes very easy to have the strange attractor be everybody
default to defection. Yeah, the answer to the prisoner's dilemmas we all know is you have Tony
Soprano, right? That if you defect, I'll kill you. Yeah, which is basically, you know, some kind of
even worse punishment than the original thing was that's outside of the scenario. And that ends
up being how we do it, right? Which is underneath our tariff situation, we're willing to go to war
to uphold a lot of these things. But now let's come back to even within a country. So we know that
right now, someone's ability to get elected has to do with a largely their access to not just
capital, but also allyship. So that's going to affect how other representatives support them
and how much they can do campaigning and media they can get and all those kinds of things. And
we know that we don't have an educated citizenry. And so you have a situation where people are going
to get elected proportional to really the incentives of the system more than anything else. And because
people can say stuff that isn't true, and people still believe it, and they can do Russell conjugations
to give people the wrong sense of things and appeal to emotional triggers and in group defection
dynamics and stuff like that. So let's say that we try to make a law, then of course,
everyone who has financial interest that would be damaged by that law supports the other candidate.
So then we have to do something like campaign finance reform. But who is going to bring the
campaign finance reform through this isn't being one of the key things is that the lobbyists are
paid for by somebody, the people who are working to change the laws continuously are paid for by
somebody. And whoever it is that would the ability to get money to someone who is campaigning also
is very easy to hide through offshore banking and through third party entities and think tanks
and whatever. So because we don't have something like perfected transparent accounting, if I try
to bind incentive using law, everyone who has the incentive to change that law has more resources
than those that are trying to bind it and they end up basically winning. And the gist there is that
economics is deeper than law is in the stack of power. And so you can use law to bind economics
to a limited degree. But let's say again, the company, a country tries to put some law forth,
it's particularly bad for a multinational company, the multinational company says,
I'll move headquarters to another country that doesn't do that, I'll give them the taxes rather
than you guys, and we will, you know, support whoever is campaigning against you and we'll
put a shit ton of lobbyists on changing the law. This is all the critique of public choice theory
is that the incentive of the agents in the system doesn't align with the well being of the whole.
And the government is mediated by agents in the system, and we don't have the right coordination
dynamics. So each actor doing kind of a utility maximization function, each actor doing what makes
most sense for them in the near term rationally creates a maximally stupid hole because of the
misalignment in agency of the various actors and the inability to coordinate effectively across them.
And this gets worse with the more we can corrupt accounting, the more we can actually hide that
these things are occurring. And the larger the system is, the easier it is to do that because
who can actually monitor all of the things that are happening in the system. Actually,
a lot of this was prefigured and talked about in a very underappreciated book by Mancer Olson
called The Logic of Collective Action. I don't know if you ever read that, but if you haven't,
I would strongly recommend it. He's more well known for I think it's the rise and decline of
nations, but the deeper book is The Logic of Collective Action, where he lays out the fact
that strong small groups that have a strong interest in an issue are very likely to dominate against
a much broader community who have small levels of skin in the game. And it's a pretty strong
critique and it does exactly public choice, but it's close. So let's okay, let's deem that.
It's both who has the most incentive will work hardest, and smaller groups can coordinate
better than larger groups. And this is why if I had two groups, two countries say that had an
equal number of people and an equal number of total dollars to begin with, and one of them
tried to create law that bound economic inequality, so the richest person couldn't have more than 10x
more than the poorest person, the group that didn't bind economic inequality would win over the group
that did in any form of warfare if the group was fairly large, because if you end up getting a power
law distribution of wealth where three guys own almost everything and everybody works for them,
the one guy who has almost all the resource can coordinate with himself better than the
million people can coordinate with each other. At least so far. And I think there's an interesting
possibility to think about what comes next. You know, really, all these are coordination and
signaling problems, right? And we are operating on a single or relatively low dimensional signaling
coordination system that has given rise to the system that we have today, what I often
call the money on money return signal, right, molds our world. So presumably, if we're going to get
past this, we have to think about multidimensional signaling and multidimensional coordination.
This is exactly the center of what I'm focused on is that the problem is fundamentally the inability
to coordinate between agents where their basis for agency intrinsically has deltas, right? There's
what's best for me in the current system of a private balance sheet and money and those types
of things, which best for me is not what's best for you and best for the commons, even though
it would be over the long term over the short term, it doesn't seem to be. And so everyone is
doing utility maximization functions, but with again, unlike evolution with asymmetric power
relative to the environment and supply side relative to demand side and things like that.
And this then ends up creating a situation where it gets worse, like this is actually a very important
point. If I have true information about the nature of reality, that is a source of strategic
competitive information. So I want to withhold that information. We call this a trade secret or
classified or confidential information or intellectual property. But I don't just want to
withhold that. I also want to make sure to throw anyone else that would figure it out off the scent
trail. So I want to do not just withholding of information, but disinformation. And we have
a situation where everyone is incented to do withholding of true information and signaling
of disinformation. And then we have information technology that's exponential information
technology where I can do customized disinformation for different persona types and all the way down
to individuals, we get to a world where we stop being able to parse signal from noise because
there's so much radical disinformation. And we have a situation where coordination actually
becomes impossible because of these agency issues everywhere. It's supposed to be the two different
intelligence agencies within a country perfectly coordinate with each other to support that country
because they're all on team country, right? Team USA against the Russians and the Chinese or
whatever. But really, those two different intelligence agencies are also competing against
each other for a larger percentage of the budget. And then even two different departments within
that organization and even two different people competing for the same promotion will withhold
information and maybe even disinform engage in corporate politics. Corporate politics is where
someone's optimizing their own bonus structure and their fealty relationships at the expense of
what's actually good for the whole because they're not actually coupled to the whole effectively.
And so you get a situation of fractal defection, everybody defecting on everyone to some degree
while signaling that they're not doing that. And this basically means a catastrophic breakdown
in the sense making necessary to make good choices while having an exponentially increased
amount of choice making power. And if we think about that exponentially decreasing quality of
sense making relative to the overall situation with exponentially increasing choice making power,
that's another way to think about inevitable collapse. Yeah, certainly recent evolutions
in our information infrastructure have raised even higher the power of bad faith discourse.
And if nothing else, it's very substantially reduced the cost of bad faith discourse, right?
And if we talk about as we often do in our world sense making, the idea of sense making and then
the bigger issue of choice making in a world of predatory disinformation and bad faith discourse
leaves us in a very dangerous situation. Yeah, exactly. That is the central thing for us to solve.
All right, let's stipulate now that there's no way out within the present game. What do you
suggest we do next? Okay, so I would say that the consideration that rival risk games are necessarily
causing harm to some other agent or to the commons, but that when they are multiplied by the
leverage of technology that that harm becomes larger than the ecosystem can handle. If we take
that as one generator function of X risk, we know we have to have a situation that creates
an anti rival risk basis for coordination. That is necessary, but not sufficient. There are a
couple other generator functions of X risk that we also have to address because we can show that
all of the catastrophic risks and this would take longer to do than we have, but we could show that
all the environmental degradation issues, pollution issues, dead zones and ocean ocean
acidification, biodiversity, loss species, loss, climate change, all of those issues,
all of the what would cause World War three or large scale war and all the exponential tech
mediated issues like gray goo or AGI scenarios or biotech scenarios and all the things that would
cause collapse, grid collapse, economic collapse, that all of those have a few generator functions
in common. And we could actually do a construction where we put forward these three generator functions
and prove that that set is actually subsumptive, that there are no risks that are not a result of
those things. So then coming up with a solution to those things is solving for the class of what
creates all of them rather than instances, that becomes the kernel of a new civilization system.
And that we can say is both necessary and sufficient requirements for a non self terminating
civilization. So complicated systems subsuming their complex substrate increase fragility
and evolved systems, not the technology systems. We talked already about evolution technology
being different because of the nature of the metastability of evolved systems, we get anti
fragility. So if I burn a forest, it will regenerate itself. If I cut my body, it will heal itself.
If I damaged my laptop, it won't heal itself. I burned my house down, it won't heal itself.
So humans take the anti fragility of the natural world and turn it into fragile stuff. We turn
it into simple and then complicated stuff. So we turn a tree that's anti fragile and complex
into a two by four that is simple and then a house that is complicated, but both fragile.
But we don't stop at a certain place, we basically have complicated systems
subsume the complex system. So we're creating an increasingly higher fragility to anti fragility
ratio that we're then trying to run exponentially more energy through an exponentially increasingly
fragile system. And this is a way of thinking about what Tainter said in the collapse of
complex societies, which is the relationship between the complex and the complicated as we
continue to grow the complicatedness of the scenario ends up breaking down. And so it's
important to know humans only know how to build complicated stuff, we don't actually know how
to build complex stuff. And we also don't know how to limit our growth. That's what I was saying
whenever there's an increase in efficiency, we just exploit more stuff. So this is another kind
of generator function is that we both have to learn how to build stuff that is either anti
fragile itself or in a fundamentally different kind of well, to some degree, we have to build
stuff that's more anti fragile, and we have to not exploit all exploitable areas. And so this is
really different than everything we have ever done. This is kind of one way of speaking about it.
The other thing that we can say is a generator function of X risk is that it's much easier to
break stuff than it is to build new stuff just from a kind of entropy or thermodynamic perspective.
Second law thermodynamics, the one law that you can't repeal.
Right. And so from a information theoretic perspective on the second law, one way I would
say it is the problem I'm interested in solving is that the way humans solve problems tends to
create worse problems. So whether we're talking about us coming up with a technological solution
or a government solution or a kind of social ideology solution or an economic solution for the
solution to solve the problem, that means it overtakes the problem, the solution has to be
larger, faster, somehow bigger than the problem was. But the solution typically is to solve a very
narrowly defined problem. So we're defining a solving one metric or two or three metrics. And
yet it's going to interact with complex systems that affect lots of other metrics where it will
end up having harm externality, but they will be larger than the original thing. So the plow
solved the problem of local famines, but ended up causing desertification and species extinction,
and all these things writ large globally, right? The internal combustion engine solved the problem
of too much horseshit in the cities and the difficulty of horses, but climate change and
oil spills and wars over oil and the destabilization of the Middle East are all the unintended
externalities of the internal combustion engine. And we can see the same for like the value of
Facebook then compared to the unintended externalities it created or Twitter or whatever it is. And so
in general, it's because I can define a problem in a narrow way, but that's actually not the problem,
right? That's a little part of it. And this is the same with biotech, right? Which is, I can say
the problem is one biometric that I'm trying to address LDL or whatever it is. And I can give
something that lowers that, but it might also do a bunch of other things that are negative,
which are the side effects of that thing in the overall system, which is why that approach is not
a really good approach to medicine. And so to formalize this even further, what we can say is
that the information and the computation, the information processing that it takes to come
up with a new piece of tech is orders of magnitude less than the information processing it takes to
ensure that that tech won't have any externality in its long-term application. And it's not just
orders of magnitude. We can say that the safety analysis is going to end up being NP-hard relative
to the work that it takes to come up with the tech being expressible as a polynomial.
Yes, that's absolutely right. One of the things I've learned in my 17 years of rolling around in
complex system science is the truth of the matter is the ability to project the future evolution of
a complex system is way less than most people think if only for the very simple reason of
dependency on initial conditions. We know from the very simple Lorenzi and Strange Attractors
that surprisingly simple systems are effectively impossible to predict because they're highly
dependent on very small differences in initial conditions. And when you add into that strategic
