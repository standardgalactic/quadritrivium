Hello everyone and welcome to ML Don. So today we are having an interview with Professor
Carl Friston. Professor Friston is a theoretical neuroscientist and a world renowned authority
on brain imaging. He has invented statistical parametric mapping or SPM, which is a software
package designed for the analysis of brain imaging data sequences. And also, I think
his main contribution to theoretical neurobiology is the free energy principle for action and
perception that hopefully will get the time to also explore. Professor Friston, thank
you so much for being with us today. My pleasure. Okay, so let's just get started. The free
energy principle, right? Could you just tell us about what the free energy principle is
and how has it enriched our understanding of the brain as a unifying brain theory, if
you will? Right. So I normally respond to that question by asking whether you want the
sort of the high road or the low road approach to understanding what the free energy principle
is about. I'll very briefly try to surmise both. So the low road, which is like a sort
of bottom up way of understanding it, which is a kind of way that I think of a neuroscientist
and psychologist who'd understand things or more broadly, people in neurobiology would
be the denouement at the present time of very old and compelling ideas about the brain as
literally a fantastic organ, an organ that constructs hypotheses, explanations, fantasies
that are for the best explanation for all the sensory impressions, all the sensory data
that we have to assimilate and literally make sense of. So the principles that underwrite
that perspective were articulated in various disciplines by people like Kant and Helmholtz,
people like Richard Gregory, narcissists by synthesis, perceptions, hypothesis making,
leverage to great effect in machine learning with people like Geoffrey Hinton and Peter
Diane and the notion of the Helmholtz machine, in turn, borrowing technical ideas from the
physics and engineering literature such as Feynman's Free Energy Principle as a way of writing
down a prescription or a normative account of this kind of sense making. And you end up with
a formal first principle account of sense making, which you can write down as a computer
program or you can write down mathematically, but essentially casts the brain as in the service
of trying to minimize its variational free energy. And you can read variational free energy here,
essentially as a kind of prediction error. Technically, the free energy gradients are
prediction errors. So what's a prediction error? Well, it's just the difference between
what given your beliefs about the current state of affairs out there beyond the skull bound brain.
What would I expect to see? What would I predict? And then I compare my predictions with what I'm
actually sensing and that constitutes a prediction error. Then I use the prediction error to drive
my beliefs in a way that eliminates that prediction error. So I've got a sufficient account of what's
going on. And that's often referred to as predictive processing in its inactive form or
active inference. And when we're drilling down just on the perceptual synthesis that is afforded by
this account, the most popular schema would be predictive coding, sort of the reciprocal exchange
of top down predictions from the inside out to the parts of the brain dealing with sensory processing,
but is complemented by an ascending outside in flow of prediction errors, where you need the
predictions to form the prediction errors, and then the predictions or the expectations
that are generating those predictions are informed and updated by the prediction errors.
Technically, that's a process called Bayesian belief updating, hence the Bayesian brain and the
Helmholtz machine formulations of it. So that would be an account of the free energy principle,
really as a prescription, an algorithm, a theological account of a normative theory for
the brain as an active organ, making sense of data, with the key twist that of course the brain,
or you and I, are in charge of the data that we're trying to make sense of. There's an added
inactivist twist here, it's not just that we're good Bayesian filters or data assimilators,
we actively have to decide what kind of data we want to base our inferences upon,
literally in terms of where I'm going to look next, how I visually palpate the world,
or which news channels to listen to, or which Wikipedia page I'm going to forage next.
So that's possibly more of a difficult problem the brain has to solve than just simply making
sense of the data, but both are bound up in this sort of generic account in terms of free energy
minimization, or the explaining away and accounting for providing the best explanation
that eliminates prediction errors. Very briefly, the high road is the kind of approach
that a physicist would take, and you can derive exactly the same mechanics of belief updating,
the same sort of explanation for sentient behavior, where behavior is important because
that's the active part. Just from an analysis of things that exist in the sense of physics,
and the particular physics here is the physics of non-equilibrium steady state
systems that have the remarkable property of returning to states that they were once in,
mathematically have attracting sets or characteristic states of being, that I
am found at this temperature in this kind of location and this sort of environment.
So all the things that would characterize me represent states that I keep on revisiting,
and that can be written down mathematically in terms of the dynamical systems theory of
attracting sets. You can work out the probability distributions, or you can appeal to Feynman's
work on pathological formulations to work out, well, if things exist, then what properties,
what dynamics must they have been, must they possess in order to exist, and to maintain
themselves within these viable states of being. So in life sciences, this would be known as
auto-poesis or self-creation in the computational chemistry of the self-assembly, how the molecules
assemble themselves and retain their sort of morphology and structural or configurational
organization. And it turns out that you end up with exactly the same equations that you're
trying to, everything that we do, either on the inside or in terms of acting upon the world,
is in the service of minimizing this variational proxy for prediction error,
but also mathematically happens to be the quantity that statisticians would try to
optimize, namely the marginal likelihood or the evidence for models of the world. So you get this
sort of tenological account that systems that self-organize look as if they're trying to maximize
the evidence for their models of the world, and sometimes people refer to that as self-evidencing.
So that would be a philosophical account of the top-down physicist's explanation for the way that
you and I actively organize our exchanges with the lived world in order to exist and maintain our
existence, some generalized homeostasis that can be described mathematically as a minimization
of variational free energy. Okay, very, very interesting. So as far as I understand,
so it's about, as you said, we're trying to sort of do actions that would maximize the
probability of our existence, of the model that we have from the environment that we're living in,
which is, I think, the same thing that you refer to as marginal likelihood,
the probability of observation given the model that I currently have of the world.
What I'm trying to understand is trying to marry that idea with the other idea that
the brain selects which one of these inputs to consider how to affect the environment,
because when we say the probability of my input from my receptors given the model I have
from the environment, it is as if I have a control of what I will necessarily do,
because I mean there are so many receptions coming, I might choose to ignore some of them,
but then it might not be the right choice. Also, when you talk about optimization,
I realize that now, if I look at this from a machine learning perspective,
since we're optimizing, could we overfit and fail to generalize?
Well, that's a great question. There are about four great questions in there.
Oh, sorry, sorry. No, no, no, no. Pick one.
Full attention. First of all, I am sorry, I will use technical terms with very little
theory of mine, because I'm so used to doing so, so you're absolutely right. The marginal
likelihood is nothing magical. It's just the probability that these sensory data would be
solicited, experienced under my model of how those data were generated. You're absolutely
right. The marginal bit comes from marginalizing out or averaging over the unknown parameters
of a generative model, but we don't need to worry about that. The model evidence is the
marginal likelihood and the logarithm of that is the negative free energy.
But your more interesting question. I mean, if you now review this podcast and just listen
to the words you use, they're very telling. You're talking about selection. That's a great
word to use. You also said I. So immediately, there's an inactive, a genital aspect to your
question. So a predictive coding scheme that's just assimilating data and making sense of data
has no notion of me, has no selfhood, and certainly doesn't have any, it is not equipped with the
ability to choose and to select which data it wants to actually assimilate or to classify or
to categorize. So we're going beyond machine learning now. We are going beyond simply passively
in a sort of outside in way, making sense of data. We are now asking about this really deep
question. How on earth do we select the data to assimilate? So the answer is very simple.
In the same spirit that everything on the inside that's doing this assimilation
has to minimize a prediction error, sometimes cast in terms of surprise in a technical sense.
So this is if you're an information theoretician, this would be the self-information.
If you're doing that, then the natural question then arises. Well, look, what happens if I'm in
control of the data that I can now select or turn towards? Well, now I have to choose the
actions that will produce data that have the minimum expected surprise or the minimum expected
prediction error. So if surprise, which is another word for mathematical expression for this bound
on surprise or evidence called variation free energy is the thing in the moment,
then after I have acted in the future, then what I want to do is to minimize my expected surprise,
my expected prediction error. So what's that? Well, expected surprises is technically entropy.
And more anthropomorphically, this would be uncertainty. So what you're doing is if you're
subscribing to the free energy principle, and you now have to choose the best kind of act or
sampling or selection, then you would select those actions that minimize your uncertainty,
the resolve your uncertainty. Another way of exercising that mathematically is that they
have the greatest information gain that those kinds of data resolve my uncertainty more than
those kinds of data. So that's a actually a well known Bayesian principle. And it's
often cast in terms of optimal Bayesian design and has been known since the 1950s through the work
of people like Lindley. There are well defined objective functions, which are simply how much
do I shrink my uncertainty and technically measured in terms of a divergence mathematically,
but you know, we just need to know the information gain is just the degree to which I shrink my
uncertainty about states of affairs out there. So what that means is that we are, if we minimize,
if we choose those actions, or we want to minimize our free energy, then we have to choose those
actions and minimize the expected free energy, which necessarily makes us into curious creatures.
Now you were saying to me, I don't know how to do that. I don't understand how to do that. You do,
you're a scientist. Even if you don't have a PhD, every every child and every student and every
adult is a curious scientist. They want to know how their world works. Either it's your scientific
field of study, or if you're a little baby, it's just how your body works. What kind of thing is
mum? Is mum the same kind of thing as me? All of this information has to be carefully selected
in order to resolve uncertainty about this hypothesis or that hypothesis in exactly the same way
that you design a good experiment. So when you design an experiment, then you design the experiment
to yield data that are going to be maximally informative in relation to your null versus
altered hypothesis. So it's exactly the same principle governs the selection of natural
experiments that we deploy with our eyes, looking over there or looking over there. It's exactly
the same mathematics as the same imperatives. And indeed, you actually see the same information
theoretic constructs emerging in the visual search literature in the neurosciences.
You can construe these as savings maps that they score the uncertainty reduction, the epistemic
affordance of selecting those data by looking over there or by again looking at this Wikipedia
page or listening to this news channel and not that fake news channel. So it's a great question
because not only does it make I think the really profound point that the curiosity of all kinds
is baked into the first principle account of self-organization, that anything that exists
must in part be acting or appear to act. Look as if it is acting to resolve uncertainty about the
exchanges with its eco and its environment, its culture or the visual scene at hand.
Furthermore, because you're talking about action, it's you that's acting. So there's an implicit
sense of agency that is owned by the artifacting question. So we're now talking about something
necessary to equip an agent with some very elemental self-hood. The third thing at the table
is really commonsensical, but it's a very interesting observation and that in order
to select this action over that action, then I have to have a model of the consequences of my
action. But the consequences live in the future, which says that your genetic models now must
cover the future, which means that you've got something with a lot of things in this universe
don't have, which is a model of the future. It has temporal depth. So you could apply the free
energy principle to a thermostat. You could apply it to a virus and it would work perfectly well.
You could simulate thermostats and indeed make a thermostat or simulate a virus or possibly even
construct one. But these kind of the genetic models that you would use that has to be in place in
order to work out the surprise due to the predictions generated by that model would not have the
temporal depth of the models that you and I have. So as soon as you talk about selection,
you're talking about essentially selecting a plan of action, a course of action, and that means
necessarily rolling out into the future. So your genetic models have this temporal depth.
And furthermore, as soon as you entertain the notion that I could do this or that,
then there is an active selection in play. The thermostat doesn't have that. There's only one
thing it can do because it acts in the moment. But you and I have a much more deeper generative
model we bring to the table that allows us now to set amongst a series of counterfactual
courses of action. You can choose what to say next or not to say anything at all. And in that choice,
there is an act of Bayesian, technically an act of Bayesian model selection or policy selection.
So now you've got thermostats that select provided they have these deep generative models that
sort of free you from the moment because they're really about trajectories and paths,
how we navigate the lived world. And about the overfitting thing, could we overfit?
Or actually, why don't we overfit? Maybe that's the right question.
Yeah, that was the fourth clever part of your question. The reason you don't overfit and I'll
probably, if you indulge me, I'll answer this from the point of view of sort of machine learning
or statistics. So the answer here lies in an understanding of the nature of
models and their evidence. So the first thing to say is that there is no true model of the
world. There is a best model in the world and that's simply the model that has the greatest
evidence, but there's no true model in the world. There are many, many ways of explaining
how some data for sensory data was generated and providing an account of that. All we can do
is find the one that has the greatest evidence, the greatest marginal likelihood,
probability of the data under that model. So we can change the model until the model
provides or account gives you the greatest marginal likelihood. So what is evidence? This
is the key point. Evidence is accuracy minus complexity, which means that to find the model
with the greatest evidence is to provide an account of these data in the simplest way possible.
And that simplicity underwrites the ability to generalize. In the absence of that complexity
part of the model evidence, you would certainly overfit. So if you were allowed to use as many
degrees of freedom as you wanted with no constraints on that, i.e. no complexity constraints,
you would certainly overfit these data so that the next day's data would not be expelable,
because you'd fit all the little random effects and fluctuations in the data at hand.
So that complexity term is absolutely crucial in ensuring that maximizing model evidence,
minimizing surprise, minimizing free energy, all of these are statements essentially of the same
thing, goes hand in hand with an automatic penalty on the complexity that precludes
overfitting. And it's really interesting just to sort of unpack what complexity is here. So
on a technical account, the complexity is basically the degree to which I change my
mind having seen some data. So technically it's the difference between some posterior
Bayesian belief, some probabilistic explanation for some causes of some data
machine learning, this might be called some latent states generating
unobservable latent states, hidden states that are generating observable measurements or
observations or sensory input from our perspective. So I've got two beliefs before we're talking about
this process of dynamically reducing free energy or minimizing prediction error as a process of
belief updating. That belief updating is literally updating a prior belief before I see data into
a posterior belief after I've seen a posteriori after I've seen the data. And the degree to which
you have to move your belief from the prior to the posterior is a complexity. So it's the degrees
of freedom that you're using up in explaining the data. Now if you've got really good prior beliefs,
you don't have to change your mind very much and therefore you can provide an accurate account of
this data with a minimal complexity cost because your belief updating has been extremely efficient
and minimally complex. So it is those kinds of high evidence generative models that have this
complexity penalty in place that are able to generalize to new data because you've got the
right kind of priors. So it all goes hand in hand and it's a really important point. I don't know,
I'm wondering whether you knew this already in virtue of the fact you asked the question but
this issue is really a difficult issue in conventional machine learning because if you
just look at the way the field has gone in machine learning, they've gone to sort of big data,
sort of committing to the ideology of big data science with massively overparameterized neural
networks. So if you read a neural network as a generative model, certainly in the context
of an autoencoder, then what you're starting with is an overly parameterized, overly complex
generative model which can beautifully explain any given data set that is hopeless in generalizing
to another context or some new data simply because the complexity is too high, because there are too
many parameters in that model that generally say connection weights in a current neural network
or a convolutional neural network or a variational autoencoder. So what that leads to then is
all sorts of problems with sharp minima that ensue because you have not put in the complexity term
that you can think of in terms of this Occam's principle. So Occam's principle has to get into
the objective function. So the game now would be to take a neural network in machine learning,
same deep learning, and try and find out which connection weights or what architectural aspects
of your network you didn't need to provide an accurate account, which is actually the
opposite direction of travel from most of deep learning at the moment, which are aspiring to
involve parameters with bigger computers. There's an interesting twist there, of course, that there
are a number of fundamental principles, such as Landauer's principle, the Jauzynski equality,
which effectively say that information is energy and energy is information. And in this present
context, the degree of belief updating as scored by the complexity cost has a thermodynamic cost,
which means if you're recognizing things properly and selecting the right things to do properly
with the minimum of complexity, you're doing it with the greatest thermodynamic efficiency.
So you can measure the quality of any artifact, any neural network or neuronal network, simply by
the amount of energy it has to expend to exist. So again, the drive towards high performance
computing is exactly in the wrong direction. What you're looking for is sort of sentient
edge computing devices based upon an evidence maximization or free energy minimization approach
that should do it like your brain, which would consume the same energy as a light bulb as opposed
to a power station. So that's a really good question that has some really pragmatic engineering,
computer engineering and sort of, you know, energetics associated with it and explains to a
great extent why you're so much better than any neural network that lives on a high performance
computer at the moment. The idea of overfitting and overparameterized models just reminded me
of this paper by Professor Michael Belkin. So it's a paper about this phenomenon
that happens in overparameterized models called the double descend, as they've called it. Now,
their question was why overparameterized models don't overfit and why they can generalize so well
and it's a very interesting paper. It was really shocking to me because we have this famous,
I mean, in machine learning, we have this famous U-shaped sort of plot that shows that, okay,
the more you parameterize your model, the error in the predictions of your model keeps going down
and down and down and after a certain point, it fails to generalize. So it just keeps going
higher and higher and higher. Now, what they did was they didn't stop there. They kept adding more
parameters. They kept adding more and more. Now, what happened then was after a certain point,
we have a double descent, a second descent in the generalization ability or in the error,
let's say, the prediction error, it again keeps going down and down and down. In some cases,
it goes even below the point where the minimum of the first descent. It could even get to a
better generalization and in some cases, no, the minimum that it could ever reach would not be as
good as the minimum of the first descent. So they've called this double descent and I'm trying to,
and didn't happen, the double descent always happened based on their experience. They use it
with decision trees. They use it with neural networks and a couple more models. I don't remember
exactly what they were, but the notion of accuracy minus complexity and that being the idea of
that being the reason as to why we don't overfit. I'm trying to sort of have that next to the idea
because their argument was that when you have too many number of parameters,
the model sort of goes towards like you have more knobs to control your search in the hypothesis
space. You inevitably go to, as if you can, you can search better and then you find,
maybe I think they were arguing that you find simpler solutions, I think, if I'm not mistaken.
But how would you marry that idea with this accuracy minus complexity being the reason of
not overfitting? Right, that's a great question. Okay, so I think if you think of this, the problem
in terms of finding minima in an objective functional landscape, then that phenomenon of double
descent makes entire sense in the context of escaping local minima. So let's assume that we've
basically got a difficult problem, say a brittle, dynamical system or a highly nonlinear problem,
where the objective function, let's assume it's either a free energy objective function
or some approximation to it, then it's likely you're going to have lots of local minima around.
One instance and one particularly problematic aspect, probably you have in mind is this notion
of sharp minima, minima that have very steep sides that are very difficult to escape from with any
sort of stochastic annealing or any sort of device that would sort of bring you out of that base
of attraction of that sharp minima. So the first point to make before I pursue that analogy and
really basically just reiterate the solution that you've actually just described, which is
absolutely great. I'll do it from a dynamical perspective. The first point to make is that
the failure to put the complexity term into the objective functions used by typically by
lots of vanilla machine learning, not all, I mean high-end, say, variational autoencoders
use exactly the variational free energy, which is in machine learning there is an evidence
level, that is exactly the same objective function we're talking about, but those
algorithms that don't do that and just try to maximize the likelihood as opposed to the marginal
likelihood. So, you know, just try to maximize the accuracy as scored by the sum of square residuals,
for example. So they're forgetting about the complexity and they're just drawing down the
accuracy part of it to create these sharp minima. The reason that these minima are sharp is that
the complexity term creates it into a U-shaped minima. This is, if you like, one expression
of Occam's principle, that the good explanations have to allow for some latitude. So the curvature
of the minima scores the latitude you have in the sense that you're not committing to a very
particular explanation that provides an accurate account of these data. There are a number of
understanding this. One of them is, in fact, from the point of view of Jesus' maximamentary principle,
that under constraints afforded by, say, a likelihood and some prize on the charity model,
the best explanation is always the belief that has the maximum entropy, which means
it has the minimum curvature in some free energy landscape. So the free energy functional provides
an objective function where all the minima are flat, thereby eluding the sharp minima
problem that plagues most conventional machine learning. So I think that's the first thing
to recognize that you will find those free energy minima more efficiently provided you can escape
the local minima, simply because the free energy minima are always flat and they're less likely
to trap you. So let's now take what other ways could you escape sharp minima that are local
minima that are high in a free energy landscape. So just one sort of picture I use for my students
is very much like a mountain range that is the genesis of a little stream that turns into a river.
So the lower you get in the landscape, the wider the rivers, the shallower the valleys until
ultimately you get to an estuary. And so exactly the same sort of, if you like,
landscape exists in a free energy or very short free energy context. So stuff high up,
all the sharp minima that have a very undesirable high free energy are the rough stuff in the
higher mountains. And your job is to get down, flow down to the nice shallow global minima
that have this sort of maximum entropy Occam's principle shallowness to them with low curvature.
So how might you want to get out of those high hanging valleys, those sharp minima?
Well, you've said it basically. If you imagine from the point of view of dynamical systems theory
flows on a manifold or a, in this instance, a free energy landscape. Let's imagine we just
got one parameter to play with. So we've found ourselves in a V shaped minima. And we can't
get out of it. But if I add another dimension and now make my one dimensional objective function
two dimensional, then I can change that fixed point to tractor that corresponds to the point
of the minimum, the local minimum in question into a subtle point into an unstable fixed point.
And I can escape from it. So now I got a subtle point where there are now roots away from that
local minima. Because as you keep on adding dimensions, i.e. adding parameters to your model,
the opportunities for converting a stable fixed point, which is a local minima
into an unstable fixed point from which you can escape increases geometrically or at least
superly. So I think that's the explanation in conversation with people in machine learning.
And I joined one such conversation with people at NYU just a couple of weeks ago. I think that
that is probably the story that you would tell to account for this sort of throwing lots and lots
of parameters at a model just to destroy stable fixed points that constitute local minima so that
you can access the global minima. And then once you've done that, you then crew away all the parameters
that you use as your escape routes to get back to the simple model. So what we're talking about,
I think this double descent thing is a really lovely notion. And it does remind me a little bit
of how the brain works in the sense that it starts off with an overly parametrized
geritim model. So babies and children actually have more connections, they have more W's,
more external connections, white matter tracks than adolescents and then you and I do.
So you get this progressive pruning as you grow older. So you start off with this overly
parametrized, overly complex, virgin tabla rasa where you can escape all the local minima.
And then as time goes on, you then do your pruning and reduce the number of parameters or
connection weights in your model to simplify it to the in accord with Jason's Max Ventura principle
outcomes principle from the point of view of the free energy minimization. It's the complexity
part of the minimization. So you now get a smaller parameter model, but you've only done
the second wave of descent, if you like, after you've escaped all the local minima by having an
overly parametrized model. So I'm sure that this must be a universal principle that I suspect
is so universal, it has been discovered by evolution millions of years ago.
Yeah. Yeah. That was, that was amazing. Wow. That was a totally different perspective
that the, I mean, the reduction in the number of parameters as you grow old. I never thought
about that. First of all, I didn't know that, but it was, it was really amazing.
Yeah. Well, it has its penalties because I am much older than you and therefore I'm much sparser
and wiser than you, but I could not adapt to a new world. So I can't go, why don't you jumpy
or go to disco. Oh my God. Wow. Wow. Wow. Mind-boggling. I don't know what to say. That was,
that was really enlightening, actually. So it seems to me that there are two methods of learning
in general when it comes to biological systems. So I did a little bit of research on this.
I don't know whether I'm right or not, but I could narrow this down to two general methods.
One is perceptual inference, where, and the other one is through action inference. Now,
the perceptual inference, I suppose, is when you generate a model from the world and you keep
updating the parameters, whereas the action one is where you actually choose actions.
So I know we've discussed this a little bit at the beginning of the interview, but
could you explain these two methods in more detail and compare them as to
in what situation am I learning through perceptual inference and in what situation
do I choose to learn through action inference?
Right. Yeah, he does, I think, speak to this issue of sort of machines with agency that we're talking
about before, artifacts that have sentient behavior. So the sentient part would be the
perceptual inference and the behavior would be the active inference. I think that they,
you know, once you think about sort of artifacts that do actively exchange with the world, I think
they both have to go hand in hand. So I think it's, although another excellent question, I will
qualify my answer after I've given you the vanilla answer, which is, you know, this is just a joint
optimization process over two kinds of unknowns, two kinds of latent states out there that generate
data. One kind of latent state is the latent states over which I have no control. So this
would be the standard data assimilation classification problem, recognition problem.
I am given some data and I have to classify, learn its causal structure infer the particular state
generating, latent state generating these data in this particular context. So that would be
perceptual synthesis, perceptual inference, learning the parameters of that judging model,
I would call perceptual learning. So in my world, I distinguish between fast processes that infer
time dependent states and slow processes that infer the parameters that underlie the contingencies
and the laws. We would be learning in the machine learning sense of the W's in a neural network,
but the states of the network at any one point in time out interpreters inference. You know,
this is the activity of these nodes and therefore I am seeing this kind of face.
So that's one set of unknowns, but there's another hugely important set of unknowns,
latent states out there, those which I can control, those which I have some agency over.
So if I make this movement or I secrete this or I switch on this knob, then the states out there
will also change. If that's the case, then you're now conditioning a certain set of hidden states
on latent states which can be thought of as plans or control variables of the U in a
sort of a standard machine learning treatment. So these control variables are, say,
control theory now play a really important role because the transitions are dynamics amongst the
latent states that are controllable and are conditioned upon this U, but this U is another
random variable. This means I have to infer two sets of random variables. I have to infer
the latent states over which I have no control and I also have to infer the control variables,
if you like, the plans upon which the variables, the states that I do have control over are conditioned.
So there's two kinds of inference here which you can think of in terms of the distinction
between somebody just applying a Kalman filter to estimate the states of some plant versus
estimating the best thing to do if you were doing applying control theory and, you know,
when you're controlling a particular plant, say, in engineering. But because we're now
committed to understanding everything in terms of optimizing a free energy functional of beliefs
or probability distributions, that now becomes that planning or control theoretic part of the
problem, the active inference part of the problem, now becomes planning as inference. So, you know,
in a sense, you can read the two perceptual and active sides of inference as really the same process
working hand in hand, in parallel. But acknowledging there are two different kinds of variables
that you have to infer, you have to work out, basically, what's the state of affairs out there
and what am I doing about it? And then, of course, you can see now that the active inference side,
the planning as inference, now has to consider a number of plausible plans and then we get back
to selecting the most likely plan which minimizes the expected free energy or
resolves which is uncertainty or avoids surprising outcomes like, you know, your machine crashing
if you're in an autonomous driving situation. So, you can write in all your prior preferences
into that expected free energy in the future conditioned upon the plan.
So, I would say that there is no choice to do active inference means that you have to infer
both the latent states of the world and the way that you're intervening on those states
through controlling state transitions, that just means planning as inference. That's part and parcel
of active inference and just being existing in a world that you're continually sampling
and it's continually generating data for you to make sense of, you know, there's a circular
causality there. The twist here, the qualification, is that I think you're absolutely right that in
we do switch between active and perceptual inference in reality. So, if I was building an
artifact, I probably wouldn't worry about what I'm about to say, but if I wanted to describe
you and me, I would describe you and me as basically machines that are built to,
in a solitary way, in an intermittent way, alternatively, plan an action and execute it
and then gather some data and then make perceptual inferences or, you know,
do the perceptual learning side of things. So, I mean this in the sense that the way that we,
the temporal scheduling and the alternation between the action and the perception or the
active inference and the perceptual inference, the way that it seems to be scheduled and
organized in biological artifacts, not like ourselves, is that every 250 milliseconds,
about four times a second, we act and within those actions, we do our perceptual synthesis.
So, this, you'll see this, for example, every 250 milliseconds, about four times a second,
you'll move your eyes with stochastic eye movements. So, you're getting little snapshots
every 250 milliseconds at four hertz and during the acquisition, during the action,
you actually switch off your sensory channels, it's called stochastic expression.
If you're an engineer, you're basically turning the Kalman gain down to zero
when you're doing the action bit. So, you don't see the optical flow induced by your eye movements,
you just don't see that. What you see is the thing that you've actually fixated on and you
use that to construct a scene, you assimilate, you build up this illusion that there's a visual
scene out there, but in fact, in reality, there are little snapshots from here and here at about
four hertz. The way that you speak, if you think about, or just listen to me, the way that you
hear things, well, that's not quite so true, but certainly the way that you
fulfill your proprioceptive predictions during speech, all my phonemes are roughly 250 milliseconds
long. So, I am just producing a discrete sequence and little packages of phonemes
every 250 milliseconds. If I was a mouse, I would sample my world by whisking my whiskers
and the frequency of that is 250 milliseconds or four hertz. And if you now look at the belief
updating on the inside, which would correspond to the perceptual inference making sense of
the data sampled by my whisking around my burrow, you see again this profound theta rhythm that
dominates an active exploration of the environment, that within it is nested all the fast perceptual
belief updating usually at what's called a gamma frequency, about 40 hertz. So, you've got this
lovely separation of time scales where we make a move on the world four times a second. We get
some data and then we quickly process what that means. When we've reached a conclusion, then we
go off and get the next bit of data and then we process this. So, for artifacts like you and me,
there is actually, I think, a temporal separation of the active part and the perceptual part of
inference. So, I'm waiting to see until they build that into robots because I think you get very
realistic behaviors and the way that people respond to things and orientate to things or
understand things through this sort of discretization or quantization of discrete packets of information
that live within, say, a few hundred milliseconds and takes the temporal scheduling of at least
biological belief updating. Wow, unbelievable. That was amazing. So, it feels like there is no
parallelization. There's actually very fast switching between the two, like a joint optimization.
As I say, if I was building a computer, I wouldn't do that. Obviously,
there's a better way of doing it and evolution has found that this fast switching is the best way to
do it. To explain the kinds of worlds in which we live in, where biological motion has certain
time constants, where meteorological events or things around us change to a certain time scale,
it's clearly the most efficient way according to evolution anyway. Oh, yeah.
Absolutely. Wow, that was amazing. So, we've kept talking about active inference and
of all the actions that I could choose, I'm choosing the one that on average would maximize
or say minimize the surprise in the future or minimize free energy.
I'm trying to think as to, so first of all, it requires, I suppose, I don't know whether the
word is, my wording is right or wrong, but some sort of a knowledge about the future, as you said,
it requires looking to the future. I can't help but notice that there was a similarity between
active inference and reinforcement learning. However, in reinforcement learning, what we do
is we have the ultimate, like we have an end goal, an end state, and we just let the machine
do whatever it wants to do, like exploration, exploitation, whatever it wants to do,
many, many, many times. So, eventually, it has a way to back propagate the expected reward to
the current state, but a human being that say it's experiencing something for the first time
and wants to choose the action, it doesn't have that option, right? So, first of all,
how would you differentiate between active inference and reinforcement learning?
And second of all, let's just discuss this, I forgot the second question, let's just discuss
this first, the difference between active inference and reinforcement learning, yeah.
But I relieve because that's a very big question. Feel free to prepare the next one while we're
dealing with this. Okay, perfect. So, there are a number of ways of answering that question and
the simplest way, of course, is to look at what is on the tin when you pick up active inference
versus reinforcement learning. One's about inference and one is about learning. So, active
inference has an inference and learning, which has the important consequence that you have
beliefs not just about the states of the world, but also beliefs about the parameters of your
geratin model. So, it comes equipped with beliefs about the W's in say a neural network.
Reinforcement learning is just about learning the parameters. You could look at reinforcement
learning as a mechanism to enable machines to learn to infer and in part, I think that's how
people often understand inference in the context of machine learning. But strictly speaking,
the optimization process is applied to the weights, the parameters of a geratin model,
the contingencies. So, there's one fundamental difference between reinforcement learning
and active inference in the context of belief updating. I don't think that's quite the spirit
of your question though. So, I think another way of looking at the difference between active inference
and reinforcement learning is just in terms of the teleology and asking what is it that you
want to optimize. So, reinforcement learning, you assume the existence of a reward function,
a loss function, a cost function, a value function, and then you optimize your neural
network or your artifact to respond to any given either explicit state or inferred state
with a particular action and that action is chosen by the Bellman optimality principle
to maximize the expected reward. So, that, please suppose, is the existence of a value function
and if that exists then by the Bellman optimality principle you can then
evince an optimal state action policy. Can that be, if you like, framed in terms of active
inference? The answer is yes but as a very, very special case. So, what is that special case?
Well, it's a special case where you can reduce the function that is implicit in active inference,
which is a functional of beliefs. So, remember your planning as inference,
belief updating in terms of perceptual inference and data assimilation, evidence accumulation,
and all that good stuff, Kalman filtering, and the action selection by planning as inference.
This is inference. So, the objective function is a function, a function of a function or a
functional and that function is a belief. So, the objective function in active inference
is belief-based. So, under what conditions could you actually reduce an active inference
formulation of a problem to the kind of problem the Bellman optimality principle would apply,
was when you shrink your belief down to have zero uncertainty. So, if you take uncertainty off the
table then you can get back to effectively the situations in which you can apply the Bellman
optimality principle. Interestingly, just for your entertainment, the way that you do that
technically is just by describing your reward as a log prior probability of some outcome that you
prefer. So, we often call it a prior preference. So, we say that every outcome is equipped with
a reward or a loss function. So, it's sort of having two separate channels, sensory information
reinforcement information. We simplify the problem and say no, every output, every observation
or sensory input comes labeled with a reward in aspect. So, your reward function now becomes
very high dimensional and continuous and it's just basically the log probability of a priori
means something, that outcome. And once you write that down and you take away uncertainty,
then you can apply the Bellman optimality principle. So, you can look at reinforcement learning as a
special case of active inference when there's no uncertainty. The problem with doing that though
is if there's no uncertainty, that means that the curiosity part of the objective, the expected
free energy is zero, which means that you've lost the opportunity now to simulate artificial
curiosity and exactly the kind of sentient behavior that we were talking about half an hour ago.
Because there is no information to be gained, there is no information seeking at hand,
because you know everything. Why do you know everything? Well, because in my model of the
world, in my reinforcement learning model of the world, there is no uncertainty. I know everything.
I can see all the hidden states or at least I can infer them. So, what you lose by converting
an objective function, which is a functional of beliefs, which is the
variational free energy or the marginal likelihood or the model evidence, all of these
are usually logarithms of or KL divergences or free energy functionals of probability
distributions, beliefs. If you're now trying to coerce that into a function of states as opposed
to beliefs about states, you take away all the epistemics, you preclude any proper discussion
or treatment or account of artificial curiosity in your robotics as the intrinsic motivation.
So, you know, the intrinsic value, the value of information, the epistemic value
of making that move. What would happen if I did that? Has its own, if you like, epistemic reward,
but that part of the reward function also, more explicitly, that part of the expected free energy
or the expected evidence or the expected surprise. That part has been removed when you dispense with
or you try to cast the problem in terms of the Bellman optimality principle.
Just a little technical, interesting technical twist here.
When you consider the full problem where you're dealing with
functionals or probability distributions for our spacing beliefs, you are technically dealing with
usually path integrals because, as we've spoken about before, our paths go into the future.
So, the plan has a trajectory or a path into the future. So, now you've got a path integral
of a functional of a long, basically a long probability, which is an energy.
So, you've got a path integral of an energy, which is called an action, and you try to minimize
expected free energy, which is this action. So, all you're saying is that I can explain
sentient behavior with Hamilton's principle of least action. So, now you're replacing the Bellman
optimality principle, which applies to value functions of states with Hamilton's principle
of least action, which deals with, if you like, value functionals of belief states or beliefs
about states. So, that's for me how I would see that there's a fundamental difference between
reinforcement learning and active inference, but it's not a difference that should be overly
celebrated in the sense you can get from one to the other. You can repair that dialect if you like,
as long as you're prepared to take all the reducible uncertainty
out of the game or off the table. Does that kind of answer you?
Yeah, yeah. I'm trying to understand one bit of this. So, when you say that in reinforcement
learning, we actually know there's certainty. In reinforcement learning, sometimes we are not
aware of the world. Like many things could happen that could put obstacles on the path of our agent
that could happen, but then the agent somehow learns to get out of them. I don't get the idea of
not having uncertainty in reinforcement learning. I didn't get that part.
To say that there is no uncertainty in reinforcement learning would be disingenuous.
I think possibly, for me, most easily understood in terms of reinforcement learning
being one application of Bayes' optimality in the context of optimal Bayesian decision making
with a loss function. So, it was really the nature of the loss function I was talking about.
What I was trying to get across was that Bayesian decision theoretic approaches
don't have as part of the loss function any reduction of uncertainty. So, you have to build
that in by hand, so that normality bonuses or exploration bonuses and things. So, you have to
apply some widgets or little tricks to make your loss function even treated under uncertainty in
the Bayesian decision theoretic context, or say partially observe Markov decision process.
You have to actually write in what you get for free if you start with an expected free energy
or expected marginal likelihood. So, it's a kind of uncertainty that matters in terms of what I
believe I was talking about. So, it's a difference between a neural network that's trying to
be on target for as much time as it can in a noisy and certain situation, for example,
and a neural network that knows it doesn't know certain things that would enable it to find out
what particular times it should be aiming for the target or from what particular direction,
because there are certain unknowns out there that it can actually resolve with particular
epistemic moves that have this intrinsic value or this intrinsic motivation
that give it this curious aspect. So, it contextualizes the optimal Bayesian decision theory
exactly by adding in the optimal Bayesian design principle we're talking about before,
which is this sort of information seeking aspect. So, the full picture from my perspective,
perspective of the free energy principle is that you can decompose the objective function
of the expected free energy into two parts, literally linearly, into an epistemic part
and the sort of reward or pragmatic part. And it's the epistemic part that disappears when you
go straight for optimizing the reward function or your priorities. There's an interesting
intermediate case, which you can get at this by rearranging certain terms in the
expected free energy function, which is KL control. So, this would be like risk-sensitive
control. So, I'm not sure whether you put this under reinforcement learning or not.
So, I'd have to ask you, but so from my point of view, this is an interesting middle ground
where you've now included uncertainty about the consequences of my action into the game. So,
instead of now just trying to maximize the expected utility of expected loss function
on the expected reward under some belief about the consequences of action, I'm now
going to minimize the KL divergence or the difference between what I think will happen,
the outcomes that I under my posterior predictive density, technically, but more intuitively,
what I think the outcomes will be if I take this plan or this course of action,
and then I have this set of preferences that encode my preferred outcomes, and then I just
choose the action that minimizes the KL divergence between the two. So, this has uncertainty
in it that is not quite the full uncertainty reduction, but it certainly accommodates a degree
of uncertainty when it comes to real-world applications. Again, I'm thinking about sort
of engineering applications in control theory, where this would be known as KL control,
and economics, it's called risk-sensitive control. So, that would be like a finessed
reinforcement learning value maximizing scheme that's got an uncertainty baked into it.
That's an interesting halfway house between the sort of vanilla expected utility,
which I'm reading as reinforcement learning, possibly unfairly, and the full of active
influence expected for the energy. It's a really great game if you have time in terms of just
writing down the probability distributions and just rearranging them to different kinds
of divergences, and just trying to read them as if you're in a columnist, or you're an engineer,
control theoretician, or you're a behavioral psychologist doing reinforcement learning,
or you're a neuro-roboticist, developmental neuro-roboticist trying to understand intrinsic
motivation. But by switching them around, you can get all sorts of interesting different perspectives
of this overall expected surprise minimization, like objective function.
I see. That was really amazing. So, we talked about perceptual inference and active inference.
Now, both of those ideas highly rely on the environment. One is modeling the environment,
and the other one chooses actions that would maximize the probability of my existence.
Both of them rely on the environment, but what about when we come up with some
abstract knowledge that never existed before? The first time that someone thought about
the Wright Brothers thought about an airplane, or the first time someone thought about a mermaid.
Any time that something, if that's even possible, I don't know, the first time that
something jumps into one's mind that never existed in the environment,
and they try to make it, like, create it into reality.
Can we explain that part, the knowledge generation, with these two ideas,
active inference or perceptual inference?
Yeah. Again, another very challenging question. I would contend that you can. So, I think you're
touching now upon sort of the links between the mechanisms of the underwrite model optimization
and to a certain extent curiosity, but more, I think, the notions of creativity and exploring
different model spaces. So, we're now moving, in fact, actually, completely away from active
inference and perceptual inference in terms of optimizing both the states and parameters of a
generative model as it is exposed to data, and now turning to a third level of optimization,
which would be the optimization of the model in and of itself. So, in various fields, this will
be known as structural learning, for example, in medical and constructivism. The problem of not,
let's say, forget about, let's assume we've got the perfect scheme, if you give me a neural
network or a generative model, and a particular environment that can generate data, I can do
all the good inference or learning to infer actively and do all of that as inference. So,
we've got this little agent now, coupled, exchanging with her environment
in a base optional way. Now, we move to a next level, hierarchically speaking, of an optimization
of free energy minimizing process, which is now not applied to the parameters of the model, but to
the model itself. How many hidden layers? How many connections? Do you use rectified linear or
sigmoid trans, you know, how do you carve things up in terms of not just hierarchical levels, but
in terms of, you know, factorizing or putting modules or conditions at any level, all sorts of
fundamental structural issues that you would practically have to deal with and contend with
when writing down the architecture of your neural network that, you know, will read as the structural
form of your generative model. So, this structural learning problem, and from the point of view of
a statistician is a Bayesian model selection problem. It's basically integrating the marginal
likelihood over all the kinds of data that you could see or all the kinds of data that you have
seen. And then you choose the model that best accounts for, that has the greatest evidence.
So, now this is a categorical act of selection of selecting the best model. And I use those words
because this is mathematically, I think, an apt description of natural selection. So, natural
selection evolution is just nature's way of being Bayesian model selection, just selecting
the hypothesis, namely the phenotype, where evolution thinks that this particular hypothesis
is a good fit to me, the equation that I'm supplying. And therefore, the equation then
selects the phenotype through this process of the structural form of the epigenetically specified,
for example, structure of the geratin model. And as soon as you think of it like that,
there's a big question. And the big question is, well, how do you explore the model space?
Because we've just said, well, you're doing this Bayesian model selection after you've acquired
the data. So, now what kinds of model spaces would you explore? And is it necessary to explore?
There is an argument, which we actually touched upon earlier on, that we can actually start off with
a completely overparameterized model and prune it. So, this is a kind of Bayesian model selection
technically called Bayesian model reduction, where it is literally, if I remove this parameter,
this connection from this model or this hidden layer from this model, would I increase the
pathogen of its evidence? In other words, would I be minimizing complexity without
sacrificing too much accuracy with the kinds of data that this model is fit to explain?
So, you could argue that both the model expansion that typically dealt with
in a Bayesian context through nonparametric Bayes, so by having expandable models and
priors of the way that you would add in different parts or chunks. So, for example,
in a hierarchical Dirichlet process, you might have some sort of stick breaking process on top
of that, which says, is it justified to bringing the new hidden latent variable in my Dirichlet
distribution? And you've asked that question by basically evaluating the evidence for that model
or the Bayesian free energy with and without that extra bit. And if it improves by adding
complexity, which more than pays its way in terms of the accuracy, then you keep it if not,
you wouldn't. But what the stick breaking process brings to the table is a principled way of
expanding or growing the model, I repeat, in the context of nonparametric Bayesian approaches.
But I think you can also, I think that both model expansion, and I'm reading that at the moment in
terms of nonparametric Bayes, or from the point of view of natural selection, having recombinations
of certain sort of codes or parameterizations of geratin models that you can think of in terms of
spit and merge like operations that you get in jetting recombination, ways of exploring a model
space in a principled structured way, or and the Bayesian model reduction, I think both can be seen
as the processes that give rise to creativity and insight and aha moments. So I'm saying that
are using some notion of aha moments and creativity in deliberately, just to make the point that some
creative acts are actually acts of reduction. It's not so much the fact that no one's ever seen this
before. Sorry, no one has ever represented or had this, you know, this object in mind before,
it's just that no one's seen that connection before, and that it's actually a simpler way of
viewing things. So if you just think about the most creative ideas and signs, they are not actually
more complicated de novo constructs. They are simplifications that make sense of lots of other
things. In fact, they are instances of the free energy principle itself, providing a simple account
of an accurate account of everything. So it's really the simplification and the reduction,
the increase in the evidence when you suddenly see that two things that you separately represented
and say two parts on your network can actually be represented by the same thing.
And you do that by removing it to the model. So actually many acts of creation, certainly in
sort of minimalist art forms, are actually getting to the bare essentials of what's going on,
again, in compliance with Occam's principle and advertising the complexity cost of explanations
for the lived world. That seems to me, you know, an important bucket of creative
acts is actually just seeing the structure underneath, the simplicity, explaining everything
with just one canonical platonic-like dynamical construct or narrative or sort of factorization,
however you want to articulate it. And in that sense, the first person to have that simple
insight that, oh, it's just one of those. This is just one of those. The first person is really
the genius and the creative person, but they haven't created anything new. They've just seen
something simpler. I see. And in a sense, you know, I think that's the scientific process.
That's exactly why we're having this conversation. That's what you do with your, well,
certainly what I do in my life and I guarantee what you do with your life. It's this journey
of finding the simplest explanation for everything that we experience and, you know,
leaving it as a legacy, you know, speaking to the encultured aspects of, you know,
one could appeal to to answer your question for the next me, you know, my children or my students
or whatever. So, you know, that's, I think, the most, that's one way of looking at creativity.
An interesting point that ensues from that is that, you know, it is perfectly possible
to be the first sentient artifact to see that simplicity that is actually in the real world.
So it's not a question really of making new things that aren't out there. They've probably
already been out there. It's just you're the first person to have to be able to infer them
in that simple kind of way. The other point to make here, of course, is, well, is it really out
there? Well, probably not. This is your simplest explanation. It comes back. There is no true model.
It's just this is the best explanation for what's going on. So you may be the first person to find
this simple explanation for what's going on. So I would put most of, most of art and creativity
in that bracket. But then you can press me on music, for example, and, you know, why is music so
attractive? And, well, that would be another question, maybe one of the questions. Yeah, perfect.
So that was amazing. Actually, Professor, I was watching one of your talks. Actually, it was
3am as I was watching that. And you said something that, I mean, my mom was sitting next to me,
and you said something that it was so interesting that I, it was like I got a nervous tick or
something. Then my mom asked me, what happened? I said, you said something amazing. And I just
want to share that with you. And just maybe we could dig a little bit deeper in this. You said
something along the line, if I'm putting this correctly, that the way our biological neural
network forms itself, tells us something about the environment that we're living in.
In other words, show me your nervous system, then I'll tell you about the environment you're living
in. And it's fascinating to me because maybe a couple months earlier, I was discussing this,
this gap between artificial neural networks and biological neural networks. And I was asking my
friends that these distances, like why these axons are long or why not? Or these dandroids?
We never sort of create those structural foundations into our artificial neural network. It seems
like a very simplified version of those. What do those features entail really? And when I heard
the piece that you just said in that talk, it was just amazing. And you mentioned the
idea of action in the distance and how light reflects into our eyes. And that's how we have
long neural connections in our reflection of action in the distance. Now, in practice,
if we actually see, in a literal meaning, if we actually see the nervous system of
biological beings, how much can we really say about the environment? Is it really possible to
say, okay, probably it has these rules, it has gravity, it has whatever? Is it really possible?
Yes, it's a lovely story. It's more interesting than my answer. Yes, I think it is. You've just
convinced your listeners it is possible. I've given some of my favorite examples, but I'll give you
a couple more just to reinforce the point you're making. I think the first thing, there's a couple
of things before I get into this. You were noting that the architectures that have emerged in neural
network theory and machine learning and, luckily, deep learning have a very simple architecture
that is, if you like, that is inherited from neural networks. And that's a good thing. So remember,
simplicity is good and inheriting from neural networks if you want your machines to deal with
the kinds of things that human beings deal with. Those are two good things. They may be a bit too
simple, but I think there are certain architectural features that you can actually spot, which actually
would allow you to apply your law that you've just suggested, give me a neural network and I'll give
you the world that your network is fit to explain. That law is another law from Ross Ashby. We mentioned
the law of requisite variety before the context of natural selection, but he, with colleagues,
also formulated the good regulator theorem. So I'm not sure whether you're probably too young,
far too young to remember this, but this was taught as one of the fundamentals of the
inception of cybernetics in the early 19th and 20th century. So the good regulator theorem,
which you can, there's a wiki for a wiki page, basically what it's saying is that if any artifact
and from Ashby's perspective, this would be something called a homeostat, which has all
the sort of regulatory aspects to it that we were talking about. If it regulates its environment,
basically it does the right kind of active inference and controls the latent states out there and the
right kind of way to survive, then it must be a good model of that environment. So he purportedly
thought that he could prove that and I've read the paper where he proves it. It's not an easy read,
but if you have 3am in the morning with nothing else to do, you should try and read his original
See if you're convinced. But the key point is that this is a very old idea and probably
a very true idea that anything that exists in some kind of generalized synchrony with its
environment, its heat bath, its world, its climate must effectively be a good model of that climate,
which simply means that the architecture, the structure that we refer to in terms of structure
learning must, in its structural form, re-capitalize the causal structure of what's out there. So the
structure of the Georgia model will tell you an enormous amount about the kind of world that is
generating the sensations that are being used to do the planning as inference to control that
particular world. So lots of great examples. You've given one of the most interesting ones, which is
why is the brain, why does it have an architecture that involves long thin communications at a
distance? I mean the liver doesn't, the liver does a wonderful job at doing its job and presumably
complies with the theology principle and doing its own kind of active inference in a world of
chemicals and metabolites. So you've given the viewers the answer, it's just that that must
imply a certain kind of conditional dependency that has this long range action at a distance,
and of course we, unlike thermostats and worms and viruses, really have to live in a world where
there is action and distance in the sense that I can hear you from Dublin, I can see you from
across the room. Now viruses, worms and thermostats can't do that, you know, their world is much more
immediate and just relies upon the conditional dependencies that are related to the juxtaposition
in symmetric space, but we live in a much more complicated world that has this action at a
distance that actually is necessary to expand the kind of sensations that we, that we have to
expand, particularly sort of vision and audition conveyed by waves of one sort or another or another
sort. So I think that's a wonderful example, but you can go even further to sort of coarse
the grain, you know, you just look at the brain and it's, and it has two hemispheres, that tells me
immediately that the embodied world of this particular brain has a bipedal symmetry.
Yeah, so, you know, if the world includes the body, then I can tell you almost immediately
that the kind of creature you dissect in this brain from, the part of the world which is most
important to that, to that particular brain or neural network, namely the body that it bodies,
it can move around, has a bilateral symmetry, will probably have two arms and two legs and
all that good stuff. If on the other hand you gave me the brain of an octopus, I'd be able to tell
you immediately because they have a brain for each arm, it has an eightfold symmetry. So I can
tell you a lot about the world, you know, in this instance, the, you know, the bodily world
from just the gross morphometry, but you can go much, much further. I could look at your brain
and I will find two streams that emanate from the visual parts of the brain and I will know that
they're visual parts because your eyes are part of the central nervous system and I know that
they're, they're photoreceptors. So I will know that they're somehow trying to explain
the causes of sensations in a visual scene and I will find two streams called the dorsal and
the ventral stream that sort of run eventually in this direction and dorsal, dorsal in this
direction and eventually towards the, the hippocampus more eventually from here and that tells me
immediately that there's been a factorization, there's been some sort of fundamental carving
nature at its joints in terms of the causal architecture of this creature's world and
in particular what I would be able to discern is you must believe that there are, the objects have
at least two attributes in order to produce their impressions of the nervous system in the words
of the Helmholtz and of course those will be what and where and I'd be able to tell you that you,
the typically the kind of visual objects that you're trying to explain
will have these two attributes that are conditionally independent because of the physical separation
and the, the sparse connectivity between these two streams. So what we're saying just in common
sense terms basically means for the typical visual objects that we usually look at, knowing what
something is doesn't tell you where it is and, and, and retelling you where something is doesn't
tell you what it is. So it is the most efficient, the simplest minimum complexity representation
just to represent the whateness and the awareness and then you put them together
to explain this thing in this, in this location. Another nice example of minimizing the complexity
by using the minimum degrees of freedom when carving up the world in terms of being a good
model of that world. But you see exactly the same kind of factorization and effectively weight
sharing in convolutional neural networks, which means you back to, you know, don't be dismissive
of your CNN, your favorite CNN, it's got some beautiful factorization in it. So the very fact
that you've got this weight sharing and this sort of way of carving nature at its joints
in the context of what you see emerging convolutional neural networks is that you are now saying that
there is some causal structure out there that has local continuity or contiguity aspects
that I could leverage to provide a really simple explanation for these kinds of data. So as soon
as you tell me I'm using a CNN, I know exactly what kind of data you're using, not using stuff that has
certain regularities in the metric space, spatially extensive, so image like data.
If you give me another kind of neural network that looks like a transformer network, I know
you're not doing that. I know that you're trying to deal with things mapping one
set of categorical objects onto another set of categorical objects. So just by looking at the
structural form of even, I think, sort of canonical architectures in deep learning,
you can tell a lot about the causal dependencies, the conditional
dependencies that generated the kind of data that you try to classify or to encode.
I see. Professor, this has been amazing. So the final question, the last question that
is going to be a very non-technical, I hope, like a simple and nice question to end this talk,
a light question that I'm sure many people watching would be interested to know your opinion about.
Will we ever fully understand how the brain works and how far have we come at the moment?
Right. A simple question. Or is it a
long difficult one? I appreciate your attempt to ask a light question.
I try. The lightest question one could ask. I think we've come an enormous way and there is an
enormous way to go. And I think there are some principled issues about the understandability
of ourselves that actually get into some deep questions about consciousness and metacognition
and the kind of creatures we are and, of course, the kind of curious creatures we are and have to
be in order to exist. So I'm thinking here of things like David Chalmers' meta-problem or meta-hard
problem. So now in philosophy, the question has moved away from what is it like to be or to see
red. Why do we spend all our time puzzling about the fact that we can see red or experience things?
And, of course, that speaks to this hierarchical model of Jonathan Model, where parts of this
Jonathan Model now have a construct of meanness and meanness in particular experiential contexts,
hence the metacognitive aspects when you think about very deep Jonathan models.
I'm sorry. What do you mean by meanness?
Oh, that's selfhood. The idea that you are... So you have something which you could argue
many lower forms of life, possibly not dogs and cats, but sort of possibly birds, don't have.
You have, a part of your Jonathan Model is a hypothesis or a fantasy that you are a person
and that you are in charge of orchestrating your perceptual influence in your deep Jonathan
models and moving and secreting. Now, you don't need to have that hypothesis or explanation in
order to function quite vibrantly. It's quite a sophisticated being, like a bird, for example.
But you have that, which suddenly creates the opportunity to think about counterfactual ways
of me being. So when I said meanness, I just meant me in inverted commas, less.
Selfhood. Oh, meanness. Sorry. Okay. Yeah, yeah, yeah. Sorry.
Just your question. Will we ever be able to understand ourselves? I think it's a lovely
question in the sense that the answer to that may hold the secret to awareness and particular
self-awareness, mean awareness, selfhood, and what particular kinds of Jonathan models you would
have to have in order to even ask the question, am I itself? And then the philosophers question,
can I understand myself? These are really, really, very, very literally deep problems
that rest upon deep hierarchical Jonathan models. I see. Perfect. Thank you so much,
Professor Friston. It was indeed an honor to have this talk with you. I really enjoyed it.
I hope that I didn't try to tire you out too much. I know it took a long time.
He did. And I'm exhausted, but thrilled to have spoken to you. I really enjoyed talking to you.
Thank you so much. Thank you so much, Professor. And I'm truly, truly sorry for taking too much,
is taking too much of your time because it's like one interesting argument brought another one,
and then it brought another one. And I apologize for that, but it was a priceless experience for me.
Well, thank you very much. We could have gone all night, but now I'm worried about all your editing
you have to take. But that's your problem, not mine. I got to have a nice talk with you.
Yes, exactly. Exactly. Thank you so much. Have a lovely evening. Thank you so much. Bye.
