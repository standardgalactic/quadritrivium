Hello everyone and welcome to ML Don. So today we are having an interview with Professor
Carl Friston. Professor Friston is a theoretical neuroscientist and a world renowned authority
on brain imaging. He has invented statistical parametric mapping or SPM, which is a software
package designed for the analysis of brain imaging data sequences. And also, I think
his main contribution to theoretical neurobiology is the free energy principle for action and
perception that hopefully will get the time to also explore. Professor Friston, thank
you so much for being with us today. My pleasure. Okay, so let's just get started. The free
energy principle, right? Could you just tell us about what the free energy principle is
and how has it enriched our understanding of the brain as a unifying brain theory, if
you will? Right. So I normally respond to that question by asking whether you want the
sort of the high road or the low road approach to understanding what the free energy principle
is about. I'll very briefly try to surmise both. So the low road, which is like a sort
of bottom up way of understanding it, which is a kind of way that I think of a neuroscientist
and psychologist who'd understand things or more broadly, people in neurobiology would
be the denouement at the present time of very old and compelling ideas about the brain as
literally a fantastic organ, an organ that constructs hypotheses, explanations, fantasies
that are for the best explanation for all the sensory impressions, all the sensory data
that we have to assimilate and literally make sense of. So the principles that underwrite
that perspective were articulated in various disciplines by people like Kant and Helmholtz,
people like Richard Gregory, narcissists by synthesis, perceptions, hypothesis making,
leverage to great effect in machine learning with people like Geoffrey Hinton and Peter
Diane and the notion of the Helmholtz machine, in turn, borrowing technical ideas from the
physics and engineering literature such as Feynman's Free Energy Principle as a way of writing
down a prescription or a normative account of this kind of sense making. And you end up with
a formal first principle account of sense making, which you can write down as a computer
program or you can write down mathematically, but essentially casts the brain as in the service
of trying to minimize its variational free energy. And you can read variational free energy here,
essentially as a kind of prediction error. Technically, the free energy gradients are
prediction errors. So what's a prediction error? Well, it's just the difference between
what given your beliefs about the current state of affairs out there beyond the skull bound brain.
What would I expect to see? What would I predict? And then I compare my predictions with what I'm
actually sensing and that constitutes a prediction error. Then I use the prediction error to drive
my beliefs in a way that eliminates that prediction error. So I've got a sufficient account of what's
going on. And that's often referred to as predictive processing in its inactive form or
active inference. And when we're drilling down just on the perceptual synthesis that is afforded by
this account, the most popular schema would be predictive coding, sort of the reciprocal exchange
of top down predictions from the inside out to the parts of the brain dealing with sensory processing,
but is complemented by an ascending outside in flow of prediction errors, where you need the
predictions to form the prediction errors, and then the predictions or the expectations
that are generating those predictions are informed and updated by the prediction errors.
Technically, that's a process called Bayesian belief updating, hence the Bayesian brain and the
Helmholtz machine formulations of it. So that would be an account of the free energy principle,
really as a prescription, an algorithm, a theological account of a normative theory for
the brain as an active organ, making sense of data, with the key twist that of course the brain,
or you and I, are in charge of the data that we're trying to make sense of. There's an added
inactivist twist here, it's not just that we're good Bayesian filters or data assimilators,
we actively have to decide what kind of data we want to base our inferences upon,
literally in terms of where I'm going to look next, how I visually palpate the world,
or which news channels to listen to, or which Wikipedia page I'm going to forage next.
So that's possibly more of a difficult problem the brain has to solve than just simply making
sense of the data, but both are bound up in this sort of generic account in terms of free energy
minimization, or the explaining away and accounting for providing the best explanation
that eliminates prediction errors. Very briefly, the high road is the kind of approach
that a physicist would take, and you can derive exactly the same mechanics of belief updating,
the same sort of explanation for sentient behavior, where behavior is important because
that's the active part. Just from an analysis of things that exist in the sense of physics,
and the particular physics here is the physics of non-equilibrium steady state
systems that have the remarkable property of returning to states that they were once in,
mathematically have attracting sets or characteristic states of being, that I
am found at this temperature in this kind of location and this sort of environment.
So all the things that would characterize me represent states that I keep on revisiting,
and that can be written down mathematically in terms of the dynamical systems theory of
attracting sets. You can work out the probability distributions, or you can appeal to Feynman's
work on pathological formulations to work out, well, if things exist, then what properties,
what dynamics must they have been, must they possess in order to exist, and to maintain
themselves within these viable states of being. So in life sciences, this would be known as
auto-poesis or self-creation in the computational chemistry of the self-assembly, how the molecules
assemble themselves and retain their sort of morphology and structural or configurational
organization. And it turns out that you end up with exactly the same equations that you're
trying to, everything that we do, either on the inside or in terms of acting upon the world,
is in the service of minimizing this variational proxy for prediction error,
but also mathematically happens to be the quantity that statisticians would try to
optimize, namely the marginal likelihood or the evidence for models of the world. So you get this
sort of tenological account that systems that self-organize look as if they're trying to maximize
the evidence for their models of the world, and sometimes people refer to that as self-evidencing.
So that would be a philosophical account of the top-down physicist's explanation for the way that
you and I actively organize our exchanges with the lived world in order to exist and maintain our
existence, some generalized homeostasis that can be described mathematically as a minimization
of variational free energy. Okay, very, very interesting. So as far as I understand,
so it's about, as you said, we're trying to sort of do actions that would maximize the
probability of our existence, of the model that we have from the environment that we're living in,
which is, I think, the same thing that you refer to as marginal likelihood,
the probability of observation given the model that I currently have of the world.
What I'm trying to understand is trying to marry that idea with the other idea that
the brain selects which one of these inputs to consider how to affect the environment,
because when we say the probability of my input from my receptors given the model I have
from the environment, it is as if I have a control of what I will necessarily do,
because I mean there are so many receptions coming, I might choose to ignore some of them,
but then it might not be the right choice. Also, when you talk about optimization,
I realize that now, if I look at this from a machine learning perspective,
since we're optimizing, could we overfit and fail to generalize?
Well, that's a great question. There are about four great questions in there.
Oh, sorry, sorry. No, no, no, no. Pick one.
Full attention. First of all, I am sorry, I will use technical terms with very little
theory of mine, because I'm so used to doing so, so you're absolutely right. The marginal
likelihood is nothing magical. It's just the probability that these sensory data would be
solicited, experienced under my model of how those data were generated. You're absolutely
right. The marginal bit comes from marginalizing out or averaging over the unknown parameters
of a generative model, but we don't need to worry about that. The model evidence is the
marginal likelihood and the logarithm of that is the negative free energy.
But your more interesting question. I mean, if you now review this podcast and just listen
to the words you use, they're very telling. You're talking about selection. That's a great
word to use. You also said I. So immediately, there's an inactive, a genital aspect to your
question. So a predictive coding scheme that's just assimilating data and making sense of data
has no notion of me, has no selfhood, and certainly doesn't have any, it is not equipped with the
ability to choose and to select which data it wants to actually assimilate or to classify or
to categorize. So we're going beyond machine learning now. We are going beyond simply passively
in a sort of outside in way, making sense of data. We are now asking about this really deep
question. How on earth do we select the data to assimilate? So the answer is very simple.
In the same spirit that everything on the inside that's doing this assimilation
has to minimize a prediction error, sometimes cast in terms of surprise in a technical sense.
So this is if you're an information theoretician, this would be the self-information.
If you're doing that, then the natural question then arises. Well, look, what happens if I'm in
control of the data that I can now select or turn towards? Well, now I have to choose the
actions that will produce data that have the minimum expected surprise or the minimum expected
prediction error. So if surprise, which is another word for mathematical expression for this bound
on surprise or evidence called variation free energy is the thing in the moment,
then after I have acted in the future, then what I want to do is to minimize my expected surprise,
my expected prediction error. So what's that? Well, expected surprises is technically entropy.
And more anthropomorphically, this would be uncertainty. So what you're doing is if you're
subscribing to the free energy principle, and you now have to choose the best kind of act or
sampling or selection, then you would select those actions that minimize your uncertainty,
the resolve your uncertainty. Another way of exercising that mathematically is that they
have the greatest information gain that those kinds of data resolve my uncertainty more than
those kinds of data. So that's a actually a well known Bayesian principle. And it's
often cast in terms of optimal Bayesian design and has been known since the 1950s through the work
of people like Lindley. There are well defined objective functions, which are simply how much
do I shrink my uncertainty and technically measured in terms of a divergence mathematically,
but you know, we just need to know the information gain is just the degree to which I shrink my
uncertainty about states of affairs out there. So what that means is that we are, if we minimize,
if we choose those actions, or we want to minimize our free energy, then we have to choose those
actions and minimize the expected free energy, which necessarily makes us into curious creatures.
Now you were saying to me, I don't know how to do that. I don't understand how to do that. You do,
you're a scientist. Even if you don't have a PhD, every every child and every student and every
adult is a curious scientist. They want to know how their world works. Either it's your scientific
field of study, or if you're a little baby, it's just how your body works. What kind of thing is
mum? Is mum the same kind of thing as me? All of this information has to be carefully selected
in order to resolve uncertainty about this hypothesis or that hypothesis in exactly the same way
that you design a good experiment. So when you design an experiment, then you design the experiment
to yield data that are going to be maximally informative in relation to your null versus
altered hypothesis. So it's exactly the same principle governs the selection of natural
experiments that we deploy with our eyes, looking over there or looking over there. It's exactly
the same mathematics as the same imperatives. And indeed, you actually see the same information
theoretic constructs emerging in the visual search literature in the neurosciences.
You can construe these as savings maps that they score the uncertainty reduction, the epistemic
affordance of selecting those data by looking over there or by again looking at this Wikipedia
page or listening to this news channel and not that fake news channel. So it's a great question
because not only does it make I think the really profound point that the curiosity of all kinds
is baked into the first principle account of self-organization, that anything that exists
must in part be acting or appear to act. Look as if it is acting to resolve uncertainty about the
exchanges with its eco and its environment, its culture or the visual scene at hand.
Furthermore, because you're talking about action, it's you that's acting. So there's an implicit
sense of agency that is owned by the artifacting question. So we're now talking about something
necessary to equip an agent with some very elemental self-hood. The third thing at the table
is really commonsensical, but it's a very interesting observation and that in order
to select this action over that action, then I have to have a model of the consequences of my
action. But the consequences live in the future, which says that your genetic models now must
cover the future, which means that you've got something with a lot of things in this universe
don't have, which is a model of the future. It has temporal depth. So you could apply the free
energy principle to a thermostat. You could apply it to a virus and it would work perfectly well.
You could simulate thermostats and indeed make a thermostat or simulate a virus or possibly even
construct one. But these kind of the genetic models that you would use that has to be in place in
order to work out the surprise due to the predictions generated by that model would not have the
temporal depth of the models that you and I have. So as soon as you talk about selection,
you're talking about essentially selecting a plan of action, a course of action, and that means
necessarily rolling out into the future. So your genetic models have this temporal depth.
And furthermore, as soon as you entertain the notion that I could do this or that,
then there is an active selection in play. The thermostat doesn't have that. There's only one
thing it can do because it acts in the moment. But you and I have a much more deeper generative
model we bring to the table that allows us now to set amongst a series of counterfactual
courses of action. You can choose what to say next or not to say anything at all. And in that choice,
there is an act of Bayesian, technically an act of Bayesian model selection or policy selection.
So now you've got thermostats that select provided they have these deep generative models that
sort of free you from the moment because they're really about trajectories and paths,
how we navigate the lived world. And about the overfitting thing, could we overfit?
Or actually, why don't we overfit? Maybe that's the right question.
Yeah, that was the fourth clever part of your question. The reason you don't overfit and I'll
probably, if you indulge me, I'll answer this from the point of view of sort of machine learning
or statistics. So the answer here lies in an understanding of the nature of
models and their evidence. So the first thing to say is that there is no true model of the
world. There is a best model in the world and that's simply the model that has the greatest
evidence, but there's no true model in the world. There are many, many ways of explaining
how some data for sensory data was generated and providing an account of that. All we can do
is find the one that has the greatest evidence, the greatest marginal likelihood,
probability of the data under that model. So we can change the model until the model
provides or account gives you the greatest marginal likelihood. So what is evidence? This
is the key point. Evidence is accuracy minus complexity, which means that to find the model
with the greatest evidence is to provide an account of these data in the simplest way possible.
And that simplicity underwrites the ability to generalize. In the absence of that complexity
part of the model evidence, you would certainly overfit. So if you were allowed to use as many
degrees of freedom as you wanted with no constraints on that, i.e. no complexity constraints,
you would certainly overfit these data so that the next day's data would not be expelable,
because you'd fit all the little random effects and fluctuations in the data at hand.
So that complexity term is absolutely crucial in ensuring that maximizing model evidence,
minimizing surprise, minimizing free energy, all of these are statements essentially of the same
thing, goes hand in hand with an automatic penalty on the complexity that precludes
overfitting. And it's really interesting just to sort of unpack what complexity is here. So
on a technical account, the complexity is basically the degree to which I change my
mind having seen some data. So technically it's the difference between some posterior
Bayesian belief, some probabilistic explanation for some causes of some data
