the Wright Brothers thought about an airplane, or the first time someone thought about a mermaid.
Any time that something, if that's even possible, I don't know, the first time that
something jumps into one's mind that never existed in the environment,
and they try to make it, like, create it into reality.
Can we explain that part, the knowledge generation, with these two ideas,
active inference or perceptual inference?
Yeah. Again, another very challenging question. I would contend that you can. So, I think you're
touching now upon sort of the links between the mechanisms of the underwrite model optimization
and to a certain extent curiosity, but more, I think, the notions of creativity and exploring
different model spaces. So, we're now moving, in fact, actually, completely away from active
inference and perceptual inference in terms of optimizing both the states and parameters of a
generative model as it is exposed to data, and now turning to a third level of optimization,
which would be the optimization of the model in and of itself. So, in various fields, this will
be known as structural learning, for example, in medical and constructivism. The problem of not,
let's say, forget about, let's assume we've got the perfect scheme, if you give me a neural
network or a generative model, and a particular environment that can generate data, I can do
all the good inference or learning to infer actively and do all of that as inference. So,
we've got this little agent now, coupled, exchanging with her environment
in a base optional way. Now, we move to a next level, hierarchically speaking, of an optimization
of free energy minimizing process, which is now not applied to the parameters of the model, but to
the model itself. How many hidden layers? How many connections? Do you use rectified linear or
sigmoid trans, you know, how do you carve things up in terms of not just hierarchical levels, but
in terms of, you know, factorizing or putting modules or conditions at any level, all sorts of
fundamental structural issues that you would practically have to deal with and contend with
when writing down the architecture of your neural network that, you know, will read as the structural
form of your generative model. So, this structural learning problem, and from the point of view of
a statistician is a Bayesian model selection problem. It's basically integrating the marginal
likelihood over all the kinds of data that you could see or all the kinds of data that you have
seen. And then you choose the model that best accounts for, that has the greatest evidence.
So, now this is a categorical act of selection of selecting the best model. And I use those words
because this is mathematically, I think, an apt description of natural selection. So, natural
selection evolution is just nature's way of being Bayesian model selection, just selecting
the hypothesis, namely the phenotype, where evolution thinks that this particular hypothesis
is a good fit to me, the equation that I'm supplying. And therefore, the equation then
selects the phenotype through this process of the structural form of the epigenetically specified,
for example, structure of the geratin model. And as soon as you think of it like that,
there's a big question. And the big question is, well, how do you explore the model space?
Because we've just said, well, you're doing this Bayesian model selection after you've acquired
the data. So, now what kinds of model spaces would you explore? And is it necessary to explore?
There is an argument, which we actually touched upon earlier on, that we can actually start off with
a completely overparameterized model and prune it. So, this is a kind of Bayesian model selection
technically called Bayesian model reduction, where it is literally, if I remove this parameter,
this connection from this model or this hidden layer from this model, would I increase the
pathogen of its evidence? In other words, would I be minimizing complexity without
sacrificing too much accuracy with the kinds of data that this model is fit to explain?
So, you could argue that both the model expansion that typically dealt with
in a Bayesian context through nonparametric Bayes, so by having expandable models and
priors of the way that you would add in different parts or chunks. So, for example,
in a hierarchical Dirichlet process, you might have some sort of stick breaking process on top
of that, which says, is it justified to bringing the new hidden latent variable in my Dirichlet
distribution? And you've asked that question by basically evaluating the evidence for that model
or the Bayesian free energy with and without that extra bit. And if it improves by adding
complexity, which more than pays its way in terms of the accuracy, then you keep it if not,
you wouldn't. But what the stick breaking process brings to the table is a principled way of
expanding or growing the model, I repeat, in the context of nonparametric Bayesian approaches.
But I think you can also, I think that both model expansion, and I'm reading that at the moment in
terms of nonparametric Bayes, or from the point of view of natural selection, having recombinations
of certain sort of codes or parameterizations of geratin models that you can think of in terms of
spit and merge like operations that you get in jetting recombination, ways of exploring a model
space in a principled structured way, or and the Bayesian model reduction, I think both can be seen
as the processes that give rise to creativity and insight and aha moments. So I'm saying that
are using some notion of aha moments and creativity in deliberately, just to make the point that some
creative acts are actually acts of reduction. It's not so much the fact that no one's ever seen this
before. Sorry, no one has ever represented or had this, you know, this object in mind before,
it's just that no one's seen that connection before, and that it's actually a simpler way of
viewing things. So if you just think about the most creative ideas and signs, they are not actually
more complicated de novo constructs. They are simplifications that make sense of lots of other
things. In fact, they are instances of the free energy principle itself, providing a simple account
of an accurate account of everything. So it's really the simplification and the reduction,
the increase in the evidence when you suddenly see that two things that you separately represented
and say two parts on your network can actually be represented by the same thing.
And you do that by removing it to the model. So actually many acts of creation, certainly in
sort of minimalist art forms, are actually getting to the bare essentials of what's going on,
again, in compliance with Occam's principle and advertising the complexity cost of explanations
for the lived world. That seems to me, you know, an important bucket of creative
acts is actually just seeing the structure underneath, the simplicity, explaining everything
with just one canonical platonic-like dynamical construct or narrative or sort of factorization,
however you want to articulate it. And in that sense, the first person to have that simple
insight that, oh, it's just one of those. This is just one of those. The first person is really
the genius and the creative person, but they haven't created anything new. They've just seen
something simpler. I see. And in a sense, you know, I think that's the scientific process.
That's exactly why we're having this conversation. That's what you do with your, well,
certainly what I do in my life and I guarantee what you do with your life. It's this journey
of finding the simplest explanation for everything that we experience and, you know,
leaving it as a legacy, you know, speaking to the encultured aspects of, you know,
one could appeal to to answer your question for the next me, you know, my children or my students
or whatever. So, you know, that's, I think, the most, that's one way of looking at creativity.
An interesting point that ensues from that is that, you know, it is perfectly possible
to be the first sentient artifact to see that simplicity that is actually in the real world.
So it's not a question really of making new things that aren't out there. They've probably
already been out there. It's just you're the first person to have to be able to infer them
in that simple kind of way. The other point to make here, of course, is, well, is it really out
there? Well, probably not. This is your simplest explanation. It comes back. There is no true model.
It's just this is the best explanation for what's going on. So you may be the first person to find
this simple explanation for what's going on. So I would put most of, most of art and creativity
in that bracket. But then you can press me on music, for example, and, you know, why is music so
attractive? And, well, that would be another question, maybe one of the questions. Yeah, perfect.
So that was amazing. Actually, Professor, I was watching one of your talks. Actually, it was
3am as I was watching that. And you said something that, I mean, my mom was sitting next to me,
and you said something that it was so interesting that I, it was like I got a nervous tick or
something. Then my mom asked me, what happened? I said, you said something amazing. And I just
want to share that with you. And just maybe we could dig a little bit deeper in this. You said
something along the line, if I'm putting this correctly, that the way our biological neural
network forms itself, tells us something about the environment that we're living in.
In other words, show me your nervous system, then I'll tell you about the environment you're living
in. And it's fascinating to me because maybe a couple months earlier, I was discussing this,
this gap between artificial neural networks and biological neural networks. And I was asking my
friends that these distances, like why these axons are long or why not? Or these dandroids?
We never sort of create those structural foundations into our artificial neural network. It seems
like a very simplified version of those. What do those features entail really? And when I heard
the piece that you just said in that talk, it was just amazing. And you mentioned the
idea of action in the distance and how light reflects into our eyes. And that's how we have
long neural connections in our reflection of action in the distance. Now, in practice,
if we actually see, in a literal meaning, if we actually see the nervous system of
biological beings, how much can we really say about the environment? Is it really possible to
say, okay, probably it has these rules, it has gravity, it has whatever? Is it really possible?
Yes, it's a lovely story. It's more interesting than my answer. Yes, I think it is. You've just
convinced your listeners it is possible. I've given some of my favorite examples, but I'll give you
a couple more just to reinforce the point you're making. I think the first thing, there's a couple
of things before I get into this. You were noting that the architectures that have emerged in neural
network theory and machine learning and, luckily, deep learning have a very simple architecture
that is, if you like, that is inherited from neural networks. And that's a good thing. So remember,
simplicity is good and inheriting from neural networks if you want your machines to deal with
the kinds of things that human beings deal with. Those are two good things. They may be a bit too
simple, but I think there are certain architectural features that you can actually spot, which actually
would allow you to apply your law that you've just suggested, give me a neural network and I'll give
you the world that your network is fit to explain. That law is another law from Ross Ashby. We mentioned
the law of requisite variety before the context of natural selection, but he, with colleagues,
also formulated the good regulator theorem. So I'm not sure whether you're probably too young,
far too young to remember this, but this was taught as one of the fundamentals of the
inception of cybernetics in the early 19th and 20th century. So the good regulator theorem,
which you can, there's a wiki for a wiki page, basically what it's saying is that if any artifact
and from Ashby's perspective, this would be something called a homeostat, which has all
the sort of regulatory aspects to it that we were talking about. If it regulates its environment,
basically it does the right kind of active inference and controls the latent states out there and the
right kind of way to survive, then it must be a good model of that environment. So he purportedly
thought that he could prove that and I've read the paper where he proves it. It's not an easy read,
but if you have 3am in the morning with nothing else to do, you should try and read his original
See if you're convinced. But the key point is that this is a very old idea and probably
a very true idea that anything that exists in some kind of generalized synchrony with its
environment, its heat bath, its world, its climate must effectively be a good model of that climate,
which simply means that the architecture, the structure that we refer to in terms of structure
learning must, in its structural form, re-capitalize the causal structure of what's out there. So the
structure of the Georgia model will tell you an enormous amount about the kind of world that is
generating the sensations that are being used to do the planning as inference to control that
particular world. So lots of great examples. You've given one of the most interesting ones, which is
why is the brain, why does it have an architecture that involves long thin communications at a
distance? I mean the liver doesn't, the liver does a wonderful job at doing its job and presumably
complies with the theology principle and doing its own kind of active inference in a world of
chemicals and metabolites. So you've given the viewers the answer, it's just that that must
imply a certain kind of conditional dependency that has this long range action at a distance,
and of course we, unlike thermostats and worms and viruses, really have to live in a world where
there is action and distance in the sense that I can hear you from Dublin, I can see you from
across the room. Now viruses, worms and thermostats can't do that, you know, their world is much more
immediate and just relies upon the conditional dependencies that are related to the juxtaposition
in symmetric space, but we live in a much more complicated world that has this action at a
distance that actually is necessary to expand the kind of sensations that we, that we have to
expand, particularly sort of vision and audition conveyed by waves of one sort or another or another
sort. So I think that's a wonderful example, but you can go even further to sort of coarse
the grain, you know, you just look at the brain and it's, and it has two hemispheres, that tells me
immediately that the embodied world of this particular brain has a bipedal symmetry.
Yeah, so, you know, if the world includes the body, then I can tell you almost immediately
that the kind of creature you dissect in this brain from, the part of the world which is most
important to that, to that particular brain or neural network, namely the body that it bodies,
it can move around, has a bilateral symmetry, will probably have two arms and two legs and
all that good stuff. If on the other hand you gave me the brain of an octopus, I'd be able to tell
you immediately because they have a brain for each arm, it has an eightfold symmetry. So I can
tell you a lot about the world, you know, in this instance, the, you know, the bodily world
from just the gross morphometry, but you can go much, much further. I could look at your brain
and I will find two streams that emanate from the visual parts of the brain and I will know that
they're visual parts because your eyes are part of the central nervous system and I know that
they're, they're photoreceptors. So I will know that they're somehow trying to explain
the causes of sensations in a visual scene and I will find two streams called the dorsal and
the ventral stream that sort of run eventually in this direction and dorsal, dorsal in this
direction and eventually towards the, the hippocampus more eventually from here and that tells me
immediately that there's been a factorization, there's been some sort of fundamental carving
nature at its joints in terms of the causal architecture of this creature's world and
in particular what I would be able to discern is you must believe that there are, the objects have
at least two attributes in order to produce their impressions of the nervous system in the words
of the Helmholtz and of course those will be what and where and I'd be able to tell you that you,
the typically the kind of visual objects that you're trying to explain
will have these two attributes that are conditionally independent because of the physical separation
and the, the sparse connectivity between these two streams. So what we're saying just in common
sense terms basically means for the typical visual objects that we usually look at, knowing what
something is doesn't tell you where it is and, and, and retelling you where something is doesn't
tell you what it is. So it is the most efficient, the simplest minimum complexity representation
just to represent the whateness and the awareness and then you put them together
to explain this thing in this, in this location. Another nice example of minimizing the complexity
by using the minimum degrees of freedom when carving up the world in terms of being a good
model of that world. But you see exactly the same kind of factorization and effectively weight
sharing in convolutional neural networks, which means you back to, you know, don't be dismissive
of your CNN, your favorite CNN, it's got some beautiful factorization in it. So the very fact
that you've got this weight sharing and this sort of way of carving nature at its joints
in the context of what you see emerging convolutional neural networks is that you are now saying that
there is some causal structure out there that has local continuity or contiguity aspects
that I could leverage to provide a really simple explanation for these kinds of data. So as soon
as you tell me I'm using a CNN, I know exactly what kind of data you're using, not using stuff that has
certain regularities in the metric space, spatially extensive, so image like data.
If you give me another kind of neural network that looks like a transformer network, I know
you're not doing that. I know that you're trying to deal with things mapping one
