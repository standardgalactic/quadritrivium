machine learning, this might be called some latent states generating
unobservable latent states, hidden states that are generating observable measurements or
observations or sensory input from our perspective. So I've got two beliefs before we're talking about
this process of dynamically reducing free energy or minimizing prediction error as a process of
belief updating. That belief updating is literally updating a prior belief before I see data into
a posterior belief after I've seen a posteriori after I've seen the data. And the degree to which
you have to move your belief from the prior to the posterior is a complexity. So it's the degrees
of freedom that you're using up in explaining the data. Now if you've got really good prior beliefs,
you don't have to change your mind very much and therefore you can provide an accurate account of
this data with a minimal complexity cost because your belief updating has been extremely efficient
and minimally complex. So it is those kinds of high evidence generative models that have this
complexity penalty in place that are able to generalize to new data because you've got the
right kind of priors. So it all goes hand in hand and it's a really important point. I don't know,
I'm wondering whether you knew this already in virtue of the fact you asked the question but
this issue is really a difficult issue in conventional machine learning because if you
just look at the way the field has gone in machine learning, they've gone to sort of big data,
sort of committing to the ideology of big data science with massively overparameterized neural
networks. So if you read a neural network as a generative model, certainly in the context
of an autoencoder, then what you're starting with is an overly parameterized, overly complex
generative model which can beautifully explain any given data set that is hopeless in generalizing
to another context or some new data simply because the complexity is too high, because there are too
many parameters in that model that generally say connection weights in a current neural network
or a convolutional neural network or a variational autoencoder. So what that leads to then is
all sorts of problems with sharp minima that ensue because you have not put in the complexity term
that you can think of in terms of this Occam's principle. So Occam's principle has to get into
the objective function. So the game now would be to take a neural network in machine learning,
same deep learning, and try and find out which connection weights or what architectural aspects
of your network you didn't need to provide an accurate account, which is actually the
opposite direction of travel from most of deep learning at the moment, which are aspiring to
involve parameters with bigger computers. There's an interesting twist there, of course, that there
are a number of fundamental principles, such as Landauer's principle, the Jauzynski equality,
which effectively say that information is energy and energy is information. And in this present
context, the degree of belief updating as scored by the complexity cost has a thermodynamic cost,
which means if you're recognizing things properly and selecting the right things to do properly
with the minimum of complexity, you're doing it with the greatest thermodynamic efficiency.
So you can measure the quality of any artifact, any neural network or neuronal network, simply by
the amount of energy it has to expend to exist. So again, the drive towards high performance
computing is exactly in the wrong direction. What you're looking for is sort of sentient
edge computing devices based upon an evidence maximization or free energy minimization approach
that should do it like your brain, which would consume the same energy as a light bulb as opposed
to a power station. So that's a really good question that has some really pragmatic engineering,
computer engineering and sort of, you know, energetics associated with it and explains to a
great extent why you're so much better than any neural network that lives on a high performance
computer at the moment. The idea of overfitting and overparameterized models just reminded me
of this paper by Professor Michael Belkin. So it's a paper about this phenomenon
that happens in overparameterized models called the double descend, as they've called it. Now,
their question was why overparameterized models don't overfit and why they can generalize so well
and it's a very interesting paper. It was really shocking to me because we have this famous,
I mean, in machine learning, we have this famous U-shaped sort of plot that shows that, okay,
the more you parameterize your model, the error in the predictions of your model keeps going down
and down and down and after a certain point, it fails to generalize. So it just keeps going
higher and higher and higher. Now, what they did was they didn't stop there. They kept adding more
parameters. They kept adding more and more. Now, what happened then was after a certain point,
we have a double descent, a second descent in the generalization ability or in the error,
let's say, the prediction error, it again keeps going down and down and down. In some cases,
it goes even below the point where the minimum of the first descent. It could even get to a
better generalization and in some cases, no, the minimum that it could ever reach would not be as
good as the minimum of the first descent. So they've called this double descent and I'm trying to,
and didn't happen, the double descent always happened based on their experience. They use it
with decision trees. They use it with neural networks and a couple more models. I don't remember
exactly what they were, but the notion of accuracy minus complexity and that being the idea of
that being the reason as to why we don't overfit. I'm trying to sort of have that next to the idea
because their argument was that when you have too many number of parameters,
the model sort of goes towards like you have more knobs to control your search in the hypothesis
space. You inevitably go to, as if you can, you can search better and then you find,
maybe I think they were arguing that you find simpler solutions, I think, if I'm not mistaken.
But how would you marry that idea with this accuracy minus complexity being the reason of
not overfitting? Right, that's a great question. Okay, so I think if you think of this, the problem
in terms of finding minima in an objective functional landscape, then that phenomenon of double
descent makes entire sense in the context of escaping local minima. So let's assume that we've
basically got a difficult problem, say a brittle, dynamical system or a highly nonlinear problem,
where the objective function, let's assume it's either a free energy objective function
or some approximation to it, then it's likely you're going to have lots of local minima around.
One instance and one particularly problematic aspect, probably you have in mind is this notion
of sharp minima, minima that have very steep sides that are very difficult to escape from with any
sort of stochastic annealing or any sort of device that would sort of bring you out of that base
of attraction of that sharp minima. So the first point to make before I pursue that analogy and
really basically just reiterate the solution that you've actually just described, which is
absolutely great. I'll do it from a dynamical perspective. The first point to make is that
the failure to put the complexity term into the objective functions used by typically by
lots of vanilla machine learning, not all, I mean high-end, say, variational autoencoders
use exactly the variational free energy, which is in machine learning there is an evidence
level, that is exactly the same objective function we're talking about, but those
algorithms that don't do that and just try to maximize the likelihood as opposed to the marginal
likelihood. So, you know, just try to maximize the accuracy as scored by the sum of square residuals,
for example. So they're forgetting about the complexity and they're just drawing down the
accuracy part of it to create these sharp minima. The reason that these minima are sharp is that
the complexity term creates it into a U-shaped minima. This is, if you like, one expression
of Occam's principle, that the good explanations have to allow for some latitude. So the curvature
of the minima scores the latitude you have in the sense that you're not committing to a very
particular explanation that provides an accurate account of these data. There are a number of
understanding this. One of them is, in fact, from the point of view of Jesus' maximamentary principle,
that under constraints afforded by, say, a likelihood and some prize on the charity model,
the best explanation is always the belief that has the maximum entropy, which means
it has the minimum curvature in some free energy landscape. So the free energy functional provides
an objective function where all the minima are flat, thereby eluding the sharp minima
problem that plagues most conventional machine learning. So I think that's the first thing
to recognize that you will find those free energy minima more efficiently provided you can escape
the local minima, simply because the free energy minima are always flat and they're less likely
to trap you. So let's now take what other ways could you escape sharp minima that are local
minima that are high in a free energy landscape. So just one sort of picture I use for my students
is very much like a mountain range that is the genesis of a little stream that turns into a river.
So the lower you get in the landscape, the wider the rivers, the shallower the valleys until
ultimately you get to an estuary. And so exactly the same sort of, if you like,
landscape exists in a free energy or very short free energy context. So stuff high up,
all the sharp minima that have a very undesirable high free energy are the rough stuff in the
higher mountains. And your job is to get down, flow down to the nice shallow global minima
that have this sort of maximum entropy Occam's principle shallowness to them with low curvature.
So how might you want to get out of those high hanging valleys, those sharp minima?
Well, you've said it basically. If you imagine from the point of view of dynamical systems theory
flows on a manifold or a, in this instance, a free energy landscape. Let's imagine we just
got one parameter to play with. So we've found ourselves in a V shaped minima. And we can't
get out of it. But if I add another dimension and now make my one dimensional objective function
two dimensional, then I can change that fixed point to tractor that corresponds to the point
of the minimum, the local minimum in question into a subtle point into an unstable fixed point.
And I can escape from it. So now I got a subtle point where there are now roots away from that
local minima. Because as you keep on adding dimensions, i.e. adding parameters to your model,
the opportunities for converting a stable fixed point, which is a local minima
into an unstable fixed point from which you can escape increases geometrically or at least
superly. So I think that's the explanation in conversation with people in machine learning.
And I joined one such conversation with people at NYU just a couple of weeks ago. I think that
that is probably the story that you would tell to account for this sort of throwing lots and lots
of parameters at a model just to destroy stable fixed points that constitute local minima so that
you can access the global minima. And then once you've done that, you then crew away all the parameters
that you use as your escape routes to get back to the simple model. So what we're talking about,
I think this double descent thing is a really lovely notion. And it does remind me a little bit
of how the brain works in the sense that it starts off with an overly parametrized
geritim model. So babies and children actually have more connections, they have more W's,
more external connections, white matter tracks than adolescents and then you and I do.
So you get this progressive pruning as you grow older. So you start off with this overly
parametrized, overly complex, virgin tabla rasa where you can escape all the local minima.
And then as time goes on, you then do your pruning and reduce the number of parameters or
connection weights in your model to simplify it to the in accord with Jason's Max Ventura principle
outcomes principle from the point of view of the free energy minimization. It's the complexity
part of the minimization. So you now get a smaller parameter model, but you've only done
the second wave of descent, if you like, after you've escaped all the local minima by having an
overly parametrized model. So I'm sure that this must be a universal principle that I suspect
is so universal, it has been discovered by evolution millions of years ago.
Yeah. Yeah. That was, that was amazing. Wow. That was a totally different perspective
that the, I mean, the reduction in the number of parameters as you grow old. I never thought
about that. First of all, I didn't know that, but it was, it was really amazing.
Yeah. Well, it has its penalties because I am much older than you and therefore I'm much sparser
and wiser than you, but I could not adapt to a new world. So I can't go, why don't you jumpy
or go to disco. Oh my God. Wow. Wow. Wow. Mind-boggling. I don't know what to say. That was,
that was really enlightening, actually. So it seems to me that there are two methods of learning
in general when it comes to biological systems. So I did a little bit of research on this.
I don't know whether I'm right or not, but I could narrow this down to two general methods.
One is perceptual inference, where, and the other one is through action inference. Now,
the perceptual inference, I suppose, is when you generate a model from the world and you keep
updating the parameters, whereas the action one is where you actually choose actions.
So I know we've discussed this a little bit at the beginning of the interview, but
could you explain these two methods in more detail and compare them as to
in what situation am I learning through perceptual inference and in what situation
do I choose to learn through action inference?
Right. Yeah, he does, I think, speak to this issue of sort of machines with agency that we're talking
about before, artifacts that have sentient behavior. So the sentient part would be the
perceptual inference and the behavior would be the active inference. I think that they,
you know, once you think about sort of artifacts that do actively exchange with the world, I think
they both have to go hand in hand. So I think it's, although another excellent question, I will
qualify my answer after I've given you the vanilla answer, which is, you know, this is just a joint
optimization process over two kinds of unknowns, two kinds of latent states out there that generate
data. One kind of latent state is the latent states over which I have no control. So this
would be the standard data assimilation classification problem, recognition problem.
I am given some data and I have to classify, learn its causal structure infer the particular state
generating, latent state generating these data in this particular context. So that would be
perceptual synthesis, perceptual inference, learning the parameters of that judging model,
I would call perceptual learning. So in my world, I distinguish between fast processes that infer
time dependent states and slow processes that infer the parameters that underlie the contingencies
and the laws. We would be learning in the machine learning sense of the W's in a neural network,
but the states of the network at any one point in time out interpreters inference. You know,
this is the activity of these nodes and therefore I am seeing this kind of face.
So that's one set of unknowns, but there's another hugely important set of unknowns,
latent states out there, those which I can control, those which I have some agency over.
So if I make this movement or I secrete this or I switch on this knob, then the states out there
will also change. If that's the case, then you're now conditioning a certain set of hidden states
on latent states which can be thought of as plans or control variables of the U in a
sort of a standard machine learning treatment. So these control variables are, say,
control theory now play a really important role because the transitions are dynamics amongst the
latent states that are controllable and are conditioned upon this U, but this U is another
random variable. This means I have to infer two sets of random variables. I have to infer
the latent states over which I have no control and I also have to infer the control variables,
if you like, the plans upon which the variables, the states that I do have control over are conditioned.
So there's two kinds of inference here which you can think of in terms of the distinction
between somebody just applying a Kalman filter to estimate the states of some plant versus
estimating the best thing to do if you were doing applying control theory and, you know,
when you're controlling a particular plant, say, in engineering. But because we're now
committed to understanding everything in terms of optimizing a free energy functional of beliefs
or probability distributions, that now becomes that planning or control theoretic part of the
problem, the active inference part of the problem, now becomes planning as inference. So, you know,
in a sense, you can read the two perceptual and active sides of inference as really the same process
working hand in hand, in parallel. But acknowledging there are two different kinds of variables
that you have to infer, you have to work out, basically, what's the state of affairs out there
and what am I doing about it? And then, of course, you can see now that the active inference side,
the planning as inference, now has to consider a number of plausible plans and then we get back
to selecting the most likely plan which minimizes the expected free energy or
resolves which is uncertainty or avoids surprising outcomes like, you know, your machine crashing
if you're in an autonomous driving situation. So, you can write in all your prior preferences
into that expected free energy in the future conditioned upon the plan.
So, I would say that there is no choice to do active inference means that you have to infer
both the latent states of the world and the way that you're intervening on those states
