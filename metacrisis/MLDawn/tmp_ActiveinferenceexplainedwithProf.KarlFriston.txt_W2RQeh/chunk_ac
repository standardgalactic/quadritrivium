through controlling state transitions, that just means planning as inference. That's part and parcel
of active inference and just being existing in a world that you're continually sampling
and it's continually generating data for you to make sense of, you know, there's a circular
causality there. The twist here, the qualification, is that I think you're absolutely right that in
we do switch between active and perceptual inference in reality. So, if I was building an
artifact, I probably wouldn't worry about what I'm about to say, but if I wanted to describe
you and me, I would describe you and me as basically machines that are built to,
in a solitary way, in an intermittent way, alternatively, plan an action and execute it
and then gather some data and then make perceptual inferences or, you know,
do the perceptual learning side of things. So, I mean this in the sense that the way that we,
the temporal scheduling and the alternation between the action and the perception or the
active inference and the perceptual inference, the way that it seems to be scheduled and
organized in biological artifacts, not like ourselves, is that every 250 milliseconds,
about four times a second, we act and within those actions, we do our perceptual synthesis.
So, this, you'll see this, for example, every 250 milliseconds, about four times a second,
you'll move your eyes with stochastic eye movements. So, you're getting little snapshots
every 250 milliseconds at four hertz and during the acquisition, during the action,
you actually switch off your sensory channels, it's called stochastic expression.
If you're an engineer, you're basically turning the Kalman gain down to zero
when you're doing the action bit. So, you don't see the optical flow induced by your eye movements,
you just don't see that. What you see is the thing that you've actually fixated on and you
use that to construct a scene, you assimilate, you build up this illusion that there's a visual
scene out there, but in fact, in reality, there are little snapshots from here and here at about
four hertz. The way that you speak, if you think about, or just listen to me, the way that you
hear things, well, that's not quite so true, but certainly the way that you
fulfill your proprioceptive predictions during speech, all my phonemes are roughly 250 milliseconds
long. So, I am just producing a discrete sequence and little packages of phonemes
every 250 milliseconds. If I was a mouse, I would sample my world by whisking my whiskers
and the frequency of that is 250 milliseconds or four hertz. And if you now look at the belief
updating on the inside, which would correspond to the perceptual inference making sense of
the data sampled by my whisking around my burrow, you see again this profound theta rhythm that
dominates an active exploration of the environment, that within it is nested all the fast perceptual
belief updating usually at what's called a gamma frequency, about 40 hertz. So, you've got this
lovely separation of time scales where we make a move on the world four times a second. We get
some data and then we quickly process what that means. When we've reached a conclusion, then we
go off and get the next bit of data and then we process this. So, for artifacts like you and me,
there is actually, I think, a temporal separation of the active part and the perceptual part of
inference. So, I'm waiting to see until they build that into robots because I think you get very
realistic behaviors and the way that people respond to things and orientate to things or
understand things through this sort of discretization or quantization of discrete packets of information
that live within, say, a few hundred milliseconds and takes the temporal scheduling of at least
biological belief updating. Wow, unbelievable. That was amazing. So, it feels like there is no
parallelization. There's actually very fast switching between the two, like a joint optimization.
As I say, if I was building a computer, I wouldn't do that. Obviously,
there's a better way of doing it and evolution has found that this fast switching is the best way to
do it. To explain the kinds of worlds in which we live in, where biological motion has certain
time constants, where meteorological events or things around us change to a certain time scale,
it's clearly the most efficient way according to evolution anyway. Oh, yeah.
Absolutely. Wow, that was amazing. So, we've kept talking about active inference and
of all the actions that I could choose, I'm choosing the one that on average would maximize
or say minimize the surprise in the future or minimize free energy.
I'm trying to think as to, so first of all, it requires, I suppose, I don't know whether the
word is, my wording is right or wrong, but some sort of a knowledge about the future, as you said,
it requires looking to the future. I can't help but notice that there was a similarity between
active inference and reinforcement learning. However, in reinforcement learning, what we do
is we have the ultimate, like we have an end goal, an end state, and we just let the machine
do whatever it wants to do, like exploration, exploitation, whatever it wants to do,
many, many, many times. So, eventually, it has a way to back propagate the expected reward to
the current state, but a human being that say it's experiencing something for the first time
and wants to choose the action, it doesn't have that option, right? So, first of all,
how would you differentiate between active inference and reinforcement learning?
And second of all, let's just discuss this, I forgot the second question, let's just discuss
this first, the difference between active inference and reinforcement learning, yeah.
But I relieve because that's a very big question. Feel free to prepare the next one while we're
dealing with this. Okay, perfect. So, there are a number of ways of answering that question and
the simplest way, of course, is to look at what is on the tin when you pick up active inference
versus reinforcement learning. One's about inference and one is about learning. So, active
inference has an inference and learning, which has the important consequence that you have
beliefs not just about the states of the world, but also beliefs about the parameters of your
geratin model. So, it comes equipped with beliefs about the W's in say a neural network.
Reinforcement learning is just about learning the parameters. You could look at reinforcement
learning as a mechanism to enable machines to learn to infer and in part, I think that's how
people often understand inference in the context of machine learning. But strictly speaking,
the optimization process is applied to the weights, the parameters of a geratin model,
the contingencies. So, there's one fundamental difference between reinforcement learning
and active inference in the context of belief updating. I don't think that's quite the spirit
of your question though. So, I think another way of looking at the difference between active inference
and reinforcement learning is just in terms of the teleology and asking what is it that you
want to optimize. So, reinforcement learning, you assume the existence of a reward function,
a loss function, a cost function, a value function, and then you optimize your neural
network or your artifact to respond to any given either explicit state or inferred state
with a particular action and that action is chosen by the Bellman optimality principle
to maximize the expected reward. So, that, please suppose, is the existence of a value function
and if that exists then by the Bellman optimality principle you can then
evince an optimal state action policy. Can that be, if you like, framed in terms of active
inference? The answer is yes but as a very, very special case. So, what is that special case?
Well, it's a special case where you can reduce the function that is implicit in active inference,
which is a functional of beliefs. So, remember your planning as inference,
belief updating in terms of perceptual inference and data assimilation, evidence accumulation,
and all that good stuff, Kalman filtering, and the action selection by planning as inference.
This is inference. So, the objective function is a function, a function of a function or a
functional and that function is a belief. So, the objective function in active inference
is belief-based. So, under what conditions could you actually reduce an active inference
formulation of a problem to the kind of problem the Bellman optimality principle would apply,
was when you shrink your belief down to have zero uncertainty. So, if you take uncertainty off the
table then you can get back to effectively the situations in which you can apply the Bellman
optimality principle. Interestingly, just for your entertainment, the way that you do that
technically is just by describing your reward as a log prior probability of some outcome that you
prefer. So, we often call it a prior preference. So, we say that every outcome is equipped with
a reward or a loss function. So, it's sort of having two separate channels, sensory information
reinforcement information. We simplify the problem and say no, every output, every observation
or sensory input comes labeled with a reward in aspect. So, your reward function now becomes
very high dimensional and continuous and it's just basically the log probability of a priori
means something, that outcome. And once you write that down and you take away uncertainty,
then you can apply the Bellman optimality principle. So, you can look at reinforcement learning as a
special case of active inference when there's no uncertainty. The problem with doing that though
is if there's no uncertainty, that means that the curiosity part of the objective, the expected
free energy is zero, which means that you've lost the opportunity now to simulate artificial
curiosity and exactly the kind of sentient behavior that we were talking about half an hour ago.
Because there is no information to be gained, there is no information seeking at hand,
because you know everything. Why do you know everything? Well, because in my model of the
world, in my reinforcement learning model of the world, there is no uncertainty. I know everything.
I can see all the hidden states or at least I can infer them. So, what you lose by converting
an objective function, which is a functional of beliefs, which is the
variational free energy or the marginal likelihood or the model evidence, all of these
are usually logarithms of or KL divergences or free energy functionals of probability
distributions, beliefs. If you're now trying to coerce that into a function of states as opposed
to beliefs about states, you take away all the epistemics, you preclude any proper discussion
or treatment or account of artificial curiosity in your robotics as the intrinsic motivation.
So, you know, the intrinsic value, the value of information, the epistemic value
of making that move. What would happen if I did that? Has its own, if you like, epistemic reward,
but that part of the reward function also, more explicitly, that part of the expected free energy
or the expected evidence or the expected surprise. That part has been removed when you dispense with
or you try to cast the problem in terms of the Bellman optimality principle.
Just a little technical, interesting technical twist here.
When you consider the full problem where you're dealing with
functionals or probability distributions for our spacing beliefs, you are technically dealing with
usually path integrals because, as we've spoken about before, our paths go into the future.
So, the plan has a trajectory or a path into the future. So, now you've got a path integral
of a functional of a long, basically a long probability, which is an energy.
So, you've got a path integral of an energy, which is called an action, and you try to minimize
expected free energy, which is this action. So, all you're saying is that I can explain
sentient behavior with Hamilton's principle of least action. So, now you're replacing the Bellman
optimality principle, which applies to value functions of states with Hamilton's principle
of least action, which deals with, if you like, value functionals of belief states or beliefs
about states. So, that's for me how I would see that there's a fundamental difference between
reinforcement learning and active inference, but it's not a difference that should be overly
celebrated in the sense you can get from one to the other. You can repair that dialect if you like,
as long as you're prepared to take all the reducible uncertainty
out of the game or off the table. Does that kind of answer you?
Yeah, yeah. I'm trying to understand one bit of this. So, when you say that in reinforcement
learning, we actually know there's certainty. In reinforcement learning, sometimes we are not
aware of the world. Like many things could happen that could put obstacles on the path of our agent
that could happen, but then the agent somehow learns to get out of them. I don't get the idea of
not having uncertainty in reinforcement learning. I didn't get that part.
To say that there is no uncertainty in reinforcement learning would be disingenuous.
I think possibly, for me, most easily understood in terms of reinforcement learning
being one application of Bayes' optimality in the context of optimal Bayesian decision making
with a loss function. So, it was really the nature of the loss function I was talking about.
What I was trying to get across was that Bayesian decision theoretic approaches
don't have as part of the loss function any reduction of uncertainty. So, you have to build
that in by hand, so that normality bonuses or exploration bonuses and things. So, you have to
apply some widgets or little tricks to make your loss function even treated under uncertainty in
the Bayesian decision theoretic context, or say partially observe Markov decision process.
You have to actually write in what you get for free if you start with an expected free energy
or expected marginal likelihood. So, it's a kind of uncertainty that matters in terms of what I
believe I was talking about. So, it's a difference between a neural network that's trying to
be on target for as much time as it can in a noisy and certain situation, for example,
and a neural network that knows it doesn't know certain things that would enable it to find out
what particular times it should be aiming for the target or from what particular direction,
because there are certain unknowns out there that it can actually resolve with particular
epistemic moves that have this intrinsic value or this intrinsic motivation
that give it this curious aspect. So, it contextualizes the optimal Bayesian decision theory
exactly by adding in the optimal Bayesian design principle we're talking about before,
which is this sort of information seeking aspect. So, the full picture from my perspective,
perspective of the free energy principle is that you can decompose the objective function
of the expected free energy into two parts, literally linearly, into an epistemic part
and the sort of reward or pragmatic part. And it's the epistemic part that disappears when you
go straight for optimizing the reward function or your priorities. There's an interesting
intermediate case, which you can get at this by rearranging certain terms in the
expected free energy function, which is KL control. So, this would be like risk-sensitive
control. So, I'm not sure whether you put this under reinforcement learning or not.
So, I'd have to ask you, but so from my point of view, this is an interesting middle ground
where you've now included uncertainty about the consequences of my action into the game. So,
instead of now just trying to maximize the expected utility of expected loss function
on the expected reward under some belief about the consequences of action, I'm now
going to minimize the KL divergence or the difference between what I think will happen,
the outcomes that I under my posterior predictive density, technically, but more intuitively,
what I think the outcomes will be if I take this plan or this course of action,
and then I have this set of preferences that encode my preferred outcomes, and then I just
choose the action that minimizes the KL divergence between the two. So, this has uncertainty
in it that is not quite the full uncertainty reduction, but it certainly accommodates a degree
of uncertainty when it comes to real-world applications. Again, I'm thinking about sort
of engineering applications in control theory, where this would be known as KL control,
and economics, it's called risk-sensitive control. So, that would be like a finessed
reinforcement learning value maximizing scheme that's got an uncertainty baked into it.
That's an interesting halfway house between the sort of vanilla expected utility,
which I'm reading as reinforcement learning, possibly unfairly, and the full of active
influence expected for the energy. It's a really great game if you have time in terms of just
writing down the probability distributions and just rearranging them to different kinds
of divergences, and just trying to read them as if you're in a columnist, or you're an engineer,
control theoretician, or you're a behavioral psychologist doing reinforcement learning,
or you're a neuro-roboticist, developmental neuro-roboticist trying to understand intrinsic
motivation. But by switching them around, you can get all sorts of interesting different perspectives
of this overall expected surprise minimization, like objective function.
I see. That was really amazing. So, we talked about perceptual inference and active inference.
Now, both of those ideas highly rely on the environment. One is modeling the environment,
and the other one chooses actions that would maximize the probability of my existence.
Both of them rely on the environment, but what about when we come up with some
abstract knowledge that never existed before? The first time that someone thought about
