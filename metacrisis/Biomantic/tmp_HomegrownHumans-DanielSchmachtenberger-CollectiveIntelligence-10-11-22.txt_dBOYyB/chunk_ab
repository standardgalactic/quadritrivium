people that were running a conspiracy against it, I see those as equally concerning because I see
them both as kind of conspiracy isn't the essence of what I'm focused on. What I'm focused on is
where people's sense making is getting more captured by some narrative.
And so the mechanisms that I would say are in place is one, increasing complexity makes it
impossible for anybody to process all the information. So they have to decide some things
about who do they trust. And of course, you want to trust the CDC or a trustworthy institution,
but if you don't, then you want to trust whoever seems like they're doing the research of why you
can't trust them. And so you get that kind of mercy because who the fuck is going to go through
all the CDC data and then compare it with the VAERS database data and then compare it with
this investigative journalist, not to mention the IPCC and climate change and the social justice
cop shooting things and to have a self informed opinion on all these things. And then to question,
is the data even good or was there underlying bias in the nature of what was funded in the
research? And so all of the data is on one part of a bell curve unevenly distributed. And so
I would say the just sheer work and epistemological nuance it takes to make sense of things as
complex rules almost everybody out. And then the consequentiality of it makes you want to not just
sit in uncertainty, but have some sense of it. And then like the emotional in group out group
polarization around like, and this is again, one of the social media things that like anyone who's
questioning if the vaccine was the right approach for public health can get thrown into some like
tin foil hat wearing anti-vaxxer thing, like that they're they everybody's going to die of
whooping cough and smallpox because of these fucking crazy people. When some of them are that
and some of them have a pretty nuanced view of like, Hey, I think there's more that we could have
done on small molecule research. And there are reasons why doing that small molecule research
would have made sense. And there's more safety analysis on vaccines. And so there's something
where it's like, we see the craziest left pink hair dantifa people. And we see the like dumbest,
most bothersome MAGA people as if representative of the whole groups that then makes the groups
polarize against each other. Yeah. And so I think the complexity, the total number of topics,
the speed, the crappiness of the underlying scientific substrate with replication issues,
and all of those things, the media environment that orients towards the hypernormal stimuli,
which will be confirming bias and sanctimony and in group identity, and giving us the most like
extreme versions of the other, the extreme and straw man, never steel man versions of the other
side. I think that whole suite of things leads to this where the number of people that I knew
who I felt like were doing really a good job of sense making say COVID related issues,
whether it was lab leak, or whether it was proposed small molecules like ivermectin,
or whether it was vaccines, or I saw some people at the very beginning doing a good
job of not getting captured by one side or the other. Yeah, almost all got progressively
captured by one of the sides, simply because their their ability to pay attention to everything,
they just couldn't. And so certain choices ended up making the new suite of information coming
their way more oriented in a certain direction. And it became very hard to not be affected by that.
Yeah, I always think of sort of almost like a World War One, you know, no man's land trench warfare,
and it feels like the trenches are getting pulled further and further apart. And I'm sure you've
got the accurate stats, but I feel like it's like less than 5% of all social media posters or the
majority like make 90% of the news. So it's all the it is all the extremes. And as the no man's
land gets bigger and bigger and more and more treacherous tank traps and barbed wire and nerve
gas, there's no fucking way you're going to make the sprint to the far side to jump in that foxhole
with your potential enemies. So like no man's land is dead. So I you know, there are no atheists
in foxholes, right? So you got to believe something if you're jumping in with and you're going to and
you're more prone to jump back to your closest affinity group than you are to risk crossing
the middle. And so it just seems like that that is that the price of defection from my
identified in group is so high at this point. So for instance, you could have a whole bunch of
live and let live middle Americans that are nonetheless getting pulled aggressively right
wood, because the pink haired baby killing and Tifa, you know, hooligans are such that are such
the caricature of the left at this point, that people are like, I don't know which way to go,
but there's the moderate middle is just getting completely hollowed out. I actually don't have
these stats Tristan and Isa were telling them to me, but it had to do with a really cool social
science that was the center for humane tech folks for anybody. Yeah. Yeah. And it had to do with
pulling people on the left about what they expected the percentage of certain things on the right
were and vice versa and seeing how badly off in a negative direction they were, something like
asking people on the right. And I think it was more specific. It was like asking Trump supporters
at a rally what percentage of Democrats are LGBTQ themselves. And they thought it was like
half or something like that. And it turns out was like less than 6%. And there were similar
things that people on the left, you know, what percentage of people on the right want to cut
all social services or whatever. And they estimate it to be way higher than it actually was. So you
could tell that they were both dealing with caricatures of each other that they thought were
true that like they were dealing with two standard deviations to one side of the Gaussian distribution
of what was really there thinking it was the median. And a Gaussian distribution is fundamentally
the familiar bell curve that people understand. Yeah. Yeah. So and this is, you know, Pat Ryan's
stuff he was talking about this years ago on auto colts that you would he was predicting that
that we'd have this phenomena that things like Colts would just start automatically popping up
as a function of the information technology landscape. Because when and I would say the
underlying cause of this or one thing we have to factor is that of all of the singularities that
Kurzweil talked about, the info singularity is the first one, meaning the the singularity,
meaning the point at which things change so radically that we can't really predict what's
going to happen or how to respond to them afterwards and where humans kind of can't keep
up with the pace of technological change. The idea of the information singularity is that so much
information would be published that no human could process all the relevant information about
anything. And so there would no longer be an expert. And that's already true, right? It's
already true that you'll have more peer reviewed journal articles within a specific domain because
of how big the world is and universities all around the world and any expert can keep up with
even in their own domain and obviously way more journalism on topics. And so obviously now we
have to get into computational processing of all of that since the human can't process it anymore.
But now it's well, what are the algorithms and how we're doing that computational processing?
And currently the answer is things like newsfeed algorithms, Facebook and other newsfeed algorithms
at least deciding of all the stuff out there what makes it into your feed because you're not
going to see a million news articles in the day. So the ones that make it in are based on personalized
data harvesting about you mostly focused on not trying to make you believe something but what
you actually interact with the most. But the tricky thing is what you interact with the most is mostly
based on one marshmallow stuff, right? Mostly based on like what actually makes you click with it
because the title was so salacious and effective at making you click or so many of your friends already
had or something like that. And so that phenomenon means everybody doesn't see the same thing because
there's too much stuff everybody can't see the same thing. And so what everybody sees is the stuff
that is AI split test optimized in the direction of their twitchiness, right? I think twitching
is a term you use it. And that usually means in the direction of their confirmation bias because
people are less likely to click on a thing that uses words they don't even know what they are,
which means likely to expand their uncertainty set than things that are already primed to them
as something bothersome. And so this is not just a left-right anymore thing, it's like it's a
algorithmic basis of cult generation. And then one of the really fucked up things I don't know if
you, of course you saw, you pointed out to me, I had these deep fakes made about me these audio
deep fakes. Was it catabytes? Is that that was that the term? Catamite. So there's a third one
that just came up. I am a biracial colonialist in this third one. I'm large, I contain multitudes,
you know, I can have my contradictions. So yeah, catamite. I mean, just to be clear, this means
like a young male homosexual sex slave for older powerful men, whatever. So there's this
thing with my voice talking to Lex Friedman about me being a catamite, citing a poem about it,
whatever. And of course, to be clear, this is an AI-generated deep fake. I had nothing to do
with it. I didn't authorize it. There's just enough of my voice and Lex's voice, you can make
shit like this. And this, so the first step was the AI to curate all the content, which is the
Facebook algorithm or the YouTube algorithm, whatever, or the Google algorithm. The next one
is to say, you don't even want when you do a Google search to see 8,000 result pages,
or 8 million result pages, you want to see a customized chunk of info for you that read all
of those and calibrated it to you. And that's what GPT-3 can do, right? That's what new advanced
generative text AI can do is read that and say, in a thousand words, summarize X for me.
So when you have, and it can also do it in fucking anybody's voice and pretty soon anybody's
similar to it. So now you imagine the auto-cults where not only are they being attracted by what
they're being attracted now, which is just who all is attracted to the same memetic twitchy thing,
but where you can actually produce bespoke content from bespoke content producers in that space.
So you, you can actually create gods of the auto-cults. That's all very weird.
It's super weird, but also like, I'm trying to think, so I think it was a GPT-3,
an AI kind of generating algorithm, and somebody was already starting to use, I mean,
A, talk about race to the bottom and whatever that law is that like any tech online will be used to
make porn first. It was the equivalent of that, but it was for info marketers. So there were info
marketers who were already trying to explore using AI, cutting-edge AI tech, to generate better
info marketing sales copy. Of course. Right. But like anybody who's seen that rash of Dahle,
you know, everybody and their mother posting there on their social feeds, I said this,
here are my five ingredients and here's my piece of art, right? But like from, from AI generation
to speak of like the auto-cults and even creating the deities, right? You could not only do it in
the name of, you know, like give me a summation of quantum physics and as Stephen Hawking's or
Richard Feynman would say it, right? But you could even do it as Jesus, as Buddha, as Lao Tzu,
right? You could actually literally give the AI the coding to give voice to our divine archetypes,
which to me seems super fascinating, but also wildly spooky. Okay. So totally spooky,
will get abused. This, there's an important principle about tech that is related to the
thing you said about, um, it'll be used for porn first or then internet marketing, which is
any new technology enables new things, right? That's what technology does is enable human
seduction that they weren't able to do. It will be used for all the things where there is incentive
to use it. And where there is more incentive and return on that incentive, it'll get used the most
there. So whenever somebody's developing tech, they might be developing it for a certain purpose
that seems really lovely. But what they have to realize is it's going to be used for every
purpose that is not regulated that there is incentive to use it for. And it will be
differentially used the most by the ones that have the most incentive to use it. And so when
we're developing new tech, we have to take that really fucking seriously when we're dealing with
tech that makes changes to the world as quickly as powerfully and at scale and as irreversibly as
the new tech does. So you develop CRISPR, where you meaning you develop some gene editing technology
where you have ethical review boards that you have to go through with lots of good scientists on
them to assume this is to assure this is a good thing to do in a biosecure laboratory. But then
you publish your paper and you're doing it for immuno oncology, right? To figure out how to edit
genes to cure cancer. But as soon as you for that purpose with that money in that major institution,
figure out, oh, here's how we spliced the gene to do that and you publish it. Now anybody with
really fucking basic hardware and without ethical review boards can do that for any purpose they
have a purpose for. You make AI and make it open source. And you can use the AI for any
purpose to optimize computational processing of lots of stuff. I won't even talk about what
they could be, but people, you know, it's not that hard to think about. So this, I'm actually,
I believe that we are moving into a world where and this will be a radically controversial thing
to say. Good. You, the reason why we don't like the term regulation is because we don't
trust this government or a lot of people don't trust this government or any government structure.
The idea that anything that is powerful enough to regulate will become corrupt.
All the kind of libertarian public choice theory critiques, those are all good and those are all
right. I don't trust it for the same reason, but I also don't trust open market proliferation
of exponential catastrophe tech and with no regulation where, of course, if we're talking
about drones or cyber or biotech or specifically synth bio where like the ability to synthesize
genomes corresponding with the best institutions in the world putting the pandemic grade genomes
that they're figuring out online through publishing and the same with AI, the world just doesn't
make it through that, right? The world doesn't make it through decentralized catastrophe weapons
with no oversight. And you can't regulate after the fact the way we always have, like after DDT
killed so much to finally figure out how to outlaw it. Round up, right? Took 30 plus years
and we're still sorting out forever chemicals. And lead and gasoline and all those things, right?
Lead and gasoline is estimated to have taken a billion IQ points off the world and made us
4x more violent holistically to just stop engine knocking. And that was 4x more violent. Yeah.
The real politic assessment, I'll send you a link on it. There's an incredible explainer
piece somebody did on lead and gasoline as an example of externality to fucking stop engine
knocking. We take something that you have to mind that would never occur in the biosphere and figure
out how to burn it and aerosolize it so that it's in everybody's breathing air that makes us both
dumber and meaner. And so then we get people doing social science and doing this real politic
assessment of humans are too dumb and mean for things like democracy to work or things like
nicest or dork. It's just genetics. We're nasty chimps. It's not fucking nasty chimps and genetics.
It's like shit that we did that we could do differently. And if Hoxon's in the brain piece.
That assessment, the 4x more violent and the a billion IQ points off like literally dumber and
meaner was one chemical, one chemical, right? We have 50 million chemicals in the chemical
database of the American Journal of Chemical, whatever it is. And how many of the other ones
also have negative behavioral effects, let alone on the other side of the fucked up topsoil that
doesn't have the trace minerals that were needed for healthy neurotransmitters and everything else.
So the whole let's study humans under ubiquitous conditioning and then call it nature. And then
say we're too fucked because of ubiquitous conditioning like no, we have to. So I have
massive critiques of humans are this way studies where we studied humans that were all conditioned
by the same things and then took it as inexorable. That was a tangent. Well, I mean, I remember,
wait, the controversial thing I was going to say, we have new categories of tech have to be pre-regulated.
So you can't put let out because the market just figured out how to add it and then only after
enough people die and people figure out how to measure invisible stuff and the policy groups go
against the financial vested interests that are already making billions of dollars on this.
You finally regulate it in one area, but it's hard because other areas are using it. Nobody wants
to be the first one to incur the cost of the regulation. We don't get to do that with AI.
By the time it's causing that much harm or all that or any other exponential existential tech,
right? There's no whoops do overs. Correct. So the world and you also can't do national
regulation because all these things create global effects very quickly. It's not like we can say,
oh, we're going to stop doing gain and function research in the US and that's going to matter.
Global pandemics because of global supply chains, either you get everybody to them or
doesn't matter that much. Same with AI weapons. So now what I'm saying is in the face of the
amount of power of exponential tech, you can't let it out unregulated first because either
some people want to do fucked up stuff with it or simply the accidents, right? It happened.
And you can't regulate it at a national level. So you do have to have something like global
regulation that does safety analysis on the tech to say, okay, these applications,
not all applications, these applications done in this way are a safe way to put it out. And we
continue to watch and iterate to change regulation and change tech design. I'm saying we don't make
