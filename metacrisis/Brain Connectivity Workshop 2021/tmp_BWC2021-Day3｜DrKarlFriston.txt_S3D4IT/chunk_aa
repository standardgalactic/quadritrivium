We are however at the end finishing with Karl Friston whom I'm sure you all know so I won't
introduce him further. Is there a unifying theory to unify unifying theories so Karl
look very much forward to you unpacking this for us thank you. So it's a great pleasure to
wind up this brilliant workshop. Normally my presentation is taken as a sort of light
entertainment and as I said yesterday it usually degrades into a debrief about how well we've done
so I'm assuming that at some point we're going to get bored of talking about
whole unifying brain theories and I'm going to pass back to Michael who I presume will then pass
back to Randy just to sort of bring things to closure but in the interim I'm going to try
and answer the question and I should say that this title is a wonderful title it is not my title
it's something that Randy spent at least two weekends thinking about and crafting for me
and I've taken it very very seriously so I've done my best to try and find the theoretical frame
that will happily embrace and comfort and validate everybody else's theories not just the
theories that we've been hearing about today but all other theories that could be brought to the
table to explain how we work so if you all enjoy that lovely photograph we'll move on to
slide one so the overarching notion is basically that we are creatures that are compelled to
garner or search out evidence for our existence and where our existence is entailed by our
generative models of the world and I'm articulating that just by saying that there are some internal
brain states which represent expectations about hidden states outside the world and we have some
control variables or action variables you both of which are in the service of maximizing the
probability of any observable outcomes under a given model entailed by our brains and we can
split that into perception and action why is this unifying well if you read that probability of the
outcomes under a model as describing the kinds of outcomes that I as this particular phenotype
expect to encounter then we can read this log of the probability as value as the utility of
particular outcomes that then endorses a description of our active engagement with
the world in terms of reinforcement learning optimal control theory with a lot about control
theoretic approaches but they rest upon a specification of this value function of some
outcomes and the economics it will be expected utility theory and this quantity here is going to
be approximated with free energy which is where the free energy principle gets into the game but
it's usually the free energy is articulated more from the point of view of information theory
where the negative value becomes self-information, surprise or surprise and the free energy now
becomes a bound upon i.e. in this instance the negative free energy is always greater than
the improbability of some outcome that would be very surprising if an outcome occurred to me.
From that we can license or talk about the principle of maximum utility information,
the infamax principle, principles of maximum efficiency, minimum redundancy from Horace Barlow
and indeed the free energy principle. We've seen inspiration from Herman Haykins explicitly on
paper and also implicitly in terms of some of the work that Victor has drawn our attention to
that can be easily motivated by noting the time average of this self-information or surprise's
entropy which means action and perception on this reading of self-evidencing is really in the
service of minimizing the entropy of outcomes so we're talking about the holy grail of self-organization
formally articulated in terms of synergetics and laser physics but if you are a physiologist
we're just talking about homeostasis is keeping outcomes within viable physiological bounds and
and then finally if you're a statistician or from machine learning you would read this quantity as a
probability of some data observable data under a model a Bayesian model and that would be called
Bayesian model evidence and from that we can spin off the Bayesian brain evidence accumulation
and predictive coding on all that good stuff under the watchful eye of Herman Helmholtz.
So what is this evidence? It's usually written down in terms of a variational bound and then
there's an elbow machine learning evidence lower bound or a free energy bound that is simply the
log probability of outcomes given a model of how they were generated but is formed by taking the
thing we want and just putting something that can never be less than zero on top of it to bound it
and that's the divergence between our beliefs cue about states of the affairs generating data
and the true posterior and what's there given I could observe those data and I'm writing that down
just to motivate why this is potentially a very important generic and almost universal way
of defining optimality you've got a gerative model there that leads you to expandable artificial
intelligence if you're in a pragmatic setting you have a principal account of design optimality
and what we'll see in a second is also the way that you go and gather data that the underwrites
are abductive inferences and this cartoon that here with in terms of data foraging and
epistemics but what I want to do is just rearrange those terms at the top
to provide another interpretation which I think is much closer to some of the questions and issues
that we've been contending with I can rearrange those expressions and express evidence or
evidence bound in terms of accuracy and complexity so this is the sort of like the fitness
and you could think of this as basically the the flexibility versus robustness I hope I've
got the right words from Randy's talk in the sense that you know if you want a robust explanation
of your world then that has to be fairly accurate but at the same time it has to be expressive
so this is an interesting relationship because the evidence is the difference between accuracy
and complexity which means that if you want to provide an accurate account of the world
you are compelled to do so in the minimally complex way so what does this complexity term
look like well it's just an expression of Occam's principle in effect finding the simplest explanation
at hand that affords an accurate explanation of some observed outcomes it is interesting also
from the computer science point of view because it scores the degree of belief updating as you move
from your prior beliefs to your posterior beliefs in the light of any new sensory evidence or any
new data which costs both in terms of the computational cost but also in terms of the
thermodynamic cost so there's via the Josinski equality and Landau's principle for every belief
update that is measurable you know say with EEG or fMRI there will be a number of jewels
that you have to expend to do that belief updating so this is coming to Susan's
recurrent focus on the actual thermodynamic and metabolic cost of belief updating and there's a
very simple relationship it just scores the complexity but also the complexity also holds
the expressivity of the model so as you're providing more and more accurate accounts you'll need a more
complex model you need more expressive model so this speaks to this you know many of the talks
not for example I'm going to miss a whole bunch of examples here but what is in my mind is this
sort of notion of hidden repertoires that could be brought to the table when apt explanations for
this particular context in an expressive way another example of this is the increasing hierarchical
depth of generative models endowed both by evolution but also by neurodevelopment in terms
of pruning synaptic homeostasis that lead to these sparse structures and why is sparse important
well it's just an expression of paying avoiding overly complex overly parameterized generative
models if you read synaptic connections as the parameters of those of those generative models
so a lot of if you like the imperatives which would seem beautifully unpacked for the pruning
and the simplification of generative models and their implicit thermodynamic efficacy are basically
written in just the definition mathematical definition of model evidence so I just want to
close by saying that these principles also apply in relation to actively engaging with the world
so that the imperatives to maximize your the accuracy of the explanations whilst minimizing
their complexity translate into effectively maximizing the expected accuracy in terms of optimal
Bayesian decisions whilst the complexity term translates into a
minimizing a cost in the sense of Bayes optimal decisions as opposed to design which is getting
the right kind of data to minimize your minimize your uncertainty about the causes of your data so
just written that out formally here just re-expressing the evidence bound in terms of
a mixture of accuracy and complexity and asking what would this quantity or these imperatives
so written out look like if I didn't actually have the outcomes but I knew what they might
look like under some policy pie some action or move on the world and just by taking the expectations
you can conceive of complexity and accuracy mapping to risk and ambiguity so if I want to
minimize this expected free energy I'm effectively minimizing risk and ambiguity and in terms of a
unifying theory of unifying theories this actually gracefully accommodates a number of
takes on the way that we epistemically forage the world so if I ignore this extrinsic value here
then we're left with this thing which is just the information gain or the reduction of uncertainty
that I would get if I looked over there or made that move in the visual search literature it's
called base in surprise an information theory it's just a mutual information between the causes
and consequences consequent on making that move as opposed to that move if I take another kind of
uncertainty off the table namely the ambiguity I'm left with risk sensitive control which is just
basically scoring the difference between states of affairs out there if I make that behavior
relative to my prior preferences so we've come back this is if you like a grown-ups version of
optional control theory where you're putting on a certain kind of uncertainty into the game
namely the risk if I take away all uncertainty I'm just left with this quantity which is
the the expected value and we have good old fashioned Bellarmine optimality principles and
expected utility theory so I'll close now by trying to say well why is this useful for framing
all the kind of mechanics that we've been talking about network theory graph theory and the like
well I've been talking about generative models if you subscribe to the notion where all in the
game of maximizing the evidence for our models of the world that requires that to be a model or a
generative model you can always write down a generative model in terms of a probabilistic
graphical model if you can do that you can always write that down in terms of a factor graph what
the factor graph gives you is the requisite message passing required to invert or make sense of any
observable outcomes oh here by essentially inverting that generative model in a Bayesian sense which
means that at the heart of understanding computational anatomies and the architectures
on which they live is a graph and implicitly an appeal to graph their formulations of
message passing on graphs that usually are very deep graphs so I just included this as a nod to
the importance of the hierarchical aspects of graphs that we talked about and also as you go
deeper into the hierarchy things you get a separation of temporal scales which we've also
heard about in terms of fast versus slow and their age dependencies you get that essentially for
free just by building these little factor graphs on top of each other or putting slow
transitions and flows on top of faster things lower down at the hierarchical the hierarchical
levels the off-the-shelf message passing schemes that would be apt for those kinds of factor graphs
look very much like the the neural mass models that people use to simulate activity and indeed
understand neural message passing terms perception associative passivity and learning terms and the
parameters of the generative model and indeed action selection you can carve out rough functional
anatomies based upon the functional form of the message passing implied by the factor graph
that inherits from a specification of your hierarchical or deep generative model.
This is the first slide so I've only officially used five slides but what I wanted to do I start off
by saying well look there's an imperative to maximize evidence for our models of the world
how do we actually do that well we do it with these structured flows on manifolds where
those flows via the Helmholtz decomposition can be decomposed into a gradient flow that
optimizes the model evidence the log probability of outcomes given a model that's denoted by gamma
here but is always accompanied in virtue of the Helmholtz decomposition with this irritational
nondisputative or solenoidal flow and this is the thing that does the mixing and of course if you
put this mixing into that hierarchical generative model then you've got hierarchical mixing I've
never used those words before so I hope that Gustavo is there and smiling in the background
and so that would be if you like the practical way of putting these this message passing
on factor graphs into a real-world setting where we now understand neuronal dynamics
as structured flows on manifolds that I repeat are a statistical manifold and therefore equipped
with an information geometry so I'll finish there give the last word to Einstein just as
a nod to remind you that evidence is just simplicity plus accuracy namely everything
should be made as simple as possible but not simpler and with that I just need to thank
those people whose ideas I've been talking about and of course thank you for your attention thank
you very much indeed well once again Carl in the virtual age you just get
the chair clapping but sure there's a chorus of hundreds 208 people who very much enjoyed
the unifying theory of unifying theories just two things come to mind for me while others
collect their thoughts and I know you've already thought and written about this but I'll ask the
question anyway and that is you know given the discussion that Susan just initiated about metabolic
energy and then you're been discussing sort of surprise there's sort of information free energy
how close in terms of a you know omnipresent unifying theory is Gibbs free energy which is all
about work and energy and jewels and things that we can measure with you know thermometers
and um christianian free energy are they are they you know is it always that there's a
metabolic cost that scales in proportion to this um suprisal term in your formulation
you know through synaptic chatter and synaptic plasticity or is that really just a metaphorical
but not necessarily a one-on-one mapping I'm interested in your thoughts on that
you know my thoughts are very simple it's a mathematical isomorphism so very very clearly
any movement on that statistical manifold any belief up a date has a very precise and
exact thermodynamic cost if you measure in jewels you just need to multiply
the distance you move the degree of belief updating and by Boltzmann's constant and temperature
and that will give you in jewels the amount of energy you've used to change your mind
so you know the the beauty of sort of deriving the statistical physics of inference and working
you know sort of on statistical manifolds is that the maths that underwrites the Helmholtz
and Gibbs free energy is exactly the same as the variational free energy you know it's it's
exactly the same maths and I repeat via the via the Jynnski equality you can you can you can
actually compute if you change your mind from this to that in the face of new data you can
compute the thermodynamic cost of that which is the computational complexity
which tells you that a good computer should be very very small very cool and just work on batteries
okay okay so there's some questions appearing in the Q&A and Sofia Zangila would you like and then
a follow-up by Thomas would you like to ask these by unmuting yourself or should I read them out
okay I'll read them out then is the search for one unifying theory motivated only from our
beliefs that such a theory exists or should exist or is there a logical experimental rationale that
supports unifying theory I guess so we just seeking simpler explanations this is a sort of
ontological question I guess or is there really a simple unifying theory out there that we're
approximating with our cognitive beliefs I'm not sure I'm qualified to answer that but you know
if you pursue the free energy principle that's exactly right and you'll use the word approximate I
think is actually very very very appropriate in this context so notice that that free energies
are bound on evidence it's not exact inference it's it's approximate Bayesian inference it's
bounded rationality that is doable and realizable in the physical system and if we as scientists
are compelled to find very simple explanations of the world then then yes you are you you are obliged
to find a unifying theory that's as simple as possible well that's a bit of a philosophical
reason you should you should answer that you should find or find a philosopher who can answer that
question yeah well I mean the complexity of the microscopic movement of the universe has to
collapse onto these synchronization manifolds or these structured flows on on manifolds or
you know we'd never have figured any of it out I mean you know if we were dealing with the
microscopic equations I mean this is statistical thermodynamics you know if the theory was no
more than a metaphor then we would be dealing with the microscopic degrees of freedom and and
everything would be utterly incomprehensible so there definitely are these Herman Harkin
enslaving principles and these things out there that support life and support meaningful action
and support comprehensibility so Thomas Vali your skeptical that evolution would would have a
selection pressure um that would evolve a system to have an aesthetically pleasing solution so I
just kind of must have read that and tried to make a counterfactual out of it but um do you want to
do you want to speak to a contrary view?
I do although I'd like to hear your answer to that the reason I do is I think it nicely speaks to
