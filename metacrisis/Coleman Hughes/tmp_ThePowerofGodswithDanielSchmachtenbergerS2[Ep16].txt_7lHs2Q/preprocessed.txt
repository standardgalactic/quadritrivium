THE END
Welcome to another episode of Conversations with Coleman.
If you're hearing this, then you're on the public feed, which means you'll get episodes
a week after they come out and you'll hear advertisements.
You can gain access to the subscriber feed by going to ColemanHughes.org and becoming
a supporter.
This means you'll have access to episodes a week early, you'll never hear ads, and
you'll get access to bonus Q&A episodes.
You can also support me by liking and subscribing on YouTube and sharing the show with friends
and family.
As always, thank you so much for your support.
My guest today is Daniel Schmockdenberger.
Daniel Schmockdenberger is a founding member of the Consilience Project, aimed at improving
public sense-making and dialogue.
His writing is focused on ways of improving the health and development of individuals
in society, with a focus on catastrophic and existential risk, civilization and institutional
decay, collective action problems, social organization theories, and the relevant domains
in philosophy and science.
Daniel and I spend a lot of time talking about the effect of new technology on our ability
to problem solve as a society.
We talk about the disadvantages of democracy relative to autocracy.
We talk about how much of our behavior can be attributed to human nature and how much
to culture, and much more.
So without further ado, Daniel Schmockdenberger.
Okay, Daniel, thanks so much for coming on my show.
Thanks for having me here.
I'm looking forward to our conversation.
Yeah, there's a lot to talk about, but before we start, can you give my audience a sense
of who you are and what you do?
I think that we'll come across in the conversation, hopefully.
I can say a few things I'm interested in.
I've been interested since I was young in all the things that seemed like problems in
the world that mattered, that were not getting better with the current problem solving processes
we had that looked like environmental problems and social problems, and I'm particularly
interested in why it is that we haven't made more progress with the Sustainable Development
Goals or Nuclear Disarmament or Preventing Arms Races, and then why catastrophic risks
of lots of types actually increase in the total number of types of catastrophic risk
available and increase in probability seems that there are some underlying coordination
failures and game theoretic dynamics that make our problem solving capacities not adequate
to the problems we currently face.
It seems like rather than more problem solving processes, like we just need more green tech
or more laws within nation state governments or things like that, that there is some deeper
analysis of what gives rise to the nature of why our tech causes environmental and social
issues or the way we utilize it does and catastrophic risks, why we don't solve those, and what
new civilizational capacities might need to be, they're adequate to the scope of the technological
capacities that we now are stuck with the effects of and thus have to govern better.
So someone might hear that and feel the tone you're operating in is a bit pessimistic, right?
What do you say to someone who says, well, yes, we have huge problems as a species to contend
with coronavirus being the most current salient example, but look how quickly we came up with
a vaccine relative to what we thought was possible in the past, right?
Are we not, is technology just not, despite some of its side effects, clearly, isn't something
going right if we're able to use technology to come up with a solution to, say, a global
pandemic much more quickly than we thought was possible?
Yeah, I would say there are many schools of thought, but I'll simplify it into kind of
two opposing schools of thought when it comes to technological progress and particularly
kind of the exponentiation in technological process that has started happening with the
industrial revolution and then has really picked up since the digital revolution.
There is the idea that technology and capitalism and basically the results of modernity have
delivered us from, and even longer than that, kind of this Western idea of the dialectic of
progress has made the quality of life continuously better, still problems, but roughly on the whole
better. And that any new problem is just the source of the new challenge that human ingenuity
will rise to and will figure out solutions and we're problem-solving creatures. Of course,
there's always going to be problems, but our problem-solving capacity will keep rising and
that's as it should be in great and very few people would want to trade places with a previous
time in history, not knowing what position you'd be born into. So we should keep focusing on problems,
but in a more kind of optimistic way. I would say that's kind of one school highly oversimplified.
There's another group, which is most of the existential risk world that is looking at the
unique issues that AI risk poses that are nothing like the world ever had to face before.
The unique set of risks that can come from biotech and the ability to
engineer self-replicating technology that might be fundamentally incommensurable with the life
that is here now and how cheap and distributed that technology is becoming.
You know, CRISPR gene drives being kind of universally available now. And so what does
it mean to have distributed catastrophic weapon technology? And the unique set of issues made
possible by cyber attacks, by exponential tech mediated disinformation campaigns,
nanotech, et cetera, and then also a billion people and the cumulative effects of
externality and unremovable use of resource and pollution that are hitting planetary boundaries
and the collision of all of this. And that there is a existential risk landscape unlike anything
the world ever looked at or tended to. The founding fathers didn't have to think of these
things, even the Bretton Woods world didn't have to think of these things. And that it seems like
there's pretty much no way we could make it through because if you look at the way humans
have been historically, pick any empire, pick any kind of time period and say, give people that use
power that way, exponentially more power, where they either directly use it to harm others in war
or use it in ways that indirectly and maybe even unintentionally cause harm to environments or
externalities in some way. And let's just exponentiate externality or exponentiate war.
We just don't seem like good enough stewards of that much power. And this is why most of the
sci-fi is dystopian is it's actually hard to come up with pro-topian sci-fi where you think about
people that are like the people we have been that have that much of decentralized power
that don't blow everything up, even accidentally. So then you say, well, if it's not decentralized,
then that's a different fail state and that goes kind of Orwellian, autocratic.
So I would say both of these are schools of looking at the novelty of the tech space that we have
and the relationship of that with the dynamics of how we have made choices and both, you know,
individual conflict theory, group conflict theory, those types of things.
If I was to say, let's step back and look at both of those because there's truth in both of those
narratives, really important truth about those narratives. You can see really good argumentation
from people you've had on Steven Pinker and kind of Kurzweil, Diamandis on one camp. You can go
talk to Nick Bostrom or Ali Eiser Yudkowski or others on that camp. And you can see that it's
become a more popular conversation recently, not just in academic circles with folks like
Elon Musk saying AI risk is the number one biggest risk to the world and that AI has passed the point
of our ability to probably prevent it becoming an existential risk. So neural link is a hope to
find a solution by the necessity of merging us with AI. Like that's now part of popular conversation.
What I would say is if we take a long arc on human history and we say, homo sapiens have been
around for a few hundred thousand years. And the thing that we call civilization has been around
for something like 10,000 years. We only got the possibility of global catastrophic risk in World
War II for the first time, self-induced, human tech induced global catastrophic risk. And that's,
World War II seems like a long time ago for people who were born after it where it's just
history, but it's a sliver of a moment in evolutionary time. And that was the first time,
and it is important to get that when we look at previous civilizations, none of them still exist
in the form that they were when they were a major empire, whatever, the Roman empire,
the Egyptian, the Mayan, the Aztec, whatever. It's actually, it is the precedent that civilizations
self end up falling and mostly through self-terminating processes. They end up having internal infighting
overwhelm their ability to defend their boundaries well, or they actually environmentally outstripped
their environments and then went through collapse dynamics. So it's just whenever you had a
civilization fall previously, it was a local issue, right? It was a local civilization with a
boundary and there were lots of other civilizations around it. World War II, we got a weapon that
we couldn't really viably use in war, recognizing that multiple sides would use it and have anybody
win. It was the first time of having a really unwinnable war and the ability to actually destroy
the biosphere's ability to support life if we used it. So I would say if people aren't paying enough
attention to how bright a line that is of everything before and everything after, it's really important
to get that because before that, when you study history, so much of the history you study is the
history of major kingdoms or empires warring with each other. And after that, the major empires
can't war and make it through. So we have to come up with some solution other than war.
And so the Bretton Woods world, the post-World War II, how do we figure this out?
World said, okay, nation-state governments aren't enough to prevent World War. We realized that.
We realized that at World War I, made the League of Nations, it wasn't strong enough,
so we've got to make a stronger thing. We're going to make a United Nations, but we also have to make
these other intergovernmental organizations, a World Bank and IMF, a set of trade treaties.
And let's make it to where there's so much economic interdependence across the nations,
that it's more profitable to do trade with each other than it is to war. And where the
supply chains are such that almost everything we use comes from six continents. So if we blow
somebody else up, it messes up our own supply chains, and it binds us, it binds the game
theory to being more useful to cooperate than it is to war. So then that enters a new phase.
And also, one of the key things in that Bretton Woods world was, it will allow us the
application of these vast industrial technologies across the whole world,
coordinating in the division of labor across it, that we can increase global GDP growth much
faster. So everybody, every nation, whatever, can increase its wealth without having to take it
from somebody else. And if you have to take it from someone else, because you don't have a positive
sum game, the negative sum game, or zero sum games go rivalries faster. That was the idea.
But then fast forward 75 years, and we didn't have a major empire's war. We had a Cold War and
we had proxy wars. And we go from having a major part of that was mediated by that there was only
one type of catastrophic weapon that couldn't be used. And there were only two superpowers that
had it. So you could do mutually assured destruction. They could monitor and have that set up.
But with AI weapons, with cyber weapons on infrastructure targets, with
bio, nano, et cetera, and other things, we now have dozens of catastrophic weapons
that dozens of actors, including non-state actors, have. We can't put mutually assured
destruction on a system like that. So that whole system that kept us safe after World War II
can't keep doing that. And we got so much, we got such a complicated global supply chain
through globalization that we got to see with COVID, like one part of the world shuts down
because of a local issue, and you get cascading failures across the whole world. And that was
still a relatively small issue. And so then we start to look at the fragility dynamics that come
from that, as opposed to have local collapse, stay local, when the whole system is radically
interconnected, you also get fragility dynamics. And then we're also hitting, because of that
positive sum GDP happening through unrenewable extraction of resources, humans utilizing them
and then turning them into trash, you can't run an exponential linear materials economy on a finite
planet long term. So we're also hitting planetary boundaries, not just climate change, but species
extinction, overfishing of the oceans, plastics, dead zones in the oceans, blah, blah, blah,
so many things like that. So we're now kind of at the end of the Bretton Woods world where
we can't do mutually shared destruction. The fragility is too high.
We can't keep positive sum growth dynamics with linear materials economy. And yet we also can't
go back to the previous thing where we wore with catastrophic war tech. And so it's like, what is
next? So what I would say is that the tech space does make a change in quantity, right? Like,
basically, a nuke is just a way bigger spear. And if you want to think of it very fundamentally in
terms of like, there's some in group out group dynamics and some ability to use kinetic weapons
to advance facities. But at a certain point, changes in quantity become changes in quality or
changes in type, meaning the same type of governance no longer works. And so what I would say is that
we have had changes in magnitude of issues and speed rate of issues, such that the previous
governance capacities, whether we're talking about markets, governments, idios, whatever,
aren't adequate to the scope of them. And that doesn't mean nothing could be. It means we have to
innovate in the social tech space of what is the social technology that is adequate to govern the
amount of power that physical technology gives us. Yeah. So what I've just heard you say in summary
is that there was essentially a break in history at World War Two for the purposes of this topic.
And that related to the technology of nuclear weaponry. But now we're facing a world that's even
one notch scarier than that, in that we're facing problems similar to what we face with
nuclear weaponry, but on all fronts, on the artificial intelligence front, on the biotech front.
And those problems are actually quite a bit tougher epistemologically because
nuclear bombs are fairly easy to understand relative to, for example, the problem of artificial
intelligence in the abstract. We really don't know, even the smartest people who spend all their
time worrying about this still don't really know what artificial intelligence is going to look like
a hundred years from now. But the problem of just brute force violence is
rather, you can make a nuclear bomb that's a thousand times as powerful as the most powerful
one that exists now. And I understand the nature of that threat. I just picture
an explosion but bigger. I'm not even sure what to picture exactly
with the problem of artificial intelligence. And that makes preempting it that much harder.
It also makes it more difficult to actually worry about it on an emotional level. It's easy
when you're watching a movie and they reduce the threat of artificial intelligence to something
personified, something intuitively scary to humans like killer robots. It's just
anthropomorphization of the problem of artificial intelligence. But the true threats are, I think,
abstract or more abstract than that, but nevertheless scary.
And so where does the idea of sense making and the breakdown of sense making institutions fit into
your view of the problem? Yeah. I want to take one detour first because you actually said something
super important about the fact that the bomb is in some way simpler than the issues that we face
right now or at least simpler to imagine or kind of wrap our heads around forecasting.
It's also the case that nuclear bombs are really, really hard to make
because there's not that many places in the world that have uranium. And then it's really hard to
enrich uranium. And as a result, it can't be an easily distributed technology. It takes
advanced nation-state level capacity to make it so that you are limiting the number of agents that
can play with this thing. And you can also monitor them. So the agents that can mess with it can
spy on each other. And so you can start to have something like mutually assured destruction
be a little bit easier because it's so hard to make them, right? And so where the uranium
mines, where is the stuff going, radioactive tracers, that kind of thing. When we start talking about
putting a thermite bomb on a homemade drone, homemade thermite bomb or whatever can happen
in a garage or cyber attacks on infrastructure targets, this is now a can happen in a basement
thing. It's very different. So having catastrophic tech that is not that hard to make,
that is decentralized is very different. That's one thing. Also, when there's just one type of
catastrophic tech, maybe there's different delivery mechanisms. It can come from a sub or a plane or
whatever, but it's one type. The mitigation strategies or the prevention of war strategies
are different than when you have multiple different types. And the forecasting of how could one of
these get first attack without triggering the other ones? It comes nearly impossible to figure out.
So it's actually really worth thinking through all of that. And if you read, say, a book like
The Doomsday Machine by Daniel Ellsberg, Daniel's still alive. You should get him on your show.
It would be actually really incredible to have him on your show if he hasn't been yet.
And talk about how many times we went into the countdown sequence based on computer glitches
or errors during mutual destruction and how fantastic it is that we're here at all.
You also think about, okay, so making nukes was actually a pretty hard thing to do,
figuring out how to split atoms. But yeah, it's still less complicated than the other type of
tech and the tech mitigation we're talking about. But notice what the Manhattan Project was.
It was, let's get all the best minds in the world. Let's create an unlimited black budget and figure
this thing out. We don't have something like a Manhattan Project tending to the existential risks
of today. And we have something like, at best, little bitty academic orgs, like the Cambridge
Center for the Study of Extential Risk or the Oxford Future Humanity Institute that have nowhere
near that kind of concentration of brain trust, national support, funding, whatever.
So that's another big thing of like, why is it that society can't coordinate adequately to
recognize a major risk and be able to give the adequate degree of investment needed? Because
if you want to take that argument that as we get new problems, we get better problem-solving
capacity, we can say, well, the last biggest set of problems we had was way smaller than these
and had way more problem-solving capacity put towards it. So what's going on there is important.
One thing to think about is the role of the politicization of information
and how that makes everything more difficult to solve. So for instance, I remember
in March of 2020, early March, when the first COVID numbers, the fatality rate numbers were
just coming out from various parts of the world, and we were all looking at those cruise ships
and wondering what exactly the disease we were dealing with. I remember at the time
reading articles about, you know, sort of trying to piece together a picture of what this disease was
and having no semblance of a worry in my mind that the article was distorted by political bias
in either direction, because we hadn't yet decided who was on what side of the COVID issue. There
were no sides yet. They were not drawn. You know, there were people worrying, for instance, that
Trump was going to use a kind of alarmism about COVID to institute the authoritarian policies that
he's always wanted to institute from the beginning and force the whole nation into lockdown. That
was a plausible idea or worry that many people had. I only say that just to show that people
really did not know where the left and the right were going to come down on this issue.
And I remember the feeling of relief, just knowing that I can read an article about an
extremely important topic and not worry that political bias is really infecting it in either
direction. And even then, the epistemological problems of figuring out what is true were immense.
Right? Is the fatality rate 0.1% or 5% right? That's the question people were asking at that time,
and it was a very difficult question. Now, fast forward a month, the political lines have been
drawn, and now you have to devote brain space when you're watching TV or reading an article
to trying to unspin it, because now we've just imported all of the machinery of toxic
partisanship and confirmation bias and everything into this issue. And I could palpably feel the
shift. And it occurred to me how much better off would we be if we could treat every problem in
civilization, if we could treat climate change and AI, particularly climate change like coronavirus
in the first week before it was politicized. So where does the politicization of information
fit into this problem for you? Yeah, I mean, it's pretty central. When we talk about governance
and the original thesis that we need fundamentally better types of governance for the scale and
magnitude of consequentiality and speed of the types of things we're governing on today.
Governance is about coordinated choice making. How do, especially if it's not autocratic, if
it's going to be a democracy or a republic or an open society of some kind, how do we coordinate
choice making across a lot of different people, businesses, agents? And our choice making is
informed by both our sense making and our values or our meaning making. So what do we think is
actually going on? And what do we think a solution to what's going on might look like? What will the
effect of that thing be? So that's both sense making and then forecasting. And then the meaning
maker and the value generation is what is it that we care about that we're trying to make a choice
in the service of. And so if we want anything like an open society, we have to be able to
coordinate our sense making and our values with each other to be able to have something like
coordinated choice making emerge. And so that means that we have to have something like an
epistemic common, right? A commons where we can make sense of things together and we're oriented
to be able to have clear sense making together. And we see that that isn't happening. And there's
a number of historical reasons why that isn't happening. We can see that the fourth estate has
undergone massive erosion for a number of reasons as has the quality of education relative to training
citizens to be able to have adequate civic engagement for the complexity of the issues we
face. So this is actually kind of an important point if we step back a little bit is we say
well, what is the role of education for any civilization? The role of education is that
for that civilization, there's different ways we can talk about it. This is not the only meaningful
way, but one meaningful way. For that civilization to continue to function, there's a lot of tasks
that have to happen. There's a lot of knowledge and problem solving capacities. And so the role of
education is to be able to train up new generations and have intergenerational knowledge transfer
of everything that is needed for the tasks of that society to be able to happen. And so you can
think of it as a kind of social transmission or social auto-poesis. But in an open society,
as opposed to a closed society, if we have some type of autocracy, everyone doesn't have to
be engaged in the choice making of the society as a whole, they just have to be able to carry out a
labor function or whatever specific function within that they have. If we have an open society
where people are supposed to be engaged in a governance process of a foreign by the people,
which means that the choices that are happening at the macro scale of that nation or whatever
group it is, they have to be able to understand well enough to contribute to the choice making of,
then the education in that society has to not just prepare them for their role in the market
or the workforce, but also prepare them for their role as a citizen to be able to engage in
governance. And so whether I'm voting on a proposition or whether I'm voting on a representative
who's going to vote on a proposition, do I understand the propositions and the pros and cons well
enough of what the DOE has to do with regard to the grid and energy policy or what the DOD has
to do with regard to military capacity? There's a big educational load required. There's a larger
educational load required to develop citizens capable of doing open society. I think people
can understand why the idea of democracy is actually a kind of radical idea, which is the
idea that you can have some very large number of people, most of whom will never meet each other,
who have different life experiences and want different things and different worldviews,
different religions, that are all going to somehow coordinate together effectively to run
something like shared infrastructure, as opposed to have a king or a parliament or some small group
of people do it, or a tribe where everybody coordinates together, but it's a small number
of people where everyone can be in a single conversation. Everybody can know each other
and compromising for people that you know as a different creature. And this is why modern
democracies emerged following the enlightenment of modernity and the idea that there was some
basis by which we could all come to make sense of the world together, the philosophy of science
and the Hegelian dialectic. We could all independently come to assess base reality and come to similar
understanding about it. And through something like a Hegelian dialectic, we could understand
each other's values and say, is there a value or is there a proposition that can meet everybody's
values better? And can there be a recognition that if this thing is not the optimal thing you want,
it's still better the system as a whole than autocracy or warfare between us separating.
And this is where we go back to all the founding founders of the US, the statements that they made,
like I think it was Jefferson that said, if I could have a perfect newspaper and a broken
government or a perfect government and a broken newspaper, I would take a perfect newspaper,
because if the people could all be well educated about what is actually going on in the world,
they can make a new government. If the people aren't well educated, they can't possibly be
engaged in good choice making. And so whoever's effective at predatory rule will end up ruling.
And I think the quote that is most interesting to me was, I'm going to paraphrase one of George
Washington where he said something to the effect of the number one aim of the federal government
has to be the comprehensive education of every citizen in the science of government.
And I think it's such a key insight to say the number one aim of the federal government,
after just fighting revolutionary wars, not to protect its boundaries.
And after creating a government as opposed to a pure laissez-faire market so that it could
institute rule of law, the number one aim wasn't even rule of law, because you can protect your
boundaries effectively with a military dictatorship. And you can institute rule of law well with a
dictatorship. But if it's truly going to be a democracy, if every citizen isn't comprehensively
educated into the science of government, everything that they would need to understand to participate
in governing well, and the civic virtue to actually do so, it's not going to stay a democracy.
And we can see for a bunch of reasons how both the educational system and the Fourth Estate
that would mediate their ongoing informantness have decayed to a place that is nowhere near
adequate. We don't have anything like government out for and by the people. People don't understand
most anything government is engaging in, nor could they are particularly oriented to it.
So the idea of a republic is mostly not even a good simulation anymore. It's mostly just a vestigial
story. And so then the question we have to ask is if we want something like an open society as
opposed to an autocratic society, we can see autocracy on the rise in other parts of the world
and it actually being more effective at a number of things. If we want something like an open
society, what would it take to have an open society coordinate effectively in the presence
of these types of issues we face and coordinate more effectively than autocratic ones do to not
have the world be seated to that structure? And at the foundation of what it would take
is something like the capacity to have shared sense making across the space and shared values
generation to orient towards shared coordination. And so then really key becomes what does it take
to do that? What does it take to create a systems and a culture that doesn't pollute the epistemic
commons that doesn't culture war that works towards communication that is seeking to bring
about shared understanding first rather than subgroups, strategic aims.
So this is a huge problem, obviously. And it's a problem that is unique to America and not unique
in the sense that we're the only place that faces it. But it's so on the one hand, you have a country
like China or any other autocracy where I know some people from China and mostly what they say
about political conversations is that they just don't happen. People just, for the most part,
don't discuss it. There's just a boundary on what topics you can talk about. And within that
boundary, you feel quite free, in fact, and in some sense, freer than Americans are to speak.
But there just is a hard and fast boundary about what you can talk about in polite conversation.
And that boundary is more or less chosen by the Communist Party, right? So that doesn't exist in
America. Obviously, to the extent that there are boundaries on what you can talk about in polite
conversation, those boundaries are imposed from the bottom up in some sense, or at minimum,
not from the government, from the elite, right? By elite cues and by sort of the consensus of
people with a certain status that have blue check marks on Twitter and so forth.
And the related problem is that in many European countries, they are premised on the notion
of a single ethnicity with shared values. And certain problems don't arise in that context,
the problem in Europe has been that different nations go to war with one another. And to the
extent that nations go to war within themselves, different ethnic groups, that has in many cases
been solved by simply creating nations that have one ethnicity with one religion. So there's a
substantial body of agreement about values to begin with, right? There's a kind of unity.
And then in the case of China, where that unity is sort of imposed from the top down, right? So
those are two ways you can get to unity through autocracy or through just forming a nation that
already has a substantial amount of unity to begin with because the nation is a natural outgrowth
of what was once an actual tribe of people. And then in America, you have a totally different case
where we have freedom, we don't have autocracy, but we are extremely far from a single tribe,
right? We're lots of different tribes with deep disagreement on values.
And that's a problem that is very difficult to see how we make progress on. And it seems
in my lifetime, I've only seen it get worse as a result of social media and
the way we consume information now, where you just get a personalized menu of events and opinions
that are perfectly crafted so as to either confirm what you already believe
or give you the worst impression of what the other side thinks about an issue
so that you can dismiss them entirely. And on various measures of polarization,
this problem has been getting worse. And so it is a deep question to ask.
How do we update democracy for the 21st century, for a 21st century
multi-ethnic nation that is committed to freedom and doesn't want to become an autocracy,
doesn't want to impose unity from the top down, but nevertheless can't fall back on
what works in other places that are also free, which is just having a sort of natural unity?
Oh, man, you touched like eight different things. It would be nice if we can get into,
hope we can come back and close some of these loops. On the social media thing you mentioned,
I'm not going to start there, but we should come back and double click on it because it's actually
a really, it's a fundamental part. I believe you had Tristan Harris on recently, so we won't repeat
that. It's a good one for people to listen to I'm imagining, but there's a couple pieces in terms of
that are all of the structures made in modernity were made on broadcast media, meaning some central
broadcast ability through a newspaper, through following the printing press, which then turned
into radio, which then turned into television, which meant we could make sure everybody got
similar messages. And so as soon as we moved to a decentralized kind of media where everybody
could broadcast and then there's way more answers than anyone can possibly look at. So then whoever
controls the search algorithm or the recommendation algorithm is actually curating the information
space. That's actually not just a change in magnitude. That's a change in type. That's a change in
type in a very fundamental way with the nature of the technological infrastructure that is mediating
civilization, changing, and then the new civilization, the social tech needing to change and having not
changed yet. So that's a big one I'd like to come back to. But the most fundamental thing that you said
is you can see examples where there is a certain kind of order that doesn't seem super top down
imposed. It seems more bottom up or kind of culturally supported where there is some kind of
depth of shared culture that creates enough binding energy to overcome the cleaving energy
between the differences of the people. You can think about what's going to hold a number of
people together being a ratio of the cleaving and binding energy of that population. And
when we study the history of sapiens, we can see that for the most part, for a few hundred
thousand years, the groups never got beyond a certain size, famously the Dunbar number.
And it's a big question as to why. But we see this in all other animals, right? Like chimpanzee
troops are of a certain size. They get a little bit bigger and then they end up cleaving. And it
similarly has to do with where does the bonding energy between them provide enough benefits to
overcome the cleaving energy between competing interests. But the fact that there's something
like 200,000 plus years of human activity and with brains similar to the brains that we have
that never got above 150 people is like, oh, there's something very deep there to look at.
And this is why many social philosophers talk about the fundamentally tribal nature of humans
from an evolutionary bio standpoint and why it's important to understand that.
There's a lot of ways we can think about the Dunbar number.
One way is up to that number of people. I can know everyone very well. I can know everyone
intimately. And we can actually all sit around. We can all participate in a single conversation.
We can have a tribal council where if there's a big decision to make, everybody's voice could be
heard within just literally mediated by the ability to speak and everybody hear it without
a microphone or tech of some kind. And the time that it takes for all the voices to be heard
to actually make a decision. And so if I'm going to live in a society where I'm going to be
subjugated to the choices or be responsible to mediate the choices, I want to have a say in it.
If we start to get to a situation where there's so many people they can't all have a say,
then why would I just not cleave and go make a smaller group?
So then the question of why did we start, when did the Dunbar number end,
mostly has to do with a time period around the beginning of agriculture movement from
hunter gather, large enomatic cultures to agriculture. And a number of critical changes
that occurred around then, but some of them have to do with up till that point, human populations
the population would actually outstrip the resource of an area. And then they would move.
And so you'd get this migration. And this is why we can see that humans wherever they started
ended up becoming apex predators across every single environment on the planet,
where no other animal did that. Like that's a very uniquely human thing because we could
make ourselves adapted to different environments through tools, clothes and other kinds of tools.
Once population starts to outstrip an area and you can't move anymore because we've done all
the moving that's possible, then tribal warfare becomes an obligate situation to deal with resource
scarcity. Once any group starts tribal warfare, all the other ones have to do it in an obligate
way or they die by default. And so now you start getting a tribal warfare race, a kind of multi
polar arm trap or arms race. And so now it makes sense for two smaller groups to unify together
because they have more binding energy, which is they're going to die on their own.
And so I'm willing to give up some of my choice making freedom. I'm willing to be a part of a
group I don't have as much to say in because the group that I have more say on on my own is going
to die from some larger militarized group. Now as soon as that group gets larger than the other
group gets larger, they both increase their weaponry, they increase their ability to do division of
labor and coordinate and stores of resources and those types of things. And we call that kind of
the evolution of empire. So it's important to understand that the, and this is actually why,
you know, when Machiavelli talks about the enemy hypothesis that the idea that humans are tribal
and they will keep breaking down to smaller tribal scales unless there is a unifying force and the
unifying force is almost always an enemy. Doesn't have to be, but there's actually social theory
argument on this. I would say it doesn't have to be. But that generally what will unify a group
of people together with a shared identity involves what is outside of that identity that needs them
to be bound together. And so, so yeah, this is an important idea is that you can have a shared
identity, which we call a culture that has everybody kind of police each other or create order with
each other in a distributed way. And so that when we look at Japan or Singapore or the Nordic
countries and look at the relatively lower crime, higher education, et cetera, and try to apply that
to some huge diverse country like the US, we can see that they have certain cultural prerequisites
in place we don't have. That's like a hard and really important problem. And it's why there are
some people who are proponents of the idea the US actually needs to break up into some smaller
entities that can have more cultural homogeneity. I think that is a definitely impossible way to do
it because all of the states to say we tried to separate into a red nation and a blue nation or
whatever subsets of nations, all the states have nukes, none of the forces in Europe or whatever
when we were dealing with kind of European rise did. And so do you want groups that have so much
enmity and antipathy with each other that they can't participate together and have to cleave
both having nukes with no defensible airspace in between them sharing a border? Like that's a
problematic situation to think about. So you either get order because of some shared cultural
basis or you get order because it's imposed or you don't get order and you get chaos and the
thing breaks down because of chaos. So if we say that civilizations fail either because of too much
chaos or order that comes the wrong way through imposition, those are the two different failure
modes. So what we need is emergent order. The emergent order actually has to be at the level
of culture before being at the level of institution. It has to be ultimately the fact that humans are
making choices based on what they value and what they care about and how they make sense of the
world. And if the order is not arising from within them, it's imposed on them. And then
that imposition has to be mediated by a monopoly of violence of some kind and becomes its own
failure state. So the question of how do we get emergent order in a situation like the U.S. and
some or the globe if we want to think about where we have authentically global issues that we need
to coordinate on because no country can deal with ocean dead zones or species extinction or climate
change or exponential tech media risk on its own. How do you get emergent order between people that
have very different histories and different conditioning? That's one way of framing what I
would say is like the essential question we have to figure out right now.
Yeah, so I want to zero in on this idea of the U.S. breaking into smaller countries or
not as a serious suggestion, but as a tool for thinking about what some of the problems are.
So if you, for instance, take the idea of being in the news lately and defined last summer of
the police killing unarmed U.S. citizens, particularly African Americans, and consider how
important the raw size of the United States is to this particular problem, to a problem that
results in riots and lives lost and businesses destroyed and so forth. Consider if America
were identical in every respect but the size of Canada, roughly one-tenth the size by population.
That just means any bad thing related to one person harming another would happen
one-tenth as often. If there are say 40 unarmed Americans killed every year,
then just there are four in this hypothetical version of America that's just one-tenth the size.
So it's just four stories a year as opposed to 40 and just fewer opportunities per unit time
to become outraged about something. And the important thing here is that
anything that happens anywhere in America feels like it happens here in the relevant sense, right?
So if something happens a thousand miles away in a different state, that can cause a riot
outside my apartment, right? Because conceptually it happened here in the sense that is relevant
for the rest of Americans. And so many of the problems we face are, if not a direct result, but
then exacerbated by the size of the country, right? We're trying to do something in this country that
has never been done before, which is have a population of a third of a billion people
that are not culturally the same. We've had, for all intents and purposes, open borders
longer than any other nation has ever tried to have open borders for. And I'm speaking not
about our current immigration policy, but about the long run US history, right? We let
Europe come in in the 19th century and until the 1920s or so. And then since 1965,
we've pretty much let the world come in relative to at least border policies in other nations,
such as Japan and China and so forth. And so we're trying to have a massive country.
We're trying to have a country that relative to most nations on earth has a very liberal
border policy. We're trying to be a nation of immigrants and immigrants from very different
places with very different values from one another. And we're also trying to have no autocracy and
freedom, right? So this is a very difficult problem. It's a problem most countries haven't even
been approached with a 10 foot pole because they'd be so worried about the results, right?
And it's obviously a problem. It's a challenge that we suffer from in many ways. It's also,
I think, a challenge we should feel proud for attempting, however imperfectly, because most
places on earth have just not attempted it. They've said, well, fuck it, we're just going to close
the borders, we're not even going to try, and we're going to remain a certain size,
which is fine. It's just that often, it's one thing for someone to look at the
social, the problems of divisiveness facing America right now from a different country
and wonder what the hell is going on here. But I think it's crucial to recognize the
context that we're trying something that most places simply don't try for good reason.
But it occurs to me a country like Singapore, which you mentioned,
the way that they deal with race relations there is just totally different, right? They have,
obviously it's a city-state and it's many ways not analogous, but they have three major ethnicities
there, and they have a quota system where they actually, from the top down, don't allow
residential segregation, right? They don't allow, you cannot have a situation there,
like you have in New York with Hasidic Jews, for instance, all living in one neighborhood,
or with blacks and whites heavily clustered in certain neighborhoods, because they don't allow
it. They enforce integration by policy, and they found that that's a stable equilibrium.
But whether or not that would work in America is a moot point because it could never be done,
right? We have a culture, we have a taste for freedom here, culturally, for not being told
where you can and can't live that people in other places of the world simply don't have,
right? If you never had freedom in democracy, there's nothing built in, I think, to the human
spirit that necessarily favors it. So, I chalk that all up to you as elements to think about
in this problem. Is there anything in there that you want to react to?
Well, when you say what we're trying to do here is novel, so of course we have some novel
problems, and yet the problems are the result of a meaningful set of intents and experiments.
I wouldn't even say there was a clear set of intents, right? Because when you look at the
number of people who were here at the time of the founding of the nation,
and if you even look at, in those writings, what the forecast of the most people that would
ever come about, it was nothing like what has happened. So, the intent has been a kind of
evolutionary force over this time period. And, for instance, when you think about the ideas in
early formation of the United States democracy, the town hall was a central idea. And the idea
that the people who locally lived in an area were largely affecting each other more than
anything else because transportation was slow and communications were slow. And the total
population was small enough that everybody could pretty much fit into a town hall and be part of
a conversation. So, civic duty was not mostly voting on a representative. It was actually
being able to be in a situation where the sense-making that anyone had could be heard.
Someone was like, no, that's not the situation. I've been out on the border. Here's what's going on.
Everyone's sense-making could be heard. And rather than just getting to vote on a pre-crafted
proposition where both voting for it is good for some things and damages other things, which is
why almost every proposition gets half of the vote, almost no propositions get 90% of the vote,
is because they're almost all based on theory of trade-offs that seem beneficial to some subset
of things and harmful to another. But nobody was involved in crafting the proposition outside of
usually some special interest group that wasn't even trying to look at how does this benefit
everyone. That was actually alien to the core idea of why democracy might work. The idea of why it
might work is can we actually have a conversation first to come up with what everybody thinks is
going on and what matters to come up with a proposition in a real-time conversation that
seems like the best proposition that meets everybody's needs simultaneously. And then voting
is a last step if we can't all just come to a shared agreement that that makes sense as a way
of sublimating the rivalry or warfare. So, as soon as you get to too much scale and you can't do that
anymore, then you have to over depend on representatives. Then the people who are representing
have their own set of interests that become, and as it gets to a larger and larger scale,
almost no representative is really talking with the people in a meaningful way. And so,
whose interests are they representing? The idea that the state is going to institute law that
allows the market to do the good things the market does, but where there's unhealthy market
incentive, predatory aspects, there's going to be rule of law that binds it. You can only have a
state buying the market if the people are binding the state, otherwise you end up getting regulatory
capture. And we can see that that has happened across the board. Well, how do the people bind
the state when it gets to that big of a size? And there's that much information, that much
complexity, we can't all fit into a town hall. And that's a fundamental question. We can see that
some of the founding thoughts on why we needed representatives had to do with who rides a horse
from this area to meet with the other town halls and figure out what the state's going to do.
And it's like, so when we see what Audrey Tang has done in Taiwan with working on creating a
digital democracy, someone else would be fantastic for your show. She hasn't been on.
Can we create a town hall? Can we use the kinds of exponential tech that Facebook and YouTube
are using that end up destroying democracy? Could we use them to mediate democracy where we were
able to actually have everyone be well educated on the issues we're able to assess? Do these people
even, can they state the pros and cons on a specific proposition? So it doesn't have to show
leaning. It just shows that there's actual understanding of the issues before they vote on
it. Is there ways for them to vote directly on things? So there are ways to be able to track
what representatives are doing and the financial accounting and the relationships between
the commercial sector and the state better. And this is where I'm saying the radical increase
in scale in particular debased some of the ideas upon which democracy seemed like it would even
make sense or be possible. So now you have to rethink how do we have everyone be educated enough
about the issues and be able to understand the other positions on the issues well enough and
participate in proposition creation, not just choice making on the proposition that is bad,
yes or no. How do we redo that utilizing the digital technologies and other new technologies
we have that both make it more consequential to do that and also possible?
Well, yeah, I guess there is a fundamental problem, which is that
much of the people, much of the reason people are interested in issues
is because those issues are terrain in a battle that exists. The analogy I sometimes think of is
people get into sports usually first through rooting for a team and then through learning about
the sport, becoming interested in the sport because they care about one side winning in a war.
It's a it's a rarer person who first gets interested in baseball, neutral to who wins,
and then only later begins rooting for a particular team, right? So much of the reason
people want to become educated about any particular issue is because it's already
terrain in the culture war that they can fight for, right? People first look for the fight and then
get want to get informed in the context of the fight, which is totally natural. I'm not claiming
to be exempt from this. This is something I experience in my own psychology and I see
in society in general and other people. And so it's easy for me to be pessimistic about the
possibility of finding a tech based solution to a problem that seems inherent in our psychology.
Can you talk me out of that pessimism? So the question you bring up is a kind of
real politic critique or question of is human nature inexorably the thing we're up against?
So if you read Machiavelli, we read the 48 Laws of Power or anything that is kind of doing
a real politic assessment of patterns of human behavior historically,
the argument can be made that humans are intrinsically and maybe there's a Gaussian
distribution based on their genetics or their whatever it is something of how much of this,
but that as a the distribution as a whole humans are either innately too irrational
or too rivalrous or both to be able to have something like adequate sense making and adequate
integrative values to be able to have an emergent order and culture beyond an in-group that is
unified against an outgroup. That would be the question that we are kind of bound to if there
isn't a big enough outgroup, we will never be able to get the binding energy, we will go tribal on
each other. And this is one of the arguments about one of the things that happened in the US.
Obviously the US has had rivalry for a long time, the Civil War was a good example,
there have been plenty of examples across partisanship or class or race or region or
but we can also see that there was a kind of patriotism and reunification
associated with World War II and for a little while afterwards.
And when we look at the intensity of partisanship in the US today where
even when I was a kid the senators on the floor that would be arguing with each other would go
out to lunch together afterwards and that doesn't happen, it's like what happened in that time period.
Part of it has to do with the social media thing, part of it has to do with things that occurred
in broadcast media, there's a number of things. One of the arguments that many people brought
up is that it intensified after the Soviet Union collapse and the wall came down in the end of
the Cold War because the Cold War created a binding energy for the US to all be on the same
side adequately against a threatening enough external enemy and that when it came down and
China didn't look anything like it does today and Russia didn't even have Putin yet organizing
them in a more strategic way, there wasn't a big enough enemy to unify the US against itself.
The US was the number one biggest force and the number two and number three biggest force that
the left and right turned against each other as the kind of largest source of energy to extract
or predatory advantage to incur and then that has continued as China has moved into a total
different position and that's occurring in other parts of the world. So there's this question of
can people be unified at a scale much larger than tribal without an external enemy or is that innate
to us and we can see Jane Goodall's and Jimmy Carter's and Martin Luther King's and Carl Sagan's
and people that seem to have a more universal ethic and there's a question of is that conditionable
across a population widely or are those people naturally outliers for some reason will always
be outliers and they're trying to think about conditioning qualities like that more widely is
folly. Eric Weinstein and I had a conversation a couple years ago on his podcast and this was
like the heart of the thing that we talked about because he and I had a fairly similar assessment
of the catastrophic risk landscape that exponential tech under current kinds of
rival risk international governance creates and we each had a thought on how to overcome that
and they were different and he made a joke that was fun because he's like well I think your solution
requires changing human nature but I'll indulge you on that because mine requires breaking the
speed of light and his solution was no I think we are too rival risk we have to be able to have
civilizations work that have some intrinsically shared value base if they're too close to someone
else that doesn't have enough shared value base they won't care and if they will end up wanting
the other one's resource at a certain point so we need to be able to inhabit planets that are far
enough a part of the war war is unfeasible and like you can see how someone smart thinking well
about these things could be like well maybe that's the case right maybe that is actually a necessary
thing and I don't know if my sense of it is will end up being right because obviously we don't
we don't have the basis to be able to say that much certainty but I'll tell you why I think it is
one of the postmodern critiques of modernity that was right is that the idea that there is
an unbiased objective assessment of the world is is dangerous that when we're doing
because in a complex situation which things I picked to measure even if I'm measuring and
repeating measurement and getting it right and not messing that up anywhere just by cherry picking
out of the data of a very large set or even cherry picking what I do the science to measure on I can
get something that is quote unquote true but not representative and then argue as if it's
representative or decontextualize the facts or or lay cough frame the interpretation that there's a
pragmatics in addition to the kind of semantics and that we can see this more in the social
sciences where there's increased complexity with the whole early social science in the U.S. of why
why blacks were genetically lower race in the U.S. and thus the declaration of independence
didn't pertain to them and yet we weren't morally corrupt or the science of phrenology or things
like that would be like yeah that was that was just political vested interest gibberish
guising itself in something that sounded sciency and it's not anything like real objective
science well I think the answer isn't you throw out science and say that there is no way to come
to truth because if that's the case all there is is power and then we're actually stuck in that thing
we need to just correct for it so we don't say that anyone can be unbiased but we say we can
seek bias correction we can seek to be able to take all of the perspectives well not just
performatively but really seek to inhabit them and then seek to identify where there's true
signal and seek to synthesize them and so is it possible to do bias correction better toward
shared sense making that never has a hundred percent confidence but an increasingly well
epistemically grounded confidence so the reason I say this is because a lot of the social science
that we have I think is problematic in this way I think it's after the thing called empire and then
even the thing called industrial revolution and capitalism had taken over which was not the
evolutionary history of humans in mostly tribal environments and there it created a source of
conditioning but it was fairly ubiquitous conditioning and since it was ubiquitous we
just said oh this is human nature and then where we would see something like an indigenous tribe
that was different it could be thrown out as a statistical outlier so I'm actually very interested
in studying statistical outliers to something that is not innate but is ubiquitous conditioning to
say what is possible in the in human nature so specifically I'm interested in positive deviance
on the Gaussian distribution thing cultures that have a particular trait that would be desirable
more than statistically average and so looking at things like Buddhism and Jainism and more
non-violence based religions Buddhism is an interesting example of a number of people that
have fluctuated but a lot of people 10 million people over a few millennia over different languages
and different periods of time that had different economics or etc where the majority of the
population all wouldn't hurt bugs and you're like what and didn't mediate war I'm not romanticizing
that there were never exceptions but compared to most other populations it's like something
pretty novel and interesting was happening there and we can see that it's related directly to
the ideas of Buddhist religion and the nature of how they did conditioning of humans particularly
early childhood around non-violence compassion kindness empathy those types of things
and we can similarly look at areas in Liberia or Darfur other places where you have child soldiers
where you pretty much aren't going to make it to adulthood without being a murderer it's just
not a possibility and so I see that human nature makes almost everyone a murderer or almost nobody
hurts bugs both possible under the right conditioning environments I see that conditioning
people you can't not be conditioning people people are going to learn Mandarin or they're
going to learn English or they're going to learn Navajo or whatever language because innate to
sapiens and this is really important to understand the closest relatives to us right like a horse
is up and standing in 10 or 20 minutes and it takes a human a year it's like that's a really long
period of helplessness why are we embryonic or neoteness for so long and even our closest relatives
like chimpanzees they can hold on to their mom's fur while she moves around the first day and we
can't even move our head volitionally for three months and it's like okay so why are we so developmentally
dependent for so long and then why do we also have this abstract language and abstract understanding
of tool making principle that is unique well because we make tools that modify our environment so
much where most other creatures were evolved to be fit to an environment but we would then
change the environment to something we didn't evolve to be fit to we have to be able to change
ourselves pretty rapidly so I don't want to genetically be oriented to throw spears well
if throwing spears isn't adaptive anymore and being able to code or text is so I want to come in with
it not very much code some code but the capacity to imprint the new novel environment we're in
because we're migrating and changing our own environments so conditioning is a bigger deal
for humans than it is for other things it's unavoidable so then the question is and most of the
conditioning that we have studied in social science is post industrialization and all of it
that we've been able to study in the historical period was post empire which was post tribal
warfare and not the actual evolutionary environment of humans long term and so we have to say okay well
within the conditioning environment of obligate warfare humans behave these ways is that the only
way humans could behave under any conditioning environment I think it's a very important question
when we look at the level of education and uh the yeah the level of education on the Gaussian
distribution of the Jewish population it's very different than lots of other populations in the
same country those Jews are inhabiting and it has to do with certain cultural values that affect how
they do child rearing and parenting and culture and school and so then the question of could we
develop higher rationality and a higher universal ethics and the population largely so that people
were actually seeking to understand perspectives other than their own seeking to be able to inhabit
the perspectives and the values of other people and then seeking a solution that we could actually
coordinate towards recognizing that if we just beat the other side unless we beat them completely
they don't exist on the planet anymore they don't go away they're still political actors they're
pissed off and have more enmity and they're gonna reverse engineer whatever political weapon or
economic weapon or narrative weapon or physical weapon we just use and come back so we either
bind ourselves to not just war but escalating war and arms race including cultural arms races
or we say how do we and and if under exponential tech that becomes catastrophic
which i argue it does then either we figure out how to understand each other and coordinate with
each other or we move into escalating rivalry that eventually self-terminates so then i would
say our capacity to do that is obligate so then is it possible i hope so and i think so i mean
the point you make about the importance of culture is well taken
i do worry one thing that's also true which makes it more difficult is that certain cultural memes
have advantages over others right like the you know you know there's a reason that outliers are
outliers presumably in in the case of the buddhas and the the child soldiers while both of those
are possible there must be some reason why the moderate alternative of you know neither being
so nonviolent so as to not want to kill bugs nor being so violent so as to raise your six-year-olds
as murderers there must be some reason why the moderate approach
has out-propagated the extremes in the space of possible ways to raise one's children and presumably
uh you know there's something unscalable about the extremes relative to the
cultural memes that have in fact scaled right okay yes and no the child soldier situation isn't
an extreme that's very short-sighted even in terms of rivalry it's not figuring out how to do
agriculture so the people can feed themselves well enough that they can grow their population or how
to advance their technological capacity to do warfare better long-term or whatever so it's just
not even doing group rivalry well right the nonviolent civilizations mostly made it as long
as they stayed out of the way of the more violent ones and to some degree if they figured out how
to be effective diplomats between them so they could offer some viable service but otherwise
it's actually a very interesting part of the story is that you know we have the history of those
who made it through and those who made it through usually made it through by winning at war some
kind of war um maybe economic war maybe cultural war largely kinetic war and so were there cultures
that actually provided for a higher quality of life for their people and more harmony with
their environment and just didn't make it through warfare but the warfare thing leads to
arms race meaning continuously larger warfare that once you start to get to the verticalizing
part of that curve itself self terminates and this is one of the things we can see in evolution is
that there are animals where evolution is selecting for something that is adaptive to an environment
that's about to end right it's the animal will be adaptive in the immediate term that this one
just survived better in this environment it was better able to reproduce but it's doubling down
on certain genetic capacities that if the environment changes the tiny bit it goes extinct
and so there is a certain short-sightedness and blindness to that kind of evolution
and I would say mimetically even more so than genetically and so there's a lot of things that
win in the short term meaning they are selected for that self terminate in the long term or not
even all that long term and then this becomes one of the big traps that humanity's in is if we want
to do the right long term thing but we can't get everyone to do it and anyone else does the short
term advantage thing that gives them so much advantage that they beat us in the medium term
how do we avoid that right and this is both the arms race and the tragedy of the commons
if I'm not going to over fish because I want to lead fish fisheries but the we're in shared ocean
area and anyone else over fishes I can't actually protect the fish right I can't protect the fish
population of the ocean by not fishing all I'm doing is saying the other people who don't have
the same ethic as me end up over fishing they grow their population faster and use their larger
population in war against us and so fuck it I'm actually going to try and get all the fish even
though I don't need them and I could figure out a better way and actually have to race to get it
faster than they do that trap is the thing we actually have to figure out how to overcome
and it's the same with don't really want to build AI weapons because the idea of autonomous AI
weapons is is a pretty scary bad idea we'll we'll probably all die from that but I certainly don't
want to let the other guy get them first and I can't guarantee that they aren't doing it in
some secret project so I'm not going to sign a treaty to not do it or if I sign a treaty I'm
going to assume that they're lying about it in secret so we're also going to sign it while
lying about it and everybody knows that so then we all race to build the AI weapon it is
increases the risk of existential harm to everybody and this is the
this is the great filter hypothesis on coming into exponential tech why don't we have more
encounters with advanced alien nations one one of the answers to the Fermi paradox is
when they get to this level of tech they pretty much self-extinct everywhere they their rivalry
within the planet has them used the powerful tech that would be necessary to go interplanetary
against each other before they actually get successfully interplanetary maybe that's true
maybe it's not true it's a it's a fun thought experiment I would say if we want to make it
through that great filter then we have to say how do we have the level of technological power
we're developing and be good stewards of it adequately good stewards because
if we look at any previous civilization we'd be like they did a lot of fucked up things with
the power they had in any of them could if we multiply those fucked up things by exponentially
more power is that viable what would it take for us to be to hold the power and so like I'll say it
mythopoetically and I'm not I'm not saying it literally I'm not going to train Jesus mobile
and just mythopoetically if we have the power of gods if you don't have something like the love
and wisdom and prudence of gods to guide that that power ends up leading to self-destruct and
there's a lot of myths that are you know it's kind of looking along those lines and so you
were asking will technological solutions work humans have both the capacity our abstraction
gave us the capacity for physical tech because rather than just using a sharp rock we could
like a chimp would do we could experience that one rock was sharper than the other and
understand why there's an abstract principle called sharpness and we can make something even
sharper right that abstraction of principle allowed us to not just use tools but evolve
making better tools it's also would allow to abstract language and coordination so technique
just ways of doing things abstractly understand better ways of doing things applies to physical
tech but also social tech right a market or an educational system or a culture or religion
government as a social technology of how do we have a better way of doing things together
I'm saying that the exponential tech physical physical tech that we have
mandates better social tech and that we're having more advancement in the physical tech than we are
in the social tech the previous types of social tech are inadequate to hold the power of this
physical tech but this physical tech also makes possible evolutions in social tech that
weren't possible before so it's both possible and obligate and that when the founders of the U.S.
were coming up with this system they weren't having to think about ocean dead zones and
planetary boundaries or catastrophic weapons or Facebook creating a hypernormal stimuli doubling
down on on tribalism and group bias like there's nothing in the constitution that is trying to
think about how to solve those problems there's nothing in the Scottish Enlightenment and the
theory of markets that had to deal with those problems there's nothing in Marx's critique of
it that even had those problems as things or in the Bretton Woods world it just had the first
of the exponential problems not exponential existential so what I would say is we have a
novel problem landscape and as an associate with our physical tech be but our physical tech
directed by our social tech our social tech needs to make a jump to be able to safely guide our
physical tech and the inquiry around what that is there we need something a lot like the Scottish
Enlightenment or the Enlightenment of modernity that was ended up giving rise to not just the U.S.
but modern democracies as a whole that is thinking through and you know those followed the printing
press and the industrial revolution and saying whoa we had a change in our techno industrial
base that is actually debasing those previous principles of our social technology so what is
the new set of social capacities adequate to this I'm saying we're at a new place where we have to
actually have a step function in cultures because if our problems need fundamentally
better problem solving than we currently have and we haven't argued that yet but explicitly but
what I'll basically say is this we define problems typically in a somewhat simple to try to make a
tractable way the problem is climate change and too much CO2 the problem is systemic racism and
institutions that have these policies or whatever it is and then
so let's take the CO2 situation we can come up with a solution to that narrowly defined problem
that ends up externalizing harm to some other thing we can say okay well we can get the CO2
down by planting a whole bunch of plants and using nitrogen fertilizer that's going to sequester CO2
but the nitrogen is going to create nitrogen runoff in dead zones in the ocean faster or
we're going to do a carbon tax but only the countries that ratify it are going to do it and so
if the west does and China doesn't and so there's a slowdown in GDP in the west and not in China
and China then uses that for near term advantage geopolitically militarily then in trying to
solve climate change we ceded the 21st century to technologically empower autocracy or whatever
so the problem isn't climate change right the problem is climate change and the dead zones
in oceans and the great game of geopolitical power and its relationship to GDP and they're so
interconnected that either our current problem-solving methods that don't factor that either
solve a problem and externalize harm to another problem often that is
worse or just increasing the complexity of the problem landscape or somebody else realizes
that our solution is going to cause some other problem and so they fight us and now we're stuck
in political infighting where almost all of the energy goes into just heat and we can't actually
implement large-scale solutions if we can't coordinate on them and so how can we find what it
is that everybody cares about and how the different problems connect adequately to be able to come up
with propositions that have the least unnecessary theory of trade-offs are the best synergistic
satisfies that people can coordinate towards I would say that is a that is the minimum set of
problem-solving adequate to the problem-solving landscape that we are coming into I like this
distinction you make between physical tech and social tech and that helps me understand exactly
what the problem is as you stated I'm curious what improved social tech might look like is there
anyone doing work right now concretely that you see and you say well that's a step in the right
direction or are are you still in the stage of only being able to really envision what a solution
would be in the abstract but I'd actually like to give a couple examples of how changes in physical
tech changed social tech both of the better and the worse historically a couple times because this
is not actually an abstract idea it's an idea we have a lot of concrete understanding about so then
we're being able to apply that to the current landscape I'm going to just I'm going to give
simplified narratives of topics that the top academics debate nuances of so there's a lot
of nuance in here but I think the points are at least adequately illustrative
animal husbandry and agriculture was a massive tech shift right the agricultural revolution was
okay rather than doing hunting and gathering we can now create massive surplus of grains which
are the first really storable thing and that means that we can make it through famines we can grow
our populations we can start to get into storage and surplus so we can see that the thing that we
call civilization and the development of the plow are very closely related and there's a lot of
people that have looked at the way that that physical technology proceeded changes in the
social tech because until you had surplus you didn't have to deal with the question of who gets
the surplus and what is the governance system that that mediates this surplus until you had
enough surplus that when someone dies they could actually pass it on the all the laws
regarding inheritance didn't have to come into play the same way in a very different way than it
would have been in a tribal setting so there's lots of things that the physical tech brought
about that were different until you had a lot of surplus there wasn't that much reason to raid
in another culture because or another tribe because they didn't have much stored of value
that you could extract but also the the division of labor roughly around gender that is not absolute
but is a statistically generally accepted thing that men would do the hunting women would do more
of the gathering than women would do men would do more of the hunting women would do the horticulture
with the digging stick as soon as it went from the horticulture and the digging stick to the
agriculture in the plow men's upper body strength on the plow actually made a huge difference over
women doing the plow when women would use the plow they would miscarry and so men moved to
to providing almost all the caloric surplus women moved into the home and you started to see
changes in gender relationships associated with the change in tech and the way that the tech itself
that the idea is that the tech is not totally values neutral the tech ends up predisposing
patterns of human behavior in certain ways and so then some people argued that the rise of feminism
largely followed the internal combustion engine the kind of women's liberation movement because
then for the first time men's upper body strength didn't matter because she could drive a tractor
as well as he could drive a tractor and the tractor could out pull the plow oxen situation so much that
the first wave feminism women can do what men could do was mediated by a change in tech
so we can see those are not complete arguments or a lot of other cultural factors and things
going on but folks like Marvin Harris and cultural materialism worked on seeing and McLuhan specifically
who worked on Marvin Harris was looking at how tech in physical tech in general affects social
tech McLuhan specifically how the information and communications tech ends up affecting
society's minds values we saw obviously that the printing press gave rise to
the Lutheran revolution the idea that everybody could have their own bible and didn't need to
have they could study the scripture directly and didn't need a priest to to mediate it
but also the idea of a newspaper and democracy the idea that everybody could read what was going on
and they could all learn to read have an educational system and then they could all
come to the town hall and debate the thing that was a change in physical tech that made a new
social tech possible right could you have done democracy before there was the ability to get
everybody the news in a way right like that was not at scale you could at a tribal kind of scale
but at a larger scale if the people don't all have access to information how can they participate
and choice me so then as we were mentioning with social media and the you know the conversation
that you'd have with folks like Tristan the change to decentralize rather than kind of
one to many broadcast that both the newspaper and then the news was now the many to many
information proliferation where then there's a billion search results for anything so how do I
decide which ones to look at well whatever comes up in the first three pages of google or on the
top of my youtube search or in my facebook feed is going to be what I look at the ai's that can
comb through all those things and show me a specific subset ends up determining what I'm
going to pay attention to in the information landscape those technologies are actually businesses
those businesses have an agentic motive of their own to grow profitability they do that through ads
they do that through maximizing our time on site which improves ad revenue and our time on site is
optimized by certain types of content over others and it mostly ends up being that it's optimized
through the content that creates limbic hijacks for me emotional hijacks of some kind because if I
stay in a real rational place I usually recognize I don't want to spend that much time on facebook so
hypernormal stimuli things that are outraging things that appeal to my existing biases maximize
time on site so existing biases on all sides get increased and in group out group identity gets
increased not even because facebook wants to do that it's just the nature of the algorithm optimizing
for time on site associated with business model and that thing so that's that's an example of physical
tech now that is a democracy ruining thing because it's actually replacing people reading news where
at least you know two different people's news feed in a in a very red area in a very blue
area in the US might have not a single thing in common and the reporting on each issue is very
very slanted and then it's selecting for the ones that the people are going to pay attention to
and so can you run a democracy where there is no shared reality basis at all no now that's an
example of physical tech changing social tech in a downward oriented direction in some critical
ways that doesn't mean there aren't also upward oriented directions so then the question starts
to become if we want to we can see how the changes in physical tech made possible and even forced
changes in social process social tech historically and not just social tech but the nature of even
how I identify and the nature of you know minds things like patriarchy and religions with male
gods following the plow like these are very fundamental changes
so then what we want to start saying is what are the necessary requisites of social tech
adequate to hold exponential tech what would new types of social technologies need to be
oh this is the thread that we lost earlier if the problems need new problem solving capacities
which they do because we can't keep narrowly defined problems that are this radically interconnected
with this scope of consequence then we need new institutions and social capacities to solve them
those new institutions and social capacities are either enforced by a few people who understand
them or they have to arise from the people if they are to be emergent if they are to be commenced
through with a new type of open society the people have to understand the problem space and what is
needed for it and care about it and be willing to take a responsibility in mediating it that's a
cultural revolution or cultural renaissance or enlightenment that I would say is the necessary
thing that has to happen right now to give rise to new social institutions and problem solving
capacities adequate to the problem solving landscape we're in right now so if we take the
problems and say what would necessary solutions require what would it require well it would
require the ability to have shared sense making it require the ability to take the various values
and actually hold them together and look for propositions that could meet them all better
it would take coordinate processes that coordinate choice making well
then we can start to say well if we need social tech that does that and that also means that can
that develop humans that have the capacity to do that how could we apply the new physical
tech we have to develop people and societies in that way I've reached the end of my a lot of
time for this conversation but there's a lot more we could talk about and I'll have to have
you on again at some point to get to those things but this has been a great conversation I'd like
to point my audience in the direction of your conciliance project could you talk about that
briefly yeah thank you I'm one of the members of a new project called the conciliance project that
is working on these topics working on how do we articulate what is novel about the problem
space today clearly enough and it's not narrowly defined problems and it's not kind of gross
generalizations that don't end up giving rise to adequate solutions but how do we identify
how the problems are interconnected and what gives rise to them the kind of systemic incentives
and coordination challenges like that well enough that we can identify what would necessary criteria
of adequate solutions look like and so there starts to get to be a a whole culture that is
able to think about the problems much better and recognize the need to operate towards them take
responsibility towards them much better and start to prototype new civilizational systems new
collective coordination capacities adequate to the fact that we have fully globalized supply
chains globalized tech existential tech etc and that if we want those new capacities to emerge from
the people it has to be people understanding the issues working to develop the capacities in
themselves and working to develop the right kind of good faith relationships with others that can
prototype these capacities so conciliance project is a place where there's a publishing set of things
we're going to do in a kind of movement facilitating set of things that we aim to do the publishing
thing is there's papers that are basically explaining the fundamental aspects of social
theory that we think are necessary to consider to develop new adequate social systems and the
problem space then there's the application of that social theory to situational assessments of what's
going on in the current world where people can both understand the theory in an instantiated way
and understand the problems with deeper insights that could give rise to more adequate thinking
about solutions and then there's this kind of meta-news thing we developed which is a kind of
forensic method to be able to show where there is highly polarized belief about something
COVID viral origins or climate change or systemic racism or whatever it is how do we identify the
different views and how do we identify how those views came to be both it involves mistakes and
partiality and active kind of narrative warfare and distortion and then what would synthesizing what
we can identify as both the values and the sense making across those look like so this is
if we're going to be trying to do sense making we also have to show where the breakdowns and
sense making occur and help develop in people the capacity to notice narrative warfare information
warfare both intentional and unintentional that messes up the epistemic commons well so that they
can participate more effectively so those are the kind of publishing efforts and movement
building efforts are anyone that is seeking to improve journalism or improve public education
or fix the social media algorithms or try to take what Taiwan is doing in digital democracy and
iterate on it or apply blockchain to government accounting or whatever it is that would be part of
fundamentally new adequate social systems how do we identify the ones that are doing really good
work and and signal boost them and rather than having it just look like a journalism project
or an education project be part of a fundamentally new civilization design movement that is starting
to occur so those are roughly the things we're interested in and paying attention to
so so not the big stuff just doing some casual stuff at the conciliants project
this stuff that i think like you i just don't know how not to think about and so
you know we're we're thinking about similar issues because
i think attention and conscience mandates it absolutely all right well it's been a pleasure
daniel i'll have to have you back till next time thank you comment if you appreciate the work i do
the best ways to support me or to subscribe directly through my website colman hughes.org
and to subscribe to my youtube channel so you'll never miss my new content as always thanks for your
support
