THE END
Welcome to another episode of Conversations with Coleman.
If you're hearing this, then you're on the public feed, which means you'll get episodes
a week after they come out and you'll hear advertisements.
You can gain access to the subscriber feed by going to ColemanHughes.org and becoming
a supporter.
This means you'll have access to episodes a week early, you'll never hear ads, and
you'll get access to bonus Q&A episodes.
You can also support me by liking and subscribing on YouTube and sharing the show with friends
and family.
As always, thank you so much for your support.
My guest today is Daniel Schmockdenberger.
Daniel Schmockdenberger is a founding member of the Consilience Project, aimed at improving
public sense-making and dialogue.
His writing is focused on ways of improving the health and development of individuals
in society, with a focus on catastrophic and existential risk, civilization and institutional
decay, collective action problems, social organization theories, and the relevant domains
in philosophy and science.
Daniel and I spend a lot of time talking about the effect of new technology on our ability
to problem solve as a society.
We talk about the disadvantages of democracy relative to autocracy.
We talk about how much of our behavior can be attributed to human nature and how much
to culture, and much more.
So without further ado, Daniel Schmockdenberger.
Okay, Daniel, thanks so much for coming on my show.
Thanks for having me here.
I'm looking forward to our conversation.
Yeah, there's a lot to talk about, but before we start, can you give my audience a sense
of who you are and what you do?
I think that we'll come across in the conversation, hopefully.
I can say a few things I'm interested in.
I've been interested since I was young in all the things that seemed like problems in
the world that mattered, that were not getting better with the current problem solving processes
we had that looked like environmental problems and social problems, and I'm particularly
interested in why it is that we haven't made more progress with the Sustainable Development
Goals or Nuclear Disarmament or Preventing Arms Races, and then why catastrophic risks
of lots of types actually increase in the total number of types of catastrophic risk
available and increase in probability seems that there are some underlying coordination
failures and game theoretic dynamics that make our problem solving capacities not adequate
to the problems we currently face.
It seems like rather than more problem solving processes, like we just need more green tech
or more laws within nation state governments or things like that, that there is some deeper
analysis of what gives rise to the nature of why our tech causes environmental and social
issues or the way we utilize it does and catastrophic risks, why we don't solve those, and what
new civilizational capacities might need to be, they're adequate to the scope of the technological
capacities that we now are stuck with the effects of and thus have to govern better.
So someone might hear that and feel the tone you're operating in is a bit pessimistic, right?
What do you say to someone who says, well, yes, we have huge problems as a species to contend
with coronavirus being the most current salient example, but look how quickly we came up with
a vaccine relative to what we thought was possible in the past, right?
Are we not, is technology just not, despite some of its side effects, clearly, isn't something
going right if we're able to use technology to come up with a solution to, say, a global
pandemic much more quickly than we thought was possible?
Yeah, I would say there are many schools of thought, but I'll simplify it into kind of
two opposing schools of thought when it comes to technological progress and particularly
kind of the exponentiation in technological process that has started happening with the
industrial revolution and then has really picked up since the digital revolution.
There is the idea that technology and capitalism and basically the results of modernity have
delivered us from, and even longer than that, kind of this Western idea of the dialectic of
progress has made the quality of life continuously better, still problems, but roughly on the whole
better. And that any new problem is just the source of the new challenge that human ingenuity
will rise to and will figure out solutions and we're problem-solving creatures. Of course,
there's always going to be problems, but our problem-solving capacity will keep rising and
that's as it should be in great and very few people would want to trade places with a previous
time in history, not knowing what position you'd be born into. So we should keep focusing on problems,
but in a more kind of optimistic way. I would say that's kind of one school highly oversimplified.
There's another group, which is most of the existential risk world that is looking at the
unique issues that AI risk poses that are nothing like the world ever had to face before.
The unique set of risks that can come from biotech and the ability to
engineer self-replicating technology that might be fundamentally incommensurable with the life
that is here now and how cheap and distributed that technology is becoming.
You know, CRISPR gene drives being kind of universally available now. And so what does
it mean to have distributed catastrophic weapon technology? And the unique set of issues made
possible by cyber attacks, by exponential tech mediated disinformation campaigns,
nanotech, et cetera, and then also a billion people and the cumulative effects of
externality and unremovable use of resource and pollution that are hitting planetary boundaries
and the collision of all of this. And that there is a existential risk landscape unlike anything
the world ever looked at or tended to. The founding fathers didn't have to think of these
things, even the Bretton Woods world didn't have to think of these things. And that it seems like
there's pretty much no way we could make it through because if you look at the way humans
have been historically, pick any empire, pick any kind of time period and say, give people that use
power that way, exponentially more power, where they either directly use it to harm others in war
or use it in ways that indirectly and maybe even unintentionally cause harm to environments or
externalities in some way. And let's just exponentiate externality or exponentiate war.
We just don't seem like good enough stewards of that much power. And this is why most of the
sci-fi is dystopian is it's actually hard to come up with pro-topian sci-fi where you think about
people that are like the people we have been that have that much of decentralized power
that don't blow everything up, even accidentally. So then you say, well, if it's not decentralized,
then that's a different fail state and that goes kind of Orwellian, autocratic.
So I would say both of these are schools of looking at the novelty of the tech space that we have
and the relationship of that with the dynamics of how we have made choices and both, you know,
individual conflict theory, group conflict theory, those types of things.
If I was to say, let's step back and look at both of those because there's truth in both of those
narratives, really important truth about those narratives. You can see really good argumentation
from people you've had on Steven Pinker and kind of Kurzweil, Diamandis on one camp. You can go
talk to Nick Bostrom or Ali Eiser Yudkowski or others on that camp. And you can see that it's
become a more popular conversation recently, not just in academic circles with folks like
Elon Musk saying AI risk is the number one biggest risk to the world and that AI has passed the point
of our ability to probably prevent it becoming an existential risk. So neural link is a hope to
find a solution by the necessity of merging us with AI. Like that's now part of popular conversation.
What I would say is if we take a long arc on human history and we say, homo sapiens have been
around for a few hundred thousand years. And the thing that we call civilization has been around
for something like 10,000 years. We only got the possibility of global catastrophic risk in World
War II for the first time, self-induced, human tech induced global catastrophic risk. And that's,
World War II seems like a long time ago for people who were born after it where it's just
history, but it's a sliver of a moment in evolutionary time. And that was the first time,
and it is important to get that when we look at previous civilizations, none of them still exist
in the form that they were when they were a major empire, whatever, the Roman empire,
the Egyptian, the Mayan, the Aztec, whatever. It's actually, it is the precedent that civilizations
self end up falling and mostly through self-terminating processes. They end up having internal infighting
overwhelm their ability to defend their boundaries well, or they actually environmentally outstripped
their environments and then went through collapse dynamics. So it's just whenever you had a
civilization fall previously, it was a local issue, right? It was a local civilization with a
boundary and there were lots of other civilizations around it. World War II, we got a weapon that
we couldn't really viably use in war, recognizing that multiple sides would use it and have anybody
win. It was the first time of having a really unwinnable war and the ability to actually destroy
the biosphere's ability to support life if we used it. So I would say if people aren't paying enough
attention to how bright a line that is of everything before and everything after, it's really important
to get that because before that, when you study history, so much of the history you study is the
history of major kingdoms or empires warring with each other. And after that, the major empires
can't war and make it through. So we have to come up with some solution other than war.
And so the Bretton Woods world, the post-World War II, how do we figure this out?
World said, okay, nation-state governments aren't enough to prevent World War. We realized that.
We realized that at World War I, made the League of Nations, it wasn't strong enough,
so we've got to make a stronger thing. We're going to make a United Nations, but we also have to make
these other intergovernmental organizations, a World Bank and IMF, a set of trade treaties.
And let's make it to where there's so much economic interdependence across the nations,
that it's more profitable to do trade with each other than it is to war. And where the
supply chains are such that almost everything we use comes from six continents. So if we blow
somebody else up, it messes up our own supply chains, and it binds us, it binds the game
theory to being more useful to cooperate than it is to war. So then that enters a new phase.
And also, one of the key things in that Bretton Woods world was, it will allow us the
application of these vast industrial technologies across the whole world,
coordinating in the division of labor across it, that we can increase global GDP growth much
faster. So everybody, every nation, whatever, can increase its wealth without having to take it
from somebody else. And if you have to take it from someone else, because you don't have a positive
sum game, the negative sum game, or zero sum games go rivalries faster. That was the idea.
But then fast forward 75 years, and we didn't have a major empire's war. We had a Cold War and
we had proxy wars. And we go from having a major part of that was mediated by that there was only
one type of catastrophic weapon that couldn't be used. And there were only two superpowers that
had it. So you could do mutually assured destruction. They could monitor and have that set up.
But with AI weapons, with cyber weapons on infrastructure targets, with
bio, nano, et cetera, and other things, we now have dozens of catastrophic weapons
that dozens of actors, including non-state actors, have. We can't put mutually assured
destruction on a system like that. So that whole system that kept us safe after World War II
can't keep doing that. And we got so much, we got such a complicated global supply chain
through globalization that we got to see with COVID, like one part of the world shuts down
because of a local issue, and you get cascading failures across the whole world. And that was
still a relatively small issue. And so then we start to look at the fragility dynamics that come
from that, as opposed to have local collapse, stay local, when the whole system is radically
interconnected, you also get fragility dynamics. And then we're also hitting, because of that
positive sum GDP happening through unrenewable extraction of resources, humans utilizing them
and then turning them into trash, you can't run an exponential linear materials economy on a finite
planet long term. So we're also hitting planetary boundaries, not just climate change, but species
extinction, overfishing of the oceans, plastics, dead zones in the oceans, blah, blah, blah,
so many things like that. So we're now kind of at the end of the Bretton Woods world where
we can't do mutually shared destruction. The fragility is too high.
We can't keep positive sum growth dynamics with linear materials economy. And yet we also can't
go back to the previous thing where we wore with catastrophic war tech. And so it's like, what is
next? So what I would say is that the tech space does make a change in quantity, right? Like,
basically, a nuke is just a way bigger spear. And if you want to think of it very fundamentally in
terms of like, there's some in group out group dynamics and some ability to use kinetic weapons
to advance facities. But at a certain point, changes in quantity become changes in quality or
changes in type, meaning the same type of governance no longer works. And so what I would say is that
we have had changes in magnitude of issues and speed rate of issues, such that the previous
governance capacities, whether we're talking about markets, governments, idios, whatever,
aren't adequate to the scope of them. And that doesn't mean nothing could be. It means we have to
innovate in the social tech space of what is the social technology that is adequate to govern the
amount of power that physical technology gives us. Yeah. So what I've just heard you say in summary
is that there was essentially a break in history at World War Two for the purposes of this topic.
And that related to the technology of nuclear weaponry. But now we're facing a world that's even
one notch scarier than that, in that we're facing problems similar to what we face with
nuclear weaponry, but on all fronts, on the artificial intelligence front, on the biotech front.
And those problems are actually quite a bit tougher epistemologically because
nuclear bombs are fairly easy to understand relative to, for example, the problem of artificial
intelligence in the abstract. We really don't know, even the smartest people who spend all their
time worrying about this still don't really know what artificial intelligence is going to look like
a hundred years from now. But the problem of just brute force violence is
rather, you can make a nuclear bomb that's a thousand times as powerful as the most powerful
one that exists now. And I understand the nature of that threat. I just picture
an explosion but bigger. I'm not even sure what to picture exactly
with the problem of artificial intelligence. And that makes preempting it that much harder.
It also makes it more difficult to actually worry about it on an emotional level. It's easy
when you're watching a movie and they reduce the threat of artificial intelligence to something
personified, something intuitively scary to humans like killer robots. It's just
anthropomorphization of the problem of artificial intelligence. But the true threats are, I think,
abstract or more abstract than that, but nevertheless scary.
And so where does the idea of sense making and the breakdown of sense making institutions fit into
your view of the problem? Yeah. I want to take one detour first because you actually said something
super important about the fact that the bomb is in some way simpler than the issues that we face
right now or at least simpler to imagine or kind of wrap our heads around forecasting.
It's also the case that nuclear bombs are really, really hard to make
because there's not that many places in the world that have uranium. And then it's really hard to
enrich uranium. And as a result, it can't be an easily distributed technology. It takes
advanced nation-state level capacity to make it so that you are limiting the number of agents that
can play with this thing. And you can also monitor them. So the agents that can mess with it can
spy on each other. And so you can start to have something like mutually assured destruction
be a little bit easier because it's so hard to make them, right? And so where the uranium
mines, where is the stuff going, radioactive tracers, that kind of thing. When we start talking about
putting a thermite bomb on a homemade drone, homemade thermite bomb or whatever can happen
