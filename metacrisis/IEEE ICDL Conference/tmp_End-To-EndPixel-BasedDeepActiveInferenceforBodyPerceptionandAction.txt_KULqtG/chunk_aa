Hello, everyone. I'm Jan-Susanne Jaktar, and I'll be presenting our paper, End-to-End
Pixel-Based Deep Active Inference for Body Perception and Action, that I've worked on
together with Pablo Lenelos and Marcel van Geven from Donders Institute. So this work
is motivated by a neuroscientific theory, for instance, free energy principle. The idea
is that the human brain has an internal belief of the body state and a generative world model
that delivers predicted sensations given your internal belief. The idea is to maximize
the model evidence by minimizing the so-called variational free energy term, which is an
upper bound on surprise. So you want your predicted sensations to match the sensory input of the
real world. And you can achieve this in two ways. First is perceptual inference, where
you update your internal belief, denoted here by a mu, so that it matches your sensory
observations. And the second way is through active inference, where you act in the world
to resample the sensory data so that what you think you should be seeing matches what
you actually see, thus minimizing the prediction error. So these are interesting ideas to apply
to robotics, not only because it's biologically motivated, but because they formulate action
and perception into a single framework. Robots need to deal with errors in model, as well
as sensory noise. And they need to adapt the changes in the environment and the body. Free
energy optimization offers a solution to these challenges. The idea of applying free energy
optimization and active inference to robotics has been explored in the literature before,
and you can find some references to previous work here. We will focus particularly on Oliver
et al's work, where they use joint encoder values and 3D coordinates of the end effector
to perform reaching and tracking tasks via free energy optimization. However, the internal model
in this case relied on predetermined forward kinematics. In our work, we use deep learning
to scale the algorithm to deal with high dimensional inputs, namely images. So we can achieve dynamic
body estimation and goal-driven control in humanoid robots using only monocular camera images.
This gave rise to our pixel AI framework, pixel-based deep active inference, where our
sensations are camera images. The internal belief of the robot corresponds to the estimates of the
four joint angles of the robot's left arm, and we need a visual forward model that would output
a prediction of a camera image given an internal belief value of the robot. This corresponds to
the generative world model we mentioned before. In order to learn this mapping, we need a non-linear
function approximator. To this end, we used a deep convolution decoder that you see here,
and once this mapping is learned, we can perform a forward pass through the network
to get the predicted sensation. And this predicted image can then be compared to the actual camera
observation in the real world, and then we use the visual prediction error to perform free energy
optimization and update our internal belief values and or generate action values. A very important
component here is the orange term, the derivatives of the network output with respect to the network
inputs, because they describe the transformation from the pixel domain to the joint space,
and we can get this value by just performing a backward pass through the network, where the
idea is the same as in back propagation. For our experiments, we use the Aldebarano humanoid robot,
both simulated and real, and the inputs for pixel AI were the bottom camera observations,
as you see here. And we trained deep convolutional decoder, the architecture of which is shown on
the slide. Our input is four-dimensional, and it's the joint angles, and then the input goes through
two fully connected layers, followed by a series of transposed convolution and convolution operations,
and then we get an output image. We train this network on two separate data sets, one for simulated
and one for real now, and in both cases, the network output had only one channel, since we
converted everything to grayscale images. We performed two separate types of tests for pixel AI,
perceptual and active inference tests. To this end, we created a benchmark where we generated
50 random arm configurations, a subset of which is shown here. And then we performed 10 different
tests for each of these core configurations, using starting positions with three different
levels of random perturbations. And the difficulty of the task increased from level one to three,
as illustrated in the examples on the slide. For the perceptual inference tests,
the goal is to achieve dynamical body estimation. So here we set the robot arm to a certain position,
and it stayed there throughout the test. So the sensory visual information as well as the
proprioceptive information stayed the same. And then we set the internal belief to a wrong value
by adding perturbation to the actual joint position. What we want is for the internal belief value to
converge to the true joint angle positions by minimizing the visual prediction error between
the camera image and the predicted sensation. So here the faint arm position is the
true joint angle position, whereas the darker image is the perturbed initial prediction of the
robot. So here the results show that via free energy optimization, the predicted image converges
to the true position as the internal belief of the robot converges to the true angle positions.
The robot can successfully minimize the initial error that we introduce via perturbation.
So here are the benchmark results for perceptual inference. On the left side,
the visual prediction error between the camera image and the internal prediction is shown.
You can see how the error decreases over iterations of free energy minimization
for all three levels of the benchmark. And on the right side, you see the decrease in error
between the internal belief and the true joint angle values. Since different joint angle combinations
can have very similar looking camera images, there is a local minima problem. This issue is
most prevalent in level three tests with which have the highest difficulty, hence the larger
joint angle error you see here. So here are some perceptual inference examples from the real now
robot. And on this slide, you find the benchmark results for the real robot compared with the
simulated robot results, specifically on the L2 norm of error between the internal belief and the
true joint angles. And as you can see here, the real robot results aren't as good as the simulation,
which is expected since the images in the real world are noisier, for example, due to lighting
differences and the reflective surface of the robot. So in the second set of tests, we performed
image-based goal-driven control using active inference. So the faint arm poses here are imagined
arm positions. So these are our goals in the visual space. And just as before, the initial
start position is a perturbed version of this goal with, again, three different levels of difficulty.
And in the beginning, the internal belief is set to be equal to the current joint angles of this
initial arm position. And what we want is to generate actions via free energy optimization
so that the robot arm moves to this goal position. Here are two examples. And what happens is, at
each time step, the robot's internal belief gets updated in the direction of this goal, visual
attractor, which causes a mismatch between the internally predicted image and the actual camera
observation. To compensate for this, an action is generated to move the arm in the direction of
this internal belief until, iteratively, the internal belief and the actual arm position
reach this goal. And the actions here are generated from representation without an explicit policy.
So here are the benchmark results for active inference. On the left side, you see how the
visual error between the goal and the camera images is decreased as the arm iteratively
converges to the goal. On the right side, you see the decrease in the norm of error between the
goal position and the current arm joint angles for all three benchmark levels. As before, due
to local minima, we have the largest error for the level three tests. Okay. And here are the
results of active inference with the real robot. There is mechanical backlash in the actuators,
of the now robot, as well as noise in the sensors. Therefore, the results aren't as good as in
the simulation. To conclude, we have scaled up active inference to deal with high dimensional
inputs by incorporating learning with neural networks. We have provided a proof of concept
that we can perform image based dynamical body estimation and goal driven control.
As future work, we want to explore applications to industrial tasks,
as well as multimodal systems. We also want to use more abstract representations of the body
that will allow us to capture the complexity of the real world. And with that, I want to thank you
for your attention.
