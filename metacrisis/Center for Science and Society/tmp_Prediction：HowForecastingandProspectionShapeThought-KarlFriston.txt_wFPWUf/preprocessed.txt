commander.
You have a great pleasure to be here.
I unusual pleasure to follow on from your talk.
In fact I'm just going to say the same thing using different rhetoric but
there will be a challenge mapping your rhetoric on to my rhetoric and
O addysg ond ddim—odd it'ch allwch yn dd ôl i'ch gyn gallu ein ymell Careful.
Ond ydychu'n ei t 느낌 a'u gynnydd y lle ddweud.
Rwy'n ddweud cyflwygraduated內 yw eu recommendy.
Read it.
Nick yn relying...
..yi du'r trodd cerdd.
..eg siarad y twoysbeth nifer dargliadol sy'n roedd,
a grefai'r drwaid ywir hefyd,
ond rydym wedi'n goingnydd…
...y hoffwnolydd –
Yn tystiolaeth b就到'r poi yw awr i lacityn yn cy occas lawl ei wllaeth....
...neid felly nawr- Mann aber i statuol...
Ilyw'r rhedeg, y twyntобрion bened heddor yng Ng decoration
ond will Beir E Along teariad cyntaf ar gael Ilyw'r Gw Character
ac sydd wedi cy patriarch Field, wrth gwr argues y sia Firnau Ilyw,
Wrth golygu jestroriau llyniadau Expedis knobs Head of Newar
of information theory and variational principles.
These are just glorious terms for Hamilton's principle of least action.
And that's where the challenge comes in mapping the rhetoric,
but it can't easily be mapped.
So I would split my talk into two ten minute bits.
The first will be just to review for you what people mean by predicted coding
in its simplest or vanilla sense.
And I should just preface this part of the talk
by saying that normally when people talk about predicted coding,
they're trying to predict what is happening now.
So it's using internal explanations that provide the best prediction
for the sensory impressions at hand now.
And that's a slightly simpler problem than the reason that we're gathered here today,
which is a much more prospective what will happen in the future if I do this.
I think that distinction is very important, of course,
because it speaks directly to the goal-directed nature of our behaviour.
So the whole purpose of this talk is first of all just to describe
a predicted coding vanilla and then say how it could be generalised
to embrace the challenges of making the behaviour prospective
and fit for purpose in relation to what we are trying to optimise.
And I'm going to illustrate that using simulations of cyclic eye movement.
So how do we sample?
So the audience participation was a beautiful example of that
because I was only looking at, I was only sampling information
from the lowest buttons and I'm going to try and simulate that
using the principles that we've just heard about.
So I normally start this talk just by managing some of the key intellectual architects.
I think most of what can be said that is of interest
from the point of view of predictive coding conceptually
was said by Herman Helmholtz, beautifully articulated here,
with this phrase, objects are always imagined as being present in the field of vision
as would have to be there in order to produce the same impression on the nervous mechanism.
So what he's saying is in order to see something,
in order to fit your mind to the world,
you have to have in mind an explanation or a model
of what caused your sensory impressions.
And that fits very comfortably with Richard Gregory's notion of perception as hypothesis testing.
So the first bit of rhetoric mapping, learning equals perception in my talk.
So we often talk about perceptual inference or perceptual learning.
So when I talk about perception, I mean the learning, fitting the mind to the world.
These ideas have been formalised by people like Peter Diane and Geoffrey Hinton
and many others in machine learning, boring from Bayesian probability theory.
And indeed technical advances offered by Richard Feynman,
variational principles that allows to approximate Bayesian inference.
And I use that word now or that phrase approximate Bayesian inference now
because of course if our inferences are always approximate, they are always false.
So I love this idea of a good falsehood because in my world everything is false.
It's all approximate and that approximateness comes from variational free energy formulations
devised by in statistical physics.
So those are the players.
What then do they tell us about the notion of perception and action or learning and action?
Well, this phrase, the same impression, the sensory impressions,
suggests that we are in the game of trying to explain the causes of the sensory impressions
that fall upon our sensory epithelia, say our retina or our ears.
So here we have some sensory impressions and we'll denote them by S.
So how might this notion of minimising prediction error that we've heard about,
how might this notion of optimisation be invoked to give us some feel for how the brain would actually do this in a biophysical sense?
And the example I'm going to use is predictive coding.
It's also known as Kalman filtering or Bayesian filtering.
It's also known as a minimisation of variational free energy.
It's just a variant of Hamphill's principle of least action.
Two twists or takes on the mathematical formalism.
And I'm introducing them deliberately because one is a little bit high flying,
but it will be useful when we come back to the challenge that has been set up in the first talk.
The other one is slightly more intuitive, cast in terms of prediction errors and it's useful to explain,
but they're both talking about exactly the same mechanism.
So I've cast both descriptions mathematically here in terms of a flow of things called expectations about the world,
that are expected causes of the world, that is either doing a gradient descent on variational free energy from Richard Feynman,
that can be mathematically written down in a totally equivalent way in terms of predictions and updates.
So what do these terms mean?
Well, the prediction is that if I'm given, if I have some expectations about the current state of the world,
then I have some beliefs about what will happen next.
If I had available to me the mistake or the mismatch or the prediction error afforded by a sensory sample from that world,
then I can make a little correction to improve that expectation.
So let me unpack that a little bit more intuitively.
Let's say we were given the sensory impression, the shadow here of presumably some form of canine.
If I wanted to explain that, I could have an expectation denoted by mu.
Now, if this was the cause of these sensory impressions, and if I had a generative model,
a model mapping from this expectation or belief to the sensory consequences of this expectation or cause,
I could simply create a prediction of my sensory input, g of my expectation, a generation of the prediction,
and subtract it from the actual sensory input to obtain a prediction error.
So the prediction error is just a mismatch between what I'm actually sampling and what I thought I was sampling,
given I had some expectations about the state of the world.
All that this equation says is that everything that can change will change
to minimise a sum of squared prediction error weighted by their precision.
So that we can explain all of brain function simply by minimising prediction error.
Now, again, this nice notion of falsehoods speaks to the fact we don't necessarily need to know what the truth is.
All we need to do is to minimise a prediction error, and that is perfectly good.
If I live my entire life with a minimal prediction error, then I will be well happy.
I don't need to know the truth.
That is nice because, as we've already seen,
it accounts for the reciprocal directions of fit between mind and world.
If we just have to minimise prediction error, we can do that in one or two ways.
We can either change our internal brain states, our synaptic activity or connectivity,
that encode those expectations to make our predictions more like the sensations that we sample.
Conversely, we can change the sensations by actively sampling them again,
either through visual palpation or placing ourselves in a particular relationship to the world,
so that we can change the sensory samples to make them more like our predictions.
In both cases, we are minimising prediction error just by changing either side of the equation.
We are conforming to this imperative to minimise the sum of squared prediction error or variational free energy.
Of course, from my point of view, that would be the rhetoric of action and perception,
fitting learning and action or fitting the mind to the world or the world to the mind at many, many different levels,
but all happily subsumed under one simple principle.
If I have predictions, or if I want to elaborate a prediction error,
then I have to have predictions and I have to have a model of how my sensations were caused.
Of course, for us, those models have to be very deep, hierarchically structured,
that would be very dynamic, extremely non-linear, high dimensional.
I have just cartooned what we get from that predictive coding equation
in terms of an understanding of neuroanatomy and physiology in the real brain,
because if it is the case that the brain uses a model to generate predictions
of what it should be seeing at this point in time,
then the anatomy of the brain should entail and embody that model
and should similarly have the same sort of hierarchical, non-linear and dynamical structure.
So, let's just think about the nature of these models.
This is actually a graphical model, which some of you will be very familiar with,
that contains hidden states and causes that cascade down to generate sensory impressions right at the bottom here,
all subject to random fluctuations or uncertainty.
A more intuitive version is here.
So, imagine I asked you to generate some synthetic data
that reproduced foveal samples from the retina for your students, for example.
How would you do that?
Well, the first thing that you would have to do in generating these synthetic data
was to work out what is producing the sensory samples
and where you are sampling.
You'd have to work out all the kinetics and motion-opter, motor dynamics,
that sample this object and equipped with this what and where causes.
You would then be able to mix them together in a deep cascade,
non-linearly, to actually generate a time series of sensory samples here.
So, the generative model maps from causes to consequences,
whereas fitting the mind to the world inverts that, is literally Bayesian inversion.
It's literally this predictive coding.
It's trying to solve a problem of going from the available sensory samples
back to what caused them.
Of course, that's exactly what that predictive coding equation does.
And when we actually look at the form of the equations
and how they might be mapped onto real brains,
then something very simple emerges.
We retain this hierarchical structure,
but instead of the random fluctuations entering at each level,
they now take the role of prediction errors
and start to ascend the hierarchy.
So that as the top-down predictions are used to form a prediction error,
the prediction errors are passed back up again
to assist or revise or correct the expectations at the higher level.
So, basically, we have a descending stream of predictions
in exactly the same way that you generate these synthetic data for your students.
That are accompanied by a counter stream of ascending prediction errors
that are correcting at each point.
And this is very reminiscent of real brain hierarchies
where you have reciprocal coupling of backward and forward connections
that are all in the game of suppressing activity,
eliminating prediction errors at each and every level of the hierarchy,
such that when that process has finished, given some new sensory samples,
you have a minimal prediction error,
a maximum a posterior expectation of what caused your sensory input
with multiple levels of abstraction.
So that's the basic idea.
Let me just rehearse that for you in the context, say, of sampling the world.
Here's a little cartoon of a brain.
As you imagine, we have some visual information coming into the retina here
that is passed to the lateral geniculate body,
and it is in receipt of predictions from, say, the visual cortex,
and that forms a prediction error,
and the prediction errors are passed forward to revise or update
the expectations or beliefs about the elemental causes of this visual input.
Of course, these expectations are themselves being predicted
by high-level expectations or beliefs,
and so there's a high order prediction error that's passed forward
to revise these expectations that themselves will be predicted
and so on to any arbitrary hierarchical depth.
Now let's look at exactly the same story,
but from the point of view of proprioceptive input,
input from my eye muscles that tell me about the state of my effector organs
that I deploy to actually sample the sensory information that I'm trying to fit.
So I cartoon that here in terms of proprioceptive input from the eye muscles
that come to the brainstem here,
and they are predicted by, say, descending predictions from the frontal eye fields,
and they form a prediction error that could have sent to revise my beliefs
about where I'm looking.
But here's the clever thing.
These prediction errors can suppress themselves directly
by cafling back to the real world,
by causing the muscles to contract until the proprioceptive signals
match the top-down predictions.
And all that I've just described is just a classical reflex arc.
So I don't need to revise my beliefs.
Of course, my beliefs are actually fit for purpose,
provided that my top-down predictions, my chronology discharge on my motor commands,
are realised reflexively by, in this instance, ocular motor reflexes.
So that's fitting the world to the model in a very, very simple way.
The world has just the sensation of my effector organs.
But notice these predictions, these self-fulfilling predictions,
are deeply informed by a hierarchical synthesis from all modalities.
So these are not just predictions about my eyes,
but they are contextualised by the context, what I want from the world,
and they're informed by integrating an amazing sense,
all the information that I come from multiple modalities.
So that's the basic structure that I'm going to use.
I'm going to close now, let me just quickly summarise,
where biological agents minimise their surprise or prediction error,
or their entropy, they minimise that by suppressing prediction error,
that can be reduced either by changing predictions,
maybe a perception or learning,
or by changing sensations through active sampling.
This entails perception, particularly coding,
entails recurrent message processing to optimise the predictions
while action makes those predictions come true,
and therefore minimises surprise or free energy.
So what I want to do now is introduce one of my favourite equations,
and I'll do it very gently.
It takes us back to now, we're back in the high church of variational calculus
and free energy, and I have to do this in order to speak to
the notion of what do we want to do when we sample new information,
what is our goal.
I have to say, it sounds a little simple-minded,
but it is for me the truth,
the only goal is to minimise free energy.
The preferences actually come into the free energy
through pride beliefs or pride preferences.
So we heard about goals, desires and pride preferences.
In my world, those are pride beliefs about the states
that things like me should occupy.
Therefore, if I minimise my surprise or my prediction errors,
then I will implicitly end up in my preferred states.
But there is another aspect to free energy,
which I want to explore with you at the moment,
which is what we've already heard about, the epistemic part of it.
So what I've done here is express free energy in its usual way
in terms of an energy and an entropy.
Free energy is usually an energy minus an entropy,
where the energy is a sort of information theoretic energy
that comprises the likelihood,
the probabilities and sensory samples given their causes,
beliefs about how those causes were engendered
and pride beliefs about the controllable aspects
of the hidden states of the world generating sensations
look like the positions of my eye.
Crucially, these pride beliefs entail preferences
or beliefs about what I will do.
And here's, I think, the most important point
that I wanted to communicate today.
If it is the case that it is sufficient to minimise free energy,
surprise or prediction errors,
then the only self-consistent belief that I have about my action
will be that I will act to minimise my free energy,
surprise or prediction errors in the future.
And I can write that down just by making the probability
of my control states where I am looking at the present time
and expected free energy following an action.
And if we unpack that, and I'm not going to do that here,
we can actually decompose that into the pride preferences
that we've already heard about, the goals.
So, we had to do the smaller numbers,
you had to do the bigger numbers.
That defines the hypothesis space.
And we want, we would believe ourselves in this instance
that we will provide the correct answer.
But there's another part here, which I want to focus on,
which is the epistemic value or the information gain.
And put simply, this is a component of free energy
which is all about reducing uncertainty.
And basically, all this reduces too,
is that if you believe that we act to minimise our expected free energy,
what you're saying is we are only in the game
of resolving uncertainty about hypotheses or beliefs in the world.
And I think that's really important because much of what you were,
I think trying to convey was the way that we carve our world
in terms of a hypothesis space has enormous effect
on what we do and how we resolve that uncertainty.
So, the nice audience participation example here,
we just have the hypothesis there was a minimum card
and a load of other uninteresting cards.
Whereas for you, there was a high card
and a load of uninteresting cards.
So, both of our hypothesis space were a bipartisan.
And then you were very cheekily challenging the poor subjects.
Well, in fact, there was a hypothesis space
that comprised three things, high and low cards and possibly,
yeah, intermediate cards.
So, I think it's a beautiful example of inducing a model space,
a hypothesis space.
All this is saying is everything we do is in the service
of resolving uncertainty about those hypotheses on the table.
And I'll quickly illustrate that,
how that, say, can drive cicatic eye movements
in the last slide or last couple of slides.
So, just an illustration of that
that will be familiar for those people
who are dealing with active vision and salience,
possibly also attention to a certain extent,
when we're talking about attention
just before the start of this workshop.
So, let's say I had a hypothesis
that my visual impressions, my samples,
foveal samples, limited to this little portion of the screen here,
put it by the circle,
were generated by this vast or this statue here.
This epistemic value of information game
basically scores a reduction in uncertainty
about that hypothesis, if I sample there.
And I can actually compute that, it's not difficult,
and I can compute it for every central location
determined by where I am looking you here
and create a map of the epistemic value information game,
or salience.
And this map actually does look as if it is a sort of mathematical object
that actually attracts cicatic eye movements
here from the classic work of Yabus.
So, let me just see if I can simulate this active palpatience,
visual palpatience of the world.
I won't go into this, it's just the same sort of graphical model
that we used in before,
and then convert it into a predictive coding scheme.
Here, I just want to show you the results.
So, in this example, it's the last slide now.
The agent has a hypothesis base of just three things
that can cause its sensory impressions.
There's a face out there, there's an upside down face,
or there's a face on its side.
So, its hypothesis base is threefold,
and what it has to do is to resolve ambiguity
and decide what's causing its sensory input.
So, what it's doing at each point is working out the most salient
or the parts of the visual field.
It should sample to resolve as much uncertainty as it can,
or minimise its free energy,
expect it free energy as openly as it can.
What this produces are a series of saccadic eye movements,
shown here in terms of the complete sequence here,
and here graphically in terms of the movement of the circle dot.
So, here is where the agent is looking.
These are the salient's maps driving these prior beliefs
that I will sample the world to minimise uncertainty,
or resolve uncertainty, simulated EOG,
what it actually samples,
and here's the most important graph here.
It shows the expectation of the true, normally-orientated face
relative to the beliefs about the two competing hypotheses,
the sideways of the inverted face here.
And what we can see here is that within saccadic eye movements,
and between them,
there's a progressive reduction in the uncertainty,
or the 90% posterior confidence intervals,
of a rather saltatory sword.
And of course, as it keeps on sampling,
it confirms its hypothesis and continues to resolve uncertainty,
and it happily decides this is what I'm looking at.
Of course, if it didn't,
then it would try to explore different hypotheses.
So, I'm going to give the last word to Helmholtz,
each movement we make by which we alter the appearance of objects
should be thought of as an experiment designed to test
whether we have understood correctly the invariant relations
of the phenomena before us,
that is their existence in definite spatial relations.
And with that, I'll thank the people whose ideas I've been talking about,
and thank you for your attention.
Thank you very much indeed.
