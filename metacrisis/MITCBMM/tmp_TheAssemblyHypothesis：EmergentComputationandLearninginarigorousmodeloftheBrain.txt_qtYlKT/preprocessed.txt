This is a really exciting framework on the assembly hypothesis.
We'll be hearing about it from Santosh Rampala who's really been pioneering this work for the past,
maybe six, seven years is how long you've been working on it.
And hopefully, every time, so I've seen different iterations of this talk over the last two or three years,
and every time Santosh has a new exciting demo that it makes you really wonder how something so complicated comes out of a really simple model.
Looking forward to hearing what Santosh has been working on recently.
Thanks.
Great.
The talk is a hypothesis, right?
So I'm looking for counter examples and suggestions to build an actual theory.
Yes, I have the word emergent there. I know that's a little controversial, but you'll see what exactly we mean.
Please interrupt lots of questions. It's meant to be a complete model of the brain, so that should already be controversial.
But mathematical, so of course it's a toy model and completely rigorous.
So every detail is specified.
So before that, let me step back a little bit and ask what is computation.
Take a minute to do this.
And I mean something extremely general.
It's just a well-defined sequence of state changes, perhaps with purpose.
That's in brackets, but the state and the change of state.
And so what is a state?
State could be memory content, input, output.
That's your state.
And then there is the Church-Turing thesis, which says that anything computable can be computed by a Turing machine.
It's worthwhile remembering.
Again, this is the model.
It's a thesis, not a theorem.
But it's been very, very useful and powerful.
And Turing machine, maybe we'll discuss a little bit later, it's just a finite state machine plus a tape where you can store stuff along the way.
Now, the idea is computation is universal.
We think of planetary systems as computing their next positions in time.
Weather is computational.
The temperature and pressure right here is something tomorrow, maybe outside, more interesting.
And it will be computed by nature, by change of state.
Changes of state.
Metabolic networks in your body and colonies and of course brains.
At this point, it's completely clear that the brain is a computational system.
Now, here is my, I'm presenting this slide not because I'm a neuroscientist, far from it, but because this, it's useful perhaps for you to know my view of the brain.
So the next model is passable.
So the brain, a network of 80 billion neurons, 1,000 to 10,000 connections.
These connections synapses are directed and have strengths.
Some, they may be new ones and they might, they might disappear.
And then individual neurons spike or fire based on rules that are nonlinear.
And a very common model is a threshold of weighted input, but there are more than a thousand different types.
And these firing, these signals have temporal aspects with rates and patterns.
So that's the, that's the completely, this, this is the nugget from, yeah, neuro.
And the question we're going to be, I should say naive enough to ask is how does the mind, the perception, you know, perception, cognition, higher level thought emerge from the brain,
which we think of as a substrate with neurons and synapses, as we just mentioned on the previous slide.
So just to emphasize again, this is an exciting time to be in this field, as it has been for the past 50 years.
Despite the great accelerating progress and lots of insight that keeps on coming, there's no overarching theory.
It's not like physics or, you know, there is, there is, there is a theory, even if it may not be complete,
and even if it's yet to be tested or there is evolution in biology.
Here in the brain, there isn't really much of a theory.
And so there is this gap, and this quote from about five years ago now was, continues to be a great inspiration for, for us.
So Axel said, we do not have a logic for the transformation of neural activity into thought,
and I view discerning this logic as the most important future direction of neuroscience.
Okay, so what kind of logic would qualify?
You know, what kind of formal theory would, would be reasonable?
Going back a little bit, there are things like Hopfield nets that are specific to, for specific tasks, with very nice properties.
Back in 95, there was a less valiant who also proposed PAC learning, came up with the Neuroidal model,
which is, is, is very interesting and allows for extremely general operations,
such as state changes on individual neurons, individual synapses, in order to capture computation.
So what are we looking for?
We're looking for a computational system that's consistent with our understanding of the brain,
may not capture every aspect, but it should not directly contradict some basic feature.
It explains cognitive phenomena, and then the question is, what are its basic data types and what are its operations?
Okay, so we have to ask, then, what's the right level to think about this?
There's the whole brain, there's neurons and synapses that we can understand in detail, dendrites, molecules,
there's interesting things happening at all these levels, but maybe something intermediate.
And so here's the, the, the, where we're going to take this, we'll call it NEMO, Neural Model.
It's a formal probabilistic model of the brain.
There's one basic data type, just one, right, and a few elementary operations, a handful.
There's a completeness theorem about what can be computed using this,
what, what, what kind of things can be produced, and what you might call a killer app.
So there's a bunch of papers, I'll just, we don't need to read them now, but there's a building theory.
Okay, there's one mathematical background slide I want to put up.
Forgive me if you're already very familiar with it, and that's the notion of a random graph.
So the, the stand, the classical model is the Erdos-Renyi random graph, which is a graph on N vertices,
and every pair is connected by an edge independently with probability p.
So you toss a coin of probability p, if it comes up heads, you put in an edge, otherwise no edge.
That's the model.
I mean, no graph in nature actually behaves like this, but it's a fantastic model for almost anything.
So zero would be an empty graph, one would be the complete graph for clique,
and this model has lots of very nice structure.
For example, you might expect that the maximum degree is concentrated near whatever its expectation is, it is,
but so is the size of the largest clique in the graph.
And as p increases from zero to one, and this is a more surprising property, any edge monotone property,
so a property that once true remains true upon adding edges.
So for example, is the graph Hamiltonian.
If you, if, if, maybe it's not, at some point when you add edges, it becomes Hamiltonian,
and then it will stay Hamiltonian if you keep adding edges, right?
A property that's not monotone will be planarity.
Is the graph planar, you add edges, you might break it.
Any edge monotone property has a sharp threshold, meaning there will be a narrow interval with less than any constant
of probabilities within which this property will go from being true with probability zero to bring true with probability one.
For example, the graph being connected, the existence of a matching, a perfect matching, let's say, or a Hamilton cycle, etc.
We will look at the directed version of this where every pair that, you know, the pairs are ordered,
and the edge from i to j is present with probability p, and j to i might also be present with probability p independently.
So this is a simple unrealistic model of a network, but it's going to be very useful, and we can think of it as a model for the connectome.
Yeah, so this, this, the, the classical model is undirected.
We will also use, we will also use the directed model.
In fact, for the brain, this will be the more relevant one, because as you know synapses are directed.
So in the next two slides, I'll give you the complete model, okay, and then we'll see what, what comes out of it.
Okay, so we think of the brain as a finite number of brain areas.
Finite means a small number, a dozen, half a dozen, brain areas, okay.
This is conceptual, right, doesn't have to be a physical thing, this is about organization.
Each one contains n neurons, n is a parameter.
They could have different numbers, but for the purpose of this model and for everything we've derived so far, we can assume they all have the same number, n.
In each area, so each of these regions is supposed to be a brain area, in each area, only k or the n neurons are allowed to fire.
k is going to be something much smaller than n, and only k or the n neurons are going to be allowed to fire, okay.
This is a sort of a version of plastic, of inhibition.
Some pairs of areas are connected, maybe not all pairs, and they might be directed.
For example, this area is connected to this area only in this direction, that area is connected by directed, so on.
And all areas are recurrently connected.
Everywhere will use edge probability p. Again, this could be different, there could be denser areas and less dense areas,
but let's just say within each area there's probability p of connection from one area to another area, if there are connections, every pair is connected with probability p.
So there's only two parameters, n, p, and the number of areas here, okay.
That's part one. Now what about the activity, the dynamics?
We'll assume that there's discrete time, simplicity, so there's some clock.
Neurons will fire only in these discrete steps, and which ones fire?
At each step, it's the k neurons with the highest input from the previous step, highest weighted input.
So the synapses are also weighted, they're weights, which we assume are non-negative.
And the highest total weighted input, the top k, will fire.
Now connections between areas, so from area one to area two, even though they exist, could be inhibited.
Like this connections are not allowed, nobody feels the firing activity there, or disinhibited, meaning you do feel.
There is no geometry at the moment, this is just a topological setup.
It's a metagraph that's telling you which areas are connected.
So we abstracted that away and just said that only k neurons in each area are allowed to fire, that's by inhibition.
So there is, if you like, for each area there's a population of inhibitory neurons that makes sure that at most k are firing.
How is that? Because the activity of these will stimulate the inhibitory neurons, which will then suppress the firing activity and it will stabilize to k.
At the moment, yeah, k is the same, it could be different, but we'll just assume n, k, p are just same across.
It's just population control, so each area has its own population of inhibitory neurons, which are randomly connected.
And this population of neurons, as you know, inhibitory neurons integrate faster than excited neurons, and this control actually happens very fast in a wide range of sizes.
k, every second k are firing within each area.
Great, now the second aspect of dynamics, so this is firing, the second aspect of dynamics is the weight change, plasticity.
And we'll assume the simplest form of Hebbian plasticity, which is that if neuron i fires and followed by neuron j firing in the next step,
then the connection from i to j is strengthened by a multiplicative factor of 1 plus beta, beta is a fixed plasticity rate, again same across everything.
i fires, then j fires, this connection increases by 1 plus beta.
That's it, and other things, you know, homeostasis is just to make sure that the weights don't go, because everything is non-negative, abstract away.
So every once in a while, each neuron will normalize all its input weights back down to 1, say.
Yes, no, it can happen very infrequently, we just don't want things to go off to infinity.
No, this is the complete model, I have not left out any detail, this is it.
No hidden hyperparameters, no cleverness in the training, every synapse has plasticity, every synapse has plasticity and the same plasticity.
Because we have made time discrete, that's an assumption of the model, time has some clock.
Oh, you know, I mean, is time discrete or not, I mean, I don't know, I mean, it just depends how fine you make it.
Yes, it is, one would like, I don't know, is time discrete or continuous?
In the model, it's discrete, it can be whatever scale it is, but it's discrete.
In reality, I don't know, maybe we have a real physicist here, but...
As far as the model is concerned, forget about inhibition, in each area out of the N, exactly K-fire, that's it.
And then there's a story about why that's realistic, and the story says that you can achieve K out of N-firing by using a population of inhibitory neurons for each area.
That's a back story of why this model is realistic.
How realistic is this? Well, it's reasonably so, because it's a completely precise model.
The discrete step assumption, as somebody asked, is unrealistic for multiple reasons, but hopefully it's not distortive.
We still get something useful.
Plasticity assemblies are used in this model in a rapid time scale, which is true for some aspects of brain activity,
but there's also plasticity at slower time scales, and so on, which is not incorporated.
And so hopefully this is a useful compromise between reality and usefulness, without contradicting some important thing from neuroscience.
But then the question is, will computation and learning emerge from these rules, which don't seem to have anything to do with it, without having to be programmed?
That's the point.
So what is the basic data type? Larger than a neuron, smaller than the brain.
And so the basic data type is something that neuroscientists have proposed for a long time.
It's an assembly of neurons.
And what is an assembly? It's going to be a large and densely interconnected, so more dense than the base density of synapses.
And the point is that this subset has the following interpretation.
Its firing is equivalent to recall of a particular concept.
So there will be an assembly for a person, an assembly for this room, an assembly for this course, every concept, everything that's stored in your memory is an assembly.
So this subset, when it fires, it means you're thinking of a panda, or maybe a specific panda, whatever, that could be different.
Boussaki calls the assemblies the alphabet of the brain, but we're calling it a data type.
He's not a computer scientist.
So what is the assembly hypothesis?
There is an intermediate level of brain computation for, let's say, the assembly calculus.
That's going to be implicated in higher cognitive functions, reasoning, planning, language, etc.
And assemblies of neurons are its basic representation, the data type that's doing this.
So then I have to tell you what are the operations?
And again, the operations should happen by themselves, but what are these operations that we can capture?
Remember, we didn't have anything called assembly in the model I told you.
They have to emerge.
Even the assemblies themselves have to come out somehow, we'll see.
But operations on them also have to come out somehow.
Project, which just means, let's think of some of the areas as sensory areas where the neurons that fire in the sensory area are due to external stimulus.
Maybe odor receptors, maybe things in your visual field, so on.
So that's happening because you smell something.
And that projects to a different area, a higher brain area.
And projection just means that this activity in the sensory area is creating a representation,
meaning an assembly in a different brain area, a copy of itself, if you like, we'll call that projection.
Which later, when you smell again, this will fire again.
Associate, which is this operation of things that co-occur, start increasing their overlap.
And therefore, when you think of two things that seem to co-occur, seeing one or sensing one ignites memories of the other, or fires the other assembly.
Pattern complete.
Seeing a part of an assembly allows you to see the whole thing.
Merge.
Assemblies can be hierarchical.
There's no reason for them to be flat.
You have an assembly for woods whole and for neuroscience.
And you think of, oh, the summer school, and so on.
And a few control commands we'll discuss in more detail.
So here's what projection looks like.
This is an advanced simulation, so please focus.
On the left, you have a sensory area.
And on the right, you have a brain area, some other brain area.
And so here, you have an assembly firing.
That's supposed to be k neurons.
And that results in some top k neurons on the right firing, presumably the ones that have the highest connectivity to that subset.
And now, what happens in the next step is that the input, the sensory area neurons are still firing.
You're still smelling something.
And meanwhile, these k also are firing.
Now, what's the next top k?
Doesn't have to be the same one, right?
So there's some other top k.
And this fires now, and there's some other top k.
And you've got this process.
The question is, will this process lead to any kind of convergence?
So the sensory area is still firing.
You're smelling the same thing consistently.
You have a top k, top k, top k, top k.
Is it going to converge?
The stimulus for now, let's say, is held.
Because top k depends on the previous top k.
The top k is sensing not only the stimulus, but also itself.
There are recurrent connections here.
The k neurons that have the highest total weighted input from the previous round.
Yes, from one step to the next step, if i fires, then j fires right after ij connection will be strengthened.
That's the rule, yes.
You would expect that, OK, the first cap had the highest input from there.
They have the advantage.
And now, if somebody inside also got more from this top k, which is just a random graph,
maybe there should be some small fraction at least that has this advantage.
But then, yeah, I don't want to go back to the big bang, but let's just say there's some current activity.
But the introduction of a stimulus is a huge change to the system.
Because the total firing activity is going from k to 2k, specific one.
For example, to me, that's as good an assumption as it is.
So here's what we can prove about this.
The projection process converges exponentially in a wide range of parameters.
I'm not playing with high probability.
The high probability is only over the initial random graph.
Everything else is deterministic.
It's just the original connect home that's random.
And what does convergence even mean?
It's not going to be the case that you will just get to one subset k that keeps firing.
That's actually false almost always.
What instead will happen, what we'll be able to show, is that the total support,
meaning you fire k and the next time there's another subset, another subset,
if you look at how many total neurons were activated,
not counting the ones that are repeatedly activated,
then that total is only a little bit more than k.
It's k plus little o of k, that's the number that vanishes as a fraction of k.
As long as the plasticity is more than a certain threshold.
So if the plasticity is above a threshold,
you converge with very few additional neurons fired.
And if for any positive plasticity there is convergence,
but you'll see a much larger number of neurons touched in the process.
And the threshold that's provably valid is basically a constant
over something that depends on the density.
So pk is the expected degree, right?
Because there are k neurons firing, p is the probability of an edge,
pk is the expected degree to the firing set.
And as long as pk is bigger than this logarithm, this is a constant,
so we're talking about a constant overall.
So this one, these are both upper bounds.
So it's no more than this and no more than this.
But when beta is less than the plasticity,
you see this number is bigger than this number, right?
At least one very interesting question you're asking,
which is, is there a sharp threshold here?
And that's a question that I'm going to put at the end of my slide too.
Is there a sharp threshold, does the plasticity parameter
give you a sharp threshold for the creation of these stable assemblies?
I do think that if beta goes, and that shouldn't be too hard to prove,
sufficiently smaller than this, maybe by even just a large constant factor,
then it will, the number of neurons you touch does go up.
So the convergence just is in terms of the total number of neurons
that are activated at any point in time in this process.
So the process is there's k sensory neurons that are being fired,
and then the top k are going around.
Every time you do top k, potentially if I do t steps,
there could be k times t neurons touched.
But this is saying no, because of plasticity,
as long as the plasticity is greater than a certain threshold,
the total number of neurons that will fire is actually k plus a tiny bit more.
Yes?
N is the number of neurons in the area, which is typically much larger than k.
No, no, but what, no, no, beta equal to 0 is not obvious.
What happens at beta equal to 0 is that the number of neurons becomes,
and this is not hard to show, at least the polynomial in N,
N to the, like, you start hitting, you know, even if, yeah,
you start hitting a large number of neurons, yeah, good.
Okay, so that's the first step, right?
Very basic thing, creation of memories.
Okay, so this is, assemblies are created,
and now what are these doing?
So it takes about a dozen steps, and these are plots here,
which are showing you how many neurons actually get activated,
and when you have high enough plasticity, you know, it's very close to the k.
As your plasticity becomes smaller and smaller, this is the plasticity value,
you know, you start seeing more and more neurons touched in the process.
Simulation, this is just simulation.
All of the code is publicly available.
I'll put it up.
For those of you who are curious how such a proof might work,
or you see, it's a little bit, how should I say, non-standard,
in the sense that you can't really apply a general principle from dynamical systems,
come up with a Lyapunov function or some nice potential that converges continuously,
because the fixed point is not a fixed point in any standard sense.
So we have to combine some discrete reasoning with this,
and that's what we do with probability about what we know about the random graph,
or what we can prove about the random graph.
And so what happens at every step, the main thing is that there is a competition
between the previous winners who are at an advantage,
because they had the most input from total input,
and a large population of neurons, and just because the population is large,
the best of these are genuine competitors to who will make it to the cap.
And so there is this competition between previous winners
and a much larger population of, I say, not at an advantage players,
and this balance for high enough plasticity tilts in the favor of the previous winners.
Yeah, and you can establish these thresholds.
So the fraction of previous winners that survive as winners
just keeps increasing exponentially fast.
Or the fraction of new winners decreases exponentially fast.
So that's the proof.
I'm happy to maybe save this for discussion later.
Now here's the first property to get out of these assemblies.
First thing is recall, which means that if you five present in the future
the same stimulus which created the assembly in the first place,
then I don't need to wait for these iterations.
It immediately fires the assembly that was created,
because now there's recurrence.
One fire, boom, that's the one that gets lit up.
What about firing only a subset?
This is one of these really nice properties of Hopfield nets,
and this we get this.
In particular, for any epsilon that you can choose up front,
if you present the stimulus sufficiently many times,
you could have stopped when the assembly was, let's say, stabilized,
but go a little bit further, present a few more times, rehearse.
Then igniting an epsilon fraction of the assembly
completes to the whole thing very fast,
just because within the assembly the weights of these connections
will go up with each presentation.
So this is the benefit of recurrent connections.
You don't have this in insects, but you have it in mice.
The next one is association.
If I have two stimuli that are firing, that are co-firing,
and they already have assemblies in here for them,
maybe you learn them separately,
over time their overlap increases.
The number of neurons that are actually in both increases.
There is this experiment that was for us very useful.
It came out around the time, started working on this,
I saw it at all, where they did this on a population of epileptic, yes.
So this is what happened in the experiment.
They were recording from several hundred neurons,
I think about 600 or neurons,
and in the empty of the medial temporal lobe,
and they first presented familiar, well-known places,
and recorded from 10 to 20 neurons they could see
consistently firing for specific images.
And then well-known people,
and also they saw activity and very little overlap.
And then they did this interesting thing
where they are superimposed and presented those.
Okay, fine.
And then presented just the place again.
So they deliberately created this association
between two existing assemblies.
And guess what happened?
Some of the neurons that were previously firing,
only for Obama, also started firing for iPhone.
So there was an actual increase in association
that they noticed from this experiment.
And this is a provable outcome in this model.
It's a theorem in the model that if you do this together,
you will actually increase this.
There are more complicated operations
where you build hierarchies of assemblies using merge.
And so let's just recap.
Here are a few operations.
And now for now, I'm saying there are also a few control commands,
which is something I promise shouldn't be there,
but let's say for now there are,
where you can say activate an assembly,
disinhibit an area like you were asking,
meaning that area is not allowed to,
you can't have activity going into that area,
you know, you cut off and so on.
Okay.
For the moment, using these control commands
and the set of operations that naturally run,
there's no special program for them,
you could ask how powerful is the system?
What can you actually, what computations can you execute?
And the answer is it can perform arbitrary computation,
anything that you can do on a Turing machine.
Using square root and space,
you can do in this model with half a dozen areas
and n neurons per area.
Okay, so that's the general,
that's what we mean by a completeness theorem.
By the end of the talk, I'll show you
that we don't need any control commands,
that everything will be just done using a hardware setup,
which is these brain areas and a random graph,
and a stimuli, sequence of stimuli.
Right, so how sensitive is this to noise?
It's a good question.
There is certainly a threshold of noise
that can tolerate automatically up to the point
where top K are not changed, for example.
So if the top K itself, let's say, is an approximate top K,
I expect that we should get very similar inspirations
in terms of theorems, but actual theorems,
but it's not proven yet.
So the noise sensitivity of this whole setup
is to be established.
Okay, so in the rest of the talk,
we'll get to sort of higher level things
that you might hope for from this learning course,
sequences of inputs,
which seems to be a very fundamental thing for the brain,
and then giving up control.
Control commands that say inhibit an area,
or inhibit this pair of areas and so on.
And finally, moving away from G and P,
from this complete random graph assumption,
we know the brain has geometry,
and what about it?
And also mention language.
Okay, so how do brains learn?
Well, this is a very important problem for neuroscience,
also for this model.
And the only rules we have are plasticity.
So that's it.
It's just local.
Plasticity is the only change in what's happening.
And so there isn't much evidence
that there's gradient descent going on,
even though it's super successful in machine learning.
And so the natural question is,
is synaptic plasticity an actual effective learning mechanism?
Okay.
So as a first step to demonstrate non-trivial learning,
you can ask, suppose,
so we know that the assembly projection,
you know, projection is effective,
but can it create higher level assemblies
that represent a class of stimuli,
rather than one per stimulus?
So for example, if you draw inputs from a distribution,
can you have one assembly created to represent the distribution,
and then another for a different distribution
that's sufficiently different,
so that when you get something from this distribution,
you get assembly one.
When you get from the second distribution,
you get assembly two, right?
So it's a higher level assembly, but representing the class.
And so the answer is yes.
You know, after you define this stimulus classes properly.
So one definition for which,
now this is the definition of inputs and classes,
not of the brain model.
The definition is just that, suppose you have stimuli,
which let's think of stimuli as zero-one vectors,
you know, sensory vectors,
and two different classes have two different base subsets of neurons
that they're anchor subsets, if you like,
that have very little overlap,
and then the rest of the neurons can fire randomly with some probability.
So half of their weight comes from the base anchor set of neurons,
and then the other half is spread out randomly over the rest.
So if you look at these distributions,
they look focused on these case subsets and spread out randomly.
So they're pretty different from each other.
But any two stimuli are quite different from each other, too.
And now if you present these,
it turns out it forms a higher level assembly for each stimulus class
after a small number of presentations.
So, yeah, what I'm doing here is summarizing the theorems,
and then we'll go over the simulation.
I think we'll have time for that.
Okay, what about a more classical learning theory problem
of learning a half-space or a linear threshold function?
So let's say there is a half-space, so there's an unknown vector v,
so a linear threshold,
and what we're going to do is pick Bernoulli input vectors.
We'll just present five examples that are one side of the threshold
and five from the other side,
or actually we can just do it with one side and the rest.
So it really looks like that.
There is a margin. We're assuming there's a margin.
And then here's what happens.
So this matrix is representing the overlap of two stimuli, right,
as zero-one vectors, the inner product,
from the same class and between classes.
So, of course, it's actually almost all looks random
because there's only one direction in which they are separated.
Okay, and that's a random direction in this example, in the experiment.
So if I look at two random vectors and I look at what their overlap is,
yeah, it's very high, okay, in their plus-minus-one vectors.
On the other hand, after you present these five examples
and create this assembly,
and now you ask what's the overlap of the assembly in the brain area,
the firing activity, then it becomes much more focused within versus across.
And so these firing activities are for the assembly that's formed after five examples.
There is some overlap, certainly,
but it's much smaller than in the input neurons.
So this is what I mean by creating an assembly.
So the overlap is preserved, and this is the theorem for this model.
For under this model of the input, presenting these inputs to this model
that you know exactly the entire setup of,
you will create these two assemblies that have very little overlap,
and therefore classification is easy.
If I give you a new example from the positive, this is the assembly that will fire.
Okay, now let's go to sequences.
You're welcome to keep interrupting.
We'll just stop at noon.
Memorizing and learning sequences seems to be very fundamental and natural to brains.
You know, on a computer, what's the big difference?
Sequence, table, whatever.
But here it's, yes.
Actually, we need crucially that the examples come from,
that's the only label information we're getting.
Everything on one side, you get five examples from the positive side,
and then you get five from the negative side.
Yes, yeah, just the label.
The label has to, yeah.
If I do random, then I don't get a chance to form an assembly.
But if I see a few together, that's enough to form an assembly.
And then from then on, you're okay.
Yeah, because otherwise it looks like an unsupervised problem,
but there is actually label information because of the sequence here.
Okay, but this is, I'm not talking about a different kind of sequence.
You know, I mean anybody who's learned to say A, B, C, D,
you learn it to a tune and much easier.
Whatever we do, we seem to have some kind of mnemonic or a story,
a location, a map, whatever.
Now, assembly sequences, sequences of assemblies are observed
in experiments with various mammals.
This experiment from Busaki's lab, they have these animals
running around this structure and not only do,
and they have to do complicated, you know, non-trivial decisions
at various points to get their rewards and avoid the penalty.
So there's a sequence of assemblies that are created.
And very interestingly, after the animal is trained,
before, as it enters the arena, it pre-placed the sequence
before it starts executing the task.
So, yeah, there's also pre-play.
This has been replicated in modified settings.
Okay, so how do I pose this as a problem for the assembly model?
Here's the simplest sequence problem.
I present not one stimulus, but a sequence of similes.
A, B, C, D, E, and maybe a few times.
A, B, C, D, E, A, B, C, D, E.
And this should create assemblies first of all.
1 for A, 1 for B, 1 for C, 1 for D, 1 for E, sure.
But then, what we also like is if I now fire A,
the assembly for A fires, sure, but it also leads to firing B, C, D, E,
even though I didn't present those.
It triggers the rest of the sequence.
And not only A, if you give me something in the middle,
it should trigger the suffix, the rest of it.
Okay, we'll try it soon.
The theorem is, yes, it does.
And you need log-in presentations to guarantee that it does.
Or at least we can prove that with log-in presentations, it will happen without problem.
Yes, that's a good question in general.
If I have two input, let's say sensory stimuli that have large overlap,
then the projected assemblies also have large overlap.
And it applies also to sequences.
Turns out the overlap just depends on the overlap in the sensory areas.
So not where they are in the sequence.
Yeah, this is single sequences so far.
Okay, now why are sequences fundamental?
Why are they interesting?
Well, there's powerful consequences besides the fact that they're a very human or natural thing.
The first thing is that you can then go to finite state automata.
So, I mean, finite state automata are fantastic.
That's an algorithm.
Every algorithm is in finite state automaton plus memory, plus state.
Just telling you, if you're in this state and you're seeing this input,
you should take this action or produce this.
That's an algorithm.
Or any algorithm is that.
And what we can do now is basically take a finite state machine,
memorize each arc as a short sequence.
You've got a whole set of sequences.
And then just run the machine in the brain model.
I'll show you this.
And in fact, once we do this, we will not need any more control commands.
All we'll need is, yeah, you'll see.
Okay, so both finite state machines and Turing machines will operate without control commands.
What we use are what are called long range interneurons.
So previously we had neurons suppressing activity within an area.
Long range interneurons will be used.
So when they are activated, they will suppress an entire area.
But they are set up already, a priori in the hardware.
There are long range interneurons between area A and area C,
between area whatever.
That's your substrate.
So the way we'll simulate a state machine, these are supposed to be states and transitions,
is that there will be areas for symbols, symbols like stimuli,
areas for states, and areas for transitions or arcs.
So if your finite state machine is some finite size, it has five states and ten arcs,
then you still only have three areas, only three brain areas.
There are separate assemblies for each symbol, separate assemblies for each state,
and separate assemblies for each transition, and that's it.
And the transitions will be learned by just presenting the arcs repeatedly.
So this is what we mean by a simulation of a general Turing machine without control commands.
We can also read the input and memorize it, but let's not do that.
That also can be done by simply presenting the input as a sequence multiple enough times.
Okay, let me just do one more slide and we can switch to the...
How do we make two areas fire alternately?
This seems to be an important operation to make two areas fire alternately.
You're not allowed to both fire at the same time.
And so for this, we use inhibition in a somewhat new way in the model.
Neuroscience is not new.
We have populations, so these are two areas, A, B, and I want the effect that A,
is allowed to fire, then B, then A, then B.
These are not assemblies, these are areas.
So some top K in area, A will fire, then top K in area B, then top K in area A.
I just want that alternating effect.
How do I make that happen?
Well, we have inhibitory populations for each of them, fine.
And when those fire, they suppress.
That's the nature of inhibition.
But then we also have disinhibitory neurons which inhibit the inhibitors.
And those are directly fired by the areas.
So when area A fires, it fires these disinheriting neurons for B,
which then makes the inhibitor neurons fire and that suppresses B.
And that's how we get the automation.
And now when B fires, it will suppress A.
So inhibition is enough to do this, and this is the setup.
So the conclusion here is that the realization and execution of finite state machines here
is emergent, provably.
You just need a sequence of inputs, and then all the components of the assembly calculus,
this model NEMO, operate according to prescribed rules.
There's no overall control.
All of the simulation is here.
We'll go to this in a second.
So with this setup, exactly the following behavior comes out.
A fires, then B fires, then A fires, then B fires.
I mean, some subset of K within.
Language is a fantastic domain for such models because it's unique to the brain.
And maybe you've seen this.
This is exactly about the pace at which most of us read.
And maybe you've seen this.
This is the spiking neurons, 50 hertz.
And this experiment was another one that came out around the same time, which was just fantastic.
If you read this aloud,
Fret, ship, hill, give, true, melt, fans, blue, guess, hits, then cats, nonsense.
This is what you observe in terms of the frequency of firing, basically one per word, four hertz.
And then this was the ingenious experiment.
These bad cats eat fish.
New plan gave joy.
Little boy kicks ball.
What do you think is going to happen?
Yeah, they had three different frequencies.
One for the words still, one for phrases, and one for complete little sentences.
So this is suggesting that there are little parse trees being formed on the fly as you're doing this.
And there's all kinds of neuroscience evidence suggesting this.
And indeed, this tree building step, what we've seen here about a dozen time steps are enough to create an assembly
to represent the next level for phrase and so on.
Okay, there are many research directions.
We've discussed several, but let me just go straight to the demo now in the remaining few minutes after I think.
There's a whole topic which I haven't covered, which is going away from GNP to models with more geometry,
random geometric graphs.
And I will not do that right now because I don't want to fly through it.
But there is very interesting behavior there where even without plasticity, you get convergence phenomena.
And there's some things about it are provable.
Thanks to Christos, collaborators and students.
Max and Mirabella are current students.
I should have recommended the summer school to them and to you.
And let me now switch to this demo.
So, here's what's happening here.
Oops, that's already loaded, fine.
From grain import recurrent area, input area is 1000, neurons 1000, cap size 30, that's your K, density is P, plasticity is 0.1.
Just to quickly ground this, I'm presenting a stimulus for 10 rounds.
And you're seeing what is actually activated when I present that.
And you see initially it's something, and they're sorted by activation.
And so it quickly focuses onto the same cap size, what is it, 30 neurons.
Classifying stimuli, you have generated some stimulus.
And this is what the actual stimulus looks like.
Now, the blue stimulus consists mostly of activity over here and some activities spread out everywhere.
Similarly, the green, similarly the orange.
The green was plotted last, that's why you see more of it.
Now, we create these assemblies by presenting them and then visualize them.
And this is what the created assemblies look like.
Just by the presentation, exactly what I said in the talk, you can test their overlap.
And of course, the prediction is almost perfect because the majority are from the same.
This is not a hard machine learning problem.
It's just happening without any rules here.
Now let's go to sequences.
So defining a sequence of inputs of length 25, sequence of 25 things.
And we're presenting the entire sequence 10 times, that's it.
1 through 25, 10 times.
And now we plot the results.
Now what this is showing you is they're 25 as a sequence item.
And if you present only three times, then the recall fraction, the fraction of the correct assemblies,
the recall drops off very fast.
Let me explain to you what this orange line is in a second, but think about the blue line for a second.
After six presentations, the blue line, you get good recall.
And even the last element is recalled well after about eight presentations.
So I did 10, but I could have seen what was the status after three presentations, after six presentations, after 10.
After 10 presentations, this is telling you you're recalling basically the last item perfectly.
And this is sort of the average.
But I just want to tell you what this orange line is.
This is something very human.
You know how when we learn ABC, we learn with a song, with an existing sequence.
So what we did here is we used two brain areas.
One of them already had a sequence.
And one of them is the one that was getting projected.
And we just projected inside, simultaneously let these things fire.
And when you do this, it turns out that three presentations are enough.
So basically the number of presentations you need drops by a factor of two
when you allow yourself a scaffold existing presentation, existing sequence in the brain.
It doesn't matter what it is.
It could have nothing to do sensory-wise, stimulus-wise.
So here we get to the finite state machine.
So this is sort of a classic finite state machine.
Recognize numbers divisible by three.
I give you an input binary number and you want to know,
not binary, decimal number, you want to know is it divisible by three.
This is something we can do in our head.
Just keep the count mod three.
That's a finite state machine.
Now how is this going to learn it?
We're going to learn it by just presenting the transitions one by one.
This has just four states plus the accept state.
That's the network.
We present the transitions 20 times.
And now let's give it a sequence.
Anybody want to give me a sequence of digits?
All right, fine.
Sorry, seven?
Yeah.
Let's just be, you know, there.
Now it's done its computation.
Plotting is the more complicated thing.
How do I tell you what exactly it's done?
We'll see in a second.
Here it is.
So this is a seven, two, nine, seven, two, nine, nine, two, seven end of sequence.
And this is what's getting activated.
It starts here.
This represents mod zero, zero mod three, one mod three, two mod three.
And these are the accept and reject states.
Those are the five states in the network.
When I present seven, that it moved to mod one, right?
It moved to one mod three because seven mod three is one.
And then when you present, what is it, two?
One plus two is three.
It goes back to zero.
Nine stays here.
Seven, so on.
And at the end, it accepted it.
Now, here's interesting.
How about a probabilistic finite automaton?
This is kind of closer to your question.
What if the sequences are not deterministic, right?
For example, here's a very simple probabilistic finite automaton
fragment of English.
There's a subject article, the dog chases a bo.
And then we allow ourselves to go back to the beginning.
And then the boy chases whatever.
But the point is this.
Nobody's telling us anything about this.
We're just learning this by presentation as little sequences.
And, yep, this is the, and then we present it 20 times.
Now, I sample from the model, which means I start at the start state of this PFA and see what happens.
And here is the output.
This is a sample output.
Let's just say print.
The boy throws a stick.
Let's sample again.
Now, you're not training again.
I'm just sampling.
You see?
The same model is now giving me.
The dog chases the ball, then a dog chases the stick, then a dog chases the stick.
You know, it's obviously not thinking, but the point is that the probabilities themselves,
this is what I want to say, this is ongoing work, are generated by assemblies.
How do I make a random choice between two assemblies?
This can be done using just a random initial case subset.
Turns out this probability half plot.
Okay, this is the last thing here.
I'm plotting the activations for what actually, oh, sorry, there's one more thing.
This is just a sequence of activations corresponding to the sentence that was generated.
You know, boy, what was it?
Whatever it was.
Anyway, so that's the probability phenomenon.
Now, the last one, very last one.
Simulating a Turing machine.
You see, a Turing machine is much more powerful than just a finite state machine with no memory.
One example, this is a palindrome.
Now, if I tell you, A, B, B, A, B, B, A, is that the same when I reverse it?
Can't get a finite state machine to do it, but Turing machine, no problem.
You and I, no problem.
Write it down and then start marking off the ends and so on, right?
And this is a finite state machine with the input alongside.
That's just the finite state machine.
And then you need to have the input on which you can move left and right.
This R and L represents whether you're moving left or right on the tape.
Okay?
And now, how is this model learning it?
Well, first, the alphabet right now is just A or B.
And we have an end symbol.
That's why it's plus one.
These are the transitions of the finite state machine.
We create symbol assemblies and state assemblies, present the network.
So, this is, again, just by presenting transitions of the finite state machine.
And now, we ask for a string.
For example, A, B, B, A, B, B, A, B, B, A.
Is that a palindrome?
Yes.
Okay?
And now, we run it.
And let's see.
Yeah, it takes a little bit.
Here we go.
So, what's happening here?
This is just what's on the tape, right?
A, B, B, A, B, B, A, B, B, A.
And that's the end symbol.
And in the first step, it is starting here and writing this down.
And then what it does is just checking if the last was the same as the first.
And if so, it erases it.
That's where you get the smaller tape and smaller tape and smaller tape.
I mean, it's just following the finite state machine correctly.
But for a large number of transitions, till you're down to, you know, just B, B, B, B,
and then just B, and it accepts at the end.
Move to the accept state.
Okay?
Anyway, I'll stop here.
All of this is available on GitHub.
And everything was done here with less than 7,000 neurons.
So.
Thank you.
