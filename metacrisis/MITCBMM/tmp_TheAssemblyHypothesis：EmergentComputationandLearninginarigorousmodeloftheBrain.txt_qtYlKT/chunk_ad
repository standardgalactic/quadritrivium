So if your finite state machine is some finite size, it has five states and ten arcs,
then you still only have three areas, only three brain areas.
There are separate assemblies for each symbol, separate assemblies for each state,
and separate assemblies for each transition, and that's it.
And the transitions will be learned by just presenting the arcs repeatedly.
So this is what we mean by a simulation of a general Turing machine without control commands.
We can also read the input and memorize it, but let's not do that.
That also can be done by simply presenting the input as a sequence multiple enough times.
Okay, let me just do one more slide and we can switch to the...
How do we make two areas fire alternately?
This seems to be an important operation to make two areas fire alternately.
You're not allowed to both fire at the same time.
And so for this, we use inhibition in a somewhat new way in the model.
Neuroscience is not new.
We have populations, so these are two areas, A, B, and I want the effect that A,
is allowed to fire, then B, then A, then B.
These are not assemblies, these are areas.
So some top K in area, A will fire, then top K in area B, then top K in area A.
I just want that alternating effect.
How do I make that happen?
Well, we have inhibitory populations for each of them, fine.
And when those fire, they suppress.
That's the nature of inhibition.
But then we also have disinhibitory neurons which inhibit the inhibitors.
And those are directly fired by the areas.
So when area A fires, it fires these disinheriting neurons for B,
which then makes the inhibitor neurons fire and that suppresses B.
And that's how we get the automation.
And now when B fires, it will suppress A.
So inhibition is enough to do this, and this is the setup.
So the conclusion here is that the realization and execution of finite state machines here
is emergent, provably.
You just need a sequence of inputs, and then all the components of the assembly calculus,
this model NEMO, operate according to prescribed rules.
There's no overall control.
All of the simulation is here.
We'll go to this in a second.
So with this setup, exactly the following behavior comes out.
A fires, then B fires, then A fires, then B fires.
I mean, some subset of K within.
Language is a fantastic domain for such models because it's unique to the brain.
And maybe you've seen this.
This is exactly about the pace at which most of us read.
And maybe you've seen this.
This is the spiking neurons, 50 hertz.
And this experiment was another one that came out around the same time, which was just fantastic.
If you read this aloud,
Fret, ship, hill, give, true, melt, fans, blue, guess, hits, then cats, nonsense.
This is what you observe in terms of the frequency of firing, basically one per word, four hertz.
And then this was the ingenious experiment.
These bad cats eat fish.
New plan gave joy.
Little boy kicks ball.
What do you think is going to happen?
Yeah, they had three different frequencies.
One for the words still, one for phrases, and one for complete little sentences.
So this is suggesting that there are little parse trees being formed on the fly as you're doing this.
And there's all kinds of neuroscience evidence suggesting this.
And indeed, this tree building step, what we've seen here about a dozen time steps are enough to create an assembly
to represent the next level for phrase and so on.
Okay, there are many research directions.
We've discussed several, but let me just go straight to the demo now in the remaining few minutes after I think.
There's a whole topic which I haven't covered, which is going away from GNP to models with more geometry,
random geometric graphs.
And I will not do that right now because I don't want to fly through it.
But there is very interesting behavior there where even without plasticity, you get convergence phenomena.
And there's some things about it are provable.
Thanks to Christos, collaborators and students.
Max and Mirabella are current students.
I should have recommended the summer school to them and to you.
And let me now switch to this demo.
So, here's what's happening here.
Oops, that's already loaded, fine.
From grain import recurrent area, input area is 1000, neurons 1000, cap size 30, that's your K, density is P, plasticity is 0.1.
Just to quickly ground this, I'm presenting a stimulus for 10 rounds.
And you're seeing what is actually activated when I present that.
And you see initially it's something, and they're sorted by activation.
And so it quickly focuses onto the same cap size, what is it, 30 neurons.
Classifying stimuli, you have generated some stimulus.
And this is what the actual stimulus looks like.
Now, the blue stimulus consists mostly of activity over here and some activities spread out everywhere.
Similarly, the green, similarly the orange.
The green was plotted last, that's why you see more of it.
Now, we create these assemblies by presenting them and then visualize them.
And this is what the created assemblies look like.
Just by the presentation, exactly what I said in the talk, you can test their overlap.
And of course, the prediction is almost perfect because the majority are from the same.
This is not a hard machine learning problem.
It's just happening without any rules here.
Now let's go to sequences.
So defining a sequence of inputs of length 25, sequence of 25 things.
And we're presenting the entire sequence 10 times, that's it.
1 through 25, 10 times.
And now we plot the results.
Now what this is showing you is they're 25 as a sequence item.
And if you present only three times, then the recall fraction, the fraction of the correct assemblies,
the recall drops off very fast.
Let me explain to you what this orange line is in a second, but think about the blue line for a second.
After six presentations, the blue line, you get good recall.
And even the last element is recalled well after about eight presentations.
So I did 10, but I could have seen what was the status after three presentations, after six presentations, after 10.
After 10 presentations, this is telling you you're recalling basically the last item perfectly.
And this is sort of the average.
But I just want to tell you what this orange line is.
This is something very human.
You know how when we learn ABC, we learn with a song, with an existing sequence.
So what we did here is we used two brain areas.
One of them already had a sequence.
And one of them is the one that was getting projected.
And we just projected inside, simultaneously let these things fire.
And when you do this, it turns out that three presentations are enough.
So basically the number of presentations you need drops by a factor of two
when you allow yourself a scaffold existing presentation, existing sequence in the brain.
It doesn't matter what it is.
It could have nothing to do sensory-wise, stimulus-wise.
So here we get to the finite state machine.
So this is sort of a classic finite state machine.
Recognize numbers divisible by three.
I give you an input binary number and you want to know,
not binary, decimal number, you want to know is it divisible by three.
This is something we can do in our head.
Just keep the count mod three.
That's a finite state machine.
Now how is this going to learn it?
We're going to learn it by just presenting the transitions one by one.
This has just four states plus the accept state.
That's the network.
We present the transitions 20 times.
And now let's give it a sequence.
Anybody want to give me a sequence of digits?
All right, fine.
Sorry, seven?
Yeah.
Let's just be, you know, there.
Now it's done its computation.
Plotting is the more complicated thing.
How do I tell you what exactly it's done?
We'll see in a second.
Here it is.
So this is a seven, two, nine, seven, two, nine, nine, two, seven end of sequence.
And this is what's getting activated.
It starts here.
This represents mod zero, zero mod three, one mod three, two mod three.
And these are the accept and reject states.
Those are the five states in the network.
When I present seven, that it moved to mod one, right?
It moved to one mod three because seven mod three is one.
And then when you present, what is it, two?
One plus two is three.
It goes back to zero.
Nine stays here.
Seven, so on.
And at the end, it accepted it.
Now, here's interesting.
How about a probabilistic finite automaton?
This is kind of closer to your question.
What if the sequences are not deterministic, right?
For example, here's a very simple probabilistic finite automaton
fragment of English.
There's a subject article, the dog chases a bo.
And then we allow ourselves to go back to the beginning.
And then the boy chases whatever.
But the point is this.
Nobody's telling us anything about this.
We're just learning this by presentation as little sequences.
And, yep, this is the, and then we present it 20 times.
Now, I sample from the model, which means I start at the start state of this PFA and see what happens.
And here is the output.
This is a sample output.
Let's just say print.
The boy throws a stick.
Let's sample again.
Now, you're not training again.
I'm just sampling.
You see?
The same model is now giving me.
The dog chases the ball, then a dog chases the stick, then a dog chases the stick.
You know, it's obviously not thinking, but the point is that the probabilities themselves,
this is what I want to say, this is ongoing work, are generated by assemblies.
How do I make a random choice between two assemblies?
This can be done using just a random initial case subset.
Turns out this probability half plot.
Okay, this is the last thing here.
I'm plotting the activations for what actually, oh, sorry, there's one more thing.
This is just a sequence of activations corresponding to the sentence that was generated.
You know, boy, what was it?
Whatever it was.
Anyway, so that's the probability phenomenon.
Now, the last one, very last one.
Simulating a Turing machine.
You see, a Turing machine is much more powerful than just a finite state machine with no memory.
One example, this is a palindrome.
Now, if I tell you, A, B, B, A, B, B, A, is that the same when I reverse it?
Can't get a finite state machine to do it, but Turing machine, no problem.
You and I, no problem.
Write it down and then start marking off the ends and so on, right?
And this is a finite state machine with the input alongside.
That's just the finite state machine.
And then you need to have the input on which you can move left and right.
This R and L represents whether you're moving left or right on the tape.
