even if the UN Secretary General Antonio Guterres is calling for a new international social contract,
it doesn't seem to be resonating. And if you go back in the historical record and you look at the
debates of the 1950s and the shadow of the bomb and how radical the vision was,
of course, what resulted was a compromise. But nevertheless, there were very serious people
who were thinking hard about global political federation. What's happened? Why is it, you know,
to draw on that famous phrase is easier to imagine the end of the world than the end of capitalism?
Why is it so hard for us to to work through the viable path to to address the challenge
that you've articulated so clearly? First, on the topic of there's nothing new under the sun and
our previous systems of social philosophy and social technology are adequate. I don't think that
anyone believes that who's actually studied exponential tech and X risk meaningfully,
I have not met them. It doesn't seem like a reasonable position to hold those things together.
If you look at just a single category of exponential tech, that idea will change.
I'm sure the listeners have all seen this, but like, when you saw the way that AlphaGo beat
Stockfish at chess, that was so fucking clear that we're dealing with phenomena that are nothing
like any phenomena that the Scottish Enlightenment or the founding fathers or Isaac Newton or Marx
or anyone ever had to think about that that the best chess player in the world, which is the
cutting edge of there's nothing new under the sun, like chess players got better, but like,
people been good at chess for a while, right? It's a slow evolution until AI and Stockfish just
devastated the best chess players in the world. We remember seeing Kasparov get beaten, and then
Stockfish kept getting better and better with the model of AI or programming all the human games
until it was so much better that it stopped even making sense to calibrate it relative to the best
humans. And then a breakthrough in AI says, let's do this differently, right? Let's make a type of
AI based on rival networks, and we won't actually program any human games into it. We'll just let
it play itself a bunch of times and fast forward and see what it learns. And I don't remember
exactly, but AlphaGo by Google, I think it trained itself in three hours, just playing itself with
no human input of information, just the rules of chess. And then it ended up beating Stockfish.
There were a few stales, but it was like 38 to zero in terms of the non stales. And it's like,
oh, wow, that's it. And Stockfish was so far beyond humans, and it's like three hours of training. And
then that same thing could beat us at go and start to beat us at complex strategic video games.
And this is all evolving over the course of almost no period of time, right? This is evolving over
the course of months and single digit years. Nothing new under the sun. Nobody can study
exponential curves and think that. Now, this brings us to the...
It's just such a silly thing to say. When you start looking at scaled species extinction,
when you start looking at the Anthropocene as a real thing, where humans are a bigger force
than all geologic forces combined in defining the surface of the Earth, like, fuck, it's a different
situation than the history of the world was. And like I said, just even starting with the bomb,
the world never didn't have the major empire's war. And World War II was like a second ago
in historical time, right? And the solution to not use that bomb drove all these other issues. So
a lot of our issues are just increases in the severity of the same underlying type of game
theoretic dynamics. And so we can say they are continuous with them in type, but there are places
where a change of magnitude becomes a change in kind, right? Like as soon as the magnitude gets
beyond human information processing capability, it's now a change of kind. As soon as we move
from a war that's winnable to a war that's not winnable, even though they're both the logic of
war, it's a change of magnitude that becomes a change in kind, right? So there's a lot of places
where even the things that are continuous with the past become discontinuous past certain thresholds,
meaning that the same types of solutions, the whole class of solutions doesn't apply anymore.
Now, that doesn't mean that we throw out everything that we've learned. It means that we have to make
sure that we're applying everything that we've learned that is effective, that we aren't making
the mistake of not paying attention to the total amount of human thinking and ingenuity
that's happened so far, and that the new innovation that we do is commensurate with the smart parts
of it. But it happens all the time that we're exploring a search space, and there's a couple
branches, and in the immediate term, this branch has more incentive. And so we explore this branch,
and then we just forget about this one, and we just keep exploring, and then we hit a cul-de-sac
at a certain point. But we have reasons why there's momentum to keep some combination of
sunken cost fallacies with the actual belief that this is the only path, this earlier choice,
and that we wouldn't go all the way back there to the not even knowing the other branches that were
cleaved that we didn't pay attention to, to like perverse institutional incentives of standard
models where it's hard to get a research grant to do anything outside of that thing or to get
your professor who believes in that thing to change their opinion on it or whatever it is.
So there are a bunch of places where we actually have to go back and say, okay,
there was an incentive to make faster and faster smaller and smaller computer chips,
and there was enough money around that that there were whole other directions in computational
substrate that we didn't take that for reasons of manufacturing resilience and a bunch of other
things might actually be meaningful and interesting. This is starting to be a real conversation in
theoretical physics with string theory, and like maybe we actually need to rewind and try
a fundamentally different approach. I think there are places in governance where like we've just
accepted, we've just kind of accepted capitalism is the only, in the West, is the only reasonable
answer combined with some kind of omen-ish government state. And if you think anything else,
you didn't study the history of Mao and Stalin and Paul Potts and whatever, because everything else
ends up becoming that kind of dreadful slaughter. That's kind of the dominant narrative where
it's worse than going against Christianity or it's similar to going against Christianity
in the Dark Ages, right? There's an almost religious tone to it. It's like, well,
we could come up with better shit that isn't any of those things, like
there's nothing new under the sun, blockchain's new, like the ability to have an uncorruptible
ledger where you can have a provenance of data that you can't fuck up. That makes it
where you can have a history that can't be corrupted or changed by the winners afterwards.
That's kind of new. That's a big deal. Makes it to where you can have a system of justice where
you can't actually fuck up the data, right? It means that you can have a system of accounting
where let's say the government spending was on a blockchain that was transparently
oversighted, there wouldn't be missing money anymore. Right now, there's all these places
where the total amount of money going in and the receipts coming out don't add up and there's
missing money. It's like, well, that couldn't be. Does that make something new possible?
Yeah, totally it makes something new possible. You look at the way that AI can make
new sounds. It can do error correction of sound where there is an error or make new sounds or
make new faces by doing an averaged composite of all faces that look similarish. You say, well,
could people express huge numbers of people, express their sentiments about something and
have the AI actually come up with something that is like a weighted average of all of those as a
form of proposition creation and then could we use distributed methods of proposition advancement
that didn't exist when we had to meet in a town hall and ride a horse from that town hall to
the other ones and we haven't innovated the structure of government since we had to ride horses.
Like, why do we think this particular thing is the best thing? Well, because the other things,
the last time we had that conversation seemed dreadful, at least that was the winning narrative.
But totally new things that are not just those previous things are possible.
What I would say is someone should not assume that the moment we say maybe there's a problem
with capitalism that we're instantly going to turn into Stalinism. But to say, let's make sure
we studied that history well enough to know what was wrong with those ideas and we don't do that,
yes. But let's also do the critique of the system and not just end with the critique,
but take it as a design criteria to say what would a better system look like and have we
got all the design criteria? Do we have the critiques of the communist system and the socialist
system and the capitalist system simultaneously? And then can we take all those as design criteria
and work on a fundamentally better design that might not look like any of those isms that utilizes
new technology, which means new possibilities that didn't exist before with new forcing functions
that didn't exist before? I think you're also saying, Daniel, that these kinds of challenges
do actually have comprehensive solutions. And I think there's quite a lot of people who deep down
have very, they doubt that that's actually possible. So whether they should have
haven't even tried hard enough to have that doubt mean anything. It's just an emotional default.
That was the other question you asked is, are we hitting the limits of cognitive complexity?
That is such a shit answer if you haven't actually applied the full limits of human
cognitive complexity and seen that we're failing. So we're not even trying. China's trying and they're
doing fucking amazing. In the US, we have no high-speed trains. None. None. In the time that
they've existed, China's been exporting them all around the fucking world in that same amount of
time. But a system that doesn't have term limits and that doesn't have a two-party system where
we just use all the energy wasted as heat fighting each other and then whatever you do for four years,
the other people undo for four years and nobody invests in anything with longer than four year
timelines because it won't get them reelected, that system is just stupid. That's going to fail to a
system that can do long-term planning. So if we say, okay, let's imagine just hanging out in the 30s
and saying, we got to figure out how to split an atom. No, not just split an atom. We're going to
figure out how to split an atom and deliver that as a warhead on a rocket to some other place with
some decent precision. In fact, we're going to go beyond that. We're going to use uranium to fission
something and split it to then drive nucleons into a fusion. It would be easy to say, well,
there's no fucking way. We don't have the cognitive complexity to be able to split atoms. We don't
even know what an atom is. But the Manhattan Project was a very serious investment in cognitive
complexity and we got everybody there. We got all the best thinkers in the world there. We put the
budget on. Are we doing that? Are we even fucking, where we got von Neumann, we got Turing, we got
Feynman, we got Oppenheimer, we got all those folks in Bletchley Park and in Los Alamos. Where is
the equivalent of that thing outside of very narrow areas of military? Which is why we have a dope
military. We have an awesome military, but that's innovation in military. That's not
innovation in the social technology of governance itself. We actually have to not just innovate
our military, but innovate the social technology of governance for a participatory
governance system. And this is why we come back to the, there's this quote that I always forget,
so a paraphrase of George Washington's that said something to the effect that the number one aim
of the federal government has to be the comprehensive education of every citizen in the science of
government. And science of government was the term of art. And I think it's so profound that he did
not say the number one aim of the federal government is to protect its borders. And he did not say
the name of the federal government is to protect rule of law. Because you can do rule of law
effectively with a police state. And you can protect the boundaries fine with a military
dictatorship. But they won't be democracies. If it's going to be a democracy, then democratically
the people will probably decide to protect their borders and to engage rule of law.
But if the number one goal is anything other than the comprehensive education of all citizens,
and the education was considered both a cognitive education and a moral education,
the way they described it, which is the kind of civic virtues that people are willing to
give something for the larger system that they also receive benefit from. And they're actively
participatively engaged. So that's the thing we need to be innovating in right now, not just
innovating in military while turning it into a some kind of autocratic or kleptocratic system.
But how do we apply the new digital and other exponential technologies to be able to both direct
the exponential technologies well, so that they don't cause existential risk, and in a way that
is aligned with the actual values that we care about as a people. And so then the core question
comes, what is a successful civilization? Well, it's one that doesn't fail, but that's not the
only criteria. It's one that doesn't fail and that maximizes the possible quality of life for
everybody in perpetuity. And then we have to find what is quality of life me, right? So these
there's like core existential questions of what is a meaningful human life to be able to design
a civilization that is optimizing for that, which is culture, right? Which is why we have to have
innovation in culture. Which is why I talk about that there's a cultural renaissance, a cultural
enlightenment that is necessary right now as the basis of the creation of these new institutions that
can solve the extra problems, because our current problem solving mechanisms can't solve them.
Which is why they're not being solved. We have to develop new institutions that are capable of
solving these types of problems, these types of complexity. But if those new institutions are
created by a few people that get it and impose them on force, then it's some kind of autocracy.
So they have to be created by people who want them and are willing to participate with them
and capable of participating. That is the cultural enlightenment that has to be the basis of it,
which and of course, there's a recursive process of some people engaging in that to then build
systems that in turn engage more people in it. So you get a virtuous cycle between cultural evolution
and social evolution, employing physical technologies, binding physical technologies,
and advancing them for the right purposes. It sounds like we really need a new forms of
wisdom education. And obviously I'm glad to say that we've got a Zach Stein coming on the podcast
very soon to discuss that very question. And obviously what you're saying, Daniel,
big implications for how we think about the university in the current situation.
But I'd like to hand over to Sam. I know Sam's got a burning question, so please, Sam.
Yeah, hi, Daniel. Yeah, I've got a couple of burning questions, but I'll go with one to start.
It seems like with the problems we're facing, they often, as we talked about,
they happen at a certain area. So for example, climate change is here already,
and it will exponentially grow out. And that's one of the issues that I think we're kind of
alluding to, that when there's not the immediate threat of World War II, for example, it's quite
hard to galvanize a whole group of people to solve a problem. But do you think that we think about
solutions in the same way to the logic of problems in terms of Silicon Valley out,
or it will happen in this certain area and slowly filter out? There's that quote,
the future is here, it's just not that evenly distributed. And that's quite a worrying logic
if we're thinking about the magnitude of exponential risk. And do you think that we're
then following the logic that we apply for problems that they happen and exponentially grow out?
And is that useful or harmful when we're thinking about solutions that need to
really permeate around the whole globe and not leave anyone behind?
I'm not sure that I understand the question yet. You were using the example of climate change and
saying it's already here, but because it doesn't look like an agent in a way that we
evolve to understand as an immediate threat, we don't respond to it appropriately. But that
it's already here, it's expanding in a way that maybe we don't respond to appropriately. And
you're wondering, is that the case with all of the risks? There's already AI happening that is risky,
and we're just not responding to it appropriately? Or was the question different?
Yeah, sorry, Daniel. The question was slightly different. So that's how we understand
issues like climate change. And we often talk about solutions in a similar way to
that issue of climate change, i.e. there'll be an innovation in a certain part of the world.
So the solution is already here. And then it slowly permeates out. And then eventually everyone
will have it. So you took the example of high speed trains, they are already here. The solution is
already here. It's just not that evenly distributed. And do you think that that follows the logic of
where we think about things like climate change, where it happens in a certain area and slowly
distributes out? Whereas with solutions, they need to be get around everyone very quickly.
And they can't work in that logic of slowly from one center expanding out.
I understand that. I don't think it's fair to say the solutions are already here and
unevenly distributed. It's true for some things. Obviously, we already have a solution to
caloric abundance, but it's not evenly distributed because there's extreme poverty.
That's an example. And that's one we've lived with for a long time. And we can see that it did not
