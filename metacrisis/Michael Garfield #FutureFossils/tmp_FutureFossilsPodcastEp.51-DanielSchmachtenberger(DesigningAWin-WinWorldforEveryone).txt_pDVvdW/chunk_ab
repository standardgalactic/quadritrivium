like you're saying we transit from that finite zero sum type thinking to a game, the goal of which
is to keep the game going for as long as possible. To continuously better the quality of the game.
Yes. And so that rewards all participants. And so I guess the, you know, the question then
is at its base, an economic question, isn't it? And so I think this is a very fun topic.
Like I can tell that I've become a boring grownup because I'm now like fascinated by economics.
And it's a big part of it is watching the conversation in the last few years shift around
cryptocurrencies, shift around universal basic income. A lot of these things that I didn't really
give much, I didn't put much stock in quote unquote, when they first appeared on my radar to
mix metaphors. But the thing that's always stuck with me is something that Doug Rushkoff brought
up in present shock in terms of which, you know, he's the one that introduced me to Finite and
Infinite Games. He said that you look at the really thriving societies and it's the societies for whom
the money lost value if you kept it out of circulation, where the money was pegged to the
decay of some real world resource, because we don't really, you know, news doesn't gain value
when it's a day old, bread doesn't gain value when it's a day old. And so he was, he was suggesting
that some sort of negative interest could be applied to things so that it propels everyone.
It encourages and incentivizes everyone to keep their money in circulation. It encourages a culture
of micro philanthropy. I mean, that to me seems like the real, the real thing. Or like, is that,
is that something that you're thinking about talking about in all of this? Or like, where,
what do you see as the potential strategies for this whole thing to flip overnight?
So when you say that you're a boring adult because you're getting interested in economics,
if you're getting interested in economics as a philosopher, it actually means that you're just
gaining deeper insight into how structural incentive and structural value systems and
disposition work, which means you are not being a good philosopher if you aren't thinking about
those things. And so if we say, all right, how much is air worth economically? Right now it's
worth nothing at all. And because it's worth nothing at all, and we make decisions based on
financial statements, right? Then we will pollute the air and burn it up. Now how much is
gold worth? It's worth a lot, a certain amount per ounce. So we will destroy an ecosystem if we need
to to get the gold out from underneath it to put it in bars in a safe that is not doing anything
for anyone other than affecting a financial accounting sheet. And I want the gold because
since we see it as relatively scarce in a win-lose game structure, if there are scarce things that
were that not everyone can have it abundantly, then I get differential advantage by having it.
If something is abundant like air where everybody can have it, then it gives me no competitive
advantage over anyone else so fuck it, we don't value it. Now if we were valuing what's really
relevant to life support or to quality of life or to systemic advantage, of course that would be
flipped. But in a fundamentally win-lose game theoretic structure, we value things that are
relative to our capacity to win at that game. And that is pretty much exclusively pathological.
It doesn't mean that it didn't serve some relevant evolutionary value, but that value is at the end
of its life cycle. And so let's talk about what the future of macroeconomics has to be and then
so the concept of demarrage, negative interest. Obviously when we have interest, we have an
incentive to keep money out of supply, keep resources out of supply, which means that it's
going to maximize extraction and have and maximize hoarding, right? And you're also going to get
increasing wealth inequality because those who have more will get more compounding interest on
that and etc. So demarrage is an interesting thing to address some of that nowhere near
adequate and it creates some of its own problems, which is now you have an artificial incentive to
make transactions occur and transactions movement in the system actually is entropy if the
action is not actually relevant, right? So system wide incentives of almost any kind are
usually problematic, right? So we need to actually go to a much deeper level of insight here. So we
say what is it that we're really trying to do? One way of defining it in terms of macroeconomy
would be we need to rigorously align the incentive of every agent and whether the
agent's an individual or a group of people, a government, a corporation, whatever,
we need to rigorously align the incentive of every agent with the well-being of every other
agent and of the commons with no gap. And to the degree that there's gap, meaning that
two agents have misaligned interest and you have direct harm, right? Direct competition,
which can express itself militarily or through corporate competition that can involve disinformation
or whatever, right? Or to the degree that someone's interest and the well-being of the commons are
misaligned, then you get externality. As you run exponentially more energy through a incentive system
that externalizes harm or that causes direct harm, exponential externality is existential.
That's a lot of x's, right? But is existential? So we will be is the point. Yeah, if that goes down.
And so in the face, you can think of exponential tech and specifically distributed exponential tech
as increasing our agency vector, our capacity to affect the world. But if we are not increasing
the goodness of our decision making, right, then if you have more ability to affect the world in
the way we've always affected the world, we've always murdered, we've always caused war, we've
always, we've always used war as a reasonable way to solve problems. And in fact, it's really
important to get that in every society that had private ownership, you had increasing wealth
inequality until the wealth inequality was so much that war was the only mechanism to stabilize it,
to address it. In every democratic society, you have increasing polarization and radicalization
of political views until war is the only way of stabilizing it. And yet, when you have exponential
tech, you don't get to have war anymore and make it through. And so we don't know how to do
civilization without war, right? We don't know how to do it where we actually have reasonable
ways of resolving differences and etc. So it's a fundamental shift. So if you say we're really
talking about getting off win-lose game theory completely, that's the phase that we're at.
That's of course unprecedented, right? It's unprecedented in the whole story. But as you said,
unprecedented shit actually is the precedent of universe when you take a very long view,
which is, you know, let's say that life originated on this earth and the model of
cosmogenesis that we use commonly is true. Neither of those are true, but let's just
take them as true for a minute, because it's a good analogy. Then you got 12 billion years of
universe existing and doing all kinds of shit and making galaxies and stars and supernovas and all
this stuff and it never made life. And you would think if after 12 billion years of interactions,
it would have kind of exhausted its search base. So life is not possible and then fucking life happens
and it's totally unprecedented based on the things previous, right? And then you go on and on all of
human history 250,000 years right before the plow and there's we didn't actually have economics.
We only had ecology. We were part of an ecology, but we didn't have surplus. And without surplus,
you don't have private ownership, you don't have economics, right? So then economics emerged
unprecedented and on and on, right? So it is actually the precedent of universe in the long
view to do unprecedented shit. And so the the shift for us right now is if we look at Kurzweil's
curves of everything getting exponentially better in terms of compute power and the corresponding
power to make better biotech and address health issues and it can solve problems. It's true.
All of that's true. All those exponential positive curves are true. If you look at all of the decrease
in biodiversity and increase in pollution and damage of the biosphere and increase of existential
risk probability, all the negative exponential curves, they're all true too. And people cherry
pick the ones that they like from the data set to cause a confirmation bias. But when shit is
getting exponentially better and exponentially worse at the same time, neither of those are
actually happening. It's just that the phase is destabilizing, right? The movements away from
the homeodynamic centerline is getting further and further. That means you're moving from one
type of organizational complex system into a chaotic phase, which will be a liminal phase.
And then you'll get the emergence up into a higher degree of order or an entropic
drop down to a lower degree of order. That's the precipice we're on the verge of and it is not
economic. It's economics, governance, infrastructure, culture together, right? And we can say,
but economics is a good way of thinking of the heart of it because economics can be seen as the
interface layer between our values and the way we build the world. The value equation, right,
which is that the air is worth nothing because we're only valuing things to give us competitive
advantage. Our value systems get codified into an economic value equation of how much
actual value that confers real power we give different things, right? A whale swimming in the
ocean has no economic value to anyone currently. A dead whale on a fishing boat might be a million
dollars in whale meat. So this is why we are fishing out the oceans and causing species extinction
of mass, right? But that's a value system, right? That is a memetic, spiritual, existential,
ethical value system codified as a value equation that determines what we confer power and advantage
to, what ends up winning and how we actually build infrastructure in the physical world.
So changing that is actually at the heart of and requires change in infrastructure,
change in the corresponding social structures, governance, law, etc., and in the worldview.
So clearly this is not the kind of thing that's going to happen in a linear way because there's
an obvious chicken and egg with you've got to get people to shift, you've got to get a critical
mass of people up to a developmental level where they are able to think in the terms of the complex
systems in order so that they can start assigning an economic value or at least urging an economic
value for things like that living whale. Or as Christian Schvagerl talks about in the Anthropocene,
he talks about his discomfort with the notion but his sort of settling on it as a best strategy
that the World Economic Forum was examining how we could assign a value to the atmosphere
processing that a given acreage of rainforest in the Congo would provide so that those people are
able to participate in an economy without having to succumb to predatory international
development strategies. So that all makes sense, but again, you can't put a number on what you
don't see, right? And we know through work as developmental psychologists like Robert Keegan
at Harvard that it takes years for people to learn to think at a new structure of consciousness,
like a new layer of awareness to move out of, you know, self-interest and into a wider sense of
participation and therefore ethical responsibility, you know? So I don't know where you,
you seemed like you were shaking your head about the ecosystem services too, so I'd love to
break off wherever you want to break off into that. Ecosystem services sounds like a nice idea.
Like lots of kind of new economics ideas sound like a nice idea, and it's not that they have no
possible transitional relevance, but in the face of distributed exponential tech, it doesn't work
at all. And so, you know, we can just think about that as a defining criteria for a moment and say,
okay, with the distribution of tech, right? The first existential tech the world had was nuclear
bomb. We had two superpowers that had it because it was super hard to make, super expensive, and so
you could have those two spy on each other and have a mutually assured destruction system so
nobody could use it. And we could live in the relatively dreadful piece of the Cold War, right?
Now, of course, we have a lot more than two countries that have nuclear weapons and we have
nuclear weapons in places that we don't even know, but with exponential biotech and nanotech and AI
tech, etc., you have stronger than nuclear level capacity by non-state actors, radically distributed.
How do we make it through that is a very deep question, right? And it really is if you want to
think of it in a mythopoetic sense, which I think you actually have to. If we are gaining the power
of gods, then without the love and wisdom of gods, we self-destruct. And that we are gaining the power
of gods is now both a given and unavoidable. So what is the gain the love and wisdom of gods curve
that gets there in time? And so that that power is actually used for the good of the whole, because
if it's used against, which elicits counter against, which have that much power everywhere, then you
get self-destruct. And so it's important to really get that as a framing. So we don't think that the
things that have worked before, some modified version of them will work now, because it's a
radically different context. Okay, so environmental services. So if I have a rainforest or a marshland
or whatever, it's not worth anything right now. So if it gets cut down and turned into grazing land
for cattle or whatever, it's worth something. So how do I how do I keep it from getting cut
down? Well, can I make it worth more alive? Well, I can say it's sequestering so much CO2 and
stabilizing so much, you know, soil runoff when the rains come. And then if I can get a value on
that up to CO2 can be in terms of carbon credits and the, you know, whatever, right? Then maybe we
can keep it alive to the degree that that helps any ecosystem right now. I'm delighted as a long
term strategy, if what I'm in, several things wrong with it, if I'm incenting, if the way that
it's making money is sequestering CO2, then I want to start optimizing it for heavy sequestering CO2
plants, right? Or then it makes more money as opposed to the whole rich biodiversity that makes a
healthy forest or ecosystem, many of which are going to be less optimized for CO2 sequestration,
then, you know, another way of doing it, right? And so, but then I say, well, what are the other
things that it's doing that I don't know how to monetize yet? There are other plants that are
pulling out different kinds of particulate pollution and also what is the effect to the
pollinators and to the birds and to that, right? And when I really understand what a tree is doing,
it's doing an indefinite, like an undefinable number of positive things to an undefinable
number of beneficiaries. And so, when I take some small matrix-based subset, then I will start to
optimize for that small matrix-based subset, which is exactly the fucking wrong problem that we do
everywhere, which is we take a complex system, reduce it to a complicated system, optimize for
the complicated system, kill the complex system in the process. And so, if I take an n-dimensional
set of self-organizing complex metrics and I reduce them to 10 metrics and I run optimization
functions across those, it'll be the wrong thing every time. So, matrix-based optimization, right?
I can look at somebody's low-density lipoprotein, give them a statin drug and their low-density
lipoprotein comes down. Awesome. Except that the toxicity to the liver and the toxicity to the brain
that come with that statin and et cetera, and even why the cholesterol was high that might have been
the body protecting its vascular system from certain kinds of damage, I'm not measuring those
things, right? And when if I try to measure all of them, now I get so many metrics and trying to
optimization across them that it's actually uncomputable. It becomes an NP-complete issue and
you can't actually find optimization points. And matrix-based optimization has us always think
in terms of theory of trade-offs. So, okay, how much liver damage is worth? How much brain damage
is worth? How much risk of heart disease so that I optimize between those? But they're uncomment
rateable, right? How many dead rhinos is worth? How many parts per million of mercury in the water
is worth? How many tons of CO2 to know how to allocate my resources? I don't know, but I'm going
to find out because I am... That's not... No, but I mean, you're right, though. It's like...
You have to actually stop that for Uplift Connect. There's actually a socially toxic
dimension to life optimization for this very reason. It's like the sinister side of this
trend to reduce the human being to information. It's part of that, that you say, oh, in order to
adapt to my post-industrial work existence, then I have to optimize my life so that I can
get as close as possible to an imaginary Paleolithic human being or whatever it is.
It's bonkers. So, anyway, sorry, go on. There's a couple different issues there. One is,
are the things that we think we are optimizing for the right things at all, right? If we have the
ability to genetically engineer the human is our goal to make us all the best hyper-competers
possible, right? Like, as we get exponential tech and we can actually change what we've thought of
as nature itself, we get very deep ethical and existential questions at the core of it.
But what I'm actually bringing up here is that's a core issue. I'm bringing up a separate issue,
which is actually just a result of metrics. And there is actually a way around it, which is
from a information theory point of view, I want to know what is everything that matters
in a particular situation where we're going to try to do development or, you know, enhance
something. But I want to know what is everything that matters? What is it that matters about it?
How is it interconnected? Now, if I use metrics, and I've got this thing is on a
one to a thousand scale, and this is on a negative a thousand to a thousand scale, and this is this
kind of unit and this kind of unit, how do I co-mentorate them when they're actually non-fungible
types of value, right? This species goes extinct, but this other one, like, it gets a little bit
better there. Well, did they have a say in it? Are they considered beneficiaries of the system?
And so if instead, if I use a if I use a Boolean system, where it is a zero and one based
system, and really what I'm looking for is does this thing matter? Does this particular value
