shareholder return on profit by the directors of the organization. So the idea that you cannot
possibly anticipate externalities is not true, but it's a useful idea of those who are going to
benefit by causing externalities. And the people who benefit by causing externalities because
they're privatizing all of the gains get to write the narrative in the same way the history was
written by the winners of war previously. It costs a lot of money to affect the narrative of the
world. You see this in political campaigns. You see it in marketing campaigns. But if an idea is
spreading, who is writing all that stuff and who's up regulating it and who's paying for the
commercials and who's getting the data to do the personalized microtargeted ads and who's...
So the ideas that spread are not just spreading through a kind of natural selection of the goodness
of the idea. They are getting oftentimes amplified by interests that want those ideas to spread.
Duh, right? We know this. This is how political campaigns work. This is how advertising works.
This is how propaganda works. This is how, you know, on and on. This is how religion works,
right? There's a lot of money that goes into proselytizing and getting the ideas to spread.
So there are a lot of ideas that are marketing and or apologism for the dominant class in terms of
power. So the dominant narrative is usually apologism for the dominant power system.
And that's why when we were in the age of colonialism, the narrative was that it was
generous of us to bring this colonialism to civilize the savages. It's why we were in the
age of spreading Christendom through all of the various processes that bringing Christendom to
all of the non-Christians was the most charitable good act we could do in the same way that,
like when we talk about externalities, when Facebook was in its early phases,
there were people like, you know, Jared Laney famously was saying this very publicly, but there
were people coming from the McLuhan School and, you know, coming from various, coming from the
Mumford School who were saying, hey, look, this technology where you're monetizing people's
attention and you're going to race to effectively monetize their attention. And unlike where TV
commercials did that, but it was the same for everybody, you now get to have them interact
with it, to gain personalized info, to split test how sticky you monetize their attention,
a lot of the things that are going to engage their attention more are going to be limbic hijacks.
There are going to be things that make the person scared or horny or distracted or
in-group, out-group identity or whatever it is. And this is going to be really bad for society.
It's not that no one was saying that then. It was being said, it was being ignored, it was not
being studied and researched and pursued because, and then later they get to say, we couldn't have
possibly known it was going to polarize society and break democracy and decrease everyone's
attention spans. So we couldn't have possibly known as a bullshit story. If you think about,
well, am I saying that you can't anticipate everything? Of course not, but am I saying you
can do a million times better than we even attempt to do now? Yes, of course. So in the process of
developing a new technology, say, could we proceduralize thinking through the total set of
effects, not just the intended set of effects in the market benefits of those, but thinking through
if this technology really takes off and goes to its full scale? What is the pressure of that on
all the supply chains to make it? And what is the environmental effects of that? What are the
geopolitical and et cetera effects? If people use it, is it conferring some power? If so,
what other thing is it obsolete? How will that change power dynamics, et cetera? You just kind
of think it through. Now, we have a process that we've developed called yellow teaming. Red teaming
has now become pretty famous, which is you're wanting to do something, you want it to succeed.
Red teaming is a process you can do to see how it might fail, how somebody could beat it or how
it might fail. Yellow teaming is if it succeeds, where might it mess other things up? So could we
yellow team? Well, yes, you're not going to predict everything, but you predict a lot of the things,
and then you keep watching. And when you notice other things, you procedurally internalize them.
Now, in the same way, all this was the buildup to the billionaire question,
the long buildup. The people in a multipolar trap
who are at the front of the race could stop the multipolar trap,
but they don't want to because they believe they can win. So they use the multipolar trap
as a story of their own lack of power to do anything else as plausible deniability.
The littler guy cannot necessarily stop the multipolar trap,
but someone who's at the leading edge of an arms race, if they wanted to apply the same energy and
same sophistication to agreements, to pursue, because of course, there's a situation that if
my country becomes more secure relative to other countries, which means develops better weapons,
it automatically makes everyone else less secure. So now they have to do the same thing. And
it just means that you have an arms race of increasing weapons forever and increasing
budgets going to it forever, which is great for defense contractors. It's actually great
for GDP and it's bad for everything else. Now, if we could just say, let's make an agreement to
all spend less on weapons, we can be proportionally less, proportionally less such that relative
security changes. Well, of course, people say, well, we can't possibly do that. We couldn't get
China. We couldn't get Russia. There's no way to enforce it, et cetera. And we're saying that right
now with regard to AI. Well, even if we wanted to stop this thing from advancing that has,
that accelerates every global catastrophic risk, we couldn't possibly, because there's so many
places racing and we couldn't stop China and blah, blah, blah. Therefore, the only possible
answer is to win the multipolar trap because losing at it is too bad. If you look at the
resources we invest in figuring out AI, if we invested those same resources in actually creating
healthy diplomacy where we were not assholes to our international neighbors and really tried to
create global coordination to bind the multipolar trap, we could. And so the billionaire that says,
well, I don't know how to solve the problems of the world. I'm too small, right? It's a
hundred trillion dollar economy. I'm almost nothing. I'm this little guy. I couldn't solve the
problems of the world. Therefore, me continuing to just pick up all the game theory tokens I can
under a world of increasing uncertainty, the optionality of that is what's best for me and my
family. Look at how much time you spent figuring out how to gather all those optionality tokens,
understanding markets, understanding how to do lobbying, understanding your industry,
understanding financial markets, et cetera. And look at how much time you spent trying to say,
if I applied all of that same energy, time, thoughtfulness, resource to solving some of the
great problems, could I? You have not put enough time to say that you couldn't possibly. It's just
not in your interest. This is what Jeremy Grantham's big point at the end of our podcast was, and I've
talked to him about it subsequently. He wants the elites, the rich people to divert their attention
to the ecological crisis. Not only climate, but plastics and oceans and many other things,
because they haven't. They're unaware it's just out there, and it's a big drive of his and some
others, and fingers crossed because that's what we need. And yet, I've encountered so many more
people who are conscious of what's happening to the environment. But like you just said,
they plan to give away some of their money in the future, but for now, they're focused on their
investments, and they want to have a little bit more so that they can give more to good causes
in the future, which I just scratched my head when I hear that. But you could understand
someone listening to this past hour. You've just articulated that winners write the history books,
and the situation is so bizarre and scary and complex right now that if I don't have an answer,
I might as well just try to be a winner, which means more optionality tokens.
Yes. And I think there is a, I mean, that is definitely the state of AI in development,
very centrally. It's definitely the state of like the posturing in geopolitics.
And just financial markets, it's the way of that too.
Yeah. Now, if you look at that path of winning and realize it's actually self-terminating,
and it's not even a win for you, that could be the beginning of a pause.
And then if you look at that, okay, so, you know, people talk about like what the
biggest Nobel Prize you could want to win, the biggest problem to solve for humanity
oftentimes is like cure for cancer. If you look at the graphs of incidents of different types
of cancers from 1950 till now, you see a rise in heaps of cancers, right? Lots of rise in endocrine
and reproductive cancers, in different kinds of childhood cancers, in very aggressive turbo
cancers. And that rise in those cancers in terms of year-over-year rise maps very closely to the
rise of carcinogenic chemicals put into the atmosphere. And so when you look at the 230
million chemicals in the database of the American Chemical Society, and you look at the huge number
of them that are put into the atmosphere and environment through the VOCs and the paint and
the walls and then the carpet to the industrial chemicals to the agricultural chemicals,
the percentage of them that are carcinogens is very high. The other ones are neurotoxins
or endocrine disruptors or whatever. So correspondingly, the increased rate of autoimmunity
of autism and Alzheimer's and neurodegenerative disease of infertility and reproductive failure
follow those curves quite closely. Is that the only factor? No. Is that a major factor? Yes.
The other factors are also the result of the progress of this society in other ways,
like the processed foods that follow similar curves that mess up the microbiome and micronutrient
profiles of the body and stuff like that. In your paper, you make a point of highlighting
the Haber-Bosch process and crop yields and food, and that's been viewed as progress,
but there's a whole lot of stuff it doesn't include.
Yeah, so I want to stay with the cancer case for a minute.
If you look at all of the scientific advancements trying to cure cancer,
why don't they start with how do we stop putting carcinogens in the atmosphere?
And in the food supply, where there are continuous studies by groups like the Institute for
Environmental Medicine that show there's roughly 300 carcinogens, the industrial toxins that are
carcinogenic in the breast milk of nursing mothers in the United States and whatever.
Why don't we work on getting those out? Because the vast majority of cancers we have are
anthropogenic. They're caused by human activity. We don't need to try to solve cancer in abstract,
like let's start with that, and oftentimes the right problem solving is reversing something
that was already the wrong path, but I can't patent that. I can't market that very well.
The new pharmaceutical solution that I can patent that doesn't address the upstream causes at all
might have enough ROI to pay for the research that I need to do.
But if there's a thousand of these entities out there and 990 of them
take the higher road, and why are we putting these chemicals in there? Let's do it a different way.
It's those other 10 that decide to do it in our current institutional corporation structure,
and it's just like Genghis Khan.
Okay, so this is important.
If we build new types of energy, but we don't create both cultural values and particularly
law that binds the use of the other energy, we'll just use all the energy because more energy is
awesome. More energy means more GDP means more, you know, all the things associated with what
interests of power. So creating a new thing that is supposedly more positive in a way is
not actually solving the problem if you're not binding the harmful thing because you will just
get both, and it'll end up being market diversification and more total market size. Similarly,
like the organic food movement has been awesome in a lot of ways. So there's a lot of people who
shop at the Whole Foods and buy organics and want there to be less pesticides and whatever,
and yet more pesticides have been used every single year during that entire time. So clearly,
it's not actually decreasing total pesticide use because we're bringing increasing amounts of
pesticides to conventional agriculture, to the developing world and the populations, whatever.
So it just, okay, it's like, great, we will add that organic niche market on top of the
existing market and get more total market share. There has to be more of a focus of actually binding
the things that have to stop. And so when you're mentioning if somebody does that thing,
the interests of everyone else who want to, again, if there were nine tribes that were
really wanted peace and wanted to be in peaceful arrangement and trade and whatever with each
other, and they saw an early phase of another tribe moving in another direction and they did
interfere and not let it continue in that way, they could possibly have peace for everybody.
And that actually becomes the obligation. So like I said, we have a, the system that emerged in
the context of power competition selects for people who are oriented to power competitions
and other people who are complicit with it. Those are the two things it selects for.
And is that dynamic that you just described, what underpins Malak, Watiko, Kojana,
Scottie, the super organism?
Yes. The generative dynamic can be described a couple ways. One is
humans get conditioned by their environment, right? I grow up in a tribal culture. I grow up
in a modern technological culture. I grow up in a city in the dark ages, whatever. I'm going to be,
I'm being conditioned by the environment I'm in, in terms of my language, my worldview,
my technological capabilities, desires, identities, all those things. So the civilization is conditioning
the minds, which means patterns of perception, identity, value, and behavior of the people.
And those minds are in turn creating more of the types of things that they were conditioned to.
And so there is a feedback loop between the individuals influencing the whole,
the whole influencing the individuals, and there's a particular thing that is getting
upregulated. So let's talk about theory of hyperagents for a minute.
I think we mentioned this somewhere before, but we'll do it again quickly.
The Genghis Khan's and the Alexander's and the Caesar's were obviously different types of people
than most of the people that either worked for them or they killed, right? And the kings following
that and the robber barons and industrialists and whatever, they all have that in common,
up to today of that how much power they have consolidated relative to most everyone else
is extreme. And they're, and it's not arbitrary. There were things about their psychology that
had them pursue that and be good at it. So agency, our own ability to achieve our goals and do the
stuff we want to do, you could say that a hyperagent is someone whose focus is maximizing returns on
agency. They want to do the things that make them capable of more doing, right? They want to,
and they want to use their- Towards what end? Winning?
This is a great question, right? Towards what end is, you were just mentioning it with your
billionaire friends who were like, well, the future has a lot of uncertainty. The money gives
me the ability to live in one place or another place to build this kind of tech or that kind of
tech to employ tech or people or land. It gives me the ability to influence minds through media.
It's a generalized optionality token. So I'm just going to keep working to grow my total
optionality tokens so that I always have the maximum freedom and the ability to do what I want
to do every scenario. Now, one of the reasons is that, one of the reasons is winning, one of the
reasons is their own fear of death and wanting to be remembered in the history books forever,
whatever, right? Some set of reasons that are associated with
personal expansion of power. So a hyper agent maximizes kind of return on agency. It's important
to state that return on capital can be return on agency, but it's not necessarily. There are
hyper investors who are really good at applying capital to create more capital, but are not using
that to influence the world to a vision of the world they want beyond that heavily, right?
Like Warren Buffett was not trying to change laws and politics and culture all around the
world. He was a hyper investor, not really a hyper agent, whereas Kissinger had much less
money personally than Buffett ever did, but had radically more influence on world systems
through the influence of a lot more total money of state funds and whatever.
So hyper agency is the more fundamental concept, which is kind of the return on agency. There are
people who are also motivated to do that. They're just not good at it, and maybe they become like a
very controlling middle manager or a gangster or something, but the people who are good at it are
good at scaling. And that's through both the influence of a lot of people and the employment
of technology. And that can be social technology, like writing laws and influencing capital,
which are social technologies, creating narratives like the rise of Nazism or Mao or whatever,
right? So like, but they're good at being able to scale. So the kind of tier one hyper agents,
the ones that have the most influence in the world, are not just oriented to maximize returns on
