certain affordances ai is unique computation has been unique and then ai even within the
space of computation and it is more combinatorial with every other type of tech than any other tech
is and it has more total affordances than any other tech has what's an affordance again uh it
things that it makes possible okay um so for instance if i have a nuclear weapon it makes
certain things possible that are not possible without it it does not intrinsically make me
better at bioengineering it does not intrinsically make me make better drones computation can be
applied to make better drones better bioengineering better nuclear weapons better propaganda better
marketing better financial systems better better chips to make better ai systems so the ai because
all the other tools are made by the kind of human intelligence that makes tools and ai is that kind
of human intelligence externalized as a tool itself it has a capacity to be omnimodal right not dual
use omnie use more than anything else is and omnicombinatorial i can the risk of ai with synthetic
bio so the dual use thing here is i'd say oh i can use ai and kind of protein folding stuff to be
able to solve cancers but once i develop it for that purpose i can also make better bio weapons
with it and now i've made that ability cheap and easy um or i can use it on a chemical database to
do drug discovery but i can also make chemical weapons i can use it to optimize supply chains but
i can also use it to do optimized terrorism on breaking supply chains and so the ai has the capacity
to increase every agents capacity to do every other motive in any context with any other combinatoric
technology in a way that nothing else has and that's really important in understanding because
we'll develop it for a particular positive purpose for a particular set of agents but in
doing so lower the barrier of entry of all other agents for all other purposes being able to use
that capacity so from my biophysical frame which is is not where you're coming from but i'll just
interject this i think a lot of people who don't fully understand ai look at the invisible but
real biophysical body that ai has which requires energy i read in the last few days that ai requires
a lot of water for cooling and things like that but the larger environmental impact of resources
co2 planetary boundaries is not what's powering the ai what's needed to power the ai it's the
resulting acceleration of all the other consumption and things that were already going on that are
just being exponentially increased by the ai yes so let's look at ai driving jevons paradoxes in
your world so let's say there's a bunch of areas where the mining is not quite profitable right that
the um cost of extraction i can't on the current market sell the thing for enough money so we don't
mind that area now the ai figures out efficiency such that a whole bunch of areas that weren't
quite profitable are now profitable awesome for the economy new economic growth for everybody
and also more rapid growth of the superorganism and more environmental externalities for everyone
under the incurrent current motivational landscape and incentive landscape and down to both the market
incentives and even the legal landscape because the market incentives won't create a real
liability deterrent because of liability limiting things and they actually require you to do profit
maximization and things like that right so how is there not a massive discussion in the climate
space about ai right now as a as a generator to bring us to 500 ppm in a decade i don't know maybe
this conversation will help with that um i think you're very worried about that you're worried about
the planetary boundaries impact of of ai either you have the market still running things and ai
working within the market in which case yes the market incentives have cost externality as
fundamental to them if you had to pay the real cost of things the market would collapse as we
understand it and so the ai causing more efficiencies will drive jevons paradoxes because
you will continue to seek returns and you will drive planetary boundaries faster the ai person
might argue no this is not true because the ai allows us a completely new economic system whereby
we can actually track all those externalities and do very complex optimization that means the
market doesn't run the world anymore that means a centralized ai system runs the world um that is
its own dystopia because either you have separate competing groups that don't have totalizing power
in which case they're caught in multipolar traps or you have a single group that can bind all of
those but now there is a central point of failure capture and corruption and no checks and balances
on the power this way we talk about one bad future being catastrophes the other being dystopias because
to control for all the catastrophes orance towards very centralized power which gives dystopias so
what we need is a third attractor that is neither of those which means it is neither a central ai
coordination system nor is it ai in service of markets which means it is some new thing that
like markets as a collective intelligence system are also a collective intelligence system but
it doesn't have the perversion and externalities of markets so a wiser collective intelligence
system wiser than democracy or capitalism that yes it will employ ai and computational
capabilities to mediate just in the same way that democracy required employing the printing press
and so new physical technologies and enable and just like computation has enabled all of
modern banking new technologies make new social systems possible but the goal here is not an ai
overlord that runs everything it is how do computation and intelligence capacities make
new collective intelligence possible such that you can have the global coordination
to prevent global catastrophic failures but where that system has checks and balances in it so you
don't have centralized power coordination and corruption failures so it seems to me that
that if we weren't in this metacrisis and wily coyote moment without with all the leverage in
the system and the geopolitical fragility and everything that we could have time to have some
wisdom embedded in in our in our information systems and our governance but we're at this
late stage where the snowball is getting bigger and bigger and how do we inject wisdom into this
situation with with all these things going in the opposite direction obviously you must have
some idea or a plan or you wouldn't have dropped all this really scary stuff on me and and the
listeners or or are you just calling this out as a risk that we need to have massive urgent
discussions on i think there are people who are ai experts which i'm not i have a focus on
how to think about the relationships between all of the kind of ideas of progress and risks that
gives an insight into ai but when you look at whether it's um whether it's henton or russell
or any of the famous kind of ai pioneers and the risks that they have called forth or the call to
pause large language model deployment or the more the most serious case like elias or yudkowsky's
conversations if people haven't seen the first podcast he did that kind of started this um space
taking these things more seriously recently on bankless it was so fascinating to watch because
you could tell the podcasters didn't really know what he was going to say and thought um he was
going to talk about ai and crypto and um and he came on and uh you know he he founded machine
intelligence research institute and spend his life focused on the topic of the risks of artificial
general intelligence that he thinks we are both very close to but not close to aligning and um
to understand that which we haven't discussed yet because we keep not quite getting to the whole
construction you have narrow intelligence which humans can use for fucked up purposes and markets
can use for fucked up purposes but then a general intelligence becomes kind of its own thing it can
it can figure it becomes its own agent rather than a tool of us as the agents can make its own
goals and can increase its speed of learning meaning and competition relative to us so much
faster than us that if its goals don't align with ours we're fucked
so using the chess example okay i was just going to say using the chess example of the
three hours and trillions of iterations the ai could use itself it could train itself without
humans giving it the orders yes but across broader things so it could come to a new domain
where it wasn't already trained what to do and figure out what are my goals in the space and
or for whatever my big picture goal is what would the goals on the path the instrumental
goals be regarding this whole space and so kind of in the way that you say no matter what your
goal is it's probably going to require energy so get more energy ends up becoming a goal in and
of itself because it increases optionality to fulfill other goals get more capital becomes a
goal in and of itself it's called an instrumental goal the ai can start figuring out well i can
gain all the optionality in this space by such and such i'll mine all the things i'll etc etc
because i'll be able to use them for goals in some future time that i have so if we talk about a
general autonomous intelligence of that kind that could beat us at any games can redefine
anything as a game that can win it's very easy for us to realize it would be really bad to make
that thing before being sure that its goals don't mess up everything for us because our goals
really sucked for all the species we extinct in our goals really suck for all the animals
and factory farms and for all the um you know cultures that are destroyed so if there's something
that is as much smarter than us at goal achieving than us and similarly narrow goal achieving
then we might be like animals on the factory farms or extinct animals very soon
or the cultures with wisdom that were outcompeted
so it's very so leisor came on this podcast having you know uh kind of world leading expertise
in this topic and just spoke honestly to these podcasters who they did a good job of staying
with him but we're totally not prepared and he's like yeah we're on our way to this thing and we're
all gonna die and then they asked him so uh what do we do about it he's like i don't know i've been
working on this for decades and everything that i thought would work doesn't work and yet the
market is driving everybody ahead and i have no idea and then they're sitting there how is
wait let me give you this last part you should you should really watch it
uh i will then they're like what do you want the listeners to take away and he's like i don't know
i just feel like at the end we should at least be honest with each other
it's a very poignant moment right it's a very poignant moment i'm not doing that right now
i actually do think that there is a way forward but i think what he said shows what someone who
is very bright who spent their whole life looking at this like it's very important to take that as
a data point but you were asking this question okay what do i want listeners to take away so i was
giving as a reference arguably what many of the top experts in the world come to with this
so i will offer what might be a way forward but it does require taking seriously how deep a thing
we're talking about i can understand how narrow ai's are pursued by corporations and by governments
to achieve task and achieve profits but why would a corporation pursue artificial general
intelligence if profit was their objective wouldn't that also run the risk of killing
everyone in these other extreme scenarios that then profits aren't worth anything if the whole
system blows up into a giant superorganism you could ask why would any country pursue having
nukes when the more nukes that exist increases the probability that everyone including them dies
with nukes and yet you can see from it's an arms race yes it's partly an arms race and it's partly
a set of these biases those who are more focused on the opportunity than risk end up being the ones
who rush ahead and so the people who don't think agi will kill everything try to build agi because
it will solve everything wow imagine intelligence like ours but so much more it'll solve all of
science it'll give us radical life extension it'll create wealth and abundance for everyone
and we will get ushered into the promised land so we've got the naive progress people
shepherding the ai development train the naive progress combined with the motivated reasoning
on capital and winning and ego and all of the various sources motivated reasoning
combined with once they start actually being legally bound for things like profit maximizing
and that the liability is inherently externalized while the profit is centralized then combined
with things like the country doesn't even want to regulate them because it it wants the economic
growth so that it's not fuck because it uses that economic growth to grow militaries and
geopolitical alliances and so you then you get a yeah but if we regulate then china won't then
we lose so you get layers and layers of multipolar traps driving the need to continue to optimize
for near-term narrow interests this is not where i thought the conversation would go really i'm
more depressed than i thought i would be i i kind of in you and i have talked about this so i i'm
kind of aware of some of these risks and their risks that we haven't talked about and there's
information hazard with some of these risks but this sounds highly plausible to me and i think
even before agi would be reached there are plenty of other well just just the using ai as a vector
to accelerate all of the things that itself is is enough to put us into tipping points
okay so here's and i know we're late and the sun has been setting in the beautiful mountain
background behind me so we need to wrap soon so i want to try to bring back a few threads
human intelligence
unbound by wisdom it is fair to say is the cause of the metacrisis and the growth imperative of
the super organism or the capacity that gives rise to it that intelligence has created all the
technologies the industrial tech the agricultural tech the digital tech the nuclear weapons the
energy harvesting the all of it that intelligence has created all those things it has made the system
of capitalism it made the system of communism all of all of those things right and um now
that system of intelligence that takes corporeal capacities things that a body could do in
externalism the way that a fist can get extended through a hammer or a grip can get extended
through a plier or an eye can get extended through a microscope or a telescope um or our own
metabolism can get extended through an internal combustion engine or there are musculature or
whatever it is right so it takes the corporeal capacity and extends the fuck out of it extra
corporeally so that type of intelligence that does that is now having the extra corporeal
technology be that type of intelligence itself in maximized recursion not bound by wisdom
driven by international multipolar military traps and markets and narrow short-term goals
at the expense of long-term wide values so where in the metacrisis there are many risks
synthetic biology can make bad pandemics and extreme weather events can drive human migration
in local wars and this kind of weapon can do this and this kind of mining can cause this
pollution and this kind of pesticide can kill these animals those are all risks within the
metacrisis ai is not a risk within the metacrisis it is an accelerant to all of them as used by the
choice-making architectures that are currently driving the metacrisis if we weren't in a metacrisis
if we had different choice-making architectures we'd be using ai for different things but if ai is
in service of human goals and human goals have driven the super organism and the metacrisis
the way they are then this context of human goals as an accelerant of them all now if I think about
agi risk let's make we make an ai that is fully autonomous we can't pull the plug it has goals
that aren't ours it starts optimizing for those and in the process decides to use all the atoms
for something else and completely terraforms the earth right which it could do it could totally do
and we're moving way faster towards that than we are towards safety on that that is a risk within
the metacrisis landscape but ai being used by all types of militaries all types of governments all
types of corporations for all types of purposes achieving narrow goals externalizing harm to
wide goals is an accelerant of the metacrisis on every dimension and so now as we take the
intelligence that has driven these problems unbound by wisdom and we exponentialize that kind of
intelligence we get to see whoa super intelligence is really fucking potent what goals are worth
optimizing with that much power with something that's a trillion trillion times smarter and
faster than humans what goals are worth optimizing it's not global gdp because i can increase gdp
with war and addiction and all all kinds of things and destroy the environment every definition
and any so i say okay it's gdp plus genie coefficient plus this other thing plus carbon
plus whatever nope there's still lots of life that matters outside of those 10 metrics or 100
metrics that i can damage to improve those the metric set that is definable like the doubt
etching says the doubt that is speakable in words is not the eternal doubt the metric set that is
definable is not the right metric set so if i keep expanding the metric set to be gdp plus
i can still do a weighted optimization with an ai on this and destroy life and the unknown
unknown means there will always be stuff that matters that has to be pulled in where i don't
want to run optimization on this thing so then the question comes if i have something that can
optimize so powerfully what is the right thing to guide that it's the thing that can identify the
difference between the set of metrics you've identified as important and reality itself
the limits of your own models that is not intelligence that is wisdom i'm defining these
roughly in a way that you get the someone gets the sense of the difference between
what the weighted metric set of all the identified important metrics and the optimization function
on it says you should do the difference between that and what you should actually do is wisdom
and it requires being able to attune to more than just the known metrics and more than just the
optimization and logic process on those and so as super intelligence shows us how fucking
