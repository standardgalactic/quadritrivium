dangerous narrow optimization is it even shows us our own intelligence and our own intelligence
running across all the humans via things like markets that and the market incentivizes some
humans to innovate new ways to turn the world into dollars and other humans to take those
innovations and exploit the fuck out of them right so you both search algorithms and optimization
algorithms the market makes the general intelligence of humans do those in groups so the
cybernetic intelligence of corporations and nation states in the world as a whole
is already a general autonomous super intelligence running on all the humans as general intelligence
is rather than running on cpus but also using all the cpus and tpus and gpus in service of
the collection of all the narrow goals so AI accelerates the metacrisis but it also makes
clear to us that what it would take to align it is you cannot have and this is why the question
you asked who's building it and who owns it and what goals do those groups have if you wanted to
make a super intelligence that was aligned with the thriving of all life and perpetuity the group
that was building it would have to have the goal of the thriving of all life and perpetuity which
is not the interest of one nation state relative to others and is not the interest of near term
market dynamics or election dynamics or quarterly profits or a finite set of metrics but that
but that maps right on to the global geopolitical governance conversation
which we can't have it right now i mean yeah go on if you have a group that has a goal narrower
than the thriving of all life and perpetuity and it is developing increasingly general AIs
that will be in service of those narrower goals they will kill the thriving of all life and
perpetuity so what this says is inside of capitalism and inside of separate nation
state competitive interests which are inside of molok super organism type dynamics you cannot
safely build increasingly general intelligences you have to get a you have to have the general
intelligence of the humans in the group the cybernetic intelligence of these humans in the
group be aligned with the thriving of all life and perpetuity that has the those wisdom dynamics
that thing could possibly possibly be oriented at least it doesn't have a perverse incentive
to build something that was also aligned with that where it is now seeking to scale
because remember we were talking about it would wisdom is more possible at a smaller
scale of people that can be in richer relationships with each other and then scale messes it up
can AI actually help take the dynamics that can happen at smaller scales and help us to build
governance structures and i don't mean agi i mean certain tools of computational capability can
like i don't think anyone thinks that if we were to try to
build a democracy from scratch today fighting a revolutionary war whatever they would build
it the same way we did in 1776 we would be building it in computational systems would
people be able to vote from home would voting even be the the thing would would digital identity
be a thing would cryptographic provenance of information be a thing would there be some kind
of data aggregation using everybody knows if we're going to build it from scratch would be
a different thing than when you built it in the industrial era in the industrial era it was a
different thing than when the Greeks built it because they they had different problems
that's based on their tech and also different capabilities if what a democracy in a market
are our systems of collective intelligence what we need our systems of collective intelligence
and wisdom and now we're talking about building artificial intelligence do we how do we build
systems cybernetic systems where the human interaction with each other both how the humans
are developed and how they're interacting with each other factoring all their incentives
is both intelligent and wise and the computational capabilities the artificial
intelligence is not disintermediating them but is in service of the scaling of those
collective intelligence capabilities don't we need a change in cultural goals and aspirations
or prices or systems before that would happen or could we have small NGOs that get philanthropic
donations develop ai's in the service of all of life but doesn't it need a lot of resources
and compute and it those would still get out competed by the military and giant corporate ai's
if we look at the
multi-polar traps and the competition dynamics if we look at who has the resources to build
things at scale we if we look at the speed of those curves it doesn't look good
like it just to be honest it doesn't look good
uh something has to happen that we are not currently obviously on course for
and but if people can if enough people if some people can stepping back be able to see
oh the path that we are pursuing that we feel obligated to pursue oh our own opportunity
relative to risk focus is actually mistaken and we're running a cognitive bias or other
people who are employed by it recognizing that or enough people be like fuck let's make an agreement
to slow this thing down and figure other things out if you don't have again we said wisdom will
always be bound to restraint if we do not get the restraint wisdom to stop the max race then yes
this will be some of these will be near the last chapters of humanity and
so then the the task becomes how do we do that
so do we need is a first step is to expand the wisdom within the hyperagents in the system
it's always a good question if there are some people who have disproportionately more power
than others and other people who have disproportionately more wisdom do you
try to get the people with the more wisdom to have more power and influence or do you try
to get the people with more power and influence to have more wisdom or do you try to get the
you know larger collective bodies to have some more of both of those yeah and and this gets back
to the the the market the superorganism the growth imperative the narrow boundary goal because
those hyperagents that have the power and the resources to do AI and scaling they have the
optionality of money and money can be turned into everything else is it possible that that that AI
gives people more optionality than money at some point I mean I don't know how to break that
dynamic okay there is stuff about money and optionality and pursuing instrumental goals
goals that increase your optionality to pursue other future goals that narrow goal optimization
requires in AI it's called instrumental convergence that no matter what your goal is you're going to
want to pursue certain things that increase goal achieving capability capitalism does that
we didn't finish answering is the totality of the market already a superintelligence we
defined it as a superorganism is it also superintelligence so there's some stuff that we
have not got to I will allow that we're just not going to get to that now we could go for a
biology and and ferris is eight hour record and one recording no I'm not going to do that
what I'll say is that if where we leave this with all of the open threads people have a lot of
questions and we want to come back and address those I think that would be interesting in the
need of closing because I'm also realizing that I am late for another call yeah with friends of ours
on the topic I want to share something that I think will be helpful in the thinking about the
wisdom intelligence relationship which is not saying how we enact it the enactment thing is a
real real tricky thing but just on the what we need to enact if people have not watched the
conversations that David Bohm and Krishna Murthy had together back in the day I would
recommend them as some of the most useful valuable beautiful recordings of human conversation I've
ever seen and in one short clip David Bohm speaking on a few YouTube it I think it's called
like fragmentation and wholeness something like that he basically identifies and this was maybe
the 80s the cause of the metacrisis so he didn't call it metacrisis or superorganism but like
all the problems of the human predicament that is clearly going towards a point of
self termination that was seeable at that time the way he defined it I think was exceptionally
good I think it maps to the way indigenous wisdom has defined it and other things men are not the
web of life we're merely a strand and at whatever we do to the web we do to ourselves but when we
become power capable of doing exponentially powerful stuff then our own short-term or our
when lose becomes omni lose lose our short-term optimization ends up affecting what we would
even the time scales we care about so what Bohm said is the underlying cause of the problem is
a consciousness that perceives parts rather than that perceives holes or the nature of wholeness
and because it perceives parts it can think about something separate from others so it can
think about benefiting something separate from others and either it can then care about some
parts more than others so it's okay harming the other things or it just doesn't even realize it
is right so whether it is separation of care and values or separation of just calculation
and so I can benefit myself at the expense of somebody else I can benefit my in group at the
expense of an out group I can benefit my species at the expense of nature I can benefit my current
at the expense of my future I can benefit these metrics at the expense of other metrics we don't
know about and that all of the problems come from that that in so far as we were perceiving the field
of wholeness itself and our goals were coming from there and then our goal achieving was in service
of goals that came from there that's what wisdom binding intelligence would mean which is the
perception of and the identification with wholeness being that which guides our manipulation of
parts i.e. tech technology and then Ian McGillcrest who I think you are going to have on your show
or maybe already have if not I haven't please introduce us but keep going Ian in the master
in his emissary I think advanced what David Bohm was saying in an incredibly beautiful way
and he'll share it here on the show but basically said to not hit evolutionary cul-de-sacs
there is a capacity in humans that needs to be the master and another capacity that needs to be
the emissary meaning in service of also bound by you could also say there is a capacity that
needs to be the principal and another that needs to be the agent in legal terms of principal agent
dynamics and basically the thing that needs to be the master is that which perceives not mediated
by word symbols language models perceives in an immediated way the field of inseparable wholeness
and the emissary is the thing that perceives each thing in light of its relevance to goals
and figures out how to up regulate some parts relative to others i.e. what we think of as
intelligence relevance realization salience realization information compression and so
when I was talking with him I said so it was the master and intelligence is the emissary
and I said you're basically saying that the principal that the the emissary developed all
these powerful capabilities and so in some places the emissary said fuck the master thing I want
to be the master and it had the tools to do so starting to win that thing on a runaway dynamic
is the cause of the metacrisis the super organism he's like exactly so which maps to what Bohm was
saying it corresponds to the reality that the master circuits would orient towards so if you
already look at all the problems in the world in the global metacrisis and the up impending
catastrophes being the result of the emissary intelligence function unbound by the master
wisdom function then you look at AI being taking that part of us already not bound by wisdom and
putting it on a completely unbound recursive exponential curve that's the way to think about
what that is so what is it that could bind the power of AI adequately has to be that what human
intelligence is already doing is bound by an in-service to wisdom which means a restructuring
of our institutions our political economies our civilizational structure such that the goals that
arise from wisdom are what the goal achievement is oriented towards that is the next phase of human
history if there is to be a next phase of human history so my takeaway from that and from this
whole conversation is that well first of all my takeaway from the whole conversation is I owe
you an apology from the time we met four years ago I thought limits to growth and the great
simplification dominated AI as a risk and looking back I didn't understand actually I didn't
understand until the last three hours how that merged that AI merges with the super organism
in this potentially catastrophic way that accelerates all the things so as a hyper verification
of the intelligence that is already driving the super organism exactly yeah and then my second
thought is there still is a role for education for culture and leading by example and by a
a transference of the me-based culture to a we-based recognition both between humans we are
in this together and we're part of the web of life and that's also part of the we and the more
humans that understand and feel that whether through ayahuasca or drums or being in nature or
being with others or meditation or whatever it is the higher chance we have to intervene with
with the the trajectory we're on I mean that's my just but those people mild takeaway those
those people who go spend time in nature observing non analytically but communing with the
the truth goodness and beauty the intelligence the meaningfulness of it who do the ayahuasca and have
those experiences if they become lotus eaters and simply drop out it doesn't affect the curve of the
world either it's it's the equivalent of them being deniers yeah the key is and yet if people are
working to make change but they are not actually connected to the kind of wholeness that they
need to be in service to and they continue to have what seem like good goals but they're narrow
we need to get carbon down we need to get the rights of these people up we need to protect
democracy we need to get our side elected because the other side is crazy we need to we need to
develop the ai to solve this problem anything less than the connectedness with wholeness
and anything less both at the level of care and at the level of calculus even though you
can't do them you are oriented to try with the humility that knows you will never do it properly
the humility that knows that you will never do it properly is what keeps you from being
dangerous from hubris but the part that really really wants to try is what has you make progress
in the in the direction of the service of the whole we should probably do our next conversation
in the service of the whole and take a deep dive in that and what it might look like and maybe
there will be some people that that respond to that calling and then come back and do a deeper
dive on these ai questions because it's going to take me a while to to process this um this has
been great is there anything else you want to contribute we'll add some things to this show
note so the piece that eliazer yudkowski did on bankless i think is worth watching both because
of getting what he said but also that the moment in culture um the bone christina marty pieces i
think are super valuable migil crest and um tyson you're gonna have on i think samantha would be
great to have on she and i've had conversations on these topics and has good indigenous kind of
insights um there's a guy named uh robert right who's made a bunch of short really simple ai
risk videos that i think are exceptional as a resource to share um not the guy that wrote the
the moral animal no no different one okay different guy um and let me verify uh robert
miles excuse me not robert robert miles and um so if people just want to understand the ai issue
better with like short simple explainer videos i think his are some of the better ones i know of
um okay and obviously the conversations you and i had previously in the series contextualized a
lot of it and then if people write to you with questions there is there are so many threads
we left open um would be happy to come back to it and be fun okay thank you for continuing to
think and push on our plight uh it seems daunting but um this is the time we're alive we have to
understand it uh care about it and engage with it but this this is a this is a big old red pill my
friend i mean it only takes the whole of our self in service to the whole of reality uh
and utilizing both all of our technological and trans technological capabilities uh for the purposes
that are inclusive of everybody and so uh if that could seem daunting can also seem inspiring
yeah it's both it's both yeah um to be continued and uh thank you thank you my friend these are
fun to be in these conversations with you if you enjoyed or learned from this episode of the
great simplification please subscribe to us on your favorite podcast platform and visit the
great simplification dot com for more information on future releases
