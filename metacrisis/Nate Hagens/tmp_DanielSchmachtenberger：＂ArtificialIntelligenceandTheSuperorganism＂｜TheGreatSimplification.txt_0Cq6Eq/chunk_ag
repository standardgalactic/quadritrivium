up in an evolutionary cul-de-sac so with that little aside on that tool or technology is actually
a very deep concept yes a narrow AI that is not an agent that a human is using is a is a tool but
it's a tool of a very special kind where it is it can uh take lots of steps on its own to help
achieve a goal rather than me just use it for fully specified purposes right so this is a kind of
important distinction now if i look at AI benefit first so as to not seem overly negative for all
goals humans have that would create some progress or benefit to some real thing can we use AI in
service of those yes not not all things equally today but lots of things is it all the scientific
progress we have on solving new diseases and stuff like that can we use AI to accelerate it
and accelerate movements and science and discovery medicine um so might AI be able to speed up the
rate by which we come up with a way better nuclear energy maybe fusion maybe deep geothermal yes
might it be able to solve many types of cancers yes if i have a daughter that
is dying of cancer do i want to hear anyone slowing down the systems that if they get
there fast enough might save her life i don't want to hear that that sounds like the most cruel
evil fucking thing whatever other problem it it's going to bring about i'll deal with that problem
later um is there someone listening to this that that cares mostly about the environment
and climate change is there a a progress case to be made that AI will help carbon emissions and
and reduce uh environmental impact you can i use AI to model how to do geoengineering more
precisely can i use AI to do better genetic engineering on crops to maximize their carbon
sequestration if i'm a carbon fundamentalist can i use um AI to advance stem cell meats can i use
AI to advance energy technology or battery storage technology or any number of things
like that can i use AI to affect supply chains to lower energy on supply chains and decentralize
a lot of things where you don't have as wide a supply chain yeah can you say on all those things
okay got it and so for any goal we have if i can use that to enhance the goal i don't want to
hear anybody talking about slowing that down yep i agree this is the same as i understand
same as capitalism right if i can use if i if i if i if me having access to more capital speeds
up my ability to achieve my goal and i believe in my goal then i don't want to hear anything about
taxing that or decreasing that or fucking up my capacity to achieve those goals yeah and yeah
um so are there a lot of things that human intelligence can do that are good
that increasing that type of human intelligence through artificial systems that can operate
on more data and faster could also do yes now why would we be so concerned about it then
so let's talk about a few different cases one type of AI risk is AI employed by bad actors
now nobody thinks they're a bad actor for the most part right like there's some exceptions
but most people that someone else calls bad actors right they're they're terrorists but to them
they're freedom fighters was it a protest or was it a riot blah blah blah the lake off frame on those
things very much is it progress or is it destroying all these areas for the people that are being
destroyed by it who who want to do some destruction back that they consider tiny in the name of
self-protection it is a protest and a freedom fight otherwise it might be a terror terror act
or a riot right um but let's just pretend that we weren't thinking about all that and just call it
bad actors for a moment criminal activity um can people whether it is a dude that would have just
shot up a bunch of stuff with an ar-15 because that was a technology he could use to achieve
harmful goals and obviously this is why people are concerned about assault weapons is because
they couldn't kill as many people as easily with a knife right um or or a single loader
pellet gun when the second man was created well they can do way more harm as drones become
more widely available that you can hook explosives to and so it happens to be that the bad actors
who want to do fucked up stuff oftentimes are in states of mind where they're not amazing
technologists and can't coordinate lots of people and do strategy and technology this is not
always true but sometimes it has been true we've been saved by this but as we make the destructive
capacity now it might have constructive capacity i can use a drone of course for a lot of positive
things to be able to monitor construction and railways for safety and to plant trees from
the sky and whatever but can we also use it to fly an explosive over some critical infrastructure
so um when we make the thing for the positive purpose we also enable all the things that it
can do for any purpose when we make that easy enough and in that case probably 80 plus of
those bad things we can't even imagine at the beginning that's a complex topic i want to get to
is okay how you how you do externality forecasting but um because we can do a much better job than
we ever have done and saying that we couldn't is a source of plausible deniability for not trying
well what i meant was there's unknown unknowns uh to use donald brownsfeld's term with ai yeah
but if we're okay i have to do this tangent then because it is fucking critical um okay
there are problems that well we're already in line for this to be the longest podcast
i've ever done so let's uh let's do it right keep going because there are problems that
are second third and third are effects that were not easy to anticipate can we use a technology
for a certain purpose that creates unanticipated and maybe even unanticipatable consequences
yes can you prove that you can anticipate in advance everything no it because there will be
some things that are unknown unknowns that you can't have proven that you thought through
in the safety analysis ahead of time so that's a true thing but because that's a true thing
people use that as a source of bullshit plausible deniability to say i couldn't have possibly known
where then they don't even really try to forecast the things because they will privatize the gains
and socialize the losses and this is a very very important thing to understand also related to the
progress narrative and the underlying optimist versus pessimist or i would state opportunity
versus risk orientation the people who focus more on the opportunity of a new technology this is
going to do all these amazing things blah blah blah they're going to move faster focused on that
then the people who are focused on the risk and really want to do good thorough safety analysis
and make sure it won't cause any of those risks that takes a lot of money and a lot of time and
doesn't rush you to market as fast as possible so the first mover advantage is going to happen
by the guys who take risk less seriously focus on the opportunity more they'll be able to get the
network dynamics that are very very powerful from associated with first mover advantage
and early scale they'll be able to um regulate in their interest because they state that the
risks are not that bad and they do lobbying efforts with the money they got from early revenue or
investment or etc the people who take the risk seriously do the analysis and say oh fuck there's
actually no way to advance this that is good right now we just you just not do it well those people
just get to sit on their knowledge of the problem and not do anything but also not gain any power
to affect the system because they're not going to generate money by which they can do lobbying and
public opinion effect and etc or they say there is a safe way to do it but we just spend all of our
money figuring out safety not not doing optimization so there is a perverse incentive in general
for those who are more focused on opportunity than risk and as a result we get all the opportunities
and all the risks and the risks happen the cumulative effect with with with capitalism
and the market in the last 50 years it is a market function not 50 years forever
right okay yeah um so
but the risks are getting larger as the technology gets larger and the cumulative effects are
increasing so now we're at a place where the cumulative effects of industrial tech on the
environment are reaching critical tipping points of planetary boundaries and the exponential tech
is getting to a place where its capacity the destructive capacities are so fucking huge right
like the atomic bomb was the first thing that could destroy everything quickly and until then
for all of human history we couldn't do any quick thing that would destroy everything
now the atomic bomb is not the only thing like that synthetic bio where you can make
an artificial life form that doesn't have a few natural genetic mutations but so many that it
could be in it could be an invasive species everywhere right or artificial intelligence
these are examples of things like where their destructive effects can
are are actually exponentially more than any previous kinds of thing but if you combine things
like a public corporation having a fiduciary responsibility for share for profit maximization
which you can argue why that made sense you can argue why the shareholders are giving you their
money to work with those shareholders come from pension funds you need to give that back to them
they can only trust you to have it if you have this bound fiduciary responsibility to return
their funds appropriately and you're innovating a good or service that people want that is not good
for the world blah blah blah of course as you can see where the logic of the market became less and
less true as rather than rational actors we figured out how to nudge everybody into less and less
rational action and the supply side figured out how to manufacture demand for things that don't
increase the quality of people's lives and didn't account for the cost in the environment you still
have that story so you've got the must maximize profit and then you have the nobody would want
to innovate and the innovation is good for everyone if the bad things that happen they were
personally liable for so we'll make liability limiting properties where the corporation will get
fine not any of the people or directors involved so then doing things that destroy everything
just becomes a cost of doing business no real deterrent for it so you the corporation will
privatize the gain socialize the losses the actual people who make the decisions have a
bunch of upside and not relative downside you put all those things together and you say ai is being
developed in those environments and so it has to do profit maximization it does not have the people
who are making the decision have the liability associated with the scale of risk and harm that
could occur those who are more opportunity focused build the corporations that become
worth tens of million tens of billions of dollars and have all that power to also influence lobbying
and influence public opinion and those who do the safety analysis run tiny nonprofits that nobody
listens to comparatively so it's important to get that this asymmetry but that perversely
orients those who think about opportunity more than risk as risk is moving into an exponential
scale and then rationalize that that is itself one of the underlying drivers of the metacrisis
and embedded in that and i understand that embedded in that is the natural human scale
ethical feedback loop when people are innovating and they go and work in a room for months and
they're working on something they kind of get a little bit of recognition and of and an insight
into what they're doing here you code something and you press a button and all those negative
potential externalities are just in the future and you get no emotional like sense of what's going
on is that also true yeah the fact that it happens outside that happens at a scale like many of the
harms will occur via supply chain somewhere in the world i won't see and as we already said there'll
be second and third and fourth order effect so let's say for instance now this is this is a very
important point um we can say we can say that all technology is dual use dual use being a military
term meaning and there's two different ways to think about it a technology is developed for a
military purpose so its primary use was military it also might have civilian or non-military
applications right we're developing the um rocketry capability to make missiles hit their
targets but maybe we'll also be able to use it to put satellites and outer space for communications
for everybody um and obviously computation was developed in world war two to crack the enigma
machine and whatever for military purposes and it has had a lot of other purposes on the other side
we say anything that is developed for a non-military purpose probably also has a military application
right so dual use goes both ways the fact that if i'm developing something for a non-military purpose
it still probably has a military application i.e. you can say on the positive side defense but
on the opposite side the the offense or killing capability means that that has to be factored
in the development of the technology but dumb question though if someone at microsoft is
creating an ai for some purpose they're they're not sharing that with the us government or any
government right there's gotta be it's gotta be independently developed by coders and engineers
within the government within the military no no no um no if a technology is seen as having
risks to national security then there will end up being government bodies that have oversight
into certain capacities and got it um but that's one way the other way would just be the corporation
develops the capacity and one of its clients becomes military and becomes a military contractor
in addition to other things um one would be it doesn't even say microsoft doesn't develop a
military side application it develops lm's and then palantir develops competing lm's and makes a
military application of them right so the technology itself will get developed lots of places will get
reverse engineered use for lots of purposes um now so all technologies dual use now on the side of
we're developing something for a civilian purpose that we say is positive but it also has military
okay we have to think about that but the other side is also risky because if you're developing a
military technology that is very powerful and very dangerous but at least you think you can control it
the moment of that same capacity also gets a civilian purpose means it will proliferate and
it makes it much harder to control like let's say you developed drones for military purposes
now it gets a commercial application which is i can just fucking film stuff with it
now everybody can get access to drones wow that just really affect the capacity for decentralized
terrorism so okay tell me where you want to go i'm about to go to the next part beyond dual use
and and you can i'm just gonna comment that i get this sinking feeling that we're headed
towards a risk singularity yes but the the metacrisis is a risk singularity in which the
underlying drivers over determine failure meaning if we could prevent the ai apocalypse that doesn't
prevent the synth bio one or the planetary boundary one or the you know gazillions of
other ones if we could stop planetary boundaries regarding fishing that doesn't affect what we're
doing to soil or nitrogen runoff or or PFOS pollution or whatever the underlying thing
is creating so many different sources that can lead to catastrophic risk that if you don't deal
with the underlying thing and you just deal with some of the risks you only buy a tiny bit of time
i have thought that the underlying thing was the emergent phenomenon that i call the super
organism with which is the growth compulsion of our global market system
uh yeah what you're saying what you're saying is that we had agricultural surplus
then we found flammable fossils then we accelerated our technology then we went to debt
then we had the internet and each one of these kind of exponentially increased what had come
before and you're saying that ai is the next quote unquote tool in service of this growth-based
super organism yeah so you say growth let me which is that's a fair way to say it but let me
define it slightly differently because i'm going to define it in a way that is more aligned with
this ai conversation so i think it actually gives a lot of insight into the other
narrow goal achieving is the underlying generative dynamic okay i get it
and growth is an epiphenomena of that
an wait an epiphenomena and underneath a result of okay so the second order effect
first thing is the narrow boundary goal and growth is the second order effect of that narrow
boundary goal yes so if i want to achieve a goal and having a little more surplus gives me more
optionality to do that but me having that more optionality when i'm rival risk that increased
security i have inherently decreases somebody else's security or their relative competitive
advantage in a status game for mating or whatever it is so now they have to do that then i see them
doing it so now i need to do more than i needed to do now we're in a race on it growth is the epiphenomena
what everyone is pursuing is not the growth of the whole system they're pursuing their own
narrow goals and generalized optionality for goal achieving okay
and the growth in total consumption of energy and atoms the growth of total waste and entropy
the growth of intelligence and new types of technology the growth in memetics and the ability
to convince a lot of people of it are all epiphenomena of both goal achieving and specific
and then the general capacity of increasing optionality for goal achieving could we ask ai
how to go from a narrow to a wide boundary goal so all technology has certain affordances and
all technology has combinatorial potential with other technologies and the affordances of them
together are different than them on their own and obviously not only does the hammer have
different potentials if i also include nails and saws and you know the other things and it would
on its own but i can't even get a hammer without the smithing tools that would be necessary to
make a hammer right so there's a whole technological ecosystem i'm just i mean i'm cutting to the
chase on this i'm just wondering what ai does to our certain complexity there's a reason i'm
having to try to do this all tools have combinatorial potential with other tools they all have
