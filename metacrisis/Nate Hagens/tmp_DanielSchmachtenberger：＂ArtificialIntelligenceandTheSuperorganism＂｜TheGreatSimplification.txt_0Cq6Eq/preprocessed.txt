Artificial intelligence is in the news we hear about chat gpt making people more efficient
learning quicker we hear about AI replacing artists and mid-level programmers we see
deep fakes and fake beautiful baby peacocks that are much cuter than real baby peacocks and
lots of people are debating about the benefits and risks of artificial intelligence
but today's guest is my colleague and friend Daniel Schmackdenberger who is back for a deep dive
on how artificial intelligence accelerates the superorganism dynamic with respect to extraction
climate and many of the planetary boundary limits that we face. I have not heard this
angle on artificial intelligence before I think it's really important to have this conversation
and throughout this talk with Daniel we talked about AI but underpinning it all was what is
intelligence and how has intelligence in groups in human history out competed wisdom
restraint of different cultures and different groups of humans. This is an intense dense
three and a half hour conversation and we weren't even done we'll be back in the next month or so
to have the follow-on questions. It's probably one of the better conversations I've ever had
on the great simplification and I think it's really important to merge the environmental
consequences of AI into our cultural discourse. Here's my friend Daniel Schmackdenberger.
Hello my friend. Hey Nate, good to be back with you. I prepare a lot for my podcasts. I read people's
stuff. I prepare questions. I think about it but with you I'm like I got an appointment with Daniel
at 4 p.m. I go for a bike ride. I go play with my chickens and I just show up and we have a
conversation so I'm hoping this will work because this conversation actually is the culmination
of how our relationship started a couple three years ago. Remember we came to Washington DC
for a five or six day meeting where I wanted to discuss energy, money, technology and how this
combined into a super organism and you were focused on existential risks and particularly
oncoming innovation in artificial intelligence and how that led to a lot of potential unknown
destabilizing risks for society and now we've educated each other after a couple years
and today rather than continue our bend versus break series I thought we would like merge these
two lines of thought on artificial intelligence and the super organism. You and I have done five
parts so far in this bend versus break series given all the things that are in the public
attention on AI we decided to do this one. I imagine some of the people will have heard that
series and we can reference the concepts. For anyone who hasn't do you want to give a quick
recap on super organism so we can relate it and maybe super organism and metacrisis and
those are kind of frames we've established that we're going to be bringing into thinking about AI now?
Sure. So humans are a social species and in the modern world we self-organize
as family units, as small businesses, as corporations, as nation-states, as an entire
global economic system around profits. Profits are our goal and profits lead to GDP or GWP globally
and what we need for that GDP is three things. We need energy, we need materials and we need
technology or in your terms information and we have outsourced the wisdom and the decision
making of this entire system to the market and the market is blind to the impacts of this
growth. We represent this by money and money is a claim on energy and energy from fossil
hydrocarbons is incredibly powerful, indistinguishable from magic effectively on human time scales.
It's also not infinite and as a society we are drawing down the bank account of fossil carbon
and non-renewable inputs like cobalt and copper and neodymium and water aquifers and forests
millions of times faster than they were sequestered so there is a recognition that we're impacting the
environment and all of the risk associated with this you label the metacrisis or the
polycrisis or the human predicament but they're all tied together the system fits together
human behavior, energy, materials, money, climate, the environment, governance, the economic system,
etc. So right now our entire economic imperative as nations and as a world
is to grow the economy partially because that's what our institutions are set up to do partially
because when we create money primarily from commercial banks increasingly from central banks
when governments deficit spend there is no biophysical tether and the interest is not created
so if the interest is not created it creates a growth imperative for the whole system
and we require growth now so far the market has dictated this growth but suddenly there's a new
kid on the block which is artificial intelligence created by prior intelligence by humans and
that's what we're going to talk about today. That's a good frame.
I'm an educator I've recently been a college professor my whole role today is to inform and
lightly inspire humans towards self-governance, better decisions, better pathways forward and I
my trade deals with science and facts and systems ecology and I'm afraid that AI
will spell the end of what we know is true and on both sides we won't know what's true and there
will be things that people can grab on the internet that many of which are fake or influenced by
artificial intelligence that destroys the social discourse so that's one thing I'm worried about
AI. The other is will AI accelerate climate change because it will make the super organism
it would be like playing Super Mario or Donkey Kong or something like that and pressing the turbo
button and it makes processes more efficient and it just speeds things up which means more carbon
either directly or indirectly more efficiency feeds Jeven's paradox. Another one of my worries
is a lot of jobs are going to disappear from AI and how does that factor into the super organism
so it seems to me that AI both simultaneously makes the super organism
hungrier and more voracious but also runs the risk of killing the host in several ways
so these are just some of my naive questions but I think before we get into artificial intelligence
maybe we'll just start with intelligence and humans I think in a previous conversation you
and I had that's what differentiates us from the rest of the biosphere is our ability to
problem solve and use intelligence to grow the scale of our efforts that is coupled with energy
and materials always so maybe you could just unpack how you see the historical role of intelligence
before we get to artificial intelligence so I might start in a slightly different place
which is to actually start with a couple cases of AI that are obvious and then we'll go back to
intelligence the relationship between intelligence and the super organism itself why human intelligence
has made a super organism that is different than animal and natural intelligence made in terms of
the nature of ecosystems and then how artificial intelligence relates to those types of human
intelligence not just individually but collectively as you mentioned mediated by markets or larger
types of human collective intelligence systems and and then get to what has to guide direct
bind intelligence that it is in service of something that is actually both sustainable
and desirable so let's talk about just artificial intelligence for a moment to give a couple examples
because people have heard since and and the reason it's up so much since artificial intelligence was
kind of innovated in the 50s and some could argue precursors before that the reason it is
in the conversation so much currently is the deployment of large language models publicly
and where starting with GPT-3 and the speed of the deployment of those relative to any other
technologies GPT-3 getting 100 million users and I forget exactly what it was now six weeks or
something which was radically faster than tiktok's adoption curve facebook's youtube's cell phones
anything which were already radically faster than the adoption curve of oil or the plow or anything
else so world-changingly powerful technologies at a speed of deployment which then led to other
companies deploying similar things which led to people building companies on top of them which
leads to irretractability and so the speed of what started to happen between the corporate races
the adoption curves and the dependencies is of course understandably changed the conversation
and brought it into the center of mainstream conversation where it had been only in the
domain of people paying attention to artificial intelligence or the risks or promises associated
previously so when people talk about AI risk or AI promise of which it has a lot of both
there's a few things about cognitive bias worth addressing here first which is a topic you always
address on why people come to misunderstand the superorganism and get kind of choice making
uh wrong get sense making wrong thank you that that means you actually have of course i have
watched and read your things this is my friend here you're so damn busy that i'm like hey daniel
watch this and you're like i will but you and i have not talked about cognitive biases but
you're right i do talk about them a lot so uh carry on so let's take there there are clusters
of cognitive biases that go together to define like default worldviews and they're not a single
cognitive bias or a kind of a bunch of them and you don't even have to think of it as bias it's just
like i mean it's a strong sounding word though it's true it's a it's a default basis for the
sense making and meaning making on new information people are likely to do first and so one of them
that i think is really worth addressing when it comes to ai is a general orientation to techno
optimism or techno pessimism which is a subset of a general orientation to the progress narrative
and i would argue and will not spend too long on this so it actually warrants a whole big discussion
i would argue that there are naive versions of the progress narrative uh capitalism is making
everything better and better democracy as sciences technology is don't we all like the world much
better now that there's no vacane and antibiotics and infant mortality is down and so many more
total people are fed and we can go to the stars and blah blah blah like obviously there are true
parts in everything i just said but there is a naive version of that that does not factor all
the costs that were associated adequately and there's a naive version of techno pessimism
so first on the naive version of techno optimism when we look at the progress narrative there's
so much that has progressed that if you want to cherry pick those metrics you can write lots and
lots of books about however everything's getting better and better nobody would want to be alive
at any other time in human history there's there's two things that the naive progress is missing one
is is the the costs like climate change and the oceans and insects and the other is is the one time
subsidy uh of of non-renewable energy and inputs and the the source capacity of the earth and and
those are are not finite so those are the two two blind spots i think in that narrative
so we could say the costs and the sustainability of the story um yes and
so if you talk about the story of progress particularly like the post-modernity version
of science technology and the associated social technologies not just physical tech because
capitalism and democracy and international relations are all kind of coordination systems
that we can call a social technology a technique a way of applying intelligence to achieving goals
and doing things um of which you can consider language in early social technology which it is
if you ask the many many indigenous cultures who were genocided or extincted or who have just
remnants of their culture left um or if you ask all of the extincted species or all of the
endangered species or all of the highly oppressed people uh their version of the progress narrative
is different and just like the story of history written by winners or losers but if you add all
of those up the totality of everything that was not the winner's story is a is a critique on the
progress narrative and so one way of thinking about it is that the progress narrative is there
are some things that we make better maybe we make things better for an in-group relative to an out
group maybe we make things better for a class relative to another class for a race relative to
another race for our species relative to the biosphere and the rest of species or for some
metrics like whatever metric our organization is tasked with up regulating or gdp or something
relative to lots of other metrics that we are not tasked with optimizing or for our generation
versus future generations exactly short term versus long term and so the question is where it is
not a synergistic satisfactor where there are zero some dynamics that are happening the things
that are progressing are at the cost of which other things and we're not saying that nothing
could progress in this inquiry we're saying are we calculating that well and if we factor all of
the stakeholders meaning not just the ones in the in-group but all of the people and not just all
the people but all the people into the future and not just all the people but all the other life
forms and all of the definitions of what is worthwhile and then what is a meaningful life not
just gdp then are the things that are creating progress actually creating progress across that
whole scope so i have two replies to that my first is amen and my second is you're advocating for a
wide boundary definition of progress as opposed to a narrow boundary one and the definition
between wide boundary and narrow boundaries very related to the topic of intelligence too
are our goals narrow goals or are they very inclusive goals if we have a goal to improve
something for whom are there if we're is it for a small set of stakeholders is it for a set of
stakeholders for a small period of time is it measured in a small set of metrics where in
optimizing that in being effective at goal achievements we can actually externalize harm
to a lot of other things that also matter and whether we're talking about technology itself
or nation-state decision-making or capitalism or whatever we can talk about something where
all of the problems in our world we could say have to do the human-induced problems have to do
to do with the capacity to innovate at goal achieving decoupled from picking long-term
wide definition good goals and that doesn't mean that there is nothing good about the goal it means
that the goal achievement process is fragmented the world enough that and sometimes it's not even
perverse right i'm going to get ahead economically and i'm going to fuck the environment and the
people doing slave labor in the mines and i know it and i'm just a sociopath so i do it sometimes
it's not that sometimes it's we are the world is complex nobody can focus on the whole thing so
we're going to make say a government that has different branches that focus on different things
so they can specialize and specialization and division of labor allow more total capacity
and so this group is focused on national security of this type or focused on whatever it is let's
focus on if it's the UN world hunger now is it possible to have a solution to world hunger
that is where now my organization has specific metrics how many people are fed etc and whether
how much of the budget we get next year to be able to do stuff and whether we get appointed again
or elected again have to do a specific metrics where it is possible to damage the topsoil it's
possible to use fertilizers and pesticides that will harm the environment and cause dead zones
and oceans and destroy pollinators that advance our metric but if we don't there is actually no way
to continue within that power structure this is an example where it's not even necessarily perverse
in a knowing way but the structure of it the institutional choice making architecture is such
that what is being measured for and optimized and tasked can't not prioritize some things over
others and that with increasing capacity to goal achieve what is externalized to the goal
is increasingly problematic so is the narrow boundary focus versus the wide boundary focus
could that itself be a basic fundamental difference between intelligence and wisdom
and then building on that if an entity a tribe a nation a culture focuses on the narrow boundary
goals won't that out compete a nation a tribe a culture that focuses just on the wide boundary
broader multivariable things wonderful like fairness or environment or future generations
so let's come back to that definition of wisdom and the relationship between wisdom and intelligence
but let's address that we were saying earlier there is a naive version of the progress narrative
or the kind of techno capital optimist narrative there's also naive version of the technopessumist
narrative the technopessumist narrative over focuses on all of the costs and externalities
who who lost in that system and basically orients in a luddite way and is like
no fuck tech and new things it was better before there's various versions one is there was more
wisdom before and this is a descent from wisdom in terms of like cleverness that is
somewhere between less wise and evil the harm the benefits that come from this will be more
like hypernormal stimuli that actually cause more net harm that we're moving towards tipping points
of catastrophic boundaries for the planet etc so let's just let's just not tech and extreme versions
of that look like the Amish but unabomber wrote a lot of things on this topic right
and and they were not dumb things he was doing a real critique of the advancement of technology
and then being like how do we not destroy everything if we keep it on this track now
and we will also see that there have been indigenous perspectives that wanted to keep
indigenous ways that wanted to resist certain kinds of adoption of things that would as far as
technological implementation is considered be considered a non embrace of progress now you
were just mentioning if tech is associated with goal achieving and some goals have to do with how
to upregulate the benefits of an in group relative to an out group doesn't tech mean power yes
doesn't a group that rejects some of it mean less power in the short term so where those
competitive interests come particularly if the other side both has tech and the mindset to use it
does that end up meaning that that doesn't forward and we can see that when china went into
Tibet and it was kind of the end of Tibetan culture as it had been uh was that because
Tibet was a less good culture meaning it provided less fulfilling life for all of its people than
china and nature was selecting for the truly good thing for the people or the world or like no we
can see that whether we're talking about Genghis Khan's intersection with all of the people he
intersected with or alexander the greats or whatever that but that was worrying yes yes
that those who innovated in the technology of warfare the technology of extraction the technology
of surplus the technology of growing populations coordinating them and being able to use those
growing coordinated populations to continue to advance that thing relative to others there were
cultures that might have lived in more population sustainability with their environment maybe more
long-term harmony maybe said let's make all our decisions factoring seven generations ahead and
they were just going to lose in war every time and so um the naive techno negative direction
just chooses to not actually influence the future right is going to say i'm going to choose something
because it seems more intrinsically right even if it guarantees we actually have no capacity for
enactment of that for the world and that's why i'm calling it naive i don't i don't understand that
if that last thing could you give an example yeah if someone says i think a particular advancement
of i think advancement of tech in general focuses on the upsides that are easy to measure because
we intended it for that purpose doesn't focus on all the long-term second third fourth order
downsides that are going to happen i don't want to do that we want to have a much slower process
that fact that pays attention to those downsides only incorporates the things with the right
use and guidance and incentives it will lose in a war it will lose in an economic growth to
the other cultures that do the other thing if you want to take a classic example and go back
to um and it didn't happen exactly this way anywhere because it happened in such a different
ways in the fertile crescent and in india and whatever but but as a kind of thought experiment
illustration the plow emerges animal husbandry for being able to use the plow now we have to
domesticate a bowl turn it into a or a buffalo turn it into an oxen and um and that involves all
the things it does it involves castrating it it involves having a whip and you stand behind it to
get it to pull you know the plow for row cropping whatever so certain animistic cultures were like
i don't want to do this we'll hunt a buffalo but we also will protect the baby buffalos we'll make
sure that our body goes into the ground to become grass for the future ones we're part of a circle
of life we believe in the spirit of the buffalo i can't believe in the spirit of the buffalo and
beat one all day long and do things to it where i wouldn't want to trade places with it but the
culture that says now we're not going to do that thing is not going to get huge caloric surplus
it's not going to grow its population as much it's not going to make it through the um hard
weather times as well and so when the new technology emerges those who use it if the technology
converse confers competitive advantage it becomes obligate because whoever doesn't use it
or use at least some comparable technologies loses when you get into rivalrous interactions
let me take a brief uh rabbit hole side side step here um but while it's fresh in my mind
i think this dynamic that you're talking about now and i know we're going to get to artificial
intelligence um but in my public discussions people are recognizing the validity of the systemic
risk that that i'm discussing and that we're headed for at least potentially a great simplification
simplification is the downslope of a century plus of intensive complexification based on energy
but those communities and you could talk about countries that simplify first because it's the
right long-term thing to do in the meantime they're going to be out competed by communities that don't
because those communities will have more access to government stimulus and money and technology
and other things but it almost becomes a tortoise in the hare sort of story uh had a podcast a few
weeks ago with um Antonio Turiel from spain and he said europe is in much worse shape than the
united states because the united states has 90 of its own energy so europe is going to face this
simplification simplification worse first in a worse way so the united states has another decade
so you guys are off the hook and i was thinking to myself really because yes the united states
is mostly energy independent but europe will be forced to make these changes first and maybe
they will have some learnings and adaptations that will serve them in the longer run when
we just ride high in the superorganism for a while longer i mean that's that's really a
complicated speculation but what do you think about all that is that relevant or
well this so this is why we talk about the
the
need to be able to make agreements that get out of the race to the bottom type dynamics the
multipolar trap the social trap because of course if anybody starts to cost resources properly
price resources properly meaning pay for what it would cost to produce that thing renewably
via recycling and whatever it is and not produce pollution in the environment through
known existing technology they would price themselves out of the market completely relative
to anyone else not doing that so either everybody has to or nobody can right and whether we're
talking about pricing carbon or pricing copper or pricing anything as as you say well we price
things at the cost of extraction plus a tiny margin defined by competition and that was not
what it cost the earth to produce those things or the cost to the ecosystem and other people of
doing it so the proper pricing at pricing is really very deep to the topic of perverse incentive
and yet if we talked about how do we ensure that in our and this is core to the progress
narrative right because the thing that we're advancing that drives the revenue or the or the
profit is the progress thing the cost to the environment of that we're extracting something
unrenewably that is going to cap out that we're turning into pollution and waste on the other
side and we're doing it for differential advantage of some people over other people and affecting
other species in the process if you were to the stakeholders that benefit you get a progress
narrative the stakeholders that don't benefit you you get a non-progress narrative but until
industrial tech like it's important to get that before industrial tech we did extinct species
right we over hunted species in an area and extincted them we did over we did cut down all
of the trees and cause desertification that then changed local ecosystems led to flooding ruin the
topsoil we did over farm areas so environmental destruction causing the end of civilizations
as a thousands of year old story but it could only be just now global so until we had industrial
tech we could not actually we just weren't powerful enough to mess up the entire biosphere
so how powerful we are is proportional to our tech because we can see that a polar bear cannot
mess up the entire biosphere no matter how powerful it is corporeally right the thing that can mess up
the entire biosphere is our massive supply chain technologically mediated things starting with
industrial tech and so given that we are for the first time ever running up on the end of the
planetary boundaries because we figured out how to extract stuff from the environment much faster
than it could reproduce and turn it into pollution and waste much faster than it could be processed
and we're hitting planetary boundaries on both sides of that on almost every on almost every
type of thing you can think about right in terms of biodiversity in terms of trees in terms of fish
in terms of pollinators in terms of energy in terms of physical materials in terms of the chemical
pollution planetary boundary so the things that are getting worse are getting very near tipping
points that were never true before those tipping points will make it through the things that are
getting bad better won't matter even for the stakeholders they're intended and that's a key
change to the story is it can no longer be that the winners can win at the expense of everybody
else it is that we are actually winning at the expense of the life support systems of the planet
writ large and when that cascade starts obviously you can't keep winning that way which is optimize
narrow goals at the expense of very wide values you've you've described the naive progress optimist
and the naive oh yes progress pessimist is there such a thing as a progress realist yes
yes so i am a techno optimist meaning there are things that i feel hopeful about that require
figuring out new ways to do things new technique both social tech and physical tech but i'm cognizant
that the market versions of that tech are usually not the best versions the because of the incentive
the because of the incentive landscape in the same way that if facebook hadn't had an ad model
it would have been a totally different thing right if we're just talking about the technology
of being able to do mini to mini communication but you had something that was not a market force
driving it could you have had something that was much better that was not trying to turn people
into a commodity for advertisers which means behaviorally nudge them in ways that manufacture
demand and drive the emotions that manufacture demand maximize engagement x which causes the
externality of every young person having body dysmorphia and ubiquitous loneliness and
confusion about base reality and polarization could we have done it where rather than drive
engagement the goal was to actually look at metrics of cognitive and psychological development
and interconnectedness across ideologic divides and do that thing yeah of course right so the
same technology can be applied to wider goals rather than more narrow goals and you get a very
different thing so the base technique it's the technology and the motivational landscape that
develops its application space we have to think about together so there is a there there are
ways that we can repurpose existing technologies and develop new ones both social and physical
technologies that can solve a lot of problems but it does require us getting this narrow
goal definition versus wide goal definition and the if intelligence guiding technology
is as powerful as it is and actually exponentially powerful what and we're defining
intelligence here as the ability to achieve goals what is it that defines what good enough goals
are that being able to optimize them exponentially is not destructive
that's how you would get a progress narrative that is post naive and post cynical
in contrast i'm probably a techno pessimist or at least a mild one because i see how technology
is acted as a vector for more energy and more climate co2 and and degradation of nature at
the same time i think it's how we choose what technology we use like a golden retriever is
probably the best technological invention ever of our species even though it's really more of a
co-evolution but you know what i mean it's it's something that that we came about and and sexually
selected and for companionship and they don't they give us the complete suite of evolutionary
neurotransmitters for not a lot of resource input and there's lots of other technologies
that are appropriate that help us make meet basic needs and give us well-being that don't
destroy the biosphere but this gets back to i don't think individuals wait wait this is
let's just important yeah okay the superorganism thesis that you put forward
shows that the superorganism is oriented on a path that does kill a toast and thus itself
right it does destroy the space reality the substrate that it depends on
the metacrisis narrative that i put forward says a similar thing is why we did this whole five
series to kind of show the relationships and so i would say as long as the axioms of that thing
are still in place yes i am a technopessimist meaning i think that the good things that come from the
new tech don't outweigh the fact that the new tech is in general more often than not accelerating
movement towards catastrophic outcomes factoring the totality of its effects so but this is why i said
there is a post naive and post cynical version in which i'm a techno optimist but it requires
not being on that trajectory anymore it requires that the technology is not being built by the
superorganism in service of itself but is being built by something different in service of something
different well in that way i'm also a techno optimist because after growth ends and after
superorganism is no longer in control efficiency will no longer be a vector for jevons paradox
because then efficiency is going to save our vegetarian bacon because as things as the economy
is shrinking efficiency is going to be really important and and innovation just right now it's
feeding more energy and stuff into the hungry maw or the people who haven't heard the previous
stuff on jevons paradox will you do that briefly why efficiency because obviously ai can cause
radical efficiencies which can help the environment that's part of the story of why it's an environmental
hope so would you explain why as long as jevons paradox is the case yeah so humans get smarter
on how we use energy around 1.1 percent a year so we get more energy efficient every year
um because we're smarter coal plants use less coal to generate the same amount of electricity
we invent solar panels um our our televisions are a little bit more energy efficient in our
laundry machines and one would think on the surface that that would allow us to use less energy
but what ends up happening is that money money savings get spent on other things that use energy
and writ large new innovation ends up system-wide requiring a lot more energy since 1990 we've had
a 36 percent increase in energy efficiency at over the same time we have a 63 percent increase
in energy use so as long as growth is our goal and our cultural aspiration is profits and GDP
more energy efficiency will paradoxically unfortunately result in more energy and environmental
damage that's called jevons paradox it was based after a 19th century economist who
observed this walter stanley jevons who uh observed this in steam engines that steam engines wouldn't
reduce our energy use but they would scale because they helped everyone and and were so useful so
let's talk about first versus second third and the order effects here because jevons paradox is um
it's important to understand that we make a new technological daniel what are what are the um what
are the odds that we actually don't get to artificial intelligence on this conversation uh low
okay keep going first second third order go for it so if we create a new technology that
creates more energy efficiency on something whether it's a steam engine or a more energy
efficient energy generation or transportation or storage technology the first order effect of it is
we use less energy the second order effect is now that it takes less now that we have more available
energy and energy costs less there's a bunch of areas where there was not positive energy return
profit return that now become profitable and so now we open up a whole bunch of new industries and
use more total energy but it's a second order effect or even a third order effect because it
makes some other technology possible that does that this is one of the asymmetries that we have to
focus on in the progress narrative is the progress narrative is and technology in general when we
make a new technology and by technology i mean a physical technology or even say a law or a business
to achieve a goal where we're generally making something that is trying to have a first order
effect on a narrow goal that is definable in a finite number of metrics for a small set of
stakeholders the stakeholders are called the total addressable market of that thing
and very rarely is the total addressable market everything right and so
we're making things whatever it is so i'm using technology in the broadest sense of human innovations
towards goal achievement here we're making technologies to achieve first order effects
meaning direct effects for a defined goal for a defined population even if we're talking about a
non-profit trying to do something for coral it's still focused on coral and not the amazon and
everything else right and so it can optimize that at the expense of something else throughout
in terms of the second third order effects of whatever putting that thing through does
and so we put out a communication to appeal to people to do a thing politically well it appeals
to some people it really disappears to other people one of the second order effects is you
just drove a counter response the counter response is people who think that the thing you're benefiting
harms something they care about and now you just up-regulated that is that being factored
and so the progress narrative the technology narrative and all the way down to the science
narrative and this is where we get to the human intelligence versus wisdom and then how this
relates to artificial intelligence is it is easier to think about a problem this way here's a
definable problem it affects these people or these beings it is definable in these metrics we can
measure the result of this and we can produce a direct effect to achieve it we did we got progress
awesome and the progress was more GDP the progress was people who could communicate faster the progress
was less dead people in the ER the progress was less starving people the progress was whatever the
thing was that we were focused on even if it seems to be a virtuous goal but that same thing that you
did maybe polarized some people who are now going to do other stuff that is a second order and maybe
third order effect maybe it had an effect on supply chains maybe it had a so the second third
and third order effects on a very wide number of metrics that you don't even know what they are to
measure on a very wide number of stakeholders that you don't even know how to factor is harder
in kind to think about so it is it is logically cognitively easier as we talk about intelligence
to figure out how to achieve a goal than it is to make sure that goal doesn't fuck other stuff up
so efficiency too has a narrow boundary and a wide boundary lens with which to be viewed
yes and and but here's here's one of the challenges though it's easy for a group of humans
or a full culture to optimize one thing it's very difficult to optimize multiple things
at once multivariable inputs and outputs are incredibly complex so optimizing
dollar profits tethered to energy tethered to carbon combining technology materials and energy
that is a thing that was very easy you know akin to the maximum power principle
so I what do you think about that I think that optimization is actually the wrong framework
when you think about everything that matters you're not thinking about optimization anymore
you're thinking about a different thing so optimization is now let's come back to what
is distinct to human intelligence why did that cause a metacrisis or a superorganism
how does AI relate to that and then what it would take what thing other than intelligence is also
relevant to ensure that the intelligence is in service to the to what it needs to be in service to
so we're not saying that humans are the only intelligent thing in nature obviously not nobody
nobody reasonably would say that but there is something distinct about human intelligence
so how do we define intelligence it's fascinating go look up on a bunch of
encyclopedias and you'll see that there are a lot of different schools of thought that define
intelligence differently some do it in terms of formal logic and reason some do it in terms of
pragmatics which is the ability to process information to achieve goals some do it just kind
of from a information theoretic point of view of the ability to intake information process it and
make sense of it and all of these are related I'm not going to try to formalize it right now but
I'm going to focus on the applied side because it ends up it ends up being the thing that's selected
for and it ends up being the thing that wins short-term goals and that obviously we're building
AI systems for so there we can say intelligence is the ability to figure out how to achieve goals
more effectively and so or we can just say the ability to achieve goals we can see that a slime
mold has the ability to achieve goals and it will figure out and reconfigure itself a termite colony
figures out how to achieve goals and it reconfigures itself there's some element of learning
and when you watch a chimpanzee figuring out using this stick versus this stick to get larvae out of
a thing you can watch it innovating and learning how to achieve goals so all of nature has intelligence
what is unique about human intelligence relative to other things relative to other let's just say
animals we could talk about plants funguses all the kingdoms but that gets harder so let's just stick
with other animals well first we realize that we can't talk about this properly because from an
evolutionary perspective there were things between the other animals that we look at now and humans
meaning earlier hominids and so where do we call humans in that distinction so since they're not
around we can mostly talk about sapiens versus everything else on the planet that we're aware
about this point but we can say that the thing we're calling humans starts before homo sapien
probably with homo habilis or australopithecus or somewhere around there having to do with not
we are the ninth we're the ninth homo and perhaps the last we don't know
so people might question what kind of weird anthropocentrism is it that would have you
say that you know that you have some kind of intelligence the whales don't have or the chimps
don't have or whatever and i think it's very fair to say what it is like to be a whale we really
don't know and what about the experience of whaleness the quality of the sentience of it
might even be deeper than ours might be more interesting in some ways totally right like
no no that's a harder problem in kind to address but we can in observing their behavior
say there are types of goal achieving that they clearly don't have the ability to do in a prima
evidenced way that we obviously have the ability to do they have not figured out how to innovate
the tech that makes them work in all environments the way we have they have not even figured out
how to stay away from boats that are whaling boats and so from their most obvious evolutionary
motive how to figure out that thing is not a thing that they've really done and so we can
see in a prima facie sense that they are not innovating in technology and changing their
environment making the equivalent of an Anthropocene in a similar way even when we see the way that
beavers make beaver dams and change their environment or ants do they do it roughly the
same way they did it 10 000 years ago humans don't do it roughly the way we did it 10 000 years ago
so we can see something unique about humans in our behavior related to the innovation in technology
and environment modification in whales defense they don't have opposable thumbs and they're
underwater but um but i'm with you keep going well this is not putting whales down i think
opposable thumbs are pretty significant to the story right i think there are things about the
evolution of homo sapiens that probably have to do with the combination of narrower hips from
uprightness that allowed us to de-weight the hands that allowed them to be more nimble and
opposable with the larger heads that involved neoteness birth and there's this whole complex of
things um and so in no way to saying that humans have more of this particular kind of innovative
intelligence mean have a more meaningful right to exist those are totally separate things right
doesn't mean have a deeper experience of the world those are different things so so let me let me
get back to uh something you just said a minute ago unless this is where you were heading but you
said intelligence is problem solving um en route to a goal and most animals in nature their goal is
is uh well it's security and mating and reproduction but energy uh energy return is a primary goal
in nature to invest some energy and get a higher amount back because that enables all sorts of
other optionality energy calories in nature are optionality for organisms so the problem with humans
isn't the intelligence per se it's the goal i don't even want to call it a problem yet i want to
call it a difference we'll get to the problem in a minute okay so all right um i want to say
something because this is related here about modeling because human intelligence all forms
of intelligence have something to do with modeling they can take in information from the
environment and be able to forecast what happens if they do something enough to be able to inform
their next choice which choice is more likely to achieve some future goal even if future goal is a
second right so let me let me interrupt there doesn't that does that differentiate humans
that we have if we model something that we have the perception and ability to consider time
and how does time factor into intelligence we're not the only animal that has a relationship to
time but we definitely have the ability to have abstractions on time that seem to be unique from
what we can tell and we also have the ability to have abstractions on space and abstractions
on other agents and there's something about that uh the nature of abstraction itself that is related
to the what is novel in human intelligence the type of recursive abstraction and so but there's
a reason i'm talking about modeling there's something a model of reality is taking a limited
amount of information from reality and trying to put together a proxy of that that will inform us
for the purpose of forecasting and ultimately choice making ultimately goal achieving in so far
as the model gives us accurate enough forecasts that it informs actions that achieve our goal
we consider it useful that doesn't mean that it is actually comprehensively right the models end up
being that they're optimizing for a narrow set of sense making just like what we were talking about
before that we optimize for a narrow set of goals and the reason i bring this up is because
all of our models that can be useful even in trying to understand the metacrisis and whatever
themselves can also end up blinding us to being able to perceive outside of those models so when
Lao Tzu started the Tao Te Ching with the Tao that is speakable in words or understandable uh
conceptually is not the eternal Tao it was saying keep your sensing of base reality
open and not mediated by the model you have of reality otherwise your sensing will be limited
to your previous understanding and your previous understanding is always smaller than the totality
of what is um i would even argue that uh thou shall have no false idols a model of reality that says
here's how reality works is the false idol which messes up our ability to be in direct perception
of new things where our previous model was inadequate so i say this because there are places
where a particular thing we're going to say is useful but it is not the whole story and it's
important to get where it's not the whole story so for instance if we talk about energy that doesn't
include the parts about materiality and the parts about intelligence and even if we talk about those
three that's actually not the whole story so it's useful but i'm wanting to call this out
if an animal is eating are they eating only for energy no they're also eating for
uh minerals and for enzymes and for vitamins and for proteins and for fats and not just fats that
will get consumed as energy but they'll become part of the phospholipid membrane of cells and
they're eating for materiality as well right and so it's not true that all energy is fungible
i can't really feed an elephant meat products well even though there's plenty of energy in it
so so so hold on a second so elephants also have what i refer to as the trinity
which is energy materials and technology they they try to get acacia trees they use their trunk or
some other tool and in the acacia leaves are energy the photosynthesis from the sun but also as you
said atoms minerals materials so there it's the same for animals as humans there's a reason why
how so i can't make certain amino acids from other amino acids i can't make some minerals from
other minerals no matter how much calcium i get i get no magnesium from them and i need a certain
amount of magnesium so of course if i don't get enough dietary input of vitamin d we get rickets
and die even if we get plenty of b vitamins and vitamin c and other things and so those nutrients
are non fungible to each other and you need all of them you needed the whole suite that a thing needs
which is why uh there's very interesting health studies that show people who are dying of obesity
are actually dying of diseases of malnutrition because i know we have a diet that has basically
optimized for calories with while stripping all the micronutrients out of them so you can be eating
tens of thousands of calories a day and actually becoming profoundly deficient in minerals and
phytochemicals and other things like that where then your body is wanting to keep eating because
it's actually starving and you continue to give it something that creates a neurochemical stimulus
that says you ate and satiates the hunger thing for a moment but what you're actually starving for
is not in that food um and so i don't i don't want to oversimplify that energy is a part of the story
everything we say is a part of the story but the the totality of the story is more complex
than however we talk about it right there's just there's something so important and sacred about
that because what is wrong about our narrow goal achieving is uh what's upstream from that is our
narrow modeling of reality and what even equals progress who is worth paying attention to how is
it all connected when i make a model that separates it all then i can up regulate this harm something
else but i don't even realize i'm harming something else because that's not in my model
um i don't even realize that that thing that i'm optimizing for isn't the actual thing
or is only a part of the whole thing and probably and then probably by definition we choose models
or inputs into the models that kind of confirm our own built identity up until that moment
and or win in some game theoretic way so if it achieves if the model damages lots of things
but makes me win the war that model will probably win and you can you notice how it's like
okay so we we get the inquisition we get the crusades we get some
fuck and gnarly violent cruel like figuring out how to optimize torture stuff
in the name of the guy who said let he who has no sins among you cast the first stone
and you're like how the fuck did we go from principles of forgiveness and let he who has
no sins cast the first stone into this version that says the inquisition is the right way to
to do that what you see is the interpretations there's lots of interpretations the interpretations
that lead to kill all your enemies and proselytize win in in the winning and short term warfare
not because they're more true or more good but because they orient themselves to get rid of
all of the enemies and have more people come into them and have nobody ever leave the religion
because they're afraid of hell and whatever so there is a and this is an example of there are
models that win in the short term but that actually move towards comprehensively worse
realities and or even self extinction evolutionary cul-de-sacs and i would argue that humanity is
in the process of pursuing evolutionary cul-de-sacs where the things that look like they are forward
are forward in a way that does not get to keep forwarding and at the heart of that is narrowing
is optimizing for narrow goals and at the heart of that is perceiving reality in a fragmented way
and then getting attached to subsets of the metrics that matter models which leads to us
wanting to optimize those models and those metrics and now i would start to define the
distinction between intelligence and wisdom here and that wisdom is related to holes and
wholeness intelligence is related to the relevance realization of how do i achieve a goal a goal
will be a narrow thing for a narrow set of agents bound in time modifying a fixed number of parameters
and so then i would say if human intelligence distinct from other types of animal intelligence
where we don't just have the ability to work within a range of behaviors that are in capacities
that are mostly built into where the primary physical technology is our bodies right the
animals evolved to have claws to have blubber for the cold to have whatever it was that was a
technological innovation to be affected within its environment and it can't become radically more
of that thing by choice by its own understanding it becomes more of that thing through genetic
selection which is super slow and it doesn't control and the mutation that makes the giraffe
have the slightly longer neck or the cheetah a little faster or whatever it is is happening as
the rest of the environment is going through similar mutations so the cheetah is getting a
little faster but so are the gazelles and there's co-selective pressure so that if the cheetah
gets a little faster first and eats the slower gazelles then what's left is the faster gazelles
whose genes inbreed right so you have tiny changes happening across the whole system
and co-up regulating each other so there are symmetries in the rivalry that lead to the entire
system still maintaining its metastability not stability meaning a fixed equilibrium a homeo
dynamics not a homeostasis that continues to increase in complexity over time but that metastability
is the result of that type of corporeal evolution right but then humans adaptive capacity is not
mostly corporeal it's mostly extra corporeal you call it extra somatic meaning outside of just our
body and it's both we can use a lot of calories outside of our body which started with fire
fire was the beginning of us being able to warm ourselves and all of a sudden make new
environments possible and make foods edible that weren't edible before right that was
but that's calories right that's extra somatic calories then our ability to get more
calories from the environment meaning gather more stuff kill more things
involve the innovation of tools right those spears those stone tools allowed a little group of primates
to take down a mastodon the combination of their coordination technologies with each other
because a single person couldn't do it and their stone tools together was able to do that you get
more caloric surplus the agricultural revolution really advanced that the oil revolution really
advanced that but at the heart of how did we figure out how to get oil and how to use it was
intelligence it was this kind of recursive intelligence that figures out I can use this in
service of my goals I wonder if
30,000 years ago some Neanderthal interbreeding with a human could ever imagine that 30,000
years later there would be a brain evolved like Daniel Spongeburgers well check this out
Tyson young caporta I don't know if he's been on your show yet or not
two weeks from now okay so great then you can follow this conversation up with some
things I learned from Tyson and other indigenous people who hold some indigenous wisdom knowledge
have told me similar things that one date back more than the standard current archaeological
narrative of when humans knew certain shit but also with kinds of wisdom that have definitely
been lost in the progress narrative and one of the things that Samantha Sweetwater told me
originally and then Tyson said something similar was that when that many of the indigenous cultures
had a story that when humans develop the first stone tools the apex predator of the environment
Samantha's version was the saber tooth came to the human obviously this is a story right but you
you get what it would mean that the early people had made the story and they said we were the ones
who were taking care of and maintaining the complex diversity of the whole system in this
kind of apex predator role you now clearly are and we're turning over the mantle of stewardship
of the whole to you your job because now you have the ability to destroy the whole ecosystem you
must be the steward of the whole thing and imagine that there is that that even recognizing because
maybe those stone tools were two million years ago right and maybe we already had just killed
a bunch of megafauna extincted them extincted some of our other hominid cousins through
gruesome kind of interspecies genocidal tribal warfare destroyed some environments and already
had time to learn those mythos and be like no no no we're not going to maximum power principle
kill take everything we're going to live in sustainability think seven generations ahead
and there was wisdom about appropriate use of technology and restraint 40 000 years ago
that was my question is we are homo sapiens our appellation which is wise man but any small
percentage of tribes or individuals or nation states or warring clans that pursued a narrow
boundary goal would have out competed those tribes with wisdom not any of them and here we are with
the super organism any of them above a certain threshold right um what what what do you mean
let's say that we had a number of tribes in an area that had all developed some kind of wisdom
by which they bound intelligence and i'm not saying we don't do that today it's called law right
um right right and it's supposedly also what religion is about the development of wisdom
of what is the good life what is worth pursuing and not pursuing in which you get things like
religious law you're not going to work on the Sabbath you're going to take that day to do different
things for instance if you just if you want to think about that as an example you could think
about the Sabbath as an example of law binding a multipolar trap associated with a naive version of
progress if you don't have a Sabbath some people will work seven days a week in the short term
before they burn out they'll get ahead they will get so much ahead because they'll be able to keep
investing that differential advantage and rent seeking behavior that anyone who doesn't will
have no relevance to be able to guide their own lives and now you have a world where no one spends
any time with their kids no one reflects on the religion nobody enjoys their life everything
sucks for everyone because somebody did that thing in the name of progress because they moved
ahead faster so we say no no no you're gonna have a day where you don't fucking progress stuff that's
actually the gist you're not going to be focused on external progress in the world and so there's 27
or 29 ways in Leviticus that you can violate the Sabbath and you'll be killed if you violate it
which seems like just wackadoodle religious nonsense but if you're like wait no you never
have to actually do that if you if you if you hold that law that extremely is everyone's like
all right we're not gonna fuck with the Sabbath now what do I get to do that day I get to reflect
rather than achieve goals I reflect on what are good goals so I get to spend time with my family
I get to spend time with nature I get to read the scripture I get to meditate I don't get to
achieve goals I get to experience the fullness of life outside of goal achieving and I get to
reflect on what goals are truly worthwhile and in doing so bind the multipolar trap that I don't
have to because everybody else is rushing ahead that would be an example of the way religions
were supposed to have something like wisdom that created something like law and restraint to buy
naive versions of progress in a way that was actually better for the whole long term
so two comments there one when I was much younger I had some Jewish friends
and I didn't ridicule them but I was kind of haha you guys have Sabbath today I'm gonna go
to the arcade or go on a boat ride or go fishing or whatever but now as I'm older everything you
just said about the good life and spending time with family and reading and spending time in nature
and not using the internet on a Saturday or whatever sounds freaking wise and makes sense
and appealing to me so maybe with age and maturity I'm flipping from intelligence to wisdom and then
the second thing the implication and maybe this is where you're heading is to uh muzzle or forestall
the risk singularity that is coming from the superorganism we have to have have some Sabbath
equivalent applied to AI it's not just Sabbath equivalent but you know almost all of law is
about restraint right things that you don't do in the presence of having incentive to achieve
narrow goals what are the things that for the collective well-being which also means the capacity
for your own individual well-being for all individuals into the future what are the things
that we say we don't do if you have Samantha on this is a topic she'll talk about cares a lot
about which is there's no definition of wisdom worth anything that is not bound to the concept of
restraint but our but how yeah I don't know how our culture is approaching a biophysical
wily coyote moment in 20 or 30 or 40 years ago before all this over leverage and and
different systemic risk we could have added restraints but now our restraint would would
almost default uh create this this rubber band snapback in the economic systems but we can talk
about that another time well so this is where you end up having
the embedded growth obligation of a system the embedded continuity of a system the kind of
institutional momentum um that partner Zach was writing something recently a couple of people
in the team were contributing and in the beginning it's it was talking about where do we
find ourselves now and it says we find ourselves in the relationship between the life-giving
nature of the biosphere and the life-giving nature of the civilizational system and the
unique point in time at which the ladder is threatening the ongoing continuity of the
former upon which it depends and that what it takes to maintain that civilizational system
will destroy the biosphere the civilizational system depends on so you we must remake the
civilizational system fundamentally we do need civilizational systems we do need technological
systems but we need ones that don't have embedded exponential growth obligations we do need ones
that have restraint we do need ones that don't optimize narrow interests at the expense of
driving arms races and externalities we do need ones where the intelligence in the system is bound
by and directed by wisdom right which is the equivalent of Sabbath plus law plus
emergence now come back to for a moment what the you know ha ha what idiot's reaction you had
when you were young i had a similar one and lots of young people probably even many people who were
raised jewish have a similar one before they understand the full depth of it let's so let's
talk about chesterton's fences for a moment um never heard of that i don't know uh actually the
history of why i've got that name but there's a thought experiment and philosophy that says
called chesterton's fence which is there's a fence up and you think oh the purpose of that fence was
x that's no longer here the fence is ugly and in the way let's cut let's take the fence down
is there a chance that the purpose for the fence included several other things that you don't know
and you don't know that you don't know at them before you take the fucking fence down you better
make sure you actually understand why it was put up and now this comes to a very deep intuition
we were talking about biases earlier in the progress narrative progressives it's funny how
right now that is somehow associated with left in some weird way but like progressive and traditional
is a deep dialectic and neither one are supposed to be the one you choose it's a dialectic you're
supposed to hold them in balance right and um very much in the same way this is an important
point and it relates to holes and wisdom versus narrow goals narrow value sets are as bad as
narrow goals they're a part of it so any value that is a real value exists in a dynamic tension
with other values a dialectical one oftentimes but other values where if you optimize the one at the
expense of everything else you optimize it you get these reducto ad absurdums right so
meaning the optimization of any value by itself can end up looking like evil so if I want to
optimize truthfulness and all I'm going to do is speak the truth all the time then when the
Nazis come and ask me are there any Jews inside I say yes no I don't the truthfulness is not the
only value at that point to the preservation of life and kindness and other things are we can even
see an example where it's like if someone in a naive sense says I'm I'm my value is honesty
there's a bunch of places where you can see a person who in the name of honesty is just an asshole
right and they just say kind of mean things and say it's in the name of honesty we can see
that in the name of say kindness people will lie to say flattering things and avoid painful things
we can see that if you hold them in dialectical tension
you actually get a truth that is more truthful and you get kindness that is more effective
because the kindness that doesn't want to tell the person they're an addict when they are and
nobody does or tell the emperor they have no clothes or say anything painful that is necessary
feedback isn't even kind so sometimes for that value to even understand itself fully there's
this kind of dialectical relationship that helps that come about now here's where I'm coming to
Chesterton's fence and then we'll then I want to hear okay there is a dialectic between a traditional
impulse and a progressive impulse the traditional impulse basically says I think there were a lot
of wise people for a long time wise and smart people who thought about some of these things
more deeply than I have who fought and argued and that the systems that made it through evolution
that made it through made it through for some reasons that have some embedded wisdom in it
that I might not understand fully and it makes sense for me to kind of have as my null hypothesis
my default trusting those systems they wouldn't have made it through if they weren't successful
didn't work and likely the total amount of embedded intelligence in them is more than I've
thought about this thing that's the without knowing it that's the traditional intuition
the progress intuition is we collective intelligence is advancing built on all
that we have known we're discovering new things and we're just and we're moving into new problem
sets where the previous solutions could not possibly be the right solutions because we
have new problems so we need to have fundamentally new thinking obviously these are both true now
on the traditional side the chesterton's fence thing is I might have as a kid or you might have
as a kid thrown out the Sabbath and said that's dumb before we actually understood it because
we understood a straw man version of it said it was stupid and threw it out and so when we're
talking about wisdom and restraint and all like that there is something around are we seeking to
because in the name of progress there will always be something that is focused on restraint that
seems like it's fucking up that progress I could get and I if I don't understand all the reasons
for the restraint that factor second third fourth nth order effects long into the future in the
short term I should do the thing in the short term like no of course I should advance the AI applied
to genomics to solve cancer without thinking through the fact that the fourth order effects
might involve increasing bio weapons capability for everyone and destruction of the world so even
the cancer solutions don't matter in the course of those people's lives and so this is is there a
whole enough perspective to be able to see how the things that are actually wise from a narrow
perspective look stupid two questions one is the metaphor the same as when I was younger I thought
those things were stupid and now I recognize the validity of them that's where we are as a culture
we we are the younger version of Nate in in the the intelligence versus wisdom dynamic I'm I'm
just speculating I think that's probably the case and then two I mean you in in all the people that
I know I know a lot of smart people you're certainly up there but you also have wisdom
and I don't know as many people that have both intelligence and wisdom and you
in my in my sphere rank near the top but is it in our genome is it in the human behavioral
repertoire to hold more than those single values to hold multiple values and wide boundary
views of the world what do you think about that so we said that there is something unique
about the types of in recursive intelligence that lead to technology innovation and the
Anthropocene the superorganism etc in human intelligence relative to other species so
let's talk about the genetic predisposition and the what the predisposition is actually for
and the nature nurture thing a little bit I would say and again everything I'm gonna say
here will be at a high level that is hopefully pointing in the right direction but totally
inadequate to a deeper analysis of all the topics our nature in terms of the genetic fitness of
humans homo sapiens it would be fair to say that our nature selected for being more quickly and
recursively changeable by nurture than anything else
that is individual humans
and the individual human is not the unit of selection and evolution the tribe is
right well the tribe of the band the group of humans
there are sometimes individuals sometimes tribes I don't think there is much of a case
for individual humans surviving in the early evolutionary environment by themselves very
well and the behavior of them as individuals separate from social behavior leading there
are certainly some animals that are largely solitary and they have a different set of
selection criteria than primarily social animals humans are a primarily social but
I mean this was this was EO Wilson David Sloan Wilson's paper
that selfish individuals out compete within groups and cooperative groups out compete selfish
groups so I think I think both are hardwired in us but let's not get detracted by that
actually what I'm saying holds with this the individual there are some selection of an individual
within a social environment but there's no selection of an individual outside of other sapiens
right right and so the unit of selection that is driving the dominant feature for sapiens the
unit of selection is a group that's actually really important thing to think about as opposed
to that the unit of selection is an individual because we have such an individualistically
focused culture today and we think in terms of individual focus way excessively to the actual
evolutionary fitness of an individual outside of a tribe was dead in almost every environment for
most of history so a set of behaviors that made you alienate the tribe was not an evolutionary
strategy for most of the evolutionary basis of humans and the problem now is our tribe is
eight billion strong pursuing profits tethered to carbon which is no longer a tribe the tribe
was capped out at the scale at which there were certain types of communication across that group
that allowed it to be the unit of selection and so everyone knowing everybody everybody being able
to communicate with everyone everyone being able to participate in some choices that then everyone
would be bound by so they stayed in it rather than defect against it which is why you got
kind of dunbar limitations and then there is a series of things where you went from say
couple hundred thousand years of dunbar to huge cities in a relatively short period of time
which is the beginning of the thing we call recorded civilization and we would say that
most of the super organism properties we talk about now were at that junction started
because in the smaller size lots of lying and sociopathy and whatever really don't pay because
people are going to know you're lying and enough people can beat you up if you are very problematic
so we as individual humans because evolutionary selection acted at the tribal unit
but we are have the capacity for wisdom but once the number of people and the self-organization
went to the city-state nation-state size there became downward causation of the
emergent phenomenon at the aggregate level that started to focus on intelligence and out
competed the wisdom of individuals in smaller smaller units yeah the multi-polar trap the
moloki behaviors really took off there because you can think of homo sapiens with tools they
were already different than the rest of nature because they had stone tools and fire and language
so they were already different which is why they had already extincted other species and moved
become the apex predator everywhere right the the beginning of the story is the beginning of
the type of intelligence that leads to recursive innovation recursive abstraction leading to
recursive innovation so when you look at what various people call the defining characteristic
of humans very or the earliest technique that made humans really distinct stone tools fire and
language are three very common ones brought up stone tools and innovation in the domain of matter
fire is innovation in the domain of energy language is an innovation in the domain of intelligence
for information and they were all intelligence applied to those innovations recursive intelligence
of this kind right a group of humans with no stone tools even can't do anything to a mammoth
just with their fangs and claws they just they're not going to hunt a mammoth right
eight eight dudes aren't going to do that and one dude with a spear isn't going to hunt a mammoth
it is the coordination protocols and the physical tech together that led to those capacities so
that both the social tech and the physical tech and so I'm meaning tech here as intelligence
applied to goal achieving the innovation of new fundamental capacities that most broad definition
of tech and so in so for a couple hundred thousand years of sapien you have this very
small size a couple million years of hominids writ large and then we can argue why exactly we
started to get way above beyond the Dunbar number and where we went and beyond the couple Dunbar
numbers into um sumaria egypt gobleci tepe like the first large ones but um one function that
certainly comes up in the analysis is is having already capped out migration as a strategy and
tribal warfare you start getting into resource limits with another tribe you're sharing the
same environment competing for the same stuff you just move when you've moved to all the places
there's nowhere else to move there's resource issues and one tribe is willing to be warring
now the small the other tribes have to unify together to survive so they're willing to sacrifice
some of the collective intelligence of their smaller scale and their intimacy with each other
and whatever for survival collective wisdom of their smaller scale yes and now there is collective
intimacy actually and collective wisdom are very deeply coupled because if everybody can know
everyone you can have intuitive mappings to everyone you can care about everyone you can
have shared pathos and you don't want to hurt someone where you hurt if they hurt right as
soon as we get to much larger scale where i don't know everyone i don't trust everyone i don't have
intimacy now i have to have some way of knowing we're in group we are all under the same flag we
all have the same um indicator of some kind uh and now we now i don't get to act in a unique way
to each person i have to have rule sets that mediate it so we start to get this kind of
intelligence rule-based law that is different than wisdom now one could say that all of the
wisdom traditions were trying to there's different ways of saying this having built an environment
that we were not naturally fit to and the way you would talk about this our neurochemistry our
dopamine opioid axis is evolved in a certain environment and then we've created a new environment
for ourselves that we're no longer fit now it's easy to get more calories and in in the early
environment it wasn't so there was a dopamine relationship to always get more calories whatever
now in this environment we're no we're kind of actually in some ways genetically misfit so you
could say the the wisdom was trying to tell us how to deal with the dopamine opioid axis in a
environment that we were no longer evolutionarily fit for to deal with evolutionary mismatch
you could also say it was the wisdom was trying to deal with how do we guide the
collective relationship to the technological powers we have that has some long-term viability built in
number one as you're speaking on this grand arc of history i kind of feel sorry for all
the peoples and the cultures that expressed wisdom and were out competed by the larger entity
number one and number two can we have wisdom with eight billion humans in our current
environment or is wisdom only able to be had with smaller scale smaller groups that aren't faced with
the evolutionary environmental scale limits and pressures in terms of feel sorry for
now with that beginning of an insight talk to any indigenous person and you'll get to get the context
of their life so talk to tyson about this and have them get to share like oh so the colonialist
story was that we were savages that need civil civilized let me actually tell you about what
civilization means what something that was more truly civil was let's say why the habsi
and narrative that our lives were short brutish nasty and mean as the apologism to be able to
genocide the fuck out of us is the dumbest story in human history dumbest and cruelest right
but here's where also the naive technopess misnarrative doesn't work is that if you don't embrace
the tools that will end up being what defines the world if any of them are there
then you don't actually get to have any say in the future and which is why right now
i am advocating for something that recognizes both the problems of tech and the problems of a
multipolar trap where if you just try to get things right for your people and not everybody
you still fail are are we ready to talk about artificial intelligence so the reason i was
talking a little bit about that the evolutionary history of humans was in these smaller scales
and totally different types of environment with oral language rather than written language with
lots of things very different but that unique to humans is the capacity to be more changed
by their environment you could say that our nature is to be very nurture influenced and
as much as you can take a wild animal and train it to be different in a different
environment in one generation how much you can do that with a human is so radically different
and because the other animals are genetically fit to their environment they evolved to have
capacities that work within a niche and yet humans evolved to make tools that would change
our whole environment including make whole new environments right homes and cities and etc we
couldn't come hardwired to a particular environment or a particular way of being because our evolutionary
adaptation was not our bodies our body extended through tools that we change and environments
that we change so we had to be able to upgrade to okay new safety and new environment what language
are we speaking what tools are we optimizing for what type of in in environment are we navigating
we just migrated so we have to come incredibly not prepared for a specific set of things which
is also why we're neoteness for so long right the the human sapien is dependent and helpless for a
very long period of time compared to the chimpanzee or any other close relative and but we also get
our violent sheets don't right keep going yet we have a totally different like we don't come
preset to do the evolutionarily adaptive thing within that environment because our evolutionary
adaptation was to change how we were adaptive meaning which tools and everything we use which
languages we use um which cultures we identify with way more environments way more plastic and
flexible in our behaviors than i mean the the the elephants today are doing similar things to
elephants 10 000 years ago so think about it a human growing up learning how to throw a spear
versus learning how to use chat gpt our very different skill sets and throwing a spear is not
that advantageous today in most cities and most environments even though it's the most advantageous
thing so you don't want a kid growing up with genetically inbuilt spear throwing capacity
you want them growing up to figure out what is the tech stack around me what's the language what
are the goals how do i do that thing which is why they're also fairly useless at everything to begin
with because they haven't imprinted what is useful in this environment yet so the horse is up and
walking in 20 minutes and it takes us a year and you think about the multiples of how many 20 minutes
go into a year for another mammal and you're like wow that's really different but then you look at how
different human culture now to 10 000 years ago and even one human culture to another is right so
now we do we started doing our social science after certain aspects of culture nurture had
become so ubiquitous that we took them for granted like alphabetic language is not natural it is not
natural to have a written language that is not a natural part of the evolutionary environment
that was an invention and then we taught it to everybody and we got and it makes it changes
the nature of mind very fundamentally because rather than relate to the base reality out there i
relate to the mapping of reality to these sounds that are arbitrary and then how those sounds work
so that affects the nature of mind and then i go to school and i learned this is geography and this
is english and this is math and this is history and it's all divided and there's different principles
and the goal is get the right answer not understand why that's not natural so we're ubiquitously
conditioning people and then calling that nature whereas if you go to the few indigenous cultures
that are left you'll see that most of what we call human nature in terms of the current
life experience of sapiens in the developed west is not true for them and so it's important to get
that our relation now do we have a culture that is systemically conditioning what would
most support wisdom so look at what are called wisdom cultures right look at the way that a child
is raised in say pre-mal buddhist Tibet and what they're being developed for and how
fucking amazingly different it is so are we genetically misfit as beings to wisdom no
did we produce a civilization that is misfit to wisdom and then humans are being made fit to that
civilization i.e the super organism i.e molok i.e the mega machine i.e the generative dynamics that
lead to a metacrisis yes and the humans who are born into that who become fit in that are fit for
the thing that is killing its own substrate i get that that is clear but we were selected for
historically at smaller scales and those systems optimized that had intelligence merged with wisdom
to protect the tribe or the smaller unit that dynamic doesn't doesn't map to my knowledge to
eight billion in a global consumption based culture right so the first question is
is it not possible for humans individual humans groups of humans at different scales to
develop wisdom i.e relationship to base reality rather than the symbols that mediated and the
models that mediated and relationship to the wider holes both temporally spatially and which
agents is it pause is there anything about human nature that makes that impossible no
is there anything about the current conditioning environment of the civilizational system not
the biosphere that makes that not what is being incented to be conditioned at scale absolutely
is it possible for the eight billion person thing to continue without the wisdom no it
will self-terminate so there is nothing innate to our biology that is the problem there is
something innate to the particular trajectory of the civilizational system that is and we do
not get to continue this civilizational system and what it conditions in people so then the question
is what does it take to create environments that could condition the wisdom and people that in
turn reinforce those environments right the bottom-up effects of the wiser humans creating
different societies and the top-down effects of different societies having an incentive to develop
different things in the people how do we get that collective individual top-down bottom-up
recursion moving in the right direction given that there is no other answer for humans long term
so last week on the phone you mentioned when we were discussing AI that capitalism
was an early form of compute can use that as a bridge maybe to get us to talking about AI
within a body all the cells in a body have the same genome but they're epigenetically
differentiated so that a red blood cell and the liver cell and a neuron are different and so there
is specialization division of labor that allows synergy across differences and capacities of the
whole system that none of the systems on their own have so the differentiation and then integration
is a thing even at the level of a body right and at the level of a tribe there was specialization
division of labor so that the tribe could do more than any individual person could do if
everyone was trying to do every everything when we got kind of an economic theory of comparative
advantage economic advantage the story of progress and economic advantage is very coupled to this
idea of specialization um division of labor and the increase in the total cumulative complexity
of the space that that facilitated of which capitalism was seen to be the dominant system for
doing so um now i want to say something before going any further which is it is very common that
if one says anything bad about capitalism the default reaction by many people from a historical
perspective is um this guy is a neo marxist and is going to suggest something that had Stalin kill
50 million people and Mao kill 50 million people and blah blah blah and doesn't he realize that
capitalism is the best solution in my in my earth day in my earth day talk i did last week it'll be
out tomorrow um i did word pairings and how the importance of semantics influences our our behaviors
and gives us permissions one of the word pairings was fossil fuels no the reality is fossil hydro
carbons but another was capitalism and communism are both industrial growth based systems so um
i'm i'm i'm with you there on on the exculpatory definitional clause keep going so
when we talk about problems and capitalism they were different expressions but many of those problems
in terms of environmental harm optimizing narrow goals whatever happened in communism happened
under feudalism happened under um various other types of systems that operated at scale
and we're not even going to say that at the very small scale that everything was wise and awesome
that would be a kind of romantic naivete that we don't want to do we don't want to do the naivete
on the other side that says it was hobsy and brutish short nasty and mean like neither the full
romantic tribal picture nor the full regressive they were dreadful animals picture are true it was
complex and different ones were different and whatever um but when we're critiquing capitalism
now it's not that we don't understand why it was selected for the things that it did that were more
effective or how gruesome some of the other systems were it's recognizing that this system
on the trajectory it is happens to be self-terminating so we have to come up with a new thing and the
new thing will be a new thing it won't be a previous thing so i'm saying that so that the
default reaction of why am i listening to these commies is not someone doesn't have to default
into that um but if we have a system even if it was the best and and we can critique democracy
in the same way church hills it's the worst of all governance systems save for all the other ones
when when they're it's the best thing but the best thing is still self-terminating
then we have to do new thinking right and um so capitalism and democracy should not themselves
be golden calves beyond critique but we are going to do the critique that in a way that is
historically informed a quick quick question on capitalism because i've never asked you this
i don't think i've asked anyone this there was never a person or a group of humans that said
hey let's invent capitalism it was always an emergent response to the challenges and the
innovation and and the coordination of intelligence towards problem solving of the day and it took on
momentum and then institutions and everything built on top of it right there wasn't a let's form this
there were definitely points in the process that were important and there were points that
were conscious in the process um what you actually define as capitalism right because
classical capitalism versus kind of neoclassical versus kinsy and are all quite different different
than some of the axioms so um do one could say capitalism started with the beginning of private
property that would be one way that one could talk about one could say it began with surplus
the moment there was surplus and we had to figure out who got to do what with the surplus and
and who has the choice making associated that that's the beginning one could say
no the communism was one answer capitalism was a different answer so it's surplus plus private
ownership and exchange or one could say actually it was only once the medium of exchange was
not itself in in intrinsic value so once we got currencies or you could say actually only
once you had financial services where the currency made made more of itself so the beginning of rent
seeking or loans or debt um or you could go forward to not until you actually have a formalized
banking system which you could say started with the templars or you could say started with dutch
capitalism and the kind of ship based mercantilism so there each of those are various steps in the
story of what we could call capitalism then of course central banking then international
central banking agreements global reserve currencies um blah blah blah right i mean these
are topics that i think for both of us are just intrinsically fascinating to understand how the
human condition evolved and they also happen to be maximally important um what's funny right you and
i have as a in common is we're we are not so motivated by within the existing dominant systems
what is useful to advance success within that system but recognizing that those systems have
limits that we're approaching what is useful within the system is inherently self-terminating
so what is useful long term is not based on what's useful in the system which means you have to go
into what do we even mean by useful what do we even mean by good how did we get here right
so when you're at the end of a particular kind of success paradigm if you recognize that you
have to actually go pretty deep in the historical stack the theoretical stack to even be pragmatically
focused islands of coherence have the ability to shift the entire system says ilia preogen
um so keep keep going on the why capitalism why you think capitalism was an early form of
compute and compute is is relevant to artificial intelligence so in one of our talks we discussed
molok which was kind of uh an analogy for um you're giving you're actually asking the question
now did anyone design capitalism or was it kind of emergent step by step and in so far as they're
as the most significant features of our world are not the result of anyone's intentional choice
it's like who the fuck made that and so being able to anthropomorphize as a thought experiment
what the collective behaviors predispose you can get different kinds of god so adam smith
did this with the invisible hand of the market right that um the progress was defined by this god
of the invisible hand that nobody was choosing the overall topology of the system they were all
doing local point-based choices of i want this product or service at this price i would like
to make a business that provides this product or service and yet the totality of the collective
intelligence that was an emergent property of that moved things in a particular direction
he was defining the invisible hand as a good god right um roughly uh but but he did say
there was an end to that uh benevolence uh and the distant horizon once once we would
exhaust the surplus but keep going so if humans have desire demand for things that
actually improve our lives and we only want things that improve our lives and we're
rational actors who will make a rational choice to utilize the resources that we have intelligently
for the things that improve our lives the most and that creates an incentive niche for people to
innovate how to make goods and services that improves people's lives more at better value
then yes you would get a good god emerging from that decentralized collective intelligence
right and that's kind of the idea of the market and capital mediating that and having this currency
that is not worth anything itself but represents optionality for all forms of value
and so so was the market an early form of artificial intelligence the global market just
in time delivery everything optimized for profit sufficiency etc so if we define intelligence
in the pragmatic definition of goal achieving if we define general intelligence narrow intelligence
is can achieve a specific kind of goal an artificial intelligence can win at chess
is a narrowly trained system on one thing can win it go is another thing can translate languages
is another thing can do image recognition is another thing can optimize supply chains is
another thing narrow intelligence general intelligence is increasingly generals can
do more of those things fully general is can in general figure out how to optimize an eagle
is the market so humans have general intelligence yes and one definition of artificial intelligence
is trying to make things that behave in ways that we would see as intelligent that so it's just
taking the term intelligence kind of intuitively or heuristically and if it behaves in a way that
we kind of would assess that is intelligent that's one definition another definition is
that intentionally aim to model human capacities human intelligent capacities there are other
definitions that are more interesting than those but when you ask this question about so
are humans generally intelligent yes I think we actually have to back up and define what
artificial general intelligence versus narrow artificial intelligence is to explain why artificial
general intelligence is scary to then be able to come back and describe this market thing for a
moment please do so if people probably most of the people here have heard conversation about
are we nearing artificial general intelligence maybe we are nearing it in a very short period of
time and maybe that is really catastrophic for everything and why and probably some people
have a clear sense of why they've maybe heard le izer yudkowski or other people like that talk
about the nick boss drum talk about the cases regarding artificial general intelligence that
are the most catastrophic types of cases that one can imagine meaning much worse than nuclear bombs
but for anyone who's not familiar I'll just try to do very briefly if I have a narrow intelligence
like the ability to play chess first if people don't understand how much better at
chess the best AI systems are than the human systems go study that for a minute and get a
sense look at how the AI that beat Kasparov at chess IBM system then evolved into stockfish is
the best system that had been programmed with every human game that was so much better than humans
it wasn't even calibratable and then how a totally new approach to AI that google innovated the kind
of alpha go system was able to beat that system rough there were some ties but roughly 38 zero
without having programmed any human games just letting two of them play themselves a trillion
times in three hours and get so fucking good based on no human system just the rules themselves
that they could beat the previous god as if it was nothing and it took three hours of training
and you're like whoa we aren't even in the running of relevant we don't even have a reference for
how not relevant we are at being able to do anything with that narrow goal right now it happens
that that narrow system that was able to train that way and beat us on chess also was on go
also did on starcraft also like you just line them up and you're like oh whoa okay so this
is a sense of the power of artificial intelligence meaning intelligence like we have achieved a goal
goal here is when it chess when it go when it's starcraft does that are all those military
strategy games yes those are all military strategy games could you apply those to real
world military and economic environments can you define real world environments like games train
ai's on them to be able to win would they dominate that excessively that is the whole direction of
things right does a real quick real quick question do so when when people are applying
ai to war and economic problems and innovation or whatever it's a series of individual
narrow artificial intelligence they're not applying an artificial general intelligence
there is no artificial general intelligence as far as we know today we are it is the biggest
goal in the space we're rapidly moving forward towards it i'm going to make an argument that
there is something like it but not in the obvious sense but the computational systems the purely
computational artificial intelligence systems are narrow but they're increasingly wide meaning
that the same system that can be trained on one narrow goal can be relatively easily trained on
other goals which means that the you don't have to start from total scratch right so that is
increasing generality but not full generality um and but if i want to give a real world example
if you make drones let's talk about the copter type drones that use swarming algorithms
where they they fly in kind of a swarm pattern and so they can go through obstacles and some of
them can be taken out and the other ones will reconfigure um the ai that regulates their flight
is so radically advanced beyond what a human controller could do right like a human controller
trying to control that fleet of drones not even in the ballpark just like they aren't in chess
so when you start to think about autonomous weapons and what would that mean
you can begin to get a sense of it and there are already examples of this that you know there's
a lot of things that are run by forms of artificial intelligence cybernetic systems so ai high
speed trading has already meant that a normal human engaging with a maximum amount of knowledge
without those tools cannot play in that space ai optimized high speed trading can only be
competed by computation by competitive ai systems and so then can i ask another naive question
yes so when you say that let's just use chess as an example even though military finance
applications are probably more dangerous and and relevant to our futures but taking chess you said
you have to train the system and the training took three hours so so what does that really mean
there's some business owners or coders that say okay we've got these algorithms this artificial
intelligence and now we want to apply it to be really good at chess they write a little code
and they give the ai the objective and then they just press go and and and three hours later they've
got a model that they can apply in a real game so this is the techniques of artificial intelligence
the techniques of learning those are all evolving and there are to understand something about the
evolution rate there are many different exponential curves that are intersecting in general and you
probably address this on your show in the evolutionary environment in which we evolved
yes we have abstract intelligence but we don't have intuitions for ongoing exponential curves
because they never happened in our environments right something would start as an exponential
curve and then it would turn into a logistic curve into an s curve and so the rates of
exponentials are non-intuitive people are used to think about oh it's getting faster we have time
to do something not like it 10x and it 10x again then it 10x again and the speed in which it is
doing it is dropping by 10x right the time period um so we intuitively get this thing wrong that's
for a single exponential so if I take ai the hardware right how we are not just having cpu's
but gpu's and tpu's and different kinds of arrangements of those and network cards and
whatever the and and how many transistors we can get in a small area there are exponential curves
in the hardware in terms of the increase in progress morse law and other stuff there are
exponential curves and how much data is being created through sensor networks and through
social media being able to aggregate all this human data and stuff there are exponential curves
in the not in the total amount of capital going in the total amount of human intelligence going
into the space the innovation in the models themselves in the hardware capacities for actuators
in the types of things that can be sensed and so you have intersections of many different
exponential curves so what we were meant by how an ai learns at the time of stockfish
to the time of alpha go already was different it's a completely different answer to them
and so where stockfish you are programming in all of bobby fishers games and all of spatsky's
games and giving it opening moves and putting in all of the books of chess and all like that
as a type of learning that then has a lot of human feedback on is it getting it right
this other technique all that was programmed is what are the definitions of a win and what
are the allowed moves and then you have two versions of the system play themselves with
some memory and learning features built in where they know what they're seeking to optimize for
is how to win no human games built in no theory of opening moves but because of the speed they
can really play a trillion games in a small number of hours then they have opening moves that have
never been seen that are not aligned with any theory totally different approaches humans have
never done because they can take very different branches and so they have the equivalent of
what would have been millions of years of human chess playing or maybe no amount of human chess
playing because human memory could never hold that much human combinatorics couldn't but in
three hours it can be trained to do that compared to how long does it take to get a human to just
even catch up with what has already been known in chess so so who owns the ai's some ai's
are built by corporations mostly public corporations some are built by branches of military or
governments so national laboratories make some very powerful ai's and obviously the ai's that
are in public deployment as large language models right now are microsoft and google and
anthropic and then increasingly you know other companies that have to follow on meta and bydo
and like that but tesla has very powerful ai's very very powerful ai's that are trying to get
full-blown self-driving down that take information from sensors and process it through artificial
intelligence to control actuators and how did it happen that all these are companies that don't
really talk to each other like microsoft doesn't really talk to google at a competitive sense so
there's just a bunch of engineers that around the world are are simultaneously developing these ai's
in competition with each other and they're all figuring out the same codes and etc but some
are doing a little bit faster and better i mean ai is not that different in this sense than
other categories of emerging technology where you have different companies competing where
anybody can see once someone deploys what they did reverse engineer it and try to figure it out
they can all have they can all try to spend enough money to hire key talent away from the other
company there are academic groups that are publishing stuff that is also at the cutting
edge of knowledge and then the companies take that knowledge and do stuff with it they can
all do corporate espionage on each other so so there isn't one ai there are dozens hundreds
thousands of ai's depending what they're applied on and depending what vector of society they're
developed in and different fundamental approaches to ai what do you mean oh there are ai's that are
trying to take visual sensor data to control motor actuator data to move a self-driving car
which is different where it's sensing and actuating a movement of a vehicle through space
there are other ones that are trying to read content online read read language
your chatbot and so they're sensing in language and they're actuating in language
so they're optimized for language input output so those are very different kinds
but the so the goal of ai in these examples is not profit maximization the goal of the people
that control the ai is profit maximization the goal of the ai is whatever people code its goal
to be its objective function or its loss function the inverse of the loss function but it's
but the people are developing it in service to whatever the goals the organization they're part
of is seeking so if it is a military group it might be the goal of national security if it is
a corporation especially a public corporation it has to be profit maximization through some
specific domains of action if it's an academic group it might be advancing the knowledge of
that field and in you know whatever ways that is so it is an important thing that you're asking
which is if the group that is developing an ai has certain goals the ai will be developed
and service to those goals and so are those groups wise can you if the ai is super intelligent
and we're already starting to get at that intelligence has to be bound by wisdom
is the goal of the group a wise set of goals or is the goal of the group a narrow set of goals in
which case it'll be building the ai for the optimization of that is actually a really key question
but here's a subset of that question if a few of the groups choose wisdom as their goal
or if more most of them do they'll still be outcompeted by those ai's that have the narrow
focus and and win no so the question of are multi-polar traps obligate or is there a way
to get out of them agreement is a way out of a multi-polar trap the agreements are hard because
if everybody can realize that their likelihood of winning the race is low the race as a whole
might mess everything up for everybody and not doing the thing is better was mutually
war games with Matthew Broderick yeah mutually assured destruction was a way of saying
don't think that you can win first strike advantage let's be be clear that winning
equals losing at this level of power there are things built in place where winning equals losing
so we have an agreement to not try to do the win because anyone trying to do the win would be so
catastrophically bad for everybody and the challenge with these agreements is you can enforce
such a thing within a nation right within a nation where you have a monopoly of violence
backing up rule of law you can make sure nobody cuts the national force forests down
even if there's a lot of motivation to do it because you can make a law that says you're
not going to end the monopoly of violence is stronger than whatever the capacity to login
companies are going to bring to bear but if you don't have monopoly of violence and rule of law
how do you enact it so internationally and this is why we face so many global catastrophic risks
whether it's the destruction of the oceans or the destruction of the atmosphere or biodiversity
or developing synthetic bio or AI is if anyone else is we lose in the short term by not doing
it should they deploy it and yet if everybody does most likely everybody dies so how do we
bind those the tricky thing is how do we know the other guys keeping the agreement and that
they're not defecting and secretly doing it so with regard to AI can you get there are and
so we see that open AI was originally created for safety purposes and concern about how far ahead
deep mind was and everybody else and the need to be able to have an open source approach so there
was not only one centrally concentrated power and through whatever series of things happened
the thing that was originally a non-profit dedicated to safety is a public corporation
for profit wholly owned subsidiary of microsoft deploying in competitive types of races against
other ones and then anthropics that broke off from there to do safety that took 300 million
from google ends up being in similar dynamics and so can't without some kind of shared agreement
that nobody does a particular thing then yes you end up having an incentive gradient for
everyone to race to get there first even ethically where they hold me getting there first i'm the
ethical guy so i'll win that race and then i'll use that power for good um and that food yeah that
conversation is very active billionaire it's like a billionaire being aware of the great
simplification and still trying to maximize their optionality um to have more money so
that they can do more philanthropic good in the in the future it is amazing how powerful motivated
reasoning is unbiased let me ask more naive questions daniel we're getting we're getting
to the heart of it here could you program ai for wisdom this is the agi alignment question
and there's different ways of kind of getting to this so let me construct the agi concern versus
the narrow ai concern for a moment so we were on the track of what is narrow ai it is optimized for
a certain set of goals and it can get extremely good at it it can get better than humans by
not just a little bit but in many domains by so much that it's we have a hard time even
understanding it and and that's so much as we speak is accelerating by the month yes
and okay so without fully general intelligence fully autonomous systems then the ai is in service
of humans using the ai for purposes the humans are the agents the ai is an enhancement of our
goal achieving capability right it's just a tool i would argue that even the word tool makes us think
badly um because um language as a in this way i was saying technology meaning goal achieving is
such a broad set of things that the types of tools that an end user uses versus the types of tools
that make other tools machine tools versus the types of tools that innovate how to make lots of
other tools like computation yes they're all tools but they're really fucking different in kind and
they have um we wrote this paper called technologies not values neutral that should link here which
discusses why it's not just that we have our values and we use our tools it's the tools it give the
capacity to do certain things better to achieve certain goals better and in so far as they do the
humans that use them win at things and as a result everybody has to use them or they kind of lose
so one the tools become obligate two the tools code different patterns of human behavior now
i'm doing the behavior where i'm using that tool as opposed to doing some other thing in coding that
pattern of human behavior changes the nature of human mind and societies at large so it is not
true that you've got values and then tools are just neutral the use of the tools changes the human
mind individually and aggregate and then becomes obligate so um psychology affects the tools we
create but the tools in turn affect the psychologies this is and this is why i was saying our nature
is to be malleable to the environment because we change the environment so much but if we don't
consciously know how to direct that it will get directed by a kind of downhill gradient that ends
up in an evolutionary cul-de-sac so with that little aside on that tool or technology is actually
a very deep concept yes a narrow AI that is not an agent that a human is using is a is a tool but
it's a tool of a very special kind where it is it can uh take lots of steps on its own to help
achieve a goal rather than me just use it for fully specified purposes right so this is a kind of
important distinction now if i look at AI benefit first so as to not seem overly negative for all
goals humans have that would create some progress or benefit to some real thing can we use AI in
service of those yes not not all things equally today but lots of things is it all the scientific
progress we have on solving new diseases and stuff like that can we use AI to accelerate it
and accelerate movements and science and discovery medicine um so might AI be able to speed up the
rate by which we come up with a way better nuclear energy maybe fusion maybe deep geothermal yes
might it be able to solve many types of cancers yes if i have a daughter that
is dying of cancer do i want to hear anyone slowing down the systems that if they get
there fast enough might save her life i don't want to hear that that sounds like the most cruel
evil fucking thing whatever other problem it it's going to bring about i'll deal with that problem
later um is there someone listening to this that that cares mostly about the environment
and climate change is there a a progress case to be made that AI will help carbon emissions and
and reduce uh environmental impact you can i use AI to model how to do geoengineering more
precisely can i use AI to do better genetic engineering on crops to maximize their carbon
sequestration if i'm a carbon fundamentalist can i use um AI to advance stem cell meats can i use
AI to advance energy technology or battery storage technology or any number of things
like that can i use AI to affect supply chains to lower energy on supply chains and decentralize
a lot of things where you don't have as wide a supply chain yeah can you say on all those things
okay got it and so for any goal we have if i can use that to enhance the goal i don't want to
hear anybody talking about slowing that down yep i agree this is the same as i understand
same as capitalism right if i can use if i if i if i if me having access to more capital speeds
up my ability to achieve my goal and i believe in my goal then i don't want to hear anything about
taxing that or decreasing that or fucking up my capacity to achieve those goals yeah and yeah
um so are there a lot of things that human intelligence can do that are good
that increasing that type of human intelligence through artificial systems that can operate
on more data and faster could also do yes now why would we be so concerned about it then
so let's talk about a few different cases one type of AI risk is AI employed by bad actors
now nobody thinks they're a bad actor for the most part right like there's some exceptions
but most people that someone else calls bad actors right they're they're terrorists but to them
they're freedom fighters was it a protest or was it a riot blah blah blah the lake off frame on those
things very much is it progress or is it destroying all these areas for the people that are being
destroyed by it who who want to do some destruction back that they consider tiny in the name of
self-protection it is a protest and a freedom fight otherwise it might be a terror terror act
or a riot right um but let's just pretend that we weren't thinking about all that and just call it
bad actors for a moment criminal activity um can people whether it is a dude that would have just
shot up a bunch of stuff with an ar-15 because that was a technology he could use to achieve
harmful goals and obviously this is why people are concerned about assault weapons is because
they couldn't kill as many people as easily with a knife right um or or a single loader
pellet gun when the second man was created well they can do way more harm as drones become
more widely available that you can hook explosives to and so it happens to be that the bad actors
who want to do fucked up stuff oftentimes are in states of mind where they're not amazing
technologists and can't coordinate lots of people and do strategy and technology this is not
always true but sometimes it has been true we've been saved by this but as we make the destructive
capacity now it might have constructive capacity i can use a drone of course for a lot of positive
things to be able to monitor construction and railways for safety and to plant trees from
the sky and whatever but can we also use it to fly an explosive over some critical infrastructure
so um when we make the thing for the positive purpose we also enable all the things that it
can do for any purpose when we make that easy enough and in that case probably 80 plus of
those bad things we can't even imagine at the beginning that's a complex topic i want to get to
is okay how you how you do externality forecasting but um because we can do a much better job than
we ever have done and saying that we couldn't is a source of plausible deniability for not trying
well what i meant was there's unknown unknowns uh to use donald brownsfeld's term with ai yeah
but if we're okay i have to do this tangent then because it is fucking critical um okay
there are problems that well we're already in line for this to be the longest podcast
i've ever done so let's uh let's do it right keep going because there are problems that
are second third and third are effects that were not easy to anticipate can we use a technology
for a certain purpose that creates unanticipated and maybe even unanticipatable consequences
yes can you prove that you can anticipate in advance everything no it because there will be
some things that are unknown unknowns that you can't have proven that you thought through
in the safety analysis ahead of time so that's a true thing but because that's a true thing
people use that as a source of bullshit plausible deniability to say i couldn't have possibly known
where then they don't even really try to forecast the things because they will privatize the gains
and socialize the losses and this is a very very important thing to understand also related to the
progress narrative and the underlying optimist versus pessimist or i would state opportunity
versus risk orientation the people who focus more on the opportunity of a new technology this is
going to do all these amazing things blah blah blah they're going to move faster focused on that
then the people who are focused on the risk and really want to do good thorough safety analysis
and make sure it won't cause any of those risks that takes a lot of money and a lot of time and
doesn't rush you to market as fast as possible so the first mover advantage is going to happen
by the guys who take risk less seriously focus on the opportunity more they'll be able to get the
network dynamics that are very very powerful from associated with first mover advantage
and early scale they'll be able to um regulate in their interest because they state that the
risks are not that bad and they do lobbying efforts with the money they got from early revenue or
investment or etc the people who take the risk seriously do the analysis and say oh fuck there's
actually no way to advance this that is good right now we just you just not do it well those people
just get to sit on their knowledge of the problem and not do anything but also not gain any power
to affect the system because they're not going to generate money by which they can do lobbying and
public opinion effect and etc or they say there is a safe way to do it but we just spend all of our
money figuring out safety not not doing optimization so there is a perverse incentive in general
for those who are more focused on opportunity than risk and as a result we get all the opportunities
and all the risks and the risks happen the cumulative effect with with with capitalism
and the market in the last 50 years it is a market function not 50 years forever
right okay yeah um so
but the risks are getting larger as the technology gets larger and the cumulative effects are
increasing so now we're at a place where the cumulative effects of industrial tech on the
environment are reaching critical tipping points of planetary boundaries and the exponential tech
is getting to a place where its capacity the destructive capacities are so fucking huge right
like the atomic bomb was the first thing that could destroy everything quickly and until then
for all of human history we couldn't do any quick thing that would destroy everything
now the atomic bomb is not the only thing like that synthetic bio where you can make
an artificial life form that doesn't have a few natural genetic mutations but so many that it
could be in it could be an invasive species everywhere right or artificial intelligence
these are examples of things like where their destructive effects can
are are actually exponentially more than any previous kinds of thing but if you combine things
like a public corporation having a fiduciary responsibility for share for profit maximization
which you can argue why that made sense you can argue why the shareholders are giving you their
money to work with those shareholders come from pension funds you need to give that back to them
they can only trust you to have it if you have this bound fiduciary responsibility to return
their funds appropriately and you're innovating a good or service that people want that is not good
for the world blah blah blah of course as you can see where the logic of the market became less and
less true as rather than rational actors we figured out how to nudge everybody into less and less
rational action and the supply side figured out how to manufacture demand for things that don't
increase the quality of people's lives and didn't account for the cost in the environment you still
have that story so you've got the must maximize profit and then you have the nobody would want
to innovate and the innovation is good for everyone if the bad things that happen they were
personally liable for so we'll make liability limiting properties where the corporation will get
fine not any of the people or directors involved so then doing things that destroy everything
just becomes a cost of doing business no real deterrent for it so you the corporation will
privatize the gain socialize the losses the actual people who make the decisions have a
bunch of upside and not relative downside you put all those things together and you say ai is being
developed in those environments and so it has to do profit maximization it does not have the people
who are making the decision have the liability associated with the scale of risk and harm that
could occur those who are more opportunity focused build the corporations that become
worth tens of million tens of billions of dollars and have all that power to also influence lobbying
and influence public opinion and those who do the safety analysis run tiny nonprofits that nobody
listens to comparatively so it's important to get that this asymmetry but that perversely
orients those who think about opportunity more than risk as risk is moving into an exponential
scale and then rationalize that that is itself one of the underlying drivers of the metacrisis
and embedded in that and i understand that embedded in that is the natural human scale
ethical feedback loop when people are innovating and they go and work in a room for months and
they're working on something they kind of get a little bit of recognition and of and an insight
into what they're doing here you code something and you press a button and all those negative
potential externalities are just in the future and you get no emotional like sense of what's going
on is that also true yeah the fact that it happens outside that happens at a scale like many of the
harms will occur via supply chain somewhere in the world i won't see and as we already said there'll
be second and third and fourth order effect so let's say for instance now this is this is a very
important point um we can say we can say that all technology is dual use dual use being a military
term meaning and there's two different ways to think about it a technology is developed for a
military purpose so its primary use was military it also might have civilian or non-military
applications right we're developing the um rocketry capability to make missiles hit their
targets but maybe we'll also be able to use it to put satellites and outer space for communications
for everybody um and obviously computation was developed in world war two to crack the enigma
machine and whatever for military purposes and it has had a lot of other purposes on the other side
we say anything that is developed for a non-military purpose probably also has a military application
right so dual use goes both ways the fact that if i'm developing something for a non-military purpose
it still probably has a military application i.e. you can say on the positive side defense but
on the opposite side the the offense or killing capability means that that has to be factored
in the development of the technology but dumb question though if someone at microsoft is
creating an ai for some purpose they're they're not sharing that with the us government or any
government right there's gotta be it's gotta be independently developed by coders and engineers
within the government within the military no no no um no if a technology is seen as having
risks to national security then there will end up being government bodies that have oversight
into certain capacities and got it um but that's one way the other way would just be the corporation
develops the capacity and one of its clients becomes military and becomes a military contractor
in addition to other things um one would be it doesn't even say microsoft doesn't develop a
military side application it develops lm's and then palantir develops competing lm's and makes a
military application of them right so the technology itself will get developed lots of places will get
reverse engineered use for lots of purposes um now so all technologies dual use now on the side of
we're developing something for a civilian purpose that we say is positive but it also has military
okay we have to think about that but the other side is also risky because if you're developing a
military technology that is very powerful and very dangerous but at least you think you can control it
the moment of that same capacity also gets a civilian purpose means it will proliferate and
it makes it much harder to control like let's say you developed drones for military purposes
now it gets a commercial application which is i can just fucking film stuff with it
now everybody can get access to drones wow that just really affect the capacity for decentralized
terrorism so okay tell me where you want to go i'm about to go to the next part beyond dual use
and and you can i'm just gonna comment that i get this sinking feeling that we're headed
towards a risk singularity yes but the the metacrisis is a risk singularity in which the
underlying drivers over determine failure meaning if we could prevent the ai apocalypse that doesn't
prevent the synth bio one or the planetary boundary one or the you know gazillions of
other ones if we could stop planetary boundaries regarding fishing that doesn't affect what we're
doing to soil or nitrogen runoff or or PFOS pollution or whatever the underlying thing
is creating so many different sources that can lead to catastrophic risk that if you don't deal
with the underlying thing and you just deal with some of the risks you only buy a tiny bit of time
i have thought that the underlying thing was the emergent phenomenon that i call the super
organism with which is the growth compulsion of our global market system
uh yeah what you're saying what you're saying is that we had agricultural surplus
then we found flammable fossils then we accelerated our technology then we went to debt
then we had the internet and each one of these kind of exponentially increased what had come
before and you're saying that ai is the next quote unquote tool in service of this growth-based
super organism yeah so you say growth let me which is that's a fair way to say it but let me
define it slightly differently because i'm going to define it in a way that is more aligned with
this ai conversation so i think it actually gives a lot of insight into the other
narrow goal achieving is the underlying generative dynamic okay i get it
and growth is an epiphenomena of that
an wait an epiphenomena and underneath a result of okay so the second order effect
first thing is the narrow boundary goal and growth is the second order effect of that narrow
boundary goal yes so if i want to achieve a goal and having a little more surplus gives me more
optionality to do that but me having that more optionality when i'm rival risk that increased
security i have inherently decreases somebody else's security or their relative competitive
advantage in a status game for mating or whatever it is so now they have to do that then i see them
doing it so now i need to do more than i needed to do now we're in a race on it growth is the epiphenomena
what everyone is pursuing is not the growth of the whole system they're pursuing their own
narrow goals and generalized optionality for goal achieving okay
and the growth in total consumption of energy and atoms the growth of total waste and entropy
the growth of intelligence and new types of technology the growth in memetics and the ability
to convince a lot of people of it are all epiphenomena of both goal achieving and specific
and then the general capacity of increasing optionality for goal achieving could we ask ai
how to go from a narrow to a wide boundary goal so all technology has certain affordances and
all technology has combinatorial potential with other technologies and the affordances of them
together are different than them on their own and obviously not only does the hammer have
different potentials if i also include nails and saws and you know the other things and it would
on its own but i can't even get a hammer without the smithing tools that would be necessary to
make a hammer right so there's a whole technological ecosystem i'm just i mean i'm cutting to the
chase on this i'm just wondering what ai does to our certain complexity there's a reason i'm
having to try to do this all tools have combinatorial potential with other tools they all have
certain affordances ai is unique computation has been unique and then ai even within the
space of computation and it is more combinatorial with every other type of tech than any other tech
is and it has more total affordances than any other tech has what's an affordance again uh it
things that it makes possible okay um so for instance if i have a nuclear weapon it makes
certain things possible that are not possible without it it does not intrinsically make me
better at bioengineering it does not intrinsically make me make better drones computation can be
applied to make better drones better bioengineering better nuclear weapons better propaganda better
marketing better financial systems better better chips to make better ai systems so the ai because
all the other tools are made by the kind of human intelligence that makes tools and ai is that kind
of human intelligence externalized as a tool itself it has a capacity to be omnimodal right not dual
use omnie use more than anything else is and omnicombinatorial i can the risk of ai with synthetic
bio so the dual use thing here is i'd say oh i can use ai and kind of protein folding stuff to be
able to solve cancers but once i develop it for that purpose i can also make better bio weapons
with it and now i've made that ability cheap and easy um or i can use it on a chemical database to
do drug discovery but i can also make chemical weapons i can use it to optimize supply chains but
i can also use it to do optimized terrorism on breaking supply chains and so the ai has the capacity
to increase every agents capacity to do every other motive in any context with any other combinatoric
technology in a way that nothing else has and that's really important in understanding because
we'll develop it for a particular positive purpose for a particular set of agents but in
doing so lower the barrier of entry of all other agents for all other purposes being able to use
that capacity so from my biophysical frame which is is not where you're coming from but i'll just
interject this i think a lot of people who don't fully understand ai look at the invisible but
real biophysical body that ai has which requires energy i read in the last few days that ai requires
a lot of water for cooling and things like that but the larger environmental impact of resources
co2 planetary boundaries is not what's powering the ai what's needed to power the ai it's the
resulting acceleration of all the other consumption and things that were already going on that are
just being exponentially increased by the ai yes so let's look at ai driving jevons paradoxes in
your world so let's say there's a bunch of areas where the mining is not quite profitable right that
the um cost of extraction i can't on the current market sell the thing for enough money so we don't
mind that area now the ai figures out efficiency such that a whole bunch of areas that weren't
quite profitable are now profitable awesome for the economy new economic growth for everybody
and also more rapid growth of the superorganism and more environmental externalities for everyone
under the incurrent current motivational landscape and incentive landscape and down to both the market
incentives and even the legal landscape because the market incentives won't create a real
liability deterrent because of liability limiting things and they actually require you to do profit
maximization and things like that right so how is there not a massive discussion in the climate
space about ai right now as a as a generator to bring us to 500 ppm in a decade i don't know maybe
this conversation will help with that um i think you're very worried about that you're worried about
the planetary boundaries impact of of ai either you have the market still running things and ai
working within the market in which case yes the market incentives have cost externality as
fundamental to them if you had to pay the real cost of things the market would collapse as we
understand it and so the ai causing more efficiencies will drive jevons paradoxes because
you will continue to seek returns and you will drive planetary boundaries faster the ai person
might argue no this is not true because the ai allows us a completely new economic system whereby
we can actually track all those externalities and do very complex optimization that means the
market doesn't run the world anymore that means a centralized ai system runs the world um that is
its own dystopia because either you have separate competing groups that don't have totalizing power
in which case they're caught in multipolar traps or you have a single group that can bind all of
those but now there is a central point of failure capture and corruption and no checks and balances
on the power this way we talk about one bad future being catastrophes the other being dystopias because
to control for all the catastrophes orance towards very centralized power which gives dystopias so
what we need is a third attractor that is neither of those which means it is neither a central ai
coordination system nor is it ai in service of markets which means it is some new thing that
like markets as a collective intelligence system are also a collective intelligence system but
it doesn't have the perversion and externalities of markets so a wiser collective intelligence
system wiser than democracy or capitalism that yes it will employ ai and computational
capabilities to mediate just in the same way that democracy required employing the printing press
and so new physical technologies and enable and just like computation has enabled all of
modern banking new technologies make new social systems possible but the goal here is not an ai
overlord that runs everything it is how do computation and intelligence capacities make
new collective intelligence possible such that you can have the global coordination
to prevent global catastrophic failures but where that system has checks and balances in it so you
don't have centralized power coordination and corruption failures so it seems to me that
that if we weren't in this metacrisis and wily coyote moment without with all the leverage in
the system and the geopolitical fragility and everything that we could have time to have some
wisdom embedded in in our in our information systems and our governance but we're at this
late stage where the snowball is getting bigger and bigger and how do we inject wisdom into this
situation with with all these things going in the opposite direction obviously you must have
some idea or a plan or you wouldn't have dropped all this really scary stuff on me and and the
listeners or or are you just calling this out as a risk that we need to have massive urgent
discussions on i think there are people who are ai experts which i'm not i have a focus on
how to think about the relationships between all of the kind of ideas of progress and risks that
gives an insight into ai but when you look at whether it's um whether it's henton or russell
or any of the famous kind of ai pioneers and the risks that they have called forth or the call to
pause large language model deployment or the more the most serious case like elias or yudkowsky's
conversations if people haven't seen the first podcast he did that kind of started this um space
taking these things more seriously recently on bankless it was so fascinating to watch because
you could tell the podcasters didn't really know what he was going to say and thought um he was
going to talk about ai and crypto and um and he came on and uh you know he he founded machine
intelligence research institute and spend his life focused on the topic of the risks of artificial
general intelligence that he thinks we are both very close to but not close to aligning and um
to understand that which we haven't discussed yet because we keep not quite getting to the whole
construction you have narrow intelligence which humans can use for fucked up purposes and markets
can use for fucked up purposes but then a general intelligence becomes kind of its own thing it can
it can figure it becomes its own agent rather than a tool of us as the agents can make its own
goals and can increase its speed of learning meaning and competition relative to us so much
faster than us that if its goals don't align with ours we're fucked
so using the chess example okay i was just going to say using the chess example of the
three hours and trillions of iterations the ai could use itself it could train itself without
humans giving it the orders yes but across broader things so it could come to a new domain
where it wasn't already trained what to do and figure out what are my goals in the space and
or for whatever my big picture goal is what would the goals on the path the instrumental
goals be regarding this whole space and so kind of in the way that you say no matter what your
goal is it's probably going to require energy so get more energy ends up becoming a goal in and
of itself because it increases optionality to fulfill other goals get more capital becomes a
goal in and of itself it's called an instrumental goal the ai can start figuring out well i can
gain all the optionality in this space by such and such i'll mine all the things i'll etc etc
because i'll be able to use them for goals in some future time that i have so if we talk about a
general autonomous intelligence of that kind that could beat us at any games can redefine
anything as a game that can win it's very easy for us to realize it would be really bad to make
that thing before being sure that its goals don't mess up everything for us because our goals
really sucked for all the species we extinct in our goals really suck for all the animals
and factory farms and for all the um you know cultures that are destroyed so if there's something
that is as much smarter than us at goal achieving than us and similarly narrow goal achieving
then we might be like animals on the factory farms or extinct animals very soon
or the cultures with wisdom that were outcompeted
so it's very so leisor came on this podcast having you know uh kind of world leading expertise
in this topic and just spoke honestly to these podcasters who they did a good job of staying
with him but we're totally not prepared and he's like yeah we're on our way to this thing and we're
all gonna die and then they asked him so uh what do we do about it he's like i don't know i've been
working on this for decades and everything that i thought would work doesn't work and yet the
market is driving everybody ahead and i have no idea and then they're sitting there how is
wait let me give you this last part you should you should really watch it
uh i will then they're like what do you want the listeners to take away and he's like i don't know
i just feel like at the end we should at least be honest with each other
it's a very poignant moment right it's a very poignant moment i'm not doing that right now
i actually do think that there is a way forward but i think what he said shows what someone who
is very bright who spent their whole life looking at this like it's very important to take that as
a data point but you were asking this question okay what do i want listeners to take away so i was
giving as a reference arguably what many of the top experts in the world come to with this
so i will offer what might be a way forward but it does require taking seriously how deep a thing
we're talking about i can understand how narrow ai's are pursued by corporations and by governments
to achieve task and achieve profits but why would a corporation pursue artificial general
intelligence if profit was their objective wouldn't that also run the risk of killing
everyone in these other extreme scenarios that then profits aren't worth anything if the whole
system blows up into a giant superorganism you could ask why would any country pursue having
nukes when the more nukes that exist increases the probability that everyone including them dies
with nukes and yet you can see from it's an arms race yes it's partly an arms race and it's partly
a set of these biases those who are more focused on the opportunity than risk end up being the ones
who rush ahead and so the people who don't think agi will kill everything try to build agi because
it will solve everything wow imagine intelligence like ours but so much more it'll solve all of
science it'll give us radical life extension it'll create wealth and abundance for everyone
and we will get ushered into the promised land so we've got the naive progress people
shepherding the ai development train the naive progress combined with the motivated reasoning
on capital and winning and ego and all of the various sources motivated reasoning
combined with once they start actually being legally bound for things like profit maximizing
and that the liability is inherently externalized while the profit is centralized then combined
with things like the country doesn't even want to regulate them because it it wants the economic
growth so that it's not fuck because it uses that economic growth to grow militaries and
geopolitical alliances and so you then you get a yeah but if we regulate then china won't then
we lose so you get layers and layers of multipolar traps driving the need to continue to optimize
for near-term narrow interests this is not where i thought the conversation would go really i'm
more depressed than i thought i would be i i kind of in you and i have talked about this so i i'm
kind of aware of some of these risks and their risks that we haven't talked about and there's
information hazard with some of these risks but this sounds highly plausible to me and i think
even before agi would be reached there are plenty of other well just just the using ai as a vector
to accelerate all of the things that itself is is enough to put us into tipping points
okay so here's and i know we're late and the sun has been setting in the beautiful mountain
background behind me so we need to wrap soon so i want to try to bring back a few threads
human intelligence
unbound by wisdom it is fair to say is the cause of the metacrisis and the growth imperative of
the super organism or the capacity that gives rise to it that intelligence has created all the
technologies the industrial tech the agricultural tech the digital tech the nuclear weapons the
energy harvesting the all of it that intelligence has created all those things it has made the system
of capitalism it made the system of communism all of all of those things right and um now
that system of intelligence that takes corporeal capacities things that a body could do in
externalism the way that a fist can get extended through a hammer or a grip can get extended
through a plier or an eye can get extended through a microscope or a telescope um or our own
metabolism can get extended through an internal combustion engine or there are musculature or
whatever it is right so it takes the corporeal capacity and extends the fuck out of it extra
corporeally so that type of intelligence that does that is now having the extra corporeal
technology be that type of intelligence itself in maximized recursion not bound by wisdom
driven by international multipolar military traps and markets and narrow short-term goals
at the expense of long-term wide values so where in the metacrisis there are many risks
synthetic biology can make bad pandemics and extreme weather events can drive human migration
in local wars and this kind of weapon can do this and this kind of mining can cause this
pollution and this kind of pesticide can kill these animals those are all risks within the
metacrisis ai is not a risk within the metacrisis it is an accelerant to all of them as used by the
choice-making architectures that are currently driving the metacrisis if we weren't in a metacrisis
if we had different choice-making architectures we'd be using ai for different things but if ai is
in service of human goals and human goals have driven the super organism and the metacrisis
the way they are then this context of human goals as an accelerant of them all now if I think about
agi risk let's make we make an ai that is fully autonomous we can't pull the plug it has goals
that aren't ours it starts optimizing for those and in the process decides to use all the atoms
for something else and completely terraforms the earth right which it could do it could totally do
and we're moving way faster towards that than we are towards safety on that that is a risk within
the metacrisis landscape but ai being used by all types of militaries all types of governments all
types of corporations for all types of purposes achieving narrow goals externalizing harm to
wide goals is an accelerant of the metacrisis on every dimension and so now as we take the
intelligence that has driven these problems unbound by wisdom and we exponentialize that kind of
intelligence we get to see whoa super intelligence is really fucking potent what goals are worth
optimizing with that much power with something that's a trillion trillion times smarter and
faster than humans what goals are worth optimizing it's not global gdp because i can increase gdp
with war and addiction and all all kinds of things and destroy the environment every definition
and any so i say okay it's gdp plus genie coefficient plus this other thing plus carbon
plus whatever nope there's still lots of life that matters outside of those 10 metrics or 100
metrics that i can damage to improve those the metric set that is definable like the doubt
etching says the doubt that is speakable in words is not the eternal doubt the metric set that is
definable is not the right metric set so if i keep expanding the metric set to be gdp plus
i can still do a weighted optimization with an ai on this and destroy life and the unknown
unknown means there will always be stuff that matters that has to be pulled in where i don't
want to run optimization on this thing so then the question comes if i have something that can
optimize so powerfully what is the right thing to guide that it's the thing that can identify the
difference between the set of metrics you've identified as important and reality itself
the limits of your own models that is not intelligence that is wisdom i'm defining these
roughly in a way that you get the someone gets the sense of the difference between
what the weighted metric set of all the identified important metrics and the optimization function
on it says you should do the difference between that and what you should actually do is wisdom
and it requires being able to attune to more than just the known metrics and more than just the
optimization and logic process on those and so as super intelligence shows us how fucking
dangerous narrow optimization is it even shows us our own intelligence and our own intelligence
running across all the humans via things like markets that and the market incentivizes some
humans to innovate new ways to turn the world into dollars and other humans to take those
innovations and exploit the fuck out of them right so you both search algorithms and optimization
algorithms the market makes the general intelligence of humans do those in groups so the
cybernetic intelligence of corporations and nation states in the world as a whole
is already a general autonomous super intelligence running on all the humans as general intelligence
is rather than running on cpus but also using all the cpus and tpus and gpus in service of
the collection of all the narrow goals so AI accelerates the metacrisis but it also makes
clear to us that what it would take to align it is you cannot have and this is why the question
you asked who's building it and who owns it and what goals do those groups have if you wanted to
make a super intelligence that was aligned with the thriving of all life and perpetuity the group
that was building it would have to have the goal of the thriving of all life and perpetuity which
is not the interest of one nation state relative to others and is not the interest of near term
market dynamics or election dynamics or quarterly profits or a finite set of metrics but that
but that maps right on to the global geopolitical governance conversation
which we can't have it right now i mean yeah go on if you have a group that has a goal narrower
than the thriving of all life and perpetuity and it is developing increasingly general AIs
that will be in service of those narrower goals they will kill the thriving of all life and
perpetuity so what this says is inside of capitalism and inside of separate nation
state competitive interests which are inside of molok super organism type dynamics you cannot
safely build increasingly general intelligences you have to get a you have to have the general
intelligence of the humans in the group the cybernetic intelligence of these humans in the
group be aligned with the thriving of all life and perpetuity that has the those wisdom dynamics
that thing could possibly possibly be oriented at least it doesn't have a perverse incentive
to build something that was also aligned with that where it is now seeking to scale
because remember we were talking about it would wisdom is more possible at a smaller
scale of people that can be in richer relationships with each other and then scale messes it up
can AI actually help take the dynamics that can happen at smaller scales and help us to build
governance structures and i don't mean agi i mean certain tools of computational capability can
like i don't think anyone thinks that if we were to try to
build a democracy from scratch today fighting a revolutionary war whatever they would build
it the same way we did in 1776 we would be building it in computational systems would
people be able to vote from home would voting even be the the thing would would digital identity
be a thing would cryptographic provenance of information be a thing would there be some kind
of data aggregation using everybody knows if we're going to build it from scratch would be
a different thing than when you built it in the industrial era in the industrial era it was a
different thing than when the Greeks built it because they they had different problems
that's based on their tech and also different capabilities if what a democracy in a market
are our systems of collective intelligence what we need our systems of collective intelligence
and wisdom and now we're talking about building artificial intelligence do we how do we build
systems cybernetic systems where the human interaction with each other both how the humans
are developed and how they're interacting with each other factoring all their incentives
is both intelligent and wise and the computational capabilities the artificial
intelligence is not disintermediating them but is in service of the scaling of those
collective intelligence capabilities don't we need a change in cultural goals and aspirations
or prices or systems before that would happen or could we have small NGOs that get philanthropic
donations develop ai's in the service of all of life but doesn't it need a lot of resources
and compute and it those would still get out competed by the military and giant corporate ai's
if we look at the
multi-polar traps and the competition dynamics if we look at who has the resources to build
things at scale we if we look at the speed of those curves it doesn't look good
like it just to be honest it doesn't look good
uh something has to happen that we are not currently obviously on course for
and but if people can if enough people if some people can stepping back be able to see
oh the path that we are pursuing that we feel obligated to pursue oh our own opportunity
relative to risk focus is actually mistaken and we're running a cognitive bias or other
people who are employed by it recognizing that or enough people be like fuck let's make an agreement
to slow this thing down and figure other things out if you don't have again we said wisdom will
always be bound to restraint if we do not get the restraint wisdom to stop the max race then yes
this will be some of these will be near the last chapters of humanity and
so then the the task becomes how do we do that
so do we need is a first step is to expand the wisdom within the hyperagents in the system
it's always a good question if there are some people who have disproportionately more power
than others and other people who have disproportionately more wisdom do you
try to get the people with the more wisdom to have more power and influence or do you try
to get the people with more power and influence to have more wisdom or do you try to get the
you know larger collective bodies to have some more of both of those yeah and and this gets back
to the the the market the superorganism the growth imperative the narrow boundary goal because
those hyperagents that have the power and the resources to do AI and scaling they have the
optionality of money and money can be turned into everything else is it possible that that that AI
gives people more optionality than money at some point I mean I don't know how to break that
dynamic okay there is stuff about money and optionality and pursuing instrumental goals
goals that increase your optionality to pursue other future goals that narrow goal optimization
requires in AI it's called instrumental convergence that no matter what your goal is you're going to
want to pursue certain things that increase goal achieving capability capitalism does that
we didn't finish answering is the totality of the market already a superintelligence we
defined it as a superorganism is it also superintelligence so there's some stuff that we
have not got to I will allow that we're just not going to get to that now we could go for a
biology and and ferris is eight hour record and one recording no I'm not going to do that
what I'll say is that if where we leave this with all of the open threads people have a lot of
questions and we want to come back and address those I think that would be interesting in the
need of closing because I'm also realizing that I am late for another call yeah with friends of ours
on the topic I want to share something that I think will be helpful in the thinking about the
wisdom intelligence relationship which is not saying how we enact it the enactment thing is a
real real tricky thing but just on the what we need to enact if people have not watched the
conversations that David Bohm and Krishna Murthy had together back in the day I would
recommend them as some of the most useful valuable beautiful recordings of human conversation I've
ever seen and in one short clip David Bohm speaking on a few YouTube it I think it's called
like fragmentation and wholeness something like that he basically identifies and this was maybe
the 80s the cause of the metacrisis so he didn't call it metacrisis or superorganism but like
all the problems of the human predicament that is clearly going towards a point of
self termination that was seeable at that time the way he defined it I think was exceptionally
good I think it maps to the way indigenous wisdom has defined it and other things men are not the
web of life we're merely a strand and at whatever we do to the web we do to ourselves but when we
become power capable of doing exponentially powerful stuff then our own short-term or our
when lose becomes omni lose lose our short-term optimization ends up affecting what we would
even the time scales we care about so what Bohm said is the underlying cause of the problem is
a consciousness that perceives parts rather than that perceives holes or the nature of wholeness
and because it perceives parts it can think about something separate from others so it can
think about benefiting something separate from others and either it can then care about some
parts more than others so it's okay harming the other things or it just doesn't even realize it
is right so whether it is separation of care and values or separation of just calculation
and so I can benefit myself at the expense of somebody else I can benefit my in group at the
expense of an out group I can benefit my species at the expense of nature I can benefit my current
at the expense of my future I can benefit these metrics at the expense of other metrics we don't
know about and that all of the problems come from that that in so far as we were perceiving the field
of wholeness itself and our goals were coming from there and then our goal achieving was in service
of goals that came from there that's what wisdom binding intelligence would mean which is the
perception of and the identification with wholeness being that which guides our manipulation of
parts i.e. tech technology and then Ian McGillcrest who I think you are going to have on your show
or maybe already have if not I haven't please introduce us but keep going Ian in the master
in his emissary I think advanced what David Bohm was saying in an incredibly beautiful way
and he'll share it here on the show but basically said to not hit evolutionary cul-de-sacs
there is a capacity in humans that needs to be the master and another capacity that needs to be
the emissary meaning in service of also bound by you could also say there is a capacity that
needs to be the principal and another that needs to be the agent in legal terms of principal agent
dynamics and basically the thing that needs to be the master is that which perceives not mediated
by word symbols language models perceives in an immediated way the field of inseparable wholeness
and the emissary is the thing that perceives each thing in light of its relevance to goals
and figures out how to up regulate some parts relative to others i.e. what we think of as
intelligence relevance realization salience realization information compression and so
when I was talking with him I said so it was the master and intelligence is the emissary
and I said you're basically saying that the principal that the the emissary developed all
these powerful capabilities and so in some places the emissary said fuck the master thing I want
to be the master and it had the tools to do so starting to win that thing on a runaway dynamic
is the cause of the metacrisis the super organism he's like exactly so which maps to what Bohm was
saying it corresponds to the reality that the master circuits would orient towards so if you
already look at all the problems in the world in the global metacrisis and the up impending
catastrophes being the result of the emissary intelligence function unbound by the master
wisdom function then you look at AI being taking that part of us already not bound by wisdom and
putting it on a completely unbound recursive exponential curve that's the way to think about
what that is so what is it that could bind the power of AI adequately has to be that what human
intelligence is already doing is bound by an in-service to wisdom which means a restructuring
of our institutions our political economies our civilizational structure such that the goals that
arise from wisdom are what the goal achievement is oriented towards that is the next phase of human
history if there is to be a next phase of human history so my takeaway from that and from this
whole conversation is that well first of all my takeaway from the whole conversation is I owe
you an apology from the time we met four years ago I thought limits to growth and the great
simplification dominated AI as a risk and looking back I didn't understand actually I didn't
understand until the last three hours how that merged that AI merges with the super organism
in this potentially catastrophic way that accelerates all the things so as a hyper verification
of the intelligence that is already driving the super organism exactly yeah and then my second
thought is there still is a role for education for culture and leading by example and by a
a transference of the me-based culture to a we-based recognition both between humans we are
in this together and we're part of the web of life and that's also part of the we and the more
humans that understand and feel that whether through ayahuasca or drums or being in nature or
being with others or meditation or whatever it is the higher chance we have to intervene with
with the the trajectory we're on I mean that's my just but those people mild takeaway those
those people who go spend time in nature observing non analytically but communing with the
the truth goodness and beauty the intelligence the meaningfulness of it who do the ayahuasca and have
those experiences if they become lotus eaters and simply drop out it doesn't affect the curve of the
world either it's it's the equivalent of them being deniers yeah the key is and yet if people are
working to make change but they are not actually connected to the kind of wholeness that they
need to be in service to and they continue to have what seem like good goals but they're narrow
we need to get carbon down we need to get the rights of these people up we need to protect
democracy we need to get our side elected because the other side is crazy we need to we need to
develop the ai to solve this problem anything less than the connectedness with wholeness
and anything less both at the level of care and at the level of calculus even though you
can't do them you are oriented to try with the humility that knows you will never do it properly
the humility that knows that you will never do it properly is what keeps you from being
dangerous from hubris but the part that really really wants to try is what has you make progress
in the in the direction of the service of the whole we should probably do our next conversation
in the service of the whole and take a deep dive in that and what it might look like and maybe
there will be some people that that respond to that calling and then come back and do a deeper
dive on these ai questions because it's going to take me a while to to process this um this has
been great is there anything else you want to contribute we'll add some things to this show
note so the piece that eliazer yudkowski did on bankless i think is worth watching both because
of getting what he said but also that the moment in culture um the bone christina marty pieces i
think are super valuable migil crest and um tyson you're gonna have on i think samantha would be
great to have on she and i've had conversations on these topics and has good indigenous kind of
insights um there's a guy named uh robert right who's made a bunch of short really simple ai
risk videos that i think are exceptional as a resource to share um not the guy that wrote the
the moral animal no no different one okay different guy um and let me verify uh robert
miles excuse me not robert robert miles and um so if people just want to understand the ai issue
better with like short simple explainer videos i think his are some of the better ones i know of
um okay and obviously the conversations you and i had previously in the series contextualized a
lot of it and then if people write to you with questions there is there are so many threads
we left open um would be happy to come back to it and be fun okay thank you for continuing to
think and push on our plight uh it seems daunting but um this is the time we're alive we have to
understand it uh care about it and engage with it but this this is a this is a big old red pill my
friend i mean it only takes the whole of our self in service to the whole of reality uh
and utilizing both all of our technological and trans technological capabilities uh for the purposes
that are inclusive of everybody and so uh if that could seem daunting can also seem inspiring
yeah it's both it's both yeah um to be continued and uh thank you thank you my friend these are
fun to be in these conversations with you if you enjoyed or learned from this episode of the
great simplification please subscribe to us on your favorite podcast platform and visit the
great simplification dot com for more information on future releases
