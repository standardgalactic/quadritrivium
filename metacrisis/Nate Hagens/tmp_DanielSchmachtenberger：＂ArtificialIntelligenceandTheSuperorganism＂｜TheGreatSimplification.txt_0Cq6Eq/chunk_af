in the pragmatic definition of goal achieving if we define general intelligence narrow intelligence
is can achieve a specific kind of goal an artificial intelligence can win at chess
is a narrowly trained system on one thing can win it go is another thing can translate languages
is another thing can do image recognition is another thing can optimize supply chains is
another thing narrow intelligence general intelligence is increasingly generals can
do more of those things fully general is can in general figure out how to optimize an eagle
is the market so humans have general intelligence yes and one definition of artificial intelligence
is trying to make things that behave in ways that we would see as intelligent that so it's just
taking the term intelligence kind of intuitively or heuristically and if it behaves in a way that
we kind of would assess that is intelligent that's one definition another definition is
that intentionally aim to model human capacities human intelligent capacities there are other
definitions that are more interesting than those but when you ask this question about so
are humans generally intelligent yes I think we actually have to back up and define what
artificial general intelligence versus narrow artificial intelligence is to explain why artificial
general intelligence is scary to then be able to come back and describe this market thing for a
moment please do so if people probably most of the people here have heard conversation about
are we nearing artificial general intelligence maybe we are nearing it in a very short period of
time and maybe that is really catastrophic for everything and why and probably some people
have a clear sense of why they've maybe heard le izer yudkowski or other people like that talk
about the nick boss drum talk about the cases regarding artificial general intelligence that
are the most catastrophic types of cases that one can imagine meaning much worse than nuclear bombs
but for anyone who's not familiar I'll just try to do very briefly if I have a narrow intelligence
like the ability to play chess first if people don't understand how much better at
chess the best AI systems are than the human systems go study that for a minute and get a
sense look at how the AI that beat Kasparov at chess IBM system then evolved into stockfish is
the best system that had been programmed with every human game that was so much better than humans
it wasn't even calibratable and then how a totally new approach to AI that google innovated the kind
of alpha go system was able to beat that system rough there were some ties but roughly 38 zero
without having programmed any human games just letting two of them play themselves a trillion
times in three hours and get so fucking good based on no human system just the rules themselves
that they could beat the previous god as if it was nothing and it took three hours of training
and you're like whoa we aren't even in the running of relevant we don't even have a reference for
how not relevant we are at being able to do anything with that narrow goal right now it happens
that that narrow system that was able to train that way and beat us on chess also was on go
also did on starcraft also like you just line them up and you're like oh whoa okay so this
is a sense of the power of artificial intelligence meaning intelligence like we have achieved a goal
goal here is when it chess when it go when it's starcraft does that are all those military
strategy games yes those are all military strategy games could you apply those to real
world military and economic environments can you define real world environments like games train
ai's on them to be able to win would they dominate that excessively that is the whole direction of
things right does a real quick real quick question do so when when people are applying
ai to war and economic problems and innovation or whatever it's a series of individual
narrow artificial intelligence they're not applying an artificial general intelligence
there is no artificial general intelligence as far as we know today we are it is the biggest
goal in the space we're rapidly moving forward towards it i'm going to make an argument that
there is something like it but not in the obvious sense but the computational systems the purely
computational artificial intelligence systems are narrow but they're increasingly wide meaning
that the same system that can be trained on one narrow goal can be relatively easily trained on
other goals which means that the you don't have to start from total scratch right so that is
increasing generality but not full generality um and but if i want to give a real world example
if you make drones let's talk about the copter type drones that use swarming algorithms
where they they fly in kind of a swarm pattern and so they can go through obstacles and some of
them can be taken out and the other ones will reconfigure um the ai that regulates their flight
is so radically advanced beyond what a human controller could do right like a human controller
trying to control that fleet of drones not even in the ballpark just like they aren't in chess
so when you start to think about autonomous weapons and what would that mean
you can begin to get a sense of it and there are already examples of this that you know there's
a lot of things that are run by forms of artificial intelligence cybernetic systems so ai high
speed trading has already meant that a normal human engaging with a maximum amount of knowledge
without those tools cannot play in that space ai optimized high speed trading can only be
competed by computation by competitive ai systems and so then can i ask another naive question
yes so when you say that let's just use chess as an example even though military finance
applications are probably more dangerous and and relevant to our futures but taking chess you said
you have to train the system and the training took three hours so so what does that really mean
there's some business owners or coders that say okay we've got these algorithms this artificial
intelligence and now we want to apply it to be really good at chess they write a little code
and they give the ai the objective and then they just press go and and and three hours later they've
got a model that they can apply in a real game so this is the techniques of artificial intelligence
the techniques of learning those are all evolving and there are to understand something about the
evolution rate there are many different exponential curves that are intersecting in general and you
probably address this on your show in the evolutionary environment in which we evolved
yes we have abstract intelligence but we don't have intuitions for ongoing exponential curves
because they never happened in our environments right something would start as an exponential
curve and then it would turn into a logistic curve into an s curve and so the rates of
exponentials are non-intuitive people are used to think about oh it's getting faster we have time
to do something not like it 10x and it 10x again then it 10x again and the speed in which it is
doing it is dropping by 10x right the time period um so we intuitively get this thing wrong that's
for a single exponential so if I take ai the hardware right how we are not just having cpu's
but gpu's and tpu's and different kinds of arrangements of those and network cards and
whatever the and and how many transistors we can get in a small area there are exponential curves
in the hardware in terms of the increase in progress morse law and other stuff there are
exponential curves and how much data is being created through sensor networks and through
social media being able to aggregate all this human data and stuff there are exponential curves
in the not in the total amount of capital going in the total amount of human intelligence going
into the space the innovation in the models themselves in the hardware capacities for actuators
in the types of things that can be sensed and so you have intersections of many different
exponential curves so what we were meant by how an ai learns at the time of stockfish
to the time of alpha go already was different it's a completely different answer to them
and so where stockfish you are programming in all of bobby fishers games and all of spatsky's
games and giving it opening moves and putting in all of the books of chess and all like that
as a type of learning that then has a lot of human feedback on is it getting it right
this other technique all that was programmed is what are the definitions of a win and what
are the allowed moves and then you have two versions of the system play themselves with
some memory and learning features built in where they know what they're seeking to optimize for
is how to win no human games built in no theory of opening moves but because of the speed they
can really play a trillion games in a small number of hours then they have opening moves that have
never been seen that are not aligned with any theory totally different approaches humans have
never done because they can take very different branches and so they have the equivalent of
what would have been millions of years of human chess playing or maybe no amount of human chess
playing because human memory could never hold that much human combinatorics couldn't but in
three hours it can be trained to do that compared to how long does it take to get a human to just
even catch up with what has already been known in chess so so who owns the ai's some ai's
are built by corporations mostly public corporations some are built by branches of military or
governments so national laboratories make some very powerful ai's and obviously the ai's that
are in public deployment as large language models right now are microsoft and google and
anthropic and then increasingly you know other companies that have to follow on meta and bydo
and like that but tesla has very powerful ai's very very powerful ai's that are trying to get
full-blown self-driving down that take information from sensors and process it through artificial
intelligence to control actuators and how did it happen that all these are companies that don't
really talk to each other like microsoft doesn't really talk to google at a competitive sense so
there's just a bunch of engineers that around the world are are simultaneously developing these ai's
in competition with each other and they're all figuring out the same codes and etc but some
are doing a little bit faster and better i mean ai is not that different in this sense than
other categories of emerging technology where you have different companies competing where
anybody can see once someone deploys what they did reverse engineer it and try to figure it out
they can all have they can all try to spend enough money to hire key talent away from the other
company there are academic groups that are publishing stuff that is also at the cutting
edge of knowledge and then the companies take that knowledge and do stuff with it they can
all do corporate espionage on each other so so there isn't one ai there are dozens hundreds
thousands of ai's depending what they're applied on and depending what vector of society they're
developed in and different fundamental approaches to ai what do you mean oh there are ai's that are
trying to take visual sensor data to control motor actuator data to move a self-driving car
which is different where it's sensing and actuating a movement of a vehicle through space
there are other ones that are trying to read content online read read language
your chatbot and so they're sensing in language and they're actuating in language
so they're optimized for language input output so those are very different kinds
but the so the goal of ai in these examples is not profit maximization the goal of the people
that control the ai is profit maximization the goal of the ai is whatever people code its goal
to be its objective function or its loss function the inverse of the loss function but it's
but the people are developing it in service to whatever the goals the organization they're part
of is seeking so if it is a military group it might be the goal of national security if it is
a corporation especially a public corporation it has to be profit maximization through some
specific domains of action if it's an academic group it might be advancing the knowledge of
that field and in you know whatever ways that is so it is an important thing that you're asking
which is if the group that is developing an ai has certain goals the ai will be developed
and service to those goals and so are those groups wise can you if the ai is super intelligent
and we're already starting to get at that intelligence has to be bound by wisdom
is the goal of the group a wise set of goals or is the goal of the group a narrow set of goals in
which case it'll be building the ai for the optimization of that is actually a really key question
but here's a subset of that question if a few of the groups choose wisdom as their goal
or if more most of them do they'll still be outcompeted by those ai's that have the narrow
focus and and win no so the question of are multi-polar traps obligate or is there a way
to get out of them agreement is a way out of a multi-polar trap the agreements are hard because
if everybody can realize that their likelihood of winning the race is low the race as a whole
might mess everything up for everybody and not doing the thing is better was mutually
war games with Matthew Broderick yeah mutually assured destruction was a way of saying
don't think that you can win first strike advantage let's be be clear that winning
equals losing at this level of power there are things built in place where winning equals losing
so we have an agreement to not try to do the win because anyone trying to do the win would be so
catastrophically bad for everybody and the challenge with these agreements is you can enforce
such a thing within a nation right within a nation where you have a monopoly of violence
backing up rule of law you can make sure nobody cuts the national force forests down
even if there's a lot of motivation to do it because you can make a law that says you're
not going to end the monopoly of violence is stronger than whatever the capacity to login
companies are going to bring to bear but if you don't have monopoly of violence and rule of law
how do you enact it so internationally and this is why we face so many global catastrophic risks
whether it's the destruction of the oceans or the destruction of the atmosphere or biodiversity
or developing synthetic bio or AI is if anyone else is we lose in the short term by not doing
it should they deploy it and yet if everybody does most likely everybody dies so how do we
bind those the tricky thing is how do we know the other guys keeping the agreement and that
they're not defecting and secretly doing it so with regard to AI can you get there are and
so we see that open AI was originally created for safety purposes and concern about how far ahead
deep mind was and everybody else and the need to be able to have an open source approach so there
was not only one centrally concentrated power and through whatever series of things happened
the thing that was originally a non-profit dedicated to safety is a public corporation
for profit wholly owned subsidiary of microsoft deploying in competitive types of races against
other ones and then anthropics that broke off from there to do safety that took 300 million
from google ends up being in similar dynamics and so can't without some kind of shared agreement
that nobody does a particular thing then yes you end up having an incentive gradient for
everyone to race to get there first even ethically where they hold me getting there first i'm the
ethical guy so i'll win that race and then i'll use that power for good um and that food yeah that
conversation is very active billionaire it's like a billionaire being aware of the great
simplification and still trying to maximize their optionality um to have more money so
that they can do more philanthropic good in the in the future it is amazing how powerful motivated
reasoning is unbiased let me ask more naive questions daniel we're getting we're getting
to the heart of it here could you program ai for wisdom this is the agi alignment question
and there's different ways of kind of getting to this so let me construct the agi concern versus
the narrow ai concern for a moment so we were on the track of what is narrow ai it is optimized for
a certain set of goals and it can get extremely good at it it can get better than humans by
not just a little bit but in many domains by so much that it's we have a hard time even
understanding it and and that's so much as we speak is accelerating by the month yes
and okay so without fully general intelligence fully autonomous systems then the ai is in service
of humans using the ai for purposes the humans are the agents the ai is an enhancement of our
goal achieving capability right it's just a tool i would argue that even the word tool makes us think
badly um because um language as a in this way i was saying technology meaning goal achieving is
such a broad set of things that the types of tools that an end user uses versus the types of tools
that make other tools machine tools versus the types of tools that innovate how to make lots of
other tools like computation yes they're all tools but they're really fucking different in kind and
they have um we wrote this paper called technologies not values neutral that should link here which
discusses why it's not just that we have our values and we use our tools it's the tools it give the
capacity to do certain things better to achieve certain goals better and in so far as they do the
humans that use them win at things and as a result everybody has to use them or they kind of lose
so one the tools become obligate two the tools code different patterns of human behavior now
i'm doing the behavior where i'm using that tool as opposed to doing some other thing in coding that
pattern of human behavior changes the nature of human mind and societies at large so it is not
true that you've got values and then tools are just neutral the use of the tools changes the human
mind individually and aggregate and then becomes obligate so um psychology affects the tools we
create but the tools in turn affect the psychologies this is and this is why i was saying our nature
is to be malleable to the environment because we change the environment so much but if we don't
consciously know how to direct that it will get directed by a kind of downhill gradient that ends
