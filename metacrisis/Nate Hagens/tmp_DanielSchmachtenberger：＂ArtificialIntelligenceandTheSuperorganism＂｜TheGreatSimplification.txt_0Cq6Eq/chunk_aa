Artificial intelligence is in the news we hear about chat gpt making people more efficient
learning quicker we hear about AI replacing artists and mid-level programmers we see
deep fakes and fake beautiful baby peacocks that are much cuter than real baby peacocks and
lots of people are debating about the benefits and risks of artificial intelligence
but today's guest is my colleague and friend Daniel Schmackdenberger who is back for a deep dive
on how artificial intelligence accelerates the superorganism dynamic with respect to extraction
climate and many of the planetary boundary limits that we face. I have not heard this
angle on artificial intelligence before I think it's really important to have this conversation
and throughout this talk with Daniel we talked about AI but underpinning it all was what is
intelligence and how has intelligence in groups in human history out competed wisdom
restraint of different cultures and different groups of humans. This is an intense dense
three and a half hour conversation and we weren't even done we'll be back in the next month or so
to have the follow-on questions. It's probably one of the better conversations I've ever had
on the great simplification and I think it's really important to merge the environmental
consequences of AI into our cultural discourse. Here's my friend Daniel Schmackdenberger.
Hello my friend. Hey Nate, good to be back with you. I prepare a lot for my podcasts. I read people's
stuff. I prepare questions. I think about it but with you I'm like I got an appointment with Daniel
at 4 p.m. I go for a bike ride. I go play with my chickens and I just show up and we have a
conversation so I'm hoping this will work because this conversation actually is the culmination
of how our relationship started a couple three years ago. Remember we came to Washington DC
for a five or six day meeting where I wanted to discuss energy, money, technology and how this
combined into a super organism and you were focused on existential risks and particularly
oncoming innovation in artificial intelligence and how that led to a lot of potential unknown
destabilizing risks for society and now we've educated each other after a couple years
and today rather than continue our bend versus break series I thought we would like merge these
two lines of thought on artificial intelligence and the super organism. You and I have done five
parts so far in this bend versus break series given all the things that are in the public
attention on AI we decided to do this one. I imagine some of the people will have heard that
series and we can reference the concepts. For anyone who hasn't do you want to give a quick
recap on super organism so we can relate it and maybe super organism and metacrisis and
those are kind of frames we've established that we're going to be bringing into thinking about AI now?
Sure. So humans are a social species and in the modern world we self-organize
as family units, as small businesses, as corporations, as nation-states, as an entire
global economic system around profits. Profits are our goal and profits lead to GDP or GWP globally
and what we need for that GDP is three things. We need energy, we need materials and we need
technology or in your terms information and we have outsourced the wisdom and the decision
making of this entire system to the market and the market is blind to the impacts of this
growth. We represent this by money and money is a claim on energy and energy from fossil
hydrocarbons is incredibly powerful, indistinguishable from magic effectively on human time scales.
It's also not infinite and as a society we are drawing down the bank account of fossil carbon
and non-renewable inputs like cobalt and copper and neodymium and water aquifers and forests
millions of times faster than they were sequestered so there is a recognition that we're impacting the
environment and all of the risk associated with this you label the metacrisis or the
polycrisis or the human predicament but they're all tied together the system fits together
human behavior, energy, materials, money, climate, the environment, governance, the economic system,
etc. So right now our entire economic imperative as nations and as a world
is to grow the economy partially because that's what our institutions are set up to do partially
because when we create money primarily from commercial banks increasingly from central banks
when governments deficit spend there is no biophysical tether and the interest is not created
so if the interest is not created it creates a growth imperative for the whole system
and we require growth now so far the market has dictated this growth but suddenly there's a new
kid on the block which is artificial intelligence created by prior intelligence by humans and
that's what we're going to talk about today. That's a good frame.
I'm an educator I've recently been a college professor my whole role today is to inform and
lightly inspire humans towards self-governance, better decisions, better pathways forward and I
my trade deals with science and facts and systems ecology and I'm afraid that AI
will spell the end of what we know is true and on both sides we won't know what's true and there
will be things that people can grab on the internet that many of which are fake or influenced by
artificial intelligence that destroys the social discourse so that's one thing I'm worried about
AI. The other is will AI accelerate climate change because it will make the super organism
it would be like playing Super Mario or Donkey Kong or something like that and pressing the turbo
button and it makes processes more efficient and it just speeds things up which means more carbon
either directly or indirectly more efficiency feeds Jeven's paradox. Another one of my worries
is a lot of jobs are going to disappear from AI and how does that factor into the super organism
so it seems to me that AI both simultaneously makes the super organism
hungrier and more voracious but also runs the risk of killing the host in several ways
so these are just some of my naive questions but I think before we get into artificial intelligence
maybe we'll just start with intelligence and humans I think in a previous conversation you
and I had that's what differentiates us from the rest of the biosphere is our ability to
problem solve and use intelligence to grow the scale of our efforts that is coupled with energy
and materials always so maybe you could just unpack how you see the historical role of intelligence
before we get to artificial intelligence so I might start in a slightly different place
which is to actually start with a couple cases of AI that are obvious and then we'll go back to
intelligence the relationship between intelligence and the super organism itself why human intelligence
has made a super organism that is different than animal and natural intelligence made in terms of
the nature of ecosystems and then how artificial intelligence relates to those types of human
intelligence not just individually but collectively as you mentioned mediated by markets or larger
types of human collective intelligence systems and and then get to what has to guide direct
bind intelligence that it is in service of something that is actually both sustainable
and desirable so let's talk about just artificial intelligence for a moment to give a couple examples
because people have heard since and and the reason it's up so much since artificial intelligence was
kind of innovated in the 50s and some could argue precursors before that the reason it is
in the conversation so much currently is the deployment of large language models publicly
and where starting with GPT-3 and the speed of the deployment of those relative to any other
technologies GPT-3 getting 100 million users and I forget exactly what it was now six weeks or
something which was radically faster than tiktok's adoption curve facebook's youtube's cell phones
anything which were already radically faster than the adoption curve of oil or the plow or anything
else so world-changingly powerful technologies at a speed of deployment which then led to other
companies deploying similar things which led to people building companies on top of them which
leads to irretractability and so the speed of what started to happen between the corporate races
the adoption curves and the dependencies is of course understandably changed the conversation
and brought it into the center of mainstream conversation where it had been only in the
domain of people paying attention to artificial intelligence or the risks or promises associated
previously so when people talk about AI risk or AI promise of which it has a lot of both
there's a few things about cognitive bias worth addressing here first which is a topic you always
address on why people come to misunderstand the superorganism and get kind of choice making
uh wrong get sense making wrong thank you that that means you actually have of course i have
watched and read your things this is my friend here you're so damn busy that i'm like hey daniel
watch this and you're like i will but you and i have not talked about cognitive biases but
you're right i do talk about them a lot so uh carry on so let's take there there are clusters
of cognitive biases that go together to define like default worldviews and they're not a single
cognitive bias or a kind of a bunch of them and you don't even have to think of it as bias it's just
like i mean it's a strong sounding word though it's true it's a it's a default basis for the
sense making and meaning making on new information people are likely to do first and so one of them
that i think is really worth addressing when it comes to ai is a general orientation to techno
optimism or techno pessimism which is a subset of a general orientation to the progress narrative
and i would argue and will not spend too long on this so it actually warrants a whole big discussion
i would argue that there are naive versions of the progress narrative uh capitalism is making
everything better and better democracy as sciences technology is don't we all like the world much
better now that there's no vacane and antibiotics and infant mortality is down and so many more
total people are fed and we can go to the stars and blah blah blah like obviously there are true
parts in everything i just said but there is a naive version of that that does not factor all
the costs that were associated adequately and there's a naive version of techno pessimism
so first on the naive version of techno optimism when we look at the progress narrative there's
so much that has progressed that if you want to cherry pick those metrics you can write lots and
lots of books about however everything's getting better and better nobody would want to be alive
at any other time in human history there's there's two things that the naive progress is missing one
is is the the costs like climate change and the oceans and insects and the other is is the one time
subsidy uh of of non-renewable energy and inputs and the the source capacity of the earth and and
those are are not finite so those are the two two blind spots i think in that narrative
so we could say the costs and the sustainability of the story um yes and
so if you talk about the story of progress particularly like the post-modernity version
of science technology and the associated social technologies not just physical tech because
capitalism and democracy and international relations are all kind of coordination systems
that we can call a social technology a technique a way of applying intelligence to achieving goals
and doing things um of which you can consider language in early social technology which it is
if you ask the many many indigenous cultures who were genocided or extincted or who have just
remnants of their culture left um or if you ask all of the extincted species or all of the
endangered species or all of the highly oppressed people uh their version of the progress narrative
is different and just like the story of history written by winners or losers but if you add all
of those up the totality of everything that was not the winner's story is a is a critique on the
progress narrative and so one way of thinking about it is that the progress narrative is there
are some things that we make better maybe we make things better for an in-group relative to an out
group maybe we make things better for a class relative to another class for a race relative to
another race for our species relative to the biosphere and the rest of species or for some
metrics like whatever metric our organization is tasked with up regulating or gdp or something
relative to lots of other metrics that we are not tasked with optimizing or for our generation
versus future generations exactly short term versus long term and so the question is where it is
not a synergistic satisfactor where there are zero some dynamics that are happening the things
that are progressing are at the cost of which other things and we're not saying that nothing
could progress in this inquiry we're saying are we calculating that well and if we factor all of
the stakeholders meaning not just the ones in the in-group but all of the people and not just all
the people but all the people into the future and not just all the people but all the other life
forms and all of the definitions of what is worthwhile and then what is a meaningful life not
just gdp then are the things that are creating progress actually creating progress across that
whole scope so i have two replies to that my first is amen and my second is you're advocating for a
wide boundary definition of progress as opposed to a narrow boundary one and the definition
between wide boundary and narrow boundaries very related to the topic of intelligence too
are our goals narrow goals or are they very inclusive goals if we have a goal to improve
something for whom are there if we're is it for a small set of stakeholders is it for a set of
stakeholders for a small period of time is it measured in a small set of metrics where in
optimizing that in being effective at goal achievements we can actually externalize harm
to a lot of other things that also matter and whether we're talking about technology itself
or nation-state decision-making or capitalism or whatever we can talk about something where
all of the problems in our world we could say have to do the human-induced problems have to do
to do with the capacity to innovate at goal achieving decoupled from picking long-term
wide definition good goals and that doesn't mean that there is nothing good about the goal it means
that the goal achievement process is fragmented the world enough that and sometimes it's not even
perverse right i'm going to get ahead economically and i'm going to fuck the environment and the
people doing slave labor in the mines and i know it and i'm just a sociopath so i do it sometimes
it's not that sometimes it's we are the world is complex nobody can focus on the whole thing so
we're going to make say a government that has different branches that focus on different things
so they can specialize and specialization and division of labor allow more total capacity
and so this group is focused on national security of this type or focused on whatever it is let's
focus on if it's the UN world hunger now is it possible to have a solution to world hunger
that is where now my organization has specific metrics how many people are fed etc and whether
how much of the budget we get next year to be able to do stuff and whether we get appointed again
or elected again have to do a specific metrics where it is possible to damage the topsoil it's
possible to use fertilizers and pesticides that will harm the environment and cause dead zones
and oceans and destroy pollinators that advance our metric but if we don't there is actually no way
to continue within that power structure this is an example where it's not even necessarily perverse
in a knowing way but the structure of it the institutional choice making architecture is such
that what is being measured for and optimized and tasked can't not prioritize some things over
others and that with increasing capacity to goal achieve what is externalized to the goal
is increasingly problematic so is the narrow boundary focus versus the wide boundary focus
could that itself be a basic fundamental difference between intelligence and wisdom
and then building on that if an entity a tribe a nation a culture focuses on the narrow boundary
goals won't that out compete a nation a tribe a culture that focuses just on the wide boundary
broader multivariable things wonderful like fairness or environment or future generations
so let's come back to that definition of wisdom and the relationship between wisdom and intelligence
but let's address that we were saying earlier there is a naive version of the progress narrative
or the kind of techno capital optimist narrative there's also naive version of the technopessumist
narrative the technopessumist narrative over focuses on all of the costs and externalities
who who lost in that system and basically orients in a luddite way and is like
no fuck tech and new things it was better before there's various versions one is there was more
wisdom before and this is a descent from wisdom in terms of like cleverness that is
somewhere between less wise and evil the harm the benefits that come from this will be more
like hypernormal stimuli that actually cause more net harm that we're moving towards tipping points
of catastrophic boundaries for the planet etc so let's just let's just not tech and extreme versions
of that look like the Amish but unabomber wrote a lot of things on this topic right
and and they were not dumb things he was doing a real critique of the advancement of technology
and then being like how do we not destroy everything if we keep it on this track now
and we will also see that there have been indigenous perspectives that wanted to keep
indigenous ways that wanted to resist certain kinds of adoption of things that would as far as
technological implementation is considered be considered a non embrace of progress now you
were just mentioning if tech is associated with goal achieving and some goals have to do with how
to upregulate the benefits of an in group relative to an out group doesn't tech mean power yes
doesn't a group that rejects some of it mean less power in the short term so where those
competitive interests come particularly if the other side both has tech and the mindset to use it
does that end up meaning that that doesn't forward and we can see that when china went into
