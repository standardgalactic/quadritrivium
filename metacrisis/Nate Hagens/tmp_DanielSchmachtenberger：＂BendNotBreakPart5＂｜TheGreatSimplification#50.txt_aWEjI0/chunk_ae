first
How do you think through the externalities on second third fourth order effects?
By talking to more people with different perspectives about the thing
That's the first really key part. This is where diversity of perspective and pluralism is really a real thing
If I'm thinking okay
I'm going to solve this issue. I'm going to protect this area of the amazon from being harmed by this combination
Of law regarding mining rights and regarding agriculture rights and etc
Okay, cool. Sounds good
Go talk to the indigenous people about the solution and they'll be like
No, no, no, there's some other groups that did things like that and here's what happens
The they just hire mercenaries that do this thing or it gets worse for this reason or whatever and you're like, oh fuck
I didn't know that
And go talk to the legal scholars
And they're like, oh, there's no way to implement this in law because of such and such and go talk to the economists who are like
Law is written by lobbyists and they have more money to keep writing law than you do. You're going to lose this one and go
You go talk to the environmental scientists and they'll be like actually you're protecting the wrong part of the forest
there's not even high biodiversity here relative to this area and
So
There's a lot of people who will know stuff that you don't know
And they will be able to see where some of the externalities will occur
Including people that are on the opposite political side of the thing, right? Whoever would fight against it
There's something that they care about that is really even if they're wrong about some stuff
They're not wrong about a hundred percent of everything
So finding out why do they not like it and say how can I factor that in so one?
I can not cause the problems for them
And as a result decrease their enmity and maybe get them as allies rather than fighting the thing, right?
You'll become both more successful and it'll be a better solution
So the way to think through externalities better is when you come up with a possible solution talk to lots of people about
What might be wrong with it? So red team it
But then don't give up say ask them. Okay. How would you make it better to still serve the thing?
I'm wanting to serve but to improve the thing that you're seeing
And so you use it as a proposition or design refinement process
And so that's the first part and then the second part is once you
Implement it try to implement it in experimental ways, right? You have some safe to fail probes. You don't implement the largest thing possible
Okay, I think we figured out that this
This version of ai or this genetically modified organism is safe. Let's just fucking take it everywhere
Maybe our safety experiments were not totally complete. Let's do contained experiments
So the first thing is think through it well, right?
Which you can't do on your own
The next part is run some experiments and try to do if you missed stuff that will show up in the experiment
Try to do it in a contained way
Right
And then as you implemented at scale there will still be stuff that you missed
So have a broad listening and ensure that when you find where something is being harmed and you need to change the strategy
Doesn't mean you stop it means you update it that you maintain the governance capacity to do so
That you haven't turned it over to where now that it's initiated the fiduciary responsibility to shareholders means you can never change
That thing or whatever mean ensure that the governance structure is such that when we learn new stuff the underlying thing can be changed
And when you say you uh, who do you mean by you in this case try to do anything
Right, we're talking to people here because on the chance that people here are going to try to
They're running a non-profit or they're running a
Institution or they're wanting to build a thing if you are wanting to
Do things these are things to keep in mind
And if you are supporting someone who has more agency than you in the organization or whatever, these are things you would help support them with
So I haven't heard you frame this holistic precautionary logic of the last few minutes
Did you just like have a sandwich yesterday and come up with this or?
No, I think this is like actually
This is the thing that I would arguably like to start with it
but
If someone doesn't have the metacrisis frame, they don't yet it's not obvious that the way we
Try to solve problems is the cause of the other problems and is a major part of the generator function of the metacrisis
And so once you've understood that then you can be like, okay
So part of how we solve it is by a much more holistic
Set of frames and how we go about doing things
Um
And now I don't think this is impossibly hard to teach or train. I think this should be
Being trained in k through 12 education at various levels of development
I think that people who are learning
anything from design science to
Becoming technologists to being lawyers or studying political economy should be thinking and being trained in these things and thinking through the specifics in their domains
Okay, now the next part is I want to identify two types of externalities
Physical externalities and psychosocial externalities
A physical externality is where we do something to benefit
We do something that has a physical effect to benefit something
But part of the supply chain of that thing happening
Causes some physical harm somewhere
right
So we want water repellent coats and water repellent umbrellas and everything like that and we want industrial surfactants. So we make
um
fluorinated
surfactants to be able to do that
But then they go into the water supply and they're forever chemicals and they never break down and they're carcinogens and endocrine disruptors and neurotoxins
so that's a
You know where we want an herbicide to make agriculture more effective, but it messes up the soil bacteria and
uh
human health and whatever those are classic examples of physical externalities
Obviously co2 and climate change is a physical externality. No one is intending to cause climate change. They just want
generate energy
So everybody knows physical externalities. They didn't used to
But rachel carlson and friends kind of got people thinking about that now. I think everybody is at least somewhat aware
But that the supply chains that make anything happen
Have lots of effects. That's physical externalities
There are also psychological
A point on that
Is the physical benefits we get in terms of profits or services
um
Art on equal footing with the negative externalities that rachel carlson warned about and and others because those things happen
In the future and we're a biological species. So emotionally they have
And culturally they have very little weight in our brains
So that's why this stuff has happened because our capitalist
Exponential system has back loaded some of those costs that are second third order effects from our decisions and our products
Right, I'd actually like to explain a few reasons why it happened
so one is a lot of the harms will occur in the future, but the benefits occur now and so there is a
uh
A perverse orientation to now
It's also that whoever does the thing that provides more advantage now probably wins
Having more power to influence the overall system and those who don't do the thing that will cause the future harm
Also, don't get the power and then don't influence the whole system. So there's a perverse incentive not just a psychological bias
There's also a game theoretic perverse incentive to do that thing
Um, it's also true that as we were mentioning earlier
Making a tech that produces a certain benefit is epistemologically easier
than preventing all of the harms from doing so because
Creating a first order effect on a small number of metrics is just epistemically easier
Than considering nth order effects on a very large number of undefined metrics
So there is also something epistemically about it
and then
The other thing is that many of the problems
Only occur through very large-scale cumulative action, but the individual action produces my benefit
If I cut this tree down, I get money from the lumber
But I have just as much air to breathe
It's everybody cutting it down that messes up the atmosphere
But me not cutting this tree down doesn't make any seeming difference to the atmosphere
But it does make a difference to my balance sheet
So each person
The risk-reward ratio orients them to do the thing
Because the collective because the harm is more collective and the benefit is more personal
You have to factor all of those asymmetries
And then one version of this that I want to point out for people is that
There's obviously many narratives that tech will make everything better AI will solve all of our problems and
Health care is going to become awesome with genetic engineering and immuno-oncology and all those things and
Nanotech is going to make environmentally friendly everything manufacturing
And capitalism makes things better
There's obviously many narratives like the environment is fucked and every positive thing was the result of
First was only positive for a few and the result of colonialism and etc. There's partial truth to both of these
And there's obviously risks associated with AI that some people are aware of and benefits or of any tech
But there is a perverse incentive that's worth paying attention to
That those who focus on the opportunity more than the risk will get ahead more than those that focus on avoiding the risk
More than their own advancement of the opportunity even even in our hunter-gatherer days. That was probably true
And well even more now because
The hunter if he did something too risky might have died now. I socialized the losses of the risk. I can declare bankruptcy
I'm not going to jail limited liability corporation is just going to take a bit of a loss whatever
So I get to privatize the gains and socialize the losses. We can do an oil spill
It's not going to hurt me as the executive, but I am going to make money when I exploit more oil
Yeah, and
So the ability to privatize gains socialized losses
um
the
ability to
Cause some new tech
facebook social media destroying
Democracy in the social contract in the epistemic commons and yet no accountability for that thing
But just a fucked on the profit
um
I can always say afterwards. Oh, there's no way we could have known
But nobody tried to do the external calculus that I'm mentioning and or to the degree that anyone did they said shut up
We don't want to hear that and the focus was that's going to be amazing. It's going to connect the world
It's going to whatever the narrative of you're being too negative. Let's focus on the opportunity
Whoever runs that narrative will get wealthier and be able to upregulate the narrative and show the success and then highlighting the thing
Well, you've just described while I'm why I'm poor and still teaching but go on
So there is this as we move forward toward the tech now where one the we can't keep
Externalizing costs to the commons because we're too close to planetary boundaries
And the world is too fragile and two the new tech is way too fucking powerful the speed at which
Everyone got smartphones compared to the speed at which everyone got railroad or the plow, right?
It was like thousands of years for the whole world to get the plow
You know 100 years for railroad less than 10 years on the phone
It's like the speed and scope of impact and then you start to think about genetically modified or you know synthetic biology and AI
We can't keep externalizing exponential. We can't keep externalizing harm with exponential tech
So we have to get much more fucking careful on the risk reward, right?
And get less reward seeking and willing to externalize risk as a whole society if we're going to make it
We actually have to get much more conscious of that
And the default in
47 years is one human is worth a quadrillion dollars. Everyone else is serfs and it's a dead planet
Um
Yeah, that's one of the futures that we're not that interested in
Um, okay, where are we I interrupted you a couple times. Are you on on track?
you had another part of the externalities that
I kicked you off your social externalities psychosocial there are physical externalities
And the physical externalities were clear on they produce a physical harm on the environment or human health, right?
the psychosocial externality
um, and
They're not always perfectly preventable sometimes that you have to manage them like I'm on a laptop right now
One of the externalities of the convenience of being able to carry this computer as opposed to a desktop
Is that it's going to mess my neck up and almost everybody today has neck issues because they look down too much
And we're not supposed to look down physiologically evolutionarily that much so at home set your monitor off and you get the proper
you know
C-curvature in the cervical vertebra
I might not be able to have the benefit of portability and not have that but I can at least say all right
Well, don't be lazy and stay on the laptop when I get home really do get a raise monitor or so
The physical externalities don't only look like effects on the environment
They also look like the ergonomic effects and the behavioral effects and lots and lots of things
Tiny tiny observation. I knew everything you just said
But the social mirror neuron cultural evolution while you were saying that I straightened up in my chair a little bit subconsciously
So thanks
Funny how that works. So the psychosocial externality is that
my
technology or my movement or my whatever might also affect have an effect on
people's
experience people's
emotions beliefs
Identities not just individually which has a psychological effect but collectively which has a sociological effect and I'll give a couple examples
one of the biggest examples is
