And what happens if, well, it'll take longer, but it'll still be lightening fast by human
time scales. What happens if the music preferences of the youth in society end up
gravitating towards AI generated music? What happens to the artists that currently exist
that are human creative minds and not computers? Yeah, great question. So I think the nouns of
what we make, the music, the art, those things are going to get commodified. And also I should
point out there's a whole set of copyright issues here too, because I can ask computer to generate
something in the style of say, my friend Victor Nye, and it's not any of her work. So she doesn't own
the copyright. In fact, the current case law says nobody does, it's in the public domain, the computer
made it. But computers can't take away the the verb of the act of creation. So I think handmade
art or human made art will become artisanal, sort of like artisanal soap. It's going to be artisanal
art. The thing is that humans are going to have a hard time competing. So let's go back to the
language as a joystick for a second. We're about to see the rise of emotive media. What does that
mean? That means right now, when you watch a movie, it's the same every time through, it does not
change based on your emotions. You know, with the language as a joystick, you can just say, and
this technology already exists, and Vidya put out some some some demos of this where you can
take a scene and re render it nearly in real time to make the character feel more anguished or make
the character smile more. And this works on CG characters, and it works on like real life
actors. So I can rewrite sort of like a style sheet for the emotions and the way that a actor
is acting. That's already cool, right? That means in another like five years, we're going to look
back at every piece of media we consume now as flat. Like why is it that we watch a movie? It
should feel more like a play. It should be different every time and it should react to how I feel
every time versus the same every time. And of course, how is this going to show up in the
marketplace? It's going to show up with an engagement based metric. The easiest thing to
measure is the human paying attention. Now you can imagine Netflix turning on the camera. It's
not about analytics, right? It's about this better experience where the characters are reactive to
you. They gather, you know, hundreds of millions of watches, they find the 10,000 people that are
similar to you in your psychological sort of emotional reactive profile. And now the movie is
uniquely tailored to your current emotional situation and your people that are like you.
These things are going to be so much more sugary and sticky and engaging than anything we've ever
had before. I think that's fucking horrible. I have to tell you. I just, I didn't know that.
I was just thinking about these images and the political thing, which I want to get into. But
so you mean I can, depending on my mood, I can watch an early version of a Star Trek movie or
when Harry Met Sally and the emotional timbre of the movie will change based on the computer's
perception of my current state of mind and my mood? Yeah, absolutely. And you'll be able to
start saying things like, you know, I want to watch when Harry Met Sally, but at the end, I want to
feel happy. I want to feel this way. And it'll just gently manipulate the whole thing to get
you feel that way. So, so Black Mirror was not that sci-fi in reality. That guy was freaking
brilliant those scenes he made. I don't know if you've watched them all.
I have not watched them all, but it's a mix of spot on exactly and also a little chintzy cheesy.
And when I watch them, I'm like, oh, this part, that's going to happen. That part's a little silly.
So, but are we strong enough emotionally, psychologically for this sort of phase shift
on top of everything else going on with climate and resource depletion and the great simplification?
I mean, isn't this just a cultural battle for the brainstem and technology is going to
turn society into an idiocracy where it's all self medicating with technology where our world
becomes this, like you said, sugary, sweet, immersive tech, Netflix marriage. I mean,
I don't, I can't even process it, to be honest, Aza.
Yeah, this is a continuation of a process that started long ago, magicians, con artists,
discovering facts about how the human mind works, and then learning how to use them towards
whatever end that they have. Well, the end is profits, right? Profits for the companies that
design these technologies. Yeah, that is exactly right. And because they have an asymmetric amount
of information about us and are discovering new species of ways to persuade us, unless they are
acting as a fiduciary to us that is in our interests, we're sort of sunk. We, as humanity,
have told ourselves these just so stories. Creativity is the thing that defines us and will
save us. Empathy is that core part of the human experience that will save us. And actually,
isn't it surprising that creativity is sort of the first thing that the AI is coming for?
And that empathy, as beautiful as it is, and is a core part of my life,
Earth Species is about, is also the biggest backdoor into the human mind. And that loneliness
is going to be every country's largest national security threat.
Oh, my God. Okay. I definitely want to carve out a chunk of time to go into your big project,
Earth Species, because I really believe in it. But you also wear another hat as the co-director
of the Center for Humane Technology. How does AI merge into these risks of polarization, social
media hijacking our attention? And then we get further and further apart on the political spectrum
so that we can't even really have a discourse about our reality. How is AI going to change your
work at CHT? Yeah, well, there is sort of like narrow AI and this more like general sense of AI
that we're talking about now. Can you define narrow AI? Yeah, narrow AI, I just really mean
is like the dumb stuff. If you click on something, show you more of those things. If other people
click on something like you do, show you more of what they click on. This is not advanced AI in
the same sense of Dolly, but it's pernicious because you end up having a trillion-dollar
market cap company like Facebook pointing... Not anymore, 500 billion.
But go up. 500 billion. Things in part to like, well, maybe some of our work to Francis Hogan,
them like TikTok eating their lunch. You never know exactly what makes what happen.
But $500 billion of market cap, powering quite literally the largest deployed AI systems in
the world, looking for whatever generates the most engagement. And actually, that's not exactly right
in the beginning of 2018, Facebook switched to using a metric called meaningful social interaction.
And what does that mean? They were measuring, they're up-leveling the content,
which causes the most reaction from your friends. You post something, if your friends react to it,
and if they react to it with an angry face, it gets a 5x boost, then that is the content that
gets promoted. Wait, if my friends hate what I said because it gave them an angry face,
that gets a 5x vote in the algorithm? Yeah. Now, I don't know if that's still true,
but this is some of the documentation that came out from Francis Hogan's Facebook disclosures.
What about a love icon? At that case, I think it was ranked less, but now I don't remember.
Well, I'll have to look at that because if hate is valued more as an engagement than love,
that's a fundamental problem with our entire system.
I completely agree. And Jonathan Hate just pointed out some new research
that the most viral thing, the thing that gets the most engagement,
the most reaction on social media is hate against outgroups. That is number one, the most viral
thing. He's gonna be on the show after his book is done, so I love his research. But that is who
we are as tribal animals, right? Is ostracizing outgroups is a core conserved aspect of our
genome, and we're seeing it unfold in real time. Yeah, but human beings are complex.
And the thing I think to take away, it's like, are we narcissistic and tribalistic?
Or are we creative and altruistic? And the answer is like, well, neither. It's that we're both.
And it's that the environment that we are living within can sort of imagine these aspects of
ourselves as resonant tuning forks. And if the environment outside is humming at a certain
frequency, it'll activate different tones within us. And so if we live in an environment where we
are literally getting paid in likes and follows, and an engagement for hating on the outgroup,
it's that part of us that's going to be most activated. And it's not that technology is an
existential threat, it's the worst of society is an existential threat, and technology is
activating the worst of society. So I wanted to give one really concrete example
of the way that the sort of narrow AI's cause a kind of global psychosis,
where we cannot hear each other, or even believe that we're coming in good faith.
And it sort of works like this, it's like a trauma inflation. So, you know, let's say you have,
you have something that you're particularly sensitive to, let's say it's Asian American hate.
You're scrolling on Twitter, you see an example of the video of Asian American hate, and of course
that activates you, that activates your trauma. And so you click on it. And because you've clicked
on it, it's very activating for you. Your feed starts to become the very worst thing you've
ever clicked on, you start getting shown more and more first person examples of videos of
Asian American hate. So you, you now know, like you, you are, you are inflate, your trauma is
inflating, you're like, this thing is happening everywhere. Why can't other people see it?
On the other side, you might have someone who is like, you know, their trauma is hit when they see,
say protesters throwing things or beating police, like inflicting violence against police.
And so they click on that because there are certainly examples of that out there. They
see a first person video, their feed starts getting filled with the worst thing they've
ever clicked on. And now they have a never ending infinite feed of videos of protesters
beating police. So everyone has their own little nightmares expanded on Facebook as a,
if those things are all happening as 1% of our reality that each of us think that it's 20%
of our reality, whatever we're most triggered by. And so we all have a different sense of what's
really going on. All those things are happening, but at a lower percentage than our perception
from using social media. Yeah, that's exactly right. It's like a fisheye lens and things are
distorted. So it's really happening, but you're getting a incorrect view, a non representative
view of what's happening in the world. And now when I come to talk to you, I know what I've seen.
I have seen these first person perspectives. And so if you tell me that's not an issue,
I know that you're disconnected from reality. You're not really seeing the real world or maybe
you're just coming in bad faith. And you know the exact same thing from me. And so you can see
how we now cannot come together. We disagree. We both say the other person is not based in
reality. If you're not based on reality, how can we even have a conversation in the first place?
And that's happening along every division at scale with that $500 billion market cap AI finding,
you know, as Tristan likes to say, finding the fault lines of society and then fracking them
for profit. So this is happening now. How is the
Dolly and GPT three and open AI and the other advancements plus compute
coming in the next 24 months going to accentuate the problem you just outlined?
Well, you know, one of the things I tried when I started playing with these
image detect these image to text translations was explosion over a car key of and it generated a
pretty realistic looking photo of a breaking news style story. And then I hit another button and
I generated 16 of them. We are about to have this sort of almost think of it as a a a trust
jubilee in the best possible sense. But it's it's it's really like a neutron bomb for trust.
We will not trust in the next, you know, I don't know, two years or so,
any photographic or video evidence. So this is horrifying to me because I'm a teacher. And so
I'm trying to construct a 10 hour this is my project. The second half of this year is the
the earth they talk was a horizontal all the 80 ecological concepts relevant to our future. And
now I'm going to do a vertical drop down to do the depth in each of these. But I'm doing these
online video educational resources for young people around the world at the same time that AI
is stripping out the ability for us to trust anything. So isn't this just a threat to science
and in addition to trust but into science and truth and facts writ large?
Yes, we as a society do not yet have the antibodies to deal with this. And of course, it's not just
the images. And mind you, like Photoshop, we're going to look back as being so antiquated, because
with Photoshop, you have to manipulate like contrast and exposure and pixels. And the new
again, this is already available in research. So it's just really now about like productizing
is that you look at a photo and you'd be like, well, make that person smile more,
add an image of Nate into the background and change it from daylight to sunset.
Like that's the kind of semantic edits, we're just going to be going to become second nature.
We're also going to do that with text, right? You're going to be able to say, hey,
GPT three, generate me a scientific sounding paper for why vaccines are harmful, please cite real
studies, use real graphs, and now generate me 1000 of them and do 1000 in the other direction
for why vaccines are actually the best thing. Now do it for masks. And what you see here is
that it's not about any one viewpoint, it's sort of like how Russia sort of demoralizes its enemies.
This is about just flooding the zone with artifacts that the human brain has to process,
and it's going to be unable to process these things at scale. So really terrifying, my hope
is this, that after sort of the game is up, and we know that whatever text we see on the internet
images we see on the internet may not be true. We're going to hit rock bottom just like in
depression. Sometimes you have an addiction, you have to hit rock bottom before you start coming
up and say, okay, from this place where we can all point at the fact that we know none of this
stuff is true, how are we going to rebuild our trust and our epistemics? That's I think the
conversation we need to be having now. And it can start simply as you know, like maybe our phone
starts signing our images so that it looks at the depth sensor, it puts all of that into a package,
signs it and says this was taken on a real device, why a depth sensor? So you can't just take a
picture of a picture, you have to actually take a picture of a 3D object. People then start
preferencing images that don't have filters because if it's a filtered thing, it's been modified,
you can't trust it, who knows it's underneath. And we start having a new currency of trust built
from the ground up. Okay, so I want to get to your new project, but I have a few more questions
to follow up on here. First of all, could you define the Turing test and why is that currently
relevant and how close are we to that moment? The Turing test, Allen Turing came up with it as a way
of measuring how far computers have got in terms of intelligence because you don't really know
from the outside whether something is conscious or not, you can't tell. And this was the sort of
phenomenological way of testing. It's a lot of words for a really simple test. It's sit down,
you're typing with a computer, or it may be human, you don't know, and you have to guess
in conversation, is this a person or is this a computer? If you cannot tell, then the two things
are indistinguishable to you in that medium, then the computer is past the Turing test.
And you just said that they could write a paper referencing real literature on
pro-vaccine or anti-vaccine, and could humans tell that it was a fake or not?
So the computers can't do that well enough yet. Where we're working on is
a longer form. So at the paragraph level, the computer stays coherent, the two or three paragraph
can stay pretty coherent. Whether it's the time you get to a full scientific paper,
it's not really maintaining coherence. It still takes a lot of brain power to go figure out why,
but it can write a college admissions essay that will pass.
So right now it's at the annoyance time-sync distraction level, but we may not be far off from
us being unable to distinguish what's real and what's fake.
That's right. I think of it as like we are just finished crossing through the uncanny valley,
we're sort of like climbing up the final slopes, and we're heading into the synthetic valley,
where we don't know what's real and what's synthetic. But mind you, functionally,
we've already passed the Turing test. Bots on Twitter, people interact with and think that they're
real. A couple years ago, Microsoft released a chatbot, and really a chatbot sort of like
puts it in your mind at the wrong place. I think of them as synthetic humans, and
they've been sort of crappy synthetic humans, so they're getting to be better synthetic humans.
And it's not that we're just going to like talking with chatbots, we'll have synthetic
relationships. A quarter of their users for Xiao Ice have said, I love you to their synthetic human
friend. And that makes some sense because real humans are messy. You have to work with them,
they have their own needs, they maybe don't know your hobbies, they're not always available.
AIs and synthetic relationships, they're always there for you.
Well, let me expand on that. I personally find some of this stuff abhorrent, but maybe that's
just because I'm a Neoludite and I like forests and dogs and camping and stars and things.
