Human intelligence, unbound by wisdom, it is fair to say is the cause of the metacrisis
and the growth imperative of the superorganism or the capacity that gives rise to it.
That intelligence has created all the technologies, the industrial tech, the agricultural tech,
the digital tech, the nuclear weapons, the energy harvesting, the all of it. That intelligence has
created all those things. It has made the system of capitalism, it made the system of communism,
all of those things. Now that system of intelligence that takes corporeal capacities,
things that a body could do in externalism, the way that a fist can get extended through a hammer
or a grip can get extended through a plier or an eye can get extended through a microscope or a
telescope or our own metabolism can get extended through an internal combustion engine or there
are musculature or whatever it is, right? So it takes the corporeal capacity and extends the fuck
out of it, extracorporeally. So that type of intelligence that does that is now having the
extracorporeal technology be that type of intelligence itself in maximized recursion,
not bound by wisdom, driven by international multipolar military traps and markets
and narrow short-term goals at the expense of long-term wide values.
So where in the metacrisis there are many risks, synthetic biology can make bad pandemics
and extreme weather events can drive human migration and local wars and this kind of
weapon can do this and this kind of mining can cause us pollution and this kind of
pesticide can kill these animals. Those are all risks within the metacrisis. AI
is not a risk within the metacrisis, it is an accelerant to all of them.
As used by the choice-making architectures that are currently driving the metacrisis,
if we weren't in a metacrisis, if we had different choice-making architectures,
we'd be using AI for different things, but if AI is in service of human goals and human goals
have driven the superorganism and the metacrisis the way they are, then this context of human
goals as an accelerant of them all. Now if I think about AGI risk, let's make an AI that is
fully autonomous, we can't pull the plug, it has goals that aren't ours, it starts optimizing for
those and then the process decides to use all the atoms for something else and completely
terraforms the earth, which it could do. It could totally do and we're moving
way faster towards that than we are toward safety on that. That is a risk within the
metacrisis landscape, but AI being used by all types of militaries, all types of governments,
all types of corporations for all types of purposes, achieving narrow goals, externalizing
harm to wide goals, is an accelerant of the metacrisis on every dimension. Now as we take
the intelligence that has driven these problems, unbound by wisdom, and we exponentialize that
kind of intelligence, we get to see, whoa, superintelligence is really fucking potent,
what goals are worth optimizing with that much power, with something that's a trillion trillion
time smarter and faster than humans, what goals are worth optimizing? It's not global GDP because
I can increase GDP with war and addiction and all kinds of things and destroy the environment,
every definition. So I say, okay, it's GDP plus Gini coefficient, plus this other thing, plus carbon,
plus whatever. Nope, there's still lots of life that matters outside of those 10 metrics or 100
metrics that I can damage to improve those. The metric set that is definable, like the
doubt itchings is the doubt that is speakable in words is not the eternal doubt. The metric set
that is definable is not the right metric set. So if I keep expanding the metric set to be GDP plus
I can still do a weighted optimization with an AI on this and destroy life.
And the unknown unknown means there will always be stuff that matters that has to be pulled in where
I don't want to run optimization on this thing. So then the question comes, if I have something that
can optimize so powerfully, what is the right thing to guide that? It's the thing that can
identify the difference between the set of metrics you've identified as important and reality itself,
the limits of your own models. That is not intelligence that is wisdom. I'm defining
these roughly in a way that you get the someone gets the sense of the difference between what the
weighted metric set of all the identified important metrics and the optimization function on it says
you should do the difference between that and what you should actually do is wisdom and it
requires being able to attune to more than just the known metrics and more than just the optimization
and logic process on those. And so as super intelligence shows us how fucking dangerous
narrow optimization is, it even shows us our own intelligence and our own intelligence running across
all the humans via things like markets that in the market incentivizes some humans to innovate
new ways to turn the world into dollars and other humans to take those innovations and
exploit the fuck out of them, right? So you both search algorithms and optimization algorithms,
the market makes the general intelligence of humans do those in groups. So the cybernetic
intelligence of corporations and nation states in the world as a whole is already a general
autonomous super intelligence running on all the humans as general intelligence is rather than running
on CPUs, but also using all the CPUs and TPUs and GPUs in service of the collection of all the
narrow goals. So AI accelerates the metacrisis, but it also makes clear to us that what it would
take to align it is you cannot have and this is why the question you asked who's building it and who
owns it and what goals do those groups have. If you wanted to make a super intelligence that was
aligned with the thriving of all life and perpetuity, the group that was building it would have to
have the goal of the thriving of all life and perpetuity.
