and no one was very few people were worried about climate change.
And now 50 years on, everyone is talking about climate change.
And virtually no one is talking about nuclear war.
It's kind of a flip of that conversation,
despite them being linked, like you say.
Yeah, exactly.
And again, it's if you look at the risks now, I would argue.
And again, my I've entered government service
and my career began in the early 1980s.
So I lived, I had to study the Cuban Missile Crisis.
I didn't I wasn't alive or just a baby at the time.
But you look at I did live through the Abel Archer incident.
I live through reforge the reforger incidents.
I don't know what those are.
Yeah. And see, that's it.
You don't know what those are.
And you probably you've heard of the Cuban Missile Crisis.
Yes. You probably did not hear about Abel Archer.
I'm sure you don't know what the Christina Airport incident in 1999.
That was when the so-called peacekeeping forces in Yugoslavia,
when it was disintegrating, just a quick summary of it.
You had Russian forces that occupied an airport as part of the Serbia force.
And then you had the NATO forces had come in.
Russia had occupied an airport that NATO was expecting to occupy.
And General Wesley Clark actually ordered a British airborne unit
to go and take that airfield away from the Russians
and order them to open fire on them.
And one of the reasons I don't care for General Clark,
he ordered them to open fire on it.
It was really a misunderstanding and miscoordination.
Yet here we had the NATO commander ordering.
We ordered Lieutenant General Jackson to as a British commander
to go in and by force, we take that field.
Well, my Jackson told Clark, look, I'm not going to start World War Three for you.
That's his exact words.
So, you know, that's just here in the last little over 20 years.
There have been other incidents where we've actually come close to these kind of exchanges.
So I read two reasons why, one, we don't have access to that sort of information.
But perhaps more importantly, even if we did have access,
that's such a thing that shuts down human agency so much that it's like,
I don't want to hear about that because there's nothing I can do about it.
Or at least there's a perception there's nothing I can do about it.
So what you're saying then in the field of nuclear exchange,
peer on peer globally, that we have in experienced risk homeostasis.
In other words, in the last 50 years, the odds of global nuclear war
have been much greater than zero, even though emotionally,
we feel like there was virtually no risk.
Wasn't there a paper written or something that the last 50 years
there was like a 60 percent chance of nuclear war or something like that?
I don't remember.
Yeah, that was a PNAS paper that was done a couple of years ago.
And there have been other papers and other journals that have come up with 40 to 60 percent.
Some will go as low as 30 percent.
But most of the thinking is that it was if you want to say a coin flip,
whether or not we wipe ourselves out, that's probably pretty reasonable.
And it's important.
I think it was actually higher than that.
Higher than 60 percent.
Yeah, I don't know how we did not end up blowing ourselves up.
And right now, how we've managed to avoid it.
And, you know, again, if you look at these incidents,
there's someone familiar with that you can't really discuss too much.
But you look at, again, the Pristina incident,
you look at the B, the submarine incident during the Cuban Missile Crisis.
All of these incidents.
And I sometimes joke with people.
And the sad thing is, it's not really a joke.
Is I said, well, you know, I personally stop global nuclear war
on three separate occasions.
Once it wasn't even my fault for almost starting it.
And, you know, what it boils down to is how many times we came so close
and it was one person, the fire breaks didn't seem to work right.
The normal processes, you know, it was down to the individual.
It was the actions of one individual and had it been a different person there.
Had it been slightly different circumstances, the probabilities,
you can't calculate that.
And the plus side, and this gets back to why I'm afraid of the current situation.
On the plus side, you said, human agency, individual human beings, you know,
need to you look into the abyss, the abyss looks back and maybe you turn away
from the abyss and I think that's what happened in a lot of cases.
But what happens when you're no longer afraid of a nuclear conflict?
And that's what concerns me about the current generation of American leaders.
You're just here in the last week, we've had two separate congressmen say,
well, you know, in the situation in Ukraine, we need to kill some Russians.
Imagine someone saying that in 1970 or 80 or 60.
Yeah, we need to make a point.
We need to kill some Russians.
You're talking about whether you agree or disagree, whatever you think of the
the Russian government or our situation in the relative geopolitics.
You're talking about shooting at military forces of a
global nuclear power.
I mean, my God, is that where we are?
I know one of the reasons that you agreed to do this conversation with me
is because you deeply care about this and hope that something can be done about it.
Would more public awareness of these risks help?
What can we do to make a nuclear exchange globally
less inevitable than than you're kind of painting it?
Well, and that gets to all of these other problems.
All of these have a common thread, whether it's
nuclear risk or environmental risk, which, you know, the climate system
or financial system, it all comes down to governance.
Oh, governance.
OK, I was going to say we have a brain mismatch with our ancestral environment.
So you're taking it a step further and it's governance.
Governance is how we overcome this.
And that's another thing, too.
I think we sometimes forget is and I really love your expression,
fiery, because that to me really captures kind of where we are.
And the other expression that said derogatory way sometimes is
you'll say, you know, here people say that a situation is like
monkeys in a nuclear power plant.
Yeah, you know, that's actually what we are monkeys running,
trying to run a nuclear power plant.
And of course, we're smart enough to build these things.
And are we really smart enough to manage them properly?
And the problem is when you start to how you run these systems,
a lot of our instincts like tribalism and our lack of under our lack
of ability to perceive more than the short term gain.
And that gets into as you're talking about brain chemistry
and all those other factors, all this boiled down to how do we
create systems of governance that take into account the complexity
of the modern world, the extreme dangers, but also the extreme
opportunities of a lot of these technologies.
We've talked recently, and of course, all in the news about
artificial intelligence and the networking and all of these things,
CRISPR gene editing and all of these amazing technologies
that have come into play.
And yes, even nuclear for nuclear power and being able to do these
kinds of things to make the world a better place, make it safer for us
and make it safer for the dolphins, make it safer and better for the environment.
This technologies can go either way.
But unfortunately, what tends to happen is our instincts,
our tribalism, the need for getting that rush of power and success
and accumulating stuff even well beyond the point where it makes
any difference to us.
So we've got to build systems of governance that are capable of dealing
with the fact we are basically just toolmaking animals.
You know, we still have all those drives.
We need to make sure that there are mechanisms in place to manage that aspect
and make the take advantage of the opportunities of all the amazing
technologies that we've developed while minimizing their risks.
And so that's not a good place to be in a lot of ways,
because our current systems of governance are utterly incapable.
And they're, in fact, aggravating and making those technologies more dangerous.
I'm going to come back to that because that's the heart of it.
But you said something else that made me question something.
You brought up dolphins and I'm just wondering the example you gave earlier
that some 10 or 12 nukes exchange would shut down because of the soot
in the atmosphere would shut down the food webs in the Atlantic Ocean.
Yeah. What about the Pacific Ocean?
Would the dolphins there be okay and the food webs just be slightly less affected?
Or is this a global thing or a regional thing?
No, no, that was probably hemisphere.
If you're talking a small scale exchange and I just use the Atlantic as a
example because I'm sitting here just a couple of miles from the Atlantic.
So again, that's narrow human interest.
It's like, I care about the Atlantic, what goes on in the Pacific.
That water is cold and I can't swim in it anymore.
So but yeah, you're right.
It's we're talking about a hemispheric kind of thing for a small scale exchange.
Of course, a global exchange, you're screwing everything up.
Why aren't people in the climate community talking about this at all?
Largely because the majority of the information and the parameters
you need to do these simulations are largely classified.
So that's a major issue.
You can winkle out some of this from public documents.
And that's what the public studies have been done in the last few years.
You go back to even the landmark studies that Carl Sagan and the crew did
with nuclear winter scenarios in the 70s.
So it's possible to do it.
But it's also if you look at who's going to fund that kind of a study,
well, it's probably going to be Defense Department or one of the defense related
think tanks, what are they going to do?
They're going to classify it.
So it's never going to reach the public view.
And especially if you're talking about
doing something that's limiting a weapon system that you feel like
you need to build, then maybe you're not even going to look at it that closely.
So this brings up another question, just thinking out loud.
I'm just starting this podcast.
You're one of my few guests.
But for better or worse, I'm a systems synthesis and I'm trying to put together
energy, money, the environment, the brain as individual and aggregate levels,
governance, politics, geopolitics.
And it strikes me that I want to help society navigate what's coming.
And there's a universe of things that are true and relevant and interesting.
But a big chunk of that are things that create information hazard that actually
won't be helpful if people hear them.
A big chunk of that is things are depressing to people.
And a big chunk of that is things that will get me canceled in culture
because people won't want to hear them.
And what's left is then less relevant to our real situation.
I just had that thought as you were speaking.
Do you have any thoughts on that or?
Welcome to my world, because if you look at virtually any of the areas I work in,
it's pretty depressing.
And if you go in and try to do an earthquake plan for a community and you're
going to tell them, well, you're basically, you're one in 500 chance,
you're going to get wiped out.
The instinct is to say, well, why do anything about it?
