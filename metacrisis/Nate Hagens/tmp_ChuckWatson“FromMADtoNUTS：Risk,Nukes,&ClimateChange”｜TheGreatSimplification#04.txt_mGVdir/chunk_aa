You're listening to The Great Simplification with Nate Hagins, that's me.
On this show, we try to explore and simplify what's happening with energy, the economy,
the environment and our society.
Together with scientists, experts and leaders, this show is about understanding the bird's
eye view of how everything fits together, where we go from here and what we can do about
it as a society and as individuals.
Today's episode, we will be talking with Chuck Watson.
I've known Chuck for about 15 years when we used his damage infrastructure
maps on the oil drum dot com.
Every time there was a hurricane that was threatening the oil and gas
infrastructure in the Gulf of Mexico.
But over the years, I've come to know Chuck and realize that he is one of the
smartest humans I've ever met.
And he pairs his intelligence with wisdom, which is a trait that I really value.
Chuck has worked on risk in many fields related to civilization.
And he's worked for governments, for government agencies and in the private sector.
And to be honest, I don't even know the particulars of Chuck's professional
background, because it's complicated and confidential.
And I'm probably not supposed to know.
Suffice to say that Chuck has become a friend, a colleague, and is on a team of
humans trying to educate and wake people up on the risks that we collectively face.
This episode surprised me because we ended up talking about a risk that most
people don't talk about, which is geopolitics and the increasing chance
of a nuclear exchange.
I hope you learned some things as I did about this increasingly important
underlying risk to our future.
Here's a conversation with Chuck Watson.
So, Chuck, you and I have known each other going on 15 years.
We met when you were doing risk analysis for the hurricane impact on Gulf
of Mexico oil infrastructure when I was running the oil drum.
And we've become friends and collaborators and often compare notes about
the world situation.
But you are a risk expert.
You've spent your whole life analyzing risk.
And my job right now is telling stories about the human predicament to leaders,
government officials, students, the general public.
And risk is kind of a central theme to my work, the human predicament, because
we face so many risks at different timescales, at different levels of consequence.
But how do we even deal with risk?
How do we understand risk?
I don't think our modern world is one that our ancestors ever had to deal with.
So could you just start us off by explaining what risk is and how our society
thinks about it and how we should think about it and we'll go from there?
Yeah, it's interesting because you really can't untangle risk from uncertainty
from the decision-making process.
And in fact, a lot of what I end up doing ends up only indirectly being what
you would call risk in the sense of what are the odds something bad is going to
happen or not happen.
A lot of it boils down to looking at data, looking at the uncertainty in that data,
looking at how do you convert that data into information?
In other words, so let's say you've got this climate model or you've got this model
of the probability of a nuclear war happening or some obscure thing like that.
Well, OK, that model has information, has data going into it.
You're then crunching that data, creating more data, derived data, technically.
Then you're going to look at that and try to convert that into something actionable.
Some information is what I tend to call actionable data.
It's boiled down.
It's something that you think you understand.
You think it's I hate to use the word truth and facts.
And we can talk about that a little bit later because as you're saying,
a lot of the work you do and a lot of your emphasis has been on how us fire apes
try to navigate a modern world.
And I think that's so critical because we don't deal with a lot of high
consequence, low probability situations.
And so if you think of something like a hurricane and a hurricane coming
and making landfall someplace, well, a bad hurricane may be a one in 50 year event.
It may be a one in 100 year kind of thing.
So that's usually at or outside our lifetime experience.
So, you know, maybe we heard about it from our parents or a grandparent or a story
we read about, but it's not really real to us till it happens to us.
So that tends to create a problem that we call risk homeostasis,
which is one of those fancy words and other piece of this that's kind of interesting.
But, you know, nothing happens for a long time.
We make decisions, we do things, we run red lights, we speed a little bit.
Nothing bad happens.
Then that one day when we're speeding or that one day we run the red light
and somebody else comes barreling through the intersection.
Uh oh, well, we didn't get away with it that time.
So then that creates an opposite problem that when something bad happens to us
within our short frame time experience, we overreact to it.
So we tend to sweep the pendulum swings based on our short term experiences.
And so that has a huge problem propagating through the whole process.
We tend to overreact to risks we've experienced recently and
underreact to risks that we haven't experienced recently.
Well, actually we underreact to events that we haven't experienced,
but the risk was still there.
If there was a 10% or a 20% risk of some of nuclear war or something,
but there was no nuclear war, our brains emotionally think that odds
were 0%, even though it was 10% and we navigated it.
So I can think of several examples of where that's relevant and why it makes
yours and my work more difficult, specifically the financial problems with debt.
We've never really had a deflationary Minsky moment in the financial markets.
Every time we've run into problems, the central banks or other institutions
have come to the rescue.
So it seems like we can continue to borrow indefinitely without a problem.
Another is in your field, nuclear war or nuclear exchange.
We've never really had an event like that in the last 50 years,
but the risks have been non negligible.
So what you're saying on risk homeostasis is that psychologically,
our brains over and underreact to our perception of risks.
And in this case with financial and nuclear risk is we've made it through a lot
of metaphorical landmines and therefore our behaviors naturally take on more risk
because we were never faced with the consequences.
Is that correct?
Yeah, exactly.
And you can look at that in lots of fields.
And one of the areas where a lot of research and a lot of really interesting
papers were done on this is trying to study NASA after the Challenger accident.
You had an initial engineering study, which basically said, don't launch below 40 degrees.
Well, they came in and one time did a launch at 38 degrees and everything was fine.
Then you do another launch.
Well, we got away with it at that time.
What we can shave another degree off.
And so you keep resetting your window until it doesn't work.
And then what happened after Challenger, the opposite.
They bumped the criteria up to where they weren't launching.
Oh, there's a rain shower out of the Bahamas.
We better not go today because it's it's potentially risky and we don't understand it.
And so the pendulum swings and we tend to underreact or overreact to and you're
correct in the terminology to the hazard, you underreact or overreact to that hazard
based on your short term experience, even though the probabilities haven't changed.
So you've told me before that low risk often means high consequences.
Can you explain what that means and why that's relevant today?
Sure. So you look at something like, again,
Hurricane is probably the one that people are familiar with that the low
probability events, the one in a hundred year events, if you look at, say,
the U.S. economic system, that's a hundred to one hundred and fifty billion
dollar event, if you look at long term.
Now, that's where you get into a problem because what was again,
using the hurricane example, we are with because of growth,
because of development in the coastal zone, population growth.
What used to be a fifty billion dollar event is now a one hundred billion dollar event.
So I've seen, for instance, Noah just published a new set of billion
dollar disasters. And if you look at it, it looks all front loaded.
It's or back loaded. It's all happened in the last 10 years.
That's just because the coastal zone has two or three times as much
exposure on it. So there's more stuff there to break.
Well, that gets into the whole decision making process as well.
And you can call it the law of unintended consequences or how trying
to fix one thing breaks something else, FEMA flood zone regulations.
So we go in and go, well, you know, we've got this stuff in the coastal zone.
Let's increase the building codes and make it so that you have to build
something that's not as vulnerable. Well, OK,
let's say you reduce your vulnerability by half, but it costs you five times
as much to build that thing. Well, guess what?
Your lost costs, your lost probabilities are actually bigger
after you made this supposedly smart fix than it was before.
So that's where you've got to be really careful in making sure
risk, probability, uncertainty, hazards.
It's a dynamic landscape, the floods we just had in the Midwest.
Again, that's a hazard where if you look at it,
it was maybe a 50 year rainfall event.
But the problem is we had built, there was a Walmart Supercenter
and Distribution Center that was built paved surfaces.
All that water that used to absorb into the ground was now running off
into those creeks and ended up flowing right through that town.
So just because you have an event and you understand what that risk is today.
Now, next year, you may design a 25 year plan based on that risk.
And in three or four years, it may be invalid because something else changed.
So here's a dumb question.
How does the risk homeostasis kind of morph into the risk singularity
in our culture? For instance, we have endocrine disrupting chemicals.
We have climate change. We have biodiversity loss.
We have resource limits. We have financial limits.
We have geopolitical risk.
We have all the things that you're working on and many other things.
If we did the appropriate thing to avert all those risks,
that would be like the entire focus of our economy.
How do we invest a little to protect against these?
And you talk about one in 100 years, nuclear war or climate change
of the of the risks that are being discussed are kind of one in a species
sort of timeline. Yeah.
So how do we even think about all these risks?
And maybe you could explain the concept of how risks are additive or not,
because if you have seven risks and they're all 10 percent chance of happening,
what is the math of that?
Maybe just explain how we think about all this, because to me,
it seems like we're living in a world of unbelievably numerous risks
and they're not all independent.
There's some overlap and some of them trigger others.
How can we think about all that?
Yeah. And that's a problem that goes back to the problem
with how humans perceive their world.
So you hit on a really good one, because a lot of people would look at it
and go, well, climate, you do climate and you do nuclear war.
How do those two mesh?
And of course, they mesh quite a bit and the probabilities are all
interwoven in an extremely complex way.
So what happens if there's even a small scale exchange of nuclear weapons?
And let's, you know, I can think of multiple scenarios,
but one that's on people's mind right now is Ukraine.
And you think of what might happen there where the U.S.
gets involved and starts to lose a conventional conflict.
Well, we've changed the rules of engagement and we've changed the U.S.
has how it thinks about tactical nuclear weapons and has expanded the arsenal
and expanded the situations in which we think it would be appropriate.
It used to be, you think of the 60s and the Cuban Missile Crisis.
Everybody just assumed once you start slinging tactical nuclear weapons around,
it's going to end up going strategic with the big missile exchanges.
Let me pause you right there.
