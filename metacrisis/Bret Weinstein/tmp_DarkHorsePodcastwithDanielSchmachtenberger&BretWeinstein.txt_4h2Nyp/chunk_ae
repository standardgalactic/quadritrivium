So then we say, okay, well, what do we do here? And we see that civilizations fail towards either
oppression or chaos, right? Those are the two fail states. They fail towards oppression.
If trying to create some coherence happens through a top down forcing function. They fail towards
chaos. If not having enough top down forcing function, everybody kind of believes whatever
they want, but they have no unifying basis for belief. And so then they will end up going into
they'll they'll balkanize, they'll tribalize and then the tribal groups will fight against each other.
If you don't want to so and so either we keep failing towards chaos, which we can see is happening
in the West and in the US in particular right now. And then China, which is happy to do the
oppression thing and oppression beats chaos and war, right? Because it has more ability to execute
effectively, which is why China has built high speed trains all around the world when we haven't
built a single one in our country. So either we lose to China in the 21st century and oppression
runs the 21st century, or we beat China at being China, I mean, be it an oppression or it's like,
fuck, those are both failure modes. What is the what is there other than oppression or
or chaos is order that is emergent, not imposed, which requires a culture of people who can all
make sense of the world on their own and communicate effectively to have shared sense making as a
basis for shared choice making. The idea of an open society is that some huge number of people
can all make choices together, a huge number of people who see the world differently and are
anonymous to each other, not a tribe. That was an Enlightenment era idea, right, born out of the idea
that we could all make sense of the world together, born out of the philosophy of science
and the Hegelian dialectic that we could make sense of base reality, and that we could make
sense of each other's perspective, dialectic, find a synthesis, and then be able to have that be the
basis of governance. So what what I think is this is not an adequate long term structure, because
we can talk about why tech has made nation state democracies obsolete, and it's just not obvious
yet, but it has. But as an intermediate structure, the reboot of the thing that was intended has to
start at the level of the people at culture, and that collective sense making and collective good
faith dialogue, because without that, you can't find state without that, you can't find market
incentive. Okay, I love this riff of yours. Okay, I think there's a tremendous amount that's really
important. And the synthesis is super tight. I know people will have a little bit of trouble
following it, but I actually would advise them to maybe to go back through it and listen to it
again, because it's right on the money as far as I'm concerned. There's one place where I wonder
if it doesn't have two things inverted. So you talk about the two characteristics that are necessary
in order for, what did you call it, liberal democracy or whatever it was that you used as a
moniker to function? One of them had to do with the idea that the state was big enough to bind
the most powerful and well resourced actors. And the second was that the people have to be capable
of binding the state. Now, I understood you to say that what failed first was the people's ability
to bind the state. Is that correct? I'm saying that's at this foundation of the stack that we
have to address the failure with recursion. So as I see it, what happened was the power,
the fact that there is always corruption, it's impossible to drive it out completely.
The corruption self enlarges the loopholes and becomes subtle enough that it's hard to see
directly. The most powerful actors suddenly got an infusion of power and we could trace down the
cause of it. But let's just say somewhere in recent history, the most powerful actors became
more powerful than the state. And what they did with that power was they unhooked the ability
of the state to regulate the market. I believe the reason for this was that each individual
industry had an interest in having its regulations removed in order to create a bigger slice of the
pie for it. And so effectively what you had was each industry agreeing to unregulate every other
industry. You can unregulate, if I'm a pharmaceutical company and you're an oil company and you want
to make money but you have to be able to fuck up the atmosphere to do it, and I want to make money
giving people drugs that they shouldn't have and corrupting the FDA, then we'll partner. And so
what you got was many industries partnering to unhook the ability of the state to bind the market.
But one of the things that they had to do in order to make that work
was they had to eliminate the ability of the people to veto. And so this is where we get this
incredibly toxic duopoly that pretends to do our bidding and pretends to be fiercely opposed,
the two sides of it. But in fact, the thing they're united about is not allowing something else to
compete with them for power. So it's the wolf in sheep's clothing is in charge of the thing that
is supposed to be protecting us from wolves. In any case, we don't have to go too deep there.
This is actually super important. Go for it. This is related to the thing we said about
as the market as a whole gets bigger, then the individual consumer stays an individual consumer,
but the supply side, the company gets much larger. As that happens, the asymmetry of the war between
them of the game theory between them gets larger. And so manufactured demand becomes a more intense
thing. Well, the same thing is true in terms of the market capacity to influence the government
and the market government complexes capacity to keep the population from getting in the way of
the extraction. And so there's a heap of mechanisms that happen. And there's not like five guys at
the top who are coordinating all this. It's a shared attractor or incentive landscape that
orients it. Yeah, largely emergent. Yeah. And where there are people conspiring, it's because
they're shared incentive and capacity to do so, which so the conspiracy is itself an emergent
property of the incentive dynamics, which then in turn doubles down on the types of incentive
dynamics that make things like that succeed. So, okay, let's take a couple examples.
If people haven't read it, they should all read at least the Wikipedia page on public
choice theory, a school of libertarian thought that critiques why representative democracy will
always break down that the founders of the US basically said this, which is
all right, we'll come back to cemetery for a moment. At the time that we were creating the
structure of liberal democracy, the size of choices and the speed of them was smaller and
slower, such that the town hall was a real thing. And when the town hall is a real thing,
the coupling between the representative and the people is way higher, right? Because the people
are actually picking representatives in real time that are really representing their interests and
they get to have a say in it. There was a statement by one of the founders of the country that voting
is the death of democracy, because the idea is we should just be able to have a conversation that
is good enough that we come up with a solution and everyone's like, that's a good idea. If we can't,
then we vote, but that means that some big percentage close to half the population feels
unhappy with the thing that happened. And so it's a sublimated type of warfare. It's a sublimation
of violence, but that leads to a polarization of the population. And so the goal is not voting.
Voting is the last step of when we couldn't just succeed at a better conversation and specking
out what is the problem? What are the adjacent problem? What are the design constraints of a
good solution? Can we come up with a solution that meets everybody's design constraints as best as
possible? Okay, so I disagree with this at one level, as I'm sure you will as well. I'm not sure,
but I suspect, but I love something about the formulation that voting is itself a kind of failure
mode, right? That ideally speaking, if you had a well oiled machine, if you had a, you know,
military is the wrong analogy here, but let's say you had a, you know, a ship of people fighting
impossible odds to make it back to safe harbor, right? The point is, you really shouldn't want a
system in which you're voting between two different approaches to the problem. You should want a
discussion in which everybody by the end is on board. And if you tried to do that in civilization,
we'd never accomplish anything, right? You effectively have to give the majority the ability
to exert a kind of tyranny over the minority in order to accomplish the most basic stuff.
But that's because the system is incapable of doing what a better system would do,
which is to say, this is the compelling answer and you're going to know why by the time we decide to
do it. Wait, there's a cemetery here between the conversation that we had about the market
incenting people who focus on the opportunity and not the risks. That's that it actually
suppresses those who look at the risk. Once you say, Hey, there's always going to be somebody
talking about a risk that isn't going to happen. We'll innovate our way out and that becomes the
story. Now you have plausible deniability to always do that. Once you say, there's no way to
get everybody on the same page. We can't do that. It'd be too slow. Now I don't even have any basis
to try, right? And so I don't ever even try to say, what is it that everyone cares about relative
to this? So I even know what a good solution would look like to craft a proposal. No, we're going to
vote on the proposition having never done any sense making about what a good proposition would be.
And that's just mind blowingly stupid, right? And so then who's going to craft the proposition?
A lawyer, a lawyer is paid for by who some special interest group. And so now, so most of the time,
what happens is you have some situation where one thing that matters to some people has this
proposition put forward that benefits it simply in the short term, but it externalizes harm to
something that matters to other people. But ultimately, all of it matters to everybody,
just differentially weighted. And how do we put all those things together? So, okay, we're going to
do something that's going to benefit the economy, but harm the environment. Well, everybody cares
about the economy and everybody cares about the environment. But if I put forward a proposition
that says, in order to solve climate change, we have to agree to these carbon emission controls
that China won't agree to and therefore China will run the world in the 21st century and we all
have to learn Mandarin or be like the Uyghur or something. Okay, well, now I have a bunch of people
who, because they hate the solution space, because it harms something else they care about,
don't believe in climate change. It has nothing to do with not believing in climate change and
not caring about the environment. It's that they care about that other risk so much as well.
But if I said, okay, well, let's look at- It's a negotiation tactic is what you're saying,
that at the point that you want X prioritized over Y, you'll descend into a state in which
you'll make any argument that results in that happening, including Y doesn't exist.
Exactly, because I'm so motivated by this other thing and the solution has a
theory of trade-offs built in that is not necessary. Sometimes the theory of trade-off is
necessary, but oftentimes a synergistic satisfactor could be found, but we didn't try in the same way
that a way to move forward with the opportunity without the risk could have happened. We could
have found a better way to do the tech that internalized that externality. We just need to
try a little bit more, but there isn't the incentive to do it. So let's say we said, no, we don't care
about climate change by itself. We care about the climate and we care about the economy and we care
about energy independence and we care about geopolitics. And we're going to look at the
adjacent things. We're making a choice in one of the areas necessarily affects the other area.
And we're going to bring those design constraints together and we say, what is the best choice that
affects these things together? Then we could start to think about a proposition intelligently.
We don't do this in medicine either. We make a medicine to solve a very narrow definition of
one molecular target of a disease that externalizes side effects in other areas without addressing
upstream what was actually causing the disease. And then the side effects of that med and of
being another med and then old people die on 20 meds of iotrogenic disease. So in complex systems,
you can't separate the problems that way. You have to think about the whole complex thing better.
So the first part of fixing, one part of fixing democracy that we have to think about
is we have to define the problem spaces better, more complexly. And we have to be able to actually
have a process for coming up with propositions that are not stupid and intrinsically polarizing.
Because almost no proposition ever voted on gets 90% of the vote. It gets 51% of the vote,
which means half of the people think it's terrible. And so what that means is you care about the
environment. I care about the economy on proposition A. Well, you petition to get the thing to go
through because you care about the owls there, but I think that you're making my kids poor.
You're my fucking enemy now, and I'll fight against you. Now all the energy goes into internal
friction fighting against each other. And any other country that's willing to be autocratic
and force all their people onto one side will just win. And we will increasingly polarize
against each other over something where we could have found a more unifying solution.
Now, this is fascinating. For one thing, you blazed by it there. But I think so there's a
place where Jim rut tells me that some place that you and you and he overwhelmingly agree also,
but there's a place in which you and he have hung up where he says that you believe that a
properly architected system can do away with the trade offs. No, right? Right. I think I just heard
you give the answer that he must have understood to be that, but wasn't it? Am I right that the
answer there are lots of times when you don't see a trade off, because you have two characteristics,
both of which are suboptimal, and you could improve them simultaneously. And so it looks
like there's no trade off between them. If you push it far enough, you'll eventually reach the
efficient frontier where you do have to choose. But if you're not near the efficient frontier,
there's no reason to treat it as a trade off. Is that? Yes, I'm not saying that we get out of
having constraints. I'm saying we can do design by constraints much better than we currently do.
And so I'm saying that there's a lot of things that we take as inexorable trade offs that aren't.
Well, so you and I will have to chase this down at some point. My argument will be
any two desirable characteristics have an inherent trade off between them,
even if you never see it, right? There are reasons you wouldn't see it, but that if you push these
things far enough, you'll find that there are no desirable things that can be components of the
same mechanism that will not exhibit a trade off relationship. Right? Interesting. Initially,
I don't agree with that at all, but I'm sure you've thought about it a lot. So I'm curious why you
say it. Well, let me give you let me give you the example I used to battle my friend Scott
Pecour over with this, which is he said, why can't you make a car that's the fastest and the
bluest? Right? And it, you know, the first time I heard that I was like, well, okay, maybe blue is
trivial enough, but it's not. In fact, if you wanted to make a car that was the fastest and by
fastest, let's say fastest accelerating, well, you're going to have to decide how to paint it.
If you also decide that there's some color of blue that is bluest and you want the car to
be that color, well, then it has done a lot of the choosing of what paint you're going to put on it
at the point you decide to paint at that color. That paint will have components that will weigh
something, right? The chances that the bluest, whatever you define that to be is also the
lightest and has the best laminar flow characteristics are essentially zero, right? Because
they're an infinite diversity of colors, they will be made out of a wide variety of materials,
and the chances that the blue just happens to be the one that is lightest and has the best,
you know, slipperiness relative to the wind are going to be vanishingly small. And that means
that if you want to make truly the fastest car, its color will be chosen by whatever paint has
the best characteristics. And if you want to make it the bluest as well, you'll make some tiny compromise
that will, you know, probably not matter to you, but it's there. So the tradeoff is there,
even if we don't see it. But here's the thing, Daniel, I discovered many years after my argument
with Scott was long since put to bed that I was right about this. And the way I found out was that
there is a case where the Navy wanted to set the time to climb record for an aircraft, and they
took an F-15 and they souped it up a little bit. And in order to set the basically the vertical
climb rate of this aircraft, they stripped the paint off it. And so if you look at pictures of
this aircraft in its, you know, its record setting run, it isn't any particular color. It's many
different colors because effectively you've got the bare metal underneath with the paint stripped
off it to save however many pounds of paint they were able to remove. Okay, there are three points
I'm up to address my initial thoughts on this here. So one is
with this particular case of a car, the difference between the blue and the optimal color might be
at the boundary of measurement itself. Yep. And so while it's true that there, it might not be a
perfect optimum of both at the level of like a nanoscale optimization, it is irrelevant to the
scale of choice making for the most part. And when we look at something like 100%. And when we look
at something like Tesla cars that became faster off the line than Ferraris and safer than Volvos
and greener than Priuses at the same time, you could see that ground up design, just doing a
better job of ground up design was able to optimize for many things simultaneously so much better.
Now, had they made it less comfortable, could it be faster still? Sure, of course. So it's
optimizing for a product of a bunch of things together, but still in a whole different league
