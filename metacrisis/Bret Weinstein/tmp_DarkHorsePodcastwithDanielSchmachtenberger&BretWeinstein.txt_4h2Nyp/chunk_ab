I didn't look for it.
And then I'm going to have pessimism about my own solution.
So I'm going to try to red team my solution so that I can find out how they're going to
fail before finding out how they fail the hard way in the world, but then not be devastated
by the fact that that solution wasn't it and keep trying.
And I think that's kind of how the committedly inventive, innovative principle works.
So again, we could do almost a one to one mapping of your schema onto mine.
I do this in terms of prototyping rather than red teaming and discovering it's wrong.
It amounts to the same thing.
When you say, actually, it's hard, you and I would have to define two different kinds
of hard, probably, there is hard to make function to stabilize.
And there's hard to figure out what the solution is.
And those are distinct.
We might find elements of both of them here.
But let me just give a, maybe it's the canonical example of a solution to a game theory
problem that everyone will recognize.
I divide you choose, right?
It's the perfect solution to an obvious problem of choice and selfishness, right?
If there is cake and I know that you're going to get your choice and that you are
incentivized to pick the larger piece, then I am incentivized to get the cut as even as
possible.
And the point is it neutralizes the concern.
So we are looking for solutions of that nature.
Now, I don't think they are all that hard to understand in broad terms in general.
It may, there may be a lot of work on the discovery end, but when you see them, they
end up being surprisingly simple.
My biggest fear is that the there is, it is very rare for people to understand how much
danger we're in and why and therefore what solution we are inherently looking for and
how urgently we should be seeking it.
In other words, as long as things function pretty well in the present and people get
fed and housed, it is very easy for them to ignore the evidence that we are in grave
danger, even if we are fat and happy and enjoying a period of good luck.
Yeah, when you, like one of the interesting things in the study of previous civilizations
is that none of the great civilizations of the past still exist.
They all failed even if they had been around for hundreds or thousands of years.
And so to, to understand that civilizations failing is the only thing that's ever happened
and then recognize that since World War Two, we have for the first time a fully globalized
civilization where none of the countries can make their own stuff, that the supply chains
that are necessary to make the industrial and productive base are globalized and that
we're running up against the failure points of a globalized civilization, which it that's
an important thing.
And what's so interesting is that all the previous civilizations that failed had so
much hubris before their fall, because there had been so many generations where they had
succeeded that they had forgotten that failing was a thing.
It was just some ancient myth that didn't feel real.
So we don't have an intuition for things not working or for catastrophe because we
haven't experienced it and our parents didn't experience it and it's only myth.
And as a result, we just make bad choices.
And I mean, this is where studying history and studying civilization and collapse is
really helpful.
And you can see that even as the system starts to fail in partial ways, to me, it seems
very clear that when we look at the George Floyd protests turning into riots over the
summer that happened, they were following the COVID shutdowns and specifically all the
unemployment from it.
When and whenever the unemployment goes up, whenever the homelessness goes up, when
people can't, when the society makes it to where people who are trying can't meet their
basic needs, then it gets a lot easier to recognize there's something wrong with the
system as a whole and go against it.
But we also never had a point in human history where it was like, no matter how
outraged I am, all I have to do is start scrolling for a second.
And I forgot on everything, not to mention the fact that I'm probably on opioids and
benzos.
And so that, that makes it to where the frog can keep boiling in hot water longer.
Yeah.
So I, I often say that people are too comforted by the idea that people are always
predicting the end of the world and it hasn't happened yet, because in fact, it
happens all the time, right, the ends of these civilizations.
But it's even worse than the analysis that you and I appear to agree on here because
many of those civilizations that have ended, in fact, most of them, the
civilization, the organizational structure ended, but the people didn't, right?
So the Romans continued on as other things.
The Maya are still with us, right?
They are not with us as the Maya.
And the point is actually in this case, the jeopardy that we are creating is to our
very capacity to continue as a species, not just to our ability to continue with
the structures that we have built.
So not only are we all in it together this time, but we're all in it in a way
that we never have been before, or at least very rarely have been before.
And that really ought to have people's attention, but you're right, the
capacity to distract ourselves from it has never been better either.
I think something that I find particularly important when thinking about catastrophic
risks now relative to previous examples of civilization collapse is that until
World War II, we couldn't destroy everything.
Like we just didn't have enough technological power for catastrophe weapons.
And so you could fight the biggest, bloodiest war, violate all of the rules of war,
and they would still be a local phenomena.
And with the invention of the bomb, we had now the new technological power to
actually destroy habitability of the planet kind of writ large, or at least
enough of it that it was a major catastrophic risk.
And on the time scales that you think about as an evolutionary biologist of
how long humans have been here and the proto-humans since World War II is no
time at all to have really adapted to understanding what the significance of
that is.
And the only reason we made it through was because we created an entire global
order to make sure we never used that new piece of tech.
And in all of history, we always used the arms that we developed.
And so we made this whole Bretton Woods world and mutually assured destruction
that said, okay, well, let's have so much economic growth so rapidly that
nobody has to fight each other and they can all have more because the pie
keeps getting bigger.
But that starts running up against planetary boundaries and
interconnecting the world so much it gets so fragile that a viper,
a virus in Wuhan shuts the whole world down because of supply chain,
interconnected supply chain issues.
So that thing can't run forever.
And the mutually assured destruction was one catastrophe weapon and two
superpowers.
So mutually assured destruction works, the game theory that works.
Well, as soon as you start to add to that the bio weapons and the chemical
weapons, the fact that bio weapons can be made very, very cheaply now with
CRISPR gene drives and things like that, grown weapons.
We have dozens of catastrophe weapons held by many dozens of actors,
including non-state actors.
And that just keeps getting easier.
Mutually assured destruction can't be put on that situation.
It doesn't actually have a stable equilibria.
So now we have to say, how do we deal with many, many actors having many
types of catastrophe weapons that can't have a forced game
theoretic solution with a history where we always used our power
destructively at a certain point?
How do we deal with that?
It's novel, right?
Like we have no precedent for that.
Yeah, it's absolutely novel.
I mean, when I became cognizant, let's say 1975 is where I first started having
coherent thoughts about the world.
That was only 25 years after the end of World War II.
And it seemed like World War II was a very long time ago.
But of course, we've covered that distance twice since then.
So the ability for the tools with which for us to self-destruct as a result
of aggression are brand new.
And you're absolutely right.
The thing that prevented us from using them, that force disappeared.
It no longer exists.
There's no stable equilibrium here.
So what's protecting us is not well understood at best.
And then add to that all of the various industrial technologies that we are now
using at a scale where they imperil us.
And I don't know about you, but I keep having the experience of a catastrophe
happens, and that's the point that I get alerted to some process that is very
dangerous to humanity that I didn't know about until the catastrophe.
This has happened with the financial collapse of 2008.
It happened with the triple meltdown at Fukushima.
It happened at Alizzo Canyon.
I believe it has now happened with COVID-19 and gain of function research.
And the point is it paints a very clear picture.
We do things because we can't see why we shouldn't.
Or this is also a game theory problem.
Those who can see why we shouldn't don't, and certain number of people don't
see why we shouldn't, and they do.
And we all suffer the consequence of their myopia.
And so on multiple fronts, we are playing, we are rolling the dice year after year.
And the people who can think independently, looking at that picture,
looking at the series of accidents, looking at the hazard of something like
a large-scale nuclear exchange without an equilibrium to prevent it.
Those people wake up, but the problem is the mechanism to actually begin to steer
the ship in a reasonable direction in light of these things doesn't seem to
exist for reasons I've heard to explore many places.
So what does it mean as far as you can tell?
Well, there's one thing that you said that I think is worth us addressing first
is that some of the things that caused the catastrophe either were unknown,
or those who knew them were, game theoretically, less advantaged than those
who were oriented on the opportunity rather than the risk.
Because those who orient towards opportunity usually end up amassing more
resource that equals more ability to keep moving stuff through.
There is an article and a conversation in the less wrong community about
regarding catastrophic risk, a mistake theory versus conflict theory.
What percentage of the issues come from stuff that we knew would cause a problem,
or at least could cause a problem, and game theoretically, we went ahead with it
anyways versus stuff where we just couldn't have anticipated or really didn't anticipate.
And I think it's fair to say these are both issues, right?
There's true mistake theory stuff, like we just couldn't calculate,
and then there's true conflict theory stuff.
We knew that escalating this military capacity would drive an arms race
where the other people would, that if we calculated it,
there's an exponentiation on all arms races that takes us to a very bad long-term global situation.
One of the insights that I think is really interesting is that the fact
that the mistake theory is a thing and everyone acknowledges it,
ends up being a cover, a source of plausible deniability for what's really conflict theory.
So we know there is an issue.
We pretend not to know.
We do a bullshit job of due diligence and risk analysis,
and then afterwards say it was a failure of imagination and we couldn't have possibly known.
I have actually been asked by companies and organizations to do risk analysis for them
where they did not want me to actually show them the real risk.
They wanted me to check a box so they could say they did risk analysis,
so they could pursue the opportunity.
And when I started to show them the real risk, they thought we don't want to know about that.
And so when it comes to the, could we have possibly factored that?
So, I mean, a classic example I like to give, because it's so obvious in retrospect,
is could we have known in the development of the internal combustion engine
that making streetcars, which seemed like a very useful upgrade of having the horse internalized
to the carriage would end up causing climate change and the petrodollar and wars over oil
spills and mass and ocean oil spills, whatever.
It seems like that would have been hard to know 100 years in the future that it would do all that stuff.
And this is a classic example of also where we solve a problem and end up causing a worst
problem down the road in the nature of how we do it, which you can't keep doing forever.
The story is, oh, we cause a worst problem, then that's the new market opportunity to solve that
problem and the ongoing story of human innovation.
But when you start running up against it, the problems are actually hitting catastrophic points.
