Well, let's get going. So welcome everyone. Nice to see so many people. Well, it's online.
So this is an afternoon devoted to applied category theory. A lot of you in the audience
would have heard me speak about category theory before. What is applied category theory is
going to be the question. Okay, so I'm interested in getting philosophers on board. I fell
in love with category theory 30 years ago. When you fall in love with something, you
kind of want everybody to see the beauty of the thing you fall in love with. It's been
a bit of an uphill battle, perhaps, to get philosophers interested. So when Yale University
announced officially replacing the grad program logic requirement with a broader formal methods
requirement that they can go off and study logic probability stats game theory, I tweeted back,
teach them category theory. Why not? You can have all of those at the same time. If you just give
them a basic basic grounding in category theory, we can do all of the above with just a few little
add-ons. So, yeah, I mean, philosophers typically through their training almost inevitably will
encounter propositional logic. And some first order logic depends where you are, perhaps a
modal logic too, because they'll be dealing with metaphysics and I want to know about the issues
to do with this esteem possibility and so on. If some people will be exposed to second order
logic, perhaps some probability theory, decision theory and so on. Unlikely, perhaps, I mean,
this isn't a scientific survey, I guess, I hope people will pop up and say, oh, my department,
we do this, this, this. It may be, but I mean, I think in general, it's probably fair to say that
something like type theory would be unlikely to be studied in a philosophical training. And
category theory really very, very unlikely. Okay, now, of course, you can be exposed to these
things in different ways. And I'm kind of more interested in bringing about quite a shift in
attitude, shall we say, because if you approach things in a certain kind of way, if you approach
it from the top down, you could be exposed to category theory just by a well, you know,
I've learned some first order logic and now I can express category theory as some sort of first
order theory, you know, always to do with a collection of things, a collection of objects,
a collection of connections, arrows between these objects, and satisfies this and this and this
property. So you could see it as just a sort of specific theory within a kind of existing
formalism. But if you do that, you don't really kind of get at the heart, the heuristic heart of
the matter, you don't get the mindset that comes from the sort of correct destruction into
category theory. And that's more what I'm interested in. So we talked about, I mean,
there's a slight controversial, the lack of this language, we'll see what it's come to mean
this applied category theory later. But you know, there've been applications of category
theory right from the beginning, very beginning. So it gets going in the 40s, within mathematics
itself. There's this great effort to systematize sorts of situations when you've got a kind of
kind of entity, maybe, maybe like a category of spaces or collection of spaces. And you want to
know things about the spaces. And they're very hard to deal with in themselves. But if only you
could transfer them somewhere more amenable to you, maybe into some sort of some collection of
algebraic objects, you could resolve questions back in the geometric side, the spatial side,
by this transfer over to something algebraic. Okay, so you're finding these sort of patterns,
these common patterns appearing in different areas of mathematics. From the 40s onwards,
this becomes a popular approach through the 60s. I mean, these are, yeah, these approximate dates,
you'll find exceptions all over. But you start to get people thinking, you know, this category
theory is pretty good. Maybe I didn't need to see it in terms of, oh, it's just a kind of
a category that's sort of set with some properties. Maybe I can make it its own foundations.
I'm recently departed, William Levine in the 60s, who suggests exactly that the category theory
could provide its own foundations. And later on in the 60s, he's even so absolutely fundamental
construction. Adjoint adjunctions, adjoint punctures. So this is very much when you've got a
situation and you've got to sort of transfer in one direction from one space to another space.
And it might not be the case that you can just reverse that process and head yourself back
in the second place back to the first place. But then you could do something as good as you can
as the sort of best approximation to kind of reversing the process. These things are adjunctions.
And Levine discovers, well, they were there, even in your first order logic, they were there.
Right. Right. And you're very, very fundamental. Those quantifiers you dealt with, those universal
and existential quantifiers themselves are part of that process of this reversal.
Okay. So category theory is sort of seeping right into the roots of the very basic formalisms.
Okay. Then computer science gets going. That date's fair enough. I mean, maybe it's even earlier.
We've got computer scientists here who can tell us perhaps about that.
We'll kind of get a sense, perhaps a little bit of a sense of why they got interested in this.
And then physics perhaps from the 80s.
So we're going to have John Byers when he wakes up. He's in California. So I guess,
you know, we had to push his talk back to five to give him time to wake up.
So what is California? It's pretty early. So I think he's not with us at the moment.
So he wrote a paper with a student of his, Mike Stain, 2009. It's called, it's a Rosetta Stone.
Physics, topology, logic and computation. Just to see that, can you, perhaps the people
want to hear a bit more? You can, so category theory has been used to understand
ideas of physics, topology, logic and computation. So it's always about, you have a kind of entity
of a kind, and then you've got mappings between these entities. That's the thing that interests you.
So physics, you're interested in Hilbert space, you're doing quantum mechanics,
you're interested in Hilbert spaces, and you're interested in operators between Hilbert spaces.
Okay. If you're interested in topology, you might be interested in a space that has a boundary,
an input boundary and an output boundary, and you're interested in the manifold that
joins the input to the output. You're doing with logic, you're interested in having a proposition
and an inference through another proposition. With computation, you have data types, data inputs,
of various kinds, people put in their data and it gets processed, and after you come through the
other end into the output. Okay. And these parallels go on. I mean, so they'll be famously with,
when you put two quantum systems together, they're not just a straightforward product of
their parts, you can just separate them off again, because they're entanglement ideas.
But still, all the same, you put two quantum systems together and you want the space of
these two quantum systems, this tensor product. I mean, topology, you're going to form the
disjoint union, you're going to put your kind of input spaces together, next to each other,
and so on. In logic, it's exactly the same as forming conjunctions, X and Y, and in computation,
the product of data types, and so on and so forth. So you're getting these very similar ideas that
just being manifested right across the board. Okay, so that we got this fun expression at some
point. They've got a bit controversial. Somebody wanted to call it computational trinitarianism,
and some people didn't like the kind of spooky theological tone to it, so sometimes it gets
designated as computational trilogy. Okay, but this idea that these things are just part and
parcel, so it's not just that there's an analogy going on, it's that they really are three faces of
the same thing. Okay, so computation spaces and logic, they're just three sides of the same thing.
With the thought that therefore if you make an advance in any one of these areas, it ought to
manifest itself in one of the other other areas. It's really that tight if you see it from this
point of view. Okay, now we're going to still have to head off and start a different direction.
Very good things are still going on. I devoted quite a lot of my life to this sort of thing.
Okay, so book 2020, modal homotopy type theory, suggesting that it would be a very good thing if
philosophers got on board with this language. Yeah, all sorts of ways I think philosophers
could use these kind of ideas. I mean, people know, there's people work on category mistakes,
people with Wiles ideas have been updated. Everything the computer scientist wants to do
by controlling input data to be of the right type. And this is so close to the kind of
idea of categories that the Wiles were in back in the 30s and 40s and stuff.
Yeah, I'm going to be giving another talk in a couple of weeks on, I mean, it really lends itself
the kind of brand name in Robert Branden's approach to Frenchism and expressiveism.
I think it could be beautifully realized in this program, but that's not for today.
Yeah, another recent bit of work I did, which was really trying to sort of chase up the passage,
the path that takes us right up to the kind of cutting edge of category theory,
type theory meets physics. There's something expressed in this paper here, which is kind of
pointing to this work, just to point out, so this kind of work goes on, this really,
really kind of cutting edge modal type theory meets physics, right up in the highest, highest
heights of, there is a quantum gravity and whether you think that's going to work or not.
But it's coming much, much closer down to earth now, some of the work they're doing out there,
which is looking at quantum computation, sorry, quantum computation is down to earth.
But it's a closer to earth than quantum gravity anyway, using this linear
homotopy type theory, the linear or the linear logic. But again, this is not today's work,
because we're going somewhere slightly different. So yet a further set of applications starts to
take place, let's say from the 2010s, although maybe again you can find precursors. And what's
that looking at? Okay, well these are people scattered through the country, I'm sorry, through
the world, but pretty well represented in the UK. But they're looking at things like causality,
probabilistic reasoning, statistics, learning theory, deep neural networks, dynamical systems,
information theory, database theory, natural language processing, cognition, consciousness,
systems, biology, genomics, epidemiology, chemical reaction networks, neuroscience, complex networks,
game theory, robotics, quantum computing, and the list goes on. Okay, so we're away now from the
very high-end physics applications, and we're dealing with some sort of things that of course
philosophers should be very interested in. Okay, and in a sense it shouldn't be such a surprise,
because you're dealing with both, if you notice back there that list, they're dealing both with
physical processes, the various kinds of natural processes out there in the world, and then they're
also dealing with inferential processes. Okay, so kind of interestingly, the category theory
is good at dealing with both, because they're both to do with movement of some kind, from starting
place to a finishing place. If you want to go all Hegelian on us, you're kind of unifying that
subjective and objective logic. People tend not to use Hegel's language these days.
And yeah, I mean something all over the place, these string diagrams, if you
just click anywhere, any sort of applied category theory, you just get these endless
beautiful diagrams, these 2D diagrams, lots of wiring diagrams going on, the various kinds,
wiring things with nodes, right across the board. And in fact, this is kind of one of the ways that
well, there's certainly, I think, John Biles will tell us about it at five o'clock,
but he was just fascinated by the way you found these very similar diagrams across engineering,
and chemistry, and biology, and so on and so forth. And there had to be something in common
that unified it, and he thought category theory would be the way to find a common essence to
why these diagrams are all appearing across these different domains.
Okay, so you're going to find these all over the shop. Okay, a bit of a version of this
compositionality that that links you off, there's a journal now called compositionality,
which is a journal devoted to this applied category theory. And why it's so appropriate is,
well, there's various things you want to do, some very basic moves is, I mean,
we actually already saw it in that Rosetta stone thing, the idea that if you have one system
and some process of some kind going on that takes you from a beginning to an end,
we can put together two systems in parallel. Okay, we can compose systems in parallel,
and we have to, again, diagrammatically, it's all very obvious why you should do that, you're just
sort of joining two diagrams, and you put it next to each other. And you don't need very much
compatibility between the systems to do this. If, on the other hand, you want to put them
in series, if you want to make one process feed into the second process, you're going to have
to make sure that the output of the first one matches the input of the second one. Okay,
or they're not going to join together. Very, very category theoretic, it wants its inputs
to match its outputs. Yeah, or there could be something a bit more subtle going on,
there could be sort of several inputs, several outputs, and maybe only some of the outputs being
matched by the new input to some other new device, and you could just plug in those wires that match
into the new system. Yeah, or sometimes you can take a wire coming out and bend it around,
that could just move you might want to make as well. Sometimes also, you've got a system
with a networking system, and you've got a little chunk of it, and you want to replace,
you want to take a little of the box inside that system and plug in something more complicated
with the right inputs and outputs. So you can kind of compose by plugging one system inside another.
Okay, so this is, I'm sure we're going to be hearing a lot about this over the next two hours
and a bit. So Toby Smyth is talking first. I'll give John Byer a bit more of an introduction
when it's that time for him. So I was intrigued, I was very attracted to Toby's work,
looking around some of the papers he's made available, and this is his D-Phil thesis,
so that's what I just last year, I guess it was in 1922,
mathematical foundations for a compositional count of the Bayesian brain.
We'll be hearing about that today. So primary motivation in writing this thesis is to lay the
groundwork for well-typed cognitive science and computational neuroscience. So this is pretty
high, pretty interesting stuff. This isn't just simple physical processes just going around
in their simple way. It's tapping into, I feel like I've been looking into a bit of late,
this world of predictive processing, active inference, you know, free energy,
if we head to the Carl Friston, his big name in the field. There's some interesting stuff going
on there, kind of hierarchical Bayesian models involved in the way that we deal with the world.
We make all these predictions about the way that we, you know, most of the world is fantasized
by us, it's just slightly constrained by our influence in some sense. So there's all sorts
of ways, this very strong top-down component to our perception. Interesting stuff. So curious
stuff, I find myself, it would almost plunk the gap. So I've spent a lot of my life, as I'd say,
reaching, had a great theory and type theory, good spell being interested in machine learning,
Bayesianism. I had a former life that kind of pops up from time occasionally, which was an
early interest in psychoanalysis and then co-wrote a book on psychosomatic medicine.
So sort of extraordinarily, there could even be this kind of bridge that takes place between
those two. So there are plenty of people that are, there's a lot of psychotherapists now talking
to this active inference approach to the mind. You know, the brains are kind of optimizers,
the trouble is that very often they get kind of caught in some optimal optimizations. They get
locked in in a certain kind of way. And the heavy top-down processing sort of makes it very difficult
to sort of shift the system and gets kind of locked in in a certain way. I mean, for example,
delusions, for example, cases where there's such a sort of strong top-downness to the
perceptual apparatus that the input from the world does nothing to constrain it or not enough to
constrain from the delusions. That's a very extreme case. But all sorts of psychiatric
problems may be seen in this light.
Yeah, so gosh, if Toby can help us sort of plug the gap there,
kind of find if I could, if I could bridge the gap of my interest. So I'm certainly
very much looking forward to hear what he's going to say today.
I noticed this rather longer bit is taken from his thesis, this well-typed cognitive science
and computational neuroscience. And it's got, yeah, they say, so types, types of what render
these concepts precise, they allow categorical models to be cleanly compositional. So it's again,
it's very much things I mean, sort of plugging together if the interface is right. So I think
this is something that maybe from what I've seen of the active inference literature,
well, I've been true to see if he can make them something more category-theoretically
active inference. Really exciting project. Okay.
That's, yeah, I think that brings us to the end of the introduction because we're
keeping rigidly on time. And here we go. It's 10 to four. So let me close down my thing.
Can you share now, Toby?
How's that? Yeah, that's, that's good. Do we get that full screen here? What do we?
This is so useful. I should always say it.
We can get it mostly full screen, I think, by just getting the cross on them.
We can do that. And we can do that. And there's one more thing I can do.
Well, that's probably enough. That's probably enough. I don't think I can do it in the browser
one. Okay. Yeah. Great. Okay. So my great pleasure to introduce Toby Smyth to talk to us on Bayesian
Brains cybercategorically. Thanks, David. And thanks for the very warm introduction.
I'll try to live up to it. So my, yeah, so my first degree was actually in philosophy and psychology.
And I guess it's possibly thinking also my enjoyment or predisposition to
thinking about things like the mind in this kind of abstract way that possibly philosophers are kind
of, you know, inclined to is what's led me to enjoy using this language that David so clearly
loves using as well. I'm not going to be saying very much on the sort of philosophical side today,
though, you know, I believe, as David does, that this language has a lot to give to philosophy.
What I'm going to be doing really is to talk a little bit about this journey
that David nicely summarized at the beginning that I've been on, basically trying to replace one
kind of confusion about what's really going on with this idea that's called predictive coding in
the computation neuroscience literature and the sort of related set of ideas called active inference
with a different kind of confusion, which is all about, you know, that category theory and
this sort of high dimensional algebra that goes along with it. And I suspect I'm sort of,
I'm in a happier place with that second kind of confusion. It suits me better. But I also think that
it helps us to understand a kind of idea, which at least as far as I was concerned, wasn't very
well, wasn't, wasn't crisply or clearly expressed so far in the literature that had been written
in computation neuroscience, possibly because, you know, fair the interest is in specific models
or specific bits of the brain, whereas my interest is really in just, you know, what brains do or
more generally what adaptive systems do. And what's emerging now in the applied category theory world,
bringing together all those different domains that were listed on that slide earlier,
is this kind of this idea, this thing called categorical cybernetics. It sort of uses this
kind of nice general language of category theory to bring together ideas in, you know,
economic theory and machine learning and reinforcement learning and computation neuroscience
and show what are the commonalities and allow people as a result to talk to each other a bit
better and to understand what they're really thinking about in each other's language a bit better.
And so what I'm going to do is maybe sketch some of this. I'm going to try not to assume too much
background in either neuroscience or in mathematics or particularly in category theory.
I'm going to talk about this idea of the Bayesian brain and this related idea of predictive coding.
I'm going to talk about kind of current understanding of probability theory using this
compositional language. I'm going to talk about dynamics and I'm going to talk about how we can
see predictive coding is basically a way of translating certain kinds of statistical models
expressed in this first compositional language to the kind of compositional dynamical systems
that are expressed in this second language. And so when David was talking about how categories
are really useful for talking about things like translation between, you know, different
mathematical languages, that's really what we're making use of here.
Okay, the first thing I want to do though is to say a little bit more about what I mean
or what people mean when they talk about the Bayesian brain. And this
related set of ideas called the free energy principle and predictive coding. It's a very
sort of prominent idea in computational neuroscience and it may be slightly controversial too,
but it's a claim that most of what the brain does and indeed sort of most of what kind of
living systems or adaptive systems do generally can be understood as approximate Bayesian inference.
And so what that means is that you have a system and it's trying to predict what's going on in the
world, more precisely maybe predict what set of data it's receiving. And then on the basis of the
set of data it does receive, update its beliefs about the structure of the world or what's out there
in order to make its predictions better or more vertical. And this thing called the free energy
is a quantity that you can optimize in order to do inference of this kind in a kind of better way
or to improve your doing of inference. And if you minimize this quantity, at least
in certain contexts, you get dynamical systems which look a bit like dynamical systems that
people had already written down earlier in the literature in computational neuroscience that
relate to various bits of say the visual cortex. And so that alignment says to a lot of people,
okay, well maybe this is what those circuits are actually doing. One of the difficulties you get in
if you're just going straight from the statistical model that you might be interested in to dynamical
systems, you're going very manually, is that there are kind of a lot of moving parts and
particularly if you go from one situation to another you might have to
re-derive all of your dynamical equations again. Even if when you look at the brain it has some
kind of nice modular structure and if you look at these statistical models so they do too.
And this is a kind of hint of compositionality and so it seems to be kind of a kind of neater
thing to do to see what the essence is of this process and to try to make this translation
once in a compositional way so that you don't need to do it over and over again
and end up with very sort of impenetrable papers full of difficult equations. You just end up with
impenetrable papers that require you to know a language that you maybe didn't already know.
But I think what I hope is that people start to learn mathematics in a compositional way
in the future and so that these papers or this work that is expressed in this category
theoretic language doesn't scare people off in a way that it might do now. I can hear some
That's fine thanks. Okay so I mean I don't want to spend too long talking about
why category theory because David's done a very good job already. I'm just going to say some
generalities and hopefully they sound nice and convince you that this is not a crazy thing to
be doing. That category theory is like the mathematics of composition of sort of pattern
because it's about translation, it's about interconnection, it's kind of about metaphor
too. It's about saying okay well here's this pattern it crops up in lots of different places
and maybe this is like this other thing and that's kind of like a kind of translation.
And as we've been indicating that it seems to promise to supply a kind of lingua franca
for mathematical science generally so that you know each different sort of category of systems
could be like a little library in this in the sense of computers and computer programming
that you can import it and have access to all the kind of nice tools that are available but
because it's in this common language they all these different parts plug together very nicely.
And that's you know this happens because categories they collect into this bigger category of
categories so they it's that kind of supplies its own foundations as David was indicating
and this allows us to use the morphisms between the categories themselves these known as functors
to translate from one of these libraries or mathematical universes to another and thereby
sort of connecting them together but so that's kind of like one major benefit of using this
language and another one is that it kind of because you have to say like what are the interfaces of
your system like where do they start and where do they end or you know what do they connect from and
to it kind of enforces this discipline it kind of enforces a kind of clear thinking it enforces
an effort to carve nature at its joints in this kind of platonic way.
Okay so to make a little bit more progress at least with the mathematics I want to try to say
briefly what a category actually is it's a collection I often do it with C for category
like this kind of with this little tick in it it's a collection of objects so these are the
things that are being related often denoted C0 and then for each pair of objects you have a set of
ways of relating them to each other it's a kind of it's a set of edges or morphisms
so if you write them like this it goes from A to B and the key detail is that if you have a way
from going you know away from way to go from A A to B and then B to C as I've drawn down here you
get a composite morphism like this and so it's a bit more than I mean it's quite a lot more than
just a graph it's it's a graph where the edges compose and this is where the compositionality
comes from there's also an important extra detail which is that each object is witnessed by a morphism
of its own called its identity and then you've got some axioms that say okay well this composition is
associative and it's unital with respect to that identity those kind of details aren't super important
from what I'm saying today but I think I just want you to have in your mind this notion of
compositionality and particularly compositionality of morphisms once you've got this structure
you can start to study it in itself as David was also talking about and one of the things you
can do one of is prove this fundamental theorem and it has a sort of quite in some sense trivial
but very deep and sometimes confusing proof it's called the yuneda lemma and it and it says something
that's very familiar to us in everyday life and this is a kind of a maxim from a linguist I think
called Frith who said you shall know a word by the company it keeps and the yuneda lemma generalizes
this to all these categorical contexts that you shall know an object by the company it keeps in
the sense that if you know how a thing is related to the other things in its milieu then you can
identify the thing itself and that you know we know not just from linguistics but if you know
how to get to a place from the places around it then you can you know figure out what where that
place is on the map so you can sort of identify that place and it's a very it's a kind of very
it's a very powerful result and it's a kind of nice kind of nice evidence that category theory
captures something about the way the world is built up okay but the thing that you know I want
to talk about really today is the brain and using the tools of categorical cybernetics to model it
so the brain is this nice complex system it's made up of other complex systems so it's made up
of neurons which of course have a lot of structure of their own and it has this kind of computational
behavior it does prediction and it does planning and it does all these things that we have programmed
computers to do now and you know brains themselves compose into bigger systems societies so it has
a lot of features that make it again that seem to suggest that it should be amenable to description
to description in this categorical language and indeed it should be comparable to other systems
that people have started to study in category theory or in applied category theory cybernetic
systems this really I guess at least in this form kicked off with the the introduction of
compositional game theory by Jules Hedges and his colleagues and so there they study economic systems
and those are very closely related to systems of reinforcement learning
and indeed structurally also related to systems in machine learning as it is
in the form that you know is given rise to things like chat gbt and these things all have a similar
kind of structure there's some kind of process which might have some kind of control inputs
and then you know there's another part which goes backwards the kind of retrojectory part
a sort of feedback process and the process and possibly the control and the feedback
you could think of them as all being contained within one system that is in kind of traditional
cybernetics maybe called a plant or in control control theory called a plant but this is really
just like a part of a bigger system it's open to some environment and the brain has you know
similar structure at least in these kind of visual cortical areas that have these predictive
coding model models that I introduced at the beginning and so there like a little bit of
neural circuitry maybe has one part which points from deep inside the brain here to out towards the
world here and it does say prediction about what what you know the eye should expect to receive
and then there's another set of neurons which goes from nearer the world to deeper inside the brain
that that feeds back the error in that prediction and allows the brain to form a sort of updated or
new belief in Bayesian probability it's called a posterior belief and so you have this kind of
bi-directional process which looks a bit like a kind of cybernetic loop and so I'm interested in
not just the brain of course the brain is a great target and obviously I you know pardon me as a
brain I want to understand that but I'm interested in systems that do this kind of thing adaptive
systems in a sort of general way and because of that I'm I'm keen to use these general tools
and to study this system that is so good at doing this kind of adaptation which is the brain
okay and so you've seen me draw on the previous slide a diagram and it's a diagram of the kind
that if you know about category theory or you did some searching as David was suggesting
is known as a string diagram and so I want to introduce categories again or at least composition
again using this language so that when I use these diagrams again a bit later you'll know what I'm
know what I'm doing the great thing about using string diagrams is of course for most of us
we have this very well developed visual system and so we can not only use category theory to
reason about it but we can also use it to help us reason about mathematical things like categories
themselves in particular you know when we use pen and paper it's a two-dimensional medium and it's a
kind of often for processes where things can go in parallel as well as sequentially sequentially
it's often more convenient to to draw the processes out as we see when people draw other
kinds of more informal diagrams than it is to write them in this kind of more sort of traditional
algebraic symbolic way that you could write on that drawn on the right hand side and so what
we can do is if we have a process F that goes from A to B so we're going to start by reading on the
left and then if you have another process G that goes from B to C we can plug them in and so then
you have a process that's G after F which in traditional mathematical notation is denoted
like this and so that's like the sequential composition and we can also put another process
H from X to Y by the side of it and so this thing now goes from the tensor A tensor X to C tensor Y
and you have as I said each object has an identity morphism and this is just what the string itself
represents and you can put this next to you know processes and you know you can use this to plug
things together you have special processes that start in like the empty diagram which we write
with this I and they we think of as like states so in physics they will be like vectors or in
probability theory they're like distributions and then you have things that go the other way that
sort of terminate in an empty diagram and these might represent like say predicates on some space
or they might represent effects that you could measure or measurements on your physical system
and these are kind of morphisms in the other direction into this other into this unit diagram
into this unit object and with these diagrams okay you can not only sort depict the processes
themselves but you can write down equations and start to manipulate the diagrams accordingly
and so here at the bottom as an example I've written the equation that represents the co-associativity
of of copying so it says if you copy once and then you copy one of the copies it doesn't matter
which one of the copies you copied and you know this has this nice kind of fairly obvious graphical
representation here which is a bit more I think natural to use at least for this kind of thing
than the sort of algebraic sort of symbolic notation that maybe is more familiar to people from
mathematics. One of the things that we can represent in this language is probability theory
and this you know the kind of natural kind of like you know mathematical formalism to use for
that is that of Markov kernels which you could really think of as like conditional probability
distributions and so here I've got this thing c it goes from x to y and we could think of this as
like a conditional probability distribution of y given x but now we have them these things in this
compositional language we can plug them together so if we have one and then another we can compose
them together by sort of marginalizing over the middle thing and this says okay well we'll sample
an output we'll sample an output from y and then we'll give it to z give it to d and then we'll
run whatever process d represents and that'll give us something in a distribution in z and so this
is like some something it's like d after c z given x we can do copying in probability too and this
is like you know it just sends a thing to the delta distribution on the copy x equals x prime
which you might be familiar with and so here we can and we can use this to formalize in this kind
of graphical language joint distributions and so here we've got a joint distribution with one thing
on with one marginal on x and a kind of what's called a probability a likelihood which goes from x
to y and this thing depicts the kind of this sort of this representation of the joint distribution
is like a likelihood a conditional probability distribution on y and a prior and this is the
kind of one fundamental part of Bayes law that you might be familiar with and as I indicated on
the previous slide these kind of co-states or effects they represent things which we could
think of in probability is like fuzzy predicates and that might be something that if you're
philosophically inclined you you've thought about we're not going to use them very much but we will
use them to represent loss functions in machine learning and in predictive coding later on
okay so at the heart of this kind of predictive coding idea is Bayesian inference and what Bayesian
inference does is effectively takes one process here this goes we're reading the diagram upwards
now just yeah first reasons of space here this goes from x to y and so we can think of this
as predicting something in y given something in x but you know it's a kind of stochastic prediction
it's fuzzy and so Bayesian inversion allows us go to go instead of from this process which goes from
x to y to a process which goes from y to x here and so this is like reversing this process it
allows us to say okay given what I observed what should my belief what my belief a better belief
have been and the the criterion that it satisfies is that the kind of these are two ways of writing
the same joint distribution so you start with a prior belief that says okay well this is how I
think the world is and then you have this way of generating data and this says okay well this
the joint distribution you get over the say causal structure of the world and the observations is the
same whether you go forwards or backwards and so this this kind of makes it like a well behaved
inversion so it's a kind of it's a kind of fuzzy way of inverting this this this predictive process
and Bayes law is often written like this thing it's just a way of expressing the conditional
probability in the other direction and it just comes out of the kind of laws of probability
that you might be familiar with this is the product rule of distributions
but I'm not I'm not here to give a tutorial about you know about fundamental probability theory I'm
going to try to build up to predictive coding I'm start I've already introduced categorical
probability theory and I want to get from there in the kind of piecemeal way from like simple steps
using like simple you know simple like compositional parts towards the dynamical systems that represent
how the brain does this inferential process actually dynamically
and so as I indicated you know a brain it has lots of parts and they sort of plug into one
another in particular it has some parts of the brain which are nearer the retina or the eye
and other parts of the brain which are kind of what we think of as more sort of high level
that represent you know objects or concepts rather than say what we might think of on a
computer as pixels and so somewhere in the brain you have to go from these kind of low level features
to the high level you know high level ideas and this happens in this kind of hierarchical way
so zoom me out piece by piece and so you know you might imagine being say at the cinema
and you're sitting right at the front and you're looking at you know looking at the
the screen that you can only fit in your visual field say a part of the screen at one time
but yet you can sort of keep track of what's going on everywhere and so you maybe have this
process where you've got you know a belief about what's happening on the whole screen
and also not the process where the eye is receiving signals from part of it and
the first thing that happens is your brain updates the belief about what's going on in that little
part and then it can update using that new belief yeah the you know the some high level
belief about what's going on on the whole you know in the whole scene or in the story more
generally and this is a hierarchical process and really it's got this kind of compositional structure
and so we might wonder okay given in this kind of abstract language a sort of kernel
from x to y and another one d from y to z what its inversion is so what is the inversion of the
composite channel or kernel and how does it relate to the inversions of the parts and so this is a
kind of question about the compositionality of Bayesian inversion and it's not only relevant
and predictive coding of course it's relevant in all sorts of contexts particularly in you know
say things like probabilistic machine learning and things like that and so it's actually quite
easy to show that the composition of of the inversions gives you the composition of gives
you the inversion of the composite channel an important thing to notice from these pictures
is that the inversion say if we just go back to this this this kind of defining law this is
what we could think of as a kind of abstract Bayes law that the inverse channel here the whole
law in fact but particularly the inverse channel depends on the choice of prior because if I
change this if I change it to pi prime it wouldn't necessarily satisfy this law and so there's a
kind of dependence here you see it comes in the in the in the sort of defining equation here you've
got pi popping up and so we can't just sort of in some sense reverse the channel directly
we need to say okay well what is this prior that I'm reversing it with respect to and so we'll see
okay well if we use this law twice we can you know invert first of all d but you see we've got c
coming in first and so that the the prior for this inversion of d is c after pi and if we invert you
know if we invert if we invert oh sorry we should invert c first the prior is pi and then we invert
d afterwards but the prior is c after pi we get this composite inversion over here
and so this says okay well we can decompose the Bayesian inversion of composite channel
in terms of the inversions of the factors here and here and in particular this kind of
this kind of structure of this dependence what is this this thing how does this arise if we try to
package this up neatly we will end up using what is called a lens and this is something that's
already been used in categorical cybernetics and in categorical dynamical systems theory
and in all sorts of bits of category theory already to formalize these kinds of binary
processes and so it's very nice and so maybe somehow mathematically inevitable but this
kind of formalization also happens to be the way that Bayesian inversions compose
and so we can say a little bit about this so I've indicated that these inversions
see this is the inversion of the channel c on the previous slide c x to y the inversions they
depend on the priors and the prior is a state on the domain of the channel
and so we call the inversions which we might write as just c dagger it's dagger for you know
technical reasons it's it because it effectively makes up what's known as in other bits of category
theory a dagger functor but this thing is like a state dependent channel and so it takes in a
prior and it gives us a channel in the backwards direction so this is a function from you know
states to channels like this and in fact we can compose these things if we have a state
dependent channel from a to b and it depends on x and another one from b to c that also depends on
x we can copy the state that we depend on and give it to both parts and then compose those parts
together and that you know is something that should be hopefully quite natural to see in this
diagram if we sort of follow along the flow of the information through it and this means that
the state dependent channels on a particular state so these all depend on distributions on
x but these things form a category and we can you know write down the morphisms the set of
morphisms in this way so not only though do the state dependent channels on x form a category
for each x but also if we have a process you know from y to x we can say reindex these these
state dependent processes so now we can pre compose the inputs to f and f prime g prime over here
by say this morphism from y to x so now if we have a state on y we can also feed that through p
and then feed that that composite state into f prime and into g prime and still do the same
composition and so this gives us a mapping indeed a functor from the category stat x to stat y
and so that's going to be important because as we've seen here it maps like this
but what we did before is we had an inversion on d and we pre-composed it with c to give this thing
d dagger c after something and so this this this reindexing captures what's going on
in the composite inversion story
and we can sort of you know use category theory or categorical tools to package all
of that stuff up very nicely um we can pair an individual predictive channel
c with its state dependent inversion c dagger and a pairing of a channel or kernel
with an inversion which may or may not be the exact Bayesian inversion that satisfies that equation
but a pairing of that kind is what I call a Bayesian lens and I don't have time to explain
how we obtain the category of these things but it is obtained using a sort of canonical tool
of category theory called the growth and deconstruction so we had this index category stat
of state dependent channels a family of categories for each possible space x
that's compatible with this reindexing if you have an index category of that kind
you can apply the growth and deconstruction and that gives you another category that is a you know
the morphisms which are pairs like this and there's a kind of composition rule that comes
out of the growth and deconstruction which tells you how to compose these lenses and it in fact
captures exactly what's going on with Bayesian inversion and so Bayesian inversion at least
its compositionality has this nice and sometimes universal property that because it because it
arises in this kind of abstract way it's not an ad hoc thing that depends on the situation at hand
or the kind of problem you're studying it's it's a kind of mathematical consequence of the structure
of you know what's going on in Bayesian inference and you know we can use string diagrams to really
spell out this compositional structure and it captures the picture I drew earlier of this
predictive and updating process so now we have these two two little lenses here and here
and so one now you know goes from x to y and we can think of this as doing like a high level prediction
and then here we've got another goes from y to z and we can think of this as being this
lower level prediction and we can compose those things together to get a prediction that goes
from the high level to the lowest level and we can also follow the information and flow backwards
so now suppose we've got some feedback from the world here well okay we flow through
the forward information and this gives us the kind of prior
d c after pi and then this gives us an updated intermediate level belief
and now say we had pi here now the inversion here takes in pi c prime pi and this gives us the
higher level belief and this this thing here is exactly what we had from basal and so this is
what I mean when I say that you know the Bayesian updates compose optically they compose according
to this lens pattern and as I've already indicated this this comes up a lot in categorical cybernetics
in fact this kind of chain rule is exactly how that propagation works and it's it's how sort of
utility is passed back in economic games and so it's interesting that this this kind of nice
general construction captures all of those all of those all of those situations and in such a
way that actually applies to this physical system that that is the brain
so I've said that there are some of these lenses you know c c dagger where this is
some state dependent channel that actually satisfies basal law as I drew it a couple of slides ago
but there are lots of other lenses where c prime has this type the same kind of type say it goes
it takes in oh sorry takes in a y and it depends on some prior in x and it gives you some new
updated x there are lots of lots of state dependent channels of this form and they're not all
exact Bayesian inversions we could think of them as like approximate inversions
the way that we could formalize this chain rule in in the kind of neatest way of exact
Bayesian inference is as what's called a section of this vibration so Bayesian lenses
form a category where the morphisms of these pairs and we can sort of project out we can forget
this part just to end up in c and a section is effectively a mapping from c into this
vibration above it such that if you do this round trip you get back where you started
and what makes a mapping a functor is that it preserves composition and so what's going on here
is that we map c to c dagger and we also map d to d d dagger such that if we map dc
we get the the inversion of dc which as we've already seen
is this thing c dagger after d dagger c
and so the fact that this this mapping of channels to lenses where the second part is
the exact inversion is a section exactly says that Bayesian updates compose according to this law
now again this isn't the only section of Bayesian lenses there are others and those ones represent
approximate inversions that are particularly well behaved because they satisfy this
compositionality property and I call those things inference systems and and finding an
inference system is like the first step in in sort of defining the approximate inference process
that is predictive coding but you know once you've got an arbitrary inference system you
you know it might not be very good at actually doing prediction or updating of beliefs if we
think of Bayesian inferences like the optimal such process and yet nonetheless you know evolution
hasn't just picked brains to be instantiating some like arbitrary inference system it's chosen ones
that are actually quite good at doing prediction and updating and so there must be something going
on that allows evolution effectively to measure how good a system is and you know that's what
natural selection is doing and similarly there must be something on something going on that
allows brains to improve their predictions or effectively to learn and what I'm going to talk
about now is how we can equip each Bayesian lens with a loss function effectively a way of measuring
this this this goodness of a prediction but because we have this compositionality hanging
around it satisfies some nice you know some nice constraint that is enforced by biology which is
that neuro computation is what you know what biologists or neuroscientists say is local that
means that the the data required of the computation must be sort of available to the neurons themselves
it shouldn't depend on something that's going on further away in the brain the information has to
be accessible in order for the neurons to actually do the computation they need to do
and so because we have this compositionality if we enforce that of say the loss functions or the
lenses as well that it that makes all of the the information local to the individual parts of
the system and so that makes it kind of nicely biologically plausible thing which is a bit
different from what happens in say back propagation machine learning where changes to some parts of
the system may depend on the way you measure how good the performance is sort of filters by
sort of filters back to the whole system at once the loss functions there aren't necessarily
compositional another thing that's important to bear in mind at least when it comes to
approximate inference systems is that their performance kind of depends on the problem
they're trying to solve so obviously evolution has selected different organisms for different niches
and so you know if you change an animal from one niche and you put it in a different environment
it might not survive as long and that's because you know it's yeah we could say that abstractly
that's because its inferential performance is kind of context sensitive it depends on the
beliefs it has about how its environment is structured and it also depends on like how it
actually you know yeah absorbs these observations that it has you know what those observations are
it might be it might not pay attention to certain things which in one environment might be less
important than in another one example of a loss function that's particularly natural and relevant
in probability theory is known as the relative entropy or the Kuwak Leibler or KL divergent
and this measures the difference from one or it kind of measures the distance or divergence
from one distribution to another on the same space and we'll see in a minute that these things that
these relative entropies also themselves satisfy a nice compositionality property which again comes
out of a grid deconstruction and so in its structure is also kind of captured by this
abstract categorical machinery so one of the reasons the relative entropy is so important in
approximate inference that you may know or have already guessed is that if we minimize the divergence
say between this approximate posterior which I've written as C prime pi and the exact Bayesian
posterior C dagger pi effectively what that does is it pushes the approximation closer to the kind
of true or the optimal the optimal inference and so often people start an approximate inference
process by trying to figure out a way to minimize this quantity but you can see here again that
we have this dependence of the this quantity this divergence on the prior because of course
the inversions depend on the prior and on the observation so why is the thing that goes into
yeah the system like this we had this diagram before
and so the actual divergence in any particular context depends on what the observation was
and so if what we want to do is compose these these loss functions for one say one little neural
circuit and another little neural circuit we should you know figure out a way to pass the
information of this observation and this prior into those loss functions
if we write down the type as I said at the beginning this kind of discipline about types is
one of the things that makes category theory to me so useful if we write down the type of this
function we'll see that it corresponds to one of these effects or co-states that I drew earlier
as long as you think of it also depending on this prior so what I think I call that a state
dependent effect effectively it's a state dependent channel that lands in this kind of trivial kind
of empty diagram object here what that means if we interpret this type in the mark of kernel
language that I introduced earlier is that it's a function that takes in a prior and then an
observation and it gives us back a number and in particular it could give us back this relative
entropy and one thing that is known about the relative entropy and in fact is quite easy to check
is that it also satisfies the chain rule the chain rule has this form but I you know don't
need to go into details about what it mean what what it except to say that effectively the composition
of the relative entropy for one lens if we think of this part and then this part
there's two lenses this we might have a relative entropy which lands in the postive reels for
here and a relative entropy up here another one up here this chain rule says that the composite
if you apply this kind of lens composition rule to it
that the composite relative entropy or the relative entropy of this composite lens
is equal to the kind of lens sum of the relative entropies of the kind of independent lenses
and so that's that's this locality that I was referring to
so this is a nice equality
again this is only a particular example of a loss function and because if you notice here
sorry I'll go back a slide just momentarily you notice here that the relative entropy depends
on this exact inversion well that means in order to be able to compute it you need to know
what that exact inversion is and actually often the reason we want to do a proximate
inference is because computing the exact inversions is quite hard and so yeah we don't want to try
to compute that thing and so we need to do something else and so these other loss functions are
available and the relative entry is one example but we could we could sort of pair a lens with
another example of a loss function and I've given the composition rule for lenses and I've sketched
the composition rule for the loss functions and so these things form these kind of pairings of
lenses with loss functions also form a category and I call this category the category of statistical
games and the reason I think of it as a kind of game is twofold the first is that it you know shares
a lot of structure with categories of economic games as I've indicated already but it also I you
know it also captures something that's quite nice you know to think about the use about these
processes which is that you know the way we improve our inference is by playing this game of you know
how can I do my inference better so that's this statistical game that you play that they say
these neural circuits play so what we've done here is we've attached loss functions to lenses
and so just as we had a vibration by attaching the the inversions to the channels themselves
we've done this attaching thing again so we've got another vibration and so we could ask okay so
what are the sections of this vibration so what are the ways of assigning in a kind of nice compositional
way to each lens a loss function there are various examples so one of them as I've said is the
relative entropy that's particularly nice because it makes this thing an equality
but there are lots of other examples that people use in applications because computing
relative entropy is difficult so we've got things which are like the free energy which
is a bound on the relative entropy and we've got things which is we've got something which
is slightly different we've got the maximum likelihood which is a way of saying okay well
how do I improve my predictions if I don't care so much about the actual the inferences that come
out of those the free energy I should note is basically the sum in fact it's defined as the sum
of the the relative entropy and maximum likelihood and it's nice that if you make that sum not only
do you get something that's a bit easier to compute but you also find a loss function which
kind of encourages you both to improve your predictions that's the maximum likelihood thing
as well as improve your inferences in machine learning a process which does both the improvement
of predictions and of inferences is called an auto encoder if you've heard that word
and then in neuroscience often it's necessary to make further approximations to get to something
that seems to be like or possible for the brain to compute and so one such approximation is known
as the Laplace approximation and if you apply that to the free energy you get another loss model
that is as I say lacks in the sense that there's some other term here to make this equation hold
so these things all form loss models possibly lacked loss models
and that means that they are nice kind of local compositional ways of attaching
um loss functions to these Bayesian lenses and indeed this Laplacian free energy loss model
is the kind of family of loss functions which underlies uh say the most prominent models of
predictive coding so we've started with these mark of kernels or channels as I call them
we've attached inversions to them first to give Bayesian lenses and then we've attached loss
functions to those lenses to give statistical games but there's not much freedom in that process
to actually improve the loss functions because once you've fixed the lens that's it the only thing
you can change is the inputs either the kind of belief the prior belief you start with or the
input you get from the world what if to improve the loss function you also need to change the lens
itself so we can use the same kind of idea that gave us state dependent channels here
and parameterize the the Bayesian lenses and the loss functions to give um parameterized lenses
and statistical games and in machine learning in particular it's updating these parameters is
what happens during you know the training of models and in models of neural circuits um it's
we can think of the sort of synaptic weights on the neurons as the parameters and parameterizing
a structure is something that is kind of um quite well well known in category theory and
particularly in categorical cybernetics um and the way I like to think of it is as a kind of choice
of process so you get something which takes in a theta and gives you a morphism and you can compose
those morphisms themselves and then learning amounts to kind of improving that choice process
so you know this is just a particular it's a general morphism in the category C but we can
make this category the category of statistical games and then we have a statistical game which
depends on some parameter and then we can optimize the loss functions which are of course part of
the morphisms of these games with respect to those parameters and we can do that you know with respect
to each parameter on each component in this kind of local way as neural circuits seem to do
and exactly this is how predictive coding models are obtained so as I said we've we've paired
predictive channels with state dependent inversions um these exact inversions constitute a section
and other other sections other lenses represent approximate inversions we can pair the lenses
with loss functions in the same kind of way to give statistical games and this formalizes the
chain rule of the relative entropy the free energy is another example of one of these loss models
and then we can make space for learning by introducing parameters I'm aware I don't have
much time left and so I won't spend an awful lot of time on dynamics I just want to say that the
same kind of idea of filling in boxes so earlier I drew a box and I added a parameter into it
we can use that to give a notion of kind of compositional dynamical system and I think
John's going to talk more about this later possibly I mean a lot of what John has done is about
dynamical systems and so what we can do is say okay well we've got a category that represents the
interfaces of our dynamical systems and then we can use the morphisms of that category to
represent how those systems are kind of wired together and this this this category of interfaces
and wiring is something that I'm going to call int and then on each interface so this is the
interface which has inputs x and inputs y we have a category of dynamical systems which
take in interfaces x and y and so here we can fill in one system here we can fill in another system
and you can see that this is the same again the same pattern that we saw earlier of an indexed
category and so here the re-indexing is the rewiring of systems and so what we can do is
apply this wiring to the systems themselves to get a sort of bigger system on this composite
you know on this on this out for this outer box a kind of composite system
then we can define dynamical systems which emit the data of Bayesian lenses
I call these these systems cilia because they're like the ciliary muscle of an eye they control
the lens I won't go into details about the structure I just wanted to write it down for
reference but a cilia the cilia have the same type as the lenses but they're now sort of
dynamical lenses and so a cilium of this type that goes from xa to yb it has a state space
it has a forward's output channel that's parameterized by the state space
backwards inversion that's also parameterized by the state space and then an update function or an
update map which is like a stochastic dynamical system which takes in a state space an observation
and a prior belief and gives a new element of the state space and so that's like the
the updated parameters that we used for learning and these things compose by
composing the lenses together and by forming like the the sort of product of the of the
parameter spaces so you have a parameter on each on each bit of the the composite system
and this so this story it starts to give us a sort of general recipe for predictive coding
systems which are you know we could think of as like local approximate inference systems
you know we have parameterized kernels we form lenses then we make statistical games the first
is the thing I call an inference system and the second is a loss model and then we've gone from
a parameterized statistical game into this dynamical category by say doing optimization
as David mentioned at the beginning on the loss functions and so a process that starts
in this category of stochastic maps and ends in dynamical systems that do inversion is something
I like to call an approximate inference doctrine it tells you a way to do approximate inference for
each channel in your original category so I will end briefly I just want to say
finally that this is all about approximate inference effectively perception we can extend
this to active systems by saying that the the agent say is trying to predict not only
um what it perceives but also what it should do effectively predict everything on its on its
interface the inputs as well as the outputs in the literature in neuroscience and related areas
this interface is sometimes known as its markoff blanket and so we can also package up systems
that do this kind of prediction into a category or if we call the interface p we get a whole category
agent p of agents with that interface to do this kind of prediction and indeed we can rewire these
things according to wiring of the kind I mentioned earlier so you could think of that as like if you
get in the car like the your your sort of interface changes because you you know you're you're now
contained within the car but the car itself has some intelligent behavior you could think of a
corporation which again is made up of lots of agents but it itself has this yeah legal personhood
and so at this point we start to consider how agents themselves compose
and this kind of gets to some subtleties which are kind of more philosophical questions
in particular um that agents yeah they need to predict not only how the environment causes
their sense data but they also need to predict not only their sense data itself but also how the
environment causes their sense data and that's something a bit like theory of mind and this
is something that's necessary to make this structure compositional um and then once we
have this you might think okay well two different agents may disagree about how they're wired together
and then we need we need a way of sort of coming to overcoming that that sort of disagreement
this is really a sign that these Bayesian models are kind of inherently subjective
and there are various ways we can deal with that in mathematics there's a lot of stuff a lot of
work done in cohomology to measure disagreements of various kinds and we can use diffusion to sort
of smooth out these disagreements or we can just have a higher level agent which imposes
resolution to the disagreement and because of the compositionality of all of this we can
do this or do each make each of these choices in a kind of nicely modular way
as I say the choice depends on the situation at hand and so finally this this kind of
points to the kind of broad ambition of what's known as the free energy principle which as I
said at the beginning asserts that kind of all adaptive systems um at least that maintain their
interface can be understood as performing approximate Bayesian inference and at least I
don't believe that this claim although it's been stated and there's lots of evidence has been supplied
to substantiated I don't think it's been sort of conclusively established and I think using
categorical tools and this formalism of compositional active inference is a nice framework in which
it might be possible to do and so what I've talked about is a mapping that goes from agents
as say statistical models to dynamical systems and the free energy principle claims that there's
a mapping in the other direction which goes from sort of dynamical systems in say of a sort of
physical kind to um agents or statistical models in a world that kind of has a more
sort of biological flavor and we can sort of think of this as a kind of mind-body dualism
and in particular one way that we might be able to formalize this claim is as an adjunction which
is something again I think David mentioned at the beginning um thank you I know that was quite
fast but I wanted to cover a lot of ground to say how how there's quite a lot of you know
great stuff to be you know found by using categorical methods for complex systems like the brain
great thanks so much for that
yeah philosophy talks they sort of always leave a very long time to each other for the audience to
sort of slash it the speakers arguments but um obviously we're not we're not in that domain here
but perhaps if we have one one or two questions which which actually I did I did there was a thought
because we've got somebody here who works on explainable AI PhD student and what do you do so
so say you've trained you've trained a neural net half and you've got these sort of different
different levels I mean if each level is to be seen as a Bayesian lens of some kind
I mean how do I get at what is what is the type that is the output of one layer and the input to
the next like how can I get to understand what it means it sounds like you've already got the type
articulated doesn't it if you're if you're composing these yes so that's I think
that gets to a point that the category theory is kind of more about composing systems and
decomposing them so I'm assuming yes that you started with um two lenses that represent say
your models of small parts of the system and you're trying to use that to build up a bigger system
but yes if you have a kind of big complicated neural network there might be various ways of
partitioning it into smaller components to try to understand it but I'm not sure I have a kind of
nice universal story for how to do that right now I'd love to have a good connection from this to
um to that kind of explainable AI but I think a sort of less ambitious aim that we start to get
from this kind of toolkit is one that says okay well at least we know how our systems are built
and we know what the different parts are if we work like this if you look at machine learning code as
it is um it it's often full of lots of you know very informal commentary because people need to
sort of be careful about where certain bits are which how the information flows effectively which
index goes to which other index um and so I think people actually don't understand the models as they
are right now um having some types on those interfaces might make that process a bit easier
okay great so thanks thanks very much it was one one question from we've got Mark here from a PhD
student yeah thank you so much for that it was a lot extruded that was a very very intense
I really did appreciate that um there's uh the there's something that I wanted to uh get on
first of all I'd like the uh an explanation of the um impregnatedness of the whole project
but the the question I want to ask you is it doesn't seem like there's
necessarily be a clear demarcation on the biological side the ancient biology side
to the the process of of uh going from um Bayesian lensing to the silly uh concede institutions
and I think you hinted on that corporations and uh um you know the the focus of of at the
silly level might end up being at uh so I want to know what your thoughts on that are
yeah it's very interesting so I yeah I this is a this is a kind of scale-free theory in some sense
um I don't mind I don't think of a particular neuron as being more like adaptive than a whole
brain or a corporation they can all be understood as doing this process at least on this account
and so there's nothing to say that say a corporation is somehow less like biological
than sort of a single or multi-celled small organism and in fact I think what we see of
the process of the evolution of life itself is a kind of growing um sort of growth of complexity
like you start with these single celled organisms you get multi-celled organisms you get you know
simple plant life animal life and now we've got societies and in biology you have smaller
societies like anthills and those things are all kind of you know sensitive sensible conglomerates
of their own kind they are adaptive systems um and I'm trying to capture this process in its
generality um you know corporations are part of that but you know nation states and the whole world
is one too um so I I'm not trying to make a distinction between biological systems as they're
commonly understood and these kind of broader economic systems I think of those things as all
kind of ecological in some sense and to be honest I think economics could do a lot better
if it started to think a little bit more ecologically too okay thank you so much
thanks thanks very much Toby we've got a few minutes to set up
three four minutes till we get to the hour and uh and we can hear from John hi hey John
how's it going hi yeah great should we try and let's try out the it worked with Toby so it should
work with you if I get the and then I find you and then I'm going to make a presenter
so that should a waste if you share now that should okay let's see
so I told you I'm a complete novice the yuzuma not a team sir so how do I share
does it not have a top right there was uh John yeah share tray icon okay yes it says open
share tray okay that's another roundabout way of saying you want to share okay okay
thank you so am I you see a big first slide here the first slide we've got
okay so we can be going oh yeah take it as a okay yeah I think we close that side then that's
pretty good let's see let's see yeah so we're good good from our end is everyone on lines okay
so we've got um I can do three weeks uh till we till we begin okay great
how's it going home working you up too early in California uh not too early
what is it it's nine days yeah it's yeah uh that's the problem of living on the very west
coast of the world is that everyone else wakes up too early
but I like but I like it's a miracle we can we can do this sort of thing right I was gonna say
I really like the ability to give talks without leaving my house so we were worth it
so yeah you were wondering about recording we've got it still be going
I know it is in the process of recording that's fine okay great uh so we'll wait so
did I quit at 10 minutes to the hour um yeah I mean it's funny it's sort of philosophy talks
I remember the first time I gave a talk to um to to maths well to a sort of non-philosophers
you kind of you know you're expected to do about sort of two to one ratio kind of talk to questions
and uh some places even worse than that and then you go then the mathematicians wouldn't have
really had questions to put you uh in 95 percent talk isn't it in five percent questions but um
actually really here you carve it okay well I'll try to I'll try to leave time for questions I should
okay yeah there's raising lots of questions still
okay so I guess we're there so let's get started so uh well gosh what an influence John John had
in my life uh there I was a uh fledgling philosopher trying to uh interested in in mathematics
and John through his writings in that in that got a long way a long time here in the 1990s
who gave me the confidence to think that you could get access to the forefront of of mathematical
and and scientific knowledge through his writings I mean what gosh what a what a joy that was to
to engage with his writings back at that time huge influence on me and you know a bunch of people
will know that we started a blog together a while ago we had a lot of conversations
over over great many years so it's great pleasure to have him speak to us here today on on the
applied category theory thank you John well thanks very much thanks for inviting me here
the influence was reciprocal uh it's really exciting to think that what one's doing mathematics
could actually be interesting to a philosopher so that one isn't just like fiddling around with
obscure equations uh so I want to talk about applied category theory which is the latest
turn in this game uh and I'll talk more about the history of the subject and what
people are trying to do than uh any mathematics per se I've given lots of talks about the
mathematics and so if all the people who I'll be uh referring to here so it's certainly easy to
click on any of the links in my slides and get into the actual mathematics but I'll try to
not say as much about that as I sometimes do so we often have this picture of different
disciplines or subject areas uh talking to their neighbors and information flowing
between pure math which is at some very abstract end uh to say the business world which is maybe
at the very concrete end in stages with each discipline talking mainly to its neighbors so
pure mathematicians talk to applied mathematicians they give uh suggestions to applied mathematicians
and they find out about problems from applied mathematicians and learn techniques from applied
mathematicians and similarly for each link down the chain with each discipline talking to its
neighbors and in this model which of course is very oversimplified and and you can easily find a lot
of faults in it uh in this model we could think of category theory as the purest of pure some
super pure mathematics that only can talk to mathematicians who then relay their insights
down the chain to the other or practical subjects um I think that's what a lot of people implicitly
feel about category theory or at least used to uh and so when people started talking about applied
category theory people laughed because it sounds like an oxymoron it just goes completely against
this picture here however I think this picture is inaccurate um for one thing a very important
fact is that starting maybe around 1980 or so it became clear that computer science really cuts
across this traditional flow of information computer science interacts almost directly
with every single piece of this uh chain computer science has a big relation to category theory
which I'll explain but it also has a huge relation to other aspects of pure mathematics
and applied mathematics and so on down to business so business businesses will need to
keep abreast of developments in computer science and so on basically we now communicate and think
with the help of these machines called computers that we have devised to help us
and and they change the whole game at every single step of the way now I have this crazy
idea that maybe category theory actually really should be thought of in a somewhat similar way
as interacting with all the different links in the chain from the pure end to the applied end
rather than being tucked away at the super pure end of the spectrum now at first this seems
ridiculous or laughable I mean for starters category theory is sort of nothing compared to
computer science everybody's walking around carrying cell phones nobody's even heard of
categories by comparison so category theory is an ant compared to the elephant of computer science
however I'm thinking about what could be not what yet is and secondly
when you think about how computers have unified the whole chain it's done so by means of the
internet which is an enormous network and category theory is nothing if not the
mathematics of networks or the science of networks it's other things too I shouldn't
have said it quite that way I mean it's it is the science of networks but it does many other
things too so so to really devise good methods of getting computers to do what we want we need
a way to think about them and I think category theory is one of the important ways to think about
them so you may think this is ridiculous still because for example if category theory was talking
directly to business you'd expect that people would be going into business doing category
theory or something like that setting up category theory businesses that sounds sort of silly but
in fact it's true so in fact people are setting up category theory businesses and that's part of
the kind of thing I want to talk about I want to talk about how category theory is now being used
at all steps along this spectrum and for that reason my talk will be less focused on the math
than on the history of how category theory became applied giving you a flavor of how it's being
applied and pointing out some of the challenges that show up before I dive in I should say that
this history is going to be extremely self-centered I am not really in a position to write a good
or talk about a good history of category theory so hopefully someone who eventually tries to
write a history of applied category theory will listen to this talk and fold it in as one data
point I'll talk about a lot of people but I will also leave out a lot of people and I really
apologize to all the people who work on applied category theory I'm not mentioning there's just
no way for me to do a fair treatment of this big subject okay so I'll start arbitrarily in 1963
when Bill LaBeer introduced a concept called functorial semantics so it's a very general concept
where you have a category that describes syntax describes ways of writing things down
and then you describe the process of interpreting those expressions or giving a meaning to those
expressions via a functor so the functor will send expressions to their meanings but a crucial
aspect of this whole setup is there's not one god-given functor f that does this you can choose
various functors from the category c to various other categories g because there are different
ways to assign meanings to expressions and that flexibility is very important
and it became important in computer science so for example there's a nice there's a way to think
about it where a sufficiently well-behaved programming language may give a category where
you think of the objects as types or data types we've already heard about that idea so types x y z
etc and then a morphism between objects like a morphism f from x to y we think of as a program
that accepts data of type x as input and outputs data of type y and then the fact that you can
compose morphisms tells you that you can hook up two programs to get a bigger program
where the output of the first program becomes the input of the second and that combination
that is a program in itself so I could say there's like the relationship between the
lambda calculus which is sort of an idealized programming language simplified programming
language and cartesian closed categories understood by labbeck is an example of this paradigm of
using functorial semantics for computer science but here I'm only so far talked about the category
c I haven't talked about the the functor so what the functor could do well we could for example
take a functor from our category to the category of sets map each data type x to a set I know that
would be the set of possible values of that data so like you could have a data type like real numbers
and then F would map it to the actual set of real numbers and then each program gets mapped
ideally to a function and the key fact of a functor is that it preserves composition
so that composing programs would compose the would give rise to composing the functions they
compute and the identity program which does nothing at all you interpreted via this functor
F it gives you the identity function which does nothing at all on a set so that's a that's a
paradigm that became important in theoretical computer science I wouldn't say that most computer
scientists know or care about functorial semantics but in a certain community of the more mathematical
theoretical computer scientists this idea has become quite important and there are certain
languages like haskell for example and others that that really take advantage of this viewpoint in
particular haskell uses a category theoretic construct called monads which has sent many
generations of young computer scientists running for for help from category theorists
to try to figure out what a monad is I don't want to explain what a monad is it's a category
theoretic construct which like many other constructs become important when you start thinking about
programming in this functorial semantics way but in the 1980s something very different happened
particle physicists realized that any quantum field theory
specifies a category and now the objects are roughly speaking collections of particles
and the morphisms are roughly speaking ways for particles to interact in terms of other particles
and in fact Feynman drew pictures called Feynman diagrams for describing how particles interact
with each other this was a bold move because although it seems very intuitive here you have a
proton and an anti-proton coming in from the left and then they collide and all sorts of other
particles shoot out in reality particles are not little dots moving along little straight lines
quantum mechanics says that particles are are wave-like in character so when Feynman introduced
this pictorial language he got a lot of opposition from older quantum physicists like Heisenberg
and Bohr and yet it turned out Feynman was correct in doing this what Feynman diagrams
actually are later became clear much later is there are pictures of morphisms in specific
categories and each individual quantum field theory gives a category and then that determines
what allowed Feynman diagrams you can write so if you have a theory that doesn't have quarks in
it you're not allowed to write this Q line for quark and then what do you do with these pictures
well your your goal in particle physics is to get a bunch of numbers like what's the probability
that if I smack two particles into each other that some other particle sheets out so for calculating
numbers we need to interpret our Feynman diagram that is in the functorial semantics attitude we
need to functor from our category to some other category which assigns meaning to the Feynman
diagrams in this category we could take the category of Hilbert spaces a widely loved
mathematical structure in quantum mechanics and so then it will this functor will turn any Feynman
diagram into a morphism in the category of Hilbert spaces and that's a called a linear operator
between Hilbert spaces and so you can look at what physicists are doing with Feynman diagrams
as basically using them as a syntax which they then apply the semantics to to then grind out
numbers now Feynman probably would have laughed at this whole business because he knew what he
was doing he didn't need any categories just to tell him what what he was doing but by the 1980s
particle physics had become elaborate enough and deeply connected with enough pure mathematics
that this attitude of thinking of quantum field theory in terms of functorial semantics became a
really profitable attitude you could do things with category theory that particle physicists were
struggling to do without category theory and so it helped and this led to a whole line of work
trying to relate category theory to quantum mechanics and in 2004 Sampson Abramsky and
Bob Kirke wrote a very influential paper showing that a certain quantum process that goes by the
name quantum teleportation and also other processes could be understood using category theory and
using diagrams that resemble Feynman diagrams but here the edges in these diagrams which now I guess
we're reading from top to bottom rather than across the edges are not describing elementary
particles but just states of an arbitrary quantum system so it could be an electron or it could be
some larger system and I don't want to try to explain this in detail but there are but these
are pictures of morphisms in some category and they're equal to each other and these morphisms
describe some process where one participant who always is called Alice in cryptography communicates
a message to another participant it's called Bob and in quantum teleportation you have the superficial
appearance that you're communicating faster than light but in fact what's really going on is that
you've laid the ground for the communication by some classical process that has already transferred
some information and then the rest of the information is transferred on the basis of that
in any event quantum teleportation was a sort of magical wonderful thing that you can do in
quantum mechanics and this gave a kind of category theory theoretical count for how it how it's working
and that was quite exciting to people because it took the quantum applications of category theory
out of the realm of elementary particle physics and into a new realm that was developing which is
quantum information theory or quantum computation which has the potential for being a quite practical
thing and indeed already quantum cryptography is used so on the basis of this and other work
and also on the basis I should say of a huge amount of charisma Bob Kirker and Samson Abramsky
built a huge group of people at Oxford sometimes called the quantum group working on category
theory the foundations of quantum physics and quantum computation one interesting thing about
this enormous group here is that it's in the computer science department so not a mathematics
department not even a physics department but a computer science department here's Samson Abramsky
here and here's Bob Kirker looking sort of disreputable over on the edge that's completely
intentional he's like that and here are many many people including I see Jules Hedges who
has mentioned in the previous talk he works on categorical cybernetics so Kirker and co-authors
developed this diagrammatic method for explaining quantum physics and they eventually decided it
would be very good to write a textbook on this and so here is one of two textbooks that Bob Kirker
helped write on pictorial methods of understanding quantum physics and the one interesting thing
about this book so I first of all I recommend this book if you're interested in learning
category theory from an unorthodox and sort of easier approach than the conventional one
because it's very pictorial but I mentioned it because part of what what he wound up doing
is hiding the category theory in the sense that you don't run around saying category and
functor and all sorts of other dark and like that a lot instead you draw pictures of processes and
you explain these pictures in a hands-on way and you explain how you can stick one picture onto the
other which is composing morphisms but you don't say the word composing morphisms necessarily
because you don't need to and this is one of a generation I should say a new generation of
category theory textbooks that are part of the applied category theory movement so if
a category theory is going to reach directly to many different communities and different stages
of the knowledge chain that I drew at the top of the talk it means that you can't force people to
first become mathematicians before they then start doing things with category theory the traditional
approach to category theory treats category theories like some finishing school that you engage in
after you've already learned abstract algebra and this and that and algebraic topology and so on
that's just too onerous a path to category theory if you're trying to get computer scientists
much less business people to get some inkling of category theory and it's not really necessary to
take that elaborate route because category theory is something so fundamental that it can be to
explain more directly I especially recommend Eugenia Chang's book The Joy of Abstraction
which is a really wonderful introduction to category theory for people who have not had much
prior exposure to mathematics at all and it goes all the way up to the UNEDA embedding theorem
so in 2021 Perke actually left Oxford University and he became the chief scientist
of a company called Quantinium which uses category theory for quantum computing and also
natural language processing this is a picture of him on the Quantinium webpage as you can see
he's trying quite hard not to fall into the stereotypical academic image here he's a great
guy by the way you shouldn't mind me teasing him like this I think though that it is important
that's having people with personalities who are willing to break out of the stereotypical
mold of a math professor is important in this whole category theory application movement
I should say that there's a lot of interaction between quantum computing ideas and natural
language processing ideas which I won't really have time to get into to some extent they're
invisible in the last talk but if you want a real emphasis on the natural language processing
aspect the works of Ty DeNay Bradley are very good she's another newer explainer of category
theory it's a non-category theorist and she is also working in a company that uses quantum methods
and categorical methods for natural language processing
but punctorial semantics is extremely general so we it's not just applicable to the topics that
I've already mentioned so in lots of areas of science and engineering people use a syntax
where you draw diagrams and then a semantics where you convert those diagrams
into math which is very often the systems of differential equations you're describing how
things change in time you're describing dynamical systems so you use differential equations
but the interesting part is that hooking up diagrams to each other is actually composing
morphisms in a category and the fact that you're you're working with functorial semantics
says that when you compose those morphisms in the syntax category your functor
will map it the resulting composite to the composite of the two systems of differential
equations that each picture separately is mapped to so in other words there's another category
working around here where the morphisms are actually systems of differential equations
systems of differential equations with some specified inputs and outputs to surface
as the source and target of your morphism here are some different pictures you've seen a bunch
of these types of pictures I hope so electrical circuit diagrams are are one of the earliest
and very widely used this thing here's a called a petri net and it's a petri net of a particular
model for the AIDS virus and its interaction with blood cells infection of of white blood cells
this here is a very small diagram in the subject of control theory where you have some
process that in an industrial plant that you're trying to keep under control and so you take
feedback you read off information from that process and you use it to feedback
affect the the process itself to keep it on track I mean the very simplest example here would be
like a thermostat but in industrial chemistry they become much more elaborate and so there's a
whole diagrammatic language of control theory that you can read about in textbooks on that that
subject and this is a little diagram of some molecules in molecular biology molecular biology
or biochemistry they have so many different diagram languages that in fact they've set up a
committee to standardize the diagram languages so that people reading different biochemistry
papers can look at the diagrams and not have to be explained each time what the diagrams mean
but it's sufficiently broke that they uh that the committee wound up choosing three different
diagram languages because they they need multiple languages to describe different things they're
interested in so these are very widespread and the people in these different subjects don't think
of what they're doing here as category theory they don't think of it as mathematics they think
of it as this is the stuff we draw to understand the differential equations that you can convert
these pictures into the the equations of the math but these pictures are our own private
little trick for understanding what's going on but I claim that these pictures are actually also
mathematics these diagrams are morphisms in various categories which are using functorial
semantics to convert to differential equations so I decided that had to be true and I wanted to work
out the details around 2012 so I asked Brendan Fawn who eventually became my grad student to
tackle what I thought would be a very good first example namely electrical circuits
because everybody knows about electrical circuits and the mathematics of them is in
substance very well understood linear electrical circuits for example made out of just resistors
or maybe capacitors and inductors as well um I asked them to create and study a category
having open electrical circuits as morphisms so these are electrical circuits which have
certain things hanging out which serve as inputs and outputs I think the last talk they were called
so these are just these dots here um so he took forever to do it so it seemed but it turned out
that what he was doing was not just answering the question that I asked but in fact answering a
much more general question he invented a general theory which he called decorated cospans for handling
this problem but also all the other problems that I described that is potentially all the other kinds
of diagram languages I showed can be understood using the theory of decorated cospans although
the details have to be individually worked out in each case this decorated cospan formalism
is a is a recipe for getting categories of this general sort and so naturally it did take him a
while to do a good job of it and he finished this thesis on it in 2016 I won't attempt to explain
decorated cospans anymore than I just have I've given lots of talks on decorated cospans and you
can see on YouTube me giving talks about them if you're interested um I just want to mention one
little thing is that well first of all here's Brendan Fong and here's another student of mine
Blake Pollard who together with Brendan applied this theory of decorated cospans to the second
example that we tackled which was Markov processes so so so Blake did a lot of work on Markov
processes and another thing I want to mention is that Brendan went to get his PhD at Oxford so he
was in the enormous group that I mentioned of Kirke and Abramsky and I believe Kirke was his
de facto it was his de jure advisor but in fact I was the de facto advisor telling uh
talking to Brendan about this this problem and so nonetheless it's really important that this
large body of people at at at Oxford is is spreading across the world doing many things
and Brendan is one of them so after doing his PhD Brendan went to MIT in 2016 to work as a postdoc
with David Spivak so Spivak is another very important figure in applied category theory
so among other things he had helped to create a language for databases called Functorial Query
Language which has now become the basis of a company called Conexus AI with Ryan Wisniewski
as a as a head of our company but David Spivak also involved this approach to databases really
relies heavily on functorial semantics so the idea is that a database schema is is an object
in some category and that by a functor you can map it to a sort of populated or inhabited
database schema where where the your table of is actually filled with entries you can imagine
like taking a spreadsheet and actually typing in the entries that's one possible uh interpretation
of of the abstract database that has not yet been interpreted with with specific people's names and
numbers uh and in 2014 David wrote a textbook on this idea and others called Category Theory for
the Sciences so I also highly recommend this as yet another of this new generation of category
theory textbooks is trying to reach out beyond the traditional confines of the category theory
community of highly pure mathematicians toward other audiences
and in 2018 Brendan Fong and David Spivak wrote another textbook explicitly on applied category
theory which is uh fondly called Seven Sketches but its actual title is here is Lager uh emphasizing
the principle of compositionality in many guises and they taught a course on it which you can see
on youtube if you click on this link in my slides and the textbook is also available for free on
the archive as well as for purchase and I'm mentioning this stuff about things being
available online for free partially because when you're trying to reach out of the pure math
realm into the rest of the world and teach the rest of the world category theory you quickly
realize that making things easily available for free is going to speed up the process a lot
and so I think there's a general commitment so far on the part of a lot of applied category
theorists to make things open source and uh easy to get at now in 2018 my graduate student Kenny
Corsair there and I came up with structured cospans which is a simpler alternative to decorated
cospans turns out to be not quite as flexible as decorated cospans but in many cases either
formalism works and in those cases I'd say structured cospans are simpler in 2020 Cristina
Vasiliopoulou who's standing to the left there helped us understand how decorated and structured
cospans are related for a while I thought structured cospans would replace decorated cospans
but it turns out we we now see that they can't cover quite as much territory and that we really need
decorated cospans in some cases so the full story is somewhat elaborate right now and I've
given talks about that but it's it's actually very pretty so by now structured and decorated
cospans have been used to study open systems of many kinds so by open systems I mean systems that
have loose ends that are meaning systems that are morphisms in a category that you can stick
together or compose to get bigger systems and your open graphs are just a simply a graph with
loose ends it's a very combinatorial structure that's a very simple example open petri nets is
a more useful example here's a picture of an open petri net where you have
things describing other they're called they're called places they could represent like populations
of different kinds of chemicals or organisms and then you have these blue boxes called transitions
which describe how things can come out of various places and turn into things of other kinds
if you attach numbers to these transitions describing the rate at which the transition
tends to occur you get something called an open petri net with rates or maybe an open chemical
reaction that work with rates and my student Blake Pollard worked on those open electrical
circuits I've mentioned open mark-off processes I've mentioned open dynamical systems those are
the systems of differential equations interpreted as morphisms in some category
so there are papers on all of these and if you click on these brown things you'll get to those
papers so at a certain point I thought that maybe someday all of this would lead to some
unified language of the sciences and my ambition which seems very limited in retrospect
was that by interacting with scientists maybe eventually this unified language would help
scientists in one discipline talk to scientists in another discipline and share tricks of various
sorts but then something quite different happened which is much more exciting in my mind namely
in 2019 James Fairbanks who's an engineer and Evan Patterson who is a statistics
PhD began developing a computer system called Algebraic Julia for it's a system for high
performance scientific computing built on the language called Julia which is designed for
crunching numbers really fast but Algebraic Julia lets you program and using constructs from
category theory so in Algebraic Julia you can talk about categories bunkers and all these other
things that category theorists like to talk about and write your programs using those concepts so
it's a very very high level system but coupled to the raw computational power of Algebraic Julia
so they were trying to revolutionize scientific computing in this way
and in 2020 Evan implemented structured co-spans in Algebraic Julia and then using this system
for the example of open petri nets he and Mika Halter reconstructed some of a COVID-19 model
which was currently being used by the UK government so the UK government had a very elaborate
model of COVID-19 with many populations of different people with different conditions
possibly living in different locations etc and then I guess sorry the populations of these
orange boxes and able to turn into other kinds of people via these transitions in blue the
whole model is much larger than this this is just a tiny fraction of it and what they wanted to
demonstrate was that you could build the whole model by building smaller models and pieces and
then subsequently sticking together those pieces which is in fact something that had not been done
before models were sort of had to be constructed in whole cloth by essentially by one person keeping
track of the whole thing but this was a new approach compositional epidemiology and so it
attracted the attention of some epidemiologists especially since COVID-19 was a very live issue
at the time then in a different direction in 2021 Brendan Fong and David Spivak who had been
remember over at MIT decided to quit traditional academia and start their own institute called
the Topos Institute in downtown Berkeley it's an institute for applied mathematics
but with a heavy slant on applied category theory and they hired a number of people including Evan
Patterson and also Sophie Libkin who's here who will appear later in the story and it's a real
real incubator for ideas on applied category theory now and so this is another one of the
sociological moves that I had to be made I think to bring applied category theory forward
I recommended Brendan not try to start an institute right after his first postdoc
but luckily he was much braver than I would ever be and he went ahead and did it and it's
quite successful now so in 2022 Sophie Libkin, Mika Halter, Evan Patterson, James Fairbanks
another person I haven't mentioned Andrew Boss used Algebraic Julia to systematically
build models of epidemiology using open petri nets with rates so here are some open petri
nets the rates that is the number data is not being shown here and here they're being stuck
together to form a larger open petri net the way that they're being stuck together you'll notice
is not end to end this is not the normal end to end composition that you see in a category
this is something a little bit more general we're sticking them together using a trick called an
operad so operatic composition is one of the things that they incorporated in their Algebraic
Julia software which is so this was done this previous work was done in a slightly
kind of hot basis but that this really used operads in a systematic way
and then in 2022 also another team of people including myself used Algebraic Julia to create
software for building models of epidemiology using another style of diagrams called open
stock flow diagrams well stock flow diagrams but we're making them open so here are some of those
kind of diagrams being stuck together in an operatic style that is not end to end but sort
of gluing them all together to form a bigger model so part of the point here is that when we
started talking to actual epidemiologists we applied category theorists learned more about
how they actually want to do their models and they didn't really want to do their models using
petri nets primarily they wanted to use a different technique that they considered congenial
called stock flow diagrams so in fact there's a community of epidemiologists who use stock flow
diagrams to model the spread of disease and it includes my co-authors Nathaniel Osgood
and his grad student Shaoyan Li and one crucial fact is that they did COVID modeling for the
government of Canada so they were requested by the government of Canada to set up COVID models
for a number of different provinces and in the process they gained a lot of experience
with stock flow models they already had a lot of experience and they wouldn't have been asked to
do it but they gained a lot more experience and they learned some of the problems of the existing
technology there's a beautiful thing about stock flow diagrams which is that with a little bit of
just a little bit of training you can read them and you can understand what they're talking about
so a fellow named Hogman has pushed the idea of community based modeling where you go to a
community that has some problem like a disease or some other type of problem and you help model it
and you model it by drawing stock flow diagrams which here are hanging on the wall in a room
in rural India and then people can look at those diagrams and comment on them and help you improve
the model because these people the local people they will have the knowledge of what's actually
going on that you the modeler who just flew in from from Oxford or wherever may not have
and here we see a woman here looking at the picture on the wall and this is actually from a story
where this woman noticed that some ingredient was being left out of the model that these
village elders hadn't known about or hadn't thought about and so she helped fix the model
so that's the example of what you can do and you notice that none of the people in this
story here had to know category theory but they were in fact drawing morphisms in a category
unbeknownst to themselves now most stock flow modeling on the commercial level or the height
or the government level is done using a piece of software called any logic which is very powerful
but it has several big problems it doesn't let you compose models you let's you make a model but
it doesn't let you take a bunch of models and stick them together it doesn't separate syntax from
semantics it only has one semantics namely the differential equation semantics which is the
most important semantics but there are others that you would like it has no support for what's
called stratifying models that term was very confusing to me it's sort of jargon in this
community what it means is taking a model and splitting one stock one type of group of people
into several stocks so for example you might have a simple model and then you may say oh we
should subdivide people according to age groups because you know older people would get the disease
more more than younger people or something like that and you would like your software to make it
easy to to do that um it had no support for collaboratively building models it's basically
one person as the software and they make the model they could talk to other people but they're the
one who's building the model and it's not free it's a commercial piece of software and it's not
open source so people can't get in there and modify it and improve it our new work aimed to fix
all these problems and I could say we're well on the way to fixing all these problems so the reason
it's so important to close models is that the models that get actually used in the real world
are immensely complex so here's a picture of osgood and lee's covid model this is a page
in any logic the software this is the whole model as you can see it's an enormous web of
of stocks and flows and such and you you really would prefer actually not to have to deal with
this whole entity at at once you'd prefer to have it be thought of as being made up out of smaller
pieces because mentally it is made up out of smaller pieces but but that's not captured by the the
software unfortunately any logic just doesn't do that for you so we've not created software that
does that it supports compositional modeling with the stock flow diagram so it's called stock flow
and if you click on this link you'll go to github which is where computer programmers keep their
open source software so you can get download the software and so it lets you create and compose
open stock flow diagrams and lets you stratify them that is subdivide stocks into smaller stocks
for those of you who like category theory I'll mention that that's done using a technique called
pullbacks in category theory it lets you choose among several different semantics
functors so one of which is the differential equation semantics and of course it lets you
solve the differential equations once you get them and then Nate Osgood and another student of
his Eric reticop went further and made a graphical user interface for stock flow so stock flow was
written in this programming language called algebraic julia to interact with it you needed to know
algebraic julia but this graphical inner graphical user interface called model collab
it runs on a browser and you manipulate the the model by actually dragging and dropping little
boxes and and drawing little edges between boxes so and because it runs on multiple browsers
schemes of different people located in different locations can collaborate to build stock flow
diagrams so I could build my piece you could build your piece we could both save them they'd
be saved on on the cloud I see there's this little thing called cloud here oh that that's
not what it's actually for though sorry that's a mistake uh but but you can you can save Nate's
gonna laugh at that um actually clouds are these little funny things here um but you can save you
can save your your models on uh on the internet and so that other people in different locations
can access them and then glue them together to form larger models so in the goal to really apply
mathematics there are many levels because the next step is to actually train epidemiologists to use
model collab so we have some things going for us so first of all Nate Osgood runs regular training
sessions which goes by the quasi militaristic terminology favored in computer science of boot
camps because they're supposed to be intense so he runs boot camps on epidemiological modeling so
he is beginning to teach people model cloud second no understanding of category theory or the
programming language julia is required to use model cloud so it's really crucial when you're
applying category theory that you can ultimately black box the category theory in so that people
do not explicitly need to know it to use what you're creating although they may be using it in
some kind of implicit way uh and third it's free while any logic you have to pay for which
should ultimately be an advantage but i should still say it will take a lot of work because when
you have a generation of modelers trained to use any logic this new paradigm uh is not so easy to
shake so as i just already said applying mathematics takes many steps and as i moved from being
more of a pure mathematician towards a bit more at least some of the some days of the week of an
applied mathematician i realized how easy pure mathematicians have it you just write a paper
you explain your idea you throw it out there and you hope that somebody reads it but when you're
trying to actually carry out the the full application of an idea you have to interact with many people
with many skills persuade many different communities that what you're doing is actually
worth doing and it's much more elaborate but i should also say very satisfying endeavor
so i'll conclude by saying that applied category theory is interesting in in many ways some of
which i hope will be interesting to philosophers uh so some of these ways are separate from sort of
the core mathematical ideas so one thing as i hope i've illustrated here by my somewhat
self-centered history is that it requires new forms of communication and collaboration
because they're talking to new types of people and luckily uh these are starting to come into
existence so there's an annual applied category theory conference that's first started in 2018
uh run that time but by brendan fong and some of his other young friends uh another is a journal
so this is a journal called compositionality which is again started by brendan fong and some
of his young collaborators and friends this is an open access free to publish free to read journal
of applied category theory which i recommend to your attention and another is called the
category theory community server which is sort of like a a chat site or a discussion site for
applied category theory actually for all category theory but it has a lot of applied
category theorists there uh this was initiated by my grad student christian williams and so these
are different ways for category theorists to start talking to other kinds of people people
who want to might want to apply category theory or learn category theory and i hope that these
catalyze the the process then try to outline here another thing is that category theory raises
ethical issues in ways the category theorists aren't used to
sure mathematicians tend to feel themselves somewhat insulated from a lot of ethical issues
because what although their math make it used for all sorts of purposes
they don't consider themselves particularly responsible for how their mathematics gets used
because it's so abstract that they sort of can blame someone else for understanding it
doing something with it um but when you start doing explicitly applied category theory that
sort of veil of protection is is ripped open and you see that what you're doing may have a fairly
immediate effect so for example the biggest funder of research on applied category theory
is currently the u.s military right now and also a lot of applied category theory is being used
for artificial intelligence or machine learning or large language models or so on
so i'm not saying that those are necessarily bad things i'm saying that those are
things that definitely raise serious ethical issues and so i've worried about the danger that
applied category theory will wind up merely intensifying the already existing
um patterns in society that is it will only make the richest and most powerful people even
richer and more powerful i would prefer it if applied category theory could do something
really different really new this is one reason why i've tried to focus on epidemiology
which is a little bit different than than some of the other uh applications of course even
epidemiology ultimately feeds into the healthcare system so there are questions about that as well
i would really like to use applied category theory for environmental issues but i haven't
quite gotten there yet anyway sorry i just wanted to emphasize that uh you know ethics
is the provenance of philosophers and this is a really interesting intersection of mathematics
and ethics which i hope that people think about and another thing applied category theory does is
revitalize what is a very old question which mathematical structures are best for describing
and designing systems i emphasize designing because with the rise of computers whatever you
can think about clearly enough you can make so it's not merely describing the external world
it's you're also making worlds based on your descriptions and so this immense freedom that
you have in computer science that the question is what's what should guide you and it opens progress
it opens possibilities for some really radical progress because these are some really brand new
ideas that we're talking about here that make let us organize systems in new ways so i think there's
a lot to think about here and i hope hope some people here uh give it a try okay i think i'll
declare it done
brilliant thank you
see why i got drawn in by him 30 years ago it's uh
these ways of conveying ideas so clearly okay um do let's have some some questions then um
let me bring up the people if people online have some questions that they can raise a hand and i
potentially should be able to see for anyone here
uh okay uh mark's good i do have a question and i'm i'm finding stuff that i
thanks for that by the way um this is really exciting stuff for me but i'm wondering if i'm
biased and reading too much into this that uh that just works in my self-interest um i do
philosophy of time and um i'm one of the uh the few people who defend the notion that time flows
and um i see it in your work uh but i'm maybe it's through these biased eyes that this uh this
project is bringing back the idea of uh of thinking of uh differential equations for instance as
things that flow uh that functions or something that flow and i noticed the importance of diagrams
as uh and these diagrams of uh are reminiscent of what organizations would call flowcharts and um
i'm i'm wondering whether i'm reading too much into this as something that might be an answer to
the the static view of uh of uh in in in maths that came out of set theory and uh um thinking of
um functions as as something that uh is static and not doesn't flow so i'm i'm wondering whether
or not i'm reading too much into it so maybe maybe that's the question sure um yeah the question of
static versus dynamic is as you know very well extremely tricky question because i could take
any dynamical system and when it's when it's done or when we imagine it as being laid out
in time then we can think of it as as a static thing with time as just another space like
dimension and and i think there's very hard very hard to like definitively break out of that uh so
but nonetheless category theory is a generalization of set theory in a sense in which
instead of just having elements in a set you have objects and also morphisms and the morphisms
have are meant to convey some kind of dynamic uh aspect of the system that is an amorphism goes
from an object to an object and that's a very primitive sense of dynamics there of the start
and the end and and and it brings a kind of dynamical quality to to the foundations of mathematics
which is is a real thing it's not it's not just an illusion uh i mean even though you could if
you ask a traditional mathematician what is the category and they're saying oh it's a set of objects
and a set of morphisms and some functions doing this and that right so you could say oh we're just
back in the same old paradigm and that's what i meant by it's hard to definitively break out of
that paradigm i i think it's more in some ways more of a question of attitude rather than than
anything else uh and anyway i think the interesting thing about the differential equations here may
be the interesting new thing maybe the study of open differential equations which are differential
equations where some quantities are explicitly labeled as affected by the outside world affected by
the part of the world which is not modeled by the differential equation so uh traditional
differential equations well no uh physicists often like to study what are called autonomous
systems of differential equations where you know everything there is to know once you know the
differential equations uh whereas open differential equations which i should say are also studied by
which is this and especially engineers have certain quantities which are not determined by the
differential equation they're regarded as being affected by the outside world and and that that
on the one hand allows you to see systems of differential equations as morphisms that it
could stick together by sticking some of these six quantities on the edge to some other quantities on
the edge but it also means that you're you're not just dealing with a deterministic system in
the classic sense anymore because you have quantities that are affected by the unknown
so so they it definitely changes your attitude towards differential equations
Thanks uh oh a question from Toby it's not so much a question sorry although i would love to i mean
there's lots of questions i could ask but i i need to head off because i actually have to go give this
talk again just now so thank you very much for inviting me and uh i hope the discussion is
profitable okay thanks thanks to me uh i had a um i was talking about the interest of people around
here is is the difference between when you get variables that are just sort of associated with
each other there's some kind of statistical association versus when you think there's some
kind of causal connection between them uh and you might think you might think you can go much
better grip on the world if it isn't just association data but there's a you know that a causes b
other ways of getting at this through your um or is it that you i guess the modelers are kind of
coming already with their sense of what causes wild and is there any way representing the difference
yeah so this particular line of work that i was outlining as you say it i was mainly
most of my talk was about model building where you you have your opinion about what causes what
and how it affect what how x affects y and you build the model into that so there's a flip side of
the whole um modeling process which i didn't talk about at all which is incredibly important in
epidemiology which is in inference so you measure data and you try to establish causality you know
and also you know various numerical quantities which you would then put into your model um so
so there's a lot that applied category theory has to offer to that and i'm afraid i'm not quite as
uh knowledgeable about that i will say that um Brendan Fong for his master's thesis uh worked on
a categorical approach to uh Bayesian networks and so and and and that has subsequently
grown into things like what toby was discussing uh and so that should that's that's also part of
the big story and another part of it is that there's a technique called
particle filtering where you which is widely used by epidemiologists where once you have one of your
models of the sort that i've been discussing you run it and you run it a bunch of times and you
see what kind of outcomes it predicts but then you use that to uh to compare to reality and you use
that to estimate the parameters that should be in your model so so i guess what you could say is that
like what i've been describing was just sort of one half of a feedback loop of interaction between
modeling and then comparing model with reality and and yeah so the whole thing should be should be
carefully understood and i'm afraid i have sort of focused on one half of it
yeah i've seen very diagrammatic techniques from category theorists and
really get it kind of causal modeling i mean that exists out of that doesn't it
yeah it does and that's important okay um well i just think there's no further questions and we've
reached six o'clock anyway so let's just oh sorry was there one there was a one sorry those dominates
go here you go yes sorry i'm i'm casting around i've lots things to say i'm trying to formulate
them to a question that's difficult by john at this point and so one thing that in i know in
computer science is quite a big deal is sort of algorithms and data structures um and kind of
getting the right data structure for the right task to get good performance um and
i think you know there's a lot of things that category theory has to say about this um uh
kind of describing data structures in in in a more abstract ways for polynomial punctures and things
like this um so what i haven't quite sort of understood is is how much of that kind of work
is going on in in the applied category sort of movement recently sort of linking with with back
back with computer science the sort of like programming aspects around what i'm called kind
of classical compute science sort of algorithms data structures um do you know if there's much of
much of that going on within the sort of act world um or is that largely more back with the
compute scientists um so the algebraic julia community is really trying to blend uh
computer programming with category theory in a really rich way so that you can write
software and you can say you know define functor blah blah blah define pre-chief category blah
blah so that that and that's just one of several different initiatives to try to
bring category theory into programming so not just have it help programming it but have it
actually be part of the programming so that that's one thing there are lots of other things and I
probably don't know the other things so I will just mention again this company uh connexus.ai
uses a categorical approach to databases so it turns out that updating databases is very
nicely described using uh categorical ideas like plant extensions and um Mike Johnson in Australia
has been serving as a consultant for uh for database companies using category theory ideas
for many years but now this company based on david spevex work is also doing that
uh and there are and there are a lot of other things I should say that there's just a lot more
on the computer science side that I'm not very familiar with okay all right thanks thank you very
much okay so well here we've reached time so let's see what we have to do is thank John very much
for his very interesting talk thanks so I've reported this session and we can uh you said
you're thinking of putting it online we can yeah I'd like to if that's acceptable yeah yeah let's
do that good all right okay well thanks thanks so much John okay thanks very much bye bye everyone
online I'll see you see them
