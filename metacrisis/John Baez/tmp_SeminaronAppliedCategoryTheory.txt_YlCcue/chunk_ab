How's that? Yeah, that's, that's good. Do we get that full screen here? What do we?
This is so useful. I should always say it.
We can get it mostly full screen, I think, by just getting the cross on them.
We can do that. And we can do that. And there's one more thing I can do.
Well, that's probably enough. That's probably enough. I don't think I can do it in the browser
one. Okay. Yeah. Great. Okay. So my great pleasure to introduce Toby Smyth to talk to us on Bayesian
Brains cybercategorically. Thanks, David. And thanks for the very warm introduction.
I'll try to live up to it. So my, yeah, so my first degree was actually in philosophy and psychology.
And I guess it's possibly thinking also my enjoyment or predisposition to
thinking about things like the mind in this kind of abstract way that possibly philosophers are kind
of, you know, inclined to is what's led me to enjoy using this language that David so clearly
loves using as well. I'm not going to be saying very much on the sort of philosophical side today,
though, you know, I believe, as David does, that this language has a lot to give to philosophy.
What I'm going to be doing really is to talk a little bit about this journey
that David nicely summarized at the beginning that I've been on, basically trying to replace one
kind of confusion about what's really going on with this idea that's called predictive coding in
the computation neuroscience literature and the sort of related set of ideas called active inference
with a different kind of confusion, which is all about, you know, that category theory and
this sort of high dimensional algebra that goes along with it. And I suspect I'm sort of,
I'm in a happier place with that second kind of confusion. It suits me better. But I also think that
it helps us to understand a kind of idea, which at least as far as I was concerned, wasn't very
well, wasn't, wasn't crisply or clearly expressed so far in the literature that had been written
in computation neuroscience, possibly because, you know, fair the interest is in specific models
or specific bits of the brain, whereas my interest is really in just, you know, what brains do or
more generally what adaptive systems do. And what's emerging now in the applied category theory world,
bringing together all those different domains that were listed on that slide earlier,
is this kind of this idea, this thing called categorical cybernetics. It sort of uses this
kind of nice general language of category theory to bring together ideas in, you know,
economic theory and machine learning and reinforcement learning and computation neuroscience
and show what are the commonalities and allow people as a result to talk to each other a bit
better and to understand what they're really thinking about in each other's language a bit better.
And so what I'm going to do is maybe sketch some of this. I'm going to try not to assume too much
background in either neuroscience or in mathematics or particularly in category theory.
I'm going to talk about this idea of the Bayesian brain and this related idea of predictive coding.
I'm going to talk about kind of current understanding of probability theory using this
compositional language. I'm going to talk about dynamics and I'm going to talk about how we can
see predictive coding is basically a way of translating certain kinds of statistical models
expressed in this first compositional language to the kind of compositional dynamical systems
that are expressed in this second language. And so when David was talking about how categories
are really useful for talking about things like translation between, you know, different
mathematical languages, that's really what we're making use of here.
Okay, the first thing I want to do though is to say a little bit more about what I mean
or what people mean when they talk about the Bayesian brain. And this
related set of ideas called the free energy principle and predictive coding. It's a very
sort of prominent idea in computational neuroscience and it may be slightly controversial too,
but it's a claim that most of what the brain does and indeed sort of most of what kind of
living systems or adaptive systems do generally can be understood as approximate Bayesian inference.
And so what that means is that you have a system and it's trying to predict what's going on in the
world, more precisely maybe predict what set of data it's receiving. And then on the basis of the
set of data it does receive, update its beliefs about the structure of the world or what's out there
in order to make its predictions better or more vertical. And this thing called the free energy
is a quantity that you can optimize in order to do inference of this kind in a kind of better way
or to improve your doing of inference. And if you minimize this quantity, at least
in certain contexts, you get dynamical systems which look a bit like dynamical systems that
people had already written down earlier in the literature in computational neuroscience that
relate to various bits of say the visual cortex. And so that alignment says to a lot of people,
okay, well maybe this is what those circuits are actually doing. One of the difficulties you get in
if you're just going straight from the statistical model that you might be interested in to dynamical
systems, you're going very manually, is that there are kind of a lot of moving parts and
particularly if you go from one situation to another you might have to
re-derive all of your dynamical equations again. Even if when you look at the brain it has some
kind of nice modular structure and if you look at these statistical models so they do too.
And this is a kind of hint of compositionality and so it seems to be kind of a kind of neater
thing to do to see what the essence is of this process and to try to make this translation
once in a compositional way so that you don't need to do it over and over again
and end up with very sort of impenetrable papers full of difficult equations. You just end up with
impenetrable papers that require you to know a language that you maybe didn't already know.
But I think what I hope is that people start to learn mathematics in a compositional way
in the future and so that these papers or this work that is expressed in this category
theoretic language doesn't scare people off in a way that it might do now. I can hear some
That's fine thanks. Okay so I mean I don't want to spend too long talking about
why category theory because David's done a very good job already. I'm just going to say some
generalities and hopefully they sound nice and convince you that this is not a crazy thing to
be doing. That category theory is like the mathematics of composition of sort of pattern
because it's about translation, it's about interconnection, it's kind of about metaphor
too. It's about saying okay well here's this pattern it crops up in lots of different places
and maybe this is like this other thing and that's kind of like a kind of translation.
And as we've been indicating that it seems to promise to supply a kind of lingua franca
for mathematical science generally so that you know each different sort of category of systems
could be like a little library in this in the sense of computers and computer programming
that you can import it and have access to all the kind of nice tools that are available but
because it's in this common language they all these different parts plug together very nicely.
And that's you know this happens because categories they collect into this bigger category of
categories so they it's that kind of supplies its own foundations as David was indicating
and this allows us to use the morphisms between the categories themselves these known as functors
to translate from one of these libraries or mathematical universes to another and thereby
sort of connecting them together but so that's kind of like one major benefit of using this
language and another one is that it kind of because you have to say like what are the interfaces of
your system like where do they start and where do they end or you know what do they connect from and
to it kind of enforces this discipline it kind of enforces a kind of clear thinking it enforces
an effort to carve nature at its joints in this kind of platonic way.
Okay so to make a little bit more progress at least with the mathematics I want to try to say
briefly what a category actually is it's a collection I often do it with C for category
like this kind of with this little tick in it it's a collection of objects so these are the
things that are being related often denoted C0 and then for each pair of objects you have a set of
ways of relating them to each other it's a kind of it's a set of edges or morphisms
so if you write them like this it goes from A to B and the key detail is that if you have a way
from going you know away from way to go from A A to B and then B to C as I've drawn down here you
get a composite morphism like this and so it's a bit more than I mean it's quite a lot more than
just a graph it's it's a graph where the edges compose and this is where the compositionality
comes from there's also an important extra detail which is that each object is witnessed by a morphism
of its own called its identity and then you've got some axioms that say okay well this composition is
associative and it's unital with respect to that identity those kind of details aren't super important
from what I'm saying today but I think I just want you to have in your mind this notion of
compositionality and particularly compositionality of morphisms once you've got this structure
you can start to study it in itself as David was also talking about and one of the things you
can do one of is prove this fundamental theorem and it has a sort of quite in some sense trivial
but very deep and sometimes confusing proof it's called the yuneda lemma and it and it says something
that's very familiar to us in everyday life and this is a kind of a maxim from a linguist I think
called Frith who said you shall know a word by the company it keeps and the yuneda lemma generalizes
this to all these categorical contexts that you shall know an object by the company it keeps in
the sense that if you know how a thing is related to the other things in its milieu then you can
identify the thing itself and that you know we know not just from linguistics but if you know
how to get to a place from the places around it then you can you know figure out what where that
place is on the map so you can sort of identify that place and it's a very it's a kind of very
it's a very powerful result and it's a kind of nice kind of nice evidence that category theory
captures something about the way the world is built up okay but the thing that you know I want
to talk about really today is the brain and using the tools of categorical cybernetics to model it
so the brain is this nice complex system it's made up of other complex systems so it's made up
of neurons which of course have a lot of structure of their own and it has this kind of computational
behavior it does prediction and it does planning and it does all these things that we have programmed
computers to do now and you know brains themselves compose into bigger systems societies so it has
a lot of features that make it again that seem to suggest that it should be amenable to description
to description in this categorical language and indeed it should be comparable to other systems
that people have started to study in category theory or in applied category theory cybernetic
systems this really I guess at least in this form kicked off with the the introduction of
compositional game theory by Jules Hedges and his colleagues and so there they study economic systems
and those are very closely related to systems of reinforcement learning
and indeed structurally also related to systems in machine learning as it is
in the form that you know is given rise to things like chat gbt and these things all have a similar
kind of structure there's some kind of process which might have some kind of control inputs
and then you know there's another part which goes backwards the kind of retrojectory part
a sort of feedback process and the process and possibly the control and the feedback
you could think of them as all being contained within one system that is in kind of traditional
cybernetics maybe called a plant or in control control theory called a plant but this is really
just like a part of a bigger system it's open to some environment and the brain has you know
similar structure at least in these kind of visual cortical areas that have these predictive
coding model models that I introduced at the beginning and so there like a little bit of
neural circuitry maybe has one part which points from deep inside the brain here to out towards the
world here and it does say prediction about what what you know the eye should expect to receive
and then there's another set of neurons which goes from nearer the world to deeper inside the brain
that that feeds back the error in that prediction and allows the brain to form a sort of updated or
new belief in Bayesian probability it's called a posterior belief and so you have this kind of
bi-directional process which looks a bit like a kind of cybernetic loop and so I'm interested in
not just the brain of course the brain is a great target and obviously I you know pardon me as a
brain I want to understand that but I'm interested in systems that do this kind of thing adaptive
systems in a sort of general way and because of that I'm I'm keen to use these general tools
and to study this system that is so good at doing this kind of adaptation which is the brain
okay and so you've seen me draw on the previous slide a diagram and it's a diagram of the kind
that if you know about category theory or you did some searching as David was suggesting
is known as a string diagram and so I want to introduce categories again or at least composition
again using this language so that when I use these diagrams again a bit later you'll know what I'm
know what I'm doing the great thing about using string diagrams is of course for most of us
we have this very well developed visual system and so we can not only use category theory to
reason about it but we can also use it to help us reason about mathematical things like categories
themselves in particular you know when we use pen and paper it's a two-dimensional medium and it's a
kind of often for processes where things can go in parallel as well as sequentially sequentially
it's often more convenient to to draw the processes out as we see when people draw other
kinds of more informal diagrams than it is to write them in this kind of more sort of traditional
algebraic symbolic way that you could write on that drawn on the right hand side and so what
we can do is if we have a process F that goes from A to B so we're going to start by reading on the
left and then if you have another process G that goes from B to C we can plug them in and so then
you have a process that's G after F which in traditional mathematical notation is denoted
like this and so that's like the sequential composition and we can also put another process
H from X to Y by the side of it and so this thing now goes from the tensor A tensor X to C tensor Y
and you have as I said each object has an identity morphism and this is just what the string itself
represents and you can put this next to you know processes and you know you can use this to plug
things together you have special processes that start in like the empty diagram which we write
with this I and they we think of as like states so in physics they will be like vectors or in
probability theory they're like distributions and then you have things that go the other way that
sort of terminate in an empty diagram and these might represent like say predicates on some space
or they might represent effects that you could measure or measurements on your physical system
and these are kind of morphisms in the other direction into this other into this unit diagram
into this unit object and with these diagrams okay you can not only sort depict the processes
themselves but you can write down equations and start to manipulate the diagrams accordingly
and so here at the bottom as an example I've written the equation that represents the co-associativity
of of copying so it says if you copy once and then you copy one of the copies it doesn't matter
which one of the copies you copied and you know this has this nice kind of fairly obvious graphical
representation here which is a bit more I think natural to use at least for this kind of thing
than the sort of algebraic sort of symbolic notation that maybe is more familiar to people from
mathematics. One of the things that we can represent in this language is probability theory
and this you know the kind of natural kind of like you know mathematical formalism to use for
that is that of Markov kernels which you could really think of as like conditional probability
distributions and so here I've got this thing c it goes from x to y and we could think of this as
like a conditional probability distribution of y given x but now we have them these things in this
compositional language we can plug them together so if we have one and then another we can compose
them together by sort of marginalizing over the middle thing and this says okay well we'll sample
an output we'll sample an output from y and then we'll give it to z give it to d and then we'll
run whatever process d represents and that'll give us something in a distribution in z and so this
is like some something it's like d after c z given x we can do copying in probability too and this
is like you know it just sends a thing to the delta distribution on the copy x equals x prime
which you might be familiar with and so here we can and we can use this to formalize in this kind
of graphical language joint distributions and so here we've got a joint distribution with one thing
on with one marginal on x and a kind of what's called a probability a likelihood which goes from x
to y and this thing depicts the kind of this sort of this representation of the joint distribution
is like a likelihood a conditional probability distribution on y and a prior and this is the
kind of one fundamental part of Bayes law that you might be familiar with and as I indicated on
the previous slide these kind of co-states or effects they represent things which we could
think of in probability is like fuzzy predicates and that might be something that if you're
philosophically inclined you you've thought about we're not going to use them very much but we will
