use them to represent loss functions in machine learning and in predictive coding later on
okay so at the heart of this kind of predictive coding idea is Bayesian inference and what Bayesian
inference does is effectively takes one process here this goes we're reading the diagram upwards
now just yeah first reasons of space here this goes from x to y and so we can think of this
as predicting something in y given something in x but you know it's a kind of stochastic prediction
it's fuzzy and so Bayesian inversion allows us go to go instead of from this process which goes from
x to y to a process which goes from y to x here and so this is like reversing this process it
allows us to say okay given what I observed what should my belief what my belief a better belief
have been and the the criterion that it satisfies is that the kind of these are two ways of writing
the same joint distribution so you start with a prior belief that says okay well this is how I
think the world is and then you have this way of generating data and this says okay well this
the joint distribution you get over the say causal structure of the world and the observations is the
same whether you go forwards or backwards and so this this kind of makes it like a well behaved
inversion so it's a kind of it's a kind of fuzzy way of inverting this this this predictive process
and Bayes law is often written like this thing it's just a way of expressing the conditional
probability in the other direction and it just comes out of the kind of laws of probability
that you might be familiar with this is the product rule of distributions
but I'm not I'm not here to give a tutorial about you know about fundamental probability theory I'm
going to try to build up to predictive coding I'm start I've already introduced categorical
probability theory and I want to get from there in the kind of piecemeal way from like simple steps
using like simple you know simple like compositional parts towards the dynamical systems that represent
how the brain does this inferential process actually dynamically
and so as I indicated you know a brain it has lots of parts and they sort of plug into one
another in particular it has some parts of the brain which are nearer the retina or the eye
and other parts of the brain which are kind of what we think of as more sort of high level
that represent you know objects or concepts rather than say what we might think of on a
computer as pixels and so somewhere in the brain you have to go from these kind of low level features
to the high level you know high level ideas and this happens in this kind of hierarchical way
so zoom me out piece by piece and so you know you might imagine being say at the cinema
and you're sitting right at the front and you're looking at you know looking at the
the screen that you can only fit in your visual field say a part of the screen at one time
but yet you can sort of keep track of what's going on everywhere and so you maybe have this
process where you've got you know a belief about what's happening on the whole screen
and also not the process where the eye is receiving signals from part of it and
the first thing that happens is your brain updates the belief about what's going on in that little
part and then it can update using that new belief yeah the you know the some high level
belief about what's going on on the whole you know in the whole scene or in the story more
generally and this is a hierarchical process and really it's got this kind of compositional structure
and so we might wonder okay given in this kind of abstract language a sort of kernel
from x to y and another one d from y to z what its inversion is so what is the inversion of the
composite channel or kernel and how does it relate to the inversions of the parts and so this is a
kind of question about the compositionality of Bayesian inversion and it's not only relevant
and predictive coding of course it's relevant in all sorts of contexts particularly in you know
say things like probabilistic machine learning and things like that and so it's actually quite
easy to show that the composition of of the inversions gives you the composition of gives
you the inversion of the composite channel an important thing to notice from these pictures
is that the inversion say if we just go back to this this this kind of defining law this is
what we could think of as a kind of abstract Bayes law that the inverse channel here the whole
law in fact but particularly the inverse channel depends on the choice of prior because if I
change this if I change it to pi prime it wouldn't necessarily satisfy this law and so there's a
kind of dependence here you see it comes in the in the in the sort of defining equation here you've
got pi popping up and so we can't just sort of in some sense reverse the channel directly
we need to say okay well what is this prior that I'm reversing it with respect to and so we'll see
okay well if we use this law twice we can you know invert first of all d but you see we've got c
coming in first and so that the the prior for this inversion of d is c after pi and if we invert you
know if we invert if we invert oh sorry we should invert c first the prior is pi and then we invert
d afterwards but the prior is c after pi we get this composite inversion over here
and so this says okay well we can decompose the Bayesian inversion of composite channel
in terms of the inversions of the factors here and here and in particular this kind of
this kind of structure of this dependence what is this this thing how does this arise if we try to
package this up neatly we will end up using what is called a lens and this is something that's
already been used in categorical cybernetics and in categorical dynamical systems theory
and in all sorts of bits of category theory already to formalize these kinds of binary
processes and so it's very nice and so maybe somehow mathematically inevitable but this
kind of formalization also happens to be the way that Bayesian inversions compose
and so we can say a little bit about this so I've indicated that these inversions
see this is the inversion of the channel c on the previous slide c x to y the inversions they
depend on the priors and the prior is a state on the domain of the channel
and so we call the inversions which we might write as just c dagger it's dagger for you know
technical reasons it's it because it effectively makes up what's known as in other bits of category
theory a dagger functor but this thing is like a state dependent channel and so it takes in a
prior and it gives us a channel in the backwards direction so this is a function from you know
states to channels like this and in fact we can compose these things if we have a state
dependent channel from a to b and it depends on x and another one from b to c that also depends on
x we can copy the state that we depend on and give it to both parts and then compose those parts
together and that you know is something that should be hopefully quite natural to see in this
diagram if we sort of follow along the flow of the information through it and this means that
the state dependent channels on a particular state so these all depend on distributions on
x but these things form a category and we can you know write down the morphisms the set of
morphisms in this way so not only though do the state dependent channels on x form a category
for each x but also if we have a process you know from y to x we can say reindex these these
state dependent processes so now we can pre compose the inputs to f and f prime g prime over here
by say this morphism from y to x so now if we have a state on y we can also feed that through p
and then feed that that composite state into f prime and into g prime and still do the same
composition and so this gives us a mapping indeed a functor from the category stat x to stat y
and so that's going to be important because as we've seen here it maps like this
but what we did before is we had an inversion on d and we pre-composed it with c to give this thing
d dagger c after something and so this this this reindexing captures what's going on
in the composite inversion story
and we can sort of you know use category theory or categorical tools to package all
of that stuff up very nicely um we can pair an individual predictive channel
c with its state dependent inversion c dagger and a pairing of a channel or kernel
with an inversion which may or may not be the exact Bayesian inversion that satisfies that equation
but a pairing of that kind is what I call a Bayesian lens and I don't have time to explain
how we obtain the category of these things but it is obtained using a sort of canonical tool
of category theory called the growth and deconstruction so we had this index category stat
of state dependent channels a family of categories for each possible space x
that's compatible with this reindexing if you have an index category of that kind
you can apply the growth and deconstruction and that gives you another category that is a you know
the morphisms which are pairs like this and there's a kind of composition rule that comes
out of the growth and deconstruction which tells you how to compose these lenses and it in fact
captures exactly what's going on with Bayesian inversion and so Bayesian inversion at least
its compositionality has this nice and sometimes universal property that because it because it
arises in this kind of abstract way it's not an ad hoc thing that depends on the situation at hand
or the kind of problem you're studying it's it's a kind of mathematical consequence of the structure
of you know what's going on in Bayesian inference and you know we can use string diagrams to really
spell out this compositional structure and it captures the picture I drew earlier of this
predictive and updating process so now we have these two two little lenses here and here
and so one now you know goes from x to y and we can think of this as doing like a high level prediction
and then here we've got another goes from y to z and we can think of this as being this
lower level prediction and we can compose those things together to get a prediction that goes
from the high level to the lowest level and we can also follow the information and flow backwards
so now suppose we've got some feedback from the world here well okay we flow through
the forward information and this gives us the kind of prior
d c after pi and then this gives us an updated intermediate level belief
and now say we had pi here now the inversion here takes in pi c prime pi and this gives us the
higher level belief and this this thing here is exactly what we had from basal and so this is
what I mean when I say that you know the Bayesian updates compose optically they compose according
to this lens pattern and as I've already indicated this this comes up a lot in categorical cybernetics
in fact this kind of chain rule is exactly how that propagation works and it's it's how sort of
utility is passed back in economic games and so it's interesting that this this kind of nice
general construction captures all of those all of those all of those situations and in such a
way that actually applies to this physical system that that is the brain
so I've said that there are some of these lenses you know c c dagger where this is
some state dependent channel that actually satisfies basal law as I drew it a couple of slides ago
but there are lots of other lenses where c prime has this type the same kind of type say it goes
it takes in oh sorry takes in a y and it depends on some prior in x and it gives you some new
updated x there are lots of lots of state dependent channels of this form and they're not all
exact Bayesian inversions we could think of them as like approximate inversions
the way that we could formalize this chain rule in in the kind of neatest way of exact
Bayesian inference is as what's called a section of this vibration so Bayesian lenses
form a category where the morphisms of these pairs and we can sort of project out we can forget
this part just to end up in c and a section is effectively a mapping from c into this
vibration above it such that if you do this round trip you get back where you started
and what makes a mapping a functor is that it preserves composition and so what's going on here
is that we map c to c dagger and we also map d to d d dagger such that if we map dc
we get the the inversion of dc which as we've already seen
is this thing c dagger after d dagger c
and so the fact that this this mapping of channels to lenses where the second part is
the exact inversion is a section exactly says that Bayesian updates compose according to this law
now again this isn't the only section of Bayesian lenses there are others and those ones represent
approximate inversions that are particularly well behaved because they satisfy this
compositionality property and I call those things inference systems and and finding an
inference system is like the first step in in sort of defining the approximate inference process
that is predictive coding but you know once you've got an arbitrary inference system you
you know it might not be very good at actually doing prediction or updating of beliefs if we
think of Bayesian inferences like the optimal such process and yet nonetheless you know evolution
hasn't just picked brains to be instantiating some like arbitrary inference system it's chosen ones
that are actually quite good at doing prediction and updating and so there must be something going
on that allows evolution effectively to measure how good a system is and you know that's what
natural selection is doing and similarly there must be something on something going on that
allows brains to improve their predictions or effectively to learn and what I'm going to talk
about now is how we can equip each Bayesian lens with a loss function effectively a way of measuring
this this this goodness of a prediction but because we have this compositionality hanging
around it satisfies some nice you know some nice constraint that is enforced by biology which is
that neuro computation is what you know what biologists or neuroscientists say is local that
means that the the data required of the computation must be sort of available to the neurons themselves
it shouldn't depend on something that's going on further away in the brain the information has to
be accessible in order for the neurons to actually do the computation they need to do
and so because we have this compositionality if we enforce that of say the loss functions or the
lenses as well that it that makes all of the the information local to the individual parts of
the system and so that makes it kind of nicely biologically plausible thing which is a bit
different from what happens in say back propagation machine learning where changes to some parts of
the system may depend on the way you measure how good the performance is sort of filters by
sort of filters back to the whole system at once the loss functions there aren't necessarily
compositional another thing that's important to bear in mind at least when it comes to
approximate inference systems is that their performance kind of depends on the problem
they're trying to solve so obviously evolution has selected different organisms for different niches
and so you know if you change an animal from one niche and you put it in a different environment
it might not survive as long and that's because you know it's yeah we could say that abstractly
that's because its inferential performance is kind of context sensitive it depends on the
beliefs it has about how its environment is structured and it also depends on like how it
actually you know yeah absorbs these observations that it has you know what those observations are
it might be it might not pay attention to certain things which in one environment might be less
important than in another one example of a loss function that's particularly natural and relevant
in probability theory is known as the relative entropy or the Kuwak Leibler or KL divergent
and this measures the difference from one or it kind of measures the distance or divergence
from one distribution to another on the same space and we'll see in a minute that these things that
these relative entropies also themselves satisfy a nice compositionality property which again comes
out of a grid deconstruction and so in its structure is also kind of captured by this
abstract categorical machinery so one of the reasons the relative entropy is so important in
approximate inference that you may know or have already guessed is that if we minimize the divergence
say between this approximate posterior which I've written as C prime pi and the exact Bayesian
posterior C dagger pi effectively what that does is it pushes the approximation closer to the kind
of true or the optimal the optimal inference and so often people start an approximate inference
process by trying to figure out a way to minimize this quantity but you can see here again that
we have this dependence of the this quantity this divergence on the prior because of course
the inversions depend on the prior and on the observation so why is the thing that goes into
yeah the system like this we had this diagram before
and so the actual divergence in any particular context depends on what the observation was
and so if what we want to do is compose these these loss functions for one say one little neural
circuit and another little neural circuit we should you know figure out a way to pass the
information of this observation and this prior into those loss functions
if we write down the type as I said at the beginning this kind of discipline about types is
one of the things that makes category theory to me so useful if we write down the type of this
function we'll see that it corresponds to one of these effects or co-states that I drew earlier
as long as you think of it also depending on this prior so what I think I call that a state
dependent effect effectively it's a state dependent channel that lands in this kind of trivial kind
of empty diagram object here what that means if we interpret this type in the mark of kernel
language that I introduced earlier is that it's a function that takes in a prior and then an
