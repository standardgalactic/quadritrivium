And it needs to be able to select between the two.
In other words it needs to be able to make inferences about the outcomes of certain actions were it to actually pursue those actions.
So that suggests that to do surprise or negative log evidence it must be able to sample across different probabilistic outcomes of its own actions.
So survival is premised on having generative models with a particular kind of temporal thickness which allows you to make inferences across possible counterfactual scenarios where you have to act in a particular way.
Okay, so now I'm going to segue back.
So I started with Chalmers and the extended mind thesis and Chalmers' proposal for the strongest possible objection.
And Chalmers works from a kind of common sense view of perception and action.
And he thinks that there is a criterion in appeal to demarcate between the mind and the rest of the non-mental world.
There's a sense in which you find a similar kind of argument in Yakovsky's work.
But not in a commonsensical way.
So this is formalized through the notion of a Markov blanket under variational base.
Let's see how such an argument could play out.
Hope that's an okay picture.
So here we have the brain again and forget about formalisms.
But think here that the brain is enveloped, if you like, by a Markov blanket, allowing you to have a separation of internal and neural states from say external states.
Think of external states as bodily states and environmental states.
And that separation is given by a third set of states, active and sensory states.
So here one claim is, given the Markov blanket, the Markov blanket insulates the internal states from external states.
If that's the case, then the internal states don't have any sort of direct means of assessing the external states.
In other words, you can't leave your brain and go check on what external causes are currently impinging on your sensory.
So that suggests that you need to make inference about what is the most likely external cause that explains the sensory states that you are currently having.
Conditional independence.
So knowing sensory and active states renders external states uninformative with respect to the internal states.
So I don't have to go out and consult the external states in question.
Once I have information about sensory and active states, I have all the information that I require.
Or the brain has all the information that it requires in order to carry through the set of top-down predictions.
It's just a quote.
So in more technical terms, there is sensory input and active output at this boundary forms a so-called Markov blanket,
such that observation of the states of these parts of the system together with observation of the prior expectations of the system
in principle will allow prediction of the behavior of the system such.
Courses beyond this blanket, such as bodily states or external states, are rendered uninformative once the states of the blanket are known.
So the Markov blanket just repeats, separates internal states of the system from external environmental states.
Sensory active states comprising the Markov blanket can be thought of in terms of perception and action,
essentially shielding the cognitive agent from the rest of the cognitive world.
If that's correct, it gives you a Markov blanket inspired perception-action boundary for the mind.
There's lots more to be said in terms of the technical details of this, but I take it this is roughly the picture.
I quite like the Markov blanket.
It doesn't involve having to appeal to, say, intrinsic or non-derived content as a way of separating the cognitive from the non-cognitive.
It doesn't involve having to draw, say, kinds of distinctions between what's causation and what's constitution.
It doesn't involve having to appeal to minimal supervenous bases, etc.
In other words, it doesn't carry with it a bunch more, at least some potentially unjustified philosophical assumptions
about how to carve up the mark that meant for the joints of the cognitive system.
In that sense, it has a lot of versions.
So the question is why worry about this particular picture?
Why worry at all about, say, a brain-bound view induced by a Markov blanket,
and why worry about it for your life as a cognition?
So this is Michael Anderson, and he has one word.
It's the kind of converse of cognitive bloat in extended cognition.
So if we call, if we're willing to accept that by manipulating or having a notebook available on a certain conditions,
the notebook or at least the inscriptions should count as dispositional beliefs,
then you might worry that, well, Macquarie University Library has a bunch of books,
and they may serve the same sort of purposes on this set of conditions,
so that's the kind of worry about cognitive bloat.
Anderson's worry is something like close to unrestricted, as far as I can tell, shrinkage of the mind.
So the idea here is that if you draw a boundary, a kind of action-perception boundary between mentality and non-mentality,
then at least on the Markov blanket view, it seems like you can continue to push that boundary further and further and further in.
That seems pretty straightforward because, statistically speaking, the singular cells got the same properties as a bunch of Markov blanket cells.
So there is active and sensory states.
Those active and sensory states separate external and internal states in that statistical sense that I've been speaking about.
It's not as hard to see how it's any different if you've got a much larger Markov blanket, say, bounding the brain,
or bounding, say, internal, neural states from external, bodily and environmental states.
So you get the kind of same separation of the internal and external states via sensory and active states.
So it seems like if you want to have, say, a single or a stable Markov blanket that separates mentality from something else,
you can push that kind of picture further and further and further in.
That's how I read the worry at least.
So maybe the idea is to move away from thinking that there is a particular blanket we should appeal to.
So I take my staff from a nice paper by Mika Allen and Carl Frister, and they say this is a 2016 paper in Sintesa.
They say the delineation of the boundaries of the Markov blanket is essentially relative and variable.
Any organism will be defined not by a singular Markov blanket, but by a near-infinite regress of course of the interacting Markov blankets within Markov blankets.
So that kind of speaks to this idea that biological systems are hierarchically organized.
There are kind of systems of systems or systems within systems.
So you can think here of cells assembled to form tissues.
Think of tissues organizing to organisms.
You can think of organizing only into organisms.
And of course you can think of all that as by taking in an embedded environmental niche as well.
So the organism is a system with a much larger environmental system.
Any one of these systems micro to macro, if we thought to have a Markov blanket,
merely by being able to pick it up from something else.
This is implied as I said in virtue of having distinct boundaries.
So complex living systems that comprise the many Markov blankets,
all of which exhibits the same statistical structure.
They all have active and sensory states and they all separate internal states from external states.
Effectively what I said, kind of systems of systems view or a complex view of the organization of biological systems.
So what you can start doing if you're interested in this sort of stuff,
is to start creating simulations of Markov blankets.
I'll take it as what Andy referred to earlier as one of these very basic, simple forms of Fristonian simulation.
So hopefully it's not overly significant.
What we have here is 16 cells.
We're not scaling up to anything that smells like human cognition or anything like that.
All we're wanting to explore here is whether you can kind of make sense of the idea
that single cells can self-organize into something that has a Markov blanket structure.
So imagine that you have an undifferentiated bunch of cells.
You run the simulation, you kind of see the spread out like this.
We have a prescribed point attractor for each cell, an ensemble of 16 cells.
The point attractor determines how the cell or the example, in this case, will converge.
The point attractor here takes the form of a Markov blanket.
So cells come to accept the conditional dependencies and independencies highlighted by the petitioning rule governing the Markov blanket.
Internal states, in this case the little kind of orange blob, are surrounded by active states
and the active states are surrounded by sensory states.
All you have to do in order to achieve this, at least on the sort of, the simplistically speaking,
is to equip each individual cell with a generative model that carries expectations about its own identity, if you like.
Not so much about where it's meant to be placed, but what kind of cell is meant to be.
So if you're predicting that you're an internal state and you're receiving sensory stimuli from sensory states,
then you're going to move into a position of the orange.
Similarly, if you're an active state, you can only have direct influence from internal states.
So if that prediction is fulfilled, you're going to see the active states organizing themselves around the internal state and so forth.
So that's the idea.
What we want to try to ask is if we're comprised of a bunch of different cells or a bunch of different systems,
all of which have a Markov blanket, and we're interested in thinking about systems that are Markov-blanketed systems
in the numerous sense of being Markov-blankets, of Markov-blankets.
Then we want to try to see if we can scale those simulations up,
so if you have the self-organization of Markov-blanketed systems at the micro-scale
and Markov-blanketed systems at a much larger scale,
comprised of a bunch of other Markov-blanketed systems.
Sorry for saying that word by many times.
So here we have 16 ensembles, comprised of 16 cells, 256 cells in total.
Exactly the same as the first simulation.
Each cell comes equipped with a generative model,
but this time of its identity at the micro-scale and at the level above at the macro-scale,
what particular type of cell it's meant to be, whether it be an active sensory or internal state.
Given posterior beliefs about its role in both scales of organization,
the cell or the collective can predict the local and global intracellular signals it would respect to receive.
The ensembles, or the collective of collectives in this case,
have a point of tractor to which it converges, just as in the previous slide.
Again, the point of tractor is a Markov-blanket,
where the Markov-blanket is made up of 16 photoplankets.
So we can see, take an individual cell here, like that one,
see it's comprised of internal states, active states and sensory states,
and similarly you can take that for the much larger or bigger Markov-blanket,
which is comprised of the large red cells, the large orange cells and the large yellow cells
distinguished from the surrounding blue cells or the darker cells.
So the organization of the macro-scale Markov-blanket can be thought in this case to be involved
in free energy minimization, just like the organization of all the Markov-blankets at the micro-scale.
That should follow fairly straightforwardly from the claim that,
if you think about the minimization of free energy in this particular cell,
it's carried through by internal states, active and sensory states,
something similar is going to happen when you scale that picture up.
So there's no principle reason for thinking that you can't recursively scale up this organizational structure.
So in this we've only done simulation of how cells that are Markov-blankets assemble into systems
that are the larger scale that has a Markov-blanket,
but there's no reason to think that you can't have much larger and larger Markov-blankets,
comprised of even larger Markov-blankets.
You go all the way down, which we've heard, you go all the way up to UNI,
and in certain cases I suspect we can go out to include part of the environment as what comprises our Markov-blanket.
Okay, so here's just a few implications.
There seems to be kind of no unique or fixed Markov-blanket that we can appeal to.
If there is a Markov-blanket and that Markov-blanket can work as a boundary of the mind,
it seems that we should accept that the mental boundary is multi-layered and nested.
So this doesn't prove anything, but it opens up the door for thinking of the boundary of the mind
as not simply being the boundary of the brain.
Appealing to perception and action is setting a principle boundary for the mind as unlikely to work,
putting channels as internal states in the Markov-blanket can all be cast at doing precisely the same thing,
optimizing model evidence, which is the same as minimizing free energy.
I don't want to, actually, I have put that in just in case I didn't have enough time,
but since I am nearly out of time, I'm going to skip my own objection.
Thanks.
Thank you.
The first question would easily be, if you would like to do a presentation.
Sure.
But Ken, press, and then, for the back, please, can you hand up the phone for me?
Thanks for the topics of educating me on this business.
You said there are nested Markov-blankets.
Okay, so I want to ask you, and that slide from molecules to ecosystems really caught my mind.
So I want to ask, no, one before biological systems.
So how far down does the Markov-blankets go?
Are the absence in our eyes, do they have Markov-blankets around them?
Do they have atoms and ions that, you know, they have?
Are biophysicists and biochemists missing something by not learning about Markov-blankets?
Or does it go down to a level of where the minimal cognitive is, as I call it,
defined cognition, which is some form of defining boundaries?
Pharrella and Thompson, who's the other, what's the other name?
Well, I guess the simple sort of answer to that question is,
wherever you can find a statistical separation of internal and external states,
states through a set of other states, you're going to have something that has an open field of Markov-blankets.
So how do molecules?
So if it turns out, not a molecular biologist,
but if it turns out that you can carve out that separation between internal and outer states,
given a third set of states, active and sensory states, then you have a Markov-blanket.
Yep.
Oh, sorry.
This is just really sort of adding Christy on the other thing with regards to the nesting of Markov-blankets.
But in the upholstery newspaper, he points out, in a small paragraph,
that you can actually, by taking the sort of bottom level of a generic model out,
you effectively have a Markov-blanket predicting that all the states up,
all that it lays in the model up until that,
and until that later you can have a Markov-blanket that's doing the job of predicting
what that top model is going to send back.
So it looks like you can have, as you say, you can have Markov,
you can extend the Markov-blankets outside what we used to think was the mine,
we can make it right down to the cellular level and beyond, perhaps.
So within what we traditionally think is cognition, you can have, I think,
a quote-unquote-trainish cognition.
Notice that at no point in this talk have I said something like,
will we be defending that there is one Markov-blanket that rules them all,
or there's one that owns them all?
No. If you're committed to hierarchical predictive processing,
or just hierarchical generative models,
the reason, in part, that you have those is because you've got Markov-blanket
organizations running up and running down the hierarchy.
Yes, I guess what I was wondering is this,
