I've got a bit of trivia for you guys.
A 5th year old woman, no symptoms, participates in routine screening for breast cancer.
She tests positive, is alarmed, and wants to know from you whether she has breast cancer
for certain or what her chances are.
Apart from the screening results, you know nothing else about this woman.
The prevalence of cancer among women her age is 1% and the test is 90% accurate.
90% of the time it gives the correct diagnosis as to whether or not someone has the disease.
So the question is, in light of her positive test result, what are the woman's chances of having cancer?
Is it 80%, 90%, 22%, 8% or 1%?
You can think about that for a second, come up with your answer, I'll wait.
Now, you might have said B and if so, I'm sorry to say but that's wrong.
It's false, no way, not this time, we created it.
Not this time, no, not this time.
The answer is actually D, 8%.
And after watching this video, you'll know exactly why that is.
Welcome everyone to The Majesty of Reason, I'm Joe Schmid and today we're talking about Bayes Theorem.
This video is actually requested by a patron, Jeremy, so thank you Jeremy so much for your support.
If you guys see value in the work that I do, consider becoming a patron.
There are so many perks included such as exclusive videos, notes, papers, books, and depending on your level, personal meetings with me and video requests.
All patrons, by the way, have access to the 37 page script that I made for this video.
But anyway, back to Bayes Theorem.
So named after Reverend Thomas Bayes, an 18th century English statistician, philosopher and Presbyterian minister,
Bayes Theorem is a powerful tool for updating our beliefs in light of new evidence.
Its applications range from artificial intelligence and machine learning to medicine and science to philosophy and well beyond.
My goal in this video is to give you a foundational understanding of what the theorem says, why it's true, how to use it, and how not to use it.
Also, don't worry if this all sounds intimidating or beyond your grasp, I'm going to try to make it as accessible and understandable as possible.
So even if you're not a math person or even if you're somewhat new to philosophy or you haven't studied probability theory or statistics, this video shouldn't be too daunting.
It may stretch you at times, but I'm here to help as much as I can.
Alright, so all of y'all know that I love my outlines.
I'll begin by briefly introducing Bayes Theorem itself, after which I'll give some background on belief and credence and probability and things like that.
Then we'll look at the theorem itself, and in particular we're going to be looking at two different forms of the theorem as well as how it interacts with the Bayesian notion of evidence or evidential confirmation.
We'll also be considering some proofs of Bayes Theorem, why we know that this is true, how we know that it's true, and something very exciting, or at least I find very exciting, I find it titillating, is visualizing Bayes Theorem.
So we're going to walk through some videos about how to visualize Bayes Theorem and how to think about probability visually.
It's super-duper helpful, trust me.
We'll then look at some pretty common mistakes to avoid when using Bayes Theorem and assessing Bayesian arguments, and finally I'll end with pointing you guys to some helpful resources.
And note that all throughout the video we'll be applying the theorem to tangible problems.
Alright, but the big question in this first section is, what even is Bayes Theorem?
Well, Bayes Theorem is many things.
The simplest answer is that Bayes Theorem is a mathematical equation, expressible in several different forms, that allows us to calculate conditional probabilities.
Where a conditional probability is the probability that one claim is true, given that another claim is true.
It also follows strictly from the axioms of probability theory, I'm going to explain how that is later, but that's all that you need to know right now.
That's why it's called a theorem.
A theorem is basically what you can prove to be the case within a given axiomatic system.
But Bayes Theorem is far more than a mere equation that follows from the axioms of probability theory.
It's also a powerful tool for updating our beliefs in light of new evidence.
It's a powerful tool for assessing the probability of various hypotheses in light of data.
And that relates to confirming and disconfirming theories, getting evidence for and against theories.
It's also a formal constraint on rational credences.
That is, it's a constraint that your confidence in various statements needs to meet in order to be rational.
Don't worry, I'm going to be talking about what credences are soon.
Right now, I'm just giving you a bird's eye view of what Bayes Theorem is.
So, yeah, if you're interested in being rational, which I presume you are, you should be interested in Bayes Theorem,
since the theorem expresses how your credences or your confidence levels or your degrees of confidence should rationally change in order to account for new evidence.
Now, again, this is a very coarse-grained articulation of Bayes Theorem,
and I'm not even putting the theorem on the screen yet because we don't have the requisite background to be able to understand it yet.
For this section, again, I'm just trying to give you a bird's eye view.
As we progress through the video, we'll gain a deeper understanding of the theorem, its significance, and its applications.
Before getting to the theorem itself, however, we need to cover some essential background on belief, credence, probability, and Bayes' epistemology.
Don't worry, I'm going to try my best to make this as accessible as possible.
Let's turn to belief and credence.
So, most of the epistemology concerns propositional attitudes.
A propositional attitude is an attitude that an agent adopts toward a proposition.
While a lot of ink has been spilled over the nature of propositions,
here we'll just treat them as the meanings of declarative sentences.
They're basically what sentences express.
They're also capable of having truth values, that is, they're capable of being true or false.
So, for example, the sentence, nuclear fusion is a viable energy source, that sentence on the screen there,
expresses the proposition that nuclear fusion is a viable energy source.
If I believe that fusion is viable, this belief is a propositional attitude.
It's an attitude that I take toward the proposition that fusion is viable.
Epistemology focuses in particular on propositional attitudes that attempt to represent what the world is like.
Belief is in some sense a purely representational attitude.
When we attribute a belief to someone, we're simply trying to describe how they take the world to be, or how they represent the world as being.
We're saying that they're taking a stance on the relevant proposition, that they're affirming its truth,
that they're taking it to be true, or representing the world as being the way the proposition says it is.
That's what we're saying when we say that someone believes a proposition.
And belief, of course, is not the only purely representational attitude that an agent can have toward a proposition.
An agent might also be certain that a proposition is true, or disbelieve a particular proposition,
which is, in other words, believing that the proposition is false.
Philosophers often discuss the class of doxastic attitudes into which belief, disbelief, and certainty all fall.
Doxastic attitude, by the way, is an umbrella term for propositional attitudes that are belief-like.
Doxa is Greek for belief, in fact.
And they're belief-like in the sense of being purely representational attitudes.
And so doxastic attitudes, that category includes things like belief, of course,
but also disbelief, certainty, doubt, suspension of belief, or suspension of judgment, credence, or confidence level, and others.
Now, basing epistemology in particular focuses primarily on a type of doxastic attitude known variously as
degrees of belief, or degrees of confidence, or levels of confidence, or credences.
And that's a term that we're going to be going with going forward.
Your credence in a proposition is basically how confident you are in that proposition.
It's your degree of confidence in it, and it's represented using a quantitative degree to scale from 0% to 100%, or from 0 to 1.
0 representing 0% confidence in something, and then 1 representing 100% confidence, and then, of course, everything in between.
0.75 would be 75% confidence in a proposition.
So, for instance, you might be 100% confident, that is absolutely certain, that 1 plus 1 equals 2.
You might be 0% confident that triangles have four sides, that is, you're absolutely certain that that's false.
You might be 50% confident that the number of stars in the universe is even.
You might be 80% confident that it will rain tomorrow, and so on down the list of your credences.
All of these numerical descriptions represent your credences in the various propositions just mentioned.
Flussers generally like to use the 0 to 1 scale, so your credence in those propositions would respectively be 1, 0, 0.5, and 0.8.
Moving forward, I'm going to be mostly using the 0 to 1 scale, though occasionally, context will make clear when I'm using the 0% to 100% scale.
So, at this juncture, I think it's useful to ask, firstly, what's the difference between beliefs and credences?
And also, why should we countenance credence in addition to beliefs?
That is, why should we even believe that there are such things as credences?
Why not instead just simply represent our doxastic lives as merely containing beliefs?
Why do we have to postulate credences in addition?
Or why do we have to describe ourselves as having credences in addition?
Well, on that first question, beliefs are binary, right?
Given any particular proposition and any agent, the agent either believes it or doesn't.
End of story.
But credences aren't binary like that, right?
They're quantitative and graded in a way that beliefs aren't.
Your belief is simply how you take the world to be, whereas a credence represents your degree of confidence in taking the world to be that way.
It's basically about the certainty or confidence with which you hold your beliefs.
But on that second question, as to why we should even countenance both beliefs and credences,
the first thing to note is that, at least for many propositions, it just seems introspectively obvious that we have credences.
We simply find ourselves with different degrees of confidence in different propositions,
and these aren't binary but are rather graded, right?
For some things, we're very confident, others less so, and still others were not at all confident in them.
So that's the first point.
But secondly, if we only describe agents' doxastic attitudes with beliefs,
we'd be unable to account for many facts about agents' doxastic lives.
To account for those facts, we need to be able to attribute quantitative attitudes to agents.
That is, we need to countenance credences.
So this is the point here about explanatory power.
We basically need to explain various facts about agents' doxastic lives,
and to do that, we need to have credences in our repertoire.
So suppose my physicist friend believes that nuclear fusion is a viable energy source.
She also believes that her car will stop when she presses the brake pedal.
She's willing to bet her life on the latter belief,
and in fact she does so multiple times daily during her commute to and from work.
But she's not willing to bet her life on the form of belief, right?
She's not willing to bet her life on nuclear fusion being a viable energy source.
This difference in the decisions she's willing to make
seems like it should be traceable to a difference between her doxastic attitudes
toward the proposition that fusion is viable, on the one hand,
and the proposition that pressing her brake pedal will stop her car on the other.
Yet if we don't countenance credences, we'll be unable to make out any difference
between my friend's doxastic attitudes toward those propositions, right?
Once we note that my friend believes both propositions, that's all we'd be able to say.
Unless, of course, we avail ourselves of credences, right?
If we don't avail ourselves of credences, then all we can say is that, yeah,
she believes one, she believes the other,
but we can't explain her differences in risk-involving decisions.
In one case, she bets her life on the proposition, and in another case, she doesn't.
Now suppose my physicist friend reads about some new research into nuclear energy.
The research reveals new difficulties with designing more efficient nuclear power plants,
which will make fusion power more challenging going forward.
After learning of this research, she still believes fusion is a viable energy source.
She has mountains of evidence, we can suppose, for thinking that that is the case.
And this recent research is only a small piece of evidence being added to that monstrous pile of evidence.
So she still believes fusion is a viable energy source.
Nevertheless, it seems that this new evidence should cause some change in her attitude toward the proposition that fusion is viable.
Yet, if we don't count in its credences, we'd lack the tools to ascribe any such change, right?
My friend believed the proposition beforehand, and she still believes it now.
She still has belief both before and after learning this evidence against her belief.
So if we only availed ourselves of belief, we'd be unable to explain the change in her attitude toward the proposition that fusion is viable.
And what that means is that we need beliefs and credences in order to describe her doxastic life and, more generally, agents' doxastic lives at large.
So we've been talking about credences and ascribing credences to agents, but of course, all of this is, at best, a kind of idealization.
In truth, there isn't usually a precise answer to the question of exactly how much I'm confident in the truth of P for every single proposition P.
There are plenty of propositions that I've never even thought of, and even among those that I have thought of, many of those are such that I've got a pretty fuzzy attitude towards them.
Nor is it very realistic to suppose that I can attach numbers to all the things that I care about.
Still, we can go along with the idealization in order to simplify our discussion going forward.
But credences, of course, aren't only a useful simplification, since for at least a good deal of propositions, credence ascriptions at least roughly capture our decreed levels of confidence toward those propositions.
And as we saw when covering some of the reasons to believe in credences in the first place, talk of credences seems both justified and quite important for capturing elements of our doxastic lives.
So yes, there's some idealization and simplification involved here, but there's also utility and approximate accuracy.
We are really picking out something in our doxastic lives.
What we're picking out is going to be a little bit more messy and a little bit more fuzzy than our very precise mathematical models, but they're still approximately accurately describing the phenomenon, at least in many cases.
Of course, credence ascriptions do have some disadvantages, for instance, numerical representations may provide more specific information than is actually present in the situation being represented.
Similarly, I might be more confident that the Democrats will lose the next election than I am that they will win without there being a fact of the matter about exactly how much more confident I am.
Representing my attitudes by assigning precise credence values here to the proposition that the Democrats will lose and the proposition that they'll win attributes to me a confidence gap of a particular size.
And that may be over attributing things to my doxastic life.
And also assigning numerical credences over a set of propositions creates a complete ranking among them, which makes it impossible to retain any incommensurabilities among the propositions involved.
Incommensurabilities basically means like there's no way to compare the relevant beliefs in terms of some metric.
And yet you might worry that confidence in commensurability, that is, for at least some propositions, your confidence in one may simply be not even commensurable with your confidence in another.
There may be no common metric by which you could compare them.
You might think that that's a common and rational feature in agents' doxastic lives.
So again, there's idealization, there's simplification involved here, but at least for many propositions, it's plausible that agents' doxastic attitudes can be usefully and approximately accurately represented as credences or numerical degrees of belief.
Now, credences are closely related to the notion of probability in Bayesian reasoning.
Given a proposition p, we can speak of the probability of p.
For instance, the probability that the next card from this pack will be an ace, the probability that this radium atom will decay before the year 2100, the probability that it will rain tomorrow, and so on.
We can write p of p, or pr of p, to represent the probability of p.
Sometimes you'll also see cr of p, and that's just representing an agent's credence in p, so keep these notational variants in mind.
But the major question of this section is, what is probability?
What are we even talking about when we're talking about the probability of one statement or the probability of h given e, or those sorts of things?
Well, usually, probability is introduced first through a distinction, and that distinction is between two broad kinds of probability.
Objective probability and subjective probability.
Objective probabilities are mind-independent features of reality.
There are features out there, as it were.
For instance, there might be an objective probability of 50% that an indeterministic atom will decay within the next hour.
This probability doesn't depend on us in any way.
For instance, it's not a measure of our credence or our degrees of belief.
Instead, it's a matter of some objective feature of the atom itself, perhaps some propensity or tenancy inherent to it,
or perhaps it's a statistical fact about the frequency of decays among atoms of its kind, or perhaps it's something else entirely.
What matters here is that this probability is objective or mind-independent.
It's a feature of the world out there.
It doesn't depend on our attitudes or subjective mental states, etc.
By contrast, subjective probabilities are, you guessed it, mind-dependent.
And one dominant way of understanding subjective probabilities simply identifies them with credences or degrees of belief.
So an agent's subjective probability in a proposition just is their credence in that proposition.
Some philosophers, though, think this distinction between objective and subjective probability confuses more than it illuminates.
Perhaps there are really a panoply of different notions of probability, some of which are entirely mind-independent,
some of which are entirely mind-dependent, and others of which have some aspects which are mind-independent,
and other aspects which are mind-dependent.
And speaking of a panoply of different notions of probability,
that brings us to different interpretations of probability, different interpretations of probability talk,
and different interpretations of probability itself, like what is a probability?
What is this feature that we're picking out?
Now, before I go through these and explain them,
note that some people argue that all probability claims are captured by just one of these understandings of probability,
or one of these interpretations of probability.
But I don't think that's plausible.
I think that in different contexts we use different notions of probability.
Sometimes we're talking about frequencies, sometimes we're talking about propensities or tendencies,
sometimes we're talking about what credences someone actually has,
sometimes we're talking about what credences are rational or what credences someone should have, and so on down the list.
I doubt that any of these is quote-unquote the notion of probability.
Instead, they're all different ways of using probability talk in different contexts.
As a result, I'll simply treat each of these interpretations of probability as a distinct way of understanding probability talk,
and which understanding is at play when certain parties are engaged to a debate or engaged to a discussion,
or when you're reading a paper or book or listening to a video,
which understanding is at play will largely depend on context.
But anyway, here are some of the major interpretations of probability on offer.
Oh, and I should note that I left off this list the so-called classical interpretation of probability
because it totally fell out of favor like a long while back,
but according to that view, probability is basically counting the number of favorable outcomes
out of the total number of possible outcomes.
So if you're asking about the probability of rolling six on a die to say that the probability is one-sixth,
is to say that there's one way of rolling a six out of the total number of possible rolls.
So it's one way out of six ways, so one-sixth.
So anyway, that's the quote-unquote classical interpretation of probability.
I left that one off the list.
Okay, anyway, we're moving on to the list now.
One understanding treats probabilities as objective tendencies or propensities for certain outcomes.
Propensities are, roughly, degrees of causal influence.
Different sets of causal factors have stronger or weaker tendencies to produce certain outcomes,
and probabilities measure the strength of those tendencies or propensities.
This is arguably the notion of probability at play in quantum indeterminism.
If that's so, well, then when we say that a radioactive atom has a 50% probability of decaying
in a given time period, we're attributing to the atom, perhaps together with its environmental conditions,
a certain quantitative propensity or tendency or inclination or disposition to decay.
These tendencies would still have existed even if agents with credences had never existed.
So this is an objective understanding of probability.
Another understanding treats probabilities as the frequencies of certain outcomes or events under such and such conditions,
where those frequencies can be understood as either actual frequencies or hypothetical frequencies.
For example, the frequency with which the outcomes would occur if those conditions were repeated many times,
perhaps even infinitely many times.
Again, this is quite clearly an objective understanding of probability.
There's a fact of the matter independently of us with what frequency a certain outcome or event would occur or obtain
under certain conditions if those conditions were repeated.
A further understanding treats probabilities as an agent's actual credences or degrees of belief.
On this understanding, probability is always relative to an agent.
You can only ever talk about an agent's probability for a given proposition,
and it's also talking about their actual credences.
So probabilities on this view are thoroughly mind-dependent,
and this is therefore a subjective understanding of probability.
In fact, as we've seen, many people simply identify actual credences with subjective probabilities.
Like, to say that someone has a credence of 0.5 in something is to say that their subjective probability is 0.5.
But anyway, what matters for present purposes is that this is one way of talking about probability.
This is one interpretation of probability.
A related understanding treats probability as rational credences or rational degrees of belief.
That is, the credence an agent should have in a proposition in light of their evidence,
or perhaps in light of a restricted set of facts in the case of unconditional probabilities.
Don't worry, I'll talk about unconditional probabilities later.
Now, there's a question.
Is this an objective or subjective understanding of probability?
Well, there are elements of both here, I think.
Once you fix the evidence you're taking as input, there's arguably a perfectly objective fact of the matter
about what the rationally appropriate credence in a proposition is in light of that body of evidence,
which bears on the truth of that proposition.
So there's an element of mind independence here.
But there's also a clear element of mind dependence here,
since that input body of evidence is always an agent's body of evidence, right?
And that's going to vary by agent, and it's going to depend on various facts about the agent.
So again, probabilities here are rational credences in light of evidence.
So you have a kind of output, which is the rational credence,
and then you have the input, which is the evidence,
and the probability is like the function which takes that input and spits out the output,
takes the input of the evidence, spits out the output of the rational credences.
So with that in mind, another element of subjectivity within this account
is that the output of the probability function,
that is the rational credence one should take in light of the input evidence,
is still a credence, right?
Albeit a credence that an agent should have.
Still, because it's a credence, there's a sense in which it's a property of agents.
And so this is another element of mind dependence in this account.
But anyway, that's another interpretation of probability.
Viewing probabilities as rational credences,
what credences agents should have in light of their evidence.
A still further understanding treats probabilities as mind independent
degreed relations of support between propositions.
And these degreed relations of support between propositions
in turn determine which degrees of belief are rational.
Importantly though, these mind independent degreed relations of support
between propositions are not themselves degrees of belief.
They determine which degrees of belief are rational,
they determine which credences are rational,
but they are not themselves credences.
This view is defended by Nevin Kleinman-Haga and others.
Also note that under this account, the unconditional probability of a proposition
is the degree to which the proposition is supported by a priori truths or tautologies.
And I'll define tautologies soon, so don't worry about that.
This view is easy to categorize.
It's a thoroughly objective understanding of probability.
And then a final interpretation of probability that we're going to talk about here
is one that's pretty closely related to the rational credence account,
but it treats probability as a measure of the degree of justification
that a proposition has in light of one's current evidence.
It differs from the rational credence one because it's not saying that
probabilities are rational degrees of belief,
but instead, it's saying that probabilities are degrees of justification,
and one might think that those can come apart in principle.
So a proposition has probability of one in this understanding of probability,
just in case we have conclusive justification
that is the strongest justification possible for believing it.
And that applies to at most very few propositions,
like perhaps the proposition that something exists or that two is less than three.
And a proposition has probability zero for us on this understanding of probability,
just in case we have conclusive justification for denying it.
And so the proposition that I do not exist has, for me, right now,
a probability of zero, as does the proposition that one equals four.
Generally speaking, on this account, the only propositions with probability zero
are going to be those that are contradictory or otherwise absurd.
Now, like the rational credence account, this account has both objective and subjective elements.
Probability, under this view, is in principle relative to an individual.
A proposition may have different probabilities for different individuals
if those individuals have different evidence.
So this is an element of mind dependence within this account.
But there's also an element of mind independence,
since it's an objective matter to what extent your evidence justifies a given proposition.
Now, note that there are other interpretations of probability besides these.
I just want to give you a flavor of some of the main ones on the table that philosophers discuss.
Now, you may have heard of epistemic probability.
This is a term that's used differently by different philosophers.
Sometimes it's used interchangeably with subjective probability, that is, credence.
But I would strongly caution against that usage.
Instead, I think epistemic probabilities are best understood minimally,
as the probabilities we reason about in epistemic contexts,
where an epistemic context is a context in which we reason about things like
how probable a scientific theory is,
or how strongly a theory, whether scientific or not, predicts some evidence,
or to what degree some evidence confirms a theory or hypothesis, etc.
It's then an open question which of the above understandings of probability,
if any, epistemic probability should be identified with.
It's also then an open question whether epistemic probability admits
of different precisifications in different contexts.
And that's a good result,
because there's disagreement among philosophers on that very question.
Now, to give you a glimpse into different usages or different understandings
of epistemic probability,
Tyler Hildebrand and Thomas Metcalf, in their news article on the Nomological
Argument for God's Existence,
they seem to use epistemic probability to refer to a probability pertaining
to your credences, and then they divide up epistemic probability
into subjective and objective epistemic probabilities.
So, subjective epistemic probabilities describe your personal credences
and the relevant propositions, that is, they describe what your credences are,
whereas objective epistemic probabilities describe how strongly you ought
to believe the propositions given your total evidence,
that is, what your credences ought to be.
And know that that's not a moral sense of ought,
like no one is saying here that you're a bad person,
you're an immoral person, etc.
This is an epistemic sense of ought.
It's basically what's rational or justified or something like that,
what's rationally appropriate.
And those of you who are paying attention will notice that this objective
epistemic probability, the way that Hildebrand and Meck have articulated,
is just the rational credence account that I talked about
under the interpretations of probability.
But other people view epistemic probability quite differently.
For instance, my humor understands epistemic probability as degrees of justification,
and Nevin Klimenhege argues that epistemic probability is best understood as degrees of support.
So, the degree of support view, Nevin Klimenhege argues that that's the best understanding
of epistemic probability, my humor argues that the degree of justification account is the best,
and still others go for different interpretations.
So, as you can see, there are a number of different accounts of probability on offer,
and there are also different ways of spelling out what epistemic probability is.
In light of that, I'd suggest that if you're going to use a term for a view,
if you're going to use the term epistemic probability,
or even probability for that matter,
I just recommend explicitly defining how you understand the view
as you introduce the term for it.
And that way we don't have to engage in linguistic squabbles.
So, for instance, you could just say,
well, okay, listen, by epistemic probability, I mean this.
Or you might say, by epistemic probability, we should mean this
because it's the best way of interpreting what we do in epistemic contexts, etc.
Anyway, you get my point.
So, all of this is meant to serve as tools for your conceptual toolkit.
Going forward, we'll mainly just talk in terms of probability,
since that's all we really need to talk about in order to understand the basics of Bayes' theorem.
Which precise notion of probability is at play will largely be determined by context or stipulation.
Just a note that going forward, I'm mainly going to be talking in terms of just probability.
It's important, though, to have these tools in your conceptual toolkit.
It helps you weed out confusion about probability.
It helps you identify distinct notions of probability
and potentially disambiguate them when they come up in your thinking and the thinking of others.
And ultimately, it's a path toward a greater understanding of reality
and a greater understanding of probability and Bayes' theorem itself.
We can turn to Bayesian epistemology.
So, Bayesian epistemologists study norms governing credences,
including how credences ought to change in response to a varying body of evidence.
And at least generally speaking,
Bayesian epistemology addresses epistemological problems
with the help of the mathematical theory of probability.
It turns out that the probability calculus,
that is the mathematical theory of probability,
is especially suited to represent credences or degrees of belief,
and to deal with questions of belief change,
confirmation, evidence, justification, coherence, and more besides.
Compared to the informal discussions in traditional epistemology,
Bayesian epistemology allows for a more precise and fine-grained analysis
of these epistemological problems and issues.
Bayesian epistemology therefore complements traditional epistemology.
It doesn't replace it or even aim at replacing it.
Now, although I've been talking here about Bayesian epistemology and Bayesian epistemologists,
unfortunately, there's really no single Bayesian epistemology.
Instead, there are many Bayesian epistemologies.
Nevertheless, any Bayesian epistemology characteristically endorses
the following two principles that you can see on screen here.
Firstly, agents have certain doxastic attitudes, namely credences,
that can be usefully represented by assigning real numbers to propositions.
Secondly, rational requirements on those credences can be represented by mathematical constraints
on the real number assignments given by the probability calculus.
Now, again, the probability calculus, that's just the mathematical theory of probability.
It includes the axioms of probability, the formal language within the theory,
what logically follows from those axioms, etc.
So these are two characteristic principles that Bayesian epistemologies endorse.
Firstly, agents have credences and we can represent those by assigning real numbers to propositions.
And secondly, there are certain rational requirements on those credences
and we can represent those rational requirements with the help of the mathematical constraints
provided to us by probability theory.
We've already touched on the first principle and so we can now consider the second principle.
On the second principle, credences are subject to various rational constraints or requirements.
And these constraints are given to us by various elements of probability theory.
These rational constraints or requirements are also known as normative rules
in that they're standards for how we should reason.
So we're not necessarily talking about how we in fact reason,
but instead how we should reason, how we ought to reason.
These are normative rules, normative standards that we should live up to.
Now, there are five core normative rules of Bayesian epistemology.
Kolmogorov's three probability axioms, the ratio formula, and the conditionalization principle.
Also sometimes called the principle of conditionalization.
This isn't to say that these are the only normative rules that Bayesians accept,
but you can't understand any additional rules without understanding these five first.
They also serve as the basis for understanding Bayes' theorem,
which of course is what we're building up to, right?
This is a video about Bayes' theorem, but we've got to build up to that first.
We need to have certain foundational knowledge so we can have a deeper understanding of Bayes' theorem
and its use and its significance and its applications.
So what we're going to do going forward is basically just going through these core normative rules
and explaining them.
Before doing that, though, we should at least get three important notes on the table.
First, it should be noted that these normative constraints are sometimes articulated
in terms of rational credence,
which in turn means that they'll be articulated using CRP, CR not Q, CRP or Q, etc.
Other times, however, they're articulated in terms of probability,
which means that they'll instead be articulated using P of P, P of not Q, etc.
Going forward, I'll articulate the normative rules in terms of probability.
I think this is more illuminating because it emphasizes that the rules are not merely descriptions of actual credences.
Our actual credences are often inconsistent.
They often flout or violate the normative rules of Bayesian epistemology.
And CRP, right? This little formalism here, CRP,
that can easily mislead you into thinking that we're talking about someone's actual credence in P
rather than what credence it's rational to have in P.
And in fact, in some contexts, CRP is used to refer to someone's actual credence.
So expressing the normative rules in terms of probabilities
allows us to more clearly see that these are rational constraints on agents' credences.
They are not mere descriptions of actual credences, but instead constrain what credences are rational.
They set bounds around what credences one rationally ought to have.
If you're in the rationality business, in other words, you need to respect these rules.
So going forward here, when I speak of probabilities in articulating these five-core normative rules,
you can, hopefully, think of them as rational credences.
Not actual credences, but rational credences.
But I mean, again, for clarity, I'm going to be using probability talk.
I'm just going to be talking about the probability of P, probability of not Q, and so on.
All right, so that's the first note, the second note.
The second note is that we need some very basic background in propositional logic
before introducing the normative rules.
In propositional logic, we have a propositional language.
And this propositional language contains atomic propositions
that are usually represented using the letters P, Q, R, and so on.
Atomic propositions, by the way, are just propositions that don't embed any larger constructions within them.
They're basically like simple.
They're the building blocks of larger constructions.
So an atomic proposition might be grass is green,
and a non-atomic proposition might be the conjunction of grass is green and the sky is blue.
It's a construction out of the atomic propositions, grass is green, and the sky is blue.
Atomic propositions are the atoms from which those constructions are built up.
Okay, so our propositional language includes atomic propositions, and it also includes various connectives.
These are called connectives because very often they connect different atomic propositions together to form larger constructions.
So once we have our atomic propositions, the rest of the propositions in our language are constructed
from the atomic propositions using these five propositional connectives.
This little sign for negation, this ampersand for conjunction,
or for disjunction, a single arrow for material conditional,
and a double arrow for material biconditional.
Don't worry, this background and propositional logic is not going to get too convoluted, too complicated.
Again, I'm going to try to make this as accessible as I can.
But anyway, here's how these connectives work.
A negation, not P, so that's how we're going to read that,
a negation not P is true just in case P is false.
So if not P is true, then P is false, and moreover if P is false, then not P is true.
Not P, in other words, is just equivalent to the falsity of P.
A conjunction P and Q is true just in case both of its conjuncts, namely P and Q, are true.
So if the conjunction is true, then each of P and Q is true,
and moreover if each of P and Q is true, then the conjunction P and Q is true.
It's false otherwise.
So if at least one of these is false, then the whole conjunction is false.
A disjunction P or Q is true just in case either P is true or Q is true, or both P and Q are true.
In other words, the disjunction is true just in case at least one of its disjuncts is true,
and the disjuncts, of course, are just the individual atomic propositions within the disjunction.
And so the disjunction is therefore false exactly when both of its disjuncts are false.
So in order for P or Q to be false, both P and Q need to be false.
Otherwise, this disjunction as a whole is true.
A material conditional, also called material implication,
and usually we're going to read this as if P then Q or P implies Q.
A material conditional like this is false just in case P is true while Q is false,
and the conditional is true otherwise.
So a material conditional as a whole is going to be false exactly when its antecedent,
the if part, the part before the arrow, exactly when its antecedent is true,
and yet its consequent is false.
Its consequent is the then part.
It's the atomic proposition after the arrow.
But the material conditional is true in every other case.
So if at least one of these conditions aren't met, if either P is false or Q is true,
well then the material conditional as a whole is going to be true.
The reason I put this in terms of falsity is because I think that's the easiest way to get your mind around it.
Like imagine that I say if you get a 90 or above on this test, you get an A.
Well like under what condition is what I just said false?
Well, it's exactly under the condition in which you get something above a 90% on the test,
and yet you don't get an A, right?
So it's the condition where the antecedent is true, you did get above a 90% on the test,
and yet the consequent is false, where I don't give you an A, right?
That's exactly when what I said is false.
And think of the material conditional as true in all other cases.
Okay, so that's the material conditional.
And then finally a material biconditional, P if and only if Q,
or P just in case Q, or P exactly when Q.
That material biconditional is going to be true just in case P and Q have the same truth value.
That is just in case either both are true or both are false.
So if these two differ in their truth value, if one's true while the other is false,
well then the material biconditional as a whole is going to be false.
But the material biconditional is going to be true so long as these two agree in their truth value,
as long as they're both true or they're both false.
Okay, so that's some background on atomic propositions and the various truth functional connectives.
So we also need to know going forward what a tautology is and what a contradiction is.
A tautology, T, is a construction that's guaranteed to be true by its logical form.
Any way of assigning truth values to T's propositional symbols results in T itself being true.
So whenever you have a construction and you look at its propositional symbols,
this one only has one propositional symbol P, of course it recurs,
but it's still the same propositional symbol, it's P rather than Q or R or S.
Any way that you assign truth values to P, so whether you assign true to P or whether you assign false to P,
the construction as a whole is guaranteed to be true. That's what a tautology is.
And so T is in that sense logically necessary, right?
It's necessary that T is true because of its logical structure.
And examples here of tautologies are going to be things like it's not the case that both P and not P.
What do you mean think about that? It's not the case that P is true and P is false.
Yeah, I mean that's guaranteed to be true by its logical form.
Like whether P is true or false, whether, for instance, I exist or not,
it's still going to be true that it's not the case that I both exist and don't exist.
So whatever way you assign truth values to the propositional symbols here,
the construction as a whole is going to be true.
It is logically necessary. There's no way for it to be false.
It just logically cannot be false.
You could also think of it as saying denying a tautology is going to be a contradiction,
but I haven't even defined contradiction yet.
So another example of a tautology is P or not P.
That's guaranteed to be true by its logical form.
It's logically necessary that either I exist or I don't exist.
P implies P is also a tautology.
If I exist, then I exist.
Or if I don't exist, then I don't exist.
Both of those constructions are true.
Any construction like that is guaranteed to be true by its logical form.
A contradiction by contrast is a construction that's guaranteed to be false by its logical form.
In other words, any way of assigning truth values to its propositional symbols results in it being false.
And so a contradiction is in that sense logically impossible.
Its truth is impossible because of its logical structure.
Examples of contradictions include P and not P.
So it's just a contradiction to say I exist and I don't exist.
Another example of a contradiction is P if and only if not P.
Because remember, given how the bi-conditional is defined,
the bi-conditional as a whole is only ever going to be true if these two share their truth value.
And yet P and not P can never share the same truth value because then P would be both true and false.
So yeah, a contradiction is a construction that's guaranteed to be false by its logical form.
Alright, we just have two more small pieces of background and propositional logic
before we get onto the five core normative rules of Bayesian epistemology.
The first is about mutual exclusivity.
So the propositions in a set are mutually exclusive just in case the truth of any one member of that set
logically entails the falsity of all the other members of the set.
Thus no logically possible world makes more than one of the propositions in the set true.
To put it differently, each proposition in the set is incompatible with all the rest.
So for example, the set containing P and not P, like that set is mutually exclusive
because the truth of any one member of the set logically entails the falsity of the rest of the members of the set.
The truth of any one of the members of the set precludes the truth of all the others.
If one of them is true, the rest of them can't be true.
And so they mutually exclude one another from truth, as it were.
And then the second piece of conceptual machinery that we need to get on the plate is logical entailment.
So a proposition P logically entails a proposition Q just in case it's logically impossible for P to be true while Q is false.
So the conjunction P and not Q is a contradiction.
It is literally contradictory to say that P is true while Q is false.
And so P logically entails Q.
The point of this is that in any logically possible world in which P is true, Q is also true.
P, just in virtue of its logical form, implies Q, or P implies Q is a tautology.
All of these are basically the same way of conveying logical entailment.
Alright, so that's all the background that we need in propositional logic.
I hope you stuck through it.
If so, congratulations.
I hope we didn't lose any arms or legs or souls.
So the third and final note going forward is really just anticipating a question you might have.
In particular, you might be wondering like, Joe, why haven't we gotten to Bayes theorem yet?
Give me the Bayes!
Now listen, I understand your eagerness for Bayes theorem truly.
I understand it.
And I applaud you for your eagerness to cover the almighty equation.
But I have an answer for you, okay?
The reason why we haven't gotten to Bayes theorem yet is that to truly understand Bayes theorem,
to have a deep grasp of what it is, what it's saying, and why it's true,
you need to know a sizable chunk of background information.
Bayes theorem follows from the axioms of probability theory.
That's how we know that it's true.
And these axioms are the ones that we're about to cover.
To understand Bayes theorem then, it's important to understand those axioms.
And to understand those axioms, you need to know some important facts about propositional logic
and about probability and about how these various axioms are not about actual credences,
but instead represent rational constraints on credences, etc.
That is why we have taken the facially circuitous route that we have.
It is only facially circuitous.
It's not in fact circuitous.
I think this is a really good way to teach Bayes theorem.
But okay, having covered all that, we can now proceed to the five core normative rules of Bayesian epistemology.
Beginning with the first three, and these three are of course Kolmogorov's axioms.
The first such axiom is called non-negativity.
It says that for any proposition P, the probability of P is greater than or equal to zero.
And you can see why it's called non-negativity.
It's saying that the probability of any proposition is not negative.
The second Kolmogorov axiom is called normality.
It says that for any tautology T, the probability of T is one.
And that makes sense.
A tautology is guaranteed to be true by its logical structure.
And so there's a 100% probability that it's true.
The probability that it's true is one.
There's literally no logically possible way for a tautology to be false.
The third axiom is finite additivity.
It says that for any mutually exclusive propositions P and Q,
the probability of P or Q is equal to the probability of P plus the probability of Q.
And remember, P and Q are mutually exclusive just in case they're incompatible with one another.
The truth of one logically entails the falsity of the other.
Now Kolmogorov's axioms are often referred to as the probability axioms.
The great 20th century Russian mathematician Andrey Kolmogorov was the first to articulate these axioms
as the foundation of mathematical probability theory, or the probability calculus.
But yeah, that's it for the first three normative rules of Bayesian epistemology.
Pretty simple.
Now actually things get a little more complex since these axioms have lots of very important consequences.
One immediate consequence of them is that the probability of not P is equal to one minus the probability of P.
So for instance, if the probability that it will rain is 0.3,
well then the probability that it won't rain is 0.7, right?
It's just one minus 0.3, which is 0.7.
I mean that makes intuitive sense, but to see why this follows from the axioms,
note that P and not P are incompatible and so they're mutually exclusive.
And so by finite additivity, right, which says that for any mutually exclusive propositions P and Q,
the probability of P or Q is just equal to the sum of the probabilities of P and Q,
we can get that the probability of P or not Q is equal to the probability of P plus the probability of not Q.
Right, but P or not Q, that's just a tautology, right?
That is guaranteed to be true by its logical form.
That is logically necessarily true.
Any way of assigning truth values to its propositional symbols results in the construction as a whole being true.
And so since that's a tautology, we can conclude by normality that its probability is 1, right?
So the probability of P or not P is 1.
But now look what we have, right?
We have the probability of P or not P. We got the probability of P or not P here.
So because this is equal to this and because these two are equal with one another,
we can say that 1 is equal to this, right?
So we get 1 is equal to the probability of P plus the probability of not P.
And by doing some handy-dandy rearranging, we can conclude that the probability of not P is equal to 1 minus the probability of P.
Here's another useful consequence of Kolmogorov's axioms.
In general, whether or not P and Q are incompatible or mutually exclusive, the following holds.
The probability of P or Q is equal to the probability of P plus the probability of Q minus the probability of P and Q.
The proof of this from the axioms is slightly involved, so we're not going to cover it here.
It's actually much easier to see why it must be true by inspecting a Venn diagram.
So in a Venn diagram like this, we take the points in a circumscribed plane to represent all logically possible worlds.
And then we use sets of points to represent sets of possible worlds.
And in particular, to represent all those possible worlds where some proposition P is true, or where some proposition Q is true.
The areas of these spaces can then be used to represent the probabilities of the relevant propositions.
In this diagram here, the proposition P or Q corresponds to the points which are either in the area labeled P or in the area labeled Q or in both.
And the proposition P and Q corresponds to the points which are in both the area labeled P and the area labeled Q.
So that's going to be this cross-hatched area here.
It's pretty easy to see that if we tried to work out the area corresponding to P or Q by simply adding the area for P to the area for Q,
well then we would count the cross-hatched area twice, right?
If we add this whole circle to this whole circle, we will have counted this cross-hatched area twice.
We'll have double counted it.
So to get the right answer for the probability of P or Q, we need to correct that double counting by subtracting the cross-hatched area.
And of course, that cross-hatched area is just the probability of P and Q.
And so we get that the probability of P or Q is equal to the probability of P plus the probability of Q minus the probability of P and Q.
Because again, when we look at the diagram, we see that simply adding the probability of P to the probability of Q would count the probability of P and Q twice.
And so to get the probability of P or Q, we need to correct that by subtracting out the probability of P and Q.
In some cases, the probability of P and Q will be zero, namely when P and Q are incompatible, and so then there won't be a cross-hatched area.
Their Venn diagrams won't overlap at all.
And in that case, the probability of P or Q will be the simple sum of the probability of P and the probability of Q, as in Kolmogorov's third axiom.
And general additivity applies in that case, right?
It's just that the probability of P and Q in that case is zero, because if P and Q are incompatible, then obviously it cannot be the case that P and Q.
That's a contradiction.
And so it has a probability of zero.
And so when P and Q are mutually exclusive, we get that the probability of P or Q is equal to the probability of P plus the probability of Q.
And if you remember, that's just finite additivity.
So here are some other pretty important consequences of Kolmogorov's axioms.
We won't derive them here, but they're important to note nonetheless, and I just want to say something to their intuitiveness.
So one is maximality, which says that for any proposition P, the probability of P is less than or equal to one.
Together, non-negativity and maximality establish the bounds of our probability scale, right?
Remember, non-negativity just says that probabilities are non-negative.
They're going to be greater than or equal to zero.
Maximality, by contrast, says that probabilities are less than or equal to one.
So now we have bounds for our probability scale.
Probabilities, which again, you can helpfully think of them for present purposes as rational credences.
Probabilities always fall between zero and one, inclusive.
Now, this upper bound is kind of arbitrary, of course, but using zero and one lines up nicely with everyday talk of being 0% confident or 100% confident in particular propositions.
And also with various considerations of frequency and chance.
So it's useful.
Another consequence of Kolmogorov's axioms is contradiction.
For any contradiction F, the probability of F is zero.
One way to see that just intuitively is that a contradiction is just the negation of a tautology.
And a tautology has a probability of one.
And we already saw that the probability of not P equals one minus the probability of P.
Which means that the probability of a contradiction is going to be one minus the probability of a tautology, which is just one minus one.
Which is just zero, goose egg, nada, zilch.
But anyway, it also makes sense.
A contradiction is logically guaranteed to be false.
There's literally no way it could be true.
So its probability is zero.
Another consequence is entailment.
This says that for any propositions P and Q, if P logically entails Q, then the probability of P is less than or equal to the probability of Q.
So think about this one.
If P logically entails Q, well then any world in which P is true is also a world in which Q is true.
So Q occupies at least as much of the possibility space as P.
And Q must therefore be at least as probable as P.
Q would be more probable, of course, if there are worlds in which Q is true while P is false.
For then, the worlds in which P is true will be a proper subset of the worlds in which Q is true.
So anyway, I hope this is kind of intuitive on its face.
But you can also think about it in Venn diagram terms.
So you can imagine little boxes around these that represent the total space of possible worlds, but I was kind of lazy, I didn't want to put those in.
You can just imagine the boxes there.
And these are three separate Venn diagrams.
And these Venn diagrams are representing the relationship between P and Q.
In this first diagram, the set of logically possible worlds in which Q is true is a subset of the set of possible worlds in which P is true.
Editing Joe here now.
More precisely, the set of possible worlds in which Q is true is a proper subset of the set of possible worlds in which P is true.
I think I also go on to say that P in this third Venn diagram is a subset of the set of possible worlds in which Q is true.
But I should have clarified that more precisely, it's a proper subset.
Alright, back to the video.
In this case, P and Q are logically equivalent.
They're true in exactly the same worlds.
In this case, the worlds in which P is true are a subset of the worlds in which Q is true.
If P logically entails Q, well then this one cannot be the case.
This cannot be a representation of the situation that we have, right?
Because remember, if P logically entails Q, then any world in which P is true is also a world in which Q is true.
But that's not the case on this diagram, right?
Like, pick a world that's like right here, where my cursor is.
Well, that's a world in which P is true, but in which Q isn't true, because this is outside the circle for Q.
And so this represents a case where P does not logically entail Q.
By contrast, in both of these cases, any world in which P is true is indeed also a world in which Q is true.
And so these are the only two candidate representations of the case that we have when P logically entails Q.
But notice that in both of these cases, the probability of P is less than or equal to the probability of Q.
In this case, the probability of P is equal to the probability of Q.
They take up the exact same amount of the possibility space.
And in this case, the probability of P is less than the probability of Q.
And so look, we've just shown it visually that if P logically entails Q, then the probability of P must be either less than or equal to Q.
Finally, think about this one in everyday terms, right?
Compare the proposition that John has a dog with the proposition that John has a female dog with white fur for brown spots,
a curly tail, floppy ears, and brown eyes.
Which of these is more probable?
Clearly, the first is more probable, right?
The reason is that the second entails the first, but the first doesn't entail the second.
The first is, in that sense, more modest.
It could be true even while the second is false.
It doesn't demand as much of reality.
So the first is going to be more probable than the second.
Alternatively, you can think of this in terms of ways of being false.
Any way for this first claim to be false is also a way for this second claim to be false.
But there are indeed ways for the second to be false while the first is true.
So for instance, John could have a dog, even though his dog is male, or even though it has brown fur,
even though it has five brown spots, even though it doesn't have a curly tail, etc.
So there are more ways, in principle, for the second claim here to be false than for the first to be false.
And so the second is going to be less probable than the first.
There are more ways, in principle, for the second to be false.
And so it's going to be less probable.
Now again, in this example, the first claim entails the second while the second claim does not entail the first.
If you have mutual entailment between the propositions,
well then we get the next consequence of Kolmogorov's axioms, which is equivalence.
This says that for any propositions, p and q, if p logically entails q and q logically entails p,
well then the probability of p equals the probability of q.
And this makes perfect sense, right?
Any way for one of the propositions to be false is also a way for the other to be false,
and moreover, any way for that one to be false is also a way for the first one to be false.
You can't find any world in which one is true while the other is false.
So they're true in exactly the same worlds.
They're equi-probable.
It's literally impossible for there to be a case in which one of them is true while the other is false.
And if you think about this in Venn diagram terms,
their Venn diagrams are going to perfectly overlap one another.
And also just notice that equivalence is a consequence of entailment.
If p logically entails q, then the probability of p is less than or equal to the probability of q.
And by the same token, if q logically entails p, then the probability of q is less than or equal to p.
And when you have a is less than or equal to b, but also b is less than or equal to a,
it just follows from that, that a and b are equal to each other, right?
Just think about that.
A further important consequence of Kolmogorov's axioms is finite additivity extended
to any finite set of mutually exclusive propositions.
So this says that for any finite set of mutually exclusive propositions, p1, p2, through pn,
the probability of the disjunction of those propositions is just equal to the sum of the probabilities of those individual disjuncts.
And this is basically just general additivity extended to cover any finite set of mutually exclusive propositions,
rather than just a two-membered set.
Now, you might be wondering, Joe, okay, you know, these axioms are cool and all,
but like, what's so irrational about flouting them?
Like, who cares if I flout these normative constraints on credences?
Like, so what if my credence in p is 0.6 while my credence in not p is also 0.6?
What are you gonna do about it?
Well, I mean, of course there's nothing in psychology that rules out the possibility that an agent at a time
might attach a credence of 0.6 to, say, the proposition that it'll rain tomorrow
and simultaneously credence of 0.6 to the proposition that it won't rain,
thus violating the immediate implication of the probability axioms that the probability of p equals 1 minus the probability of not p.
So yeah, there's nothing in psychology that rules that out.
But in addition to its just being seemingly self-evident that that's irrational, set that self-evidence aside,
in addition to that seeming self-evidence, there's an argument that any rational credences must conform to the axioms of probability,
even if someone's actual credences don't always do so.
And the argument is that anybody whose credences violate the axioms of probability can have a Dutch book made against them.
A Dutch book is a set of bets which guarantee that you'll lose money no matter what outcome occurs.
So by way of illustration, consider the person whose credence that it will rain is 0.6
and whose credence that it won't rain is also 0.6.
Well, this person will happily pay 60 cents to win $1 on its raining
and also will happily pay 60 cents to win $1 on its not raining.
They'd be willing to make those bets, those bets are perfectly fair by their lights.
But anybody who makes this pair of bets will be guaranteed to lose money no matter what happens, whether it rains or not.
Because they'll have paid out $1.20 in total and will only win $1 whether or not it rains, right?
It's either going to rain or it's not, if it rains we're only going to be making $1
and if it doesn't rain they're going to be making $1.
And yet they'd be willing to pay 60 cents on both of these bets
and so they'd be willing to pay out $1.20 even though they can only win $1 in this situation.
And so they're guaranteed to lose money here.
And in general, it's not that hard to prove that a Dutch book can be made against you
just in case your credences fail to satisfy the axiom of probability.
And since it seems clearly irrational to adopt attitudes that make it certain that you will incur a loss
it follows that any rational agent will have degrees of belief that do conform to the probability calculus.
And that also includes Bayes theorem by the way because that's a consequence of the axioms of probability theory.
There are other reasons for thinking that these axioms normatively constrain credences
so there are for instance accuracy based approaches to arguing for these rational constraints
that basically has to do with how well you're doing getting at the truth
and we basically assess your credences in terms of how close they are to the truth.
It turns out that by your own lights you maximize your chances of getting to truth if you obey the probability axioms.
So anyway, there are accuracy based approaches to arguing for these rational constraints
and there are lots of other approaches besides but we're not going to get into that here.
That's a huge literature unto itself.
We've covered the Kolmogorovs axioms enough
and now we're going to be turning to the fourth rational constraint on credences
or the fourth core normative rule of Bayesian epistemology.
And that is the ratio formula. Yes indeed.
So again, the ratio formula is the fourth core normative rule of Bayesian epistemology.
Before getting into it though, we need to get clear on conditional probability.
A conditional probability is the probability of one proposition being true
on the assumption of another proposition being true.
It's a probability that one proposition is true given that another proposition is true.
And we represent the probability of P conditional on Q as the probability of P given Q.
It's this little thing on screen here.
This is the probability of P given Q.
Okay, so again, this is the probability of P given that Q is true
or on the supposition that Q is true or assuming that Q is true
or on the condition that Q is true.
Okay, you get it.
Oh, and by the way, an unconditional probability is just a probability that isn't conditional.
So yeah, that is an unconditional probability there.
It's not conditional on anything, but alas, we introduced the conditional back
because that's how the slide was originally.
All right, with that out of the way, here is the ratio formula.
The ratio formula says that for any propositions P and Q,
the probability of P given Q equals the probability of P and Q
divided by the probability of Q,
where of course the probability of Q is greater than zero
because you can't divide by zero.
That is undefined.
So that's a bit abstract and it might be hard to see on its face why it would be true.
And that's where handy-dandy Venn diagrams come in.
So consider the following Venn diagram.
Again, the rectangle itself is going to be representing all logically possible worlds.
The probability of P is the fraction of that rectangle taken up by the P circle.
And by the way, the area of the rectangle is stipulated to be one.
And so the fraction of the rectangle taken up by the P circle
is just the area of the P circle divided by one,
which of course is just the area of the P circle.
So yeah, the probability of P is the fraction of the rectangle taken up by the P circle.
And the same thing goes for the probability of Q.
When we ask about the probability of a proposition conditional on the assumption that Q,
we temporarily narrow our focus to just those possibilities that make Q true.
In other words, we exclude from our attention the worlds shaded in within this diagram.
And we then consider only what's in the Q circle.
The probability of P given Q is then the fraction of the Q circle
occupied by the P worlds.
And so it's the area of the P Q overlap
divided by the area of the entire Q circle.
And notice that that's just the probability of P and Q
divided by the probability of Q.
And that of course gives us the ratio formula, right?
The probability of P given Q is equal to the probability of P and Q
divided by the probability of Q.
I just want you to think about this intuitively, right?
The probability of P given Q, the probability of P on the assumption that Q is true.
Well, what do we do there? What do we do to assess that?
Well, we narrow our focus to the worlds in which Q is true.
And then we ask, among the worlds in which Q is true,
what's the proportion of those in which P is also true?
And notice that that just is the probability of P and Q
out of the probability of Q, right?
So it's the probability of P and Q divided by the total probability of Q.
That is the probability of P given that Q is true.
I hope that that's intuitive.
To give a concrete example, in the scenario in which I roll a fair die,
the initial range of possibilities includes all six outcomes of the die roll.
I then ask for the probability that the die comes up six,
conditional on its coming up even.
That is the probability of six, given that it's even.
To assign this value, we exclude from consideration all the odd outcomes.
Note that we haven't actually learned that the die outcome is even.
We've simply been asked to suppose that it comes up even
and assign a probability to other propositions in light of that supposition.
We then distribute the probability equally over the outcomes
that remain under consideration, which is two, four, and six.
And so the probability of six, conditional on even, is one out of three, one-third.
And of course, we get the same result from the ratio formula.
The probability of six given E is equal to the probability of six and E
divided by the probability of E.
The probability of getting a six and even just is the probability of getting a six, right?
So that's one-sixth.
And then you divide that by the probability of even, which is, of course, one-half.
And one-sixth divided by one-half is one-third.
So importantly, the ratio formula allows us to calculate conditional probabilities
from unconditional probabilities, that is, probabilities relative to no suppositions
beyond perhaps our background information.
Again, keep in mind that the ratio formula is a rational constraint
on how an agent's conditional credences should relate to their unconditional credences.
And as a normative constraint, rather than a definition,
and as a normative constraint, it can be violated by assigning a conditional credence
that doesn't equal the specified ratio.
And if you violate it, you'll be susceptible to a Dutch book.
Anyway, onwards we march to the fifth and final core normative constraint on rational credence,
and that is the conditionalization principle.
So according to Bayesians, when you acquire new evidence,
you should update your beliefs that the evidence bears on by conditionalization.
So yeah, we have some hypothesis H that you have a certain credence in,
and we also have information E or evidence E that you learn.
According to the conditionalization principle,
you should update your credence in H by conditionalizing on the new information E.
And what that means is that when you learn new information E,
your new probability of H, after learning E,
should equal your old probability of H given E, before you learned E.
So that's what the new and the old represent.
Your new probability of H is new insofar as it's after you learn E,
and your old probability of H given E is old insofar as it's before you learn E.
Also, we don't necessarily need to cast this in terms of learning,
we could just cast it in terms of considering.
Maybe you knew E all along, but like, never stop to consider
how E bears on some of your beliefs or some of your credences.
So we could say when you newly consider information E within an epistemic context,
your new probability of H should equal your old probability of H given E.
So yeah, that's it.
That's the simplest articulation I can muster of the conditionalization principle,
or again, sometimes known as the principle of conditionalization.
And by the way, obeying the conditionalization principle
is often known as updating by conditionalization.
And hopefully you find this pretty intuitive.
Suppose you don't know whether it's raining or not right now.
I ask you, hey, if it's raining right now,
how likely do you think it is that it will still be raining in two hours?
Suppose you answer about 40%,
and then you look outside and see that it actually is in fact raining,
and that's all you learn.
I then ask you, well, now how likely do you think it is
that it will be raining in two hours?
Well, if you're rational, you're gonna answer 40%, right?
If that isn't intuitively obvious, it's also possible to construct a Dutch book
for anyone who fails to update by conditionalization,
but we won't get into that here.
So again, the conditionalization principle is a rational constraint on credences,
and in particular, it's a rational constraint on credences across time.
The Kolmogorov axioms and the ratio formula relate rational credences at a single time.
They're basically rational demands on consistent and coherent credences at any given time.
By contrast, conditionalization relates rational credences at different times.
It requires rational credences to line up in a particular way across time,
and the central idea is that rational agents should conditionalize whenever they gain new information.
So anyway, that's basically the least formal way that I could have stated the principle,
is that when you gain new information E,
your new probability of H should equal your old probability of H given E,
but I do want to give two slightly more formalized versions of the principle
while still retaining accessibility.
And to do that, I'll be continuing with concrete, tangible examples
to make things as comprehensible as possible.
So suppose that you have the following probability assignments,
and again, remember that we can helpfully treat probabilities as rational credences,
at least for the purposes of this section.
So the probability that Johnny goes to the party is one-half,
and the probability that Johnny goes to the party conditional on Jane's going to the party is two-thirds.
So yeah, suppose that those are your probability assignments.
Well, now you learn that Jane is going to the party.
The question then is, what should your new probability in Johnny's going to the party be?
Well, the answer is obvious enough, it's two-thirds, right?
If it was right to think beforehand that the conditional probability of Johnny going
on the assumption that Jane goes is two-thirds,
and if now it turns out that Jane really is going,
well then it must be right to think that the new unconditional probability that Johnny goes
has increased to two-thirds.
And again, think of it in Venn diagram terms.
You now know you are inside the area of the Venn diagram for Jane's going,
and you have already decided that the proportion of this area that covers Johnny's going as well is two-thirds.
And so it must now be true that the probability of Johnny going is two-thirds
after you learn that Jane is in fact going.
You basically narrow your focus to the space occupied by Jane going,
and your new probability that Johnny goes to the party is just equal to your old probability that Johnny goes
given that Jane goes, which is just the proportion of this whole circle
taken up by the crosshatched area.
Again, updating your credences in this kind of way is called conditionalization,
and we can offer a slightly more formal version of the principle as follows.
If your old conditional probability of P given Q is equal to X,
and you come to know that Q is true,
well then the new probability that P is true is equal to X.
Notice here that Q needs to be understood as representing everything you come to know.
The principle doesn't work if Q is only part of your new knowledge.
To illustrate this, suppose that in the above example you learn not only that Jane is going to the party,
but also that she will be accompanied by Jill.
And suppose that you would always thought that there was almost no chance that Johnny would go
if both Jane and Jill did.
You had in other words a very low old conditional probability that Johnny goes
conditional on the assumption that both Jane and Jill go.
Even though your old probability that Johnny goes conditional on Jane going was two-thirds.
You can imagine that this little red circle represents the probability that Jill goes,
and as you can see, the amount of overlap between Jill's circle,
Jane's circle, and Johnny's circle is a super-duper tiny fraction of the entire Jill circle.
So here, the probability that Johnny goes,
given that both Jane and Jill go, is super-duper tiny.
And so while it is still true that you have learned that Jane will go,
it's no longer a good idea to attach a two-thirds probability to Johnny going,
simply on the grounds that your old probability that Johnny goes conditional on Jane going is two-thirds.
And the reason that's not a good idea is precisely because you have learned more than that Jane will go to the party.
You now know not just that you are inside Jane's Venn diagram,
you now know not just that you are inside of Jane's circle, so to speak,
but more specifically that you are inside of that bit of Jane's circle,
which is also inside of Jill's circle, which is that bit of the probability space where Jill also goes to the party.
And the proportion of that area where Johnny goes in addition to Jane and Jill going is very, very small indeed.
So this just illustrates that when you're updating by conditionalization,
you need to update on everything you come to know within the given time period.
Finally, here's a still more formal version of the conditionalization principle,
still though I think it's relatively understandable.
So the principle says that for any time t and later time t star,
if Proposition E represents everything an agent learns between t and t star,
and if the agent's probability of E at t is greater than zero,
then for any Proposition H, the agent's probability of H at t star equals the agent's probability of H given E at t,
where the p subscript t and p subscript t star are basically the agent's probability distributions at the two times.
A probability distribution is basically just a way of assigning probabilities to different propositions within our propositional language.
So yeah, these are basically just assignments of probabilities to different propositions indexed to a given time,
and of course, relative to a particular agent.
So we're talking about an individual agent's probabilities in certain propositions indexed to particular times,
and how we represent that indexing is by subscripting the time, t star or t.
So as you can see from this principle, conditionalization captures the idea that an agent's probability,
or rational credence of H at t star after learning E, equals their earlier probability at t, right?
Equals their earlier probability in H had the agent merely supposed that E is true.
So their later probability in H after learning E is equal to their earlier probability of H given E,
and when an agent updates their credence distribution by applying this principle of conditionalization to some learned Proposition E
and some hypothesis H, we say again that the agent conditionalizes on E.
That's what Bayesians are saying when they talk about conditionalizing on some evidence.
Keep in mind, again, and I emphasize this because it is very often overlooked, this is a rational constraint.
An agent's actual credences may not obey the conditionalization principle,
like an agent's actual credences might not satisfy this.
The point is that if the agent is being rational, then their credences obey this.
Now, the conditionalization principle is especially relevant to Bayes' theorem,
since Bayes' theorem offers us a way to calculate the probability of H given E,
and in particular to calculate an individual agent's probability of H given E at time t.
So the conditionalization principle, together with Bayes' theorem, provides for us the rational way to update our beliefs and credences
as well as to confirm or disconfirm our theories and hypotheses in light of new evidence, in light of new data.
This is the ultimate significance of Bayes' theorem and these various normative constraints on rational credence.
This is the rational way to update your beliefs, to update your credences, to confirm or disconfirm theories, hypotheses, etc.
And that is the broader significance of all this.
Now, that would be a perfect bridge into Bayes' theorem itself,
but there's one final distinction within Bayes' epistemology that you should be aware of before we get into the almighty theorem,
and that's the distinction between subjective and objective Bayesianism.
So there are different ways to articulate the distinction between subjective and objective Bayesianism,
but a central way to delineate them, and this is sometimes called the normative distinction between subjective and objective Bayesianism,
a central way to delineate them is in what rational constraints they put on credence.
The subjective Bayesian basically stops at the five core normative rules that we've just been over,
together, of course, with whatever is entailed by them, such as Bayes' theorem itself.
Objective Bayesians, by contrast, put further rational constraints on credence.
As a result, for the subjective Bayesians, there is no one uniquely rational response to a set of evidence.
It all depends on your initial credences in the propositions involved, or your prior probabilities in the various hypotheses.
Sometimes called your priors.
As long as your priors satisfy the axioms of probability, as long as they meet those relatively minimal coherence conditions
and, like, being within zero and one and, you know, those sorts of things,
and, of course, as long as you update by conditionalization and obey the ratio formula,
you're going to count as perfectly rational for the subjective Bayesian.
The main objection to subjective Bayesianism is that, of course, it's too subjective.
For instance, you might think that, in light of all the evidence we currently have,
it's simply not rational to think that the Earth rests on the back of a giant pink turtle.
That just isn't a possible justified response to our current evidence, you might think.
But on the subjective Bayesian view, it is a possible justified response to our evidence, right?
All you have to do is assign a super-duper high prior probability to that sort of turtle cosmology,
high enough that it will sufficiently outweigh even the mountains of evidence that we have against this turtle cosmology.
In essence, then, subjective Bayseans think that a rational belief is just a belief
that doesn't violate any of the five core normative constraints on rational credence.
And it's not that difficult to assign a super-duper high prior probability
in what otherwise seems to other people to be a very implausible theory,
while nevertheless respecting all of those core normative rules.
Other philosophers, however, think that rationality is more demanding,
and these people fall into the objective Bayesian camp.
Objective Bayseans think there are substantial additional constraints on rational credences
beyond the comagore-vaxims the ratio formula and the conditionalization principle.
One of the main motivations for this, again, is to avoid excessive subjectivity
and to put stricter constraints especially on one's prior probabilities in various hypotheses.
One of those additional constraints might be the principle of indifference, for instance,
which basically says that if there are no reasons favoring A over B or B over A,
then the probability of A is the same as the probability of B.
There are other candidates for additional constraints on rational credence,
and there's a lively debate about the principle of indifference.
But you get the point. Objective Bayseans add further rational constraints.
I just wanted to give you a sense of this distinction between subjective and objective Bayseans
before moving on, since it's important for understanding the broader framework of Bayesian epistemology.
Alright, we're finally onto the theorem itself! Woohoo!
Yes, the most famous consequence of the ratio formula and comagore-vaxims is Bayes' theorem.
For any propositions H and E, the probability of H given E is equal to the probability of E given H
times the probability of H divided by the probability of E.
We can label H and E as A and B or P and Q or whatever we want.
For present purposes, I did it in terms of H and E,
because Bayes' theorem is typically used to talk about the confirmation or disconfirmation
of various hypotheses in light of evidence or data.
So sometimes you'll also see D used, but E is more often used.
Also, part of Bayes' theorem is the stipulation that the probability of E is greater than zero,
because, of course, you can't divide by zero.
So the first thing to say about Bayes' theorem is just a reminder that it is indeed a theorem.
It can be proven straightforwardly from the comagore-vaxims together with the ratio formula.
We'll actually go through a relatively simple proof of Bayes' theorem in a second.
But before that, I want to briefly talk through the terms used in this formula.
So again, strictly speaking, H and E are just going to be propositions.
But when we're talking about Bayesian epistemology,
we're really concerned with hypotheses and evidence.
So going forward, we're going to be treating E as some piece of data or evidence.
We're going to be treating H as some hypothesis.
The probability of H given E is the posterior probability of H.
It's the probability of H posterior to considering E.
The probability of E given H is what's often called the likelihood,
or sometimes the likelihood of H.
I know that's unfortunately named because likelihood sounds like probability,
so it sounds like you're talking about the probability of H, but no.
But no, when we're talking about likelihoods,
we're talking about the probability of pieces of evidence given hypotheses.
So you can think of it as the probability or the likelihood
that E would be true conditional upon the truth of H.
The probability of H is the prior probability of H.
That is the probability we assign to H prior to considering the piece of evidence E.
And then the probability of E is just, of course, the probability of the evidence itself.
It's a probability that the evidence itself would be true at all.
It's a probability that the evidence itself would be true or would obtain at all.
Now, regarding the prior probability of H, don't confuse prior probabilities with intrinsic probabilities.
The prior probability of H means the probability that H is true
before considering some specific item of evidence,
but usually after encountering other items of evidence.
By contrast, the intrinsic probability of H is the probability that H is true
before considering any evidence whatsoever.
It's determined solely by the content of H itself,
without taking into account any evidence for or against H.
Paul Draper offers a plausible theory of intrinsic probabilities,
according to which they depend on the modesty and coherence of a hypothesis.
A hypothesis is modest to the extent that it does not commit us to saying very much about the world.
Modesty is thus a measure of how much the hypothesis asserts.
The more a hypothesis claims, the more ways there are in principle for it to be false,
and, as a result, the less likely it is to be true, prior to looking at any evidence, that is.
A hypothesis is coherent to the extent that its conjuncts or elements cohere well with each other.
They fit together well, or they raise, or at least don't lower each other's probabilities.
So think of coherence as a measure of how well the various parts of a hypothesis fit together.
If the different parts count against each other, if conditional on one of the parts being true,
the other parts are unexpected, or unlikely, well then the hypothesis is less coherent,
and thus less likely to be true.
So yeah, just keep these notions distinct in your mind.
Intrinsic probability refers to the probability that hypothesis,
independent of any and all evidence we have for or against it.
Whereas prior probability may very well take into account various pieces of evidence
that you've conditioned on in the past.
Prior probability, again, is simply the probability of the hypothesis
prior to looking at some particular item of evidence under consideration.
So we've got the theorem here, but we can ask how do we know that Bayes' theorem is true?
And the answer is because we can prove it.
In fact, we can prove it in several ways,
and I'm just going to give what I think is the most intuitive and the easiest and the simplest way to see why it's true.
So recall the ratio formula.
For any propositions p and q, the probability of p given q
equals the probability of p and q divided by the probability of q.
Notice that this holds for any p and q,
and so we can get one instance of the ratio formula by first plugging in h for p and e for q.
And then we can get a second instance of the ratio formula by plugging in e for p and then h for q.
So when we plug in h for p and e for q, we get one here.
The probability of h given e is equal to the probability of h and e divided by the probability of e.
So look, we're plugging in h for p and we're plugging in e for q.
And as I just said, we can also plug in e for p and h for q.
So that's just what 2 is. This follows from the ratio formula.
It's the same thing as the ratio formula, but you're just plugging in e for p and then h for q.
So we've got the first two steps of our proof.
Let's then do some rearranging.
The probability of h given e times the probability of e equals the probability of h and e.
This follows from 1.
Look, all you have to do to get from 1 to 3 is just multiply each side by the probability of e.
When you multiply this side by the probability of e, the probability of e's cancel out
and then you're just left with the probability of h given e times the probability of e equals the probability of h and e.
You're moving the probability of e from the denominator over here to the left side of the equation.
And we can also do some more rearranging.
We're doing basically the exact same thing to premise 2, right?
We're multiplying both sides of the equation in premise 2 by the probability of h.
And when you do that, you get the probability of e given h times the probability of h
and so we get our next two steps of the proof.
But notice that the probability of h and e is equal to the probability of e and h.
Now, I hope that this step is just intuitively obvious on its face, right?
I mean, obviously, like the probability that it rains and it's Tuesday is just the same as the probability that it's Tuesday and it rains.
Like, this step should be obvious.
But also note that this step follows from the fact that h and e is logically equivalent to e and h.
Together with the consequence of Komagorov's axiom known as equivalents, which we went over earlier in the video.
For any propositions p and q, if p logically entails q and q logically entails p, then the probability of p equals the probability of q.
So given that h and e is logically equivalent to e and h, and what that means is that they each logically entail the other
and that just follows from the truth conditions for this logical connective for conjunction.
And so given that they're logically equivalent and so each logically entails the other,
it follows from equivalents that their probabilities have to be the same as well.
And then from 3 through 5, it follows that the probability of h given e times the probability of e equals the probability of e given h times the probability of h.
Right? Notice what 5 is saying. It's saying that the probability of h and e, which shows up here, is equal to the probability of e and h, which shows up here.
Right? So given that these two are identical with each other and that the top one here is identical to this and the bottom one here is identical to this,
well then these two have to be identical with one another as well.
Right? We know that these two are identical from 5 and so it follows that these two are identical.
That is what 6 is saying. It's saying that the probability of h given e times the probability of e equals the probability of e given h times the probability of h.
That's just what 6 says. And finally, it's then just a straightforward step from there to get to Bayes' theorem.
Look, all you have to do is divide both sides by the probability of e.
When you divide this side by the probability of e, the probability of e's cancel out in the numerator and the denominator,
and then on the right-hand side you're left with the probability of e in the denominator.
So we get Bayes' theorem. The probability of h given e equals the probability of e given h times the probability of h divided by the probability of e.
Now I know it sounds kind of complicated going through that here, but if you think about it, it's actually quite simple, right?
We just have two instances of the ratio formula and we have an instance of equivalence.
The rest just logically follows and the rearrangements aren't all that complex.
Usually we're just dividing both sides by term or multiplying both sides by term.
So anyway, I hope that this proof was illuminating for you and helps you see why Bayes' theorem follows from Kolmogorov's axioms in the ratio formula.
Now I do want to show a 3 blue 1 brown video to illustrate the simplicity of this kind of proof.
So let's get on to that next.
If your goal is simply to understand why it's true from a mathematical standpoint,
there's actually a very quick way to see it based on breaking down how the word AND works in probability.
Let's say there are two events, a and b. What's the probability that both of them happen?
On the one hand, you could start by thinking of the probability of a, the proportion of all possibilities where a is true,
then multiply it by the proportion of those events where b is also true,
which is known as the probability of b given a.
But it's strange for the formula to look asymmetric in a and b.
Presumably we should also be able to think of it as the proportion of cases where b is true, among all possibilities,
times the proportion of those where a is also true, the probability of a given b.
These are both the same, and the fact that they're both the same
gives us a way to express p of a given b in terms of p of b given a, or the other way around.
And also I just want to pause at this point and say that this right here is basically just the ratio formula.
Remember the ratio formula, it said that the probability of a given b is equal to the probability of a and b
divided by the probability of b.
And that's just a rearrangement of this equation here.
All you have to do is divide both sides by the probability of b.
So if you take this probability b and put it over here in the denominator, then you get the ratio formula.
So this is just an alternative way of expressing the ratio formula.
That's what I'm trying to emphasize here.
So when one of these conditions is easier to put numbers to than the other,
say when it's easier to think about the probability of seeing some evidence given a hypothesis rather than the other way around,
this simple identity becomes a useful tool.
And you know, while we're here, it's worth highlighting a common misconception that the probability of a and b
is p of a times p of b.
For example, if you hear that one in four people die of heart disease,
it's really tempting to think that that means the probability that both you and your brother die of heart disease
is one in four times one in four, or one in sixteen.
After all, the probability of two successive coin flips yielding tails is one half times one half,
and the probability of rolling two ones on a pair of dice is one sixth times one sixth, right?
The issue is correlation.
If your brother dies of heart disease and considering certain genetic and lifestyle links that are at play here,
your chances of dying from a similar condition are higher.
A formula like this, as tempting and clean as it looks, is just flat out wrong.
What's going on with cases like flipping coins or rolling two dice is that each event is independent of the last.
So the probability of b given a is the same as the probability of b.
What happens to a does not affect b. This is the definition of independence.
Keep in mind, many introductory probability examples are given in very gamified contexts,
things with dice and coins, where genuine independence holds.
But all those examples can skew your intuitions.
The irony is that some of the most interesting applications of probability, presumably the whole motivation for the kind of courses using these gamified examples,
are only substantive when events aren't independent.
Bayes' theorem, which measures exactly how much one variable depends on another, is a perfect example of this.
Now, you won't always see Bayes' theorem like this.
Sometimes you'll see it instead as this.
For any h and e, the probability of h given e is equal to the probability of e given h times the probability of h,
divided by the probability of e given h times the probability of h,
plus the probability of e given not h times the probability of not h.
That's a mouthful.
Now, these two are actually equivalent because those denominators are equivalent.
Notice that the numerators are exactly the same and these denominators are actually equivalent.
A more complicated denominator just offers a fleshed out way to calculate the probability of e.
So here is why these two are identical.
And this is probably going to be the most technical part of the video.
I'm going to try my hardest to make this as accessible as possible,
but I want to give you guys a really deep understanding of Bayes' theorem.
And so I want you guys to see why these two are equivalent.
I want you to have a deep, unifying, explanatory understanding of Bayes' theorem.
And because of that, I want to show you that these two are identical.
I don't want you to just take my word for it.
So we want to show that the probability of e equals the probability of e given h times the probability of h,
plus the probability of e given not h times the probability of not h.
For starters, I hope this is perhaps somewhat intuitive on its face.
Like, the probability that I go on a run is equal to the probability that I go on a run,
assuming that it's raining, times the probability that this assumption is correct,
plus the probability that I go on a run, assuming it's not raining,
times the probability that that assumption is correct.
Right?
Like, it's either raining or it's not raining.
And conditional on its raining, there's a certain probability that I go running.
And conditional on its not raining, there's a certain probability that I go running.
And so you can go down each of those horns of the dilemma,
and then find the probability that I run given that horn of the dilemma,
times of course, the probability that that horn would obtain it all,
and then just add that to the probability that I'd run on the other horn of the dilemma.
Times, of course, the probability that that horn is itself true.
So I'm hoping that that verbal walkthrough can at least make this intuitively plausible to you,
but there is a more formal way to see why they're the same. And if you're like me,
you're not going to settle just for that kind of reasonably intuitive understanding.
You want to go deeper. So here's a more formal way to show that these two are the same.
Bam! Okay, so I'm going to talk through this, but this only has six steps,
and the first step is quite convoluted. But what I'm going to do is I'm going to
give you a bird's eye overview of the proof here. And so I'm just going to go through steps one
through six, and then I'll explain why we know that step one is true. So step one says that
e is logically equivalent to the following disjunction. This is a disjunction of two disjuncts.
The first disjunct is e and h, and the second disjunct is e and not h. So what this is saying
is that e is logically equivalent to e and h or e and not h. And so once you just spot me that,
and I'm going to prove that in a second, but once you spot me that, it follows by equivalence
that the probability of e equals the probability of e and h or e and not h. All right, remember,
by equivalence, if p is logically equivalent to q, that is, if p logically entails q and q
logically entails p, well then the probability of p equals the probability of q, right? And since
e is logically equivalent to this, it follows that the probability of e is equal to the probability
of this. So two follows from one together with equivalence. But notice that e and h, on the one
hand, and e and not h on the other are incompatible. These can't both be true, right? It can't both be
true that e and h as well as e and not h, because then both h and not h would be true, and that's
contradictory. So these two are incompatible with one another. And remember by finite additivity,
whenever you have a disjunction of mutually incompatible disjuncts, it follows that the
probability of that disjunction as a whole is just equal to the sum of the individual probabilities
of the individual disjuncts. So three follows from finite additivity. But notice what we have in
two and three. Here we have probability of this disjunction. And here we have the probability
of that very same disjunction. And we have equal signs. So we have the probability of e being equal
to the probability of this disjunction. And the probability of this disjunction being equal
to the probability of e and h plus the probability of e and not h. And from those, it just follows
that the probability of E is identical to the probability of E and H plus the
probability of E and NOT H. We have here the structure A is equal to B and a B is
equal to C, and from that it follows that A is equal to C, right? So, from 2 and 3,
we get that the probability of E equals the probability of E and H plus the
probability of E and NOT H. But remember, by the ratio formula, the
probability of E and H is equal to the probability of E given H times the
probability of H. This is, again, that same rearrangement of the ratio formula.
As I originally articulated it, the probability of h was over here in the denominator, but it's the same thing, right?
We just multiply both sides by the probability of h. And remember, you can apply the ratio formula to any propositions p and q.
So we have it both being the case that the probability of e and h equals the probability of e given h times the probability of h,
and the probability of e and not h is equal to the probability of e given not h times the probability of not h.
And notice again that here, this term here is equal to this term here,
and so we can just plug this into this above equation here. And notice that this term here is equal to this term here.
And so we can plug in this for this term here. And so then from 4 and 5, it follows that the probability of e
equals the probability of e given h times the probability of h plus the probability of e given not h times the probability of not h.
And that's precisely what we wanted to prove. Okay, I understand that's a bit technical, but that's the bird's eye view.
And so we can prove that these two are equivalent,
given that you spot me this first thing, that e is logically equivalent to e and h or e and not h.
And so my task now is to prove that this is in fact logically equivalent to this. And to do that,
what I'm going to do is we're going to suppose that e is true and then show that this follows,
and then we're going to suppose that this is true and then show that e follows.
And so that'll show that kind of bi-directional entailment.
It'll show that e logically entails this, and it'll also show that this logically entails e, and hence that they're logically equivalent.
So let's go from the left to right direction, the logical entailment there.
We're going to suppose that e is true, and then we're going to conclude from that supposition
that this is true. And when you suppose something to be the case, and you prove another thing to be the case,
well, then your license to say that, well, then if the first is the case, then the second is the case.
So suppose that e is true, and remember we're going to try to derive this disjunction. So suppose e is true.
Well, either h is true or not h is true. Let's go under each horn of that dilemma. Suppose that h is true.
Well, then obviously both e and h are true, right? We suppose that e is true, and now we suppose that h is true, and so e and h follows.
But whenever you've proved something, you can add whatever you want in there to a disjunction, right? If you prove that snow is white,
well, then it follows from that that either snow is white or
Cinderella exists, or either snow is white, or 1 plus 1 equals 7. That disjunction as a whole is true, because at least one of its
disjuncts is true, namely the first one. So under this horn of the dilemma, we were able to prove this disjunction.
What about the other horn of the dilemma? Well, when we suppose that not h,
we can conclude that e and not h, right? Because we're still under the assumption that e is true, and we just suppose that not h is true,
and so it follows that e and not h is true.
But then again, we're able to introduce another disjunct into that, because we've just proved this disjunct,
so any disjunction with this as a disjunct is going to be true as a whole, and so this disjunction as a whole follows.
So whichever horn you pick, the disjunction follows, and whenever it's the case that either a or b, and whichever way you go,
c is the case, well, then you can just conclude that c is the case, right?
So since under each horn of the dilemma, you get this disjunction, it follows that we could just conclude that disjunction.
Well, like, think about this more generally. If it's either the case that I go to the park or I don't go to the park, and
either way I'm gonna be happy, well, then it just follows I'm gonna be happy.
If I go to the park, I'm gonna be happy. If I don't go to the park, I'm gonna be happy.
Well, since either I'm going to the park or I'm not going to the park, and since each of those entails me being happy,
we can conclude that I'm gonna be happy. And so we conclude that the disjunction itself is true. And notice what we've just done, right?
We suppose that e is true, and we derive the disjunction itself.
And so we can say that if e is true, then the disjunction follows. If I suppose that John is a bachelor, and then I derive from that
supposition that John is male, well, then I can just conclude that if John is a bachelor, then John is male.
So now we've just proved the left to right direction. Next up, we want to prove the right to left direction.
So we want to show how this disjunction
logically entails e.
And again, to do that, we're just gonna suppose that the disjunction as a whole is true, and then we're gonna
ultimately derive e. And so then we're gonna conclude that if the disjunction is true, well, then e follows.
Oh, and by the way, there should be one more parentheses here and here. Okay,
I just did some makeshift editing to put little tiny parentheses there. Okay, so yeah, we're doing the right to left direction now.
We're gonna show how this disjunction entails e.
So we're gonna suppose that the disjunction is true, and then we're gonna show how e follows from the disjunction.
So suppose that the disjunction is true.
Well, what we're gonna do to show how e follows from that is we're basically just gonna show that e follows from each of the
disjuncts, right? Because again, whenever you have a disjunction and you can show that each of the disjuncts entails
something, well, then you can just conclude to that something, right?
So let's suppose that this disjunct is true e and h. Well, obviously if e and h is true, well, then e is true.
So we just showed that e follows from this disjunct. Well, does e follow from this disjunct? Well, of course it does, right?
Suppose that e and not h is true, well, then obviously e follows, right?
So under either horn of this dilemma, e follows.
And so since the disjunction as a whole is true, and since either horn of the dilemma entails e, we can conclude that e.
But look what we just did, right?
We suppose that the disjunction as a whole is true, and now we just derived from that that e is true.
And so we can say that if the disjunction is true,
then e follows. So we say if the disjunction is true, then e follows.
And notice now that we've shown the right to left side of the logical equivalence. We've shown that
this disjunction logically entails e. So we can put the little bi-conditional arrow here,
and we've shown precisely what we wanted to show, that e is logically equivalent to the disjunction of e and h, or e and not h.
Okay, all right, take a deep breath. I can promise you that was the most
technical part of this video. Apologies if I lost you. I tried explaining this as best as I could,
but yeah, that is probably the most technical part of this video.
But again, the goal of this video, the goal of majesty of reason, is not to back down from those challenges.
It's to rise up to the challenges. It's to exhibit that kind of intellectual perseverance, the intellectual curiosity, the intellectual responsibility
to find out these things for yourself, to see why the probability of e is equal to the probability of e given h times the probability of h,
plus the probability of e given not h times the probability of not h.
Okay, so anyway, think of this as an exercise in cultivating the intellectual virtues.
That's what you're getting with the majesty of reason experience.
You're not only getting philosophy, but I'm trying to build up your intellectual virtues along the way.
Still, you might be thinking like, okay, Joe, I get it. That's a lot of math and logic, but like, what's the point?
To what end? Well, what's the point of anything, honestly? Uh, no, okay. We're not getting into that topic here.
The point, the significance, is one I've already mentioned. Bayes' theorem gives us a way to calculate the posterior probability of a hypothesis conditional on evidence.
Together with the conditionalization principle, this gives us a recipe for rationally updating our credences and for confirming or disconfirming hypotheses,
that is, increasing or decreasing their probability as a result of data.
Here are some other important points pertaining to Bayes' theorem.
So we want to know how likely our hypothesis is in light of evidence e.
That's given by the probability of h given e. So if we learn that e is true, we should revise our confidence in h by changing it from
the probability of h to the probability of h given e, per the conditionalization principle that we went over.
Looking at the right-hand side of Bayes' theorem, we can see that three things determine the probability of h given e.
Under this form of Bayes' theorem, what we could call the standard form. First, the probability of h given e is proportional to the prior probability of h itself.
In other words, how likely h was to be true before we learned e, or how likely we would say h was if we didn't know e.
The more probable the hypothesis starts out, the more probable it ends up.
This also supports the slogan, extraordinary claims require extraordinary evidence.
In the sense that if we have a hypothesis that is super implausible to begin with,
then we're going to need significant evidence before the hypothesis becomes believable.
Whereas if you started out with a pretty plausible hypothesis, not as much or not as strong evidence is needed for believability.
Second, it's generally quite good if the likelihood of h, the probability of e given h,
is high. That's generally quite good if that's high. Intuitively, if you want to support a theory,
you want the theory to strongly predict some possible evidence, where the evidence then occurs.
To strongly predict something is to imply that that thing is quite likely.
At the very least, you want your hypothesis to predict the data better than the alternative hypotheses.
But we'll talk about that more when we get to the odds form of Bayes' theorem and when we get to evidence.
Right now, we're just speaking in general terms. Third, in general,
it's pretty good if the probability of e is low.
Right, you can see from the theorem that as the probability of e goes down, the probability of h given e goes up.
Right, whenever you divide by something, the smaller the denominator, the larger the resulting quantity as a whole is.
So yeah, it's generally quite good if the probability of e is low.
Because as the probability of e goes down, the probability of h given e goes up. Combining this with the previous point,
we can say that ideally and generally, you want a theory that strongly predicts evidence that is otherwise highly unlikely.
You want e to be much more likely if h is true than otherwise.
That's a straightforward path to a very strong confirmation of the hypothesis in question.
But alas, I'm getting ahead of myself because I've got a whole section of this video dedicated to confirmation and evidence.
And I'll speak in much more precise terms there rather than the general terms I've been doing here.
So anyway, that's the standard form of Bayes' theorem or the standard forms, again, two different ways that you can articulate it.
But there are other forms of Bayes' theorem.
The odds form is one of the most useful and it's definitely my favorite.
It's basically like my baby. I feed it. I nourish it. I pad it.
It spits on me. It calls me daddy. Uh, I need to stop.
It's pretty useful because when we're doing the calculations,
we don't need to worry about the probability of the evidence or data, right?
Notice that p of e doesn't show up anywhere in here.
Instead, we simply worry about the prior probabilities of h and not h, as well as how well each of h and not h predict the data.
That is, how likely they each render e.
So yeah, here is the odds form of Bayes' theorem. It's on screen here.
The probability of h given e divided by the probability of not h given e
is equal to the probability of e given h divided by the probability of e given not h
times the probability of h divided by the probability of not h.
This is called the odds form because it's expressed as odds.
Like, odds are always ratios, like two to one, and that's basically like the division that's going on here, right?
This is a ratio of one quantity to another.
But anyway, here's an explanation of odds from three blue, one brown.
If you've ever heard someone talk about the chances of an event being one to one or two to one, things like that, you already know about odds.
With probability, we're taking the ratio of the number of positive cases out of all possible cases, right?
Things like one in five or one in ten.
With odds, what you do is take the ratio of all positive cases to all negative cases.
You commonly see odds written with a colon to emphasize the distinction, but it's still just a fraction, just a number.
So an event with a 50% probability would be described as having one to one odds.
A 10% probability is the same as one to nine odds.
An 80% probability is the same as four to one odds. You get the point.
It's the same information. It still describes the chances of a random event, but it's presented a little differently, like a different unit system.
Probabilities are constrained between zero and one, with even chances sitting at 0.5.
But odds range from zero up to infinity, with even chances sitting at the number one.
So we can actually put the odds form of Bayes theorem in slogan form.
The slogan is that the posterior odds equals the likelihood ratio times the prior odds.
So on the left side here, this left term is called the ratio of the posteriors, or the posterior ratio, or the posterior odds.
The middle term is called the likelihood ratio, or the Bayes factor, or the relative likelihoods.
And finally, the right term here is called the ratio of the priors, or the prior ratio, or the prior odds.
Any of these terms are adequate.
Now you might be wondering, how does the odds form of Bayes theorem relate to the standard form of Bayes theorem?
Well, the odds form is actually a straightforward consequence of the standard form of Bayes theorem.
So we have the odds form up here, and look, we have the standard form of Bayes theorem right here.
The probability of h given e is equal to the probability v given h times the probability of h divided by the probability of e.
We also have another instance of the standard form of Bayes theorem here.
But instead of plugging in h, we've just plugged in not h as our hypothesis.
But these are both just instances of the standard form of Bayes theorem.
All you have to do is just divide the probability of h given e by the probability of not h given e.
And that'll give you the left hand side of this equation up here, the odds form.
And when you do that, since this is identical to this, and this is identical to this,
when you divide this by this, it just equals this divided by this.
And of course, when you divide these two right hand sides, the p of e in the denominator of both crosses out.
That's how division works with fractions. And when you cross off p of e, you get the odds form of Bayes theorem.
So the odds form is a straightforward consequence of the standard form of Bayes theorem.
Here then is a handy dandy example of the odds form of Bayes theorem in action.
I don't know why it's telling me that I spelled diseaseitis wrong. That's totally a legitimate disease. I got it last week.
It means inflammation of the disease. It's real. Fox News told me so.
So suppose you are a nurse screening patients for diseaseitis.
The screening test involves a tongue depressor that usually turns black for patients who have the sickness.
Suppose you know the following.
20% of the patients in the screening population actually have diseaseitis.
Among the patients with diseaseitis, 90% of them turn the tongue depressor black. Those are the true positives.
However, 15% of the patients without diseaseitis will also turn the tongue depressor black. Those are the false positives.
The question now is, what is the probability that a patient with a blackened tongue depressor actually has diseaseitis?
So to answer this, we can use the odds form of Bayes theorem.
Just let H be the hypothesis that the patient has diseaseitis.
And so then not H or the negation of H is going to be that the patient does not have diseaseitis.
And then we're going to let E or our data or our evidence be the evidence of a blackened tongue depressor. With this in hand, let's reason this out.
So because 20% of the population has diseaseitis, while 80% of the population doesn't, the ratio of the priors,
that is the ratio of the probability of H to the probability of not H,
is 1 to 4. Right? Because the probability of H is going to be 20%. The probability of not H is 80%.
And the ratio of 20 to 80 is just a 1 to 4 ratio. Another way to think about that is that we're starting off with four times fewer people with
diseaseitis as people without it. So we've got the ratio of the priors. Then each person with the disease
is 90% likely to make the depressor black. And each person without the disease
is only 15% likely to make the depressor black. So we're saying that the probability that someone makes the depressor black
given that they have the disease, so given H is 90%. While the probability that they make the depressor black, given that they don't have the disease
is only 15%. So these are our likelihoods, right? This is the likelihood of H and this is the likelihood of not H.
Again, keep in mind from earlier on in the video that that's unfortunately named because likelihood sounds like probability.
Again, think of the likelihood of H as just the probability of the evidence given that H is true.
And the likelihood of not H is the probability that the evidence would be true given the truth of not H.
So what then is the likelihood ratio? Well, all you got to do is 90% to 15%, right? The likelihood ratio is 90% up here to 15% down here.
90 to 15. That is 6 to 1, right? 90 divided by 15 is 6. So the likelihood ratio is 6 to 1.
Again, another way to think about this is that
each person with diseaseitis is 6 times as likely as a person without diseaseitis to make the tongue depressor black.
So then how do we determine the posterior odds? Well, all you got to do, right?
Remember our slogan, the posterior odds equals the likelihood ratio times the prior odds.
So all you got to do is the likelihood ratio 6 over 1 times the prior odds 1 over 4
gives us the posterior odds 6 over 4. And of course, you can simplify that to give us 3 to 2.
So what that tells us is that the ratio of the posteriors is 3 to 2. The ratio between the probability of H given E
and the probability of not H given E is 3 to 2.
To convert this relative proportion into an absolute probability that a random person who blackened the tongue depressor really does have diseaseitis,
we do 3 divided by 3 plus 2 to see that 3 fifths or 60% of those who blacken the tongue depressor
really do have diseaseitis. In other words, given that a random person did in fact
blacken the tongue depressor, the probability that they have diseaseitis is 60%.
So that gives us the probability of H given E.
And that's what we wanted to determine. Now, you might not immediately see why in order to convert this to an absolute probability
we do 3 divided by 3 plus 2. Think about it like this. When you have
mutually incompatible but exhaustive
hypotheses and the ratio of one to another in the probability space is like x to y,
well then to determine the absolute probability of the former just going to do x divided by x plus y.
So let's let's take an example. I made that a little bit too abstract.
Suppose we have a kindergarten class and in the kindergarten class, of course, there are males and females. Ignore intersex conditions.
Okay, I know some of you are like, well actually, so within this class, we've just got males or females.
No one is both a male or a female and no one is neither a male nor female.
Now, suppose that I told you that the ratio of males to females in this class is 3 to 2.
So for every three males, there are two females in this class. Now, suppose I ask you like,
what's the absolute probability that a random child chosen from this class is going to be a male?
Well, I mean, just just think about that intuitively. If the ratio of males to females is 3 to 2,
well, then what that means is that
3 out of every 5 students is going to be a male, right? Because it's three males,
two females, so 3 out of 3 plus 2, right? It's the males out of the males plus the females.
So it's 3 out of 3 plus 2 or 3 fifths or 60%.
So I hope that gives you a kind of intuitive sense of why we do this calculation here.
And notice that we can now see why the answer to the original problem posed at the beginning of my video is D.
Remember, only 1% of women her age have cancer and the test was 90% accurate.
So when we let cancer be H, no cancer be not H, and E be the evidence of a positive test result,
if we want to determine the probability that she has cancer,
given that she has a positive test result, we can use the odds form of Bayes theorem.
In particular, we want to find the posterior ratio by multiplying the likelihood ratio by the prior ratio.
So what's the ratio of the priors? Well, the ratio of the priors is 1 to 99, right?
Because 1% of women her age have cancer, so the probability that she has it before looking at this
evidence is 1%. And of course, what that means is that there's a 99% chance that she doesn't have it.
And so the prior ratio is 1% to 99% or 1 to 99. And then we can ask what is the likelihood ratio?
Well, in order to determine that, we need to know the numerator here and the denominator here.
Numerator here is the probability that she has a positive test result given that she has cancer.
Well, remember, the test is 90% accurate. So given that she really does have cancer,
there's a 90% chance that the test will indicate that she has cancer,
and that is give a positive test result. So the probability of e given h is 90%.
By contrast, the probability of e given not h is 10%, right? Because conditional on her not
having cancer, there's still a 10% chance that the test would go wrong. That is,
that the test would give a positive test result positive for cancer. So the probability of e
given not h is 10%. And what that means is that the likelihood ratio is 9 to 1, right? Because
the numerator here is 90%. The denominator here is 10%. And 90% to 10%, right? That's a ratio of
9 to 1. So then we multiply the likelihood ratio by the ratio of the priors in order to get 9 to 99
or simplifying 1 to 11. Again, keep in mind that this is the ratio of the posterior is
1 to 11 or 1 divided by 11. That is not the probability that she has cancer given that she
tested positive. This is the ratio between the probability that she has cancer given that she
tested positive on the one hand. And on the other hand, the probability that she doesn't have cancer
given that she tested positive. Okay, so this is just a ratio of these two posterior probabilities.
Again, if you want to convert that to an absolute probability, the absolute probability that she
has cancer given that she tested positive, we do 1 divided by 1 plus 11, right? That was the 3 divided
by 3 plus 2 in the last example. And in this case, we just do 1 divided by 1 plus 11, which is 112,
or in other words, 8%. So 8% is the probability that she has cancer given that she tested positive.
So she doesn't have too much to worry about. It's literally 92% probable that she doesn't have cancer
in light of this evidence. And of course, given that it's 8%, we know that D is the correct answer.
And then here's a final note about the odds form of Bayes theorem. Importantly, it isn't
absolutely necessary to use H and not H as our hypotheses, right? H and then the strict
logical negation of H. We can instead use competing hypotheses that are not strict
negations of one another. For example, we could use like theism and naturalism as our hypotheses,
even though naturalism isn't the strict negation of theism and theism isn't the
strict negation of naturalism. Of course, they can't both be true, right? So they're incompatible,
but they could in principle both be false. So they don't exhaust the space of possibilities,
and they also don't exhaust the probability space. Just note, and this is a very important note,
when you use the odds form of Bayes theorem, and when you don't have a hypothesis and the strict
negation of that hypothesis, and so when you don't have mutually exhaustive alternatives,
this last step will not go through, okay? Do not use this last step when that condition is met,
right? All you're going to be able to conclude when you use, for instance, theism and naturalism
is the ratio of the posteriors, that is the ratio between, say, the probability of theism
given some evidence and the probability of naturalism given some evidence. You won't then be able
to convert that to an absolute probability in the way described on the screen here. And again,
the reason is because like naturalism and theism don't exhaust the possibility or the probability
space. And so, for instance, like a one-to-one posterior ratio between them might, in principle,
correspond to, for example, theism and naturalism each having only an absolute probability of,
let's say, 25% given the evidence in question, right? So, both of them might be 25%, and the ratio
of 25% to 25% is one-to-one, right? And yet, if you use this method down here, you would get one
divided by one plus one, one divided by two, which is 50%, right? So, just be careful when
you're converting to absolute probabilities. You're going to want to make sure that you have
mutually exclusive and exhaustive hypotheses up here. All right, now we're on to evidence.
Show me the evidence, show me the evidence, show me the evidence, show me the evidence.
Okay, Bayesians generally conceive of evidential support as probability raising. In other words,
some piece of evidence or data E evidentially supports or confirms a hypothesis H, just in case
E raises the probability of H. More precisely, E is evidence for H, or E would be evidence for H,
just in case the probability of H given E is greater than the probability of H. You can see
here that E is raising or boosting or increasing the probability of H. And this, by the way,
is often known as the positive relevance account of evidence. And it's plausible too, right? Think
about this bi-conditional. This is saying E is evident for H if and only if this condition is
met. So, let's do the left or right hand side of the bi-conditional. If E is evidence for H,
well then E is going to raise the probability of H. Yeah, that's plausible, right? If something is
evidence for a hypothesis, surely it's then going to raise the probability of hypothesis. It's going
to make the hypothesis more likely than it would be otherwise. Evidence counts in favor of a hypothesis.
It supports it. It has to raise its probability. And similarly, you might think that the right
to left direction is also plausible. If some piece of data really does raise the probability of
hypothesis, well then surely it's evidence for the hypothesis. It's counting in its favor. It's
supporting it. It's giving it a probabilistic boost. So, the bi-conditional you might think
is plausible. How, though, does this relate to Bayes' theorem, you ask? Well, it's pretty
straightforward to see that this condition holds, this condition here, this condition holds just
in case the likelihood ratio is greater than 1. That is, just in case the probability of E given H
is greater than the probability of E given not H. To put it differently, E is evidence for H.
Just in case E is more expected under H than it is under the negation of H.
So, when E is more expected on a hypothesis or less surprising on a hypothesis or more
probable on that hypothesis than the evidence is on the negation of that hypothesis,
then and only then does that piece of data count as evidence for the hypothesis. This is why you'll
often hear people saying, hey, this piece of data is more expected on theism than it is under
naturalism, and so it counts as evidence for theism. Or you'll hear people say that, hey,
this data is less surprising on naturalism, that is, it's more expected under naturalism,
it's more probable under naturalism than it is under theism, and hence it's evidence for naturalism
over and against theism. So, yeah, E is evidence for H just in case E raises the probability of H,
that is, the probability of H given E is greater than the probability of H, and that happens just
in case the likelihood ratio is greater than 1. More technically, it's when the likelihood ratio
is greater than 1 when you have H in the numerator here. If you switched up these two and you put
not H in the numerator, well then E would be evidence for H when this likelihood ratio is
less than 1, because then the probability of H given E is going to be in the denominator,
and so you're basically flipping all these, and so then if E is more expected under H than it is
under not H, well then the likelihood ratio would actually be bottom heavy, and so the bottom number
would be greater than the top number, in which case the likelihood ratio as a whole would be
less than 1, but yet E would still be evidence for H. Okay, anyway, that's a technical aside.
You get my point, right? When you have H's on the top and not H on the bottom, well then E is
evidence for H just in case the likelihood ratio is greater than 1, and just think of that in terms
of E being more expected under H than it is under not H, or E being more likely under H, or E being
more probable under H than it is under not H. So anyway, to see why this is true, to see why the
probability of H given E is greater than the probability of H just in case the likelihood
ratio is greater than 1, we're going to have to recall the odds form of Bayes theorem. The
posterior ratio is equal to the likelihood ratio times the prior ratio. Suppose we have some fixed
value for the probability of H, okay, and we're going to call that fixed value K. So I'm just
plugging in K here, and of course when the probability of H is K, well then the probability
of not H is just 1 minus K, right? If the probability of H is 0.7, well then the probability that H is
false, that is the probability of not H, is going to be 1 minus 0.7 or 0.3. So if we have a fixed
value for the probability of H, if we fix that at K, well then the prior ratio is going to be the
fixed value of K over 1 minus K. So now we're going to let the likelihood ratio be exactly 1,
so then we have 1 times K over 1 minus K, and of course 1 times something is just that thing,
right? So we get the posterior ratio is equal to K divided by 1 minus K. But then the posterior
ratio is just the same thing as the prior ratio, right? And again if you want to turn the probability
of H given E into an absolute probability here, what you do is K divided by K plus 1 minus K,
right? So in order to convert this ratio or this relative proportion into an absolute proportion
or an absolute probability, you do the numerator divided by the numerator plus the denominator.
So you do K divided by K plus 1 minus K, and of course K divided by K plus 1 minus K is just
K divided by 1, and that's just K, right? And so the absolute value of the probability of
H given E is equal to K. But remember, we suppose that the probability of H was just K,
and so we just concluded that the probability of H given E is equal to the probability of H,
and that's just equal to K. So when the likelihood ratio is 1, right, that was our
supposition, we suppose that the likelihood ratio was 1, it follows that E isn't evidence for H,
right? Look, because E didn't raise the probability of H. The probability of H conditional on E
just is the same thing as the probability of H, so E didn't raise its probability.
This condition up here isn't met, and so when the likelihood ratio is 1,
E is not evidence for H. But now suppose that the likelihood ratio here is greater than 1,
say it's 2. 2 is just an arbitrary number, my point would apply to any number greater than 1,
but we're just going to pick one for E's, okay? So we're going to pick the number 2. So we're
going to suppose that the likelihood ratio is greater than 1, we're going to say that it's 2,
and that of course is going to give us 2 times K over 1 minus K, right? Remember, we're still
keeping the assumption that the probability of H is K, and so the ratio of the priors is still K
to 1 minus K. But this time instead of multiplying by 1, we're multiplying by 2 because the likelihood
ratio is greater than 1, we're supposing that it's 2. And this in turn gives us that the posterior
ratio is equal to 2K divided by 1 minus K. Notice what we've done here, we have the posterior ratio
here, we have the posterior ratio here, this is from the previous case that we looked at when the
likelihood ratio was 1, this is the current case when the likelihood ratio is 2. Notice
what we've done here, we've increased the numerator by a factor of 2, going from this case to this
case, while keeping the denominator exactly the same. We've therefore doubled the odds of the
probability of H given E in relation to the probability of not H given E. If the odds were,
say, 1 to 1 when the likelihood ratio is 1, the odds when the likelihood ratio is 2 become 2 to 1.
If the odds were, say, 1 to 100 when the likelihood ratio is 1,
they're now 2 to 100 when the likelihood ratio is 2. This in turn means that when we shift from
a likelihood ratio of 1 to a likelihood ratio of 2, the posterior odds become more favorable
to the probability of H given E, right, they become more top-heavy. And hence the probability of H
given E is higher when the likelihood ratio is 2 compared to when the likelihood ratio is 1,
right, because the posterior odds going from the former case to the present case
have become more favorable to the probability of H given E. The posterior odds have in fact
doubled in its favor. And this point, of course, easily generalizes to any likelihood ratio greater
than 1. If the likelihood ratio were 3, well, then when we compare this case to this case,
we would have tripled the odds in favor of the probability of H given E in relation to the
probability of not H given E. So this point, again, easily generalizes to any likelihood
ratio greater than 1. But we've already seen that when the likelihood ratio is 1, right,
the probability of H given E is equal to K, right, that's what we showed last time. When
the likelihood ratio is equal to 1, the posterior probability of H, that is the probability of H
given E, just equals the prior probability of H, which equals K. That's what happens when the
likelihood ratio is 1. But we just concluded that when the likelihood ratio is greater than 1,
right, we increase the odds in favor of the probability of H given E. And as a result of that,
the posterior probability of H, that is, the probability of H given E has increased,
And hence, the probability of h given e has to be greater than k, right?
If the probability of h given e was k in this case, and then going from here to here,
we've increased the probability of h given e, well, then the probability of h given
e in this case has got to be greater than k and hence, in this case,
the probability of h given e is greater than the probability of h and what that
means is that e has raised the probability of h.
E is evidence for h.
That's what happens when the likelihood ratio is greater than one and
the likelihood ratio is greater than one.
E is evidence for
h. And similar reasoning shows that the probability of h given e is less than the probability
of h when the likelihood ratio is less than 1. Instead of increasing the odds in favor
of the numerator here, you'll be decreasing the odds against the numerator here. And the
effect of that on the absolute probability of the numerator here will be to diminish
it. The result of this is that the likelihood ratio is the key to evidential confirmation
or disconfirmation. When the likelihood ratio is greater than 1, the probability of h given
e is greater than the probability of h. And hence, e is evidence for h. In such a case,
e confirms h, or supports h, or raises the probability of h. By contrast, when the likelihood
ratio is less than 1, the probability of h given e is less than the probability of h.
And hence, e is evidence against h. e disconfirms h, or lowers the probability of h. Equivalently,
in this case, e is going to be evidence for the negation of h. e is going to confirm the
negation of h, or raise the probability of the negation of h. And finally, when the likelihood
ratio is 1, the probability of h given e is equal to the probability of h. And hence,
e is evidentially irrelevant to h. It neither confirms nor disconfirms h. It neither increases
nor decreases the probability of h. So here, then, are some important Bayesian facts.
In the preceding reasoning, I actually only showed the left or right hand side of these
biconditionals, but it's very straightforward to show the right to left direction in basically
the same way that I just did. So, e is evidence for h if and only if the likelihood ratio
of h to not h is greater than 1. In other words, e is evidence for h if and only if
e is more expected, or more probable, or more likely on h than not h. In other words, e
is less surprising on h than it is on not h. e is evidence against h if and only if the
likelihood ratio is less than 1. e is less expected, or less probable, or less likely
on h than not h. In other words, e is more expected on the negation of the hypothesis
than it is under the hypothesis itself. Finally, e is irrelevant to h if and only if the likelihood
ratio is equal to 1. e is then equally expected, or equally probable, or equally likely on
both h and not h. And once again, this is why you hear people in philosophy say things
like, this data is more expected under one hypothesis than under the other, and hence
it counts as evidence for the former over the latter. Notice also how all of this is
comparative in nature. Right, for some piece of evidence or some piece of data to confirm
a hypothesis, that is to be evidence for the hypothesis, the evidence can still actually
be quite unlikely on that hypothesis. What matters is that the evidence is even more
unlikely on the negation of that hypothesis. Right, so some data could still be very surprising
on a hypothesis, and yet not count as evidence against that hypothesis, because that evidence
might very well be even more surprising under the negation of that hypothesis, or it might
be just as surprising. So take that to heart. The probability of some evidence given a hypothesis
can be very low. The evidence can be very surprising on the hypothesis, it can be not
at all what you expect on the hypothesis, and yet it doesn't automatically, it doesn't
thereby follow that that data, that that evidence is evidence against the hypothesis. Oftentimes
it will be, because the negation of that hypothesis will predict the data better, it won't be
as surprising, but perhaps not. Perhaps the data is equally unexpected, or even more unexpected
under the negation of that hypothesis. So keep that in mind, keep the comparative nature
of Bayesian confirmation in mind. I'll cover this later on, I'll talk about it in more
detail, but it's important to note here, because this is where it's directly relevant.
And also think about that intuitively, like it makes sense. Suppose that my roommate and
I, we take turns taking out the trash, I do it every Sunday, he does it every Wednesday
or something, and suppose we're both really diligent about this, my roommate knows that
I know about this, I know that he knows about this, I've been doing this for the past year,
let's say, and I haven't missed a single day. So we're both really diligent about this,
and we know this about each other. Now suppose I wake up on Sunday to see written on the
whiteboard in the corner, don't forget to take out the trash. And suppose we have two
hypotheses and we're trying to evaluate the probability of these hypotheses in light
of this evidence, in light of the fact that this message is written on the whiteboard.
One hypothesis says that my roommate did it, and he's reminding me, the other hypothesis
is just that like the marker magically flew up and took off its cap and then wrote that
by itself. Now given my setup, it actually does seem pretty surprising that my roommate
would write this on the board. My roommate knows that I know this, like I've never failed
to bring out the trash, why would he do this? And I mean that's my whiteboard, it's my marker,
he usually doesn't touch my stuff. So like this is really out of character for him, it's
really weird, I've been doing this for over a year, neither of us have ever forgotten,
so like why would he write that? It seems really surprising. But do you know what's
even more surprising? If the marker, if the marker magically did it itself. And so even
though it's very surprising, even though the evidence is very surprising, under the hypothesis
that my roommate did this, even though it's not at all probable under that hypothesis,
the evidence is still far more probable under that hypothesis than it is under this competing
hypothesis. And so we still have very powerful evidence for the former over the latter. And
so it makes sense that something can still be evidence for one hypothesis over another,
even though the evidence is unlikely or unexpected or surprising on the former. Finally, another
way to think of evidential support that's ultimately equivalent but differs in emphasis
from the ways that we've been looking at so far, takes its cue from the standard form
of Bayes theorem. So recall the standard form, which is right here, the probability of H
given E is equal to the probability of E given H times the probability of H divided by the
probability of E. Now you can think of this standard form as basically the probability
of H given E is equal to the probability of H times this factor here. This factor, this
is not the Bayes factor, the Bayes factor is just the likelihood ratio, okay? So I actually
don't know what this factor is called here. Let's call it the Joe factor. Okay, so think
of the standard form of Bayes theorem as the posterior probability of H or the probability
of H given E is equal to the Joe factor times the probability of H. Now we can ask, under
what condition is the probability of H given E greater than the probability of H? That
is, under what condition does E raise the probability of H? Under what condition is
E evidence for H? Well, examining the equation here, that happens just in case the probability
of E given H is greater than the probability of E. Just in case the Joe factor is greater
than one. If the Joe factor is greater than one, well then when you multiply something
greater than one by the probability of H, this thing on the left here has to be greater
than the probability of H. So when the Joe factor is greater than one, that is when the
numerator is greater than the denominator, we have the posterior probability of H being
greater than the prior probability of H. And so what that means is that E is evidence for
H, that is E raises the probability of H, when H renders E more probable than E would
have been otherwise, right? This is H rendering E more probable than E would have been otherwise.
And when that happens, E is evidence for H. So this is another way to think about evidence.
Usually though, it's much easier to calculate the likelihood ratio and to use that as a guide
to evidential support. But I did just want to put this on your radar because it's important.
All right, so we've gone through Bayes theorem, we've gone through evidence, we've gone through
probability, we've gone through all this stuff. But you may still find yourself with
out a kind of intuitive grasp of Bayes theorem. And that's where I think visualizations can
really help. They can transform your thinking and help you see the power, the truth, the
utility, and the ultimate why behind Bayes theorem. And so to that end, we are going
to be watching three videos, they're not super duper long. The first one is from three
blue one brown, and I'll talk about the second two later.
The goal is for you to come away from this video, understanding one of the most important
formulas in all of probability, Bayes theorem. This formula, it's central to scientific
discovery. It's a core tool in machine learning and AI. And it's even been used for treasure
hunting. When in the 1980s, a small team led by Tommy Thompson, and I'm not making up
that name, used Bayesian search tactics to help uncover a ship that had sunk a century
and a half earlier. And the ship was carrying what in today's terms amounts to $700 million
worth of gold. So it's a formula worth understanding. But of course, there were multiple different
levels of possible understanding. At the simplest, there's just knowing what each one of the
parts means, so that you can plug in numbers. Then there's understanding why it's true.
And later I'm going to show you a certain diagram that's helpful for rediscovering this
formula on the fly as needed. But maybe the most important level is being able to recognize
when you need to use it. And with the goal of gaining a deeper understanding, you and
I are going to tackle these in reverse order. So before dissecting the formula or explaining
the visual that makes it obvious, I'd like to tell you about a man named Steve. Listen
carefully now. Steve is very shy and withdrawn. Invariably helpful, but with very little interest
in people or the world of reality. A meek and tidy soul, he has a need for order and
structure and a passion for detail. Which of the following do you find more likely? Steve
is a librarian or Steve is a farmer? Some of you may recognize this as an example from
a study conducted by the two psychologists Daniel Kahneman and Amos Tversky. Their work
was a big deal. It won a Nobel Prize and it's been popularized many times over in books
like Kahneman's Thinking Fast and Slow or Michael Lewis' The Undoing Project. What
they researched was human judgments, with a frequent focus on when these judgments
irrationally contradict what the laws of probability suggest they should be. The example with Steve
our maybe librarian, maybe farmer, illustrates one specific type of irrationality. Or maybe
I should say alleged irrationality. There are people who debate the conclusion here,
but more on all of that later on. According to Kahneman and Tversky, after people are given
this description of Steve as a meek and tidy soul, most say that he's more likely to be
a librarian. After all, these traits line up better with the stereotypical view of a
librarian than a farmer. And according to Kahneman and Tversky, this is irrational.
The point is not whether people hold correct or biased views about the personalities of
librarians and farmers. It's that almost nobody thinks to incorporate information about the
ratio of farmers to librarians in their judgments.
With the tools that we've covered so far, I hope you guys are seeing what's going on.
We're given certain data, certain evidence, and we think about how expected that is under
the two hypotheses. One of the hypotheses is that Steve is a librarian. The other hypothesis
is that he's a farmer. And when people are thinking, okay, overall, in light of this
evidence, what's the probability that Steve is a librarian? Or what's the probability
that he is a farmer? Those are the posterior probabilities. And we know that the ratio
of the posteriors is not just a function of the likelihood ratio. That is, it's not just
a function of how expected that data is under the hypotheses. Steve fitting that description
might be more expected under the hypothesis that he's a librarian than it is under the
hypothesis that he is a farmer.
But you know what else you have to keep in mind? Remember the odds form of Bayes theorem.
You have to also keep in mind the ratio of the priors. You can't forget how many farmers
there are in relation to librarians. If farmers vastly outnumber librarians in the population,
you have to take that into account when you're trying to determine the posterior probabilities.
And ignoring the priors in this kind of way is often called base rate neglect. You're
ignoring the base rate of librarians in the population or you're the base rate of farmers
in the population. Anyway, I'm kind of anticipating things here, but I did just want to say that
and connect it to what we've covered so far.
In their paper, Kahneman and Tversky said that in the US that ratio is about 20 to 1.
The numbers that I could find today put that actually much higher. But let's stick with
the 20 to 1 number, since it's a little easier to illustrate and it proves the point just
as well. To be clear, anyone who has asked this question is not expected to have perfect
information about the actual statistics of farmers and librarians and their personality
traits. But the question is whether people even think to consider that ratio enough to
at least make a rough estimate. Rationality is not about knowing facts, it's about recognizing
which facts are relevant.
Now if you do think to make that estimate, there's a pretty simple way to reason about
the question, which, spoiler alert, involves all of the essential reasoning behind phase
theorem. You might start by picturing a representative sample of farmers and librarians, say 200 farmers
and 10 librarians. Then when you hear of this meek and tidy soul description, let's say
that your gut instinct is that 40% of librarians would fit that description and that 10% of
farmers would. If those are your estimates, it would mean that from your sample you would
expect about 4 librarians to fit the description and about 20 farmers to fit that description.
So the probability that a random person, among those who fit this description, is a
librarian, is 4 out of 24, or 16.7%. So even if you think that a librarian is 4 times as
likely as a farmer to fit this description, that's not enough to overcome the fact that
there are way more farmers. The upshot, and this is the key mantra underlying phase theorem,
is that new evidence does not completely determine your beliefs in a vacuum. It should update
prior beliefs. If this line of reasoning makes sense to you, the way that seeing evidence
restricts the space of possibilities, and the ratio you need to consider after that,
then congratulations, you understand the heart of Bayes' theorem. Maybe the numbers that
you would estimate would be a little bit different, but what matters is how you fit the numbers
together to update your beliefs based on evidence. Now, understanding one example is one thing,
let's see if you can take a minute to generalize everything that we just did, and write it
all down as a formula. The general situation, where Bayes' theorem
is relevant, is when you have some hypothesis, like Steve is a librarian, and you see some
new evidence, say this verbal description of Steve as a meek and tidy soul, and you
want to know the probability that your hypothesis holds given that the evidence is true. In
the standard notation, this vertical bar means given that, as in, we're restricting our
view only to the possibilities where the evidence holds. Now remember the first relevant number
we used, it was the probability that the hypothesis holds before considering any of that new evidence.
In our example, that was 1 out of 21, and it came from considering the ratio of librarians
to farmers in the general population. This number is known as the prior. After that,
we need to consider the proportion of librarians that fit this description, the probability
that we would see the evidence given that the hypothesis is true. Again, when you see
this vertical bar, it means we're talking about some proportion of a limited part of
the total space of possibilities. In this case, that limited part is the left side,
where the hypothesis holds. In the context of Bayes' theorem, this value also has a
special name, it's called the likelihood. Similarly, you need to know how much of the
other side of the space includes the evidence, the probability of seeing the evidence given
that the hypothesis isn't true. This funny little elbow symbol is commonly used in probability
to mean not. So, with the notation in place, remember what our final answer was. The probability
that our librarian hypothesis is true, given the evidence, is the total number of librarians
fitting the evidence, 4, divided by the total number of people fitting the evidence, 24.
But where did that 4 come from? Well, it's the total number of people times the prior
probability of being a librarian, giving us the 10 total librarians, times the probability
that one of those fits the evidence. That same number shows up again in the denominator,
but we need to add in the rest. The total number of people times the proportion who
are not librarians, times the proportion of those who fit the evidence, which in our
example gives 20. Now notice the total number of people here, 210, that gets cancelled out,
and of course it should, that was just an arbitrary choice made for the sake of illustration.
This leaves us finally with a more abstract representation purely in terms of probabilities.
And this, my friends, is Bayes' theorem. More specifically, going back to my characterizations
earlier on, that is the standard form of Bayes' theorem, and remember we actually looked at
two different variants of the standard form. One of them only had the probability of E
down here in the denominator, but I also explained how that is equivalent to having this longer
denominator here. So this is just the second variant of the standard form of Bayes' theorem
that we already covered. More often, you see this denominator written simply as P of
E, the total probability of seeing the evidence, which in our example would be the 24 out of
210. But in practice, to calculate it, you almost always have to break it down into the
case where the hypothesis is true, and the one where it isn't.
Capping things off with one final bit of jargon, this answer is called the posterior. It's
your belief about the hypothesis after seeing the evidence.
Writing it out abstractly might seem more complicated than just thinking through the
example directly with a representative sample. And yeah, it is. Keep in mind, though, the
value of a formula like this is that it lets you quantify and systematize the idea of changing
beliefs. Scientists use this formula when they're analyzing the extent to which new
data validates or invalidates their models. Programmers will sometimes use it in building
artificial intelligence, where at times, you want to explicitly and numerically model
a machine's belief. And honestly, just for the way that you view yourself and your own
opinions and what it takes for your mind to change, Bayes' theorem has a way of reframing
how you even think about thought itself. Putting a formula to it can also be more important
as the examples get more and more intricate. However you end up writing it, I actually
encourage you not to try memorizing the formula, but to instead draw out this diagram as needed.
It's sort of a distilled version of thinking with the representative sample,
where we think with areas instead of counts, which is more flexible and easier to sketch
on the fly. Rather than bringing to mind some specific number of examples like 210,
think of the space of all possibilities as a one by one square. Then, any event occupies
some subset of this space, and the probability of that event can be thought about as the area of
that subset. So for example, I like to think of the hypothesis as living in the left part of the
square with a width of p of h. Now I recognize I'm being a bit repetitive, but when you see
evidence, the space of possibilities gets restricted, right? And the crucial part is that
that restriction might not be even between the left and the right. So the new probability for
the hypothesis is the proportion that it occupies in this restricted wonky shape.
Now, if you happen to think that a farmer is just as likely to fit the evidence as a librarian,
then the proportion doesn't change, which should make sense, right? Irrelevant evidence doesn't
change your beliefs. But when these likelihoods are very different from each other, that's when
your belief changes a lot. Bayes' theorem spells out what that proportion is, and if you want,
you can read it geometrically, something like p of h times p of e given h, the probability of both
the hypothesis and the evidence occurring together, is the width times the height of this
little left rectangle, the area of that region. Alright, this is probably a good time to take
a step back and consider a few of the broader takeaways about how to make probability more
intuitive beyond just Bayes' theorem. First off, notice how the trick of thinking about a
representative sample with some specific number of people, like our 210 librarians and farmers,
was really helpful. There's actually another Kahneman and Tversky result, which is all about
this, and it's interesting enough to interject here. They did this experiment that was similar to
the one with Steve, but where people were given the following description of a fictitious woman
named Linda. Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy.
As a student, she was deeply concerned with issues of discrimination and social justice,
and also participated in the anti-nuclear demonstrations.
After seeing this, people were asked, what's more likely? One, that Linda is a bank teller,
or two, that Linda is a bank teller and is active in the feminist movement. 85% of participants said
that the latter is more likely than the former, even though the set of bank tellers who are active
in the feminist movement is a subset of the set of bank tellers. It has to be smaller.
So that's interesting enough. But what's fascinating is that there's a simple way
that you can rephrase the question that dropped this error from 85% to zero. Instead, if participants
were told that there are 100 people who fit this description, and then they're asked to estimate
how many of those 100 are bank tellers, and how many of them are bank tellers who are active in
the feminist movement, nobody makes the error. Everybody correctly assigns a higher number to
the first option than to the second. It's weird. Somehow phrases like 40 out of 100
kick our intuitions into gear much more effectively than 40%, much less 0.4, and much less abstractly
referencing the idea of something being more or less likely. That said, representative samples
don't easily capture the continuous nature of probability. So turning to area is a nice
alternative, not just because of the continuity, but also because it's way easier to sketch out
when you're sitting there pencil and paper puzzling over some problem. You see, people often
think about probability as being the study of uncertainty. And that is, of course, how it's
applied in science. But the actual math of probability, where all the formulas come from,
is just the math of proportions. And in that context, turning to geometry is exceedingly helpful.
I mean, take a look at Bayes theorem as a statement about proportions, whether that's
proportions of people, of areas, whatever. Once you digest what it's saying, it's actually
kind of obvious. Both sides tell you to look at the cases where the evidence is true, and then to
consider the proportion of those cases where the hypothesis is also true. That's it. That's all
it's saying. The right hand side just spells out how to compute it. What's noteworthy is that such
a straightforward fact about proportions can become hugely significant for science, for artificial
intelligence, and really any situation where you want to quantify belief. As I mentioned,
some psychologists debate Kahneman and Tversky's conclusion that the rational thing to do is to
bring to mind the ratio of farmers to librarians. They complain that the context is ambiguous.
I mean, who is Steve, exactly? Should you expect that he's a randomly sampled American?
Or would you be better to assume that he's a friend of the two psychologists interrogating you?
Or maybe that he's someone that you're personally likely to know? This assumption
determines the prior. I, for one, run into way more librarians in a given month than I do farmers.
And needless to say, the probability of a librarian or a farmer fitting this description
is highly open to interpretation. For our purposes, understanding the math,
what I want to emphasize is that any question worth debating here can be pictured in the context
of the diagram, questions about the context shift around the prior, and questions about the personalities
and stereotypes shift around the relevant likelihoods. All that said, whether or not you buy
this particular experiment, the ultimate point that evidence should not determine beliefs,
but update them, is worth tattooing in your brain. I am in no position to say whether this does or
does not run against natural human instinct. We'll leave that to the psychologists. What's more
interesting to me is how we can reprogram our intuition to authentically reflect the implications
of math, and bringing to mind the right image can often do just that. And that, of course,
is why we're looking at these different visualizations of Bayes' theorem. Okay,
so that's the first one by Three Blue One Brown. Next up on the list are two videos produced by
philosopher Ben Page. These videos are actually based on a paper that he has on academia.edu.
I'll put a link to this in the description. And what he's using is a Bayesian bar. So this is
basically, basically, I'm terrible, this is basically the exact same thing as the square
that we just saw in the Three Blue One Brown video. But instead of a square, you're just using a bar,
a rectangle. Still, though, Ben Page uses this in the context of philosophy and gives some
philosophical examples. So I think it's really important to go through it. And also, it just
allows you a panoply of different ways to represent Bayes' theorem and probabilistic reasoning. If
you like the bar, you can go with that. If you like the boxes, you can go with that. What matters
is that we have a visual way of accurately depicting how we should reason probabilistically.
If you're anything like me, you'll have the experience of reading a philosophy essay,
one you enjoy, one you actually understand. But then all of a sudden, a section appears with
a load of mathematical formulae, you've reached the Bayesian probability section,
and your eyes glaze over, and you think, oh, I dropped maths for a reason.
Well, in this video, I want to talk about how we can do Bayesian probability visually
using a method that I learned from John Hawthorne. Before I let you in on the secret
as to how to think about Bayesian probability visually, I've got four preliminaries to run
through. The first is that the argument I will use, which is the problem of evil,
is meant as an illustration only. And so I won't be going into it in any detail here.
The second is that I won't really spend any time justifying the numbers or probability assignments
that I give in the argument, as that would take us into the details of it. These are somewhat
arbitrary values that are used for the sake of illustration. So try not to get too hung up on
whether you think they are accurate. Third, I'm not going to discuss different interpretations
of probability, such as frequency, propensity, and degrees of belief interpretations. And finally,
we're going to think that something is evidence for a theory if it raises the probability of that
theory being true, and something is evidence against a theory if it lowers the probability of
that theory being true. Now if you want to learn more about these preliminaries, then I will be
doing some future video on probability visually, remember to subscribe. So in order to think about
Bayesian probability visually, we're going to introduce something called the Bayesian bar
after the Reverend Thomas Bayes, who came up with Bayes theorem. And the Bayesian bar is just a
rectangle. What's important about the rectangle is its surface area, as this is our total probability
space and has the value one. This is because it contains all our different probabilities,
and we're certain that at least one of them is going to be true. We can now split the bar up
into further probabilities. So let's imagine someone who is what we'll call a perfect agnostic
about God's existence, since they're 50-50 as to whether God exists. So for this perfect agnostic,
we can designate half the bar to represent how likely they think it is that God exists,
which we'll call theism, and the other half to how likely they think it is that God doesn't exist,
which we'll call atheism. Now imagine our agnostic has never considered any evidence for
theism or atheism. There are a number of different types of evidences that he could consider,
but suppose he starts by thinking how likely or not the existence of God would be if there's evil
in the world. Imagine he starts with the theistic half of the bar and asks himself,
suppose God exists, how likely would it be that there is evil in the world? Maybe he thinks that
because God is all-powerful, all-knowing, and holy good, the existence of evil doesn't seem very
likely at all. But perhaps he also thinks it's not impossible that God and evil exist together,
and so he gives three quarters of the theistic portion of the bar, the theism and no evil,
and a quarter of it, the theism and evil. Importantly, keep in mind that this is three fourths
of the theistic bar, right? We're not saying that this is three fourths probable, like all things
considered, right? Remember, the total probability space is the bar as a whole, right? So when we
say three fourths, notice that he said three fourths of the theistic bar. And a quarter of it,
the theism and evil. Now he starts to think about the atheist portion of the bar and thinks,
suppose God doesn't exist, how likely is it that there would be evil? This time he thinks it's
pretty likely that there would be evil if there's no God, because there's no being like God trying
to prevent it. And he thinks it would be pretty unlikely that there would be no evil in an atheist
world. So he gives three quarters of the atheistic portion, the atheism and evil, and a quarter to
atheism and no evil. The next step is to think about these different probabilities in terms of the
whole bar, rather than just the theistic and atheistic portions. So the probability of theism
and no evil in terms of the whole bar is three eighths. A theism and evil is one eighth.
Of atheism and evil is also three eighths. And of atheism and no evil is also one eighth.
And of course, just to cross our T's and dot our I's, that's because this was originally three
fourths of the theistic bar. The theistic bar was one half of the total bar. So three fourths times
one half is going to be three eighths. And you do similar calculations for the rest of them.
And if we add up all these individual probabilities, we get the value one.
Suppose now our agnostic comes across some evil for the first time.
This gives him some evidence, namely that evil exists. And as a result, the bar is going to change.
What he now needs to do is remove two portions of the bar, where it says there is no evil,
since he has found out that there is evil. So both the theism and no evil and atheism and
no evil sections must go. Because of this, here's a bar which is smaller than the one before.
But the probabilities of this bar must also add up to one, since it now contains the whole
probability space. And at present, they don't add up to one. So he's going to have to perform
a process called renormalization. This sounds more complicated than it is. And it's just the
process of changing the values of the remaining probability segments, whilst preserving their
ratios. So this means that because atheism and evil section is currently three times the size
of the theism and evil section, it must also be three times the size after this process.
So the probability of theism and evil changes from one eighth to one quarter.
And the probability of atheism and evil changes from three eighths to three quarters.
So this gives our perfect agnostic his final probabilities.
And just to give you a way to calculate that, remember this was three eighths, and this was
one eighth. Now what you want to do if you want to renormalize that is to find the ratio between
the surface area of this bar and the surface area of this bar. Remember this one was three eighths
as the whole bar, this one was one eighth of the whole bar. So the ratio between those is three eighths
divided by one eighth. Three eighths divided by one eighth is just three over one. So what that means
is the ratio of this surface area of this bar here to this is three to one. And of course, when you
have a ratio of three to one, and you want to renormalize that so that it takes up the whole
of the probability space, you do the same calculation that we did in the case of the males and females
in the class of the kindergartners, right? Remember that one was three to two. So if you wanted to
find the absolute proportion of males in the class as a whole, you'd do three divided by three plus
two, or three out of five, which is three fifths. Similarly, when we have a three to one ratio here,
if you want to find out the proportion that the surface area of this bar takes up,
among the whole bar here, you do three divided by three plus one. And that's three fourths.
And of course, the remaining bit of the bar is going to be one fourth, because again, we're
renormalizing. Given that we've learned that there's evil, we're crossing off the spaces in which
there's no evil. And so this is our total probability space here. And it's got to add up to one. So
you have three fourths and one fourth. You've ruled out the cases in which evil doesn't exist,
because we know that evil exists. Okay, so anyway, that's basically a step by step guide as to how
you can renormalize. Given the existence of evil, he now thinks the existence of God, theism,
has the probability of a quarter or 25%. As only a quarter of his final bar is taken up by theism.
On the other hand, he now thinks that atheism has the probability of three quarters or 75%.
In other words, whilst there might still be a God who has chosen to allow evil,
it's more likely that God doesn't exist, given the existence of evil.
The overall result is therefore that evil is evidence for atheism. And the reason for this is
that evil raised the probability of atheism from a half, represented in our initial bar,
to three quarters, which is represented in our final bar. And evil decreased the probability
of theism from a half, again represented in our initial bar, to a quarter, as we can see in our
final bar. So before I end this video, let's go through one more example. And this time,
we're not going to put any values on the bar. So you can see that the Bayesian bar works just
visually on its own. So let's again think about the problem of evil. But this time,
the person we are going to be thinking about is someone who thinks that theism is really likely
and atheism is not that likely. So now we're going to have to split up the theistic portion of the
bar further, remembering that we're assuming this person is assigning probabilities in a vacuum.
So before they have any evidence that evil does or doesn't exist. Again, we're going to think a lot
of the theistic portion is going to be for God and no evil, because we think that God wouldn't
want there to be evil in the world, and we're trying to prevent it. And then the remaining part
of the theistic portion of the bar is going to be for God and evil. And then on the atheist side,
maybe this time we're 50 50, as we think that atheism and evil is kind of just as likely as
atheism and no evil. So now we add in our evidence that there is evil in the world. And so we remove
the portions of our bar, where it says there is no evil. So theism and no evil and atheism and no
evil. So we're just left with a bar that has just theism and evil and atheism and evil. And you can
see our final probabilities just by looking at the bar. So the question now is, is evil evidence
for theism or atheism? The answer to this is going to be atheism again. And that's because
atheism is now more probable than it was to begin with. And theism is less probable than it was to
begin with. And remember, we're thinking about evidence as probability raising. And so evil
raises the probability of atheism. And this is the case, even though in our example, theism is
still more probable overall, since it takes up the biggest portion of our bar. So that's how
to think about probability visually. In a future video, I'll talk about how the Bayesian bar relates
to Bayes Theorem, and how the actual formula maps onto this visual bar. In a previous video,
I explained how to think about probabilistic reasoning visually by using the Bayesian bar.
If you haven't already seen that video, I'd recommend that you watch it before continuing
with this one, as I'm now going to explain how the Bayesian bar relates to Bayes Theorem.
As a result of this, the content in this video is going to get a bit more technical. But by using
the bar, you'll see that it is simpler than it initially looks. In order to show you how the
Bayesian bar relates to Bayes Theorem, we're going to need an example. For this video, we're going
to use the example of fine tuning arguments for theism. And just as with the previous video,
I'm not really going to say anything about this type of argument, or justify the probability
assignments that I give. All you need to know for our purposes here, is that our universe has very
finely tuned values that allow for life. And that if these values had been different,
then it's very likely that life wouldn't exist. So let's draw a bar of a perfect agnostic,
who is 5050 that God exists. So half of the bar is for theism, and the other half for atheism.
Now, although you could do all of what I'm about to do on one bar, it's easier to show you how it
relates to Bayes Theorem, if I draw additional bars with extra content on them. So let's now draw
another bar, just the same as the previous one. But this time, we're going to add in how likely
we think fine tuning is on theism and on atheism. Firstly, then theism. We might think fine tuning
is not that surprising on theism, since we think it's likely that God would want a world with
living beings in it, perhaps so that he could have a relationship with them, and the goods that come
from that. As a result, we give three eighths of the whole bar to theism and fine tuning,
and one eighth to theism and no fine tuning. We then think about the atheist portion of the bar.
This time, we're going to think that fine tuning is really unlikely on atheism,
as there's no being that wants life, and it seems extraordinarily unlikely that the universe
happened to have the life permitting finely tuned values by chance. So we give one eighth of the
whole bar to atheism and fine tuning, and three eighths of the whole bar to atheism and no fine
tuning. We now get some evidence, namely that there's fine tuning in the world. And so we
delete the portions of the bar where it says that there is no fine tuning. Again, to show this,
I'm just going to draw another bar with these sections deleted. Notice also that I've renormalised
the probabilistic values of the portions of the bar so that they add up to one. As I said in the
previous video, this is just the process of changing the values of the remaining probability segments
whilst preserving their ratios. So because the theism and fine tuning portion
is three times the size of the atheism and fine tuning section, it must be three times the size
after this process. So the final probability of theism and fine tuning is three quarters,
and the probability of atheism and fine tuning is a quarter. The overall result then is that fine
tuning is evidence for theism, as it raised the probability of theism and lowered the probability
of atheism. Now before I explain to you how the bar relates to Bayes Theorem, it'll be helpful to
introduce you to some jargon. So take a look at the first bar and the half probability assignments
we've given to theism and atheism. These are called our prior probabilities, and they're the
probabilities we give to set out the degree of certainty we have or ought to have in certain
hypotheses before we've taken this bit of evidence into account. How we determine what these values
should be is subject to debate, but exploring that will have to wait for another time.
That's where you get into the distinction between subjective and objective Bayesians
that we talked about earlier on in this video. Remember the subjective Bayesians think that
so long as you are obeying the five core normative rules, you're going to count as rational. But as
we saw it, that might allow for a lot of subjectivity, and many people think that it's too
subjective. So the objective Bayesians put further constraints, and they're going to be focusing
largely on constraints imposed on the assignment of prior probabilities. Anyway, that's the debate
that he's alluding to. Just wanted to make that connection with earlier on in my video.
Now look at the second bar, and the probability assignments we see there of 1-8, 3-8, 1-8,
and 3-8. These are what's called conditional probabilities, since they're in the conditional
form. This means that they say, given some x, y has probability of dot dot dot. So in our example,
given theism, fine-tuning has the probability of dot dot dot, or given atheism, fine-tuning has
the probability of dot dot dot. And just a note here, 3-8 is not that conditional probability
for theism, right? Given theism, so restricting your focus to the space of theism, three quarters
of that space is occupied by fine-tuning. So the conditional probability under this model here,
the conditional probability of fine-tuning given theism is going to be 3 fourths, not 3 eighths.
I just wanted to clarify that. The 3 eighths here is what he wrote, taking into account the
proportion of this space here to the total probability space. But the 3 eighths is not
the conditional probability of fine-tuning given theism, that's going to be 3 fourths.
I just wanted to clarify that. Now let's look at the final bar and the two probability assignments
there. So three quarters for theism and fine-tuning, and a quarter for atheism and fine-tuning.
These are known as the posterior probabilities. That is the probability after we've taken into
account the relevant evidence. With this jargon now explained, let's think about how the bar relates
to Bayes Theorem. Bayes Theorem says the probability of the hypothesis given the evidence is equal to
the probability of the hypothesis multiplied by the probability of the evidence given the hypothesis
divided by the evidence. So here you can see the formula written out in full with pr standing for
probability, h for hypothesis and e for evidence. The jargon we have just learned is also useful
when thinking about the formula with the probability of the hypothesis given the evidence relating to
the posterior probability, the probability of the hypothesis being connected to the prior probability,
and the probability of the evidence given the hypothesis linking with the conditional probability.
In order to think about how the formula works, let's ask the question, what is the probability of
theism given the evidence of fine-tuning? So in this case, h, a hypothesis, is going to stand for
theism and e, our evidence, will be fine-tuning. In order to find out the answer to this, we're
going to need to fill in the other half of Bayes Theorem. First of all is the probability of the
hypothesis, which in our case is theism. This is going to be a half and we can see this in our first
bar. Next is the probability of the evidence given the hypothesis, which in our case is the probability
of fine-tuning given theism. This has the value of three quarters, which we can work out from looking
at our second bar. In order to do this, look at the second bar and imagine removing the atheist
half of the bar so that all we have left is the theistic half of it. Now ask yourself how much
of the theist part of the bar does fine-tuning take up? The answer is three quarters and this
is the value we put here for the probability of the evidence, fine-tuning, given the hypothesis,
theism. So far we filled in the top part of the Bayesian formula and now we have to work out what
it is that we divide it by, which is the probability of the evidence. To get this value we have to
work out how likely the evidence is in our case fine-tuning on any hypothesis and to do this we
need another formula. This one says that we first find out the value of the probability of the evidence
given the hypothesis multiplied by the probability of the hypothesis and then also the value of the
probability of the evidence if the hypothesis is false multiplied by the probability of the hypothesis
being false and then we add these two values together. So in our case we get the probability
of fine-tuning given theism three quarters which we can work out from looking at our second bar
and multiply it by the probability of theism which is a half which we can see in our first bar.
We then work out the value of fine-tuning given the falsity of theism so in our case the probability
of fine-tuning on atheism as this is the only other hypothesis we're considering and this is
the value of a quarter which we can work out from looking at our second bar and then we multiply this
by the probability of the falsity of our hypothesis and so in our case the probability of atheism
and here we can see that the value is a half which we can see in our first bar. After getting these
values we add them together the result of this is a half and we can check this by looking at our
second bar for if you look at it and add up the two sections of the bar where there is fine-tuning
we can see that it takes up half the space of the whole bar with that done we have the three
values we need to fill in Bayes theorem so a half for the probability of the hypothesis
three quarters for the probability of the evidence given the hypothesis
and a half for the probability of the evidence so let's answer the question what's the probability
of theism given fine-tuning first we input the numbers we have so it's a half multiplied by
three quarters divided by a half since many people find it easier to perform calculations on decimals
rather than fractions that's the same as 0.5 multiplied by 0.75 divided by 0.5 and the answer
we get is 0.75 or three quarters and we can also check that this is right by looking at our third
and final bar where we can see that three quarters of the bar is for theism and fine-tuning just as
we got here so that's how the Bayesian bar relates to Bayes theorem whilst this has been a bit more
technical I hope you can see that the formula is easier to fill out when we're using the bar
and the bar is also a helpful way to show you what Bayes theorem is doing but remember as I
said to you in the previous video you can always think about probabilistic arguments in terms of
the bar without numbers if you prefer but knowing how the formula connects with the bar is helpful
so that you can use the more formal apparatus if you wish and translate more formal presentations
of arguments into the bar all right so we've gone over different ways to visually represent
Bayes theorem that was the purpose of this fourth section of my video if you just follow the steps
that three blue one brown laid out or that Ben Page just laid out you'll be able to solve probabilistic
problems a lot easier and you'll also start to refine and hone your intuitions to be in line
with proper probabilistic reasoning but onwards we march to perhaps the most important section
in the whole video we'll basically be taking what we've learned so far and seeing why certain
kinds of thinking are fallacious or mistaken so one common mistake is one we've already seen or at
least one we've already hinted at and it's the base rate fallacy this basically amounts to ignoring
prior probabilities when trying to assess how probable a hypothesis becomes in light of certain
evidence so recall the odds form of Bayes theorem the posterior ratio is equal to the
likelihood ratio times the prior ratio notice that the ratio of the posteriors is a function
not just of the likelihood ratio but also of the ratio of the priors oftentimes without recognizing
it will ignore the ratio of the priors when trying to determine the ratio of the posteriors
and will instead only focus on the likelihood ratio that's disastrous since even if the likelihood
ratio is very top heavy the prior ratio may be very bottom heavy and so if we ignored the prior
ratio we'd mistakenly conclude that the posterior ratio was significantly top heavy and hence that
the posterior probability of the hypothesis is very high after conditioning on the evidence in
question this is a mistake you need to take into account the priors this sort of reasoning is also
known as the base rate fallacy or base rate neglect when thinking about statistical problems
the base rate of a feature is basically just the prevalence of that feature within the
population at large oftentimes we neglect this prevalence in assessing the probability that
some hypothesis is true for instance in assessing whether or not someone has cancer we often neglect
that base rate and instead focus on the likelihood of the evidence on the hypothesis this may have
been what happened to you in the opening problem of this video you may have failed to take into
account the 1% prevalence of cancer in the population and focused instead on the probative
or evidential value of the positive test result this may then have skewed your assessment of the
probability that the woman had cancer towards it being much more likely that she had cancer
but as we've seen that's mistaken yes while the test is 90 accurate and so count says reasonably
good evidence that the woman has cancer the prevalence of this cancer in the population
is still very low and when you plug in all the numbers the overall probability that she has
cancer is only 8% and the reason is that among the people who test positive most of them in
fact 92 percent of them are false positives this shows the importance of taking into account
both the evidence and the base rates or priors but also of course don't be too focused on the
prior there is a kind of dogmatism where someone thinks that hypothesis is so implausible that
that they either don't consider evidence against it or they ignore evidence against it
or they downplay evidence against it or they think no amount of evidence could overcome their prior
those are also all mistakes so anyway that is the first mistake that we're going to cover
the cancer example also illustrates another common mistake in reasoning namely inferring
from the fact that there is a strong piece of evidence for a hypothesis that the hypothesis
must probably be true no that's incorrect right just because e is evidence for h indeed just because
e is very strong evidence for h it doesn't follow that e makes h probable h could still be very
improbable overall remember to say that e is evidence for hypothesis h is just to say that
e raises h's probability that doesn't mean that e makes the hypothesis probable before taking
the evidence into account the hypothesis might be only one percent probable and afterwards it might
be five percent probable notice that the evidence here has raised the probability of the hypothesis
but it clearly doesn't make the hypothesis probable in other words it didn't render the
hypothesis probably true or more probable than not or anything like that so never infer from the
claim that say fine-tuning is evidence for theism that theism is therefore probably true and
never infer from the claim that say evil is evidence for atheism that atheism is therefore
probably true those don't follow to determine whether theism or atheism is probably true you
need to take into account their prior probabilities and not only that but you also have to take into
account our total range of data our total range of evidence that bears on these hypotheses and
this too bridges into another common mistake in reasoning which is ignoring the total evidence
requirement according to the total evidence requirement also called the requirement of total
evidence when assessing the overall probabilities of hypotheses we should take into account all
of the relevant evidence at our disposal instead of just some proper part of that evidence you may
determine that after conditioning on some evidence e a hypothesis is more probable than its negation
but if that's the only evidence you're taking into account and there are other pieces of data
that bear on the probability of this hypothesis then you're not licensed to conclude after only
examining e that the hypothesis on the whole all things considered is more probable than
its negation to infer that you need to take into account the total range of data or evidence that
we have so for instance even if you have a Bayesian argument from evil against the existence of god
and even if you take into account both the likelihood ratio and the prior ratio and even if
you conclude from this that the ratio of the posteriors heavily favors atheism you can't then
conclude that atheism is probably true all things considered you need to take into account other
relevant evidence like fine-tuning divine hiddenness consciousness religious disagreement religious
experience and so on the same is true for Bayesian arguments for theism of course and closely related
to the total evidence requirement is the fallacy of understated evidence which is the next mistake
in Bayesian reasoning on our list and we're going to hand it over here to real a theology to explain
what the fallacy of understated evidence is and to offer a concrete tangible application thereof
let's say a couple with two small children moves in across the street from you they also drive a
brand new car now you just happened to be reading earlier that day that this particular model and
year scored the highest in its class in every single safety measure across the board you also
learned that this car costs significantly more than other cars in its class specifically because
of this high safety rating given the total evidence available to you at this time you reasonably infer
that your new neighbors probably care a great deal for safety for their family
you wait a few days allowing them to settle in before crossing the street to introduce yourself
however as you walk up their driveway you notice that the airbags have been tampered with and
disabled and that the child safety seats are both attached improperly with duct tape naturally you're
a bit puzzled observing the general fact that your neighbors purchased a car specifically
pricey for its safety features clearly seems to be more likely on the hypothesis that the
family cares about safety than on the hypothesis that the family does not care about safety and yet
given the general fact the additional observations the specific facts of the duct taped child safety
seats and disabled airbags are more likely on the hypothesis that the family doesn't care about
safety than on the hypothesis that they do now if say somebody was in a position to know all of
these facts the general and specific facts but yet they presented an evidential argument for the
hypothesis that the family cares a great deal for safety but appealing only to the general fact about
the car's high safety rating and related high cost they would be guilty of understating the evidence
philosopher paul draper argues that many evidential arguments for theism are similarly guilty of
appearing convincing only because they understate the evidence as they quote successfully identify
some general fact about some topic that is more surprising on naturalism than on theism but all
two conveniently ignore more specific facts about the topic facts that given the general fact are
significantly more surprising on theism than on naturalism there are many examples of arguments
for theism which fit this pattern one such argument sets its sights on religious experience
if we agree that the general fact of religious experience had apparently of god is more likely
on theism than on naturalism we can say it counts as some evidence for theism over naturalism
but paul draper reminds us that the general fact of the existence of religious experience
is not the only relevant fact we know about that topic draper reminds us of three specific
facts which given the general fact of religious experience are much more likely on naturalism
than on theism first quote not everyone has theistic experiences and those who do typically
have a prior belief in god or extensive exposure to a theistic religion secondly
the subjects of theistic experience pursue a variety of radically different religious paths
none of which bears abundantly more moral fruit than all the others and third victims of tragedy
are rarely comforted by religious experience again given the general fact of the existence of
theistic religious experience each one of these facts are more likely on naturalism than on theism
and so after considering all the relevant facts about religious experience it's not at all obvious
that relevant observations regarding the topic of religious experience actually favor theism over
naturalism just as when considering all the facts about your neighbor's car it's far from obvious
that they give a damn about their children according to draper many evidential arguments
for theism suffer similarly and what matters more here is more the fallacy of understated
evidence and at least understanding the content of that than whether or not you agree with draper's
assessment there another pretty important mistake that you should be aware of and avoid
is ignoring the comparative nature of Bayesian confirmation we already covered this but this
is worth keeping in mind as a separate mistake that you need to avoid right recall that e is
evidence for h if only of the ratio between the likelihoods of h and not h is greater than one
e is more expected or more probable or more likely on h than it is under the negation of h
notice that this is essentially comparative we're comparing how likely e is under h to
how likely e is under not h and again even if e is very improbable under h e can still be very
powerful evidence for h what matters is that e is still more probable under h than it is under
the negation of h another mistake which is kind of related is demanding strict precise numerical
likelihoods of the data conditional on the hypotheses in question the thing is when you
have the comparative nature of Bayesian confirmation firmly in mind you can see that that isn't strictly
necessary at least for many purposes we saw this actually when we watched ben page's video on the
bayesian bar right he still showed the utility of the bayesian bar and bayesian reasoning even if
you don't assign precise numerical credences and richard swinburne also writes that to accept
that bayes theorem governs all claims about the support given by evidence to hypotheses
does not involve holding that the various probabilities can be given exact numerical
values and as he noted earlier inductive probabilities do not normally have exact
numerical values one can think of them as having rough values values within certain limits and one
can think of the theorem as putting limits on the values of some probabilities given the limits on
others another mistake you might see in reasoning is that you might hear someone say that some data
e obtaining would be evidence for hypothesis h but e not obtaining wouldn't be evidence against h
but that is mistaken if e is evidence for h then not e is evidence against h you could also say if
e would be evidence for h then not e would be evidence against h importantly this just follows
from bayes theorem so for instance if seeing a limb grow back after prayer would be evidence for
the efficacy of prayer then not seeing a limb grow back after prayer would be evidence against
the efficacy of prayer so let's see why this is the case what i'm going to be calling the
evidential symmetry thesis if he is evidence for h then not e is evidence against h so that's our
goal our goal is to establish this thesis so suppose that e is evidence for h then it follows that the
likelihood ratio the ratio of the likelihoods of h to not h is greater than one e is more expected on
h than it is under not h so this ratio here is top heavy and hence when you divide the numerator
by the denominator it's greater than one let's just say the probability of e given h is x and
then the probability of e given not h is x minus n so n is just going to be some positive non-zero
real number of course less than one this is just a way to mathematically represent how
this is greater than one right because x is going to be greater than x minus n
given that n is a positive real number greater than zero and less than one so we got the probability
of e given h being x and we've got the probability of e given not h being x minus n but notice
that the probability of e given h plus the probability of not e given h is equal to one
i actually prove this mathematically for patrons in the script but i hope you can see that it's
just like intuitively obvious so for instance if it's 40 likely that it rains given that it's cloudy
then clearly it's going to be 60 likely that it doesn't rain given that it's cloudy or if it's one
sixth likely that the die rolls a one given that the die is fair then clearly it's going to be
five sixths likely that the die rolls something other than one given that the die is fair these
things have to add up to one if a hypothesis predicts the truth of some data with 20 probability
then clearly the hypothesis will then predict the falsity of that data with 80 probability
so it makes sense if you think about it it's self-evident you might say rearranging this so
we're just subtracting probability of e given h from both sides we get that the probability of not
e given h is equal to one minus the probability of e given h and notice that the exact same sort of
reasoning here can be applied when the hypothesis is not h instead of h right it's the same idea if
the hypothesis not h predicts the truth of some data with 30 probability then that hypothesis
not h will also predict the falsity of that data with 70 probability so this will equally be true
if we plug in not h for h and hence we can plug in not h for here and here because this
just followed from the preceding step and that of course just gives us this but we're called
that the probability of e given h we let that equal x and the probability of e given not h we let
equal x minus n so we have the probability of e given h here and here we have the probability
of e given h here and here and so and so from all the preceding we get this notice here that
that probability of not e given H shows up right here.
The probability of not e given not h shows up here.
Remember that the probability of not e given H
is equal to one minus the probability of e given h,
which just equals x.
So, we do one minus x up here.
And then the probability of not e Given not each
is equal to one minus the probability
of e given not h, and remember the probability
of e giving not h is just x minus n,
so we have down here one minus x minus n.
And of course, this here equals 1 minus x over 1 minus x plus n.
And that, of course, equals 1 minus x over 1 minus x in parentheses plus n.
And notice that that has to be less than 1, right?
You have a number here, and that number in the parentheses is the same number as this,
and then you add a non-zero positive real number to that.
And so the numerator has to be less than the denominator,
in which case this fraction as a whole is less than 1.
And so look what we've just concluded.
This is the likelihood ratio for the hypotheses h and not h with respect to the negation of e.
This is that likelihood ratio here and here.
So this has to be less than 1.
That is what we just concluded.
And so look what we just proved.
We just proved that if the ratio between the likelihoods of h and not h
with respect to the evidence e is greater than 1,
then the ratio between the likelihoods of h and not h
with respect to the negation of e is less than 1.
In other words, we just proved that if e is evidence for h,
then the negation of e is evidence against h, right?
Because here e is more expected under h than it is under not h,
because this fraction is top-heavy, and so e is evidence for h.
But in this case, because this fraction is less than 1, it's bottom-heavy.
So that means the negation of e is more expected under the negation of the hypothesis
than it is under the hypothesis.
And so the negation of e counts as evidence against the hypothesis
and for the negation of that hypothesis.
So anyway, that vindicates our goal.
Our goal was to show that if e is evidence for h, then not e is evidence against h.
And we just showed that.
Also, again, the patrons have access to the script,
which actually, because I just recently added this additional proof.
The script went up to 40 pages. I could do not.
But there's an additional proof for patrons, which goes through a different route.
So again, that is what I call evidential symmetry.
And it's pretty significant.
Consider the meagre-moral-fruits argument or whatever against Christianity.
One very simple way that you could put that argument is
imagine if all and only professing Christians were like noticeably morally superior
to literally everyone else.
Like, despite the fact that, you know, they have very similar social structures to other churches.
They have similar support groups to other churches.
They eat roughly the same things, etc.
Suppose that really the only thing that differentiates them from other groups
is that they are professing Christians.
And of course, that also they are noticeably morally superior to everyone else.
I think it's obvious that if we were to see that,
that would be significant evidence for Christianity.
This is the fruit of the Spirit.
It's God's grace working through people.
It's the transformative power of Christ or the Holy Spirit or something.
I don't know.
The point is just that I think it's obvious that this would be some evidence for Christianity.
But then it just deductively, strictly, logically follows from that,
that if we don't observe that, and of course we don't,
then that is at least some evidence against Christianity.
So it's relevant, for instance, to the meagre-moral fruits argument.
It's also relevant to lots of other arguments and just a reasoning more generally.
So that's why it's important to keep in mind.
So anyway, that's evidential symmetry.
Importantly, though, something that isn't necessarily symmetric
is how strong the evidential confirmation or disconfirmation is.
So even if, in my imagined scenario, we had super strong evidence for Christianity,
it doesn't necessarily follow that we thereby have super strong evidence against Christianity
in the fact that we don't witness that.
It does follow that we have evidence against Christianity,
but it doesn't necessarily follow that we thereby have super strong evidence against Christianity,
despite the fact that observing the data would be super strong evidence for Christianity.
So this is an asymmetry in the strength of evidence.
Just because evidence or data E would be very strong evidence for H,
it doesn't thereby follow that the negation of E would be very strong evidence against H.
Again, it would be evidence against H.
Don't get me wrong, we just proved that earlier.
But it doesn't necessarily follow that it would be very strong evidence.
So to illustrate this, imagine that our data is that the stars in the night sky
spell out the first 14 verses of the book of John in perfect Koine Greek,
and let T be theism and not T be atheism.
Now, this data would be astronomically improbable under atheism,
but it's also very improbable under theism,
right, like out of all of the ways that God could have designed the universe to be like,
only a very tiny fraction of those have this precise manifestation of divine power.
So let's just get some toy numbers on the table to illustrate the asymmetry of the strength of evidential support.
Again, this is just an illustration. Don't split hairs over the numbers.
I don't care if they're realistic. This is just an illustration.
So again, we've got E, T, and not T.
Let's say that the probability of the evidence given atheism is one in a billion.
So that's that really tiny number here.
And what follows from that, of course, is that the probability that we wouldn't see the evidence,
given atheism, is 99.999, et cetera, percent.
By contrast, the probability of the evidence given theism is still pretty small, right?
It's one in a million, and so the probability is 0.00001.
And what that means, of course, is then that the probability that we wouldn't see the evidence given theism
is 99.999% likely.
So if we observe E, if we observe that evidence, here's the likelihood ratio,
or Bayes factor, indicating the strength of evidential confirmation of theism, right?
It's going to be the probability of the evidence given theism
divided by the probability of the evidence, and that is, of course, one in a million
divided by one in a billion, which amounts to 1,000 to one, or 1,000.
That's the Bayes factor. That's the likelihood ratio.
Now that is very powerful evidence.
By contrast, if we observe not E, that is, if we observe that is not the case that the stars
spell out the first 14 verses of John, the likelihood ratio, or Bayes factor,
indicating the strength of the evidential confirmation of the negation of T, right?
So here we're putting the negation of T on top because we're looking at the strength
of the evidential confirmation of not E for that, so I'd like to put it on the top.
I think it's just easier. The likelihood ratio, or the Bayes factor in that case,
is 1 minus 1 in a billion divided by 1 minus 1 in a million,
and that gives us 1.0000999.
So it's greater than 1. This is evidence for atheism, the fact that we don't observe
the stars spelling out the first 14 verses of John, given these probability assignments.
It does count as evidence for atheism, but obviously it's nowhere near as strong evidence
for atheism as observing that data would be for theism, right?
So E is very powerful evidence for theism, but the negation of E is only very, very weak
evidence for atheism. What this tells us is that even though E would be evidence for T,
while the negation of E would be evidence against T, the strengths of these evidential
confirmations are not guaranteed to be the same.
The reasoning that we just went through also indicates the importance of falsifiability,
or better, disconfirmability, as a theoretical virtue.
If there's nothing in principle that could disconfirm or count as evidence against a hypothesis,
then there's also nothing in principle that could confirm or count as evidence for that hypothesis,
right? This just follows from the previous section that we went through, right?
Remember, we just saw that if E is evidence for H, then not E is evidence against H.
And so if there's nothing that could count as evidence against hypothesis,
then nothing could count as evidence for that hypothesis either.
And this is why, in general, unfalsifiable or better, non-disconfirmable theories are quite bad.
There can't be any evidence for them, at least assuming the positive relevance account of evidence.
That is, construing evidence as probability raising.
The final mistake I'll cover here, and believe me, there are more mistakes besides,
is a mistake I call likelihood ratio rigging.
And I'm going to give an example of this from one of my earlier videos.
It's the most popular video on my channel.
It is 12 hours long, and it is called Over 100 Arguments for God, answered.
This is a purported argument for God's existence.
From what one says, given that God intervenes to produce life on Earth,
there is no chance at all that there will be no life on Earth.
From what this two says, if God does not intervene to produce life on Earth,
the probability that life will occur here is very low.
So look, these are conditional probabilities.
We're establishing a likelihood ratio.
This is saying, given the hypothesis that God intervenes to produce life on Earth,
it is guaranteed that there will be life on Earth.
So the probability of life on Earth, given that God intervenes to produce life on Earth, is one.
But by contrast, the probability that there's life on Earth,
given that God does not intervene to produce life on Earth, is very low.
So look, we have a likelihood ratio that very strongly favors the hypothesis
that God intervenes to produce life on Earth.
You could find the same likelihood ratio,
given that Zeus intervenes to produce life on Earth.
The next premise, it is just as reasonable to bet
that God will intervene to produce life on Earth as that he won't.
So there's a 50-50% chance of that.
And so that's talking about the ratio of the priors.
That premise is certainly false.
So yeah, the basic idea behind likelihood ratio rigging
is that you can always artificially bake something into your hypothesis
that entails or strongly probabilifies the data.
But when you do this, you're very often purchasing predictive power
at the cost of correspondingly lowering the prior probability of your hypothesis.
You're decreasing the ratio of the priors by the same proportion
in which you increase the likelihood ratio.
And so there's no overall effect on the posterior ratio.
And hence no overall effect on the posterior probability of your hypothesis.
So yes, I forgot to show this slide when I was saying that.
Take the example of playing poker.
I'm the dealer, and of course I give myself five royal flushes in the row.
Here is me scheming my plan.
You accuse me of cheating.
I say the shuffling is fair.
You go on to explain how the probability of five royal flushes,
given the cheating hypothesis, is much, much, much greater
than the probability of five royal flushes given fair shuffling.
And that the probability of me cheating is only mildly more improbable
than the probability of fair shuffling.
And hence we should conclude, assuming the only relevant evidence here
is the five flushes, that I probably cheated.
And the reason why you think, you know, probability of cheating
is only mildly more improbable than the probability of fair shuffling.
Maybe you've been with me in the past.
On occasion I cheat on card games.
This is not actually true.
Again, this is just part of the imaginative situation.
I don't cheat in card games. I'm fun, I swear.
But part of the imaginative situation, you know, you've been with me for a few years.
You know, occasionally I cheat.
You know that generally I don't.
Generally I am a person of integrity.
But it's certainly not totally out of character for me to do it.
So although it's improbable, it's not like vastly, vastly improbable.
At the very least, the contrast between these two probabilities
is nowhere near as large as the contrast between these two.
So you've got a fairly convincing argument here
for the fact that I cheated in light of this evidence.
But alas, I'm not convinced.
Here's what I'm going to say.
Let's make my hypothesis that the shuffling was fair
and that this fair shuffling gave me five royal flushes.
Now I predict the data just as well.
Indeed, better than the cheating hypothesis.
Right, my hypothesis just entails the data.
By contrast, under the cheating hypothesis,
okay, one way I might have cheated is giving myself five royal flushes
and that's not super-duper improbable under that hypothesis.
But also I might have cheated in other ways.
I could have given myself only like four royal flushes
or maybe three or something like that.
Or maybe hands that were slightly worse than royal flushes.
So actually my new hypothesis that the shuffling was fair
and that this fair shuffling gave me five royal flushes,
I actually predict the data better than the cheating hypothesis.
So you don't actually have evidence for the cheating hypothesis
and against my newly modified fair shuffling hypothesis.
So take that.
Now, I hope at least intuitively,
you see that my reasoning here is just absurd.
Yes, it's true that you no longer have evidence here.
This is true, right?
Keep this firmly in mind.
It is true here that you no longer have evidence
against my newly modified shuffling hypothesis
since the data is actually more expected
on my newly modified fair shuffling hypothesis
as it is on the cheating hypothesis.
But the problem is that I've only purchased this increase
in predictive power at the price of drastically decreasing
the prior probability of my hypothesis.
In fact, the factor by which my prior probability decreased
is the same as the factor by which the original fairness hypothesis
was disconfirmed by the evidence.
Since the probability that the shuffling was fair
and that this fair shuffling gave me five royal flushes
is just equal to the probability that the shuffling
gave me five royal flushes, conditional on the claim
that the shuffling was fair, multiplied by the probability
that the shuffling was fair, right?
This is an instance of the ratio formula
that we covered earlier on in the video.
So when we look at the posterior ratio equals
the likelihood ratio times the priors,
what I've done in modifying the hypothesis
from the fair shuffling to the hypothesis
that the shuffling was fair and that that shuffling
gave me five royal flushes.
What I've done is I've merely shifted the improbability
from the numerator of the second term here, right?
The improbability that I get five royal flushes
given that the shuffling was fair.
I've shifted the improbability from this term here,
right, the likelihood of the fair shuffling hypothesis.
I've shifted the improbability from there to the prior, right?
Now my prior that fair shuffling gave me five royal flushes
is just correspondingly lowered improbability.
You can actually mathematically show in the example that I gave
that the ratio of the posteriors,
when the hypotheses in question were fair shuffling and cheating,
that's exactly the same as the ratio of the posteriors
when the hypotheses are my newly modified hypothesis
that fair shuffling gives me five royal flushes
and the cheating hypothesis.
And so by modifying my hypothesis to purchase that predictive power
and by correspondingly diminishing my prior probability,
I actually didn't do anything to boost the epistemic credentials
of my hypothesis.
It's in the same sucky situation as before.
The posterior probability is going to be exactly the same.
And this point quite easily generalizes.
Again, you can always bake into your hypothesis
something which renders the data very probable
or even entails the data.
But be careful when you do that,
since that may very well just correspondingly lower
the prior probability of your hypothesis.
If so, then you haven't actually made any probabilistic headway.
The posterior probability of H,
that is, the probability of H given E
is going to be the same before and after
modifying your hypothesis in this way.
Again, this isn't always the case
when someone's hypothesis entails some data.
I mean, sometimes it's a very natural consequence of a view
and you know, someone just isn't like artificially
baking into some core hypothesis
and auxiliary which entails the data.
So this mistake isn't always present
whenever someone's hypothesis entails the data.
Okay, just keep that in mind.
This is again just something that you need to be very wary of.
So, at last, we've finally reached the end of the video.
It's time for me to break down into yours
and give an inspirational speech.
No, Bayes' theorem is really important.
It offers us a precise, mathematical way
to rationally update our credences
and beliefs in light of new evidence.
My goal in this video has been to give you a solid foundation
not only for understanding Bayes' theorem
but also for applying it
and seeing its profound significance.
Before leaving, though, I want to put some resources on your radar.
Two very important books in this area
that you should definitely check out if you're interested
are by Michael Tidalbaum.
It's called The Fundamentals of Bayesian Epistemology.
It's like a two-volume series or whatever.
The first one is Introducing Credences.
The second one is Arguments, Challenges, and Alternatives.
I've drawn heavily on these in making this video.
Two more books that I've drawn on in making this video
are David Papineau's Philosophical Devices,
Proofs, Probabilities, Possibilities, and Sets,
as well as Eric Steinhardt's More Precisely,
the Math You Need to Do Philosophy.
And two other relevant books are Ian Hackings'
Introduction to Probability and Inductive Logic,
as well as DH Mel or his Probability,
a Philosophical Introduction with Rutledge.
As for websites that you can go to
to learn about these things more,
you can check out various SEP entries,
like the Stanford Cyclopedia of Philosophy.
Alan Hayek has one on interpretations of probability.
There's an SEP entry on Bayes Theorem itself,
although that one's going to be far more technical
than my video here, so beware.
Anyway, I always recommend the Stanford Cyclopedia of Philosophy.
And you can look at the references therein as well
to follow things and pursue things in more detail.
As for videos, you can check out my newly created
Bayesianism and Probability playlist.
It only has one video of mine,
but lots of other videos that will help you through these topics.
And if I make videos in the future on Probability
or Bayesianism or things like that,
they will go in that playlist.
And of course, more generally, right?
Check out my playlist.
I've got a playlist on the Kalam cosmological argument,
teleological arguments, ontological arguments,
contingency arguments, discussions with philosophers,
and everything in between.
So definitely check out my playlists.
Oh, I even put a slide for them.
So yeah, that's how much I cared about this.
Now, if you've made it this far in the video,
I extend my sincere congratulations.
This video was not easy to get through,
but it showed commitment, intellectual curiosity,
and intellectual perseverance.
So I heartily praise you for that.
And I hope my literal dozens of hours of working on this
have helped you gain a greater understanding of reality.
If you enjoyed it, please hit that like button,
subscribe, turn on that little bell for notifications,
consider supporting me on Patreon
or tipping me through a one-time donation.
Right? If you tip waiters who get paid for what they do,
consider tipping me who made this video for free
who spent way longer than waiters do in helping you
and who's helping you try to get to the fundamental nature
of reality and not just helping you get some more breadsticks.
So you can do that one-time donation via PayPal.
Link is in the description. Any amount helps.
Truly any amount.
And as always, what better way to end this there than,
I'm Josh Mid. This is The Majesty of Reason.
And peace out.
