Well, the answer is obvious enough, it's two-thirds, right?
If it was right to think beforehand that the conditional probability of Johnny going
on the assumption that Jane goes is two-thirds,
and if now it turns out that Jane really is going,
well then it must be right to think that the new unconditional probability that Johnny goes
has increased to two-thirds.
And again, think of it in Venn diagram terms.
You now know you are inside the area of the Venn diagram for Jane's going,
and you have already decided that the proportion of this area that covers Johnny's going as well is two-thirds.
And so it must now be true that the probability of Johnny going is two-thirds
after you learn that Jane is in fact going.
You basically narrow your focus to the space occupied by Jane going,
and your new probability that Johnny goes to the party is just equal to your old probability that Johnny goes
given that Jane goes, which is just the proportion of this whole circle
taken up by the crosshatched area.
Again, updating your credences in this kind of way is called conditionalization,
and we can offer a slightly more formal version of the principle as follows.
If your old conditional probability of P given Q is equal to X,
and you come to know that Q is true,
well then the new probability that P is true is equal to X.
Notice here that Q needs to be understood as representing everything you come to know.
The principle doesn't work if Q is only part of your new knowledge.
To illustrate this, suppose that in the above example you learn not only that Jane is going to the party,
but also that she will be accompanied by Jill.
And suppose that you would always thought that there was almost no chance that Johnny would go
if both Jane and Jill did.
You had in other words a very low old conditional probability that Johnny goes
conditional on the assumption that both Jane and Jill go.
Even though your old probability that Johnny goes conditional on Jane going was two-thirds.
You can imagine that this little red circle represents the probability that Jill goes,
and as you can see, the amount of overlap between Jill's circle,
Jane's circle, and Johnny's circle is a super-duper tiny fraction of the entire Jill circle.
So here, the probability that Johnny goes,
given that both Jane and Jill go, is super-duper tiny.
And so while it is still true that you have learned that Jane will go,
it's no longer a good idea to attach a two-thirds probability to Johnny going,
simply on the grounds that your old probability that Johnny goes conditional on Jane going is two-thirds.
And the reason that's not a good idea is precisely because you have learned more than that Jane will go to the party.
You now know not just that you are inside Jane's Venn diagram,
you now know not just that you are inside of Jane's circle, so to speak,
but more specifically that you are inside of that bit of Jane's circle,
which is also inside of Jill's circle, which is that bit of the probability space where Jill also goes to the party.
And the proportion of that area where Johnny goes in addition to Jane and Jill going is very, very small indeed.
So this just illustrates that when you're updating by conditionalization,
you need to update on everything you come to know within the given time period.
Finally, here's a still more formal version of the conditionalization principle,
still though I think it's relatively understandable.
So the principle says that for any time t and later time t star,
if Proposition E represents everything an agent learns between t and t star,
and if the agent's probability of E at t is greater than zero,
then for any Proposition H, the agent's probability of H at t star equals the agent's probability of H given E at t,
where the p subscript t and p subscript t star are basically the agent's probability distributions at the two times.
A probability distribution is basically just a way of assigning probabilities to different propositions within our propositional language.
So yeah, these are basically just assignments of probabilities to different propositions indexed to a given time,
and of course, relative to a particular agent.
So we're talking about an individual agent's probabilities in certain propositions indexed to particular times,
and how we represent that indexing is by subscripting the time, t star or t.
So as you can see from this principle, conditionalization captures the idea that an agent's probability,
or rational credence of H at t star after learning E, equals their earlier probability at t, right?
Equals their earlier probability in H had the agent merely supposed that E is true.
So their later probability in H after learning E is equal to their earlier probability of H given E,
and when an agent updates their credence distribution by applying this principle of conditionalization to some learned Proposition E
and some hypothesis H, we say again that the agent conditionalizes on E.
That's what Bayesians are saying when they talk about conditionalizing on some evidence.
Keep in mind, again, and I emphasize this because it is very often overlooked, this is a rational constraint.
An agent's actual credences may not obey the conditionalization principle,
like an agent's actual credences might not satisfy this.
The point is that if the agent is being rational, then their credences obey this.
Now, the conditionalization principle is especially relevant to Bayes' theorem,
since Bayes' theorem offers us a way to calculate the probability of H given E,
and in particular to calculate an individual agent's probability of H given E at time t.
So the conditionalization principle, together with Bayes' theorem, provides for us the rational way to update our beliefs and credences
as well as to confirm or disconfirm our theories and hypotheses in light of new evidence, in light of new data.
This is the ultimate significance of Bayes' theorem and these various normative constraints on rational credence.
This is the rational way to update your beliefs, to update your credences, to confirm or disconfirm theories, hypotheses, etc.
And that is the broader significance of all this.
Now, that would be a perfect bridge into Bayes' theorem itself,
but there's one final distinction within Bayes' epistemology that you should be aware of before we get into the almighty theorem,
and that's the distinction between subjective and objective Bayesianism.
So there are different ways to articulate the distinction between subjective and objective Bayesianism,
but a central way to delineate them, and this is sometimes called the normative distinction between subjective and objective Bayesianism,
a central way to delineate them is in what rational constraints they put on credence.
The subjective Bayesian basically stops at the five core normative rules that we've just been over,
together, of course, with whatever is entailed by them, such as Bayes' theorem itself.
Objective Bayesians, by contrast, put further rational constraints on credence.
As a result, for the subjective Bayesians, there is no one uniquely rational response to a set of evidence.
It all depends on your initial credences in the propositions involved, or your prior probabilities in the various hypotheses.
Sometimes called your priors.
As long as your priors satisfy the axioms of probability, as long as they meet those relatively minimal coherence conditions
and, like, being within zero and one and, you know, those sorts of things,
and, of course, as long as you update by conditionalization and obey the ratio formula,
you're going to count as perfectly rational for the subjective Bayesian.
The main objection to subjective Bayesianism is that, of course, it's too subjective.
For instance, you might think that, in light of all the evidence we currently have,
it's simply not rational to think that the Earth rests on the back of a giant pink turtle.
That just isn't a possible justified response to our current evidence, you might think.
But on the subjective Bayesian view, it is a possible justified response to our evidence, right?
All you have to do is assign a super-duper high prior probability to that sort of turtle cosmology,
high enough that it will sufficiently outweigh even the mountains of evidence that we have against this turtle cosmology.
In essence, then, subjective Bayseans think that a rational belief is just a belief
that doesn't violate any of the five core normative constraints on rational credence.
And it's not that difficult to assign a super-duper high prior probability
in what otherwise seems to other people to be a very implausible theory,
while nevertheless respecting all of those core normative rules.
Other philosophers, however, think that rationality is more demanding,
and these people fall into the objective Bayesian camp.
Objective Bayseans think there are substantial additional constraints on rational credences
beyond the comagore-vaxims the ratio formula and the conditionalization principle.
One of the main motivations for this, again, is to avoid excessive subjectivity
and to put stricter constraints especially on one's prior probabilities in various hypotheses.
One of those additional constraints might be the principle of indifference, for instance,
which basically says that if there are no reasons favoring A over B or B over A,
then the probability of A is the same as the probability of B.
There are other candidates for additional constraints on rational credence,
and there's a lively debate about the principle of indifference.
But you get the point. Objective Bayseans add further rational constraints.
I just wanted to give you a sense of this distinction between subjective and objective Bayseans
before moving on, since it's important for understanding the broader framework of Bayesian epistemology.
Alright, we're finally onto the theorem itself! Woohoo!
Yes, the most famous consequence of the ratio formula and comagore-vaxims is Bayes' theorem.
For any propositions H and E, the probability of H given E is equal to the probability of E given H
times the probability of H divided by the probability of E.
We can label H and E as A and B or P and Q or whatever we want.
For present purposes, I did it in terms of H and E,
because Bayes' theorem is typically used to talk about the confirmation or disconfirmation
of various hypotheses in light of evidence or data.
So sometimes you'll also see D used, but E is more often used.
Also, part of Bayes' theorem is the stipulation that the probability of E is greater than zero,
because, of course, you can't divide by zero.
So the first thing to say about Bayes' theorem is just a reminder that it is indeed a theorem.
It can be proven straightforwardly from the comagore-vaxims together with the ratio formula.
We'll actually go through a relatively simple proof of Bayes' theorem in a second.
But before that, I want to briefly talk through the terms used in this formula.
So again, strictly speaking, H and E are just going to be propositions.
But when we're talking about Bayesian epistemology,
we're really concerned with hypotheses and evidence.
So going forward, we're going to be treating E as some piece of data or evidence.
We're going to be treating H as some hypothesis.
The probability of H given E is the posterior probability of H.
It's the probability of H posterior to considering E.
The probability of E given H is what's often called the likelihood,
or sometimes the likelihood of H.
I know that's unfortunately named because likelihood sounds like probability,
so it sounds like you're talking about the probability of H, but no.
But no, when we're talking about likelihoods,
we're talking about the probability of pieces of evidence given hypotheses.
So you can think of it as the probability or the likelihood
that E would be true conditional upon the truth of H.
The probability of H is the prior probability of H.
That is the probability we assign to H prior to considering the piece of evidence E.
And then the probability of E is just, of course, the probability of the evidence itself.
It's a probability that the evidence itself would be true at all.
It's a probability that the evidence itself would be true or would obtain at all.
Now, regarding the prior probability of H, don't confuse prior probabilities with intrinsic probabilities.
The prior probability of H means the probability that H is true
before considering some specific item of evidence,
but usually after encountering other items of evidence.
By contrast, the intrinsic probability of H is the probability that H is true
before considering any evidence whatsoever.
It's determined solely by the content of H itself,
without taking into account any evidence for or against H.
Paul Draper offers a plausible theory of intrinsic probabilities,
according to which they depend on the modesty and coherence of a hypothesis.
A hypothesis is modest to the extent that it does not commit us to saying very much about the world.
Modesty is thus a measure of how much the hypothesis asserts.
The more a hypothesis claims, the more ways there are in principle for it to be false,
and, as a result, the less likely it is to be true, prior to looking at any evidence, that is.
A hypothesis is coherent to the extent that its conjuncts or elements cohere well with each other.
They fit together well, or they raise, or at least don't lower each other's probabilities.
So think of coherence as a measure of how well the various parts of a hypothesis fit together.
If the different parts count against each other, if conditional on one of the parts being true,
the other parts are unexpected, or unlikely, well then the hypothesis is less coherent,
and thus less likely to be true.
So yeah, just keep these notions distinct in your mind.
Intrinsic probability refers to the probability that hypothesis,
independent of any and all evidence we have for or against it.
Whereas prior probability may very well take into account various pieces of evidence
that you've conditioned on in the past.
Prior probability, again, is simply the probability of the hypothesis
prior to looking at some particular item of evidence under consideration.
So we've got the theorem here, but we can ask how do we know that Bayes' theorem is true?
And the answer is because we can prove it.
In fact, we can prove it in several ways,
and I'm just going to give what I think is the most intuitive and the easiest and the simplest way to see why it's true.
So recall the ratio formula.
For any propositions p and q, the probability of p given q
equals the probability of p and q divided by the probability of q.
Notice that this holds for any p and q,
and so we can get one instance of the ratio formula by first plugging in h for p and e for q.
And then we can get a second instance of the ratio formula by plugging in e for p and then h for q.
So when we plug in h for p and e for q, we get one here.
The probability of h given e is equal to the probability of h and e divided by the probability of e.
So look, we're plugging in h for p and we're plugging in e for q.
And as I just said, we can also plug in e for p and h for q.
So that's just what 2 is. This follows from the ratio formula.
It's the same thing as the ratio formula, but you're just plugging in e for p and then h for q.
So we've got the first two steps of our proof.
Let's then do some rearranging.
The probability of h given e times the probability of e equals the probability of h and e.
This follows from 1.
