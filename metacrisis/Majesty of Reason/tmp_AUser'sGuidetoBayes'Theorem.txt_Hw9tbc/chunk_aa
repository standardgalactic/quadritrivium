I've got a bit of trivia for you guys.
A 5th year old woman, no symptoms, participates in routine screening for breast cancer.
She tests positive, is alarmed, and wants to know from you whether she has breast cancer
for certain or what her chances are.
Apart from the screening results, you know nothing else about this woman.
The prevalence of cancer among women her age is 1% and the test is 90% accurate.
90% of the time it gives the correct diagnosis as to whether or not someone has the disease.
So the question is, in light of her positive test result, what are the woman's chances of having cancer?
Is it 80%, 90%, 22%, 8% or 1%?
You can think about that for a second, come up with your answer, I'll wait.
Now, you might have said B and if so, I'm sorry to say but that's wrong.
It's false, no way, not this time, we created it.
Not this time, no, not this time.
The answer is actually D, 8%.
And after watching this video, you'll know exactly why that is.
Welcome everyone to The Majesty of Reason, I'm Joe Schmid and today we're talking about Bayes Theorem.
This video is actually requested by a patron, Jeremy, so thank you Jeremy so much for your support.
If you guys see value in the work that I do, consider becoming a patron.
There are so many perks included such as exclusive videos, notes, papers, books, and depending on your level, personal meetings with me and video requests.
All patrons, by the way, have access to the 37 page script that I made for this video.
But anyway, back to Bayes Theorem.
So named after Reverend Thomas Bayes, an 18th century English statistician, philosopher and Presbyterian minister,
Bayes Theorem is a powerful tool for updating our beliefs in light of new evidence.
Its applications range from artificial intelligence and machine learning to medicine and science to philosophy and well beyond.
My goal in this video is to give you a foundational understanding of what the theorem says, why it's true, how to use it, and how not to use it.
Also, don't worry if this all sounds intimidating or beyond your grasp, I'm going to try to make it as accessible and understandable as possible.
So even if you're not a math person or even if you're somewhat new to philosophy or you haven't studied probability theory or statistics, this video shouldn't be too daunting.
It may stretch you at times, but I'm here to help as much as I can.
Alright, so all of y'all know that I love my outlines.
I'll begin by briefly introducing Bayes Theorem itself, after which I'll give some background on belief and credence and probability and things like that.
Then we'll look at the theorem itself, and in particular we're going to be looking at two different forms of the theorem as well as how it interacts with the Bayesian notion of evidence or evidential confirmation.
We'll also be considering some proofs of Bayes Theorem, why we know that this is true, how we know that it's true, and something very exciting, or at least I find very exciting, I find it titillating, is visualizing Bayes Theorem.
So we're going to walk through some videos about how to visualize Bayes Theorem and how to think about probability visually.
It's super-duper helpful, trust me.
We'll then look at some pretty common mistakes to avoid when using Bayes Theorem and assessing Bayesian arguments, and finally I'll end with pointing you guys to some helpful resources.
And note that all throughout the video we'll be applying the theorem to tangible problems.
Alright, but the big question in this first section is, what even is Bayes Theorem?
Well, Bayes Theorem is many things.
The simplest answer is that Bayes Theorem is a mathematical equation, expressible in several different forms, that allows us to calculate conditional probabilities.
Where a conditional probability is the probability that one claim is true, given that another claim is true.
It also follows strictly from the axioms of probability theory, I'm going to explain how that is later, but that's all that you need to know right now.
That's why it's called a theorem.
A theorem is basically what you can prove to be the case within a given axiomatic system.
But Bayes Theorem is far more than a mere equation that follows from the axioms of probability theory.
It's also a powerful tool for updating our beliefs in light of new evidence.
It's a powerful tool for assessing the probability of various hypotheses in light of data.
And that relates to confirming and disconfirming theories, getting evidence for and against theories.
It's also a formal constraint on rational credences.
That is, it's a constraint that your confidence in various statements needs to meet in order to be rational.
Don't worry, I'm going to be talking about what credences are soon.
Right now, I'm just giving you a bird's eye view of what Bayes Theorem is.
So, yeah, if you're interested in being rational, which I presume you are, you should be interested in Bayes Theorem,
since the theorem expresses how your credences or your confidence levels or your degrees of confidence should rationally change in order to account for new evidence.
Now, again, this is a very coarse-grained articulation of Bayes Theorem,
and I'm not even putting the theorem on the screen yet because we don't have the requisite background to be able to understand it yet.
For this section, again, I'm just trying to give you a bird's eye view.
As we progress through the video, we'll gain a deeper understanding of the theorem, its significance, and its applications.
Before getting to the theorem itself, however, we need to cover some essential background on belief, credence, probability, and Bayes' epistemology.
Don't worry, I'm going to try my best to make this as accessible as possible.
Let's turn to belief and credence.
So, most of the epistemology concerns propositional attitudes.
A propositional attitude is an attitude that an agent adopts toward a proposition.
While a lot of ink has been spilled over the nature of propositions,
here we'll just treat them as the meanings of declarative sentences.
They're basically what sentences express.
They're also capable of having truth values, that is, they're capable of being true or false.
So, for example, the sentence, nuclear fusion is a viable energy source, that sentence on the screen there,
expresses the proposition that nuclear fusion is a viable energy source.
If I believe that fusion is viable, this belief is a propositional attitude.
It's an attitude that I take toward the proposition that fusion is viable.
Epistemology focuses in particular on propositional attitudes that attempt to represent what the world is like.
Belief is in some sense a purely representational attitude.
When we attribute a belief to someone, we're simply trying to describe how they take the world to be, or how they represent the world as being.
We're saying that they're taking a stance on the relevant proposition, that they're affirming its truth,
that they're taking it to be true, or representing the world as being the way the proposition says it is.
That's what we're saying when we say that someone believes a proposition.
And belief, of course, is not the only purely representational attitude that an agent can have toward a proposition.
An agent might also be certain that a proposition is true, or disbelieve a particular proposition,
which is, in other words, believing that the proposition is false.
Philosophers often discuss the class of doxastic attitudes into which belief, disbelief, and certainty all fall.
Doxastic attitude, by the way, is an umbrella term for propositional attitudes that are belief-like.
Doxa is Greek for belief, in fact.
And they're belief-like in the sense of being purely representational attitudes.
And so doxastic attitudes, that category includes things like belief, of course,
but also disbelief, certainty, doubt, suspension of belief, or suspension of judgment, credence, or confidence level, and others.
Now, basing epistemology in particular focuses primarily on a type of doxastic attitude known variously as
degrees of belief, or degrees of confidence, or levels of confidence, or credences.
And that's a term that we're going to be going with going forward.
Your credence in a proposition is basically how confident you are in that proposition.
It's your degree of confidence in it, and it's represented using a quantitative degree to scale from 0% to 100%, or from 0 to 1.
0 representing 0% confidence in something, and then 1 representing 100% confidence, and then, of course, everything in between.
0.75 would be 75% confidence in a proposition.
So, for instance, you might be 100% confident, that is absolutely certain, that 1 plus 1 equals 2.
You might be 0% confident that triangles have four sides, that is, you're absolutely certain that that's false.
You might be 50% confident that the number of stars in the universe is even.
You might be 80% confident that it will rain tomorrow, and so on down the list of your credences.
All of these numerical descriptions represent your credences in the various propositions just mentioned.
Flussers generally like to use the 0 to 1 scale, so your credence in those propositions would respectively be 1, 0, 0.5, and 0.8.
Moving forward, I'm going to be mostly using the 0 to 1 scale, though occasionally, context will make clear when I'm using the 0% to 100% scale.
So, at this juncture, I think it's useful to ask, firstly, what's the difference between beliefs and credences?
And also, why should we countenance credence in addition to beliefs?
That is, why should we even believe that there are such things as credences?
Why not instead just simply represent our doxastic lives as merely containing beliefs?
Why do we have to postulate credences in addition?
Or why do we have to describe ourselves as having credences in addition?
Well, on that first question, beliefs are binary, right?
Given any particular proposition and any agent, the agent either believes it or doesn't.
End of story.
But credences aren't binary like that, right?
They're quantitative and graded in a way that beliefs aren't.
Your belief is simply how you take the world to be, whereas a credence represents your degree of confidence in taking the world to be that way.
It's basically about the certainty or confidence with which you hold your beliefs.
But on that second question, as to why we should even countenance both beliefs and credences,
the first thing to note is that, at least for many propositions, it just seems introspectively obvious that we have credences.
We simply find ourselves with different degrees of confidence in different propositions,
and these aren't binary but are rather graded, right?
For some things, we're very confident, others less so, and still others were not at all confident in them.
So that's the first point.
But secondly, if we only describe agents' doxastic attitudes with beliefs,
we'd be unable to account for many facts about agents' doxastic lives.
To account for those facts, we need to be able to attribute quantitative attitudes to agents.
That is, we need to countenance credences.
So this is the point here about explanatory power.
We basically need to explain various facts about agents' doxastic lives,
and to do that, we need to have credences in our repertoire.
So suppose my physicist friend believes that nuclear fusion is a viable energy source.
She also believes that her car will stop when she presses the brake pedal.
She's willing to bet her life on the latter belief,
and in fact she does so multiple times daily during her commute to and from work.
But she's not willing to bet her life on the form of belief, right?
She's not willing to bet her life on nuclear fusion being a viable energy source.
This difference in the decisions she's willing to make
seems like it should be traceable to a difference between her doxastic attitudes
toward the proposition that fusion is viable, on the one hand,
and the proposition that pressing her brake pedal will stop her car on the other.
Yet if we don't countenance credences, we'll be unable to make out any difference
between my friend's doxastic attitudes toward those propositions, right?
Once we note that my friend believes both propositions, that's all we'd be able to say.
Unless, of course, we avail ourselves of credences, right?
If we don't avail ourselves of credences, then all we can say is that, yeah,
she believes one, she believes the other,
but we can't explain her differences in risk-involving decisions.
In one case, she bets her life on the proposition, and in another case, she doesn't.
Now suppose my physicist friend reads about some new research into nuclear energy.
The research reveals new difficulties with designing more efficient nuclear power plants,
which will make fusion power more challenging going forward.
After learning of this research, she still believes fusion is a viable energy source.
She has mountains of evidence, we can suppose, for thinking that that is the case.
And this recent research is only a small piece of evidence being added to that monstrous pile of evidence.
So she still believes fusion is a viable energy source.
Nevertheless, it seems that this new evidence should cause some change in her attitude toward the proposition that fusion is viable.
Yet, if we don't count in its credences, we'd lack the tools to ascribe any such change, right?
My friend believed the proposition beforehand, and she still believes it now.
She still has belief both before and after learning this evidence against her belief.
So if we only availed ourselves of belief, we'd be unable to explain the change in her attitude toward the proposition that fusion is viable.
And what that means is that we need beliefs and credences in order to describe her doxastic life and, more generally, agents' doxastic lives at large.
So we've been talking about credences and ascribing credences to agents, but of course, all of this is, at best, a kind of idealization.
In truth, there isn't usually a precise answer to the question of exactly how much I'm confident in the truth of P for every single proposition P.
There are plenty of propositions that I've never even thought of, and even among those that I have thought of, many of those are such that I've got a pretty fuzzy attitude towards them.
Nor is it very realistic to suppose that I can attach numbers to all the things that I care about.
Still, we can go along with the idealization in order to simplify our discussion going forward.
But credences, of course, aren't only a useful simplification, since for at least a good deal of propositions, credence ascriptions at least roughly capture our decreed levels of confidence toward those propositions.
And as we saw when covering some of the reasons to believe in credences in the first place, talk of credences seems both justified and quite important for capturing elements of our doxastic lives.
So yes, there's some idealization and simplification involved here, but there's also utility and approximate accuracy.
We are really picking out something in our doxastic lives.
What we're picking out is going to be a little bit more messy and a little bit more fuzzy than our very precise mathematical models, but they're still approximately accurately describing the phenomenon, at least in many cases.
Of course, credence ascriptions do have some disadvantages, for instance, numerical representations may provide more specific information than is actually present in the situation being represented.
Similarly, I might be more confident that the Democrats will lose the next election than I am that they will win without there being a fact of the matter about exactly how much more confident I am.
Representing my attitudes by assigning precise credence values here to the proposition that the Democrats will lose and the proposition that they'll win attributes to me a confidence gap of a particular size.
And that may be over attributing things to my doxastic life.
And also assigning numerical credences over a set of propositions creates a complete ranking among them, which makes it impossible to retain any incommensurabilities among the propositions involved.
Incommensurabilities basically means like there's no way to compare the relevant beliefs in terms of some metric.
And yet you might worry that confidence in commensurability, that is, for at least some propositions, your confidence in one may simply be not even commensurable with your confidence in another.
There may be no common metric by which you could compare them.
You might think that that's a common and rational feature in agents' doxastic lives.
So again, there's idealization, there's simplification involved here, but at least for many propositions, it's plausible that agents' doxastic attitudes can be usefully and approximately accurately represented as credences or numerical degrees of belief.
Now, credences are closely related to the notion of probability in Bayesian reasoning.
Given a proposition p, we can speak of the probability of p.
For instance, the probability that the next card from this pack will be an ace, the probability that this radium atom will decay before the year 2100, the probability that it will rain tomorrow, and so on.
We can write p of p, or pr of p, to represent the probability of p.
Sometimes you'll also see cr of p, and that's just representing an agent's credence in p, so keep these notational variants in mind.
But the major question of this section is, what is probability?
What are we even talking about when we're talking about the probability of one statement or the probability of h given e, or those sorts of things?
Well, usually, probability is introduced first through a distinction, and that distinction is between two broad kinds of probability.
Objective probability and subjective probability.
Objective probabilities are mind-independent features of reality.
There are features out there, as it were.
For instance, there might be an objective probability of 50% that an indeterministic atom will decay within the next hour.
This probability doesn't depend on us in any way.
For instance, it's not a measure of our credence or our degrees of belief.
Instead, it's a matter of some objective feature of the atom itself, perhaps some propensity or tenancy inherent to it,
or perhaps it's a statistical fact about the frequency of decays among atoms of its kind, or perhaps it's something else entirely.
What matters here is that this probability is objective or mind-independent.
It's a feature of the world out there.
It doesn't depend on our attitudes or subjective mental states, etc.
By contrast, subjective probabilities are, you guessed it, mind-dependent.
And one dominant way of understanding subjective probabilities simply identifies them with credences or degrees of belief.
So an agent's subjective probability in a proposition just is their credence in that proposition.
Some philosophers, though, think this distinction between objective and subjective probability confuses more than it illuminates.
Perhaps there are really a panoply of different notions of probability, some of which are entirely mind-independent,
