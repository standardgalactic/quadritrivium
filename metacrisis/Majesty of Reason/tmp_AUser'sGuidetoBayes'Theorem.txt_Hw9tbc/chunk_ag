Look, all you have to do to get from 1 to 3 is just multiply each side by the probability of e.
When you multiply this side by the probability of e, the probability of e's cancel out
and then you're just left with the probability of h given e times the probability of e equals the probability of h and e.
You're moving the probability of e from the denominator over here to the left side of the equation.
And we can also do some more rearranging.
We're doing basically the exact same thing to premise 2, right?
We're multiplying both sides of the equation in premise 2 by the probability of h.
And when you do that, you get the probability of e given h times the probability of h
and so we get our next two steps of the proof.
But notice that the probability of h and e is equal to the probability of e and h.
Now, I hope that this step is just intuitively obvious on its face, right?
I mean, obviously, like the probability that it rains and it's Tuesday is just the same as the probability that it's Tuesday and it rains.
Like, this step should be obvious.
But also note that this step follows from the fact that h and e is logically equivalent to e and h.
Together with the consequence of Komagorov's axiom known as equivalents, which we went over earlier in the video.
For any propositions p and q, if p logically entails q and q logically entails p, then the probability of p equals the probability of q.
So given that h and e is logically equivalent to e and h, and what that means is that they each logically entail the other
and that just follows from the truth conditions for this logical connective for conjunction.
And so given that they're logically equivalent and so each logically entails the other,
it follows from equivalents that their probabilities have to be the same as well.
And then from 3 through 5, it follows that the probability of h given e times the probability of e equals the probability of e given h times the probability of h.
Right? Notice what 5 is saying. It's saying that the probability of h and e, which shows up here, is equal to the probability of e and h, which shows up here.
Right? So given that these two are identical with each other and that the top one here is identical to this and the bottom one here is identical to this,
well then these two have to be identical with one another as well.
Right? We know that these two are identical from 5 and so it follows that these two are identical.
That is what 6 is saying. It's saying that the probability of h given e times the probability of e equals the probability of e given h times the probability of h.
That's just what 6 says. And finally, it's then just a straightforward step from there to get to Bayes' theorem.
Look, all you have to do is divide both sides by the probability of e.
When you divide this side by the probability of e, the probability of e's cancel out in the numerator and the denominator,
and then on the right-hand side you're left with the probability of e in the denominator.
So we get Bayes' theorem. The probability of h given e equals the probability of e given h times the probability of h divided by the probability of e.
Now I know it sounds kind of complicated going through that here, but if you think about it, it's actually quite simple, right?
We just have two instances of the ratio formula and we have an instance of equivalence.
The rest just logically follows and the rearrangements aren't all that complex.
Usually we're just dividing both sides by term or multiplying both sides by term.
So anyway, I hope that this proof was illuminating for you and helps you see why Bayes' theorem follows from Kolmogorov's axioms in the ratio formula.
Now I do want to show a 3 blue 1 brown video to illustrate the simplicity of this kind of proof.
So let's get on to that next.
If your goal is simply to understand why it's true from a mathematical standpoint,
there's actually a very quick way to see it based on breaking down how the word AND works in probability.
Let's say there are two events, a and b. What's the probability that both of them happen?
On the one hand, you could start by thinking of the probability of a, the proportion of all possibilities where a is true,
then multiply it by the proportion of those events where b is also true,
which is known as the probability of b given a.
But it's strange for the formula to look asymmetric in a and b.
Presumably we should also be able to think of it as the proportion of cases where b is true, among all possibilities,
times the proportion of those where a is also true, the probability of a given b.
These are both the same, and the fact that they're both the same
gives us a way to express p of a given b in terms of p of b given a, or the other way around.
And also I just want to pause at this point and say that this right here is basically just the ratio formula.
Remember the ratio formula, it said that the probability of a given b is equal to the probability of a and b
divided by the probability of b.
And that's just a rearrangement of this equation here.
All you have to do is divide both sides by the probability of b.
So if you take this probability b and put it over here in the denominator, then you get the ratio formula.
So this is just an alternative way of expressing the ratio formula.
That's what I'm trying to emphasize here.
So when one of these conditions is easier to put numbers to than the other,
say when it's easier to think about the probability of seeing some evidence given a hypothesis rather than the other way around,
this simple identity becomes a useful tool.
And you know, while we're here, it's worth highlighting a common misconception that the probability of a and b
is p of a times p of b.
For example, if you hear that one in four people die of heart disease,
it's really tempting to think that that means the probability that both you and your brother die of heart disease
is one in four times one in four, or one in sixteen.
After all, the probability of two successive coin flips yielding tails is one half times one half,
and the probability of rolling two ones on a pair of dice is one sixth times one sixth, right?
The issue is correlation.
If your brother dies of heart disease and considering certain genetic and lifestyle links that are at play here,
your chances of dying from a similar condition are higher.
A formula like this, as tempting and clean as it looks, is just flat out wrong.
What's going on with cases like flipping coins or rolling two dice is that each event is independent of the last.
So the probability of b given a is the same as the probability of b.
What happens to a does not affect b. This is the definition of independence.
Keep in mind, many introductory probability examples are given in very gamified contexts,
things with dice and coins, where genuine independence holds.
But all those examples can skew your intuitions.
The irony is that some of the most interesting applications of probability, presumably the whole motivation for the kind of courses using these gamified examples,
are only substantive when events aren't independent.
Bayes' theorem, which measures exactly how much one variable depends on another, is a perfect example of this.
Now, you won't always see Bayes' theorem like this.
Sometimes you'll see it instead as this.
For any h and e, the probability of h given e is equal to the probability of e given h times the probability of h,
divided by the probability of e given h times the probability of h,
plus the probability of e given not h times the probability of not h.
That's a mouthful.
Now, these two are actually equivalent because those denominators are equivalent.
Notice that the numerators are exactly the same and these denominators are actually equivalent.
A more complicated denominator just offers a fleshed out way to calculate the probability of e.
So here is why these two are identical.
And this is probably going to be the most technical part of the video.
I'm going to try my hardest to make this as accessible as possible,
but I want to give you guys a really deep understanding of Bayes' theorem.
And so I want you guys to see why these two are equivalent.
I want you to have a deep, unifying, explanatory understanding of Bayes' theorem.
And because of that, I want to show you that these two are identical.
I don't want you to just take my word for it.
So we want to show that the probability of e equals the probability of e given h times the probability of h,
plus the probability of e given not h times the probability of not h.
For starters, I hope this is perhaps somewhat intuitive on its face.
Like, the probability that I go on a run is equal to the probability that I go on a run,
assuming that it's raining, times the probability that this assumption is correct,
plus the probability that I go on a run, assuming it's not raining,
times the probability that that assumption is correct.
Right?
Like, it's either raining or it's not raining.
And conditional on its raining, there's a certain probability that I go running.
And conditional on its not raining, there's a certain probability that I go running.
And so you can go down each of those horns of the dilemma,
and then find the probability that I run given that horn of the dilemma,
times of course, the probability that that horn would obtain it all,
and then just add that to the probability that I'd run on the other horn of the dilemma.
Times, of course, the probability that that horn is itself true.
So I'm hoping that that verbal walkthrough can at least make this intuitively plausible to you,
but there is a more formal way to see why they're the same. And if you're like me,
you're not going to settle just for that kind of reasonably intuitive understanding.
You want to go deeper. So here's a more formal way to show that these two are the same.
Bam! Okay, so I'm going to talk through this, but this only has six steps,
and the first step is quite convoluted. But what I'm going to do is I'm going to
give you a bird's eye overview of the proof here. And so I'm just going to go through steps one
through six, and then I'll explain why we know that step one is true. So step one says that
e is logically equivalent to the following disjunction. This is a disjunction of two disjuncts.
The first disjunct is e and h, and the second disjunct is e and not h. So what this is saying
is that e is logically equivalent to e and h or e and not h. And so once you just spot me that,
and I'm going to prove that in a second, but once you spot me that, it follows by equivalence
that the probability of e equals the probability of e and h or e and not h. All right, remember,
by equivalence, if p is logically equivalent to q, that is, if p logically entails q and q
logically entails p, well then the probability of p equals the probability of q, right? And since
e is logically equivalent to this, it follows that the probability of e is equal to the probability
of this. So two follows from one together with equivalence. But notice that e and h, on the one
hand, and e and not h on the other are incompatible. These can't both be true, right? It can't both be
true that e and h as well as e and not h, because then both h and not h would be true, and that's
contradictory. So these two are incompatible with one another. And remember by finite additivity,
whenever you have a disjunction of mutually incompatible disjuncts, it follows that the
probability of that disjunction as a whole is just equal to the sum of the individual probabilities
of the individual disjuncts. So three follows from finite additivity. But notice what we have in
two and three. Here we have probability of this disjunction. And here we have the probability
of that very same disjunction. And we have equal signs. So we have the probability of e being equal
to the probability of this disjunction. And the probability of this disjunction being equal
to the probability of e and h plus the probability of e and not h. And from those, it just follows
that the probability of E is identical to the probability of E and H plus the
probability of E and NOT H. We have here the structure A is equal to B and a B is
equal to C, and from that it follows that A is equal to C, right? So, from 2 and 3,
we get that the probability of E equals the probability of E and H plus the
probability of E and NOT H. But remember, by the ratio formula, the
probability of E and H is equal to the probability of E given H times the
probability of H. This is, again, that same rearrangement of the ratio formula.
As I originally articulated it, the probability of h was over here in the denominator, but it's the same thing, right?
We just multiply both sides by the probability of h. And remember, you can apply the ratio formula to any propositions p and q.
So we have it both being the case that the probability of e and h equals the probability of e given h times the probability of h,
and the probability of e and not h is equal to the probability of e given not h times the probability of not h.
And notice again that here, this term here is equal to this term here,
and so we can just plug this into this above equation here. And notice that this term here is equal to this term here.
And so we can plug in this for this term here. And so then from 4 and 5, it follows that the probability of e
equals the probability of e given h times the probability of h plus the probability of e given not h times the probability of not h.
And that's precisely what we wanted to prove. Okay, I understand that's a bit technical, but that's the bird's eye view.
And so we can prove that these two are equivalent,
given that you spot me this first thing, that e is logically equivalent to e and h or e and not h.
And so my task now is to prove that this is in fact logically equivalent to this. And to do that,
what I'm going to do is we're going to suppose that e is true and then show that this follows,
and then we're going to suppose that this is true and then show that e follows.
And so that'll show that kind of bi-directional entailment.
It'll show that e logically entails this, and it'll also show that this logically entails e, and hence that they're logically equivalent.
So let's go from the left to right direction, the logical entailment there.
We're going to suppose that e is true, and then we're going to conclude from that supposition
that this is true. And when you suppose something to be the case, and you prove another thing to be the case,
well, then your license to say that, well, then if the first is the case, then the second is the case.
So suppose that e is true, and remember we're going to try to derive this disjunction. So suppose e is true.
Well, either h is true or not h is true. Let's go under each horn of that dilemma. Suppose that h is true.
Well, then obviously both e and h are true, right? We suppose that e is true, and now we suppose that h is true, and so e and h follows.
But whenever you've proved something, you can add whatever you want in there to a disjunction, right? If you prove that snow is white,
well, then it follows from that that either snow is white or
Cinderella exists, or either snow is white, or 1 plus 1 equals 7. That disjunction as a whole is true, because at least one of its
disjuncts is true, namely the first one. So under this horn of the dilemma, we were able to prove this disjunction.
What about the other horn of the dilemma? Well, when we suppose that not h,
we can conclude that e and not h, right? Because we're still under the assumption that e is true, and we just suppose that not h is true,
and so it follows that e and not h is true.
But then again, we're able to introduce another disjunct into that, because we've just proved this disjunct,
so any disjunction with this as a disjunct is going to be true as a whole, and so this disjunction as a whole follows.
So whichever horn you pick, the disjunction follows, and whenever it's the case that either a or b, and whichever way you go,
c is the case, well, then you can just conclude that c is the case, right?
So since under each horn of the dilemma, you get this disjunction, it follows that we could just conclude that disjunction.
Well, like, think about this more generally. If it's either the case that I go to the park or I don't go to the park, and
either way I'm gonna be happy, well, then it just follows I'm gonna be happy.
If I go to the park, I'm gonna be happy. If I don't go to the park, I'm gonna be happy.
Well, since either I'm going to the park or I'm not going to the park, and since each of those entails me being happy,
we can conclude that I'm gonna be happy. And so we conclude that the disjunction itself is true. And notice what we've just done, right?
We suppose that e is true, and we derive the disjunction itself.
And so we can say that if e is true, then the disjunction follows. If I suppose that John is a bachelor, and then I derive from that
supposition that John is male, well, then I can just conclude that if John is a bachelor, then John is male.
So now we've just proved the left to right direction. Next up, we want to prove the right to left direction.
So we want to show how this disjunction
logically entails e.
And again, to do that, we're just gonna suppose that the disjunction as a whole is true, and then we're gonna
ultimately derive e. And so then we're gonna conclude that if the disjunction is true, well, then e follows.
Oh, and by the way, there should be one more parentheses here and here. Okay,
I just did some makeshift editing to put little tiny parentheses there. Okay, so yeah, we're doing the right to left direction now.
We're gonna show how this disjunction entails e.
So we're gonna suppose that the disjunction is true, and then we're gonna show how e follows from the disjunction.
So suppose that the disjunction is true.
