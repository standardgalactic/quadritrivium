of Bayes theorem. So recall the standard form, which is right here, the probability of H
given E is equal to the probability of E given H times the probability of H divided by the
probability of E. Now you can think of this standard form as basically the probability
of H given E is equal to the probability of H times this factor here. This factor, this
is not the Bayes factor, the Bayes factor is just the likelihood ratio, okay? So I actually
don't know what this factor is called here. Let's call it the Joe factor. Okay, so think
of the standard form of Bayes theorem as the posterior probability of H or the probability
of H given E is equal to the Joe factor times the probability of H. Now we can ask, under
what condition is the probability of H given E greater than the probability of H? That
is, under what condition does E raise the probability of H? Under what condition is
E evidence for H? Well, examining the equation here, that happens just in case the probability
of E given H is greater than the probability of E. Just in case the Joe factor is greater
than one. If the Joe factor is greater than one, well then when you multiply something
greater than one by the probability of H, this thing on the left here has to be greater
than the probability of H. So when the Joe factor is greater than one, that is when the
numerator is greater than the denominator, we have the posterior probability of H being
greater than the prior probability of H. And so what that means is that E is evidence for
H, that is E raises the probability of H, when H renders E more probable than E would
have been otherwise, right? This is H rendering E more probable than E would have been otherwise.
And when that happens, E is evidence for H. So this is another way to think about evidence.
Usually though, it's much easier to calculate the likelihood ratio and to use that as a guide
to evidential support. But I did just want to put this on your radar because it's important.
All right, so we've gone through Bayes theorem, we've gone through evidence, we've gone through
probability, we've gone through all this stuff. But you may still find yourself with
out a kind of intuitive grasp of Bayes theorem. And that's where I think visualizations can
really help. They can transform your thinking and help you see the power, the truth, the
utility, and the ultimate why behind Bayes theorem. And so to that end, we are going
to be watching three videos, they're not super duper long. The first one is from three
blue one brown, and I'll talk about the second two later.
The goal is for you to come away from this video, understanding one of the most important
formulas in all of probability, Bayes theorem. This formula, it's central to scientific
discovery. It's a core tool in machine learning and AI. And it's even been used for treasure
hunting. When in the 1980s, a small team led by Tommy Thompson, and I'm not making up
that name, used Bayesian search tactics to help uncover a ship that had sunk a century
and a half earlier. And the ship was carrying what in today's terms amounts to $700 million
worth of gold. So it's a formula worth understanding. But of course, there were multiple different
levels of possible understanding. At the simplest, there's just knowing what each one of the
parts means, so that you can plug in numbers. Then there's understanding why it's true.
And later I'm going to show you a certain diagram that's helpful for rediscovering this
formula on the fly as needed. But maybe the most important level is being able to recognize
when you need to use it. And with the goal of gaining a deeper understanding, you and
I are going to tackle these in reverse order. So before dissecting the formula or explaining
the visual that makes it obvious, I'd like to tell you about a man named Steve. Listen
carefully now. Steve is very shy and withdrawn. Invariably helpful, but with very little interest
in people or the world of reality. A meek and tidy soul, he has a need for order and
structure and a passion for detail. Which of the following do you find more likely? Steve
is a librarian or Steve is a farmer? Some of you may recognize this as an example from
a study conducted by the two psychologists Daniel Kahneman and Amos Tversky. Their work
was a big deal. It won a Nobel Prize and it's been popularized many times over in books
like Kahneman's Thinking Fast and Slow or Michael Lewis' The Undoing Project. What
they researched was human judgments, with a frequent focus on when these judgments
irrationally contradict what the laws of probability suggest they should be. The example with Steve
our maybe librarian, maybe farmer, illustrates one specific type of irrationality. Or maybe
I should say alleged irrationality. There are people who debate the conclusion here,
but more on all of that later on. According to Kahneman and Tversky, after people are given
this description of Steve as a meek and tidy soul, most say that he's more likely to be
a librarian. After all, these traits line up better with the stereotypical view of a
librarian than a farmer. And according to Kahneman and Tversky, this is irrational.
The point is not whether people hold correct or biased views about the personalities of
librarians and farmers. It's that almost nobody thinks to incorporate information about the
ratio of farmers to librarians in their judgments.
With the tools that we've covered so far, I hope you guys are seeing what's going on.
We're given certain data, certain evidence, and we think about how expected that is under
the two hypotheses. One of the hypotheses is that Steve is a librarian. The other hypothesis
is that he's a farmer. And when people are thinking, okay, overall, in light of this
evidence, what's the probability that Steve is a librarian? Or what's the probability
that he is a farmer? Those are the posterior probabilities. And we know that the ratio
of the posteriors is not just a function of the likelihood ratio. That is, it's not just
a function of how expected that data is under the hypotheses. Steve fitting that description
might be more expected under the hypothesis that he's a librarian than it is under the
hypothesis that he is a farmer.
But you know what else you have to keep in mind? Remember the odds form of Bayes theorem.
You have to also keep in mind the ratio of the priors. You can't forget how many farmers
there are in relation to librarians. If farmers vastly outnumber librarians in the population,
you have to take that into account when you're trying to determine the posterior probabilities.
And ignoring the priors in this kind of way is often called base rate neglect. You're
ignoring the base rate of librarians in the population or you're the base rate of farmers
in the population. Anyway, I'm kind of anticipating things here, but I did just want to say that
and connect it to what we've covered so far.
In their paper, Kahneman and Tversky said that in the US that ratio is about 20 to 1.
The numbers that I could find today put that actually much higher. But let's stick with
the 20 to 1 number, since it's a little easier to illustrate and it proves the point just
as well. To be clear, anyone who has asked this question is not expected to have perfect
information about the actual statistics of farmers and librarians and their personality
traits. But the question is whether people even think to consider that ratio enough to
at least make a rough estimate. Rationality is not about knowing facts, it's about recognizing
which facts are relevant.
Now if you do think to make that estimate, there's a pretty simple way to reason about
the question, which, spoiler alert, involves all of the essential reasoning behind phase
theorem. You might start by picturing a representative sample of farmers and librarians, say 200 farmers
and 10 librarians. Then when you hear of this meek and tidy soul description, let's say
that your gut instinct is that 40% of librarians would fit that description and that 10% of
farmers would. If those are your estimates, it would mean that from your sample you would
expect about 4 librarians to fit the description and about 20 farmers to fit that description.
So the probability that a random person, among those who fit this description, is a
librarian, is 4 out of 24, or 16.7%. So even if you think that a librarian is 4 times as
likely as a farmer to fit this description, that's not enough to overcome the fact that
there are way more farmers. The upshot, and this is the key mantra underlying phase theorem,
is that new evidence does not completely determine your beliefs in a vacuum. It should update
prior beliefs. If this line of reasoning makes sense to you, the way that seeing evidence
restricts the space of possibilities, and the ratio you need to consider after that,
then congratulations, you understand the heart of Bayes' theorem. Maybe the numbers that
you would estimate would be a little bit different, but what matters is how you fit the numbers
together to update your beliefs based on evidence. Now, understanding one example is one thing,
let's see if you can take a minute to generalize everything that we just did, and write it
all down as a formula. The general situation, where Bayes' theorem
is relevant, is when you have some hypothesis, like Steve is a librarian, and you see some
new evidence, say this verbal description of Steve as a meek and tidy soul, and you
want to know the probability that your hypothesis holds given that the evidence is true. In
the standard notation, this vertical bar means given that, as in, we're restricting our
view only to the possibilities where the evidence holds. Now remember the first relevant number
we used, it was the probability that the hypothesis holds before considering any of that new evidence.
In our example, that was 1 out of 21, and it came from considering the ratio of librarians
to farmers in the general population. This number is known as the prior. After that,
we need to consider the proportion of librarians that fit this description, the probability
that we would see the evidence given that the hypothesis is true. Again, when you see
this vertical bar, it means we're talking about some proportion of a limited part of
the total space of possibilities. In this case, that limited part is the left side,
where the hypothesis holds. In the context of Bayes' theorem, this value also has a
special name, it's called the likelihood. Similarly, you need to know how much of the
other side of the space includes the evidence, the probability of seeing the evidence given
that the hypothesis isn't true. This funny little elbow symbol is commonly used in probability
to mean not. So, with the notation in place, remember what our final answer was. The probability
that our librarian hypothesis is true, given the evidence, is the total number of librarians
fitting the evidence, 4, divided by the total number of people fitting the evidence, 24.
But where did that 4 come from? Well, it's the total number of people times the prior
probability of being a librarian, giving us the 10 total librarians, times the probability
that one of those fits the evidence. That same number shows up again in the denominator,
but we need to add in the rest. The total number of people times the proportion who
are not librarians, times the proportion of those who fit the evidence, which in our
example gives 20. Now notice the total number of people here, 210, that gets cancelled out,
and of course it should, that was just an arbitrary choice made for the sake of illustration.
This leaves us finally with a more abstract representation purely in terms of probabilities.
And this, my friends, is Bayes' theorem. More specifically, going back to my characterizations
earlier on, that is the standard form of Bayes' theorem, and remember we actually looked at
two different variants of the standard form. One of them only had the probability of E
down here in the denominator, but I also explained how that is equivalent to having this longer
denominator here. So this is just the second variant of the standard form of Bayes' theorem
that we already covered. More often, you see this denominator written simply as P of
E, the total probability of seeing the evidence, which in our example would be the 24 out of
210. But in practice, to calculate it, you almost always have to break it down into the
case where the hypothesis is true, and the one where it isn't.
Capping things off with one final bit of jargon, this answer is called the posterior. It's
your belief about the hypothesis after seeing the evidence.
Writing it out abstractly might seem more complicated than just thinking through the
example directly with a representative sample. And yeah, it is. Keep in mind, though, the
value of a formula like this is that it lets you quantify and systematize the idea of changing
beliefs. Scientists use this formula when they're analyzing the extent to which new
data validates or invalidates their models. Programmers will sometimes use it in building
artificial intelligence, where at times, you want to explicitly and numerically model
a machine's belief. And honestly, just for the way that you view yourself and your own
opinions and what it takes for your mind to change, Bayes' theorem has a way of reframing
how you even think about thought itself. Putting a formula to it can also be more important
as the examples get more and more intricate. However you end up writing it, I actually
encourage you not to try memorizing the formula, but to instead draw out this diagram as needed.
It's sort of a distilled version of thinking with the representative sample,
where we think with areas instead of counts, which is more flexible and easier to sketch
on the fly. Rather than bringing to mind some specific number of examples like 210,
think of the space of all possibilities as a one by one square. Then, any event occupies
some subset of this space, and the probability of that event can be thought about as the area of
that subset. So for example, I like to think of the hypothesis as living in the left part of the
square with a width of p of h. Now I recognize I'm being a bit repetitive, but when you see
evidence, the space of possibilities gets restricted, right? And the crucial part is that
that restriction might not be even between the left and the right. So the new probability for
the hypothesis is the proportion that it occupies in this restricted wonky shape.
Now, if you happen to think that a farmer is just as likely to fit the evidence as a librarian,
then the proportion doesn't change, which should make sense, right? Irrelevant evidence doesn't
change your beliefs. But when these likelihoods are very different from each other, that's when
your belief changes a lot. Bayes' theorem spells out what that proportion is, and if you want,
you can read it geometrically, something like p of h times p of e given h, the probability of both
the hypothesis and the evidence occurring together, is the width times the height of this
little left rectangle, the area of that region. Alright, this is probably a good time to take
a step back and consider a few of the broader takeaways about how to make probability more
intuitive beyond just Bayes' theorem. First off, notice how the trick of thinking about a
representative sample with some specific number of people, like our 210 librarians and farmers,
was really helpful. There's actually another Kahneman and Tversky result, which is all about
this, and it's interesting enough to interject here. They did this experiment that was similar to
the one with Steve, but where people were given the following description of a fictitious woman
named Linda. Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy.
As a student, she was deeply concerned with issues of discrimination and social justice,
and also participated in the anti-nuclear demonstrations.
After seeing this, people were asked, what's more likely? One, that Linda is a bank teller,
or two, that Linda is a bank teller and is active in the feminist movement. 85% of participants said
that the latter is more likely than the former, even though the set of bank tellers who are active
in the feminist movement is a subset of the set of bank tellers. It has to be smaller.
So that's interesting enough. But what's fascinating is that there's a simple way
that you can rephrase the question that dropped this error from 85% to zero. Instead, if participants
were told that there are 100 people who fit this description, and then they're asked to estimate
how many of those 100 are bank tellers, and how many of them are bank tellers who are active in
the feminist movement, nobody makes the error. Everybody correctly assigns a higher number to
the first option than to the second. It's weird. Somehow phrases like 40 out of 100
kick our intuitions into gear much more effectively than 40%, much less 0.4, and much less abstractly
referencing the idea of something being more or less likely. That said, representative samples
don't easily capture the continuous nature of probability. So turning to area is a nice
alternative, not just because of the continuity, but also because it's way easier to sketch out
when you're sitting there pencil and paper puzzling over some problem. You see, people often
think about probability as being the study of uncertainty. And that is, of course, how it's
applied in science. But the actual math of probability, where all the formulas come from,
is just the math of proportions. And in that context, turning to geometry is exceedingly helpful.
I mean, take a look at Bayes theorem as a statement about proportions, whether that's
