But the particular way in which they do that
might not actually be the optimal way.
And we could design machine learning algorithms that
do better than humans.
Now, they're designed to achieve the same goals
that humans want to achieve.
And the overarching architecture of that system
is similar, but it's doing it differently, right?
So that's a concrete way in which it wouldn't even
make sense at that point to demand
that the machines adhere slavishly to.
It would make no sense.
What humans are doing, right?
Yeah, yeah.
So I mean, the reason, I told you what you're going to say.
I mean, that was a joke, obviously, but so many people
have said that they can't imagine another way.
And my intuition is that, wow, if you ask 100 experts,
and they all say, no way, we have to do it this way,
it must be the other way.
You know?
It must be another way.
Well, I can't imagine another way,
but I can imagine there being another way.
In the same way that I was very struck by,
I think this has very deep implications
for how we think about ourselves
and our own thought processes.
So I attended a dinner once,
and I talked to this geologist
who was working on earthquake prediction.
And he made the very vociferous point
that he felt that the classical education in geology
had ruined generations of geologists.
Okay.
Because the classical models
simply could not accurately predict earthquakes.
Like tectonic plate shift models, or?
This is what he said.
I'm not a domain expert.
And he didn't really go into the details of why that was.
That you could have sound geophysical principles
that were not actually useful at making predictions.
And he argued that this whole thing was blown wide open
when people started to use deep learning
to predict earthquakes.
And he felt that whatever it is
that we thought we knew about geology,
even if it was right,
maybe there are more correct theories of geology
that are expressed in some inscrutable language
that the deep learning system can discover.
And eludes us.
And I have a hard time swallowing that,
but at the same time,
you have to acknowledge the possibility
that it's our hubris as humans
that convinces us that the nature of the universe
has to be expressible in the language
that we have at our disposal right now,
in the scientific language in current use.
I mean, this is something that historians of science
have known for a long time
that our theories are strongly latched
to our description languages.
And even our basic observations
can't be described in a theory-independent way
when our theories change, our observations change.
We actually see things differently.
And so that's reason for humility
in terms of privileging our notions of intelligence.
Right.
Well, in that same vein,
I had John Krakauer on the show a few weeks ago,
and we were talking about different levels of explanation.
And we talked about reductionism
and the proper level of understanding phenomena.
And there are lots of higher cognitive functions
that it may not make sense
to try to understand on the level of neurons
and neural circuits, right?
But instead on the level of some behavioral function
and so on.
And the core ingredients
that you guys mentioned in your paper
confer like these advantages, right?
They make humans so awesome in various ways.
And I'm wondering, could there be disadvantages too?
There have to be, right?
That's the yin and yang of a different type.
Yeah, so that's what I was thinking too.
It's like, let's say it is possible,
we can map some of these higher level cognitive functions
to the level of neural circuits.
Are we humans, but because we need to reduce complexity
using things like compositionality and intuitive physics,
are we incapable of conceiving
how that mapping could occur?
Whereas a deep learning network or some sort of AI system
without these human skills, quote unquote, or limits
might by brute force figure it out
in a way that we could eventually be satisfied
with that kind of explanation.
Does that make sense?
Yeah, so I wouldn't say that people are incapable
of doing this since historically we've been very capable
of constructing various kinds of cognitive prostheses
that allow us to understand things
that seemed mystifying before.
That being said, it's certainly a logical possibility
that neural networks could discover these theories
maybe more easily than us
because they have different inductive biases than us.
I certainly think it's within the realm of possibility
to think about neural network scientists
that are as different from us as we are from dogs and cats.
Now, but that being said,
I think my experience in studying these systems
is that in some sense there's always less than meets the eye
that we have this strong inclination
to kind of anthropomorphize these systems
and attribute to them psychological competences
that they lack.
And when you start to really test these rigorously,
they fall short.
I mean, this is very clear in the domain, for example,
of natural language where most natural language processing
systems simply don't recover the same kind
of semantic understanding of the world that humans have.
It's not that hard to demonstrate this.
So I just think that while it's a logical possibility,
I think we need to be doing more
than just training neural networks on ImageNet
or even data sets on reasoning from natural language,
I think are too impoverished.
But see, it's really a privilege to be able to be
at the point where we can say that.
And there have been these advances that are amazing.
Now we can say it's not enough.
That's great.
I think that's a great place to be in.
So I can imagine lots of different scenarios
for this path toward fuzzily
what we could call general AI.
And reading your paper brought just one to me,
this one scenario.
And as soon as you write this in a science fiction book,
it means it's gonna be completely wrong.
But so I can imagine, and the way I sort of envision it
is what we're building toward is an AI system
that comes off the shelf or out of the box,
and it's ready to solve new problems
and learn new things efficiently
and have some of these built-in ingredients
that are proposed in the paper.
I can also imagine a scenario though
where we sort of an in-between phase
where we have to build these machines
that then we set out into the world
like with little embodied cognitive architectures.
And then we have to wait for them
to gather enough different kinds of data and experience
that we can't build into them.
You think that's a possibility
that we'll go through this phase
where we're taking these robots
and then letting them be for a while
and then we can use them, for instance,
then they're domain general enough
that we can actually use them?
Or can you imagine that kind of scenario?
Is it a ridiculous question?
Yeah, I mean, I don't see why not,
although of course there's all sorts of ethical
and practical issues setting loose robots in the world.
It depends what kind of constraints we put on them
and what kind of challenges,
do they have to enter the workforce
and work for their...
Oh, definitely, definitely.
For me, anyway.
So I came up through neurophysiology
in my graduate work and my postdoctoral work.
And this podcast I made has the tagline
where neuroscience and AI converge.
But I always have thought of and I'm only starting
to not think of it this way.
I've always thought of cognitive science
as a part of neuroscience.
That neuroscience sort of subsumes psychology even.
It seems to depend on whose definition
you like of these things,
but so this paper was written from a real cognitive science
historical perspective and modern day sense
of cognitive science.
Do you think that we're in a cognitive science resurgence
sort of over neuroscience?
So there's a lot of hype in neuroscience, right?
We're gonna figure all this out.
And now is cognitive science coming back now
