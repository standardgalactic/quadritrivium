about the necessary ingredients
for intelligence for a long time.
And I sympathize with computer scientists
in some ways because there's this feeling
of moving goalposts, like there was a time
when people said, you know,
when a computer can win at chess,
that's when we know it's really intelligent.
And then if it wins a go, then it's really intelligent.
And it's a sort of moving goalpost.
And, you know, I'm sure some people wonder,
like, when are you gonna be satisfied?
And I'm not really a big believer
in having a kind of single test of that sort.
Because I just think that
human intelligence is too multifaceted
to really be boiled down in that way.
But that being said, I do think that it's important
for us to think about what we've learned
from the study of cognitive science
and what that tells us about the nature of human intelligence
and the limitations of current AI approaches.
So with that preface in mind,
I can kind of make some broad stroke characterizations.
The basic thrust of modern machine learning approaches,
particularly deep learning approaches,
has been to construct systems
that are extremely expressive
in the sense that they can learn
lots and lots of different things.
So we could think about this
as a kind of function approximation problem.
Like, you have a function that maps images
to object categories.
And that's one way to think about
an object recognition system.
Or you have a translation system
that's mapping text in one language
to text in another language.
So these are all potentially really complicated functions.
And the general approach has been
to build very complicated neural network architectures.
Now I should say they're not conceptually complicated.
You can basically draw a picture of them
and it's pretty straightforward.
You have a bunch of neurons
that are connected to each other in layers.
But they're complicated in the sense
that they're very expressive.
They have lots of degrees of freedom,
possibly millions of parameters
that then get optimized by a learning algorithm.
And...
So you could take the same network,
same naive network or whatever
and train it on a bunch of different types of domains.
So you could use this thing
to train it on recognizing objects.
You could use this same network
to train it on language
or something within the realm.
There's a lot of possibilities to train it.
Yeah, so just to maybe put this in perspective,
before the heyday of deep learning,
the most successful approaches
used collections of hand engineered features.
So people would put a lot of thought
into defining various kinds of image
or natural language features depending on the domain.
And then they would assume a fairly simple
supervised learning algorithm
that operated on those features.
So for example, you might assume
that you take this collection of features,
you map them through a linear function,
you map them through a linear classifier,
and then you basically train the weights
of that linear classifier.
So that's a very low complexity solution
in the sense that the space of possible functions
that can be learned is relatively small, right?
So you can only learn linear functions of this feature set,
contrast that with a deep neural network
where all the different parameters
and all the layers are being optimized
to solve this classification problem.
That's what's commonly called end-to-end training.
And now you have millions of parameters
and the function space is enormous
because now you don't necessarily,
you're not necessarily restricted anymore
to linear functions.
You could learn linear functions,
but you could learn all sorts of non-linear functions.
So you might think, okay, well, that's great.
Now we can learn more functions, right?
But the catch is that the more complex
your function class is,
the more data you need to effectively learn it
in the sense that if you only have a small amount of data,
then you could learn to model those data very well, right?
If you have a very large function class,
you can fit the training data really well,
but the problem is that there may be noise
in that training data
and the training data might not be representative
sufficiently of all the test cases you might encounter.
And so your classifier is gonna fail to generalize.
And there's a whole literature
that rigorously analyzes this
from a statistical learning perspective.
It's called the study of sample complexity.
How much data do you need to effectively learn,
to effectively generalize?
And in general, more complex models need more data.
And that's why actually,
in some sense the deep learning revolution
has been just as much about big data revolution
in the sense that we've constructed bigger
and bigger data sets to feed the insatiable
sample complexity needs of these learning algorithms.
Plus the computational power to do so.
Yes, absolutely.
Yeah, and now it's the other thing.
So yeah, big computation is a big contributor.
In some sense, the deep learning architectures
we're using today are pretty similar
to the ones that we had in the 90s,
but they work better because we have a lot more data
and a lot more computing power.
That being said, it is still somewhat of a mystery
to learning theorists why these things work
because at first blush, the learning theories
would say that you would need even more data
than we have to effectively generalize.
But learning theorists are trying to crack open that nut
and that it's probably not worth getting into that right now.
But the point here is that the approach today
is about building really flexible function approximators.
Now that doesn't sound so sexy from the perspective
of artificial intelligence to boil them down
to nonlinear function approximation,
but that's really what it's all about.
Now as I was saying, the architecture
of these networks is really flexible,
but it also means that that very flexibility
allows the system to learn things
that we might think of as not being the right answer.
So it's gonna pick up all sorts of patterns,
but it might fail to learn the same kinds of things
that humans learn and it might not learn
those things in the same way.
So let me try to elaborate on what I mean by that.
So I was talking about complexity
in terms of the number of parameters
and the size of the function class,
but another way to restrict the effective complexity
of a machine learning system is to have an inductive bias.
So it says that the inductive bias says that,
or I couldn't in theory learn any function
in this function class,
but I have a preference a priori
for certain functions over others.
So I think that some functions might be more plausible
than others and I can formulate that
in the language of probability theory
and that shapes the sample complexity arguments
because now, even if I have a really large function class,
if I have very strong inductive biases,
that can kind of compensate for the need
to experience a lot of data.
The inductive biases basically say that
some things are gonna be easier to learn with less data
and other things are gonna be harder to learn
and will require more data.
And how does that compare to having stronger priors?
Those are priors, right?
So inductive biases are priors.
I mean, you can construct,
depending on what your kind of formal framework is,
you can talk about inductive biases in a non-Basian way,
but I often think it's helpful to think about prior distributions
as a particular way of representing inductive biases.
I see, okay.
So now, so let me try to loop this back to human cognition.
Human cognition, we think that humans can learn
from small amounts of data precisely
because they have very strong inductive biases.
And they also have inductive biases
that are relatively abstract,
which allows them to generalize their knowledge very widely.
So let me just talk about some of those inductive biases.
Actually, before I talk about inductive biases,
let me just try to make vivid
why you need an inductive bias at all.
There's a great book by Eric Baum called What is Thought?
And he has the following example.
So imagine I'm training a chair classifier.
So I'm giving it images and it's telling me chair
or not chair, all right?
