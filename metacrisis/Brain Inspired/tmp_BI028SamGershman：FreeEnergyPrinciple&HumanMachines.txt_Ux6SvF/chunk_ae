So I give it some finite number of these training images
and it learns to perfectly classify,
to perfectly discriminate chairs from non-chairs.
All right, now I'm gonna give it a new chair
and I want it to generalize to that.
Now let's imagine that it had no inductive bias, okay?
The implication of having no inductive bias
is that in fact, this classifier,
which could perfectly discriminate between chairs
and non-chairs in the training set,
has no basis of generalizing to this new image
that in fact, it will be completely uncertain
about it would have no basis of knowing
whether this new image is a chair or a non-chair.
And the reason is because in a continuous function space,
there are an infinite number of functions
that separate the chairs from the non-chairs.
And that means that there's an infinite number of functions
that would perfectly separate chairs from non-chairs
and classify this new image as a chair.
And then there's also an infinite number of functions
that perfectly separate chairs from non-chairs
and classify this new image as a non-chair, okay?
And what it means to have no inductive bias
is that you have no preference about which of those functions
is the correct function.
So generalization depends crucially
on having some inductive bias.
Now neural networks do have inductive biases.
The architecture of the network, the parameterization
and so on, those are all forms of inductive biases,
but they're just relatively weak inductive bias.
Now we think that humans have strong inductive biases.
They have strong inductive biases about things like
causality about objects, about relations between objects,
about compositionality of knowledge,
so that the idea that you can build complex representations
out of simpler ones.
And we have strong inductive biases in different domains
that take the forms of something like an intuitive theory,
like an intuitive theory of psychology
that lets us make inferences about other people's minds
or an intuitive theory of physics
that allows us to make inferences
about physical interactions in the world.
Anyway, I'll stop there and let you ask it.
Well, no, that's okay because you're sort of rattling off
the ingredients that are proposed in this paper.
So, and you just got there naturally,
which is a beautiful thing.
Yeah, so I was gonna ask about how humans learn, right?
And in comparison, which is where you've really
just brought us to anyway, but before we even get into
just a few maybe of the examples
of these things that you just listed,
it's an interesting conundrum
whether we understand how humans learn well enough to then,
so the whole goal is to build systems
that learn and think like humans.
But do we know how humans learn and think well enough
to even begin doing that?
And so these are things, like when you say
terms like intuitive physics, of course,
well, let's just maybe put that question on hold
unless you wanna answer it.
And then we can kind of step through
a few of the ingredients.
Well, no, I mean, I don't think that we know enough
about human cognition that we can just implement
human intelligence in a computational system.
It's more just that we have some general hints
and also some specific computational models
about how humans think.
And those are in interesting ways different
from a lot of the prevailing deep learning architectures.
Now, I will say that even at the time
that we were writing that behavioral and brain sciences
article, people were already starting to realize this.
So for example, in the domain of intuitive physics,
there was a lot of interest in building machine learning
systems that solved intuitive physics problems.
And if you build them in a kind of naive
sort of off the shelf way, they didn't work very well.
And they only started to work really well
when people started building in constraints.
And I would argue that a lot of the constraints
that worked well were precisely the constraints
that we think people have.
So for example, DeepMind and other groups
have developed models in which there are primitives
that correspond to objects and there are functional
interactions between the objects.
So it's a relational model that's object oriented.
And then you use deep learning to learn the parameters
of those functional interactions.
And that works much better for modeling intuitive physics.
So I would argue that that's kind of harnessing
the strengths of deep learning, which are really
about flexible function approximation
with the kinds of inductive biases
that we think humans have.
Okay, so let's just step through these.
So there are five core ingredients.
Is that the latest count?
I mean, this is sort of the count in the paper.
Uh-huh, yeah.
So one of them is this intuitive physics
that you're just talking about.
And this is the idea of, so this falls
under the rubric of when we're really young,
we're kind of born with and have really early on
this developmental software, right?
So that's sort of ingrained or an inductive bias, right?
So intuitive physics is the idea that we understand
that when one cue ball hits,
when one pool ball hits the other billiard ball,
we understand that there's a collision there.
These two things are solid and one affects the other.
Well, it's a lot more than that probably,
but yeah, that's one aspect of it.
Yeah, that's one aspect of it.
So the things are solid, not a liquid and so on and so on.
So this is one thing that is sort of missing
in deep learning networks that you were just describing.
And there's work being done on all of this.
Like I said, this is a nice roadmap
that people have seemingly been following actually.
So another one of those that falls
under the rubric of developmental software
is the intuitive psychology.
Like you were just, that you mentioned,
do you wanna just describe what that is briefly?
Yeah, so maybe just first for those people
who are not familiar with the notion of intuitive theory,
the reason that they're called intuitive theories
is to contrast them with scientific theories.
And the argument is that intuitive theories
and scientific theories share a lot
of the same structural characteristics,
but obviously they differ in content
in the sense that intuitive psychology
doesn't have access to functional MRI
and response time studies and so on, right?
But we still can build a theory-like representation
of another person's mind.
For example, we can reason about
another person's beliefs and desires
and combine our knowledge of those beliefs and desires
to, for example, make predictions
about what that person's gonna do.
Another example would be when kids are shown
kids can discriminate between whether
an acting agent is helping another agent
or hindering the other agent.
And so, yeah, there are these psychological aspects
of other agents that can easily be inferred
with this sort of developmental startup software.
Yeah.
Okay, and then the rest of the other three ingredients
kind of fall under what you describe
as model building elements.
And one of them is this comp,
and you've listed all these already,
you've mentioned these in passing.
So one is compositionality.
And this is basically where you can build new things
from simpler parts.
And so deep networks, and let's just run through
these real quick and then we can kind of talk about
them more broadly maybe.
So deep networks are not great at this.
Another is learning to learn or meta learning.
And so there's actually a lot to talk about
with this probably.
And causality is the last, the fifth ingredient
and building causality.
And I talked about this last episode a little bit
with Conrad Kurding and Juana Maronescu, his wife,
because they just wrote a paper on how to infer
causality in quasi experiments.
So it's a little different.
Okay, so like I said, this has been a roadmap
and it's like the paper was written
and then meta learning work immediately came out
of DeepMind and then, I don't know if you recently,
I didn't get a chance to read the paper,
but there's this recent paper out of DeepMind as well
that is using the meta reinforcement,
their meta reinforcement learning framework
to build in causality to.
Yes, it was written by my student.
Oh, is that right?
Issue to the scooper, yes.
Well, I should have.
A summer internship at DeepMind.
Okay, very good.
Yes, it's funny because in the paper
and one of the rebuttals from the people at DeepMind,
they were like, maybe we are on a converging path here
and so I suppose you are.
