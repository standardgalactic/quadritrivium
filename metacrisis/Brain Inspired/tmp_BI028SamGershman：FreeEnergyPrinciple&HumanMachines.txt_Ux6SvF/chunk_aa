you could build agents that adhere to those principles
and many agents in that set
would be what we consider smart
and many of them would be stupid.
Wait, isn't that what we have already?
No, I'm just kidding.
Yeah, right, exactly.
So I really like what Sherlock Holmes said
that to understand nature,
your mind must be as broad as nature.
That's how I feel, like really the study of intelligence
is that.
This is Brain Inspired.
Hey, everyone.
Welcome to episode 28 of Brain Inspired.
I am Paul Middlebrooks.
I had been thinking of a way to talk
about the free energy principle on the show
and I was gonna have a guest on a few weeks ago,
but we had to reschedule that for a later time.
But I stumbled on a recent paper
by the person you just heard, Sam Gershman,
who is a professor at Harvard University
and runs the Computational Cognitive Neuroscience Lab there
where they study learning, memory, decision making,
you know, cognition.
He's recently written about the free energy principle
and how it relates to other bigger picture theories
of how our brains function,
in particular the Bayesian brain hypothesis
and the concept of predictive coding in general.
So we talk about just that during the first part of the show.
Sam also co-authored a paper a couple years ago now
called Building Machines That Learn and Think Like Humans.
That paper breaks down some of the important ingredients
that current deep learning networks are missing
if we want to get to human-like intelligence.
So we talk about those concepts
and how far along we've come since then.
So those are the two major themes during this show
and of course we discuss much more
like the limits of our own cognition
and how that affects our ability
to understand complicated processes in the world,
including our own intelligence.
And hang on until the end too
because Sam gives some career advice that I love
for those of you early in your career
or even deep in your career,
but trying to figure out your next course of action,
your next direction.
Every new show I do, I realize I have more
and more questions to ask of the guests
to try to bring it all together
and keep some sort of running thread
from the previous guests.
So I try to do that here,
but of course I can never get to all the questions.
You can find out more about Sam
and links to the stuff that we discuss in the show notes
at braininspired.co.
slash podcast slash 28.
And you can also find how to support this ad-free labor
of love of a podcast by finding
and clicking on the red Patreon button at braininspired.co.
Fair y'all, see May,
Ion and Justin did just that this past week.
Thank you guys, that is beautiful.
All right, thank you for listening.
I hope that your projects are coming along smoothly
and you're immersed in and loving whatever it is
that you're pursuing.
Enjoy the show.
All right, Sam Gershman.
Welcome and thanks for being on the show.
Thank you.
So Sam, you run the computational cognitive neuroscience lab
at Harvard and you do lots of things.
For instance, you study how we learn,
you study the different systems that we use to learn
and how they interact,
how our perceptions get transformed
into these internal states in our brains
and how those states can be used to guide our actions.
You study processes like exploration versus exploitation
and plenty more.
Today, we are here to talk about two subjects in particular.
One, the free energy principle,
which is a hot topic these days in brain function
and neuroscience.
In your recent paper,
what does the free energy principle tell us about the brain?
And two, we're gonna talk a little bit
about the concepts introduced in your paper
from I guess two years ago now,
called building machines that learn and think like humans.
You ready?
The free energy principle.
What does the free energy principle tell us about the brain?
So in this paper,
you compare predictions of the free energy principle
with the Bayesian brain hypothesis
and how those two systems might fall out
and compare to each other in this realm
of what's called predictive coding.
Now, I'll just begin by reading the opening paragraph
of a 2009 review by Carl Friston,
who has been the developer of the free energy principle,
just to get us kind of rolling on this topic here.
So he writes,
the free energy principle is a simple postulate
with complicated implications.
It says that any adaptive change in the brain
will minimize free energy.
This minimization could be over evolutionary time
or milliseconds.
In fact, the principle applies to any biological system
that resists a tendency to disorder
from single cell organisms to social networks.
I could go on,
but that's his opening paragraph
to introduce the concept of the free energy principle.
Now, I'll let you maybe comment on this.
So, Sam, what is the free energy principle?
Yeah, so I should preface this by saying
that I'm not either an advocate
or an antagonist of the free energy principle.
I've just noticed over the years
that through many conversations with colleagues
that there's widespread confusion
about what exactly the free energy principle is
and what it predicts.
And so that was the motivation for me
to enter into this fray.
And I should say that the free energy principle
is closely connected to ideas that I've proposed
and have been deeply interested in.
So I think the real question here is,
what is really special about the free energy principle
relative to other ideas
that have been out there for a while?
And I can elaborate on that.
So in essence, the free energy principle
is the idea that the brain minimizes surprise.
Mathematically, it's a little bit more complicated than that,
but that really gives you the gist of it.
Now, the term free energy is a little bit unintuitive
for a lot of people
because it actually comes from physics.
The idea was that you can decompose
a complicated inference problem
into an inequality that expresses the relationship
between what physicists call the partition function
and what they call the variational free energy.
And it turns out that the object
that the physicists were studying
is closely related to problems
in machine learning and statistics
where probabilistic inference problems are intractable
and they can be made more tractable
by turning them into an optimization problem
under constraints.
So what does that mean?
And this actually is perhaps usefully introduced
by first talking about the Bayesian brain hypothesis
and then talking about
where the free energy principle enters into that.
So the Bayesian brain hypothesis states
that the brain is confronted
with ambiguous sensory evidence.
And it tries to interpret that sensory evidence
by making inferences about the hidden causes
that produced the sensory data.
So for example, on our retina,
we get a two-dimensional projection
of a three-dimensional world,
which means that the two-dimensional information
is fundamentally ambiguous.
There's no unambiguous mapping
from the 2D into the 3D.
Nonetheless, we're able to perceive three-dimensional structure
and the way we do that is we combine the noisy
or ambiguous sensory information
with prior beliefs about the nature of the world.
So for example, that objects have surfaces
that are smooth and continuous, right?
We can exploit those prior beliefs
to make inferences about the hidden structure of the world.
And the Bayesian brain hypothesis is really that
the brain is using Bayes' rule
to convert its prior beliefs into its posterior beliefs.
The posterior beliefs are the beliefs
about hidden variables in the world,
given the sensory data or other kinds of information
that the brain has received.
Now, that sounds very simple.
And if you just write down Bayes' rule,
it's actually mathematically extremely simple,
and it's a direct consequence of the axiom
of the probability theory.
