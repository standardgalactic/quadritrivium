as a coherent three-dimensional shape,
and concomitantly, activity in the lateral occipital complex,
which is known to be shape sensitive increases.
So there's a decrease in activity in V1
and an increase of activity in LOC.
And that was interpreted as arising
from this predictive coding framework
where because the shape representation
could predict the underlying line segments,
then the line segment representations become suppressed
because now they're predictable.
Gotcha.
Now, what does this have to do
with the free energy principle?
Well, it turns out that you can derive
this kind of architecture from the free energy principle,
but the catch is that you have to make
a whole bunch of assumptions that have to do with
the variational family.
So what space of distributions are you going to constrain
the inference problem to?
You have to make assumptions about ways
in which you're gonna approximate the free energy itself
because it turns out that under those chosen specifications,
you can no longer actually analytically compute
the free energy.
So anyway, the point is that there's a bunch of ingredients
that have to work well together
in order to produce predictive coding.
It's not a generic consequence
of the free energy principle as is sometimes conveyed.
Yeah, so these are the things that you actually explore
deeper in the paper, so.
Yeah, yeah.
And of course, the paper's online,
I think it's archive, right?
Yeah, so I mean, I'll link to it,
and pretty soon it'll be in science, I'm sure.
So Sam, what do you think?
Like the Bayesian brain hypothesis
and the free energy principle can both be seen
as these encompassing theories
of the brain function in general, right?
And how we aim to understand our minds
and intelligence in general.
Do you think that we'll ever have
like a single account of our brains and our minds?
Well, I think that the Bayesian brain hypothesis
and free energy principle,
they are good candidates for some general principles,
but they're not really sufficient by themselves, right?
They're too general in some sense.
They're too powerful in order to act.
Basically, you could build agents
that adhere to those principles,
and many agents in that set would be
what we consider smart,
and many of them would be stupid.
Wait, isn't that what we have already?
No, I'm just kidding. Yeah, right, exactly.
But my point is that if you want to pin things down
sufficiently that you could say,
all right, this kind of architecture,
these kinds of constraints,
these principles are only going to produce smart things
or tend to primarily produce smart things,
then you need to make more assumptions
than just that the brain is Bayesian
or minimize the surprise.
I mean, thinking about that,
do you think it's a better approach?
Of course, both approaches together
is probably the best path,
but a better approach to go to start overly constrained
or to start without enough constraints,
to start with something like the free energy principle,
and then start whittling away with constraints.
You know what I mean?
What approach is a favorable approach?
Well...
There's a balance, I know.
Yeah, I mean, it's important to make a distinction
between the engineering goals
of building artificial intelligence
and the scientific goals of trying to parse
what makes humans or animals intelligent.
Obviously, those things will converge in some ways,
but the scientific strategy is different
in the sense that we want to design controlled experiments
where we can isolate a particular hypothesis and test it.
That's different from engineering approaches,
which typically aim to construct something
that does something useful,
and whether or not some particular component of it
has been isolated is not that important.
And particularly in the era of building
very large deep neural network architectures
that don't have a lot of interpretability,
but can still accomplish certain kinds of tasks,
it's, you know, the strategy,
the research strategy is different.
Well, speaking of building things,
maybe we could transition here and talk about deep learning
in the state of AI and what it has,
what it needs, what it's missing.
So this is your paper,
Building Machines That Learn and Think Like People.
And this is in behavioral and brain sciences.
And this is a thorough paper
because it's in behavioral and brain sciences
and made even more thorough by the format.
So just, I mean, people know this,
but there's a target article which you guys wrote,
and then there's a bunch of reviews,
and then you get to rebut the reviews,
and this is all in like a single document
and it costs about a three and a half
if you really wanna print the whole thing out, you know?
So I've always liked that format,
but I've not written one like that.
And it's a lot of work just to get reviews.
I'm, one of my papers is under review right now,
and you know, just with three to four peer reviewers,
it's a lot of work just to rebut that and send it back.
But it seems like a lot more work for you guys.
I mean, what was that experience like?
It's all kind of a blur to me now.
I know, that's the other thing is
if it was written last year,
you probably did the work 12 years ago, you know, so.
Yeah, yeah.
No, it was a lot of work, but it was fun to write.
You know, it was great writing that paper
with those co-authors,
because these were ideas that we had talked about
for a long time and it finally forced us
to really coalesce them.
Yeah, yeah.
Okay, so I had Melanie Mitchell on the show
a few weeks ago and she's a complexity expert
and she's been in the field of AI for a long time.
And she had written this op-ed in the New York Times.
Yeah, I read that.
Oh yeah, okay.
So here's just a quote from like her approach to this.
Today's AI systems sorely lack the essence
of human intelligence,
understanding the situations we experience,
being able to grasp their meaning.
The mathematician and philosopher,
Giancarlo Rota famously asked,
I wonder whether or when AI will ever crash
the barrier of meaning.
And so there's this phrase, the barrier of meaning.
And this paper that you co-authored
takes that sort of broad statement
and breaks it down into cognitive components
that could help bridge that gap.
So it's really an initial roadmap of sorts
and has seemingly served as such,
which must be pretty rewarding for you
to have seen.
So maybe we can frame this this way
and maybe we can just start off talking
about how machines think and learn currently.
And then maybe bring in how humans think and learn
what we know about that
and then talk about them together.
So the premise of the paper is like,
look, yeah, we've done great work with AI
and deep learning has done amazing things,
but it's still not human-like.
And if we want to make it human-like,
we need to build in these other components,
these higher level components,
either build them into the deep nets
or however you're gonna perform the AI,
build these core ingredients in.
Do you wanna just talk for a moment
about the current state of how machines think?
Yeah, well, it's quite hard
to make a really general characterization
of how machines think
because there's 100 papers posted on archive every day
machine learning.
It's a rapidly moving target.
And our paper was really focused
on kind of general patterns, right?
So it's not the case that our critique
skates every machine learning paper
that was ever written.
But these are debates that have been going on
for a long time and we discussed this history
a little bit in our paper, I mean,
since the 70s and 80s.
But by the way, it's a beautiful paper.
I really enjoyed it.
I bet you've heard from a lot of people the same as well.
Thank you.
Yeah, so people have been going back and forth
