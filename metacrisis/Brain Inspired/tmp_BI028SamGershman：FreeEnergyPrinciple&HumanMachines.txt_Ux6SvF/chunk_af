No, no, there's definitely points of convergence.
Not completely, but I mean, I talk to those people
on a regular basis, so we're not like,
in conflict or anything.
Oh, of course not.
We're all going for the same goal anyway.
So I don't know if any of those
that you wanna really elaborate on as something that,
you know, I'm kinda curious your take
on how the paper has continued to be received over time
and how people, you know, if you've seen a lot of progress
in all of these domains
or what's really still lacking, et cetera.
Yeah, I think that there's been a ton of progress.
I think that as I was saying before,
many people have begun to acknowledge
that you need to build in some constraints
to get these systems to work appropriately.
Now, we're certainly not there yet
in a lot of practical areas of application.
You know, for example, we still, I think,
can't get systems to make reasonable captions for images.
They just, the fact of the matter is
they just don't really understand
what's going on in the image.
The barrier of meaning.
But they make reasonable captions, you know,
five out of seven times maybe now.
Is that a good?
I don't know, I mean, not in my experience.
Okay, well, yeah, yeah.
Anyway, but the point is just that these systems
can sometimes fool us into thinking
that they know more than they really do.
And yeah, so I'm encouraged by things
like the paper you mentioned,
meta-learning for causal inference.
I think that those kinds of approaches
are definitely on the right track.
I don't know to what extent the particular,
basically the thing that I wonder about
is to what extent building, exploring in the space
with kind of different neural network contraptions,
putting in different kinds of widgets
and so on is gonna get us to intelligence.
Sometimes it feels a little bit like
building a ladder to the moon.
Doing more of the same that didn't get us there initially.
Yeah.
Well, it's just, I mean, certainly,
there's no doubt that there's been a huge amount of progress,
but it also, I get the sense reading these papers
that we're kind of swirling around a local optimum.
And every once in a while,
we kind of pop out of that and go a little bit farther.
The developments don't seem of the sort
that are really getting a significantly closer
to human intelligence.
So it's just, to give one example,
think about AlphaGo, AlphaGo is
an amazing accomplishment.
I love, everyone has to always preface it
with how amazing it is.
And then, yeah, yeah.
Well, it's an amazing accomplishment.
There's no doubt about that.
But just think about what happens
at the end of a Go game, just shut it down
because AlphaGo can only play Go.
And that's in some sense true
of almost all the models that we have now.
And even within the space of, let's say, video games,
so we have algorithms that can play video games,
but you can make changes to those games
that would be completely trivial to a human player
and yet would be catastrophic to these algorithms.
Yeah.
So we're really far away, I think,
in the sense of capturing that human-like flexibility.
And I think it's because we haven't fully embraced
the principles of causality and compositionality
and intuitive theories and so on.
I think, I guess my general impression is that
we have this really useful tool
that we can build these gadgets out of neural networks
that do things, and if you can feed it enough data,
they can classify images or recognize speech.
But it's tempting to kind of over-apply those things
as a substitute, but really thinking hard
about the structure of a problem.
But it'd be fair to say that it's sort of a change
in quantity when you're adding these sorts of widgets,
et cetera, to the deep learning networks and not quality.
Yeah, yeah, I think that most of the advances
that we see on a day-to-day basis
are about getting a few more decimal points
score on some benchmark data set,
but you're not really,
I think the real contribution of cognitive science
is to get people out of the thinking in terms of,
get people, move people away from thinking
in terms of these benchmarks and,
or rather thinking more broadly
about what the benchmarks really are.
What is it that you want the system to do?
It's not just about playing go or playing video games
or recognizing objects.
There's some set of competences there
that transcend those individual tasks.
And if you only kind of play to win those tasks,
then you're gonna be missing something.
So we need to figure out where the hell
we actually want the goalposts and pour concrete
and then make them stick there, huh?
Yeah, yeah, yeah.
So I ask this to a lot of my guests.
So I'll ask you as well,
is emulating the human mind,
is that, you know, is emulating human thought and learning?
Is that the only way to get there, you think,
to general AI, to something that we would satisfy,
that would satisfy our notion of what general AI is?
Well...
Can I tell you what you're gonna say before you say it?
Yeah, go for it.
You're gonna say, well,
it's the only example that we have.
It's the best example.
It's the most complicated, you know,
the brain's the most complicated thing in the universe.
Okay, now go ahead and repeat what I said
and I'll be satisfied.
No, no, I wasn't gonna say that, actually.
I think it depends on what you mean by general intelligence
because it could potentially be a tautology
if we think that what we mean by general intelligence
is whatever we think humans do,
then that's only gonna be satisfied
when we build machines that think like humans.
But we might not, right?
Maybe we'll eventually come to some way of thinking
about intelligence that's really different from our own.
It's just, it's very hard to get to that point
because inevitably we project our own notions
of intelligence onto our machines.
Of course.
But I mean, one way to approach this potentially
is from, you know, the computational level.
So we can always talk about David Maher
and the computational highest in level.
And where would you start?
Like the free energy principle, for instance,
I think starts with existence itself, right?
As the, you know, the fact that we exist.
And if you start at that level,
well, then what's the, for living things
and what's the algorithm, maybe that's, is that evolution?
And, you know, I don't know,
is there a way to come at it
from a really high computational level
where then the algorithms could differ, you know,
for instance, and then the implementation level
below that could differ as well
to get to that same whatever the actual
computational level is of living in existence.
This is getting a little too deep, I'm sure,
for the, for the podcast, but.
Well, we, I think that's absolutely right.
I mean, there's some sense in which like
at the very highest level, it's to like do well at life,
right?
Just survive and reproduce the imperatives
of natural selection and so on.
But that, as we were talking about before,
is not very constraining on theories of intelligence,
at least in its most general form.
So we need to make more specific assumptions,
but just to make this a little bit more concrete.
So even if I said, all right, an intelligent system
has to reason according to Bayes' rule,
then there's going to be algorithmic constraints
to grapple with the interactability of that
in any large domain.
And it's perfectly conceivable that a machine,
we could design machine learning algorithms
that excel whatever algorithms humans are using,
because there's no reason to think that evolution
has necessarily selected the best possible algorithm
to solve these problems.
There is no best.
Best is just what is existing at the time, I suppose, right?
Right, well, I mean, yes.
So I'm saying within whatever particular niche
you're looking at, you could ask like,
what is the best way to solve this problem?
And humans might not have achieved that.
So for example, people might engage in some kind of tree
search to solve planning problems,
or they might engage in some kind of like,
variational optimization to solve inference problems.
