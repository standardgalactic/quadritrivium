The problem is that if the space of latent variables,
the hypothesis space is really large,
then implementing Bayes' rule computationally
is totally intractable.
And part of the algorithmic challenge
is to develop approximations of Bayes' rule
that can give approximately correct answers.
And that's where computer scientists
and statisticians drew inspiration
from the solutions that were developed in physics.
In fact, a lot of algorithmic ideas in computer science,
particularly in the domain of probabilistic reasoning,
come from physics originally.
And so the idea was that we can convert Bayesian inference
into an optimization problem.
And the way we do that is we say
that there's some family of possible probability distributions.
And I'm gonna try to pick one distribution in that family
that gets as close as possible
to the true posterior distribution.
Right, a good first guess.
Yeah, well, no, it's not necessarily a first guess.
It's that I'm gonna actually try to search
through this space of distributions
to find the one that gets closest.
Now, the problem there is that if you just state it
in that way, it's just as intractable as the original problem
because you can't compute the similarity
between distributions unless you actually know
both distributions and the whole name of the game
is that we're trying to approximate
the posterior distribution.
So the key algorithmic trick
that variational inference introduces
is to put some constraints on the family.
So for example, if you have a collection
of hypotheses about the world,
like a bunch of different dimensions
of the hypothesis space.
So for example, you're looking at,
you're getting some two-dimensional data
and you're trying to infer both the size
and the weight of some object that you're looking at.
Now, in general, size and weight may be correlated,
but you could make the approximation
that I'm only gonna consider approximations
in which those two things are uncorrelated with each other.
So that's sometimes called a factorization
of the posterior and it's related
to what physicists call the mean field approximation.
So it turns out that if you do that,
then you can simplify the inference problem
and it no longer becomes intractable.
Now, what does this all have to do
with the free energy principle?
So the free energy principle
is a form of variational inference.
When we say that we're converting an inference problem
into an optimization problem,
the thing that we're optimizing is the free energy.
And so there's really no difference
between variational inference
and the free energy principle in general.
And in fact, there's no difference
between those things and the Bayesian brain hypothesis
in general, unless you put some constraints
on the family of distributions,
this variational family as they call it.
So once you start putting constraints,
then the solution to this optimization problem
is no longer identical to the true posterior.
Okay, so that was a really nice introduction,
I think, to the topic here.
So, but you threw around a bunch of technical terms.
So your paper's all about how putting different constraints
on the free energy principle,
how it then matches up with the Bayesian brain hypothesis
and so on.
I mean, is this why the free energy principle
is notoriously difficult to understand
because it's a little slippery in that it's not,
well, to say it's not overly constrained,
maybe in an understatement
because it can go so many different ways.
It seems so plastic with the way
that it can account for different processes.
I mean, is that why it's so difficult to understand?
Well, I think there's a few different factors here.
So one is that you're right
that there are a lot of degrees of freedom
depending on how you implement the free energy principle.
So in this context, as I was saying,
a critical question is how do you parameterize
the distribution, the family of distributions
that you're gonna consider?
And then also how do you search through that space
and actually solve the optimization problem?
Efficiently, yeah.
So it turns out that those different choices
have important implications about things like
predictive coding, which we can talk about.
But I mean, another issue here is that
the papers on free energy principle
are notoriously dense and long and hard to read.
And so I think that even quite technically literate people
have trouble with that.
And so I was trying to clarify some of those issues.
Yeah.
So, I mean, as you were talking about
the Bayesian brain hypothesis,
I immediately jumped six analogies or something
and just thought about the tabula rasa
of a baby being born, let's say, right?
And they come out and obviously they don't have
an infinite number of distributions ready at hand
to then be able to parse them down
and constrain the actual problem
of sensing mom's nipple or whatever.
So, and this actually gets to what we may speak about later
in terms of what deep learning needs in the other paper.
But it's an interesting argument against a tabula rasa
because if you come out as a tabula rasa,
you do have an overwhelming number of possibilities
and you're just stuck with too many options, right?
In other words, your prior is infinite.
So how do you even come up with a posterior, right?
Because how do you choose a prior at that point?
So, it's a really intriguing
and comforting hypothesis
and makes a lot of sense, let's say, common sense to me,
the way to approach how we learn and act.
Yeah, I mean, I would just try to clarify
that the free energy principle is somewhat different
from how we choose a prior.
It's certainly true that learning about the world
is impossible without a prior.
There are theorems stating basically that
if you're totally unconstrained in what you can learn,
then basically you'll never be able to generalize effectively.
But that's a little bit different
from the free energy principle, which in my view,
is really about algorithmic approximations
of problems to get different.
So even after you pin down your priors,
there's still the question of
how do you actually compute with them?
How do you make inferences?
And that's where all the action is in free energy.
That's really what differentiates free energy approaches
from just the Bayesian brain hypothesis writ large.
It's really a subset of possible algorithmic solutions
to Bayesian inference.
Do you want to talk a bit about predictive coding?
I'm not sure that...
Sure.
I mean, I don't think it's not going to be easy
to convey the technical arguments on a podcast.
None of these things are easy, really, on a podcast.
But I would just make the point that,
well, let's start by summarizing
what predictive coding is all about.
So predictive coding has been perhaps
the most influential legacy
of the free energy principle for neuroscience.
And the idea there is that there are ascending
and descending or feed forward
and feedback pathways in the brain.
Most of the time we're talking about cortex.
And the feed forward pathways
are conveying prediction errors.
So discrepancies between observations
and expectations at each level of the hierarchy.
And then the feedback pathways
are conveying those expectations.
Let's just contrast this with sort of a classic view
of the feed forward projections, right?
Classically, you could think of,
well, not classically, I suppose.
I don't know what classic is anymore,
but the feed forward projections
sort of building up a hierarchy of representation,
becoming more and more abstract.
Would you say that's accurate?
Yeah, yeah.
That's one way to think about that.
It doesn't necessarily,
the predictive coding doesn't necessarily exclude
the possibility of some increasing level of abstraction,
but it does make the strong claim,
which there is some experimental evidence for,
that the representation at a particular level
of the hierarchy is suppressed,
that the feed forward pathway is suppressed
if that representation is anticipated.
So for example, there were studies
about a decade and a half ago using Functional MRI,
showing that when you present a collection
of line segments, activity in primary visual cortex,
V1 is suppressed when those line segments
can be understood or interpreted as a shape,
