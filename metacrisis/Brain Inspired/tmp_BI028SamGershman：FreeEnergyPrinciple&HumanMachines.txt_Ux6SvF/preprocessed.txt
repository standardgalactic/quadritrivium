you could build agents that adhere to those principles
and many agents in that set
would be what we consider smart
and many of them would be stupid.
Wait, isn't that what we have already?
No, I'm just kidding.
Yeah, right, exactly.
So I really like what Sherlock Holmes said
that to understand nature,
your mind must be as broad as nature.
That's how I feel, like really the study of intelligence
is that.
This is Brain Inspired.
Hey, everyone.
Welcome to episode 28 of Brain Inspired.
I am Paul Middlebrooks.
I had been thinking of a way to talk
about the free energy principle on the show
and I was gonna have a guest on a few weeks ago,
but we had to reschedule that for a later time.
But I stumbled on a recent paper
by the person you just heard, Sam Gershman,
who is a professor at Harvard University
and runs the Computational Cognitive Neuroscience Lab there
where they study learning, memory, decision making,
you know, cognition.
He's recently written about the free energy principle
and how it relates to other bigger picture theories
of how our brains function,
in particular the Bayesian brain hypothesis
and the concept of predictive coding in general.
So we talk about just that during the first part of the show.
Sam also co-authored a paper a couple years ago now
called Building Machines That Learn and Think Like Humans.
That paper breaks down some of the important ingredients
that current deep learning networks are missing
if we want to get to human-like intelligence.
So we talk about those concepts
and how far along we've come since then.
So those are the two major themes during this show
and of course we discuss much more
like the limits of our own cognition
and how that affects our ability
to understand complicated processes in the world,
including our own intelligence.
And hang on until the end too
because Sam gives some career advice that I love
for those of you early in your career
or even deep in your career,
but trying to figure out your next course of action,
your next direction.
Every new show I do, I realize I have more
and more questions to ask of the guests
to try to bring it all together
and keep some sort of running thread
from the previous guests.
So I try to do that here,
but of course I can never get to all the questions.
You can find out more about Sam
and links to the stuff that we discuss in the show notes
at braininspired.co.
slash podcast slash 28.
And you can also find how to support this ad-free labor
of love of a podcast by finding
and clicking on the red Patreon button at braininspired.co.
Fair y'all, see May,
Ion and Justin did just that this past week.
Thank you guys, that is beautiful.
All right, thank you for listening.
I hope that your projects are coming along smoothly
and you're immersed in and loving whatever it is
that you're pursuing.
Enjoy the show.
All right, Sam Gershman.
Welcome and thanks for being on the show.
Thank you.
So Sam, you run the computational cognitive neuroscience lab
at Harvard and you do lots of things.
For instance, you study how we learn,
you study the different systems that we use to learn
and how they interact,
how our perceptions get transformed
into these internal states in our brains
and how those states can be used to guide our actions.
You study processes like exploration versus exploitation
and plenty more.
Today, we are here to talk about two subjects in particular.
One, the free energy principle,
which is a hot topic these days in brain function
and neuroscience.
In your recent paper,
what does the free energy principle tell us about the brain?
And two, we're gonna talk a little bit
about the concepts introduced in your paper
from I guess two years ago now,
called building machines that learn and think like humans.
You ready?
The free energy principle.
What does the free energy principle tell us about the brain?
So in this paper,
you compare predictions of the free energy principle
with the Bayesian brain hypothesis
and how those two systems might fall out
and compare to each other in this realm
of what's called predictive coding.
Now, I'll just begin by reading the opening paragraph
of a 2009 review by Carl Friston,
who has been the developer of the free energy principle,
just to get us kind of rolling on this topic here.
So he writes,
the free energy principle is a simple postulate
with complicated implications.
It says that any adaptive change in the brain
will minimize free energy.
This minimization could be over evolutionary time
or milliseconds.
In fact, the principle applies to any biological system
that resists a tendency to disorder
from single cell organisms to social networks.
I could go on,
but that's his opening paragraph
to introduce the concept of the free energy principle.
Now, I'll let you maybe comment on this.
So, Sam, what is the free energy principle?
Yeah, so I should preface this by saying
that I'm not either an advocate
or an antagonist of the free energy principle.
I've just noticed over the years
that through many conversations with colleagues
that there's widespread confusion
about what exactly the free energy principle is
and what it predicts.
And so that was the motivation for me
to enter into this fray.
And I should say that the free energy principle
is closely connected to ideas that I've proposed
and have been deeply interested in.
So I think the real question here is,
what is really special about the free energy principle
relative to other ideas
that have been out there for a while?
And I can elaborate on that.
So in essence, the free energy principle
is the idea that the brain minimizes surprise.
Mathematically, it's a little bit more complicated than that,
but that really gives you the gist of it.
Now, the term free energy is a little bit unintuitive
for a lot of people
because it actually comes from physics.
The idea was that you can decompose
a complicated inference problem
into an inequality that expresses the relationship
between what physicists call the partition function
and what they call the variational free energy.
And it turns out that the object
that the physicists were studying
is closely related to problems
in machine learning and statistics
where probabilistic inference problems are intractable
and they can be made more tractable
by turning them into an optimization problem
under constraints.
So what does that mean?
And this actually is perhaps usefully introduced
by first talking about the Bayesian brain hypothesis
and then talking about
where the free energy principle enters into that.
So the Bayesian brain hypothesis states
that the brain is confronted
with ambiguous sensory evidence.
And it tries to interpret that sensory evidence
by making inferences about the hidden causes
that produced the sensory data.
So for example, on our retina,
we get a two-dimensional projection
of a three-dimensional world,
which means that the two-dimensional information
is fundamentally ambiguous.
There's no unambiguous mapping
from the 2D into the 3D.
Nonetheless, we're able to perceive three-dimensional structure
and the way we do that is we combine the noisy
or ambiguous sensory information
with prior beliefs about the nature of the world.
So for example, that objects have surfaces
that are smooth and continuous, right?
We can exploit those prior beliefs
to make inferences about the hidden structure of the world.
And the Bayesian brain hypothesis is really that
the brain is using Bayes' rule
to convert its prior beliefs into its posterior beliefs.
The posterior beliefs are the beliefs
about hidden variables in the world,
given the sensory data or other kinds of information
that the brain has received.
Now, that sounds very simple.
And if you just write down Bayes' rule,
it's actually mathematically extremely simple,
and it's a direct consequence of the axiom
of the probability theory.
The problem is that if the space of latent variables,
the hypothesis space is really large,
then implementing Bayes' rule computationally
is totally intractable.
And part of the algorithmic challenge
is to develop approximations of Bayes' rule
that can give approximately correct answers.
And that's where computer scientists
and statisticians drew inspiration
from the solutions that were developed in physics.
In fact, a lot of algorithmic ideas in computer science,
particularly in the domain of probabilistic reasoning,
come from physics originally.
And so the idea was that we can convert Bayesian inference
into an optimization problem.
And the way we do that is we say
that there's some family of possible probability distributions.
And I'm gonna try to pick one distribution in that family
that gets as close as possible
to the true posterior distribution.
Right, a good first guess.
Yeah, well, no, it's not necessarily a first guess.
It's that I'm gonna actually try to search
through this space of distributions
to find the one that gets closest.
Now, the problem there is that if you just state it
in that way, it's just as intractable as the original problem
because you can't compute the similarity
between distributions unless you actually know
both distributions and the whole name of the game
is that we're trying to approximate
the posterior distribution.
So the key algorithmic trick
that variational inference introduces
is to put some constraints on the family.
So for example, if you have a collection
of hypotheses about the world,
like a bunch of different dimensions
of the hypothesis space.
So for example, you're looking at,
you're getting some two-dimensional data
and you're trying to infer both the size
and the weight of some object that you're looking at.
Now, in general, size and weight may be correlated,
but you could make the approximation
that I'm only gonna consider approximations
in which those two things are uncorrelated with each other.
So that's sometimes called a factorization
of the posterior and it's related
to what physicists call the mean field approximation.
So it turns out that if you do that,
then you can simplify the inference problem
and it no longer becomes intractable.
Now, what does this all have to do
with the free energy principle?
So the free energy principle
is a form of variational inference.
When we say that we're converting an inference problem
into an optimization problem,
the thing that we're optimizing is the free energy.
And so there's really no difference
between variational inference
and the free energy principle in general.
And in fact, there's no difference
between those things and the Bayesian brain hypothesis
in general, unless you put some constraints
on the family of distributions,
this variational family as they call it.
So once you start putting constraints,
then the solution to this optimization problem
is no longer identical to the true posterior.
Okay, so that was a really nice introduction,
I think, to the topic here.
So, but you threw around a bunch of technical terms.
So your paper's all about how putting different constraints
on the free energy principle,
how it then matches up with the Bayesian brain hypothesis
and so on.
I mean, is this why the free energy principle
is notoriously difficult to understand
because it's a little slippery in that it's not,
well, to say it's not overly constrained,
maybe in an understatement
because it can go so many different ways.
It seems so plastic with the way
that it can account for different processes.
I mean, is that why it's so difficult to understand?
Well, I think there's a few different factors here.
So one is that you're right
that there are a lot of degrees of freedom
depending on how you implement the free energy principle.
So in this context, as I was saying,
a critical question is how do you parameterize
the distribution, the family of distributions
that you're gonna consider?
And then also how do you search through that space
and actually solve the optimization problem?
Efficiently, yeah.
So it turns out that those different choices
have important implications about things like
predictive coding, which we can talk about.
But I mean, another issue here is that
the papers on free energy principle
are notoriously dense and long and hard to read.
And so I think that even quite technically literate people
have trouble with that.
And so I was trying to clarify some of those issues.
Yeah.
So, I mean, as you were talking about
the Bayesian brain hypothesis,
I immediately jumped six analogies or something
and just thought about the tabula rasa
of a baby being born, let's say, right?
And they come out and obviously they don't have
an infinite number of distributions ready at hand
to then be able to parse them down
and constrain the actual problem
of sensing mom's nipple or whatever.
So, and this actually gets to what we may speak about later
in terms of what deep learning needs in the other paper.
But it's an interesting argument against a tabula rasa
because if you come out as a tabula rasa,
you do have an overwhelming number of possibilities
and you're just stuck with too many options, right?
In other words, your prior is infinite.
So how do you even come up with a posterior, right?
Because how do you choose a prior at that point?
So, it's a really intriguing
and comforting hypothesis
and makes a lot of sense, let's say, common sense to me,
the way to approach how we learn and act.
Yeah, I mean, I would just try to clarify
that the free energy principle is somewhat different
from how we choose a prior.
It's certainly true that learning about the world
is impossible without a prior.
There are theorems stating basically that
if you're totally unconstrained in what you can learn,
then basically you'll never be able to generalize effectively.
But that's a little bit different
from the free energy principle, which in my view,
is really about algorithmic approximations
of problems to get different.
So even after you pin down your priors,
there's still the question of
how do you actually compute with them?
How do you make inferences?
And that's where all the action is in free energy.
That's really what differentiates free energy approaches
from just the Bayesian brain hypothesis writ large.
It's really a subset of possible algorithmic solutions
to Bayesian inference.
Do you want to talk a bit about predictive coding?
I'm not sure that...
Sure.
I mean, I don't think it's not going to be easy
to convey the technical arguments on a podcast.
None of these things are easy, really, on a podcast.
But I would just make the point that,
well, let's start by summarizing
what predictive coding is all about.
So predictive coding has been perhaps
the most influential legacy
of the free energy principle for neuroscience.
And the idea there is that there are ascending
and descending or feed forward
and feedback pathways in the brain.
Most of the time we're talking about cortex.
And the feed forward pathways
are conveying prediction errors.
So discrepancies between observations
and expectations at each level of the hierarchy.
And then the feedback pathways
are conveying those expectations.
Let's just contrast this with sort of a classic view
of the feed forward projections, right?
Classically, you could think of,
well, not classically, I suppose.
I don't know what classic is anymore,
but the feed forward projections
sort of building up a hierarchy of representation,
becoming more and more abstract.
Would you say that's accurate?
Yeah, yeah.
That's one way to think about that.
It doesn't necessarily,
the predictive coding doesn't necessarily exclude
the possibility of some increasing level of abstraction,
but it does make the strong claim,
which there is some experimental evidence for,
that the representation at a particular level
of the hierarchy is suppressed,
that the feed forward pathway is suppressed
if that representation is anticipated.
So for example, there were studies
about a decade and a half ago using Functional MRI,
showing that when you present a collection
of line segments, activity in primary visual cortex,
V1 is suppressed when those line segments
can be understood or interpreted as a shape,
as a coherent three-dimensional shape,
and concomitantly, activity in the lateral occipital complex,
which is known to be shape sensitive increases.
So there's a decrease in activity in V1
and an increase of activity in LOC.
And that was interpreted as arising
from this predictive coding framework
where because the shape representation
could predict the underlying line segments,
then the line segment representations become suppressed
because now they're predictable.
Gotcha.
Now, what does this have to do
with the free energy principle?
Well, it turns out that you can derive
this kind of architecture from the free energy principle,
but the catch is that you have to make
a whole bunch of assumptions that have to do with
the variational family.
So what space of distributions are you going to constrain
the inference problem to?
You have to make assumptions about ways
in which you're gonna approximate the free energy itself
because it turns out that under those chosen specifications,
you can no longer actually analytically compute
the free energy.
So anyway, the point is that there's a bunch of ingredients
that have to work well together
in order to produce predictive coding.
It's not a generic consequence
of the free energy principle as is sometimes conveyed.
Yeah, so these are the things that you actually explore
deeper in the paper, so.
Yeah, yeah.
And of course, the paper's online,
I think it's archive, right?
Yeah, so I mean, I'll link to it,
and pretty soon it'll be in science, I'm sure.
So Sam, what do you think?
Like the Bayesian brain hypothesis
and the free energy principle can both be seen
as these encompassing theories
of the brain function in general, right?
And how we aim to understand our minds
and intelligence in general.
Do you think that we'll ever have
like a single account of our brains and our minds?
Well, I think that the Bayesian brain hypothesis
and free energy principle,
they are good candidates for some general principles,
but they're not really sufficient by themselves, right?
They're too general in some sense.
They're too powerful in order to act.
Basically, you could build agents
that adhere to those principles,
and many agents in that set would be
what we consider smart,
and many of them would be stupid.
Wait, isn't that what we have already?
No, I'm just kidding. Yeah, right, exactly.
But my point is that if you want to pin things down
sufficiently that you could say,
all right, this kind of architecture,
these kinds of constraints,
these principles are only going to produce smart things
or tend to primarily produce smart things,
then you need to make more assumptions
than just that the brain is Bayesian
or minimize the surprise.
I mean, thinking about that,
do you think it's a better approach?
Of course, both approaches together
is probably the best path,
but a better approach to go to start overly constrained
or to start without enough constraints,
to start with something like the free energy principle,
and then start whittling away with constraints.
You know what I mean?
What approach is a favorable approach?
Well...
There's a balance, I know.
Yeah, I mean, it's important to make a distinction
between the engineering goals
of building artificial intelligence
and the scientific goals of trying to parse
what makes humans or animals intelligent.
Obviously, those things will converge in some ways,
but the scientific strategy is different
in the sense that we want to design controlled experiments
where we can isolate a particular hypothesis and test it.
That's different from engineering approaches,
which typically aim to construct something
that does something useful,
and whether or not some particular component of it
has been isolated is not that important.
And particularly in the era of building
very large deep neural network architectures
that don't have a lot of interpretability,
but can still accomplish certain kinds of tasks,
it's, you know, the strategy,
the research strategy is different.
Well, speaking of building things,
maybe we could transition here and talk about deep learning
in the state of AI and what it has,
what it needs, what it's missing.
So this is your paper,
Building Machines That Learn and Think Like People.
And this is in behavioral and brain sciences.
And this is a thorough paper
because it's in behavioral and brain sciences
and made even more thorough by the format.
So just, I mean, people know this,
but there's a target article which you guys wrote,
and then there's a bunch of reviews,
and then you get to rebut the reviews,
and this is all in like a single document
and it costs about a three and a half
if you really wanna print the whole thing out, you know?
So I've always liked that format,
but I've not written one like that.
And it's a lot of work just to get reviews.
I'm, one of my papers is under review right now,
and you know, just with three to four peer reviewers,
it's a lot of work just to rebut that and send it back.
But it seems like a lot more work for you guys.
I mean, what was that experience like?
It's all kind of a blur to me now.
I know, that's the other thing is
if it was written last year,
you probably did the work 12 years ago, you know, so.
Yeah, yeah.
No, it was a lot of work, but it was fun to write.
You know, it was great writing that paper
with those co-authors,
because these were ideas that we had talked about
for a long time and it finally forced us
to really coalesce them.
Yeah, yeah.
Okay, so I had Melanie Mitchell on the show
a few weeks ago and she's a complexity expert
and she's been in the field of AI for a long time.
And she had written this op-ed in the New York Times.
Yeah, I read that.
Oh yeah, okay.
So here's just a quote from like her approach to this.
Today's AI systems sorely lack the essence
of human intelligence,
understanding the situations we experience,
being able to grasp their meaning.
The mathematician and philosopher,
Giancarlo Rota famously asked,
I wonder whether or when AI will ever crash
the barrier of meaning.
And so there's this phrase, the barrier of meaning.
And this paper that you co-authored
takes that sort of broad statement
and breaks it down into cognitive components
that could help bridge that gap.
So it's really an initial roadmap of sorts
and has seemingly served as such,
which must be pretty rewarding for you
to have seen.
So maybe we can frame this this way
and maybe we can just start off talking
about how machines think and learn currently.
And then maybe bring in how humans think and learn
what we know about that
and then talk about them together.
So the premise of the paper is like,
look, yeah, we've done great work with AI
and deep learning has done amazing things,
but it's still not human-like.
And if we want to make it human-like,
we need to build in these other components,
these higher level components,
either build them into the deep nets
or however you're gonna perform the AI,
build these core ingredients in.
Do you wanna just talk for a moment
about the current state of how machines think?
Yeah, well, it's quite hard
to make a really general characterization
of how machines think
because there's 100 papers posted on archive every day
machine learning.
It's a rapidly moving target.
And our paper was really focused
on kind of general patterns, right?
So it's not the case that our critique
skates every machine learning paper
that was ever written.
But these are debates that have been going on
for a long time and we discussed this history
a little bit in our paper, I mean,
since the 70s and 80s.
But by the way, it's a beautiful paper.
I really enjoyed it.
I bet you've heard from a lot of people the same as well.
Thank you.
Yeah, so people have been going back and forth
about the necessary ingredients
for intelligence for a long time.
And I sympathize with computer scientists
in some ways because there's this feeling
of moving goalposts, like there was a time
when people said, you know,
when a computer can win at chess,
that's when we know it's really intelligent.
And then if it wins a go, then it's really intelligent.
And it's a sort of moving goalpost.
And, you know, I'm sure some people wonder,
like, when are you gonna be satisfied?
And I'm not really a big believer
in having a kind of single test of that sort.
Because I just think that
human intelligence is too multifaceted
to really be boiled down in that way.
But that being said, I do think that it's important
for us to think about what we've learned
from the study of cognitive science
and what that tells us about the nature of human intelligence
and the limitations of current AI approaches.
So with that preface in mind,
I can kind of make some broad stroke characterizations.
The basic thrust of modern machine learning approaches,
particularly deep learning approaches,
has been to construct systems
that are extremely expressive
in the sense that they can learn
lots and lots of different things.
So we could think about this
as a kind of function approximation problem.
Like, you have a function that maps images
to object categories.
And that's one way to think about
an object recognition system.
Or you have a translation system
that's mapping text in one language
to text in another language.
So these are all potentially really complicated functions.
And the general approach has been
to build very complicated neural network architectures.
Now I should say they're not conceptually complicated.
You can basically draw a picture of them
and it's pretty straightforward.
You have a bunch of neurons
that are connected to each other in layers.
But they're complicated in the sense
that they're very expressive.
They have lots of degrees of freedom,
possibly millions of parameters
that then get optimized by a learning algorithm.
And...
So you could take the same network,
same naive network or whatever
and train it on a bunch of different types of domains.
So you could use this thing
to train it on recognizing objects.
You could use this same network
to train it on language
or something within the realm.
There's a lot of possibilities to train it.
Yeah, so just to maybe put this in perspective,
before the heyday of deep learning,
the most successful approaches
used collections of hand engineered features.
So people would put a lot of thought
into defining various kinds of image
or natural language features depending on the domain.
And then they would assume a fairly simple
supervised learning algorithm
that operated on those features.
So for example, you might assume
that you take this collection of features,
you map them through a linear function,
you map them through a linear classifier,
and then you basically train the weights
of that linear classifier.
So that's a very low complexity solution
in the sense that the space of possible functions
that can be learned is relatively small, right?
So you can only learn linear functions of this feature set,
contrast that with a deep neural network
where all the different parameters
and all the layers are being optimized
to solve this classification problem.
That's what's commonly called end-to-end training.
And now you have millions of parameters
and the function space is enormous
because now you don't necessarily,
you're not necessarily restricted anymore
to linear functions.
You could learn linear functions,
but you could learn all sorts of non-linear functions.
So you might think, okay, well, that's great.
Now we can learn more functions, right?
But the catch is that the more complex
your function class is,
the more data you need to effectively learn it
in the sense that if you only have a small amount of data,
then you could learn to model those data very well, right?
If you have a very large function class,
you can fit the training data really well,
but the problem is that there may be noise
in that training data
and the training data might not be representative
sufficiently of all the test cases you might encounter.
And so your classifier is gonna fail to generalize.
And there's a whole literature
that rigorously analyzes this
from a statistical learning perspective.
It's called the study of sample complexity.
How much data do you need to effectively learn,
to effectively generalize?
And in general, more complex models need more data.
And that's why actually,
in some sense the deep learning revolution
has been just as much about big data revolution
in the sense that we've constructed bigger
and bigger data sets to feed the insatiable
sample complexity needs of these learning algorithms.
Plus the computational power to do so.
Yes, absolutely.
Yeah, and now it's the other thing.
So yeah, big computation is a big contributor.
In some sense, the deep learning architectures
we're using today are pretty similar
to the ones that we had in the 90s,
but they work better because we have a lot more data
and a lot more computing power.
That being said, it is still somewhat of a mystery
to learning theorists why these things work
because at first blush, the learning theories
would say that you would need even more data
than we have to effectively generalize.
But learning theorists are trying to crack open that nut
and that it's probably not worth getting into that right now.
But the point here is that the approach today
is about building really flexible function approximators.
Now that doesn't sound so sexy from the perspective
of artificial intelligence to boil them down
to nonlinear function approximation,
but that's really what it's all about.
Now as I was saying, the architecture
of these networks is really flexible,
but it also means that that very flexibility
allows the system to learn things
that we might think of as not being the right answer.
So it's gonna pick up all sorts of patterns,
but it might fail to learn the same kinds of things
that humans learn and it might not learn
those things in the same way.
So let me try to elaborate on what I mean by that.
So I was talking about complexity
in terms of the number of parameters
and the size of the function class,
but another way to restrict the effective complexity
of a machine learning system is to have an inductive bias.
So it says that the inductive bias says that,
or I couldn't in theory learn any function
in this function class,
but I have a preference a priori
for certain functions over others.
So I think that some functions might be more plausible
than others and I can formulate that
in the language of probability theory
and that shapes the sample complexity arguments
because now, even if I have a really large function class,
if I have very strong inductive biases,
that can kind of compensate for the need
to experience a lot of data.
The inductive biases basically say that
some things are gonna be easier to learn with less data
and other things are gonna be harder to learn
and will require more data.
And how does that compare to having stronger priors?
Those are priors, right?
So inductive biases are priors.
I mean, you can construct,
depending on what your kind of formal framework is,
you can talk about inductive biases in a non-Basian way,
but I often think it's helpful to think about prior distributions
as a particular way of representing inductive biases.
I see, okay.
So now, so let me try to loop this back to human cognition.
Human cognition, we think that humans can learn
from small amounts of data precisely
because they have very strong inductive biases.
And they also have inductive biases
that are relatively abstract,
which allows them to generalize their knowledge very widely.
So let me just talk about some of those inductive biases.
Actually, before I talk about inductive biases,
let me just try to make vivid
why you need an inductive bias at all.
There's a great book by Eric Baum called What is Thought?
And he has the following example.
So imagine I'm training a chair classifier.
So I'm giving it images and it's telling me chair
or not chair, all right?
So I give it some finite number of these training images
and it learns to perfectly classify,
to perfectly discriminate chairs from non-chairs.
All right, now I'm gonna give it a new chair
and I want it to generalize to that.
Now let's imagine that it had no inductive bias, okay?
The implication of having no inductive bias
is that in fact, this classifier,
which could perfectly discriminate between chairs
and non-chairs in the training set,
has no basis of generalizing to this new image
that in fact, it will be completely uncertain
about it would have no basis of knowing
whether this new image is a chair or a non-chair.
And the reason is because in a continuous function space,
there are an infinite number of functions
that separate the chairs from the non-chairs.
And that means that there's an infinite number of functions
that would perfectly separate chairs from non-chairs
and classify this new image as a chair.
And then there's also an infinite number of functions
that perfectly separate chairs from non-chairs
and classify this new image as a non-chair, okay?
And what it means to have no inductive bias
is that you have no preference about which of those functions
is the correct function.
So generalization depends crucially
on having some inductive bias.
Now neural networks do have inductive biases.
The architecture of the network, the parameterization
and so on, those are all forms of inductive biases,
but they're just relatively weak inductive bias.
Now we think that humans have strong inductive biases.
They have strong inductive biases about things like
causality about objects, about relations between objects,
about compositionality of knowledge,
so that the idea that you can build complex representations
out of simpler ones.
And we have strong inductive biases in different domains
that take the forms of something like an intuitive theory,
like an intuitive theory of psychology
that lets us make inferences about other people's minds
or an intuitive theory of physics
that allows us to make inferences
about physical interactions in the world.
Anyway, I'll stop there and let you ask it.
Well, no, that's okay because you're sort of rattling off
the ingredients that are proposed in this paper.
So, and you just got there naturally,
which is a beautiful thing.
Yeah, so I was gonna ask about how humans learn, right?
And in comparison, which is where you've really
just brought us to anyway, but before we even get into
just a few maybe of the examples
of these things that you just listed,
it's an interesting conundrum
whether we understand how humans learn well enough to then,
so the whole goal is to build systems
that learn and think like humans.
But do we know how humans learn and think well enough
to even begin doing that?
And so these are things, like when you say
terms like intuitive physics, of course,
well, let's just maybe put that question on hold
unless you wanna answer it.
And then we can kind of step through
a few of the ingredients.
Well, no, I mean, I don't think that we know enough
about human cognition that we can just implement
human intelligence in a computational system.
It's more just that we have some general hints
and also some specific computational models
about how humans think.
And those are in interesting ways different
from a lot of the prevailing deep learning architectures.
Now, I will say that even at the time
that we were writing that behavioral and brain sciences
article, people were already starting to realize this.
So for example, in the domain of intuitive physics,
there was a lot of interest in building machine learning
systems that solved intuitive physics problems.
And if you build them in a kind of naive
sort of off the shelf way, they didn't work very well.
And they only started to work really well
when people started building in constraints.
And I would argue that a lot of the constraints
that worked well were precisely the constraints
that we think people have.
So for example, DeepMind and other groups
have developed models in which there are primitives
that correspond to objects and there are functional
interactions between the objects.
So it's a relational model that's object oriented.
And then you use deep learning to learn the parameters
of those functional interactions.
And that works much better for modeling intuitive physics.
So I would argue that that's kind of harnessing
the strengths of deep learning, which are really
about flexible function approximation
with the kinds of inductive biases
that we think humans have.
Okay, so let's just step through these.
So there are five core ingredients.
Is that the latest count?
I mean, this is sort of the count in the paper.
Uh-huh, yeah.
So one of them is this intuitive physics
that you're just talking about.
And this is the idea of, so this falls
under the rubric of when we're really young,
we're kind of born with and have really early on
this developmental software, right?
So that's sort of ingrained or an inductive bias, right?
So intuitive physics is the idea that we understand
that when one cue ball hits,
when one pool ball hits the other billiard ball,
we understand that there's a collision there.
These two things are solid and one affects the other.
Well, it's a lot more than that probably,
but yeah, that's one aspect of it.
Yeah, that's one aspect of it.
So the things are solid, not a liquid and so on and so on.
So this is one thing that is sort of missing
in deep learning networks that you were just describing.
And there's work being done on all of this.
Like I said, this is a nice roadmap
that people have seemingly been following actually.
So another one of those that falls
under the rubric of developmental software
is the intuitive psychology.
Like you were just, that you mentioned,
do you wanna just describe what that is briefly?
Yeah, so maybe just first for those people
who are not familiar with the notion of intuitive theory,
the reason that they're called intuitive theories
is to contrast them with scientific theories.
And the argument is that intuitive theories
and scientific theories share a lot
of the same structural characteristics,
but obviously they differ in content
in the sense that intuitive psychology
doesn't have access to functional MRI
and response time studies and so on, right?
But we still can build a theory-like representation
of another person's mind.
For example, we can reason about
another person's beliefs and desires
and combine our knowledge of those beliefs and desires
to, for example, make predictions
about what that person's gonna do.
Another example would be when kids are shown
kids can discriminate between whether
an acting agent is helping another agent
or hindering the other agent.
And so, yeah, there are these psychological aspects
of other agents that can easily be inferred
with this sort of developmental startup software.
Yeah.
Okay, and then the rest of the other three ingredients
kind of fall under what you describe
as model building elements.
And one of them is this comp,
and you've listed all these already,
you've mentioned these in passing.
So one is compositionality.
And this is basically where you can build new things
from simpler parts.
And so deep networks, and let's just run through
these real quick and then we can kind of talk about
them more broadly maybe.
So deep networks are not great at this.
Another is learning to learn or meta learning.
And so there's actually a lot to talk about
with this probably.
And causality is the last, the fifth ingredient
and building causality.
And I talked about this last episode a little bit
with Conrad Kurding and Juana Maronescu, his wife,
because they just wrote a paper on how to infer
causality in quasi experiments.
So it's a little different.
Okay, so like I said, this has been a roadmap
and it's like the paper was written
and then meta learning work immediately came out
of DeepMind and then, I don't know if you recently,
I didn't get a chance to read the paper,
but there's this recent paper out of DeepMind as well
that is using the meta reinforcement,
their meta reinforcement learning framework
to build in causality to.
Yes, it was written by my student.
Oh, is that right?
Issue to the scooper, yes.
Well, I should have.
A summer internship at DeepMind.
Okay, very good.
Yes, it's funny because in the paper
and one of the rebuttals from the people at DeepMind,
they were like, maybe we are on a converging path here
and so I suppose you are.
No, no, there's definitely points of convergence.
Not completely, but I mean, I talk to those people
on a regular basis, so we're not like,
in conflict or anything.
Oh, of course not.
We're all going for the same goal anyway.
So I don't know if any of those
that you wanna really elaborate on as something that,
you know, I'm kinda curious your take
on how the paper has continued to be received over time
and how people, you know, if you've seen a lot of progress
in all of these domains
or what's really still lacking, et cetera.
Yeah, I think that there's been a ton of progress.
I think that as I was saying before,
many people have begun to acknowledge
that you need to build in some constraints
to get these systems to work appropriately.
Now, we're certainly not there yet
in a lot of practical areas of application.
You know, for example, we still, I think,
can't get systems to make reasonable captions for images.
They just, the fact of the matter is
they just don't really understand
what's going on in the image.
The barrier of meaning.
But they make reasonable captions, you know,
five out of seven times maybe now.
Is that a good?
I don't know, I mean, not in my experience.
Okay, well, yeah, yeah.
Anyway, but the point is just that these systems
can sometimes fool us into thinking
that they know more than they really do.
And yeah, so I'm encouraged by things
like the paper you mentioned,
meta-learning for causal inference.
I think that those kinds of approaches
are definitely on the right track.
I don't know to what extent the particular,
basically the thing that I wonder about
is to what extent building, exploring in the space
with kind of different neural network contraptions,
putting in different kinds of widgets
and so on is gonna get us to intelligence.
Sometimes it feels a little bit like
building a ladder to the moon.
Doing more of the same that didn't get us there initially.
Yeah.
Well, it's just, I mean, certainly,
there's no doubt that there's been a huge amount of progress,
but it also, I get the sense reading these papers
that we're kind of swirling around a local optimum.
And every once in a while,
we kind of pop out of that and go a little bit farther.
The developments don't seem of the sort
that are really getting a significantly closer
to human intelligence.
So it's just, to give one example,
think about AlphaGo, AlphaGo is
an amazing accomplishment.
I love, everyone has to always preface it
with how amazing it is.
And then, yeah, yeah.
Well, it's an amazing accomplishment.
There's no doubt about that.
But just think about what happens
at the end of a Go game, just shut it down
because AlphaGo can only play Go.
And that's in some sense true
of almost all the models that we have now.
And even within the space of, let's say, video games,
so we have algorithms that can play video games,
but you can make changes to those games
that would be completely trivial to a human player
and yet would be catastrophic to these algorithms.
Yeah.
So we're really far away, I think,
in the sense of capturing that human-like flexibility.
And I think it's because we haven't fully embraced
the principles of causality and compositionality
and intuitive theories and so on.
I think, I guess my general impression is that
we have this really useful tool
that we can build these gadgets out of neural networks
that do things, and if you can feed it enough data,
they can classify images or recognize speech.
But it's tempting to kind of over-apply those things
as a substitute, but really thinking hard
about the structure of a problem.
But it'd be fair to say that it's sort of a change
in quantity when you're adding these sorts of widgets,
et cetera, to the deep learning networks and not quality.
Yeah, yeah, I think that most of the advances
that we see on a day-to-day basis
are about getting a few more decimal points
score on some benchmark data set,
but you're not really,
I think the real contribution of cognitive science
is to get people out of the thinking in terms of,
get people, move people away from thinking
in terms of these benchmarks and,
or rather thinking more broadly
about what the benchmarks really are.
What is it that you want the system to do?
It's not just about playing go or playing video games
or recognizing objects.
There's some set of competences there
that transcend those individual tasks.
And if you only kind of play to win those tasks,
then you're gonna be missing something.
So we need to figure out where the hell
we actually want the goalposts and pour concrete
and then make them stick there, huh?
Yeah, yeah, yeah.
So I ask this to a lot of my guests.
So I'll ask you as well,
is emulating the human mind,
is that, you know, is emulating human thought and learning?
Is that the only way to get there, you think,
to general AI, to something that we would satisfy,
that would satisfy our notion of what general AI is?
Well...
Can I tell you what you're gonna say before you say it?
Yeah, go for it.
You're gonna say, well,
it's the only example that we have.
It's the best example.
It's the most complicated, you know,
the brain's the most complicated thing in the universe.
Okay, now go ahead and repeat what I said
and I'll be satisfied.
No, no, I wasn't gonna say that, actually.
I think it depends on what you mean by general intelligence
because it could potentially be a tautology
if we think that what we mean by general intelligence
is whatever we think humans do,
then that's only gonna be satisfied
when we build machines that think like humans.
But we might not, right?
Maybe we'll eventually come to some way of thinking
about intelligence that's really different from our own.
It's just, it's very hard to get to that point
because inevitably we project our own notions
of intelligence onto our machines.
Of course.
But I mean, one way to approach this potentially
is from, you know, the computational level.
So we can always talk about David Maher
and the computational highest in level.
And where would you start?
Like the free energy principle, for instance,
I think starts with existence itself, right?
As the, you know, the fact that we exist.
And if you start at that level,
well, then what's the, for living things
and what's the algorithm, maybe that's, is that evolution?
And, you know, I don't know,
is there a way to come at it
from a really high computational level
where then the algorithms could differ, you know,
for instance, and then the implementation level
below that could differ as well
to get to that same whatever the actual
computational level is of living in existence.
This is getting a little too deep, I'm sure,
for the, for the podcast, but.
Well, we, I think that's absolutely right.
I mean, there's some sense in which like
at the very highest level, it's to like do well at life,
right?
Just survive and reproduce the imperatives
of natural selection and so on.
But that, as we were talking about before,
is not very constraining on theories of intelligence,
at least in its most general form.
So we need to make more specific assumptions,
but just to make this a little bit more concrete.
So even if I said, all right, an intelligent system
has to reason according to Bayes' rule,
then there's going to be algorithmic constraints
to grapple with the interactability of that
in any large domain.
And it's perfectly conceivable that a machine,
we could design machine learning algorithms
that excel whatever algorithms humans are using,
because there's no reason to think that evolution
has necessarily selected the best possible algorithm
to solve these problems.
There is no best.
Best is just what is existing at the time, I suppose, right?
Right, well, I mean, yes.
So I'm saying within whatever particular niche
you're looking at, you could ask like,
what is the best way to solve this problem?
And humans might not have achieved that.
So for example, people might engage in some kind of tree
search to solve planning problems,
or they might engage in some kind of like,
variational optimization to solve inference problems.
But the particular way in which they do that
might not actually be the optimal way.
And we could design machine learning algorithms that
do better than humans.
Now, they're designed to achieve the same goals
that humans want to achieve.
And the overarching architecture of that system
is similar, but it's doing it differently, right?
So that's a concrete way in which it wouldn't even
make sense at that point to demand
that the machines adhere slavishly to.
It would make no sense.
What humans are doing, right?
Yeah, yeah.
So I mean, the reason, I told you what you're going to say.
I mean, that was a joke, obviously, but so many people
have said that they can't imagine another way.
And my intuition is that, wow, if you ask 100 experts,
and they all say, no way, we have to do it this way,
it must be the other way.
You know?
It must be another way.
Well, I can't imagine another way,
but I can imagine there being another way.
In the same way that I was very struck by,
I think this has very deep implications
for how we think about ourselves
and our own thought processes.
So I attended a dinner once,
and I talked to this geologist
who was working on earthquake prediction.
And he made the very vociferous point
that he felt that the classical education in geology
had ruined generations of geologists.
Okay.
Because the classical models
simply could not accurately predict earthquakes.
Like tectonic plate shift models, or?
This is what he said.
I'm not a domain expert.
And he didn't really go into the details of why that was.
That you could have sound geophysical principles
that were not actually useful at making predictions.
And he argued that this whole thing was blown wide open
when people started to use deep learning
to predict earthquakes.
And he felt that whatever it is
that we thought we knew about geology,
even if it was right,
maybe there are more correct theories of geology
that are expressed in some inscrutable language
that the deep learning system can discover.
And eludes us.
And I have a hard time swallowing that,
but at the same time,
you have to acknowledge the possibility
that it's our hubris as humans
that convinces us that the nature of the universe
has to be expressible in the language
that we have at our disposal right now,
in the scientific language in current use.
I mean, this is something that historians of science
have known for a long time
that our theories are strongly latched
to our description languages.
And even our basic observations
can't be described in a theory-independent way
when our theories change, our observations change.
We actually see things differently.
And so that's reason for humility
in terms of privileging our notions of intelligence.
Right.
Well, in that same vein,
I had John Krakauer on the show a few weeks ago,
and we were talking about different levels of explanation.
And we talked about reductionism
and the proper level of understanding phenomena.
And there are lots of higher cognitive functions
that it may not make sense
to try to understand on the level of neurons
and neural circuits, right?
But instead on the level of some behavioral function
and so on.
And the core ingredients
that you guys mentioned in your paper
confer like these advantages, right?
They make humans so awesome in various ways.
And I'm wondering, could there be disadvantages too?
There have to be, right?
That's the yin and yang of a different type.
Yeah, so that's what I was thinking too.
It's like, let's say it is possible,
we can map some of these higher level cognitive functions
to the level of neural circuits.
Are we humans, but because we need to reduce complexity
using things like compositionality and intuitive physics,
are we incapable of conceiving
how that mapping could occur?
Whereas a deep learning network or some sort of AI system
without these human skills, quote unquote, or limits
might by brute force figure it out
in a way that we could eventually be satisfied
with that kind of explanation.
Does that make sense?
Yeah, so I wouldn't say that people are incapable
of doing this since historically we've been very capable
of constructing various kinds of cognitive prostheses
that allow us to understand things
that seemed mystifying before.
That being said, it's certainly a logical possibility
that neural networks could discover these theories
maybe more easily than us
because they have different inductive biases than us.
I certainly think it's within the realm of possibility
to think about neural network scientists
that are as different from us as we are from dogs and cats.
Now, but that being said,
I think my experience in studying these systems
is that in some sense there's always less than meets the eye
that we have this strong inclination
to kind of anthropomorphize these systems
and attribute to them psychological competences
that they lack.
And when you start to really test these rigorously,
they fall short.
I mean, this is very clear in the domain, for example,
of natural language where most natural language processing
systems simply don't recover the same kind
of semantic understanding of the world that humans have.
It's not that hard to demonstrate this.
So I just think that while it's a logical possibility,
I think we need to be doing more
than just training neural networks on ImageNet
or even data sets on reasoning from natural language,
I think are too impoverished.
But see, it's really a privilege to be able to be
at the point where we can say that.
And there have been these advances that are amazing.
Now we can say it's not enough.
That's great.
I think that's a great place to be in.
So I can imagine lots of different scenarios
for this path toward fuzzily
what we could call general AI.
And reading your paper brought just one to me,
this one scenario.
And as soon as you write this in a science fiction book,
it means it's gonna be completely wrong.
But so I can imagine, and the way I sort of envision it
is what we're building toward is an AI system
that comes off the shelf or out of the box,
and it's ready to solve new problems
and learn new things efficiently
and have some of these built-in ingredients
that are proposed in the paper.
I can also imagine a scenario though
where we sort of an in-between phase
where we have to build these machines
that then we set out into the world
like with little embodied cognitive architectures.
And then we have to wait for them
to gather enough different kinds of data and experience
that we can't build into them.
You think that's a possibility
that we'll go through this phase
where we're taking these robots
and then letting them be for a while
and then we can use them, for instance,
then they're domain general enough
that we can actually use them?
Or can you imagine that kind of scenario?
Is it a ridiculous question?
Yeah, I mean, I don't see why not,
although of course there's all sorts of ethical
and practical issues setting loose robots in the world.
It depends what kind of constraints we put on them
and what kind of challenges,
do they have to enter the workforce
and work for their...
Oh, definitely, definitely.
For me, anyway.
So I came up through neurophysiology
in my graduate work and my postdoctoral work.
And this podcast I made has the tagline
where neuroscience and AI converge.
But I always have thought of and I'm only starting
to not think of it this way.
I've always thought of cognitive science
as a part of neuroscience.
That neuroscience sort of subsumes psychology even.
It seems to depend on whose definition
you like of these things,
but so this paper was written from a real cognitive science
historical perspective and modern day sense
of cognitive science.
Do you think that we're in a cognitive science resurgence
sort of over neuroscience?
So there's a lot of hype in neuroscience, right?
We're gonna figure all this out.
And now is cognitive science coming back now
and saying, whoa, whoa, this is the way to go.
Are we in this resurgence to understand how to understand
the brain and how to program machines
that think like brains?
It depends who you ask.
I mean, there's certainly still people
who argue very strenuously that neuroscience
is really important for building intelligent machines.
I don't think either for cognitive science
or for neuroscience, psychology or neuroscience,
they're both in some sense,
very dependent on our computational notions.
We need ideas from machine learning and other disciplines,
technical, formal ideas to interpret
both psychological and neural data.
I don't think it's the case that we've gotten a lot
of mileage out of just looking at those data
and from that extracting algorithms
that we can plug into machine learning systems.
I just don't think that's how it really works.
People have different arguments about this, of course.
Yeah, so let's kind of transition.
I just have a few general kind of broad questions for you
and then I'll let you go.
Okay, we'll see how you perform on this one.
So in the spirit of human compositional skills
and our ability to produce infinite things
from a finite set of things,
can you, Sam, utter a grammatically correct
English sentence right now
that you think nobody has ever said or written?
Sure, yeah.
I don't think that cuts it.
I mean, you could do any variant
of Chomsky's classic sentence,
colorless green ideas, sleep curiously.
You could have them sleep quiet, sleep.
Effervescently.
Effervescently, yeah, colorfully.
The adverbs, it's the adverbs that allow you to do this.
Yeah, yeah, I think it's pretty straightforward to do that.
Okay.
I could say the sentence,
I don't believe that Chomsky said
that colorless green ideas sleep curiously.
I bet that's been said, but we'll see.
I'll see if someone can Google it.
Okay, so often when we're stuck
and need to figure out the solution to some sort of problem,
we need to step away from the problem, right?
You can't just like, you know, you're at your desk,
you're trying to figure the thing out,
but it's only later when you step away
that the solution comes to you.
And often like immersing ourselves
in a like long form, complicated novel, for instance,
can lead to more creative ideas
than like reading a summary of that novel, right?
Presumably because we need to sort of steep
in that mind frame for longer,
not the things bubble into our minds.
Do you think that there's something about time scales?
And this, I thought you might be going to this
when you're talking about your dinner with the geologist.
Is there something about time scales and dynamics
and processing time that is important
for how we think and learn
and how maybe machines might need to think and learn?
So thinking of, you know, the idea is can,
like the geological formations, right?
They're moving and they're a dynamical system.
Could we, if you stepped back on a really slow time scale,
would those look intelligent?
Could we make the case that those looked intelligent, right?
Or is there something special
about the millisecond processing times of our brain
that gives rise to intelligence
and that we need to consider when building machines?
I don't think so.
I mean, it's not entirely clear whether you mean
that the time scales refer here
to the time scales of computational processing
versus the time scales of events unfolding in our world.
Like clearly we can reason about events
that happen at vastly different time scales,
even though the time scale
of our neural processing is fixed.
I didn't say I asked, they're gonna be good questions
or well constrained.
Yeah, I mean, I think that there's something important
about time scale, but I'm not sure what it is.
I mean, it is an intriguing question
why thinking over long periods of time would help you.
What is one idea that,
seemingly you have time to do all your ideas,
but what's one idea that you either don't have
the resources or time to tackle
that you wish someone else might pursue?
Yeah, well, a lot of my questions about the brain
really need to be answered in animal models.
And although I do human experiments in my lab,
I don't do animal experiments.
And so typically I try to farm out those animal experiments
to other labs that do that kind of research
and that's been-
No pun intended.
That's been very fruitful for me to answer questions,
for example, about the nature of dopamine processing.
So there's lots of questions like that.
I mean, often the kinds of experiments that we do,
we often try to design them in such a way
that both humans and animals could do those tasks.
And then eventually try to port them over to animals
and there's some process of convincing
an animal experimentalist to take on that challenge.
Well, they're busy, you know,
they've got other things going on.
Well, it's not just they're busy,
I think they're conservative,
understandably because it takes a long time
as you know to train animals.
That's what I mean, they're busy.
Yeah, they're steeped in their two year other experiment.
If you could bet on a single discipline or subdiscipline
that will break open our understanding of the human mind,
what would it be?
You could say cognitive science,
physics, astrology, et cetera.
Well, I feel like the longer that I do this stuff,
it just seems that the more everything kind of converges.
We already talked about how physicists came up
with this idea of free energy
that then got picked up by computer scientists
to develop efficient inference algorithms
that then got picked up by neuroscientists
to think about the brain.
Yeah.
I mean, all these things are intertwined.
They're deep, both formal and metaphorical
relationships between all of these concepts.
And so I really like what Sherlock Holmes said
that to understand nature,
your mind must be as broad as nature.
That's how I feel like really the study of intelligence
is that in a nutshell, right?
You need the intelligence is drawing upon
all of these different disciplines and ways of thinking.
So in that vein, keeping one's mind as broad as possible,
do you have advice that you would give
to say a graduate student
or an undergraduate interested in these topics
and just wanting to start out in the arena?
Like what, how to move forward?
Yeah.
I think it's hard for students to follow the path
of greater resistance because they look around
and they see what's happening, what's successful,
what's not successful.
And it's natural for people to gravitate towards
the things that they perceive as being more successful.
But the paradox is that to be really successful,
to really make a difference, to change the way people
are thinking, to change the course of scientific discovery,
you have to swim against the current.
You have to pursue ideas that are not popular,
that people might not agree with,
that might not really make sense at first.
You have to befriend the crackpots and read poetry
and take long walks and do all the things
that aren't ostensibly the right things to do,
the productive things, the good uses of time, right?
So we have to be really attentive to
the unappreciated importance of uselessness.
Oh, that's a good quote.
I tempted in there, but I'll have one more question
for you here.
So, will AI help us understand consciousness,
what it is, how it works, et cetera?
Yeah, I'm not sure.
Should've ended with the last one.
I don't feel like I can really give a good answer to that
because to me, most of the things that we do
that are intelligent are not necessarily conscious,
or what the conscious part of them is just kind of
the surface, the perceptible surface,
but most of the interest in computation
is going on under the hood.
And that's why the study of psychology has moved away
from just asking people what they're thinking
because demonstrably, people don't really know
what they're thinking.
So in my view, consciousness is a little overrated.
Damn, all right, well, I know what I'm thinking.
I'm feeling grateful and thinking I'm glad
to have you on the show.
So, Sam, where should people go to learn more about you?
Well, they can look up my lab webpage
and if they're interested in reading my papers,
that's where they all are.
Very good, that'll also be linked to in the show notes.
So thanks for your time, Sam, and continued good luck to you.
Yeah, thanks so much, Paul.
Take care.
The Brain Inspired
Brain Inspired is a production of me.
You can support the show through Patreon
for a trifling two or four dollars per month.
Go to patreon.com slash braininspired
or go to the website, braininspired.co
and find the red Patreon button there.
Your contribution will help keep this show going
without any annoying advertisements.
So if you're interested in getting a new show
without this show going without any annoying advertisements
like you hear on other shows,
to get in touch with me,
email Paul at braininspired.co.
The music you hear is by The New Year.
Find them at thenewyear.net.
Thank you for your support.
See you next time.
The stair of a boundless blank page
led me into the snow
that covers up the path
that take me where I'd go.
