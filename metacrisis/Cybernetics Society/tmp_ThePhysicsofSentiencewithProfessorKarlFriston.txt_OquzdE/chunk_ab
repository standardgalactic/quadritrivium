I just cartoon the fundamental distinction there from the point of view of this Bayesian mechanics or interpretation of internal dynamics in in terms of this self evidencing or Bayesian mechanics of gradient flow on the log model evidence.
I'm drawing a distinction between these the behavior of large particles where the active states are no longer have direct access to the internal states.
And I've drawn that in terms of the, you know, the presumed beliefs of simple things like cells where they don't have to represent their own action.
You know, the action is directly informing and influencing the internal dynamics in contradistinction, things like you and me may well be better described as having beliefs about our own action.
And that's crucial. And I mean, basically it's not propositional beliefs here.
And that's crucial because that takes us into a different world. It takes us into a world of planning and inference where now, because I've got effectively beliefs about what I am doing.
I now have to infer what am I doing. If I'm making an inference, I have to have prior beliefs about what the kinds of things that I do.
And the argument here is what kind of things do I do. Well, I'm a free energy minimizing thing. Therefore, I must, well, I will more likely commit to those actions that minimize the free energy.
I would expect consequent upon that action.
Mathematically, that leads to, this is the other last slide now, a really interesting decomposition of the terms that constitute these, if you like, generalized prediction errors, or the free energy bound on the long evidence.
So forgive the equations, but the reason I put them up is I just want to see what happens to various interpretations of this free energy functional or function of sensory states and active states.
When we move into the future, when we now appeal to the free energy as a way of evaluating the likelihood that I will act in this way or all that way.
So what I've done here is just write out the full expression for the variation free energy.
And I've written it in two different ways. This is the kind of expression that somebody from statistics would be comfortable with. And they would read this as complexity minus accuracy, where the accuracy is just a long probability of some sensory
states, given my belief about their causes, the external states here. The complexity is interesting. It's effectively a divergence, technically a KL divergence, but can be read just as a difference between two kinds of probabilistic beliefs, namely,
what I believe, my beliefs about states of the world, given my exposure and the sensory information, namely my posterior beliefs, after seeing some data under some action, and the prior beliefs.
So it really scores the degree to which I change my mind in receipt of my sensory impressions, its amount of information gain, or the cost I have to pay in order to move my prior beliefs to my posterior beliefs as a result of this updating process driven by these prediction errors.
If you're in machine learning, you'd rearrange these terms, so that you would separate the log evidence that we've been talking about from another KL divergence between my approximate posterior beliefs that come from the free energy and the exact posterior beliefs about external states given the sensory states.
So this, I mean, because this can be never be less than zero, this free energy is often called an evidence bound in machine learning, sometimes with the acronym elbow because he's a negative free energy ELBO and evidence lower bound.
The reason I take you through that is if I now say, well, let's take these quantities and ask, what would they look like in the future before I've actually got any sensations.
So now let's take the probability distribution over the sensations given I'm acting in this way and use them to take the average of these quantities.
And what happens is they, the complexity becomes risk, the accuracy or the inaccuracy becomes ambiguity, and the divergence and the log evidences become intrinsic and extrinsic value respectively.
So what are these quantities? What will they look like? How could one interpret these?
If we just ignore the prior preferences, the attracting states, the log evidence for a moment and assume that I am equally attracted to all states of affairs, what are I left with?
I'm left with something called intrinsic value in robotics. It's also called intrinsic motivation in the visual search literature. It's called Bayesian surprise.
What is it? It's very simple. It's just the degree to which I change my mind about the external states, given my sensory observations under this action in relation to the same beliefs before without seeing the sensory observations.
So this basically scores the information gain or the reduction of uncertainty about the external states. It's, if you like, the imperative for expiration.
It is sometimes known as a scoring the epistemic affordance of acting in this way as opposed to acting in that way, where the epistemic affordance just reflects the amount of information that I will gain if I do this as opposed to that.
From the point of view of information theory, it's just the mutual information between the causes of my sensation and the sensory consequences, the external and sensory states, respectively.
Let me take some uncertainty off the table and see what happens. Let's assume that I can see all the sensory states and therefore the external states become the sensory states and therefore there's no ambiguity.
What am I left with? Well, risk. So what is risk? Well, it's just the difference between what I think will happen if I act like this and what a priori I prefer to happen by attracting external states or indeed sensory states.
So in engineering, this will be known as KL control. In economics, it could be regarded as risk sensitive control where we ignore the ambiguity.
And then finally, if I remove all uncertainty, so there's no reducible uncertainty in this environment, I've become completely familiar with it.
We're just left with the log evidence or the extrinsic value, the expected value, which we started with, which is just scoring the expected log probability of me ending up in my attracting set or sensory subset of those attracting sets.
So the final example is we're just using these equations to simulate another kind of agent that now really thinks about the future and acts in a way, not reflexively to minimize proprioceptive prediction errors, but to minimize the expected fear and the expected prediction
errors in the future by resolving uncertainty. And what we've done here is to simulate a very simple agent whose universe just comes along in three flavors.
All her sensations are caused by an upright face, a sideways face or an inverted face.
The restriction here is that this agent can only see a very small part of the visual field. So it has to choose very carefully where to look and does so in a way that responds to this epistemic affordance, this intrinsic value or expected information
gain and chooses the information rich parts of the image shown here in terms of dynamics and what is actually sampled. And in so doing resolves uncertainty about the three hypotheses that best explain the sequentially sampled
sensorium or visual input, showing a progressive reduction in the uncertainty in terms of the Bayesian confidence intervals and correctly believing now that she is sampling from an upright face.
So that can be much more succinctly expressed, as always by Helmholtz, and indeed he has done that here, each movement we make by which we alter the appearance of objects should be thought of as an experiment designed to test whether we've understood correctly the
relations of the phenomena before us. That is their existence in definite spatial relations. And so with that, it only remains for me to thank those people whose ideas I've been talking about. And of course, thank you for your attention.
Thank you very much indeed.
Thank you, Col. Thank you so much. Wonderful. Thank you. And I think we'll have time for plenty of discussion and questions.
If I just start, please, I can see some questions coming in the chat, but if I just start. So I think we're going to talk some more about the good regulator theorem, so no need for me to cover that off initially.
Let's just maybe explore one point and please don't make the mistake of thinking that maybe all I need here is a repetition of something that you've already said.
Can we decouple the internal states from the active states?
I'm trying to explore the way in which this is reliant on free energy minimization. In other words, this decoupling or disconnection, if I can call it that, of the internal from the active.
In a sense, is it happening because of the principle of free energy minimization, or in a sense, is it required by that principle?
That's a really excellent and deep question, and I did not cover it in this presentation because the answer to that question, if correct, has only become apparent in the past few years prior to much of the preparation of the material and the story that I was telling today.
It has a clear answer. It is not a consequence of the free energy principle. I think, generically speaking, the free energy principle is just a description of things that exist with a careful definition of what you mean by existence,
but also a careful definition of the particular dependencies that underwrite your definition of existence, which is a Markov blanket.
So all the free energy principle says, if you give me a random dynamical system with a particular sparse coupling that permits the existence of a Markov blanket that in turn permits me to identify something as persisting through time,
then the free energy principle can be applied should you want to. And you may ask, well, why would you want to? For fun sometimes, literally, you can have great fun simulating things.
Scientifically, the simulation aspect is very important. Being able to actually write down the dynamics of a system that you think can be explained by this kind of gender model or that kind of gender model is quite useful.
It also has applications as an observation model in phenotyping certain behaviors in psychiatry. But so back to the question. So notice that the dynamics do not inherit from the free energy principle.
The free energy principle is a description of the dynamics of this kind of thing. So the answer now is to your question is, well, what kind of things would, first of all, show gradient flows.
That are precisely these paths of least action. And in particular, the kinds of things where there's a disconnect between a removal of the influence of the active states on the internal states.
The answer to the first question is things that are not subject to lots of random fluctuations. So we're talking about big things.
Things that are certainly bigger than a quantum scale in which the random fluctuations average away. And the bigger they get, the fewer the random fluctuations until you actually get right.
You get to the scale of heavenly bodies, for example, where there are virtually no random fluctuations, those dissipated gradient flows disappear and we're just left with the divergence free cell and idle flow.
The rotation of the heavenly bodies, the earth around the sun, for example.
But on route, as we get bigger and bigger and bigger, there will be a substantial portion and an increasing portion proportion of the internal states that can't see the active states of the of the Markov blanket.
So the supposition here is, if you are big enough, you may be describable as panning, which says that there are small things are unlikely to plan.
So, you know, very, in a very simple, by the way, we wouldn't expect this planning as inference to be a feature or necessarily describe the behavior of a bacterium or a virus.
But you might expect it to apply to amounts and you'd certainly expected to apply to to you and me, just because we are bigger.
So, you know, so the free energy is a method or a description that can be applied to things.
And when applied to big things, then you can certainly then you would be licensed to incorporate this planning as inference into into the description on the application of the method.
Is that the kind of wonderful, very eloquent. Thank you so much.
And I appreciate that. So, Jonathan, shall we have a look at the chat, please.
Yes, sounds good. And what would it would take a couple of questions from chat and then we'll go to you, David.
So, couple of questions chat. So first one's from Clement Vidal. Clement, do you want to read out the question you put in chat or I can do it on your behalf.
Sure, I can do it.
So how many levels of hierarchies of predictions exist in the human brain and do they correspond to the number of brain layers?
I'm smiling because Chris Frith and I make a joke. Yes, there is number. It's six. And then and then somebody asked why six?
I said, well, because we said so. So we say it's a bit of a joke.
But it's also not a joke, really, because again, it comes down to very, very simple arguments about the size of the brain and just appealing in a very coarse grained way to
say renormalization group. What tends to happen as you move from very low level sensory levels in these hierarchical structures, which you can read literally as deep world models or deep
generative models where the deep just is an expression of that you have this hierarchical structure, which I should add depends upon sparsity of connections and Markov blankets.
So you can't define a hierarchy unless other than in terms of which connectors are not there. And that is another instance of a Markov blanket.
So each layer is a blank Markov blanket for for all the in a centripetal kind of hierarchy, all the internal states that are higher than that.
So in these deep architectures, what tends to happen is you get as characteristic of the renormalization group, you get a slowing of the dynamics.
And that and that sort tells you well how many times can you slow down dynamics in a brain that only lasts for, say, 50 to 100 years.
And it's usually about six. So you start off with the cognitive moment, which is about 250 milliseconds. So you keep on increasing it by an order of magnitude, say,
the Napa bound and the number of times you can do that and suddenly we crunch upon the lifetime, the time, you know, the timescale over which this Markov blanket actually exists.
So probably six is a Napa bound, not to be confused with the fact that the.
So what I'm talking about here are levels in a deep world model or generative model that are deployed as you move through increasingly higher levels of your brain.
From V1 to V2, from V2 to V4, it's about here, about here to V5, which would be about here. Yeah, as you get sort of closer to right or frontal regions.
The brain itself can be envisaged like a sort of like a cabbage that's covered with all the, all the, you know, the neuro pill that contains all the nodes that are connected, the gray matter.
Sits like a sort of a layer, which is about three to five millimeters thick on top of this cabbage, where the cabbage itself comprises all the connections of sparse connectivity, called white matter, because they're myelinated with little fatty wire sheets.
That, that, that cortex itself has a layered structure.
So, you, the careful when you ask somebody like me about layers and levels because the levels we're talking about in terms of generating and contextualizing predictions.
So, this notion of a hierarchical generative model that's generating predictions, and you're saying that the deeper levels, click over more slowly in universal or clock time.
You're basically saying that the deeper level to provide a context for fast things at the low level, and the fast things at the lower level, provide a context for even faster things at the level below that.
That notion of a layer is really a level in a hierarchical, in a deep hierarchical architecture.
And that's very distinct from the layers of the cortex. The less the cortex really interesting because when one drills down on the biological implementation of that predictive coding scheme that I illustrated with predictions and prediction error being passed around.
It looks as if the prediction errors live in the top three layers, and the predictions live in the bottom three layers.
And you can actually look at the wiring of the brain and start to try to make sense of the hierarchical message passing that now has a laminar or layer specificity in the context of cortical layers.
So it can be a bit confusing, but it's a fascinating area to get at the competition and has to be applied by the sparse coupling and the predictive coding like interpretations.
Great, thank you.
Go to Chance Mills next and then to you David. So Chance, again, do you want me to read out your question or would you like to read it out yourself?
I think I'm alright with reading it out myself, thanks.
So the American Society for Cybernetics proposes, you know, this move towards ontogenetic resilience in light of the change of environmental factors at an unprecedented pace.
Loss of biodiversity changes, weather patterns, things which kind of anchor our identity and connection to the world.
So my question is this, in light of the free energy principle, what is the adaptive strategy for dealing with such a situation?
A very challenging question would be tempted to answer that on two levels.
First of all, how would you describe adaptation under the free energy principle?
You could also answer it, how would you use the free energy principle to try and ameliorate some of the challenges that are currently facing us in terms of overconnectivity and runaway dynamics and a failure of self-organisation,
which is what would be predicted under the free energy principle.
Perhaps I'll just make that point clear because I think I feel quite passionately about that.
Everything that I have said in terms of being able to describe self-organisation to an attracting set,
which I think is just a description of self-sustaining systems,
that it is a definition of sustainability at the level of, certainly of physics, rests upon the sparsity of coupling.
It rests upon the connections that are not there.
So if you go around globalising and increasing connectivity, you're destroying Markov blankets.
If you destroy Markov blankets, you destroy self-organisation.
Technically, from a dynamical systems perspective, you're inducing oscillated deaths everywhere.
And one could argue that that's what we are potentially confronted with at the moment in terms of overconnectivity
and a destruction of all the sparse coupling that preserves those delicate structures that have self-organised them over millennia, organised themselves over millennia.
So if you wanted to apply the free energy principle to render a particular system, be it an ecosystem, be it a financial market,
be it a meteorological system, if you could intervene on it, to improve its resilience,
what you are looking at is a way of preventing the destruction of sparse connectivity to reclude overconnectivity,
to maintain those boundaries and that segregation in the right kind of way.
Which means, I guess, ideologically, putting more regulation in place.
It means having very clear geopolitical boundaries, Markov blankets in a sustainable way,
finding mechanisms that would preclude breaches of Markov blankets.
If there was an oncologist, these breaches would be basically cancer,
cells growing beyond their natural boundaries because they now become poor regulators or models of their local cellular eco niche.
Or you could look at the wars in Ukraine, for example, as the equivalent kind of failure of a boundary on Markov blanket at a geopolitical level.
So that's how I would apply the free energy. As an academic, how will you describe this?
I think it speaks to something we mentioned before, which is separation of temple scales.
All that I was talking about during the presentation was focusing on one temple scale, which was inferring states of the world, states of affairs out there.
But exactly the same mechanic supplies to the parameters of a geratin model, which as a neurobiologist would be the brain connectivity, for example.
If you're in machine learning, it would be the connection strength or the weights in, say, a transformer model.
And also to the structure of these good models, these geratin models.
By structure, I mean the number of levels in a hierarchical structure, the number of components, the particular spastic structure that endows it with a deep or hierarchical structure.
All of these things have to be optimized in relation to the free energy.
That takes you into the world of evolution in the very simple sense that the free energy in statistics is used to do basic model selection.
But you can also read natural selection as nature's way of doing basic model selection, selecting the right phenotype.
That's a good model of that phenotype's lived world.
So you've got these different different timescales in play.
So I think adaptation of the time, the kinds of timescales that your question intimates would have to, I think, appeal both to parametric learning and structural learning.
Both of which can be expressed as free energy minimizing or evidence maximizing processes in the face of a changing environment.
That in turn speaks to interesting differences between inference and learning, where inference is updating your embodied beliefs about states of affairs, latent states in machine learning talk.
Learning would be basically updating the parameters of your world models and then structure learning or selection will be selecting the right kinds of structures.
Each one nested within the other with a separation of timescales.
For example, I can't make sense of my sensorium unless I've learned the right kinds of contingencies.
And I can't learn the right kinds of contingencies during neurodevelopment unless I've got the right brain structure.
So every one of these scales depends upon all the other ones.
And of course, that also holds in reverse that my brain wouldn't exist unless I had a structure.
And of course, I wouldn't have the wiring in order to do the message passing to do perception.
So all of these different scales depend upon each other.
Is that the kind of answer you were looking for?
Yes, thank you. Thank you very much, Professor.
David will go to you next and after David will go to Trevor Hill there.
Well, thank you very much for elucidating and give me a far better feel for perception information processing and moving towards action.
I don't want to pretend to have understood or digested everything that you've said.
Completely, Professor Princeton, what I'm interested in is how far you see this form of analysis of moving towards kind of dissolving the sensation body problem, the hard problem, the issue of qualia or raw fields.
What is information prior to sensation, is sensation a concept we have no need of or does it emerge out of processing our processing, so to speak.
I just wanted to give you could sort of help me around this area or whether philosophically you regarded it as a non question.
I'm not allowed to regard it as a non question. I have too many philosophy friends, but because I don't do philosophy, it's not a question I can help you with.
But I can point you to the kinds of answers and the literature where people are really focusing on this, but it is a philosophical question.
From my point of view, the question really reduces to what kinds and specifically what structures specifically of the gerontic models would be necessary in order for some artifact natural or otherwise to be sentient in the way that you're implying.
And what most people end up with is a number of different arguments. You can take the agency argument, which I quite like, to be sentient is to be an agent.
So it's me actually gathering my data and possibly experiencing my data or not. But before I can experience it, I have to be me and therefore I have to be an agent.
So immediately you're talking about how would you define agency as a physicist and if you defined agency of a non trivial sort as the kind that inherits from the representing the consequences of my own action.
That tells you a number of things immediately tells you that certain things possibly viruses cannot be sentient in the way that an agent could be.
It also tells you that things that have sentience if they are agents must have a generative model of the future in the sense that in order for me to predict the consequences of my action to plan.
I have to have a generative model of the future in the sense that the consequences are not yet occurred, which means there's a certain temple thickness to these world models that you possibly wouldn't find in a virus or a thermostat, but you might find in me.
So that might be one bright line between the kinds of things that can be sentient and well to have an agentic sentence sentence.
And then you move on to other perspectives.
Some perspectives emphasize as you mentioned the body. So I'm thinking about the work of people like Anil Seth here and Lisa Feldman Barrett.
They, they say that well to have minimal selfhood is just the hypothesis that the best explanation for this myriad of sensations from my eyes and my body, my skin.
The best explanation for it is I am an embodied being and I am me and all of this is the best explanation for what's going on.
So now selfhood itself becomes another becomes another hypothesis, which you have to ask, well, why would you have that hypothesis?
So one advantage of having that hypothesis once you realize I am me is you can now exert mental action over which things you attend to.
There's quite a long story here, but I'm just slipping in the key words that that some people think that it's not any agency that makes you sentient.
It's a particular kind of agency that's on the inside.
It's basically attending to different sectors of your Markov blankets.
That basically allows you to select what information you are going to use to do your updating about myself and my plans.
So there are people usually German philosophers who would focus on mental action as necessary for for that kind of kind of sentence and minimal selfhood.
And then you get into even even more contentious issues about self awareness and having models of me in different states of mind, you know, is the hypothesis that I am a person and I am anxious.
And is that part of my generated model? And what would that look like in terms of James and hanging and sort of formulations of your of emotions and their bodily expressions.
So really fascinating answers a question and lots of fascinating answers, but there are lots of them.
So I think you just have to take your pick from your favorite philosopher.
Yeah. So will turning an artificially intelligent system off ever be morally equivalent to knocking somebody out?
Yeah, if you if you could engineer such such a sense. Yes, I think so.
I mean, that's a little bit.
I don't like knocking people out because that's usually has more sit as a new as a psychiatrist neurologist. This is that you shouldn't really do that if you can avoid it.
But it's certainly like putting something to sleep or anesthetizing them. Yeah, yeah, it would be it would be very much like that.
The argument now is, will you ever get a by memetic intelligence?
You know, in an artifact, I don't see why not provided that artifact is embodied so that, you know, you give it the opportunity to learn, oh, I am embodied and I am a thing.
And I can plan.
And yes, in principle, I would it would be like switching you switching you off when you went to sleep for example.
Yeah.
Thank you.
Thank you, Trevor. Do you want to go next and after treasure? Yeah, thank you.
Thanks so much. Thanks. Thanks, Professor.
I wonder if you could comment on the 1000 brains theory about how the neocortex works.
Yes, I can. Yeah, so Jeff.
