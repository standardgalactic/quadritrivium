I'm just taking the back end of your published bio, talking about that of the Royal Society
of Biology 2012, Weldon Memorial Prize 2013, Embo, Excellence Prize 2014, Academia Euro
Thankfully, you haven't updated since 2016. I'm 10-16, Charles Brunch Award for on Parallel Day Contribution
Grain Research and the Glass Brain Award. Really glad to have you among us, and thank you
over to you for the physics of sentience.
Thank you very much for that wonderful introduction and your kind comments. It's a great pleasure to be here.
It does feel like a very warm community. I'm going to share my screen and then hopefully that will set us all up.
So let me repeat my thanks for being here. I'm going to start with, as usual with this talk, an apology, but a very specific apology now.
I don't have a foundational training in cybernetics, but I have, over the course of the work, largely inspired by neuroscience
but appealing to physics during my career, it has become very clear that cybernetics and the kind of work that I've been engaged with
have very intimate relationships and probably share certain foundations, along with things like perceptual control theory.
But I repeat, I do not have a foundational training in these disciplines. So what I've got to offer in the first hour of this session
is really a view of self-organisation and sentient behaviour through the lens of a physicist and neuroscientist in the fond hope that you will be able to join the dots
and see how it can be translated in terms of the basic principles with which you will be much more familiar.
So I'm going to divide the talk into three parts. First of all, telling a story about behaviour, about self-organisation from the point of view of statistical physics
and information theory with a special focus on Markov blankets and how Markov blankets individuate things that possess attributes such as behaviour
and trying to understand those attributes in terms of something that we refer to as a Bayesian mechanics.
And then I'm going to tell you exactly the same story, but using the concepts and the constructs that come from neuroscience and neurobiology.
So how would these physics-like principles be applied to the brain, to things like you and me, or indeed single cells?
And I'm going to illustrate that in terms of predictive coding and neural networks as a particular instance or application of the physics that we're going to cover in the first part.
And then if we have time, I'd like to just drill down on the nature of agency within this kind of treatment and specifically talk about the difference between mere reflexive behaviours of the kind you might say find in a thermostat
and the sort of planning and deliberate behaviour in agency that we find in things like ourselves.
And I'm looking forward to a conversation about whether that bright line is necessary and how it does relate to cybernetics.
I'm going to start with a question posed by Skrodinger. How can the events in space and time which take place within the spatial boundary of a living organism be accounted for by physics and chemistry?
I'm not going to answer that question and clearly we can't do that.
But what we can do is just highlight the notion of a spatial boundary as being requisite to be able to talk about anything in the sense that to talk about some entity, some particle, some person or something, you need to be able to individuate it from everything else in the universe.
So I'm going to read that thing as a statistical object or the boundary that defines its definitive of that thing as a statistical boundary, specifically a Markov blanket.
So for those of you who are not familiar with the Markov blanket notion, it's a very ubiquitous construct in statistics and causal inference, but plays a foundational role in what I'm about to say.
I repeat simply because it is the device that allows you to separate or individuate the internal states and the blanket states of something from everything else.
So more specifically, I've just tried to cartoon the intuition here in terms of a very simple little universe where these blue circles represent some abstract states of the universe and the arrows denote an influence of one state on another state.
And if I were to pick a particular set of states and to say my states, then we'll call my these states internal states or states that are internal to me.
The Markov blanket is defined in terms of the parents, the children and the parents of the children of the internal states.
And the role of the Markov blanket is to provide a kind of insulation, a kind of separation between the internal states and the remaining states, the external states here.
So technically, what this means is that if I wanted to predict the dynamics or evolution of my internal states, given the rest of the universe, I would only need to know the Markov blanket.
All the information about the entire universe that is pertinent to me to my internal states is contained upon this boundary or this Markov blanket.
Mathematically, that just means that the internal states are conditionally independent of the external states, given the blanket states.
So that's my definition of a thing less.
And I'm going to make a further move here. I'm going to make a bipartisan of the blanket states into sensory states and active states.
According to the very simple rule that the sensory states influence but are not influenced by the internal states.
And similarly, the active states influence but are not influenced by the external states.
So I've now effectively got a partition of all the states in the universe or this particular little universe that enables me to identify a particle or some entity that comprises the internal states and the blanket states that themselves can be divided into active and sensory states.
And you would be asking, why have I done this? Well, it, it organizes a way of thinking about all the kinds of systems that we as scientists and philosophers like to think about two of my favorite systems are here, the brain.
So under this kind of partition defined by these sparse couplings, the arrows, how can we consider the brain while the brain would have internal states or my synaptic activities, my connectivity, everything I need to list in order to define the state of the brain at this point in time.
And the internal states are going to influence the active states in my actuators, my autonomic reflexes that perform actions on the external states that then change my sensory epithelium, my sensory states that are then going to influence my internal brain states.
And so the cycle begins. And exactly the same causal dependency structure or separation or partition of states can be found.
Anyway, you look so for example, a single cell will have intracellular internal states that are internal to say the active filaments of a cell that are themselves beneath the sensory states that are pushed out into the external states that reciprocate in terms of causal influences.
By activating cell surface receptors that change the intracellular states that change the action and so forth.
On both, both of these examples.
You have this notion that the inside is influencing the outside the carelessly through the active states while the outside is influencing the inside again the carelessly through the sensory states.
And this sort of completes a circular causality that as a neuroscientist one could read as your very elementary form of an action perception cycle.
And what I'm going to do now though is just to ask you to forget about the Markov blanket.
We're going to now rehearse one on one physics to the extent that we need to to pursue the argument.
Having established the core physics at hand, we're then going to put the Markov bucket back into play and see what special properties emerge.
And I'm framing it like that because the starting point for this is exactly the same starting point that you would find in all kinds of mechanics, whether it's quantum mechanics, statistical mechanics or classical mechanics.
It starts really with an expression of the dynamics of some universe.
And I've sketched that out here in terms of, in this instance, a large non equation.
So the rate of change of some states is just a function of the state, plus some random fluctuations.
And I've cartoon that here in terms of two states.
And this can be read many, many different scales.
For example, it could be neuronal oscillations in one cell of my brain, as they cause that trajectory of these two states, this two states, states, space, space over very fast time scales, the appropriate gamma oscillations.
It could be my cardiac cycle coming through the phases of the cardiac cycle.
It could be me getting up in the morning, having a cup of coffee, doing my emails and working through the day. It could be an annual cycle.
The point being here, we are interested in describing systems that have a particular characteristic.
In fact, one could say, definitional, definitional of them being existing in the sense that they possess characteristic states.
The characteristic in question is that they always, the system always revisits states it was once in.
And this could be read, in many ways, it could be read as possessing an attracting set or an attracting manifold on which the states evolve.
It can be read as the solution to an unequilibrium steady state.
And it can also be read effectively instead of being tracing out particular trajectories, we can read the density of these trajectories as a probability that you will find me in a particular state if you sampled me at random.
And that's the, if like the interpretation that I'm going to assume.
So why have I focused on that interpretation?
Well, we know a lot about the dynamics, not all the states in and of themselves, but of the density distribution, the probability distribution over those states.
And some of you may or may not be familiar with various expressions of the density dynamics are written down here in terms of a Planck equation.
So now it's a rate of change, not of the states, but of the probability density over the states that is expressed here in terms of the amplitude of the random fluctuations and the flow.
And this equation is, again, ubiquitous in physics and, you know, you could read it as a continuous form of master equation.
It could be the time independent Schrodinger equation.
Wherever you look, it's a fundamental equation that describes deterministically the evolution of the probability distribution.
But I've just said we're interested in describing systems that have a characteristic set of states such that this probability distribution doesn't change with time.
And that's the key move here.
So I'm basically looking for solutions for these density dynamics that are now characteristic of the dynamics of systems of interest that possess an attracting set.
And just by setting this to zero and rearranging it specifically using in this instance the Helmholtz decomposition.
What I can do is express the dynamics, the flow, the way that I change my states over time purely as a function of the amplitude of these random fluctuations and the gradients of these probabilities.
So there's also another term here known as a solenoidal component.
So this is the equation I want to focus on because it's got some really intriguing interpretations.
Before I sort of get into those interpretations, maybe a good idea just to explain why there are two terms here describing these flows on these probability or log probability gradients.
One, this decomposition basically is just describing the fact that you can always split the flow into two orthogonal components.
And the first one, say for example, we think about water flowing down a bath, the plug hole in a bath, then the gradient flow is flowing down the gradients.
Whereas the solenoidal flow, denoted by Q here, circulates on the iso probability contours or solenoidal, shows this kind of solenoidal effect as it circulates around.
So it's just a way of breaking up the dynamics into a gradient flow on the one hand and some circular flow on the other hand.
Both of those have interesting implications.
The gradient flow we're talking about is actually flowing, if you like, up log probability gradients.
It's like gathering, flowing up a concentration of the probability of finding me in a particular state, as if the system is trying to gather itself into its attracting set and resisting the dispersive effect of the random fluctuations.
The circular flow is interesting as well, because it says that these kinds of systems must have a form of oscillation or cyclical aspects, solenoidal aspect to them.
And of course, if you think about biological systems, they're full of biarrhythms and oscillations and indeed you can almost say life cycles themselves are an articulation or an expression of this kind of ubiquitous kind of dynamics.
So that's where we've arrived at the solution to the density dynamics has to have this functional form which can be expressed as a gradient flow on these log probabilities.
What we'll do now is bring the Markov blankets back and see if there is another kind of mechanics that complements or fits in the classical or statistical and quantum mechanics that can be derived from these density dynamics.
So that flow equation holds for all the states and the particular petition that we discussed under the Markov blanket.
So the internal states will be doing a gradient flow on the probability here of the Markov blanket states, as will the active states.
And notice by construction, I can drop the external states because the internal states and the active states are not influenced by construction by the external states.
So this leads to a slightly simpler form of these gradient flows that I'm going to read in terms of perception and action, both in the service or look as if they're in the service of maximizing the log probability of states.
So what are the interpretations of this quantity, this log probability of some sensory states given a Markov blanket or given me.
And I've listed a few interpretations here just to illustrate the, again, the importance of this fundamental kind of dynamics as gradient flow.
If you recall, this probability scores the likelihood you will find me in these states.
They are states that part of my attracting set they are literally those states that attract me. And in that sense, then they are states that are valuable for me, given my Markov blanket.
And all of these, all that's that this equation is saying is it looks as if my internal states and the way I act upon my world or my external states are trying to maximize by flowing uphill.
This quantity, which can be read now as reinforcement learning if you're a control theorist, it would be optimal control theory.
If you're an economist, you could read this in terms of expected utility theory.
And that's nice because the negative of this value.
If you're an information theorist will be known as the self information, the implausibility of this sensory observation or outcome or event, given me, more simply surprise or even more simply just a surprise.
So this we can spin out the principle of maximum mutual information or the information principle equivalently the minimum redundancy principle, and indeed the free energy principle.
Why the free energy principle. Well, the free energy is just a bound or proxy for this, this surprise and I'll unpack the functional form of this later on.
I'm trying to give it, give an intuition as to how one can interpret the components of the free energy, equipping it with, you know, a teleology or a functional interpretation which may or may not be necessary to simulate the behaviors of interest.
The average of self information or surprise is just the entropy, which means that this fundamental gradient flow will look as if it's trying to minimize the dispersion or the entropy of the blanket states and the example, the sensory states.
And of course that's just an expression of the Holy Grail and physics or the physics of self organization as articulated say by Herman Haken's synergetics, and if I was a physiologist.
It would just be a statement of homeostasis it would just be a statement that systems that persist in a physiological sense are those that show a generalized homeostasis they are keeping the essential variables, as it were, within viable bounds that are characteristic of the physiology
kind of thing that this system is. There's a final interpretation that I want to pursue here which is the interpretation that a statistician would bring to the table.
If a statistician saw this expression, she would think are you are now writing down the probability of some sensory observations some data, some sample from the world or generated by some process or world, given not me on my Markov blanket but me considered as a model of the way in which my sensory observations were generated.
And this will be known as statistics as the model evidence, also known as the marginal likelihood of these sensory data and emphasize the model of course because what we're seeing here is that to be to solicit valuable outcomes and to minimize my surprise and to self organize in a way that resists a second law.
The second law applied to open systems of this sort is just to be a good model of my world. And of course this is just a statement of the good regulator theorem from Los Ashby and colleagues, but I'm going to pursue exactly the same notion from the point of view of statistician leading to things like the
basing brain hypothesis, also formulated in terms of evidence accumulation, gathering evidence for my model of the world and predictive coding which is the example that I'll show later on that leads to a specific process theory that many people in the neurosciences subscribe to in terms of trying to understand the dynamics of a brain doing its sense making and prosecuting its actions upon the world.
So that's the, that's the physics part.
I'm now going to tell the same story from the point of view as if I was lecturing to people in psychology or cognitive neuroscience.
It's the same, it's exactly the same same same ideas. And I think nicely illustrated by this 16th century oil painter, famed for painting still lives, but when viewed from a different perspective, give you a very different visual impression.
Previously you saw a bowl of fruit, and now you see a face. The point is making is that you made that face. This is a very active construction, an interpretation, a hypothesis, an explanation, a fantasy that you have used to explain this particular set of impressions on the sensory sector of your Markov blanket.
So just to unpack that a little bit more, it really speaks to the brain literally as a purveyor of fantasies or literally a fantastic organ that is evincing very much an inside out kind of perspective on perception which somewhat contrasts with early 20th century view of a sort of
outside in where the brains, so particularly say visual hierarchies were thought to extract information from the sensory input. This turns that on its head will have done then it's strange inversions and say no, no.
It feels it is a better way of understanding perception and sense making is to treat the problem as an active problem of construction and the active nature will become even more apparent as we get deeper into the notion of active inference.
So this view, I sort of mentioned 20th century sort of outside in perspectives, but in fact they were predated by this inside out perspective by people like Herman von Helmholtz and you could argue right the way back through campus to Plato, but beautifully summarized here by Helmholtz as follows.
Objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism. So again, he's referring to this notion of imagining something on the inside and then predicting what you would see and then using what you're actually seeing to update, revise your hypothesis until you've got a good account of what's going on.
And he referred to this in terms of unconscious inference or a perceptual inference very closely related to the notions of Richard Gregory in psychology who saw perception as hypothesis testing again emphasizing emphasizing an inactive aspect to to our perceptions and visually palpating the visual scene with our eyes to gather the right kind of evidence to test the hypothesis that this particular
set of visual inputs was caused by this or that. And these ideas have been taken up in machine learning and artificial intelligence research to great effect.
For example, people like Jeffrey Hinton and Peter Diane actually built a Helmholtz machine as a mathematical metaphor image of the Bayesian brain, borrowing explicitly from Bayesian statistics and the work of Richard Feynman, which is where the free energy comes from.
So Feynman, if you like, solved the problem of evaluating that marginal likelihood of model evidence, which is an intractable problem by converting it into an optimization problem by producing a bound upon the log marginal likelihood or that self information that we're talking about earlier on.
In the context of QED or quantum electrodynamics. So putting these two together, they invented the Helmholtz machine, which is formally almost exactly what we would predict from the point of view of this physics understanding of sense making.
And we want to now extend the story to include action.
So let's come back to this notion Helmholtz's notion of impressions on the nervous mechanism. So if that Bayesian interpretation of self organization under a given attracting set that possesses a Markov blanket.
What would that mean? Or how could one describe sense making on the inside on the internal states? Well, it would look as if me on the inside.
I am in receipt of the sensory impressions on my sensory epithelia my sensory veils in my retina, or my skin, or my hearing apparatus, and they will present themselves as shadows sparsely sampled impressions upon my sensory veil.
The sensory sector of my Markov blanket, and it would look as if my internal dynamics were trying to predict or explain what caused those sensory impressions.
So how could we think of that in terms of the internal dynamics? Well, we already know how to describe the internal dynamics, say my brain dynamics, my neuronal dynamics, because it must be an instance of one of these fundamental flow equations.
So I'm going to make a couple of moves here. What I'm going to say is, if I can read my internal states as dramatizing some variational or probabilistic representational beliefs or probability distribution about the external states, then I can interpret this
fundamental gradient flow, which I've now expressed not in terms of the log probabilities but in terms of the free energies that stand in for the self information or the log values.
So we can now express this in a way that somebody in engineering would immediately recognize, and I would imagine also people in say perceptual control theory would immediately recognize the particular form here I've written down as a Kalman filter, or a simple form of Bayesian filtering, where the solenoidal part would be read as the prediction.
Imagine that the internal states mu now stood in for some expected states of the world.
So that the mu was now an expectation of some say a Gaussian probability distribution, which means that if I had some expectation about the state of the world I could predict the rate of change of the state of the world.
Now, I don't know whether that's true or not, but I can now use the gradient flow part, the non solenoidal or the dissipated part of the gradient flow, and I can now interpret these gradients of free energy in terms of a prediction error.
And I can use that prediction error to update my prediction to give me a the best guess about what's going on out there in terms of the external states.
So let me make that try and make that even more intuitive and just ask, well, what is this prediction error? Well, I've just said it mathematically it's just the gradients of the Bayesian free energy on the surprise of the self information, but intuitively what is it?
Well, imagine I had this sensory impression on my eyes on my retina, and I had an internal brain state that stood in for a belief that this particular set of sensory impressions was caused by a howling dog.
Now, if I had a generative model that could generate what I would see if this hypothesis or expectation was correct, I can then compare the ensuing prediction with the actual sensory impressions to produce a prediction error.
And then what this equation is saying is, I'm using that prediction error to drive changes in my internal states by neuronal activity, say, in order to eliminate the prediction error to destroy the free energy gradients to attenuate and resolve the prediction error until I have found the best explanation for this particular pattern in terms of the prediction, hence predicted coding.
Now notice here that this is just describing self organization of internal states in relation to external states and communicated by sensory states in terms of minimizing prediction error.
You'll never actually know what caused your sensation. So in this particular example, it was actually a cat casting the shadow that looked very much like a dog.
But it doesn't matter if I don't know all I need to do is to keep my prediction errors low for as long as I live and then job done.
So that's a very sort of parsimonious and I find a nice way of just describing the imperative sense making and action in the sense we can forget about the physics and just reduce it all now to prediction error.
And, you know, again, for those people in a perceptual control theory, this should be a story and well rehearsed.
So what about this sort of action perception cycle. Well, there are two ways I can minimize prediction error.
I can either change my mind literally by changing my neural activity to change the predictions of my sensory input and make them closer to the sensations to resolve the prediction error in the way that we've just described.
There's another way I can do that. I can actively resample my sensorium to make my sensations more like my predictions.
And this would be action changing sensations to make more like predictions, as opposed to perception, namely changing predictions to make them more like sensations.
So heuristic and what this means is that we would expect to see action fulfilling the prophecies or the predictions afforded by perception in a very, very simple way.
And this is to illustrate the simplicity. I'm not going to sort of talk explicitly about predictive coding and particularly hierarchical predictive coding of the sort that people currently in the neurosciences think is a nice way to think about message passing and dynamics amongst different
populations in the brain. So what I've done here is draw a schematic of the visual system and its entirety. So starting off say with visual signals from the eye reporting visual input.
They come into a nucleus and subcortical nucleus called the lateral geniculate nucleus. And these visual inputs are then in receipt of top down predictions and the difference then is scored by a prediction error that then is sent to drive or update beliefs expectations represented by units encoding what is being predicted to try and eliminate this first order prediction error.
In a hierarchical context, so we can just repeat this process. These are now first order predictions that themselves are being predicted by second order predictions that are used to compare with the first order predictions to produce a second order prediction error that then is used to drive these higher order predictions and then so on recursively to any hierarchical level desired.
So that would be a picture of hierarchical predictive coding or hierarchical message passing or Bayesian belief updating or all words for the same thing that will be consistent with this physics view of self organization.
What about the action? Well, let's consider another kind of input. The kind of input that in neurobiology would call proprioceptive input.
This basically just reports the state of my actuators, the state of my motor plant might say my muscles and this would come in say to the Pontine nuclei in the brain and it could be in receipt of top down predictions about where I feel my eyes pointing and I could use the ensuing prediction error to revise my beliefs about where I'm looking.
But it's a much simpler way of resolving these proprioceptive prediction errors. I can actually just return them to the external world and cause them to contract my muscles until they're reporting exactly what I predicted.
And of course what I've just described is this a classical reflex arc sometimes described in the 1980s and 90s in terms of the equilibrium point hypothesis.
The notion now is that my predictions are now furnishing set points that are reflexively realized in the periphery by using the prediction errors to destroy themselves or the implicit free energy gradients simply by acting upon the external states.
In this instance, the muscles so that I move my eyes so I get a different kind of information. Now notice here this is very much sort of closed loop thing, but it is deeply informed by the predictions that are gathering evidence from all different modalities.
So these predictions are realized in a relatively closed form setting, but the predictions in and of themselves are inherit from some very deep hierarchical sense making if you like under this predictive coding.
So that's the basic story. I just want to end now with a little bit more of a technical nuance on this free energy and just a conceptual proposition that would differentiate between the kind of reflexive behavior that I've just described and the sorts of behavior that you would study in psychology.
Your decision making under the certainty planning to do this versus that.
Just to sort of set the scene. And I've just bought another schematic here that summarizes the generic computational architecture that underwrites this action perception cycle.
So the idea is going to mark off blanket or internal states and external states. We have the sensory sector here providing the inputs that enable the the neural dynamics that are performing gradient flows on the on the variation free energy that look as if they are optimizing beliefs maximizing the
the quality of representations or beliefs about the causes of the six sensory states the external states here to form a expectations about states of affairs out there beyond sensory beyond my mark off blanket.
And then these beliefs are used to generate particular predictions specifically of my motor plant or my propriocept the the the apparatus that I used to act upon the world.
And we can read these in terms of proprioceptive prediction errors that then drive action that changes external states that supply new sensory states.
And so we have our action perception cycle now characterized or described in terms of perceptual inference and the ensuing action selection that is then driven by the predictions of what the motor plant should should be registering or reporting via proprioception.
You can get quite a long way with this with this construct in terms of producing by memetic behavior. This is one of my favorite examples illustrates a few subtle points first of all.
Sometimes the structure that you ascribe to the world actually comes from your own head, even though you think you are trying to just guess the causes of your own sensations under this view or under this description.
So what we've done here is equip a synthetic agent with a generative model in which the agent thinks that there's some autonomous dynamics in this instance a heteroclinic cycle that comes from a lot of Volterra attractor that just circulates with a heteroclinic orbit here.
And the generative model is configured such that the agent thinks that this abstract movement maps to movement in some extra personal Euclidean space and it's pulling an invisible string spring my point is that is connected to her finger.
So she expects to see and feel a finger being pulled around in this itirant orbit.
And of course those expectations now become descending predictions of what she expects to feel and what she expects to see.
But because the reflexes of fulfilling the predictions about what she expects to feel her arm will actually or finger will actually move, thereby generating the visual impressions that she predicted and satisfying those predictions.
And what that looks like from the outside is that the agent is doing a very elemental kind of handwriting generating the sensations that were predicted in both the proprioceptive and the visual domain.
So just to summarize that we have these top down predictions in the proprioceptive domain generating action and what could be called corollary discharge generating visual predictions that are then fulfilled by self authoring the causes of sensations through actually acting and generating what I expected to see.
One nice thing about this is we can actually suppress the proprioceptive prediction errors so there's no information that this agent has available to it that would suggest that she or it is actually moving, which would be a little bit like seeing stuff that's caused by somebody else.
And then by implementing this this adjustment we can now simulate action observation observing somebody else act and compare it to the simulated neural dynamics during action itself.
But from the point of view of the agent very little has changed. She's still got all the machinery to predict this kind of visual input and so can use exactly the same predictive generative model generating the predictions the same neural dynamics to actually explain the visual input.
And indeed if one looks at the activity of the simulated neuronal responses one can reproduce things you find in your barge like place field activity but crucially the same pattern of selective responses are listed during action and the observation of another performing the same action.
So that's that that particular illustration which I repeat is is is not unbiometic rests upon the Markov blanket structure that I described before here detailed in terms of the dependencies.
What I want to do now is just make one very small moon that could have a profound effect on the way that one might describe deliberative behavior.
What I'm going to do is I'm going to remove the influence of the active states on the internal states.
And my motivation for doing this is that if things get bigger and bigger and bigger and bigger at some point the internal states will lose the direct connection from the active states.
And so I'm going to remove that and that has a very interesting consequence.
It means that on the point of view of the internal states, the active states now become a precarious cause of the sensory states, which means that it'll look as if the brain is now encoding the internal states are encoding the active states as if there's a distinction between my physical
realized actions that I can only infer through the sensory consequences of actions.
