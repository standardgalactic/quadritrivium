Let's run MMM predicts posterior. In this way, we're running the MMM, our object that we've trained,
and we use the function predict posterior, and we import the X in order to predict the value,
the output variables based on the input variables that we have historically. And then what we need
to do is just plot posterior. Awesome. All right. It looks like it is pretty accurate. There are
some moments that over forgot, as you can see here, you have not blew what actually happened
to the sales. And in light blue, we have some, there is the confidence interval. And on average,
it looks like it is really accurate. But what I want to do is I want to print the R square and
map a, which are two accuracy metrics that we use in order to validate was the level of accuracy
of our MMMs. Now the R square, I have these scikit-learn R2 score and mean absolute error
function they're used to actually derive these values. The R square actually is not really a
an accuracy metric. It's more like it explained, it actually tells you how many variations in the
output variables are described by the variations in the input variables. Then we have the MAPE,
which is the percentage error that we actually see on average. And in this case, we have an 86
percent R square and a 10% 10% MAPE, which is interesting, really, really interesting. It's
not bad. On average, you start leveraging and using the MMM insights when the accuracy level,
the R square level is above 80%. The MAPE is a little big, but we can still use it. Definitely.
What I want to do is I want to scale up everything at their original scale. Awesome. And now we want
to plot the contribution of each factor. So we want to see over time, well, where the factors
that actually contributed to generating revenue. Now, there is this function, I don't love it,
because it's called plot group contribution breakdown over time. I don't love it because
it uses my plot lib. I have some problems into writing every time, but it shows you just a
small picture that we cannot filter the values inside. So as you can see here, there is a big
impact on a certain value here, the yellow one, that I don't really know what it means.
Probably I've scripted the function without including the labels of each color, my fault,
but I find it a little complicated in order to derive the real contribution, because what I
would like to have is a filter each factor, if I want, and identify and analyze which one,
one by one, in order to simplify my analysis. And ideally, I would like to compare these insights,
these contribution insights over time, to the span level that I had historically, in order to
understand whether at certain points, we were overspent or underspent in each specific channel.
Now, I'm not going to, I mean, actually, let's let's run these plot channel parameters. I want to
see the alpha, I'm not going to include a lot of insights about this, because it's a little
technical, I don't want to overwhelm you about this, this one is the same. Let's create though,
for each variable, I want to see what's the ROI measure. In here, what we have here is
channel contribution scale, we compute the channel contribution and the regional scale,
again. And we create and we analyze for each media that we have in our media mix, there was.
As you can see here, it shows us not only a deterministic output, but also the probability
distribution of the rows of each channel. And this is amazing, I would say any marketers,
even though this is extremely difficult to understand at the beginning, but any measurement
that we do based on based on our data, it always has a probability distribution, a confidence
interval is a list of values that have 95% probability of being true. And as you can see here,
this probability distribution has a lot of value. So there is this violet value, I need to print
the labels here, but it looks like this violet value can have a row as between three and zero,
while there is this blue one, it is concentrated between 0.5 and one, this is really interesting.
And this green one has a really wide distribution, like this red one. Now, awesome insights so far.
What I want to do, I want to go to Cassandra and see if it's done training the model. So in here,
we have the new model that we've trained. Let's see results. So differently to PyMC, what we receive
as an output from the training is a list of models, they're actually really similar here
with a certain accuracy. And all of them are ranked by lower rank to the highest rank. And here we
have the best. The reason why this is the best is because we actually try to understand what's
the level of uncertainty, the accuracy and the average error this model has. Now, compared to the
model that we've trained with PyMC, we have 95% accuracy with respect of 86% and 5.99% error
compared to 10% error. Now, obviously, I remind you guys that these are square, which is the
percentage of the variations in the input variable described by the output variable described by the
input variables. Now, as we can see, we have the data set, we see that it is really accurate on
average. We see that we have 28% contribution from the baseline. While we saw previously that the
baseline for the model with PyMC was 40%, search contributed for 24% to be 23. What I want to
see here is the ROI of each campaign type. Oh, this is interesting. Wow. The ROI measured according
to this model are way higher than the one trained with the PyMC. Obviously, PyMC has a higher baseline,
so what we're seeing here is that a big percentage of the baseline is not attributed to the baseline,
but is attributed to the revenue and to the input, the media variables. And the confidence
is the role of pre-consistent. The lowest ROI channel is print that can have an ROI between
2.7 and 7.9. Out of them can have between 4.47 up to 10.60. And this is really interesting.
So, let's go back to the distribution. Here we had other roles, split by each campaign type that
went from 0 to 3. That's actually, oh, this is my bad. Next, expand this to 10. All right. So,
the ROAS went from 0 to 4, not to 3. But the ROAS are really, really different. This is actually
really interesting. And here we can see the contribution of each factor over time. And we
can see the ad stock. So, I didn't find a way to export the ad stock from the MMM object that we
trained. We can actually run an additional function, which is deer, deer, open parenthesis MMM. So,
we can actually see what are all the functions that we can call from this object. Ad stock max
lag, but it's 8 is the one that we inputted. I cannot see any function to actually print or show
me what's the ad stock effect. And actually see it into a graph so I can understand how to interpret
it. But then, right, so let's use this data set, this model. I'll save it. And let's dive into all
the insights that we can try that we can derive from it. So, we go to models overview. Let's select
our YouTube model. We have some suggestions. So, it created some suggestions for our media mix.
And these suggestions are appeared here. And they tell us, right, says that Google search can
generate MMM revenue if it increases weekly budget to this volume of investment. But Facebook is not
performing as good as other channels in your media mix. So, we should reduce this weekly budget
down to this volume. This is interesting. I mean, it's implemented here just to show you
and simplified interpretation of the insights. Because most of the times, if you are an analyst,
you need to explain each graph to each marketing stakeholder. And if you need to repeat this for
every forever meeting, because it becomes extremely expensive, operational wise. So,
this is why next to each graph, there is an explanation natural language that explains
what this graph means and what to do about it in order to optimize your media mix.
Now, we saw the confidence intervals. There are between three and 10, which are bigger than the
PMC. The contribution, the baseline is lower here compared to PMC. Seasonality is actually
similar. And here we have a window of one year from 1st of January to December. But the pattern
is really similar to the one that we saw previously with the PMC. What I want to do is I want to check
for each campaign type and the contribution that they had over time. I want to see how much is
factor that inserted in my model contributed to generating revenue. So, let's say we want to
see just Facebook, and this is the contribution of Facebook. We see that Facebook generates
weekly between 149k and 103k, more or less. This is the range. This is interesting. I want to see
the baseline contribution. The baseline is around 500k per week. Let's add seasonality to it. And
thanks to seasonality, we can actually see the variations over time. And it makes sense because
during the end of the year, we have a positive spike. What I want to see is the contribution of TV,
the contribution of newsletters is really minimal. We add out of home, okay, and then we add the
holidays. There is one thing that it looks like that events have a huge effect here. Yes, they
wear the reason why there is a positive spike during the 22nd of April. So, we go back to models
overview. In the models overview, here, these spikes are explained by the event. I want to see
not only the contribution, but also the ROI over time. So, there were moments in which the ROI
was 214, which is crazy. And there is another moment in which TV ROI was really high. Now,
I don't, this is really interesting. I want to know what happens, what happened in those cases. So,
I hide this channel, I just checked TV, and this is the second of September. And here we have the
spend over time of each factor. Let's select only TV. So, here we have second of September.
Second of September, we spent zero, but we had some add stock effect, which means that we can
see it here. So, high channels, here we see. Second of September, we've spent zero, but we had some
add stock effect generated as a carryover effect, generating revenue for us. This is why we got a
this spike in ROI during that moment. I want to see not TV, not out of home, but only Facebook
over time. It's pretty consistent on average. We have 3.5 to 7. A print consistent with Facebook,
as this positive spike, but then it's consistent. And then search. Search is a really nice distribution
around eight ROI. Let's see which one is called reading emission returns. And what I want to do
now is I want to compare the emission returns found from Facebook with the ones from PMC. So,
let's go back to our diminishing returns. All right. So, let's select Facebook. 100k represents
150 more or less, right? 100k of spend represents 381k of revenue. So, this is interesting. Now,
this shows you the mean, right? So, this is how much revenue is attributed to Facebook per each
volume of investment. But the ROI and the contribution can go up to 250k of contribution
in terms of revenue for 100k of investments. It is still underestimating compared to the model
built from Cassandra. Is any other livestock effect? Print has a delayed effect. This is
interesting. The delayed effect shows us what's the lag effect between the moment in which we
invest and the moment in which we see the maximum impact coming from the previous investment,
and shows you what's the carryover effect. So, for how long will I continue generating revenue,
even if I shut down my campaigns coming from that particular campaign times? Keep this. So,
as we saw, we have big differences in the ROI measurements between the PMC marketing.
It is highly underrating these media channels contribution versus the contribution measured
by Cassandra's. Now, I'm creating an assumption here. I'm not an expert. If you have a better
interpretation of why this phenomenon happens with the same data, please share it in the
comments below. But I think that there is a different way on how we handle ad stock for the
specific PMC marketing library. So, this is why we see a different ROI. The easiest way in order
to understand what's the true incremental contribution right now, it would be to run an
increment high test, probably on the highest spend variable, which should be, according to this
model, the highest spenders are TV and search. So, I would run an increment high test on search
in order to validate its real incremental contribution. Now, going back here, what I want
to leverage now is the ability of having an open source library that we can play however we want.
So, we can actually see what's the accuracy in our square NMAPE for test data. At the beginning
of this video, we split our data set in two groups, and we've trained our MMM with the first
training data. And now we want to see how well it can predict the future. But what I want to focus on
is I want to first print all the columns. I want to be sure that the F-tests and the F-trains have
the same columns. And then here, we run this script in which we don't need this, but we define
X-tests as a DF test, as we saw, as we did before, Y-tests the same way. Then we print the shape
just to be sure that the shape is consistent with what we want to predict. Y-pred, which is MMM.predict
X-test. In this way, we predict the mean values that we're going to receive. I'm going to insert
everything into Y-true. We get Y-true, which is the same thing as Y-tests, the actual sales that
we generated, and we compare them in order to measure the R-square, as we did previously. If we
run this, we get the R-square on non-trained data is 40% and the map is 6.2. So the error is lower,
but the R-square is lower, which is not a problem by itself. We have only four data points, which
means that not all the variations in the Apple variables are well-described by the variations
in the Apple variable. I want to run this script again, because I want to see why we got it so
low R-square. The reason why is that in this point, it didn't quite get the incremental value,
and the reason why it didn't quite get here it overestimated sales and here it underestimated
sales, I assume because in here, we did not take into account the actual lag effect that
the ad stock of previous investment had on the VX, and in here, it's probably because the ad stock
that we actually generated is not big enough in order to bring the carrier effect over time.
I'm having a hard time in order to understand how to derive the ad stock effects from this
training model, and this obviously influences the accuracy. This, though, doesn't mean it is a raw
model. It means that, actually, it means that I'm not able to extract what are the upper parameters
for ad stock for this specific library, but it means that it can still be used in order to optimize
the distribution of our budget. Let's do an example. I want to do here, I want to run a
budget allocation simulations, and I want this budget, I want this model, and help me exactly
how I should distribute my marketing budget week over week. Now, it's not really flexible.
The first thing that we need to do is we need to retrieve the diminishing returns
hyperparameters with this function. So I'm going to run this, and then I'm going to use the sigmoid
parameters. I'm going to insert it inside of this function, which is optimized channel budget for
maximum contribution. I'm going to use the sigmoid methodology, which is just the shape
and the function for the diminishing returns curve. I'm going to insert the marketing budget,
and I'm going to invest, and the hyperparameters, which are these ones, I'm going to run this
to other than 8k for the next week of marketing investment. I'm going to run this, and what I see
is a new distribution, a new estimated contribution split by each campaign type, and here we have
the total. So the total is we're going to invest 2.08 times 10 to the power of five, and here
225 times 10 to the power of five, which means that we're going to have an ROI or a ROAS of 1.1.
Take into account this. When you use this budget allocator, it only suggests you the optimal
distribution, not taking into account the ad stock hyperparameters and the ad stock effect of previous
investments that we had in our market mix. So according to this simulation, we're going to have
1.1 marketing ROI. Let's try with Cassandra. So with Cassandra, we have a little bit more
flexibility. So we go to Optimus Budget. I'm going to click new budget allocation. Let's predict
the next week, right? If we do tests, I want to predict week 208k. I want to, this is 2018-11-18.
So going to do this, want to predict only the next week of marketing budget. Let's click Optimus Budget.
Now, if we invest 208k, we're going to generate an optimal ROI of 9.92, which is 35% more than a
previous week, which is completely different from what we saw in the other library. The reason
why is that we take into account all the historical data and all the historical ad stock that there
might be into this simulation. So what we do here is we bring the ad stock of previous investments
killed to the last data point that we have and we forecast it. And if you work a lot with MMM,
you're going to see that ad stock has a really big effect over time because it compounds the effect
over time. Now, according to this simulation, we need to invest less almost on everything,
but especially focusing on a lot, less money allocated to search, a lot less on Facebook,
almost half on print and a little less on TV. Let's compare these two results with PMC. Let's
see how much it suggests to invest on Facebook versus Atovo on print, etc. So on Facebook,
it suggests to invest 5.61, so 56K on Facebook versus Cassandra tells us to invest 84K,
then to invest in Atovo, I think a lot. Actually, this is actually here. No, no, sorry, sorry,
sorry. Here it suggests us to lower the marketing budget by a lot, by a lot. So it's a 5.61 times
10 to the powers of minus 10, which is we don't need to spend anything here and anything here,
I guess. And then we need to spend a lot of budget on print. So 6.39 times 10 to the power of four
should be 63,000. In here, we suggest to invest 14,000, right? In here, we need to invest 56,000,
sorry, here is 63,000 on print, 56,000 on search and 87,000 on Facebook. So on search here,
it says 95K versus the 56K that's suggested on search. On Facebook, we should spend 87,000 and
we suggest invest 84,000, fairly similar. And we should not spend anything on TV, Atovo,
and Atovo. But we're spending here some on TV and zero on Atovo. So these are the two comparison.
What we can do with Cassandra, automatically, is we can implement these suggestions and then we
can refresh the model after one month, right? We refresh the model. And every time we refresh it,
we can compare the actual contribution of media factors versus the factors that versus the
prediction that the budget locator gave us. But we can do the same thing with PMC for sure. It's
a little bit manual. Now, we've got a lot of information sold together. I'm sorry that I'm
