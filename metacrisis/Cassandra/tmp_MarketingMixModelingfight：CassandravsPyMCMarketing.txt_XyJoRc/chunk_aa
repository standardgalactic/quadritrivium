Hello everyone! Today we're going to compare PyMC Marketing, the Python open-source library
for marketing mix modeling. They use a special statistics versus Cassandra, the MMM software for
non-technical marketers that guides media locations decisions to optimize media ROI.
We're going to evaluate these two methodologies based on accuracy metrics, ease of use,
actionability of the insights, and overall utility generated for the company. So stick with me,
we're going to dive into both of them. First thing first, we're going to see what is the data
that we're going to use. Here we have the dot set, it's an extremely simple data set in which we
have the first column represents a data weekly basis, we have revenue at a weekly basis, and then
we have a list of medias plus some organic variables. So first we have tvspan, tvmedia,
autobomb, print, facebook, and search. And these represent our media factors. On top of that,
we have one organic factor that is newsletters, and then we have a context factor, which is events.
Everything is aggregated at a weekly basis, and it has between a thing four years of data,
between 2015 to 2018, is going to be enough in order to train our model. Keep in mind that
best practice is to have at least 10 rows per each variable that we're going to use in our
marketing mix model, which means that if we're going to use 1, 2, 3, 4, 5, 6, 7, 7 variables,
we need to have at least 60 rows. If each row represents one week, which means that we need
to have at least one year and a half of historical data. Now let's dive into the PyMC marketing
library, but before I do, in the description here, you're going to see this file that you can copy
and paste, then go here, file, create a copy, which is a data request checklist for Cassandra's
marketing mix modeling. So let's dive into the PyMC marketing. Let's start with that,
and then we're diving into Cassandra's. So first thing first, what we want to do as a first step
is, obviously, install PyMC marketing. So pip install mark, find some marketing, and the
drain start. What I want to start after installing PyMC marketing is to add some warnings, just in
case if anything happens, I want to know anything, any bug happens, I want to know why it happens.
So let's run the cell. What we want to import right next to it is pandas.py matplotlib,
cborn. These two are for graphs. Then we want to install our viz as az. We have PyMC as pm.
This is not strictly necessary, but I imported it anyway. Something in PyMC marketing doesn't
work. I can actually use the original library, which is PyMC. And then we are going to install and
download all the variables and all the functions of PyMC marketing. PyMC marketing, by the way,
is a really new open source library developed by an amazing team. In the past week, I've
studied the entire open source library. I'm not an expert about this library, but I'll try to
show you in the simplest way possible how you can leverage it in order to generate the utility for
your company. So we download all the all the functions from this open source library, and then
we want to actually download the max upscaler and min maxscaler that we're going to use in our
training phase and in order to standardize our data. So let's run this. Awesome. We've imported
all the requirements. The sns set theme is just a way to set the grid for our graph. So I'm just
going to run it. It's really easy. It's not a requirement. First thing first, what I want to
do is I want to upload my CSV file. So let's go here. Let's Cassandra did request checklist example,
I'll upload it here. And what I'm going to do is copy path, and I'm going to paste it here.
The first thing I do is I import using pandas, the CSV file, then replace any infinite value that
I'm having on my data set. Then I fill all the na with zero. So if I have any na value or any cell
in my data set has a non value or an infinite value is first transformed into non and then from
non to zero. Then I check if I have any non values and I print. Obviously, I want to know if there
is still any non values after these operations. This phase is extremely important in order to
validate manually. If the data set that we have has some errors inside as some it requires our
data cleaning operations. So going to run this looks like that we have no non values in our
data set and we've imported everything smoothly. So we're going to create and export the list of
the columns names that we have in our data set to simplify how we're going to fill the form of
identifying with our variables without without the output variables, etc. So let's run the f
columns. We have a list of our variables. And now let's define the output variable as revenue,
the date variable as date week, as we can see. Here we have a script that allows us to identify
and create a list of strings that represents the names of the media variables that have
spanned inside of their of their label. Now you can see here we have TV span out of home,
print, etc. And they're automatically included into media. Then we have context variables,
which are advancing and use letters. With PMC, I'm fairly sure that we don't have organic
variables feature that we can identify. So everything else that is not media is identified
as a context variable. So let's print media and see if it works. Awesome. We've imported
all the variables automatically with one click. Now, I've used Chai GPT in order to create these
scripts for building a seasonality. So what I want to extract from the patterns of my sales
was the seasonality of my business or the business that they were going to model. And it works in
this way. So function called extractable seasonality, we need to define what is the data frame,
what type of seasonalities we want, and what's the window size. So how many records represent
one month. And what we do, I mean, this is the function, we can copy and paste it.
So we run it. Also, we've run the function, we can call it now. Now, what we want to do in this
cell is first, identify what are the seasonality features. These identity features need to be
the Apple variable and the date variable. I want to print them just to be sure that I did
the right job. And then what happens here is seasonal DF, which is a dataset composed by
only the variables that are going to help me understand what's the seasonality. So it's going
to be DF, which is our data frame with our two columns. And I'm going to copy it. I'm going to
rename, I'm going to add a new columns that is going to be called DS. And DS is a normal label
name to identify dates into a dataset when you use profit or use in this case, this function.
We're going to rename revenue with Y, just to standardize. And we're going to identify that
the seasonal configurations here are the period, the quarterly period has 12 weeks inside,
and the yearly period has 52 weeks inside. In order to simplify and identify how you need to
model the seasonality here, you need to identify these two parts. It might seem a little complicated
at the beginning, but believe me, when you get used to it, it becomes really easy. So the last
thing that we do is we insert everything inside a variable extract season. So we create a new
dataset, mainly, and we run the function extract for the seasonality defining what is the data
frame that we're going to use, the configurations, our seasonalities, and what's our window size.
So I'm going to run this cell, I have the right columns, I have the seasonality. If I want to
check if there is any graph that we can run from this, right, what I would interest me is this one.
This is the graph that represents my multiplicative seasonality. So how
did the man changes over the year, over time? And it looks like there is a pattern that repeats
itself over time. Now, let's print this in an actual graph. This function in this cell just
works in order to simplify and show you all the parts of the quarterly seasonality and the yearly
seasonality. Let's run this. Awesome. So as you can see here, there is a pattern repeats itself
quarterly, and a pattern repeats itself yearly. We want to import these two inside of our actual
dataset and add the quarterly and yearly variable inside of our modeling experience.
So what we're going to do, first of all, we're going to split, create a variable that's called
weeks to split separate the test group, the test data frame with a trained data frame,
and we're going to split by four weeks. So we're going to leave the last four weeks
as an data set as a data points, they're not going to be used in order to train our model.
We need to identify, we need to add a new column called the F quarterly, and we're going to insert
the values that are in extracted season quarterly, same thing for yearly. And what we did basically
we added in this part two columns, they're going to be then use in order to train our model and
added as a context variables. Let's recheck and substitute all the infinite values into nonce
and from non values into zeros, we don't want any non value or the modeling procedure in the
training does not work. Then what we want to do is we want to separate the data frame for training
and the data set for testing. And then when we want, and that's it. So let's run this cell. Awesome.
So in here, what we are to do is what we want is we want to create, first of all, the context
variable list, I'm going to add from context the two new columns that we added into our data set.
Second thing that we do is we call MMM, our object that has the function inside called
delayed saturated MMM. This delayed saturated MMM is going to use ad stock and diminishing returns
inside of the training procedure to find the optimal ad stock, the optimal diminishing returns.
And we need to define first, what is the data frame that we're going to use? What is the target
column? And we did what is the date column here we did channel columns, we created the list inside
of media control columns is a list of strings that we actually did here, the ad stock max flag,
which is how many lag effect that can be at a maximum in our data set. And it is really wide,
it means that there can be a two month lag effect, which is crazy. But we're going to keep it like
this. And we want to have your decisionality, we want to model the early decisionality and
consider the last two years. So we're going to run this awesome, it worked, we're going to rename
x and epsilon just to simplify our training procedure. And we're going to add into x all the
values that are not the apple variable or dates, why we're going to define only the apple variable.
So we're going to run this awesome. And then we here, we start the training procedure of our MMM.
Now, these MMM is not really complex. I know I tried to model it before. And it's not really
complicated. It's really easy. There is no multicollinearity. There is consistency on the data.
There are enough records on each variable. So I'm not going to complicate it too much. I'm going to
use x epsilon MMM dot fit x epsilon, just simple target accept is the statistical significance
of each coefficient that is going to come out number of chains. So how many iterations it needs
to do, and how many tests and experiments and draws needs to do in order to train it in order
to tune it. So I'm going to run the cell. And it's going to take around 20 minutes, more or less
in order to finish. So I'm going to leave a train in a little second, we're going to see a bar plot
that actually loads over time. It started loading here. It's going to take some times in order to
load here. While it trains, let's go and use Cassandra in order to train our model, because any
training is going to take some time. So let's go inside and see how it works.
So we are inside of Cassandra. The first thing that we want to do is we want to add model,
I want to use my own data set, and I'm going to drag and drop our CSV file.
Awesome. So we've added our CSV file, we click next. And in this case, we need to do the same
thing that we did previously, we need to fill the form just to tell the model what each column
represents. So let's call it YouTube, date column, which is date week, the output variable type is
revenue, the KPI is revenue that we're going to model as an output variable. The media channels
are these ones are already selected, paid media, organic and contextual. Let's define the country.
Let's use not a state set thing. And now what we need to do now in this experience is just to
click next. That's it. Everything looks correct. Let's continue to the modeling procedure. So
dear, what we have here is an EDA analysis, already down for us that actually shows us the
pattern of each variable over time. We identify as if there are some warnings. And these warnings,
in this case, our DB, at the one previous search have lowest band volumes. And this is interesting.
So I did not know that. I thought I was using a data set that actually was working with no
problems. But it looks like there are some problems. In fact, if we hide all channels,
and we check on your Facebook, compare it with TV, we see that the dimension of this data is
completely off. This is probably because there is a problem with the data set that we need to check.
So let's go back to our data set here. And it looks like that we're taking three, yeah, 72 million
per week is a little too much. What I did wrong probably is Facebook spend one is I formatted
wrongly this data. So what I'm going to do is I'm going to divide this by 1000, not 10,000,
but 1000. And this is the actual value. I did this mistake. I did not expect that. But thanks to
Cassandra, actually, could check this before starting the training. Now going to go back here,
going to download the data set, be uploaded here. Awesome. Click Next, fill the form again, YouTube,
define this is revenue. Let's call United States in order to get the holidays from this particular
country and model the seasonality with it. Click Next, receive back the EDA analysis again. Now
our thing looks more concrete and with the same dimension. We see the revenue over time versus
this spend over time. We see all the other variables here. We want to create a model now.
We click create model and a modeling procedure actually is done. The only thing that we need to
do is click train model. And the training starts. So it's a three step procedure in order to train
your MMM. All right, training started. Click okay. We check the status. Okay,
training is starting right now. If it says it's not started yet, we need to wait for a couple
of minutes. We refresh it and we check status. Okay, not yet, but it's going to start really soon.
So we need to go back to our PMC marketing training because we noticed wrongly that our
data set is wrong. So we're going back to our PMC. Yes, it finished the training, but we need to
retrain it again. So going to delete the old data sets that we added here. One, two, and three.
Let's add the new data set, copy the path and add it again on top of it. We need to rerun everything.
So really quick. Nothing is too heavy. Everything used to is really easy to do. Now it should start
MMM fit really soon. It's going to take a couple, I think, around 10 minutes in order to finish
training. Awesome. Took around 10 minutes in order to train this model. Here we have some
posterior analysis, which are extremely hard to translate into channel value. So this is why
we're going to use graphs in order to explain all the information that we can derive from this MMM.
So first thing first, let's create a component fix MMM dot plot component contribution. The cool
thing about PMC is that it is really easy to create plots with the outputs and drive
what is the accuracy with all our actionable insights that we can implement. So let's run this.
Awesome. So in here, we see the contribution that I have. Everything is killed down with the
min max together that we use. But we can see that there is 40% of the contributions explained by
the intercept. Then we have some other variables. The medias have between zero and 20% of contribution.
And then the seasonality has a plus 20 minus 20 contribution in our media mix. This is really
interesting. And it's a cool thing to summarize all the variables that represent media, represents
control and the seasonality contribution in this way, because it simplifies and actually shows you
some macro insights that we can derive from the first training that we do. So after that,
what we want to do here is we want to get a contribution of each factor. So how each factor
contributes to generate sales and how many sales are coming from TV, out of home, Facebook, etc. So
let's run this. We created a data set in order to see what is the contribution of each factor.
And then what we want to do, I'm not going to spoil you with the real diminishing returns curve,
but we want to get the diminishing return insights. So we want to see what's the contribution of each
factor based on the volume of investment that we run that we have. And the cool thing about
PMC is that we can have the confidence interval. So we can have range that represents
the probability of the 95% probability of having that specific output if we spend X, for example,
everything is scaled down with is called down here. This is the contribution. And this is the shape
of the curve. If we want to see out of home, for example, same thing here, we see that if we invest
400k, we get around 150k of revenue. So ROI is really low here. Let's see if we have something
with Facebook. With Facebook, if we invest 75k, we get 100k of contribution. This is interesting.
It looks like in this MMM, this channel works really well. But the value can be between 25k
of contribution if we spend around here up to 200k of contribution. It's really important to
understand that any statistical analysis that we do needs to have some idea of confidence interval.
The idea of confidence interval just show you what is the level of uncertainty that each measurement
has. And having that into each specific graph that we run is extremely helpful in order to
calibrate our decision making on top of these insights. Awesome about this. So we see in here
with 100k, we get a little less than 100k of contribution out of really strange values,
really big values. This is interesting. I think this one summarizes, yes, this graph that we're
going, the running summarizes everything into one plus. So we can compare them. We got to compare
each graph simultaneously. So we can see that TV spend has the highest contribution based on the
X factor, which is how much we invest. And there are other values that we can explore manually.
But what I want to show you step by step is, first, what are the insights that we can unlock,
and then we evaluate the results, comparing the results with the MMM built by Cassandra.
