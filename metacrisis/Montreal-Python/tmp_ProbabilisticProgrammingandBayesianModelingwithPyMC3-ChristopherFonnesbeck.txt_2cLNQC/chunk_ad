So we're changing the problem from a sampling problem,
which is great, but it can be slow, to a straight optimization.
We're going to optimize the crap out of this approximation
as well as we can.
And so what do we mean by close as we can?
Well, the measurement for this we use
is essentially an information distance.
This is called the Kullback-Liebler divergence.
It tells us how far away one distribution is from another.
So Q is our approximation, P is our true posterior distribution,
and all you need to know about the math
is that it gives us an expected value in terms of Q, which
is the thing we know, if it's a normal distribution,
so we can work with it.
We can't optimize this directly because it contains
the posterior and we don't know what that is,
but with a little bit of math that's way over my head,
we can rearrange it and get a quantity that we can deal with.
And this is called the evidence lower bound.
So we're going to try to maximize this evidence lower bound.
It's the same as minimizing the Kullback-Liebler distance.
But again, just like with nuts, there's
some choices or tuning to be done here.
We have to use pick Q. How do we pick Q?
It's got to be a distribution that's useful.
We may not know what our posterior distribution looks
like, things like that.
And similarly, in the last few years, 2016, Alpe
Coseckelbier, also out of Columbia, the same as Gelman
and Hoffman, came up with an automated,
this is called ADVI, automatic differentiation
of variational inference that will just start
with normal distributions, transform them
into a real coordinate space, and standardize them
for everything so that it works across any problem.
And so what we get when we do variational inference is this.
It kind of looks like MCMC, but these aren't the values.
This is that elbow.
So I've hit some sort of asymptote here.
And whether that's a good place or a bad place
depends on how good my approximation is.
And to kind of give you an idea of what that looks like,
here's a beta distribution that I'm estimating.
It's this dashed line here.
And each of these curves is an approximation based
on 100 through 10,000 optimization durations.
And this is just straight optimized.
You know, BFGS or Neldermead or whatever optimization
that you want to use, and it's fast.
And you can see that in this case,
it does a reasonable job.
And in PIMC, all you do, you take your model, whatever
it's called, and rather than call sample, you call fit.
And then fit will choose an appropriate approximation.
And what we get out the other end
is not a bunch of samples, but this approximation, which
again, is a distribution that's been fit.
But because it's a distribution, we
can draw samples from it.
So we take that approximation and sample from it
rather than the true posterior distribution.
So we get what looks like MCMC samples,
but these are just, again, approximations.
But as they say in the machine learning world,
there is no free lunch.
These are approximations.
They generally aren't as good unless you get lucky
or your problem is simple enough.
So the blue line is the ADVI approximation,
and the nuts is a better approximation to the posterior.
And this is what you see in general,
is you tend to underestimate the variance,
because it does what's called a mean field approximation.
It kind of assumes all of the variables in the model
are independent of one another.
But it works a lot faster.
So if you have lots and lots of data,
you may be willing to make that trade-off.
And it's made faster still by the fact that we can minibatch.
So what I mean by minibatch is that rather than throwing
all of the data at the problem at once,
at every iteration of my optimization,
I throw just a random batch at it instead.
This has two advantages.
One, the computation time decreases.
And two, it does what's called stochastic gradient descent,
which tends to be more robust.
It's kind of noisy gradients rather than non-noisy gradients.
So they tend to converge faster.
So that's really cool.
So MCMC and ADVI are kind of the two main ways of using PMC.
And this fits very appropriately into machine learning.
You can combine probabilistic programming,
called Bayesian machine learning, if you like.
Thing about machine learning models
is you tend not to account as much for uncertainty,
particularly.
You tend to get a prediction or a probability
and not an entire distribution.
And so they can sometimes be easy to fool,
harder to interpret.
You can totally fit machine learning models in PMC.
And here's an example that Thomas Vicki, who's
one of our core developers, demonstrated a couple of years
ago, this is a Bayesian deep learning model.
And so this is just a neural network with two hidden layers.
So deep learning is anything more than one hidden layer.
It's deep, so this is deep learning.
And all that we do to Bayesianize it
is we take the weights of the neural network,
and we put priors on them.
Here we're just putting normal 0, 1 priors.
And this is the whole program in PMC.
So here's the weights from the input to the hidden layer,
first to second, hidden layer to output,
and then a set of activation functions.
And our output is a binary classification,
so it's a Bernoulli random variable.
And what you see on the side here
is more than you usually get.
This is not a decision boundary, which
is what you would get if you did support vector machine
or something like that or a deep neural network.
What's being shown here is the posterior standard deviation
of the estimate of the probability of classification.
So you can see the darker it is, the more uncertain it is.
So everything close along the boundary is uncertain.
Everything light and away from the boundary is very certain.
And so you can get an idea of how reliable your predictions are,
which is important, right?
You want to know if you're going to use this to make decisions,
you want to know what the risk is.
OK, I'm almost out of time.
So in terms of the future, we're lucky enough this year
to have some Google Summer of Code slots.
We've got a student in Argentina, Augustina Oroiello,
implementing approximate Bayesian computing,
so yet another algorithm for fitting these models.
Bill Engels is going to continue working
on Gaussian processes, which is really great.
And then we have another student, Sharon Yalburgy,
who's going to be working on a TensorFlow
back end for PMC3.
Unfortunately, the Tiano project is shutting down
after many, many years.
It's kind of served its purpose.
It's essentially prodded lots of companies
to make very robust and powerful open source deep learning
engines.
And so we're going to transition into what
we would call PMC4, hopefully using TensorFlow.
That's what we're going to try first anyway.
And Google Summer of Code is going to start that for us.
For those of you interested in learning more about this,
our project, the PMC repository, has a whole bunch
of Jupyter notebooks full of well-documented deep examples,
everything from regressions to survival models
to machine learning models.
And of course, I'm biased towards PMC.
We live in a great time now, rather than back
in the 90s when you had just had WinBugs.
These are just the Python tools for doing probabilistic
programming.
It doesn't not even any other languages.
Edward, which is also now TensorFlow, the Stan team
has a Python interface, and so on.
And then if you want to learn a bit more about Bayes,
a local Montrealer, actually, Cam Davidson Pilon,
I think he works for Shopify, wrote a nice open source textbook
years ago based on PMC2.
And it was recently ported by Max Marginot and Thomas Vicki
to PMC3 and Python 3.
And so you can go on there and learn
all about the basics of PMC and probabilistic programming.
So with that, I'll close.
This is our core team.
I'd like to thank all of them.
This went from a three-person operation for PMC2
to more than a dozen now.
And as a result, we have a much, much better project.
And I've been able, if I've done anything for the project,
I've been able to recruit people who are smarter and better
at all this stuff than I am.
So I thank them.
And I thank you for hanging out and listening.
So you said that we can't do integrals, but we can do derivatives.
It means that for the Monte Carlo simulation,
it's actually symbolically computing the derivatives.
It is a up-fiano does symbolic differentiation, yes.
And that's what's needed.
Yeah, you need that somehow.
Stan has built its own engine for doing that in C++.
TensorFlow can do all.
And we've just hijacked a deep learning engine
to do probabilistic programming is the idea.
So you can do it with any of those.
But yeah, so that's what the graph does.
You build this big static graph, and then
it's able to reason over that graph
and come up with the gradients.
