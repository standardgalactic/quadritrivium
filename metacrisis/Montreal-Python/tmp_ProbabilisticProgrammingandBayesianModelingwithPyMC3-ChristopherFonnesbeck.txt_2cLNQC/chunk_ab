My background's in biology.
I'm not very good at calculus.
I can do integration of one variable, or at least I could about 20 years ago.
Two, I have a lot of trouble.
Three, forget it, and most of you probably can't do three even if you're
mathematicians.
Most models will have hundreds, maybe thousands of variables.
So doing Bayes is really hard.
And that's one of the reasons that you didn't learn it in school,
particularly if you went to school as long ago as I did.
It was just really impractical to use.
And so with probabilistic programming, we're able to use complicated numerical
methods to approximate that.
And probabilistic programming abstracts those away so
that we don't have to be experts on all of these things.
And like I said, probabilistic programming is not new.
So when I was a graduate student, there was really only one way to do this.
And there was a package called WinBugs.
The win, of course, means it only ran on Windows.
But it was really great.
In the sort of the mid-90s, you had this kind of dashboard here.
And you could specify, draw samples.
You could watch the samples come in live here.
And you get summaries of everything.
And it really made it easy to describe and build and
fit and share Bayesian models, particularly for non-experts.
Again, I was a biologist.
Most of the people that use this are not statisticians or mathematicians.
And even better is there's this really nice domain-specific language
that he used called bugs, sort of an R-like syntax.
For any of you familiar with R, this looks very R-like, right?
This is a complete model, a complete hierarchical model in seven lines or so
here.
And this was really great.
It allowed me and others to do lots and lots of things.
But after a while, you kind of hit your head on the ceiling, right?
There's a few things wrong with it.
A, it was closed source.
And again, it's this kind of domain-specific language where you had to
get everything into bugs and then get everything out when it was finished.
It was coded in Object Pascal, which now that it's open source,
how many Pascal programmers in the room?
Wow, okay.
Yeah, I gave a talk in Denmark recently.
I raised that and it was like half the room with their hands up.
I was like, it's a weird place.
Yeah, I don't know.
So there are a lot of problems with that.
And so back when, again, when I was a graduate student, I had lots of time.
I didn't think I had a lot of time then, but I really did have a lot of time.
I kind of cobbled together using a language that I like to use and
which I did use for most of my other work.
Tried to kind of re-implement this stuff using Python.
And so I started this round back in 2003 at the University of Georgia.
And really what it is, it's a probabilistic programming framework for
fitting arbitrary probability models.
So it's not in any one particular class of models, not just regression models.
Any model you can write down in math, you should be able to implement in PMC.
It's based on Tiano.
This is why I come to Montreal.
A Fairbent Tiano is a package that was produced from the Lisa Lab.
They changed their name, right?
It's the Miele Lab now or something like that, the University of Montreal.
And it implements what we call next generation Bayesian inference models
that use gradient information.
And I'll talk about that in a little bit.
We have over 100 contributors now, about a dozen or so core contributors.
It's used quite a lot in academia and industry.
Companies like Quintopian and Monotate, Grubhub, Channel 4 over in England,
Allianz Insurance, things like that.
And of course it's on GitHub and freely available.
And so what Tiano does, this is kind of the sort of the computational engine
behind PMC 3.
PMC 2 is largely a Fortran project.
It's mostly Fortran with a little bit of crunchy chocolate coating
made out of Python on the outside so people didn't have to code in Fortran.
Same thing here, Tiano is the engine now.
And what Tiano is, it's sort of a meta language for specifying and
evaluating mathematical expressions using tensors,
which are just generalizations of vectors and matrices.
And it's really built to do deep learning.
That's why it came about in much the same way as TensorFlow or Torch.
Tiano kind of led the way, in fact.
And what it does is it dynamically generates C code from that.
And so this is kind of what it looks like.
So what we're going to do here is construct a matrix, populate it with values,
and then take some gradients here.
So if I specify a matrix, I'll call it x.
And then here's a function of that matrix.
This is an inverse logit transformation.
So I'm going to transform the values to the 0, 1 domain.
And then this is the cool part.
Just take the gradient of it automatically, just like that.
That's the magic.
And then you turn that into a function.
Now, up till now, no calculation has occurred whatsoever.
All that's being done here is a graph
that's being built, a static graph that Tiano can use.
It will optimize.
It will learn how to do the gradients over that whole thing.
And then the only time it actually does anything
is when you call the function for the first time.
It will compile it to C and run it, and so on.
And so this is the gradient of matrix, transform matrix.
So this is great.
And so this powers everything that's done.
So it's always best to show real-world examples
for this sort of stuff.
So I'm just going to show you what a model looks like in PIMC3,
using an example that's actually in our set of tutorial
examples in PIMC.
This is a data set from Britain across the turn
of the last century of coal mining disasters.
So every year, safety back then, I guess in mines,
isn't what it is now.
And so there are a fair number of disasters in coal mines.
And these were just counts from about the middle of the 19th
century to the middle of the 20th century.
And it's a good example because it's this nice count data.
But you can see the counts kind of change, right?
They're kind of high.
They're kind of around three near the beginning.
And then somewhere around the turn of the century,
it kind of drops.
And you still get some bad years, but on the whole,
they tend to be lower.
And so what we're going to do here
is model this count process.
And we're going to hypothesize that there's
some early mean that's kind of high,
and there's a late mean that's kind of low.
But we don't know where it is, right?
We don't know where this kind of switch point is.
And the great thing about Bayes is that anything you don't know,
you just make it a variable and estimate it.
And that's what we're going to do.
So there's going to be three variables.
There's going to be early mean, late mean, and then switch
point.
They'll all be random.
So first step, prior distributions.
We talked about these before, right?
So I'm going to choose exponential distributions
for these rates.
Why?
Because they're positive continuous values.
Rates can't be negative, obviously.
You can choose other ones.
It's always good to test whether you picked good priors.
That's a different subject.
And then switch point, I'm just going to say,
this is going to be complete.
I'm going to pretend I didn't look at the data.
I know it's somewhere in the middle,
but we're just going to allow it to be uniform
across the time series.
It could be anywhere in there, right?
And then the prime C code looks like this.
The cool thing is that it hijacks this context manager,
which you usually use for things like opening files
or sockets, sockets, ports, and things like that.
We're going to use it here to open a model.
And we're going to populate the model with variables.
And that saves us from doing the sort of thing
that you see in Keras, if you've used Keras,
where you have to add this variable,
and you have to have all these add statements.
This kind of does it with a bit of magic.
Anytime you declare a prime C variable
inside the context manager, it gets
added to that model, which has a nice named disaster model.
So there's my switch point, uniform
between the lowest year and the highest year,
early rate, exponential, late rate, exponential.
And prime C has 40, 50 pre-specified probability
distributions, all the ones you'd probably ever need to use.
But you can customize it to do weird distributions
that nobody ever uses.
And the point here is that the motivation behind prime C3
is this high level language for specifying these models,
where you have almost the same number of lines of code
as you do of math.
There's very little extra stuff going on here.
So what happens now?
What do we have?
Well, if we look at some of these things,
the type of the early rate, for example,
it's this prime C object called a transformed RV.
It's actually been transformed to the real line
rather than positive value, because it
makes sampling more efficient.
And I can do things with this.
So this is the probabilistic programming thing.
These are our primitives now.
I can do things with them.
They have attributes.
So here's the log probability of the value 2.1,
which happens to be negative 2.1.
I can take four random values from that distribution.
I can do anything I want with it.
