Thanks to the Montreal Python organizing folks for letting me talk while I was here
in Montreal.
It's great to be here, and I love coming to Montreal.
I try to do it at least once a year.
Thanks also to whoever opened the windows.
I'm going to be talking about statistics, so combining statistics and lack of oxygen
is a recipe for disaster.
So I'm ostensibly a professor of biostatistics at Vanderbilt, which is in Nashville.
But I'm really a data scientist, so what that means is I'm the worst programmer in
a room full of programmers.
I'm the worst statistician in a room full of statistician.
I'm the worst basic scientist in a room full of basic scientists.
And PIMC in general is a project that I started back when I was a postdoc, when I really didn't
know anything about Python programming.
And so I'm going to talk a little bit about this here.
So before I do that, this is way different from the first talk, which I enjoyed a great
deal.
I'm certainly not doing any live programming, sorry.
But a couple of questions, though.
How many folks in here would describe themselves as data scientists?
A few?
Good.
How many know what Bayesian statistics is?
Oh, OK.
That's great.
Awesome.
OK.
That's all I need to know.
So about this time last year, I did a different meetup in London.
There's a Bayes meetup over there, which is really great.
And it was held at the Imperial College there.
And much like this meetup, what you do afterwards is you go to the pub.
And so there's a pub that you go to after the Bayes meetup in London.
It's called the Artillery Arms, and it's in East London.
And right across the street, if you look out this window here, there's a cemetery called
Bunfield Hills Cemetery.
And John Bunions buried there, and Daniel Defoe, and William Blake, all these writers.
But if you go just inside the gate and to your left, the big Crip that you see here
is Thomas Bayes.
So you can sit there, give your talk about Bayesian statistics, and pour a little bit
of beer on Bayes, great.
So first of all, what is probabilistic programming?
It's really not a new concept, but I thought I should describe it before I go on and talk
about software that does it.
It's really the easiest definition, or maybe the least useful one, is that it's any program
that's partially dependent on random numbers.
And so the outputs of these programs are not deterministic.
And it can be expressed in any language that has a random number generator.
So certainly you can do it in Python.
And what it really amounts to is, essentially, you're adding another set of primitives to
the programming language that you would have otherwise.
So and with these primitives, you're able to do things like draw random numbers, calculate
probabilities, given those random quantities.
So you can have, for example, distributions over values, so you can say something normally
distributed, bell curve, and draw random numbers from that.
You can do more complicated things like having distributions over functions.
So rather than drawing single integer floating point values, you actually, you know, a realization
is an entire function.
In a longer version of this talk, I talk about Gaussian processes a little bit.
But an important thing that allows you to do is to condition random quantities on one
another.
So if you define, just say, a quantity P, like a probability, as a beta distribution,
just a particular kind of distribution that has values between zero and one, it's good
for modeling probabilities.
Well, then you can take another random quantum, another random stochastic primitive and condition
its value on the value of that.
So Z here, Z is a zero one based on that probability P.
And that's the important thing, is it allows you to kind of condition things and really
have kind of building blocks to let you to build more complicated things.
And so why do we do this?
Well, really, most of the time, we're doing it to facilitate Bayesian inference.
And so there are lots of hands here that recognize what Bayes is.
I'm not going to, I'm going to have like two minute talk about what Bayesian inference
is, and then I'll move on to the software implementation.
The most important thing you have to know about Bayes is that it deals with a different
sort of probability than, say, the statistics that you learned.
Most of you probably learned in school.
And most importantly, it's the fact that we're doing something called inverse probability,
where we're modeling effects based on causes.
So the only notation I'll show you here is that we have things that we don't know,
which are theta, parameters, unknown values, future predicted values.
And then there are things that we know, things we've observed, which is why, our data.
So everything in Bayes can be classified into two things.
One, things we don't know and things we know, or things we haven't observed and
things we've observed.
And we can use these quantities in conditioning statements to help determine
what the causes might be.
So we've observed the effects, why we're going to see what the probability of the causes are.
And everything can be thrown into that theta.
Anything you don't know, you can put it in your model, and
that's one of the powerful things about it.
So why do we need a whole different type of statistics?
For me, it's pragmatic.
Some people have kind of philosophical preferences here.
For me, again, way back when I was a graduate student,
it helped me solve problems that I wasn't able to solve before with sort of classical statistics.
The important thing is they're very useful.
It allows you to build really complicated models that you couldn't fit otherwise.
And ironically, it's because it allows you to build these things
from kind of simple building blocks.
So this is why Bayesian statistics is called Bayesian statistics.
This is Bayes formula.
And the important things are along the top.
So we have unknowns theta that we say before we do our experiment or
collect data or observe the world, we have some information about.
So the probability of theta is our prior probability.
It's what we know before we've collected any data.
What we want is after we've conducted our experiment or our study,
what do we know after we've collected all this information?
So it's a process of updating priors to posteriors.
And the way that we do that is through a likelihood function.
So those are kind of the three main components.
And then when you're done, you can take that posterior and
make it the prior the next time that you use it.
And you go collect more data and update it again.
You can kind of turn the Bayesian crank that way.
And the big advantage to doing this is that everything here is in terms of probabilities.
So all outputs from probabilistic programs will tend to be entire distributions.
So rather than just getting a mean or a median or some statistic,
you get an entire distribution.
This allows us to say things like, well, what's the probability this is greater
than zero?
So I build generally models of infectious disease systems.
And so this was some co-infection effect.
And we can see that it's almost certain that it's greater than zero in
a probabilistic sense.
You can pull arbitrary values from this.
So once you're able to get this posterior distribution,
you get a lot of stuff kind of for free.
The stochastic program then is where the probabilistic programming comes into play.
So we're able to specify priors and
likelihoods and come up with some joint distribution of everything in our data.
So the first step in doing Bayesian inference is to write down your model
in whichever languages you're going to be using.
And so just how do you do this?
What constitutes a prior?
What constitutes a likelihood?
So prior distribution generally just quantifies the uncertainty in whatever
variables that you're interested in fitting here.
So this is a normal distribution with a zero mean and a standard deviation of one.
And it says that we're reasonably sure that things are somewhere between
negative three and three with quite a bit of certainty.
This is also a normal distribution, that line across the bottom.
But it's got a standard deviation of 100, so a variance of 10,000.
And so here we're saying we don't know,
it could be essentially any real value almost.
So this reflects kind of lack of information, if you like.
This one was highly informative, this one's not very informative.
And we can pick these based on, it's best if you impart any prior information that
you might have.
So if I'm modeling a disease and it's a rare disease,
I'm doing kind of a model of rare disease prevalence.
I might pick something like a beta 150,
which has all its probability way down here, right?
Most of the people don't get the disease.
If I'm, say I'm a baseball fan and my favorite player gets a hit in his first
three at bats of the season, what's the probability he's gonna hit 400 or
300 for the season?
Well, you wouldn't put a flat prior on that, right?
Because we've been playing baseball for over 100 years.
There's lots of data that this prior actually comes from all of the data on
batting since the turn of the century.
On average, major leaguers hit 26.1% of the time,
with a standard deviation of 0.034.
That's prior information.
There's no way he's gonna hit 900.
There's no way he's gonna hit zero.
He won't be in the major leagues long enough.
So that's kind of the idea, you put whatever you know about the problem before
you collect your data into the problem that way.
How about the likelihood?
This is where our data comes into play.
So what we're coming up with here is a data generating mechanism, if you like.
So how did the data come to be?
And here too, it comes down to picking an appropriate distribution for this.
And this is kind of the knack.
This is kind of the art, if you like, to probabilistic programming is seeing
which distributions should be used in which cases.
And so, for example, our data might be normally distributed.
If it's human heights and weights, for example,
they tend to be normally distributed, blood pressure measurements, things like that.
If it's baseball, right, binomial distribution.
So it's in N chances, N number of at bats, and you get X hits.
You can model that with a binomial distribution, and
the batting average would be P in here.
If we're running a website, we wanna know how many unique visitors per month or
year, whatever, we might pick something like a Poisson distribution,
which is for counts.
So different distributions are good for different things.
And then we combine all of these things together to get a posterior distribution.
There's our likelihood, there's our prior.
And what I've written here is this little symbol here means proportional too.
It's not quite equal.
It's equal up to a constant.
And the constant, I kind of glossed over when I showed you Bayes formula.
It's this probability of Y, the probability of the data.
This is a marginal probability.
What it is, it's just the numerator integrated over all of the theta.
So you integrate all your variables.
So I'm a really bad mathematician.
