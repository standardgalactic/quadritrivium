Were you able to find the breaking point for the British minds?
Yeah, thank you.
Oh, didn't I show you all these?
Get down with the results.
Yeah, it was around halfway.
I mean, that's the cool thing is that you get a distribution.
There's one of those iPython notebooks
has the whole worked example.
But it was around about, I can't remember which year,
but the 40th observation, I think the 40th year,
and you get a nice distribution.
You don't just get one.
Did it correlate with some regulations or anything like that?
I would imagine so.
Yeah, I'm not sure.
And that's a simplification, right?
It probably wasn't just a switch point.
In my longer version of this talk,
I show how to fit a Gaussian process, which
I think I mentioned it earlier on,
is it's a distribution over function.
So we posit just a function of some kind.
Maybe it does that, maybe it wiggles a little bit.
And so you can come up with a function that's
more complicated that allows it to vary within those two
intervals.
So you can do something simple like that
or something more complicated, depending on what you need.
Probably the end of one of the world's most.
Could be.
Or yeah, I could just, I bet you,
it was some technology or something like that.
Or regular, it could have been regulations.
I mean, the century in England, you know?
I'm watching Peaky Blinders right now, so you can do that.
For that example that you gave us, like when you were fitting it,
at least I guess it was a gift or something, right?
Yeah.
But I mean, how fast is it really for something that size?
That was it?
So that was like real time?
That was real time, yeah.
That was very big, wasn't it?
So 111 data points and three parameters.
So the thing about the Hamiltonian Monte Carlo algorithm
is that each iteration of it is slower than you
would get in metropolis.
But it's a far more efficient sampler.
So you're going to keep all of the samples you get.
If you use metropolis sampling, you throw away 75% of them
for the most efficient algorithm.
So I don't know.
It took four seconds.
I'm happy with that.
And again, it scales well with the size of the model.
If you increase the numbers of parameters,
it's still quite fast.
When the data increase, though, it doesn't scale quite as well.
So when n gets large, things can slow down quite a lot.
Now that's 1,000 dimensions.
No, no, no, that was three dimensions.
Remember, I had three parameters, 100 data points,
and I think that's about it.
I mean, typically, I fit biomedical examples
that have dozens to hundreds of parameters.
And I don't know, it'll take five minutes, seven minutes,
something like that.
So yeah, it depends on what you need.
Maybe you could code something that's faster,
but it's not as general as this tool.
So you could have a super optimized algorithm
for your particular domain's problem.
But then when you go and do your next problem,
you've got to do that all over again.
And so the other thing with the PMC
is that you can apply this to any probabilistic programming
application.
Thanks.
