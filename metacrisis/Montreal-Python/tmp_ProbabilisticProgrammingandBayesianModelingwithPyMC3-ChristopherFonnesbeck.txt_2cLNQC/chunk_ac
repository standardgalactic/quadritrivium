And then I can transform variables arbitrarily.
So my rate here is going to be early mean
if t, the time, is less than the switch point,
or late mean otherwise.
And I can use this switch statement here for that.
And notice I don't have to do a loop here.
Everything's vectorized.
So it does this vectorize.
And this is really just theano code disguised here.
And there's nothing random about this.
This is all just deterministic transformations.
And then our likelihood, here we're going to use a Poisson.
It's what I used for the website visit counts before.
Now I'm using them for disasters.
And so whatever that rate is for any given year,
it'll be a Poisson draw from that.
And my data are the disasters.
And in prime C, the only difference here
that distinguishes this from a prior
is that I've got observed.
This observed flag includes the data.
And it says, essentially, these are fixed.
I've observed these.
Don't change them.
So the next step is, how do you get posterior distributions?
This is the obstacle.
This was the hard bit.
It's analytically impossible most of the time.
And even calculating them numerically is challenging.
And so over the years, statisticians
have come up with different approximations,
things called the map estimate, which really just
does optimization and finds the peak.
But you don't get anything.
It's not really fully Bayesian, because you
don't get any distributions.
You just get a value.
You do things like weird things like rejection sampling,
where you take random values and you look at the value
and see if it looks like it's from the distribution or not.
Whole slew of things.
The sort of de facto standard for doing this
is something called Markov chain Monte Carlo, or MCMC.
And even quicker than my description of Bayes, MCMC,
the Markov chain part is the fact
that even though we can't sample directly from our,
if we had a really simple model, we
could sample directly from our posterior distribution,
we usually don't.
So we can't sample independently from it.
We can generate a dependent sample.
And Markov chain is a dependent sample,
where the next value is dependent on the current value,
but not any of the past ones.
If I can generate a Markov chain with a particular property
called the reversibility, so if it satisfies
this detailed balance equation, then
if I sample from that Markov chain long enough,
I'm going to get samples that are
indistinguishable from the true posterior distribution.
That's kind of the magic that the math guarantees us.
And in practice, so MCMC is kind of a class of algorithms.
There isn't an algorithm called MCMC.
There's lots of specific implementations of it.
The most famous one is something called metropolis sampling.
You may have heard of Gibbs sampling.
There's lots of different ones.
Metropolis sampling is the easiest to describe.
What you do is you initialize your parameters
to arbitrary values, whatever you want.
And then you have some way of proposing new values.
So some distribution or some way of proposing values
that's easy to sample from.
And then you evaluate that proposed value
depending on the log probability of the whole model
that you've specified.
You either reject it or accept it.
If you accept it, you take that value
and you add it to your bag of values.
Otherwise, you revert to the current value.
And then you go up and do it again.
You repeat this over and over again.
And when you do that, you get something like this.
So this is a big but very simple model.
What this is is a model that is a 1,000 dimensional
multivariate normal.
So it's a multivariate normal of 1,000 values.
So it's big, but it's very simple.
And we can see as metropolis sampling is not doing very
well because of the correlation here.
And it's kind of stopping and starting.
When it stops, it means it's getting rejected all the time.
It's not really finding the meat of, oh,
this is the marginal of just two of them.
So sorry, I forgot to say that.
So this is two of the 1,000 just to kind of see
what it looks like.
It's hard to visualize in larger dimensions than that.
Pardon me?
The first two are the first two.
Oh, arbitrarily, yes, the first two.
It doesn't matter.
They'll all look like this.
And the problem is that it's this random walk.
I'm randomly selecting a candidate value
and then evaluating it.
And it works fine for small models, not really
well for big ones.
And so the whole idea of PMC-3 is
to use new, more sophisticated algorithms.
And in particular, we're going to use gradient information
of the posterior distribution to propose better values.
So it's not going to be random walks anymore.
What we do is we essentially try to simulate this
as a physical system.
So if you think of our posterior distribution
as like a landscape, like a skateboarding park.
And the skateboarder is like your point.
And you're just kind of rolling them along the surface.
And so you're simulating the physics, if you like.
So we add an auxiliary variable to this.
So we have the position and the velocity.
And we move this thing around according to that.
So no more random walk.
And what we're doing here is simulating this.
So if you're at the top of the hill,
you have lots of potential energy and not much kinetic.
And then as you go down the hill, you
got lots of kinetic energy and not much potential energy.
And you're essentially simulating that system here.
And so derivatives, right?
We see how that changes over.
And that's what we've got a model here.
And that's why we require a theano or something like it.
Because integrals are impossible to do automatically.
Derivatives you can do automatically.
You just need the right technology to do that.
And that's what is provided.
And so Hamiltonian Monte Carlo kind of looks like this.
You assemble a new velocity from a Gaussian distribution.
So you essentially give the marble or you
give the skateboarder a push in a random direction.
And then you simulate a continuous system
using steps, deterministic steps, discrete steps.
And then once you get to the other side, you stop it.
You take a point.
And you repeat the thing over and over again.
And now what you get is this.
Works much better, right?
Near independent sampling across the distribution.
It characterizes it very quickly.
Has a very high acceptance rate.
And it's applicable to much larger models than kind
of the metropolis style sampling.
The downside is there's a lot of tuning to be done.
You've got to pick how many steps
you take along that leapfrog, where to stop, right?
So what happens if you're on your skateboard
and you get to the other side of the skateboard park, right?
You start sliding back again along the path that you just
took and you don't want to do that.
And so Andrew Gellman and one of his graduate students
came up with an automated sort of self-tuning version
of Hamiltonian Monte Carlo called the no U-turn sampler,
which as it says, is trying to prevent the U-turn coming back
upon itself.
And so you don't have to know all that when you do PMC, right?
Again, with black box that we abstracted away,
all you do is you call sample.
That's it.
And it determines it's used nuts for the late rate
and the early rate.
It's used metropolis for the switchpoint
and you get a few thousand samples in a few seconds.
And then as we promised earlier on,
once you can get that posterior distribution,
you get a bunch of stuff for free.
Here are samples.
You're able to get means and standard deviations
and credible intervals and everything else
that you need, which is fantastic.
So that's kind of the primary way
that you get inference due machine learning using PMC.
But even with more sophisticated algorithms,
MCMC can be very slow, particularly for large data sets.
It doesn't scale well with large data sets
because that likelihood has to be evaluated for every data
point at every step of that sampling algorithm.
And so in those cases, we can use a different type of algorithm
that very recently has been added to PMC
and that's called variational inference.
And it's a very different approach than MCMC.
What we do here is we take the blue curve here
is like some posterior that we don't know what it is.
And then we have another distribution
that we're familiar with, like a normal distribution, something
that's easy to work with.
And we transform it and we select parameter values for it
so that it gets as close as possible to the posterior
distribution.
