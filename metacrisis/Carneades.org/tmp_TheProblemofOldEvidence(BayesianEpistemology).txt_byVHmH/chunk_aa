Welcome back to currentedities.org.
Today, we're going to be continuing with our series,
Objections to Bayesian Epistemology
as part of our larger series on Bayesian Epistemology.
In this video, we're going to be looking at the problem of old evidence
for Bayesian Epistemology
and a possible way that some Bayesians
may be able to get out of this problem.
So, the problem of old evidence goes something like this.
So imagine that we have some piece of evidence,
that we have a very high or certain degree of belief in.
Now imagine that there's some hypothesis, H,
that we are unsure of,
but we see at this point no connection between E and H.
Then later we discover that in fact H implies that E.
This discovery should mean that E confirms H,
because when you have an entailment relation,
usually that means that E will confirm H when H implies E.
The problem is, in Bayesian Epistemology,
it in fact does not.
Let's take a look at why.
So, imagine that our hypothesis is
vehicle exhaust is bad for the environment.
We're not sure if this hypothesis is the case at this point.
We give it a 0.5 degree of belief.
We also have a piece of evidence.
The world is getting hotter.
We're very certain that this piece of evidence is the case.
Not 100% certain, but 0.96 certain.
And because we don't see any relation
between vehicle exhaust and the world getting hotter,
we're then going to assign 0.48 to the conjunction of those two,
which basically is going to be half of our degree of belief
in our evidence,
so that half is going to be perfectly split
between P of E and H and P of not E and H.
This is going to table to make a little bit more sense of that.
So, we said we're 50-50 split
is vehicle exhaust bad for the environment
and this is not.
We're pretty confident that the world is getting hotter.
0.96 on that evidence right there.
And we're going to split evenly between H and not H
because right now we don't see a relationship
between that evidence and our hypothesis.
Similarly, we'll split evenly between H and not E
and not H and not E
because we don't see a relationship
between the evidence and the hypothesis.
Now imagine that we discover this is an entailment relationship,
that if vehicle exhaust is bad for the environment,
then that means that the world should be getting hotter.
This should mean that our hypothesis H
should be more confirmed from that piece of information,
but it is not because we're only allowed to change
our degrees of belief in H,
which would be the requirement for H to be confirmed by E,
when our degrees of belief in E, in fact, change.
Basically, our degree of belief in H and not E
should drop to zero because this is an entailment relationship,
which would force our belief in H and E to go up to 0.5,
which would make our degree of belief in H,
given that E greater than 0.5,
since our degree of belief in H and E
is now greater than our degree of belief in not H and E,
once we are given that E.
Basically, we would end up with 0.5 out of 0.98
instead of 0.48 out of 0.96,
so we would end up with something just a little bit greater than halfway,
but because it is an increase,
it would mean that our hypothesis is confirmed.
The problem is this doesn't happen
because we're not allowed to reassess our probabilities
to our evidence changes.
It doesn't because there's no change
in our degree of belief in our evidence E.
We become more confident that the world is getting hotter
and we can't change our beliefs.
According to our entailment rule,
as we noted in a previous video,
evidence that is implied by a hypothesis
should confirm the hypothesis,
but since we already have confidence in that evidence,
this implication is going to be moved.
However, it seems to me that in certain cases
there's a way to get around this.
Simply take the implication itself as the piece of evidence
forming H and take the conjunction of H and not E
as a hypothesis.
Let's take a look.
At first, we were unsure that H implied E,
so we can give that a probability of 0.5.
We don't know if it's the case, we don't know if it's not the case.
Our level of confidence in not E and H was already clear.
It was 0.02.
Finally, it's impossible for both H and not E
and H implies E to be true.
That's just the definition of implication,
and we have to have confidence in the laws of logic
as we learned in a previous video.
So we can assign that a degree of belief of 0.
Let's take a look at our new table now.
We have, for our evidence, H implies E,
and it's not the case that H implies E,
and H and not E,
and it's not the case that H and not E.
We have a 0.02 degree of belief in H and not E
that was given before, which gives us a 0.98 degree of belief
and it's not the case that H and not E.
We have 0.5 and 0.5 degrees of belief in H implies E,
and it's not the case that H implies E.
And of course, because of the definition of implication,
we have to have a 0 degree of belief in H implies E
and H and not E, all being the case.
This allows us to fill in the rest of our probabilities
and it should be very clear if we come to the conclusion
that H implies E, not only will that confirm
that it's not the case that H and not E,
and disconfirm H and not E,
it should reduce our degree of belief in H and not E to 0.
So imagine that we do discover that H implies E is the case.
We would reassess our probabilities
and discover that since 0 over 0.5 equals 0,
our new degree of belief in H and not E
must be 0 to stay rational.
What we've shown here is H implying E
is going to disconfirm H and not E
and it's going to confirm everything else.
And if that's the case,
this would force our original H and E to increase up to 0.5
and it would be clear that this evidence that H implies E
does confirm our hypothesis.
The interesting question here is which is it then?
The assertion that E, the very high probability that E,
or H implies E that in fact confirms our evidence.
They're both required for the evidence to confirm the hypothesis.
If one of them wasn't the case,
then we wouldn't be able to confirm the hypothesis.
But because they are the case,
it seems that the hypothesis is confirmed.
Note that this will only work
if your degree of belief in E is high but less than 1.
If it's 1, that would mean that your degree of belief in H
and not E is already 0.
So learning that E will never confirm H for you,
as noted in our video on entailment.
And using our little workaround of the implication
isn't going to help either
because there's not going to be any 0.02 for you to push over
and help confirm your hypothesis.
As noted in the new paradox of dogmatism,
certainty in Bayesian epistemology
will become very problematic very quickly.
That was the problem of old evidence.
Next up is prediction versus accommodation,
the problem of new theories,
the problem of the priors,
and that will finish off our series in Bayesian epistemology.
Watch this video and more here at Carnades.org.
Check out the SEP if you are confused here
or you want more information.
Stay skeptical everybody.
