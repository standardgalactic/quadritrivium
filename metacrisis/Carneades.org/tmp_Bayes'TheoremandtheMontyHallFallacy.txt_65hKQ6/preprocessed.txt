Welcome back to Cardades.org. Today we're going to be continuing with our series on
Bayesian epistemology, looking specifically at Bayes' theorem and the Monty Hall fallacy.
This is going to be Bayes' theorem and the Monty Hall fallacy. Now, I anticipate this
is going to be a long and pretty technical video, so I'm going to give you a little bit
of a problem to think about on the way. This is known as the Monty Hall problem. So imagine
that you're on a game show. There are three doors. Behind two of the doors, there's a
goat. Behind the other door, there's a brand new car. Now imagine that you select one door,
but before what is behind the door is revealed, the host, who knows where the car is and will
not reveal it, reveals that there is a goat behind one of the other doors. He then asks
you if you would like to change your choice. The question for you is, should you switch
doors or stay? Does it even matter if you want to win the car? If you already know the
answer to the Monty Hall problem, I want you to start thinking about this problem in terms
of conditional probabilities and Bayesian epistemology. And if you don't, stay tuned
at the end of the video and we're going to explain not only the solution to this problem,
but how Bayesian epistemology and Bayes' theorem can actually help us solve the problem and
understand why people often make a mistake. So let's get back to the technical stuff.
We're going to be talking about Bayes' theorem, we need some terminology. If we say p of h,
that is the probability of some hypothesis, h, remember this is yielding us a number between
zero and one inclusive, and p of e is the probability of some evidence, e. In order to understand
these conditional probabilities, I'm going to make a probability table that looks like
this. You may do it some other way, here's how I do it because it makes sense to me.
So going down the side, we have h not h and a total, and across the top we have e not
e and a total. And wherever these intersect, we're going to get the conjunctive probability
of them. So where e and h cross, we have the probability of e and h, where e and not h cross
with the probability of e and not h, and so on and so forth. Down with the totals, we
have just the probability of e, the probability of not e, the probability of h going across,
and the probability of not h, the total probability of those, and in the corner where the two
totals cross, we have just a probability of one. To understand how this table works and how it's
going to be useful, let's break it down a little bit. So going across our h row, we see that the
probability of h is just going to be equal to the probability of e and h plus the probability of
not e and h. So this gives us some really useful information. If we have two of these, any two
of these three, we can figure out the third one by using this simple formula. And it should be
clear that this is just going to kind of follow from the logical truths that we've proved before.
This is also going to hold true for our not h row, the total is just going to be the sum of the
two other probabilities. And for our final total row, this is just one of the logical truths we
proved in our last video about the laws of probability, which is that the probability of
e and the probability of not e is just equal to one. The same is also going to hold true going
down the columns, the probability of e is just going to equal the probability of e and h plus
the probability of e and not h. Same for not e. And finally, of course, the same for the totals
using, once again, the same law of probability that we deduced in the last video. So with that out
of the way, let's take a look at some examples to really get a sense of how these probability
tables work. So imagine an urn of only spheres and cubes. There's the only things in the urn. They
are either red or black. There are 80 red objects, 40 spheres, 15 black cubes, and 100 objects total.
If you reach into the urn randomly, what is the probability that you will draw a red sphere? It
may seem that we don't have enough information to solve this problem, but in fact, we do. If we
plug it into the probability table, it becomes quite clear. So we're going to take r as the object is
red, not r as the object is black, s as the object is a sphere, and not s as the object is a cube.
We set up our probability table and plug in the information we've been given. We know that there
are a total of 100 objects. It's going to be really nice for finding probabilities. We know that
there are 80 red objects, 40 spheres, 15 things that are not spheres and not red, so black cubes.
And we want to find out how many red spheres there are, what the probability of drawing a red
sphere is. Well, we can deduce that if there are 40 spheres and 100 objects total, there are 60
non-spheres. In the same way, we can figure out that there are 20 black objects because there are 80
red objects and 100 objects total, and the only objects in there are either red or black. With
this information, we can further deduce that there are 45 red cubes because there are 60 cubes,
15 of which are black, the only other possibility that they can be red. The same way we can figure
out there are 5 black spheres by saying that there are a total number of 20 black objects, 15 of
which are cubes, the only other possibility is that there are a sphere, so get down to that 5. And
finally, we can deduce that there must be 35 red spheres, either by subtracting 45 from 80 or 5
from 40. All we have to do is divide everything by our total number of objects, 100, to get the
exact probabilities for all of these. There's a 35% chance or a 0.35 probability of drawing a red
sphere at random. So now that we have a little bit of a sense of the way the probability matrix is
going to work, let's talk about some more Bayesian terminology. P E subscript H is something we're
going to do. We're going to call that the likelihood of H given that E. So basically imagine that we
already know that E is the case. What is the probability of H now? This is going to be defined
as the probability of H and E divided by the probability of E. Let's see how that makes sense
back to our example. So imagine now that you reach into the air and you feel that the object is a sphere.
Now, what's the probability that that object that you're holding is red? It's not the same as the
probability of drawing a red sphere. Let's take a look. We are looking for the probability that the
object is red, given that it's a sphere. It's equal to the probability of the object being red and
being a sphere over the probability of the object being a sphere. We've kind of reduced our table down
to just the part that deals with the object being a sphere. And our new total is not the total number
of objects, but just the total number of spheres. So if we plug the numbers in, we discover that the
answer is 0.875, which should make sense. There are not very many black spheres. So it would make sense
that if we have discovered that the object is a sphere, it's more likely that it's going to be red.
Now, from this information, there are a number of kind of further rules of probability that we can
discover and deduce. One of them is logical consequence. If E implies H, that implies that the
probability of H, given that E is equal to 1. Basically, if all of the spheres are red and you
feel a sphere, it has to be red. If spherehood implies redness, your feeling a sphere or having
it given that you have a sphere implies that the only thing you can have 100% probability is that
it's a red sphere, is that the object is red. Another consequence of these rules and definitions
we have is the preservation of certainties, which says that if the probability of some hypothesis
H is equal to 1, then the probability of H given that any evidence E is also equal to 1. Basically,
if H is certain, then H given that E is certain also seems to make sense. Basically, if the only
objects in the air are red, then if you feel a sphere, it still has to be red. No matter what
new evidence you get, that has to be a red object. Another consequence, which is a little more
complicated, is mixing. Let's take a look. So, probability of H is equal to the probability
of E times the probability of H given that E plus the probability of not E times the probability of
H given that not E. This seems a little confusing, and it may not be very clear how this directly
follows from the information we've been given so far. So, let's take a look at some examples
that might clear this up. Basically, this is saying probability of pulling a red object is equal to
the sum of the probability of the object being a sphere multiplied by the probability of an
object being red given it is a sphere, and the probability of an object not being a sphere
multiplied by the probability of an object being red given that it's not a sphere.
I don't know about you, but that didn't clear things up at all for me. So, let's take a little
deeper. If you look at the exact formula and the definition of the probability of R given that S
and the probability of R given that not S, you'll see that it boils down to this. The probability
of R is equal to the probability of S times the probability of S and R over the probability of S
plus probability of not S times probability of not S and R over the probability of not S.
We can plug the numbers in and see that it works, but mathematically, it should be clear,
hopefully, that in the first term there's probability of S on top and a probability of S
on the bottom. So, we can just cancel those terms, the same in the second term with the probability
of not S, and we're just left with the probability of R is equal to probability of S and R plus
the probability of not S and R, which was just R rho of R when we were understanding the probability
table at the beginning. So, it just makes sense that 0.8 is equal to 0.35 plus 0.45, which is how
it will work out if you actually do the math. All this is doing is basically taking that formula,
that definition of the probability of R given S and kind of playing with it a little bit and
adding things to make it more complicated and make it possibly useful in particular situations,
but overall, it just may seem a little confusing. However, hopefully, this is cleared it up.
Another term that we're going to use in Bayesian theorems is the inverse. So, the probability
of H given that E is going to be the inverse of the probability of E given that H, or in other
words, the probability of pulling an object given that it is a sphere is the inverse of the probability
of pulling a sphere given that it's red. With all of these in our tool bag, let's move on to
Bayes's theorem. This can be formulated in a lot of different ways. This is just one formulation.
So, goes as follows. Probability of H given E is equal to the probability of H divided by the
probability of E all multiplied by the probability of E given that H, or the inverse of our original
conditional probability. Once again, it's a little confusing in this form, so let's look at it in
terms of an example. The unconditional probability of R is 0.8. The unconditional probability of S is
0.4. And the probability of S given that R is 0.35 over 0.8, or 0.4375. If we work out the math,
we get the probability of R given that S. But looking at the numbers, it should be clear that the
0.8 on top is going to cancel with the 0.8 on bottom. And we're just going to be left with 0.35
over 0.4, which was our original way of solving for the probability of R given that S. And were
you to expand the inverse of this function, which is the probability of S given that R, you would
find that that's just going to be the probability of R and S over the probability of R. If you cancel
out the probability of R and the probability of R, you get the probability of R and S over the
probability of S, which is just our definition of the probability of R given that S. Hopefully,
that's clear or as clear as it can be. If you're confused, go back and look at the definitions
and try to reparse this out without the kind of conditional probabilities without the probability
of S given that R just using the original terms. And it should make sense. You should be able to
cancel things out so that you get just down to the definition. Now back to the Monty Hall problem.
So let's take C as you pick the correct door and the number three as door three, which is a door
that you did not pick as the correct door. So we'll set up our probability table once again.
We know that the total probability we're looking at is one. There is a one third chance that you
pick the correct door, assuming you pick randomly. And therefore, there's a two thirds chance that
you didn't pick the correct door. There's a one third chance that door three, a door that you
didn't pick, is correct. And there's a two thirds chance that it's not the case that door three is
correct. It's also can't be the case that the car is behind both doors three and the door that you
picked. Because then there would have to be two cars and we explained by definition of three,
you didn't pick door three. From this we can deduce the rest of the probabilities pretty easily.
There's a one third chance that door three is correct and your door isn't correct. There's a
one third chance that your door is correct and door three isn't correct. And there's a one third
chance that neither your door nor door three is correct. Being that the other door not the door
you picked and not door three is correct. A common solution might be to say that what you're looking
for is the probability of C given that not three and comparing it to the probability of not C given
that not three. However, this is not going to work. What we'll get by doing that method is one
third over two thirds or one half for each of our probabilities. Therefore we'll conclude that neither
option is better. You're just as good switching as you are staying the door. And yet this is not the
case. In fact, you always will have a better chance if you switch doors. The solution would be correct
if the door to be revealed was random. But it in fact is not. The host always picks a door with a
goat behind it. The host is never going to reveal the door with a car behind it and miss the rest of
the game. So we're going to reimagine this problem but with some different definitions. So C is you
picked the correct door and D is the probability that the door revealed has the car. Now imagine now
that the door revealed is random. This is the same table that we had previously. In this case,
there is a chance that the door to be revealed is the door with the car on it. There is that one
third chance that that door is right. If this is the case, your deduction that it's half and half,
it's just as good for you to switch and not switch is correct. That works. That's a correct deduction
because it's random. However, in the problem as it stated, it's not random. The table should in fact
look like this because there is no chance that the door that is revealed by the host has the car behind
it. So that one third probability from not C and D has to jump over to not D. When you think about it,
it's the idea that if you pick the wrong door, the host is forced to pick another door. He has to
pick the other door that doesn't have the car behind it. The probability of that being right gets
pushed into the probability of the other door being correct. So you're always have a better chance
to switch because were you to look at this table now in terms of Bayesian epistemology and say that
we are given not D and we want to find the probability of C or what is the probability of
C given that not D? It's very clear that it is much more likely that not C is correct and not D is
correct than C is correct originally because you only had a one third chance of getting it right
at the beginning but you have a two-thirds chance now that the host has been forced to get rid of
a door that did not have the car behind it. The point here is that depending on the mechanics
of the system that changes the probability. It may seem random at first but if you know that it's not
random you have to reassess the probabilities. This leads me to the Monty Hall fallacy. This is
going to be a problem for Bayesians. It's a problem for theists. It's a problem for some atheists that
put forward a certain explanation of the likelihood of science and it goes as follows. The Monty Hall
fallacy is assuming that a process is random which either is not random or cannot be shown to be.
This is the problem in the Monty Hall problem and it's a fallacy that I'm going to show
exists in many different arguments. That was Bayes's theorem. Whew, that was long. Next up are the
Dutch book arguments followed by Bayesian confirmation theory, variations of Bayesianism,
and finally some objections to Bayesian epistemology. Watch this video and more here at
carnades.org. Check out the SEP if you were confused by anything I said here or you want
some more information on Bayesianist theorems and stay skeptical everybody!
