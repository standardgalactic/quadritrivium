Welcome back to cardinatis.org. Today we are going to be looking at the laws of
probability. In this video we're only going to be handling the basics. We're
going to be looking at how the laws of probability can be translated into
logical laws and we're going to be looking at this with the idea of thinking
towards Bayesian epistemology, the series we're doing. We're going to then use
these laws of probability to help us understand Bayesian epistemology and what
they're talking about when they're talking about conditional probabilities.
So let's take a look. We're talking very specifically in this video about
unconditional probability over sentences. We are not talking about what it means
for something to have a certain probability. We are not talking about how
certain probabilities are generated. We're not talking about the probability of
sets or conditional probabilities, though we will get to conditional
probabilities in this series. What we are talking about is what we can
validly deduce given certain probabilities. We're talking about laws of
logic and just like other laws of logic like modus ponens, it doesn't tell us
if p is true or if p implies q is true. It doesn't even tell us what those
statements mean. What it does tell us is that if given p and given p implies q,
we're allowed by the laws of logic to conclude q. That is what we're looking
at here. What can if given certain probabilities we conclude using the laws
of probability? So we're going to introduce a notation that we're going to
use throughout this series which is the probability of a certain statement s is
going to be written as p of s and what that's going to come out as is not a true
or false like most of our logical functions come out as it's in fact going
to come out as a numeric value. There's going to be some parameters on that
value but we'll check that out in the next couple of rules. So the first rule
actually deals with that. It's the probability range principle. It says that
for all s, the probability of s is between 0 and 1 inclusive. For all
statements s, the probability of s is between 0 and 1 inclusive. I'm going to
represent that as p ran or the probability range whenever it's used in
proofs. This should seem fairly intuitive. We understand probability as a number
between 0 and 1 inclusive. It wouldn't make sense for us to have a negative
probability or a probability greater than 1. That just doesn't mesh with our idea
of what probability really is. Next up, we have the probability tautology
principle. That is for all laws of logic, the probability of that law of logic is
equal to 1. I'm using l to represent logical truths here. For all logical
truths, l, the probability of l is equal to 1. I'm going to represent that rule
with p taught or the probability tautology principle. This also seems to
make some intuitive sense. If you think that laws of logic are true in all
possible worlds, then there is no chance that a law of logic could be false. So,
the probability of any law of logic has to be 1. Finally, with the principle of
finite additivity. It's a little more complicated. For all statements s and r, if
it's not the case that s and r, that implies that the probability of s or r
is equal to probability of s plus the probability of r. In other words, for all
statements s and r, if it is not the case that s and r, then the probability of s or
r is equal to the sum of the probability of s and the probability of r. We're
going to represent that with p, f, add, or the principle of finite additivity.
This may not make sense at first glance. So, to understand it a little bit more,
let's think of a specific case. So, let's assign two separate coin flips to s and r.
This would be the reason that you need it not to be the case that s and r can happen at the same
time for this to work. So, if we threw out that condition and said, what is the probability
of s or r? What's the probability of either the coin on the left flipping heads or the
coin on the right flipping heads? If you know about probability, you'll know that that's three over
four. Three of the possible outcomes include at least one heads. Only one of the four includes
two tails, so three out of four is that probability. However, the probability of s, this one coin,
flipping heads, plus the probability of r, this other coin flipping heads, is actually going to be
one. So, if it's possible that s and r can both happen, then in fact, this statement is not going
to be true. However, if we do have something that has mutually exclusive probabilities,
let's take the idea of rolling a dice. If you are saying that s is the dice will come up a five and
r is the dice will come up a six, there's no way that the dice can both come up a five and a six
on a single roll. So, the probability of either a five or a six is equal to the probability
of it being a five plus the probability of being a six. Because the probability of rolling
either a five or a six is one-third, and the probability of rolling a five is one sixth,
and the probability of rolling a six is one sixth, and one sixth plus one sixth equals one third,
this principle seems to work out. Hopefully, that made sense. If this principle still isn't
very intuitive to you, try it with some actual probabilities and see if they work out. What
we're going to do right now is show that with just these three principles we can actually prove
some fundamental truths of probability. This is going to be the sum of the probabilities of s and
not s is one. This seems to make sense. It's kind of a variation on the law of non-contradiction
and the law of the excluded middle basically. If you have the probability of not s and the
probability of s together, either s or not s is a certainty. It's a law of logic. We had that in
one of our statements, but we're going to prove it here. So our conclusion we're trying to get to
is the probability of not s plus the probability of s is equal to 1. To start off with our probability
tautology principle, universal instantiation, instantiating the law of the excluded middle
in for l, the probability of s or not s is equal to 1. Then we're going to go ahead and take our
principle of finite additivity, universally instantiate it once again with the law of the
excluded middle to get it's not the case that s and not s implies that the probability of s or
not s is equal to the probability of s plus the probability of not s. Then we'll go ahead and
assert it's not the case that s and not s law of non-contradiction, which allows us to use modus
ponens on three, four to get the probability of s or not s is equal to the probability of s plus
the probability of not s. Therefore, using the law of identity and one and four, we can replace the
probability of s or not s just with one to get one is equal to the probability of s plus the
probability of not s. And we're allowed to by mixing things around and understanding identity
get the probability of not s plus the probability of s is equal to one. And thus we've proven our
rule of probability. So that was the very basics of the laws of probability and the way that they're
used in logic. Hopefully that's going to be enough for us to understand the rest of Bayesian
epistemology and some of the theorems and ideas that are going to come out of there. Next up,
we're going to be talking about Bayes's theorem, followed by the Dutch book arguments, Bayesian
confirmation theory, variations of Bayesianism, and finally some objections to Bayes's theorem
and Bayesian epistemology. Watch this video and more here at karnades.org and stay scapical, everybody.
