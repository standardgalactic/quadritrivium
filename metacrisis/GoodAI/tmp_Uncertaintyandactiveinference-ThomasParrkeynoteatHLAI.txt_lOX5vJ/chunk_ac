And here you can see it's looking around the world.
It's finding these dots and then it's moving on to the next one.
It's not looking back at the previous one.
It's continuing to explore.
Now, the motivation for this particular task is partly due to my interest
in thinking about how generative models can be broken
and the sort of ways we can explain pathology in terms of this.
But they also give you a nice way of thinking about what sort of influences
go on in this generative model.
And in particular, there's a clinical condition called visual neglect
where patients with a stroke to the right side of their brain normally
will often neglect the left side of space.
They'll perform fewer saccads to it, so these are eye tracking traces
just at rest looking at neglect patients and a control group.
You can see that they perform many, many more saccadic eye movements
to the right side of space than to the left.
The same patients will fail to, if you ask them to draw a clock,
they'll draw the right-hand side of it and not the left.
And there are a range of different phenomena that echo of it.
It's an interesting condition, but I won't go into too much detail about it now.
The crucial thing is that a common test that's used to assess neglect
is to give people a sheet of paper with a series of lines drawn on it
and to ask them just to cross out all of the lines.
And they'll often cross out all of those on the right rather than the left.
The reason I'm talking about this is that it's an interesting problem
thinking about the influences over exploratory behavior
to think what would lead a visual search towards cancelling
only those lines on the right versus the left
or only attending to those on the right versus the left.
What would lead to this sort of scan pattern
in terms of what would have to be broken in the generative model?
And so here are some examples.
So here we have the same thing I was showing you before,
and it looks at both sides roughly the same number of times.
It's relatively even.
But then I have three ways in which we can break this
by changing a prior belief in the generative model
to behave in an abnormal way.
The first of these is very simple.
We just say, sorry, this shouldn't be an expectation, this is a function E.
If we set a prior belief that I'm more likely to perform
saccades to the right than the left, it's an almost trivial example,
but it shows one way in which we can produce this sort of behavior.
It's just a priori more likely that I'll look over there.
The second thing we can do is change the model
such that there's a bias in proprioceptive beliefs.
I'm more likely to experience data that says I've looked to the right
than to the left.
And by minimizing my free energy, I fulfill those predictions
and I end up looking to the right more often than to the left.
But here is probably the most interesting one.
This is the one I was talking about before.
So we've got our information gain term where we're looking at how much
do my expected beliefs diverge from my current beliefs.
If that diverge a lot, then that's a good thing to do.
So in this particular example here, all we've done
is we've increased the confidence in the initial distribution
exactly as I showed you on the previous couple of slides ago.
So the two new observations will cause a relatively small change
in my beliefs about this mapping, where I look to what I'm going to see.
So the mapping from where to what, where I'm looking
to which of these targets I see becomes more confident
and is modified less after a saccade to the left versus right.
So if you think of the kind of metaphor of the brain as a scientist
who is trying to work out what's the best experiment to perform next,
you don't do an experiment that just changes nothing about your beliefs.
You want to do the sorts of experiments that will get you new data
that will change your beliefs about the world.
And that's exactly what we're saying here.
And you can see the influence that's having over behavior.
Again, this is just a gradient descent on the free energy gradients,
but it's giving us this interesting pattern similar to these other types of lesions.
And here's just a speculative link between some of these
in different regions of the brain.
So we can think about things like this where it's
altering a mapping as altering synaptic connections in the brain.
And that fits quite nicely with ideas of long-term plasticity,
synaptic modulation as a mechanism of learning what's there.
Right. So I'm going to come to the final bit now,
which is thinking about, given these kinds of structures,
given these kinds of architectures, what we've really thought about
are how do I decide where to look?
Or how do I make decisions of some other type?
They're all decisions.
They're all categorical.
I can look over there or there or there.
Ultimately, when you actually engage with the world,
you have to do it in a continuous way.
You know, we have to engage with approximately Newtonian laws
and generate continuous changes and continuous variables.
So the final bit I want to talk about is how we actually go about doing that,
how we can incorporate that into a generator model,
and again, appeal to exactly the same notion of free energy minimization that we have
to frame perception and decision-making.
So this looks very similar to what I've shown you before,
but it's actually subtly different.
And here I've changed the variables.
So these are now continuous variables where we have some causes in the world,
some hidden states, which are now X, and some sensory data.
But now this is not a progression over time.
So the thing that's changed here is that now we're looking at the mapping from
my position to my velocity, my velocity to my acceleration,
and successive temporal derivatives.
So I can now express a kind of stochastic trajectory
in terms of a sort of Taylor series approximation,
where I've just brought out all the coefficients for that Taylor series approximation,
which gives me a way of instantaneously representing a trajectory
rather than just a current state.
For those familiar with it, if you were to truncate this here,
this would essentially be an extended Kelvin filter model.
So if we find the, sorry, that's all a bit too small,
but if we find the free-owned integrations for that system,
we can reconstruct a similar network that relies upon errors here
that drive changes in expectations.
And then those influence the higher level causes and what's going on,
my errors about the states themselves.
So this can be extended hierarchically, essentially indefinitely.
So some of you may recognize this as being a predictive coding architecture
where we are just passing prediction errors and expectations around.
Again, this is the position, velocity, acceleration.
The new aspect of this that wasn't in what I showed you previously is this bit here.
You probably can't read that very clearly,
but that essentially says that action now is also going to follow
a gradient descent on the free energy.
The only thing action can change from the perspective of the generative model here
is the data itself.
So I can either optimize my beliefs about the data,
or I can change the data such that it becomes more consistent with my model.
And when we frame it like that, we see that the free energy gradients
take the form of essentially a weighted prediction error.
So I can treat action as being, I get some sensory data, I compute an error.
That drives action which then in this particular case moves my eyes
such that it fulfills any prediction error that's in play.
And so we get what neuroscientists have studied for centuries now,
the idea of a classical reflex arc.
Okay, yeah.
So what I've described here gives you a way of,
if you have some prior beliefs about what sort of trajectories you're going to engage in,
then you can then implement those trajectories.
But the big question is where do those prior beliefs actually come from?
Oh, I should show you.
So here's an example of some a simulation using these equations
that treats the eyes both as spheres.
The agent can change the acceleration or the circular acceleration of the eyes
and they obey essentially Newton's second law for rotational systems.
By adding in a prior belief about the sort of trajectory it engages in,
you can see that the eyes then follow that trajectory
and giving it different prior beliefs gives you all sorts of different,
all different sorts of ocular motion behaviors in this particular example.
We've used the same sort of thing to simulate things like handwriting
and various other types of behavior with exactly the same set of equations
but different generative models in the past.
Yes, so the next question was where do these prior beliefs come from?
So I have this network, if I set some prior belief about V,
then this will cause a particular change in trajectory
which will cause particular data or predictions about data
that are then fulfilled by acting.
But I can put the network that I described above on top of this one
so I can just say that instead of this being an observation in and of itself,
this is actually just another random variable
that indexes which of these continuous variables to make use of.
So you can think of this as a process of forming different hypotheses
that then have to be implemented in the continuous domain,
different alternative, alternative trajectories that I could choose from
and I infer which one is most probable based upon the beliefs here
which includes all of the exploratory and pragmatic behavior
but then I can map that down into this continuous model
with several different hypotheses and then pass messages back up
that are effectively the evidence for each of those models.
So I have a hypothesis that I might look over there
versus a hypothesis that I might look over there.
Each of those can be mapped to a prior belief
that my eyes will end up in this continuous location
or this continuous location
and then I can allow those continuous dynamics to play out,
evaluate the evidence for each of those
and pass that back up to say what was the more probable location
that my eyes went to.
So you get a process of decisions going down to movements
and then messages being passed back up again
and you start to get this hierarchical structure
where you can pass messages both up and down.
Again you can extend each of these indefinitely hierarchically
so you can repeat this above itself and above itself
and similarly you can have many, many layers of this
and get fairly complicated models overall.
So this is just a summary of what I've said over the previous few slides
this idea that we can have these discrete equations
that are playing out or these discrete inferences
that are playing out that are saying
what are the most probable policies I'm going to pursue
what are the most probable states that result from that
what are the most probable outcomes and observations
that I'll get based upon that.
These feed down through essentially model averaging
a bit like a mixture of Gaussians
from a discrete variable, a categorical variable
to a continuous variable.
We then perform these filtering steps
that essentially behave a bit like an extended Kalman filter
