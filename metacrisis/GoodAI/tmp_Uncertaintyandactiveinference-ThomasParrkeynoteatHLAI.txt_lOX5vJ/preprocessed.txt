Thank you very much for the introduction.
So yes, I'm going to talk about active inference, which is a theoretical framework we use to
try and understand brain function and the neurobiological processes that underwrite
a lot of the cognitive functions that we're interested in.
The kind of central idea in active inference is to treat everything the brain does as an
inference problem of some sort.
And that includes perceptual inference, most obviously, but also the notion of planning
as inference and movement itself as an inferential process fulfilling predictions about the world.
I'm going to talk about particularly the role of uncertainty in these kinds of formulations
because this is one of the things that active inference does quite well.
And those of you who are in the last session before lunch will probably recognize a lot
of the concepts that come up here.
So the basic structure is I'm going to first of all give an overview of the idea of active
inference and just a brief summary of what it is.
I'm then going to talk a bit about the sort of neuronal networks, the kinds of architectures
we might expect biological systems to use to actually solve these inference problems.
And then I'm going to give you a couple of examples and show some simulations that illustrate
the paver of this kind of system.
First one looking at the role of uncertainty, the idea of exploring to maximize my information
gain in the environment.
And the second to instead of thinking about uncertainty about what are the states in the
environment causing my sensations, actually thinking about what influence does my uncertainty
about the probabilistic structure of the world?
What influence does that have over the behaviors I choose to engage in?
So they're thinking about the role of learning as opposed to inference.
Yes?
Okay.
Okay.
Right.
Can you hear me now?
Is that okay?
Yeah?
Okay.
I'll try and let me know if I drop too quietly again.
The final thing I want to talk about will be the processes that convert the decisions
we make and the plans that we infer we should be following into movements in the real world.
And I'll give you some examples for each of those.
So the key objective function of active inference, the key thing that everything works to try
and optimize is this free energy functional here.
And this is something that comes up in variational inference.
This is just a quantity that says that if I have some beliefs that such as distribution
Q about the states S that are causing things in the world, then I can plug them into this
function.
And by minimizing that function, I can then get the best set of beliefs that best describe
what's going on in the outside world.
That depends upon there being some sort of probabilistic model that I have about the
way the world is structured.
And that's given in this distribution P. So that's saying how these states S give rise
to observations O. And the little, the tilt, the line over the top indicates that these
are trajectories.
We're interested in a process that's evolving over time.
The other term in here, the pie that everything is conditioned on, is the particular model
of how things are going to evolve or the policy that I'm going to pursue.
So this essentially says a policy is just a sequence of actions or a sequence of states
that I can follow.
And I have several different models for how I could behave, several different alternative
policies I could engage in.
And I can associate a free energy functional with each of those.
That's fine when I'm trying to make inferences about the data I currently have, the observations
that are currently available to me.
That becomes much more problematic if I then want to move into the realm of planning.
Because if I want to minimize my free energy in the future, I need to try and predict what
that free energy might be.
And you can see here that the free energy as it is is a function of observations of data.
As I don't have data for the future yet, by definition, I instead replace the expectation
here, or the distribution I'm using to average this, with one that includes the observations.
So I'm now predicting what sort of observations I'm going to get, and I'm averaging this
thing with respect to both the observations and the states that I expect to lead to them.
And that gives us something to think about in terms of how we plan and how we perceive
these things into the future.
So once we do this, we can think of perception as the process of optimizing this distribution
to see what are my beliefs about the states of the world at present, and what will they
be in the future.
And I can optimize that, and then I can simply plug that in to the expected free energy that
I've defined here.
I'll come back to that in a second, and I can use that to form beliefs about the sort
of plans I'm going to engage in, the sort of policies I'll pursue.
So here this is just the policy, the belief I have about how I'm going to act.
I can use that to generate an action.
I'll come back to that as well.
And that action changes the world, generates new observations, and we get this kind of action,
perception cycle with planning to pose that.
Now the reason I've put these two equations on is just to emphasize some important features
of these two free energy functionals and what they actually mean.
And here you can see that we can re-express this thing here in terms of this thing here,
which is just the negative log probability of a certain set of observations.
And you can think of this as the evidence for the model, or the evidence that I'm continuing
to exist.
I'm seeing sort of data that are consistent with my own model of myself.
The bit that's actually changing with respect to this Q distribution is this bit here, this
kale divergence.
This is a non-negative quantity, so this is always positive, that just quantifies how
different these two distributions are.
And here we're seeing that this is the posterior distribution, the probability of states given
the data I've already observed, and this is our beliefs.
So essentially by minimizing the free energy, I'm effectively reducing the distance, or
I'm trying to approximate what the true posterior should be as best as possible using my current
beliefs.
And of course, if you were to just substitute in the true posterior, if you could calculate
that analytically, then this would just become exact inference.
I'd point out that depending on what field you come from, sometimes the free energy is
represented as the negative of what it is here, particularly in machine learning.
And that often leads to the rhetoric of maximizing free energy or maximizing the evidence lower
bound.
I'm going to be talking about it using the kind of physicist's definition, which involves
minimizing the free energy.
So this is now an upper bound on the negative evidence.
This here is the expected free energy, rearranged in a similar way.
And you see, we still get this kind of expected log evidence term.
And we can think of this as being a kind of goal directed drive.
This is saying that I want to seek out those data that are consistent with my model, and
those might be those data, those might be interceptive data that are consistent with
maintenance of homeostasis or something like that.
They could also be extra accepted in a goal directed task of some sort.
The interesting thing here is that the expected KL divergence here has swapped round.
So the sign is now negative.
And we now have the expected posterior and the current posterior.
And the reason for that is the way we take the expectation here.
That means that to minimize the expected free energy, I need to actually maximize the difference
between my beliefs as they currently stand, this Q distribution here, and what I expect
my beliefs to be after making the observations that give rise to this posterior here.
So now we have an interesting way of thinking about what this is.
There's a drive here to fulfill the kind of goal directed predictions.
And there's a drive here to try and change my beliefs as much as possible by observing
new data.
So we can think of this as carving up the expected free energy into a component that
drives exploration, information seeking behavior, very much as many of the terms did in the
previous talk, and a drive towards exploration.
And we can give these labels like intrinsic and extrinsic value and various things like
that.
And then we can take a moment to the relationship between this and various other different kinds
of scheme.
But first of all, just to make it explicit what this looks like when you actually minimize
the free energy, when you work out what the posterior beliefs should be.
So this is assuming a sort of partially observed Markov decision process like form, where policies
cause state transitions and states cause observations.
So when we minimize the free energy, we essentially get my belief about states the present is
a function of beliefs about states the past and some transitions, beliefs about the future
and some transitions, and the evidence I'm getting at the moment.
So essentially we combine two empirical priors with sensory evidence or evidence from data
and combine those to try and form a posterior belief about the present.
And that's our perceptual inference pitch, that minimization of the free energy.
When we look at the realm of planning, if we have a prior belief that we minimize the
expected free energy, the posterior belief will contain both the expected free energy
and the free energy.
So here we have yet another expression of the free energy and the expected free energy.
And I'm just going to write out each term in that explicitly and just talk you through
how you can intuitively think about what this means and what it's doing.
So the first term here, we generally refer to as accuracy.
So this is essentially saying, given I have several beliefs about what sort of policies
I could engage in, I can take the data I've already gathered and work out whether that
carries evidence in favor of me pursuing one policy or pursuing another.
And so here it's just saying, if the observations I've observed along the course of following
this sequence of actions is consistent with me having followed that sequence of actions,
then it's probable that I am following that sequence of actions.
This term here, well, let's go in sequence.
So this term here is the sort of thing that's used in approaches like uncertainty sampling,
looking for those data about which I'm most uncertain.
So each here just means the entropy.
So if I expect something to be very, very uncertain in a particular place, then it means I'm more
likely to try and observe those data and learn more about the world.
If we combine these two terms, so here we now have a conditional entropy that says how
reliable is the mapping from states to observations and how informative will they be?
If we combine these two terms, then we get something that's being given a whole range
of different names.
Sometimes being referred to as information gain, you find this in even in the fifties
literature on Bayesian experimental design as the value of an experiment, Bayesian surprise,
salience, novelty, epistemic value, and sometimes intrinsic value.
So this is our information seeking component.
And the final bit here is just to point out that these two things together limit how far
my beliefs about the policy I can pursue deviate from my prior beliefs.
If they deviate a long way, then that's penalized.
It's an excessively complex explanation for how I can pursue this behavior.
And the final thing is this term at the end, the kind of expected evidence term.
And we can interpret this in terms of an expected utility.
This is the thing I'm trying to seek out or in terms of an extrinsic value.
Okay, so next thing to talk about is just to think about the sorts of neuronal architectures
that can actually do this sort of thing.
And the way we get to that is we think we first of all pick a particular model.
In this case, I'm sticking with the Markov decision process model.
And we express the free energy in terms of the sufficient statistics of those distributions
and the parameters of those.
So here we're dealing with categorical inferences.
So we're just dealing with vectors and matrices of different kinds of quantities.
So I can be in one state and then the next state and then the next state.
And there's a transition matrix that determines how I go from one to the other.
If we just take the free energy gradients of that system with respect to the beliefs about states,
then we get a series of relatively simple update equations.
We can do the same thing for planning.
And again, we can just express these in relatively simple linear algebra.
Again, these are just the free energy gradients with respect to each of these things.
And we just express it as a gradient descent on the variational free energy.
Or in this case, we set up a belief that I'm going to pursue those policies
that have the minimal expected free energy.
The nice thing about these equations is that the dependencies between the variables
actually look quite similar to the neuronal networks that we know are present in the brain
or things of a similar form.
So just to give you a sense of that, we know that we have sensory data that inputs
to normally granular layer four of the cortex.
I know that's three in this particular example.
But we can think of those as computing a prediction error
that's the difference between the beliefs I currently have and the sensory input I have.
And you can see that that prediction error also depends upon this future and past component.
So here I'm representing past on the left, present, future.
So influence from the past, influence from the future, influence from sensory data.
And those errors are then being used to drive that gradient descent process.
I can then evaluate the expected free energy based upon that,
work out what the expected free energy is, and then compute the probable policies.
And use a Bayesian model average to work out what are the probable states
that I could be experiencing and pursuing based upon my beliefs about the states under each policy
and my beliefs about the probability of pursuing each policy.
OK, so that's the sort of architecture we would think of.
And this is similar to the sort of corticose, subcortical loops that we expect to find in the brain.
And this gives us a way to start to think about how we can interpret neuroanatomy
in terms of the inferences that it's trying to pursue with inferences about what's actually out there
and inferences about how am I going to act and what will that do to change what's out there.
Right, so the next thing I want to do is to give you an explicit example of this
in a deliberately very simple generative model and to try and give you some intuition for how that works.
So again, this is the partially observed Markov decision process.
Here we have our beliefs about the policy, which influence the transitions from state to state.
We have transition matrices here that say if I'm in a current state,
what's the probability of being in the next state?
And similarly, we have a mapping here that goes from states to observable outcomes
and says what's the probability if I'm currently in a particular state of observing a particular outcome?
And the key thing here is we can actually parameterise these
such that we can alter the uncertainty in each of these distributions
and see what effect that has for exploratory behaviour.
So here, in this case, imagine I'm in a particular state
and each of these represents the probability that if I'm in that particular state,
I'm going to observe a particular outcome.
So here are all the different outcomes I could observe.
Here we've just set up a parameter that we've denoted zeta or zeta
that's effectively an inverse temperature parameter on this distribution
so we can make it more or less precise.
If it's very large, then this becomes almost deterministic.
If it's very small, then this becomes completely flat.
I could observe anything based upon where I am.
We've also done the same thing for transitions here.
We've set a parameter that essentially quantifies how uncertain we are about,
given where I am, where I will be next.
Again, if it's zero, this is very flat.
And then as we increase it, it gets very large.
And that just means that we can control the uncertainty here
or the sort of volatility of the sequence.
And the uncertainty here in the mapping,
the simulated agent believes occurs between the states of the world and the observations.
This is a fairly arbitrary way of doing this.
We could obviously parameterize this in lots of different ways.
This is just to give you a sense of one way in which we can do this.
This is a representation of the generative model
that I'm actually going to show the simulation for.
So here, I'll go back a second.
So here, if you imagine that each of these states is a place I can be looking.
So I can move my eyes to various different locations
and fixate different locations in visual space.
And at each of those, I can see something.
I can get some sort of visual or proprioceptive feedback
from where I've ended up looking.
And so the policy just becomes the sequence of saccades,
the sequence of fast eye movements I'm going to pursue
in order to induce these state transitions.
So here we have the hidden states that are the fixation locations.
So these are all the S's.
So I can be fixating in location one, two, three, four, or in the center.
I have another set of hidden states that I have no control over,
which are which of these three squares is present in each location.
So it can be either one, two, or three.
If I look to location one and the third of these squares is there,
then that will map to a visual outcome that says I'm looking at this particular color.
There are transitions between these,
and we can control how deterministic or how stochastic those transitions are.
And we can control the mapping between these things.
So here these are those parameters saying how uncertain
is the mapping from states to observation.
Finally, we have this other outcome modality,
this proprioceptive modality that says where am I currently looking.
And so these can transition.
I have control over how these transition together.
So these make up the visual and proprioceptive outcomes,
and the uncertainty of these transitions and the uncertainty of this mapping
can all be manipulated.
And the reason for doing that is just to show you what the behavior of the system is
will appear like.
So here we have three examples of that same environment,
but with slightly different uncertainties in play.
So here we have the simulated eye tracking trace is interpolated
between the points.
It's just choosing one point or the next.
So it's choosing to look there, there, there, and exploring the environment.
I should point out that there's absolutely no preferences in play here.
There's no rewarding location.
Everything here is intrinsically motivated through this drive information game.
So you can see in this example, in every single location,
the mapping of the likelihood that says, given a state,
what's the probable observation is the same.
And the same is true for the mapping from state to state.
So everything is equivalently precise or equivalently volatile.
Here, just to illustrate what happens, we've decreased the precision
or the inverse temperature that maps states to observations.
So now we have a square here, where if I look in this location,
the mapping from states to observations is almost completely random
or is very small relative to the others.
That essentially means that the amount of information I can get
by looking there about the state that's actually there is relatively little.
So here in this presence of relatively noisy mapping,
there's not really much point looking there
because the amount of information I can gain about states of the world
is pretty limited.
And you can actually think of this as being a bit like a dark room
where if I look around the environment,
I'm not actually going to be able to have any sort of precise mapping in play.
And you can see that it just completely ignores that location
and seeks out those where there is a lot of information to be had.
Some in psychology refer to this as the streetlight effect,
which is based upon the idea that if you're out late one night
and you've had a few drinks and you've dropped your car keys on the street,
the first place you look for them is under the streetlight,
not because it's the most likely place for them to be,
but just because it's the only place you can actually get any sort of precise information,
which is in a sense base-optimal if you've simulated in this sort of setting.
This final example, we've decreased the precision or the inverse temperature
of the transition mapping.
So here, things transition much more randomly.
And you can see that in the actual environment.
So the agent believes it, but it's also true of the environment itself.
And you can see that this location is actually fixated much more often,
contrary to what's going on here.
So in this particular scenario, you can think of this heuristically as,
I haven't looked there in a while.
It's more likely to have changed than any of the others.
So there is more uncertainty to resolve there if I haven't looked there recently,
compared to the others which will remain relatively static
or at least relatively predictable even in the absence of me looking at it.
This particular location is likely to change and in an unpredictable way.
So I'd better look back there and check.
OK.
And the reason I put this up here is you can interpret these parameters
in terms of which components of this drive to explore have been altered.
So here we've got the conditional entropy is changed by changing this.
And you can see that as this entropy gets larger,
that becomes a less probable policy.
And similarly for the change here in the opposite direction.
OK.
So the focus there in that particular example was on the uncertainty I might have
about states of the world.
The next thing I'm going to talk about is the uncertainty I might have
in the actual mappings between things,
in the probabilistic structure of the world,
and the parameters that govern that rather than the states themselves.
So again, we have the same network here.
But now we're going to parameterize these mappings,
the A mappings that go from states to outcomes.
And we're going to say that this is something that can be learned
through the course of the trial or across several trials.
And this is parameterized essentially using a Dirichlet distribution.
And here is an example of the kinds of updating that you might expect
from a Dirichlet distribution in this scenario.
So again, this is if I'm in a particular state,
these are all the different observations I could make while there.
If I make this sequence of observations here,
then every time I see one, I update that element essentially
of that probability distribution so I'm more likely to see this in future.
The same thing is true here.
And the difference between each of these is the choice
of the initial Dirichlet parameters.
So I can actually say how certain am I in this mapping
a priori, both of them start with the belief that the mapping is itself uniform,
but I can be incredibly certain that it's uniform
or I can be relatively unconfident that it's uniform.
And as an analogy, if you imagine you flip a coin 100 times
and it comes up heads half the time,
then you're relatively confident it's a fair coin
and a new observation is not going to change your beliefs a great deal.
Whereas if I've only flipped it once or twice before,
I've got heads once and tails once,
I'm going to start with the same initial expectation
of it being fair based upon those data.
But a new observation will actually cause a much bigger change in my beliefs.
And that becomes very important in the sense of information gain about parameters
because if I'm going to behave in such a way that I minimize my expected free energy
or maximize the difference between my current beliefs about parameters
and my beliefs in the future,
then it makes sense that I should look to those regions
that have a profile more like this compared to those like this
because here I'm going to change my beliefs a great deal more
than if I make an observation in a region like this.
And I'll come back to that in the example.
So the example I have here is we have a state space that is just a grid world.
So I can look at any location in this world.
It's just an 8x8 grid and the state is where am I currently looking in it?
I can then transition between those and at each state,
I can observe one of three things.
I can observe nothing.
I can observe a black dot or I can observe a red dot.
And the way that this is set up is that every time I look at a black dot,
it will change color, so it will change from black to red.
This isn't explicitly in the generative model.
This is just a feature of the environment.
And I can then simulate the same sort of thing
by using those same gradient descents on the variational free energy.
And here you can see it's looking around the world.
It's finding these dots and then it's moving on to the next one.
It's not looking back at the previous one.
It's continuing to explore.
Now, the motivation for this particular task is partly due to my interest
in thinking about how generative models can be broken
and the sort of ways we can explain pathology in terms of this.
But they also give you a nice way of thinking about what sort of influences
go on in this generative model.
And in particular, there's a clinical condition called visual neglect
where patients with a stroke to the right side of their brain normally
will often neglect the left side of space.
They'll perform fewer saccads to it, so these are eye tracking traces
just at rest looking at neglect patients and a control group.
You can see that they perform many, many more saccadic eye movements
to the right side of space than to the left.
The same patients will fail to, if you ask them to draw a clock,
they'll draw the right-hand side of it and not the left.
And there are a range of different phenomena that echo of it.
It's an interesting condition, but I won't go into too much detail about it now.
The crucial thing is that a common test that's used to assess neglect
is to give people a sheet of paper with a series of lines drawn on it
and to ask them just to cross out all of the lines.
And they'll often cross out all of those on the right rather than the left.
The reason I'm talking about this is that it's an interesting problem
thinking about the influences over exploratory behavior
to think what would lead a visual search towards cancelling
only those lines on the right versus the left
or only attending to those on the right versus the left.
What would lead to this sort of scan pattern
in terms of what would have to be broken in the generative model?
And so here are some examples.
So here we have the same thing I was showing you before,
and it looks at both sides roughly the same number of times.
It's relatively even.
But then I have three ways in which we can break this
by changing a prior belief in the generative model
to behave in an abnormal way.
The first of these is very simple.
We just say, sorry, this shouldn't be an expectation, this is a function E.
If we set a prior belief that I'm more likely to perform
saccades to the right than the left, it's an almost trivial example,
but it shows one way in which we can produce this sort of behavior.
It's just a priori more likely that I'll look over there.
The second thing we can do is change the model
such that there's a bias in proprioceptive beliefs.
I'm more likely to experience data that says I've looked to the right
than to the left.
And by minimizing my free energy, I fulfill those predictions
and I end up looking to the right more often than to the left.
But here is probably the most interesting one.
This is the one I was talking about before.
So we've got our information gain term where we're looking at how much
do my expected beliefs diverge from my current beliefs.
If that diverge a lot, then that's a good thing to do.
So in this particular example here, all we've done
is we've increased the confidence in the initial distribution
exactly as I showed you on the previous couple of slides ago.
So the two new observations will cause a relatively small change
in my beliefs about this mapping, where I look to what I'm going to see.
So the mapping from where to what, where I'm looking
to which of these targets I see becomes more confident
and is modified less after a saccade to the left versus right.
So if you think of the kind of metaphor of the brain as a scientist
who is trying to work out what's the best experiment to perform next,
you don't do an experiment that just changes nothing about your beliefs.
You want to do the sorts of experiments that will get you new data
that will change your beliefs about the world.
And that's exactly what we're saying here.
And you can see the influence that's having over behavior.
Again, this is just a gradient descent on the free energy gradients,
but it's giving us this interesting pattern similar to these other types of lesions.
And here's just a speculative link between some of these
in different regions of the brain.
So we can think about things like this where it's
altering a mapping as altering synaptic connections in the brain.
And that fits quite nicely with ideas of long-term plasticity,
synaptic modulation as a mechanism of learning what's there.
Right. So I'm going to come to the final bit now,
which is thinking about, given these kinds of structures,
given these kinds of architectures, what we've really thought about
are how do I decide where to look?
Or how do I make decisions of some other type?
They're all decisions.
They're all categorical.
I can look over there or there or there.
Ultimately, when you actually engage with the world,
you have to do it in a continuous way.
You know, we have to engage with approximately Newtonian laws
and generate continuous changes and continuous variables.
So the final bit I want to talk about is how we actually go about doing that,
how we can incorporate that into a generator model,
and again, appeal to exactly the same notion of free energy minimization that we have
to frame perception and decision-making.
So this looks very similar to what I've shown you before,
but it's actually subtly different.
And here I've changed the variables.
So these are now continuous variables where we have some causes in the world,
some hidden states, which are now X, and some sensory data.
But now this is not a progression over time.
So the thing that's changed here is that now we're looking at the mapping from
my position to my velocity, my velocity to my acceleration,
and successive temporal derivatives.
So I can now express a kind of stochastic trajectory
in terms of a sort of Taylor series approximation,
where I've just brought out all the coefficients for that Taylor series approximation,
which gives me a way of instantaneously representing a trajectory
rather than just a current state.
For those familiar with it, if you were to truncate this here,
this would essentially be an extended Kelvin filter model.
So if we find the, sorry, that's all a bit too small,
but if we find the free-owned integrations for that system,
we can reconstruct a similar network that relies upon errors here
that drive changes in expectations.
And then those influence the higher level causes and what's going on,
my errors about the states themselves.
So this can be extended hierarchically, essentially indefinitely.
So some of you may recognize this as being a predictive coding architecture
where we are just passing prediction errors and expectations around.
Again, this is the position, velocity, acceleration.
The new aspect of this that wasn't in what I showed you previously is this bit here.
You probably can't read that very clearly,
but that essentially says that action now is also going to follow
a gradient descent on the free energy.
The only thing action can change from the perspective of the generative model here
is the data itself.
So I can either optimize my beliefs about the data,
or I can change the data such that it becomes more consistent with my model.
And when we frame it like that, we see that the free energy gradients
take the form of essentially a weighted prediction error.
So I can treat action as being, I get some sensory data, I compute an error.
That drives action which then in this particular case moves my eyes
such that it fulfills any prediction error that's in play.
And so we get what neuroscientists have studied for centuries now,
the idea of a classical reflex arc.
Okay, yeah.
So what I've described here gives you a way of,
if you have some prior beliefs about what sort of trajectories you're going to engage in,
then you can then implement those trajectories.
But the big question is where do those prior beliefs actually come from?
Oh, I should show you.
So here's an example of some a simulation using these equations
that treats the eyes both as spheres.
The agent can change the acceleration or the circular acceleration of the eyes
and they obey essentially Newton's second law for rotational systems.
By adding in a prior belief about the sort of trajectory it engages in,
you can see that the eyes then follow that trajectory
and giving it different prior beliefs gives you all sorts of different,
all different sorts of ocular motion behaviors in this particular example.
We've used the same sort of thing to simulate things like handwriting
and various other types of behavior with exactly the same set of equations
but different generative models in the past.
Yes, so the next question was where do these prior beliefs come from?
So I have this network, if I set some prior belief about V,
then this will cause a particular change in trajectory
which will cause particular data or predictions about data
that are then fulfilled by acting.
But I can put the network that I described above on top of this one
so I can just say that instead of this being an observation in and of itself,
this is actually just another random variable
that indexes which of these continuous variables to make use of.
So you can think of this as a process of forming different hypotheses
that then have to be implemented in the continuous domain,
different alternative, alternative trajectories that I could choose from
and I infer which one is most probable based upon the beliefs here
which includes all of the exploratory and pragmatic behavior
but then I can map that down into this continuous model
with several different hypotheses and then pass messages back up
that are effectively the evidence for each of those models.
So I have a hypothesis that I might look over there
versus a hypothesis that I might look over there.
Each of those can be mapped to a prior belief
that my eyes will end up in this continuous location
or this continuous location
and then I can allow those continuous dynamics to play out,
evaluate the evidence for each of those
and pass that back up to say what was the more probable location
that my eyes went to.
So you get a process of decisions going down to movements
and then messages being passed back up again
and you start to get this hierarchical structure
where you can pass messages both up and down.
Again you can extend each of these indefinitely hierarchically
so you can repeat this above itself and above itself
and similarly you can have many, many layers of this
and get fairly complicated models overall.
So this is just a summary of what I've said over the previous few slides
this idea that we can have these discrete equations
that are playing out or these discrete inferences
that are playing out that are saying
what are the most probable policies I'm going to pursue
what are the most probable states that result from that
what are the most probable outcomes and observations
that I'll get based upon that.
These feed down through essentially model averaging
a bit like a mixture of Gaussians
from a discrete variable, a categorical variable
to a continuous variable.
We then perform these filtering steps
that essentially behave a bit like an extended Kalman filter
but with action added on
and then we pass that all back up again
evaluate the evidence for each different alternative model
which we can do fairly efficiently using Bayesian model reduction
and pass that up to inform the next step.
So we again get this kind of complete perception action cycle
so we can see how that might be implemented in the anatomy of the brain
and we've argued for some of these structures more than others
there's pretty good evidence that things like the basal ganglia
might evaluate the evidence for different kinds of policies
and behavioral trajectories I could engage in.
On the left we're just showing these
some of the inferences that happen at each step
so this is which the state's under each different kind of policy
which location am I going to look at
and I'm inferring those both at the present time
and at future time steps.
I then map that through with this bit
so this is representing the kind of topography
of the superior colliculus in the midbrain
and the idea here is that each location in the colliculus
represents a different hypothesis about where I could look
and this red dot indicates the error between where I'm currently looking
and where I think I should be looking
and you can see that as we form an inference about the next state
this moves out and as the error increases
I now believe I should be looking somewhere else
as the eyes move there that error closes in.
There is a cell population in the superior colliculus
that behaves almost exactly like this
so it seems again very biologically plausible.
Okay so that is the end of what I wanted to say
I just wanted to thank these various people
my funders and university and thank you for listening
so anyone have any questions?
