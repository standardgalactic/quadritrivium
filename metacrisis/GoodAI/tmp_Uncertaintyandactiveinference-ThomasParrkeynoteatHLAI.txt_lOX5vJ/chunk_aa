Thank you very much for the introduction.
So yes, I'm going to talk about active inference, which is a theoretical framework we use to
try and understand brain function and the neurobiological processes that underwrite
a lot of the cognitive functions that we're interested in.
The kind of central idea in active inference is to treat everything the brain does as an
inference problem of some sort.
And that includes perceptual inference, most obviously, but also the notion of planning
as inference and movement itself as an inferential process fulfilling predictions about the world.
I'm going to talk about particularly the role of uncertainty in these kinds of formulations
because this is one of the things that active inference does quite well.
And those of you who are in the last session before lunch will probably recognize a lot
of the concepts that come up here.
So the basic structure is I'm going to first of all give an overview of the idea of active
inference and just a brief summary of what it is.
I'm then going to talk a bit about the sort of neuronal networks, the kinds of architectures
we might expect biological systems to use to actually solve these inference problems.
And then I'm going to give you a couple of examples and show some simulations that illustrate
the paver of this kind of system.
First one looking at the role of uncertainty, the idea of exploring to maximize my information
gain in the environment.
And the second to instead of thinking about uncertainty about what are the states in the
environment causing my sensations, actually thinking about what influence does my uncertainty
about the probabilistic structure of the world?
What influence does that have over the behaviors I choose to engage in?
So they're thinking about the role of learning as opposed to inference.
Yes?
Okay.
Okay.
Right.
Can you hear me now?
Is that okay?
Yeah?
Okay.
I'll try and let me know if I drop too quietly again.
The final thing I want to talk about will be the processes that convert the decisions
we make and the plans that we infer we should be following into movements in the real world.
And I'll give you some examples for each of those.
So the key objective function of active inference, the key thing that everything works to try
and optimize is this free energy functional here.
And this is something that comes up in variational inference.
This is just a quantity that says that if I have some beliefs that such as distribution
Q about the states S that are causing things in the world, then I can plug them into this
function.
And by minimizing that function, I can then get the best set of beliefs that best describe
what's going on in the outside world.
That depends upon there being some sort of probabilistic model that I have about the
way the world is structured.
And that's given in this distribution P. So that's saying how these states S give rise
to observations O. And the little, the tilt, the line over the top indicates that these
are trajectories.
We're interested in a process that's evolving over time.
The other term in here, the pie that everything is conditioned on, is the particular model
of how things are going to evolve or the policy that I'm going to pursue.
So this essentially says a policy is just a sequence of actions or a sequence of states
that I can follow.
And I have several different models for how I could behave, several different alternative
policies I could engage in.
And I can associate a free energy functional with each of those.
That's fine when I'm trying to make inferences about the data I currently have, the observations
that are currently available to me.
That becomes much more problematic if I then want to move into the realm of planning.
Because if I want to minimize my free energy in the future, I need to try and predict what
that free energy might be.
And you can see here that the free energy as it is is a function of observations of data.
As I don't have data for the future yet, by definition, I instead replace the expectation
here, or the distribution I'm using to average this, with one that includes the observations.
So I'm now predicting what sort of observations I'm going to get, and I'm averaging this
thing with respect to both the observations and the states that I expect to lead to them.
And that gives us something to think about in terms of how we plan and how we perceive
these things into the future.
So once we do this, we can think of perception as the process of optimizing this distribution
to see what are my beliefs about the states of the world at present, and what will they
be in the future.
And I can optimize that, and then I can simply plug that in to the expected free energy that
I've defined here.
I'll come back to that in a second, and I can use that to form beliefs about the sort
of plans I'm going to engage in, the sort of policies I'll pursue.
So here this is just the policy, the belief I have about how I'm going to act.
I can use that to generate an action.
I'll come back to that as well.
And that action changes the world, generates new observations, and we get this kind of action,
perception cycle with planning to pose that.
Now the reason I've put these two equations on is just to emphasize some important features
of these two free energy functionals and what they actually mean.
And here you can see that we can re-express this thing here in terms of this thing here,
which is just the negative log probability of a certain set of observations.
And you can think of this as the evidence for the model, or the evidence that I'm continuing
to exist.
I'm seeing sort of data that are consistent with my own model of myself.
The bit that's actually changing with respect to this Q distribution is this bit here, this
kale divergence.
This is a non-negative quantity, so this is always positive, that just quantifies how
different these two distributions are.
And here we're seeing that this is the posterior distribution, the probability of states given
the data I've already observed, and this is our beliefs.
So essentially by minimizing the free energy, I'm effectively reducing the distance, or
I'm trying to approximate what the true posterior should be as best as possible using my current
beliefs.
And of course, if you were to just substitute in the true posterior, if you could calculate
that analytically, then this would just become exact inference.
I'd point out that depending on what field you come from, sometimes the free energy is
represented as the negative of what it is here, particularly in machine learning.
And that often leads to the rhetoric of maximizing free energy or maximizing the evidence lower
bound.
I'm going to be talking about it using the kind of physicist's definition, which involves
minimizing the free energy.
So this is now an upper bound on the negative evidence.
This here is the expected free energy, rearranged in a similar way.
And you see, we still get this kind of expected log evidence term.
And we can think of this as being a kind of goal directed drive.
This is saying that I want to seek out those data that are consistent with my model, and
those might be those data, those might be interceptive data that are consistent with
maintenance of homeostasis or something like that.
They could also be extra accepted in a goal directed task of some sort.
The interesting thing here is that the expected KL divergence here has swapped round.
So the sign is now negative.
And we now have the expected posterior and the current posterior.
And the reason for that is the way we take the expectation here.
That means that to minimize the expected free energy, I need to actually maximize the difference
between my beliefs as they currently stand, this Q distribution here, and what I expect
my beliefs to be after making the observations that give rise to this posterior here.
So now we have an interesting way of thinking about what this is.
There's a drive here to fulfill the kind of goal directed predictions.
And there's a drive here to try and change my beliefs as much as possible by observing
new data.
So we can think of this as carving up the expected free energy into a component that
drives exploration, information seeking behavior, very much as many of the terms did in the
previous talk, and a drive towards exploration.
And we can give these labels like intrinsic and extrinsic value and various things like
that.
And then we can take a moment to the relationship between this and various other different kinds
of scheme.
But first of all, just to make it explicit what this looks like when you actually minimize
the free energy, when you work out what the posterior beliefs should be.
So this is assuming a sort of partially observed Markov decision process like form, where policies
cause state transitions and states cause observations.
So when we minimize the free energy, we essentially get my belief about states the present is
a function of beliefs about states the past and some transitions, beliefs about the future
and some transitions, and the evidence I'm getting at the moment.
So essentially we combine two empirical priors with sensory evidence or evidence from data
and combine those to try and form a posterior belief about the present.
And that's our perceptual inference pitch, that minimization of the free energy.
When we look at the realm of planning, if we have a prior belief that we minimize the
expected free energy, the posterior belief will contain both the expected free energy
and the free energy.
So here we have yet another expression of the free energy and the expected free energy.
And I'm just going to write out each term in that explicitly and just talk you through
how you can intuitively think about what this means and what it's doing.
So the first term here, we generally refer to as accuracy.
So this is essentially saying, given I have several beliefs about what sort of policies
I could engage in, I can take the data I've already gathered and work out whether that
carries evidence in favor of me pursuing one policy or pursuing another.
And so here it's just saying, if the observations I've observed along the course of following
this sequence of actions is consistent with me having followed that sequence of actions,
then it's probable that I am following that sequence of actions.
This term here, well, let's go in sequence.
So this term here is the sort of thing that's used in approaches like uncertainty sampling,
looking for those data about which I'm most uncertain.
So each here just means the entropy.
So if I expect something to be very, very uncertain in a particular place, then it means I'm more
likely to try and observe those data and learn more about the world.
If we combine these two terms, so here we now have a conditional entropy that says how
reliable is the mapping from states to observations and how informative will they be?
If we combine these two terms, then we get something that's being given a whole range
of different names.
Sometimes being referred to as information gain, you find this in even in the fifties
literature on Bayesian experimental design as the value of an experiment, Bayesian surprise,
salience, novelty, epistemic value, and sometimes intrinsic value.
So this is our information seeking component.
And the final bit here is just to point out that these two things together limit how far
my beliefs about the policy I can pursue deviate from my prior beliefs.
If they deviate a long way, then that's penalized.
It's an excessively complex explanation for how I can pursue this behavior.
And the final thing is this term at the end, the kind of expected evidence term.
And we can interpret this in terms of an expected utility.
This is the thing I'm trying to seek out or in terms of an extrinsic value.
Okay, so next thing to talk about is just to think about the sorts of neuronal architectures
that can actually do this sort of thing.
And the way we get to that is we think we first of all pick a particular model.
In this case, I'm sticking with the Markov decision process model.
And we express the free energy in terms of the sufficient statistics of those distributions
and the parameters of those.
So here we're dealing with categorical inferences.
So we're just dealing with vectors and matrices of different kinds of quantities.
So I can be in one state and then the next state and then the next state.
And there's a transition matrix that determines how I go from one to the other.
If we just take the free energy gradients of that system with respect to the beliefs about states,
then we get a series of relatively simple update equations.
We can do the same thing for planning.
And again, we can just express these in relatively simple linear algebra.
Again, these are just the free energy gradients with respect to each of these things.
And we just express it as a gradient descent on the variational free energy.
Or in this case, we set up a belief that I'm going to pursue those policies
that have the minimal expected free energy.
The nice thing about these equations is that the dependencies between the variables
actually look quite similar to the neuronal networks that we know are present in the brain
or things of a similar form.
So just to give you a sense of that, we know that we have sensory data that inputs
to normally granular layer four of the cortex.
I know that's three in this particular example.
