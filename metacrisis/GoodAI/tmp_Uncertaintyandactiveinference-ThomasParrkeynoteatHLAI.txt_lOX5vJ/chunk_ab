But we can think of those as computing a prediction error
that's the difference between the beliefs I currently have and the sensory input I have.
And you can see that that prediction error also depends upon this future and past component.
So here I'm representing past on the left, present, future.
So influence from the past, influence from the future, influence from sensory data.
And those errors are then being used to drive that gradient descent process.
I can then evaluate the expected free energy based upon that,
work out what the expected free energy is, and then compute the probable policies.
And use a Bayesian model average to work out what are the probable states
that I could be experiencing and pursuing based upon my beliefs about the states under each policy
and my beliefs about the probability of pursuing each policy.
OK, so that's the sort of architecture we would think of.
And this is similar to the sort of corticose, subcortical loops that we expect to find in the brain.
And this gives us a way to start to think about how we can interpret neuroanatomy
in terms of the inferences that it's trying to pursue with inferences about what's actually out there
and inferences about how am I going to act and what will that do to change what's out there.
Right, so the next thing I want to do is to give you an explicit example of this
in a deliberately very simple generative model and to try and give you some intuition for how that works.
So again, this is the partially observed Markov decision process.
Here we have our beliefs about the policy, which influence the transitions from state to state.
We have transition matrices here that say if I'm in a current state,
what's the probability of being in the next state?
And similarly, we have a mapping here that goes from states to observable outcomes
and says what's the probability if I'm currently in a particular state of observing a particular outcome?
And the key thing here is we can actually parameterise these
such that we can alter the uncertainty in each of these distributions
and see what effect that has for exploratory behaviour.
So here, in this case, imagine I'm in a particular state
and each of these represents the probability that if I'm in that particular state,
I'm going to observe a particular outcome.
So here are all the different outcomes I could observe.
Here we've just set up a parameter that we've denoted zeta or zeta
that's effectively an inverse temperature parameter on this distribution
so we can make it more or less precise.
If it's very large, then this becomes almost deterministic.
If it's very small, then this becomes completely flat.
I could observe anything based upon where I am.
We've also done the same thing for transitions here.
We've set a parameter that essentially quantifies how uncertain we are about,
given where I am, where I will be next.
Again, if it's zero, this is very flat.
And then as we increase it, it gets very large.
And that just means that we can control the uncertainty here
or the sort of volatility of the sequence.
And the uncertainty here in the mapping,
the simulated agent believes occurs between the states of the world and the observations.
This is a fairly arbitrary way of doing this.
We could obviously parameterize this in lots of different ways.
This is just to give you a sense of one way in which we can do this.
This is a representation of the generative model
that I'm actually going to show the simulation for.
So here, I'll go back a second.
So here, if you imagine that each of these states is a place I can be looking.
So I can move my eyes to various different locations
and fixate different locations in visual space.
And at each of those, I can see something.
I can get some sort of visual or proprioceptive feedback
from where I've ended up looking.
And so the policy just becomes the sequence of saccades,
the sequence of fast eye movements I'm going to pursue
in order to induce these state transitions.
So here we have the hidden states that are the fixation locations.
So these are all the S's.
So I can be fixating in location one, two, three, four, or in the center.
I have another set of hidden states that I have no control over,
which are which of these three squares is present in each location.
So it can be either one, two, or three.
If I look to location one and the third of these squares is there,
then that will map to a visual outcome that says I'm looking at this particular color.
There are transitions between these,
and we can control how deterministic or how stochastic those transitions are.
And we can control the mapping between these things.
So here these are those parameters saying how uncertain
is the mapping from states to observation.
Finally, we have this other outcome modality,
this proprioceptive modality that says where am I currently looking.
And so these can transition.
I have control over how these transition together.
So these make up the visual and proprioceptive outcomes,
and the uncertainty of these transitions and the uncertainty of this mapping
can all be manipulated.
And the reason for doing that is just to show you what the behavior of the system is
will appear like.
So here we have three examples of that same environment,
but with slightly different uncertainties in play.
So here we have the simulated eye tracking trace is interpolated
between the points.
It's just choosing one point or the next.
So it's choosing to look there, there, there, and exploring the environment.
I should point out that there's absolutely no preferences in play here.
There's no rewarding location.
Everything here is intrinsically motivated through this drive information game.
So you can see in this example, in every single location,
the mapping of the likelihood that says, given a state,
what's the probable observation is the same.
And the same is true for the mapping from state to state.
So everything is equivalently precise or equivalently volatile.
Here, just to illustrate what happens, we've decreased the precision
or the inverse temperature that maps states to observations.
So now we have a square here, where if I look in this location,
the mapping from states to observations is almost completely random
or is very small relative to the others.
That essentially means that the amount of information I can get
by looking there about the state that's actually there is relatively little.
So here in this presence of relatively noisy mapping,
there's not really much point looking there
because the amount of information I can gain about states of the world
is pretty limited.
And you can actually think of this as being a bit like a dark room
where if I look around the environment,
I'm not actually going to be able to have any sort of precise mapping in play.
And you can see that it just completely ignores that location
and seeks out those where there is a lot of information to be had.
Some in psychology refer to this as the streetlight effect,
which is based upon the idea that if you're out late one night
and you've had a few drinks and you've dropped your car keys on the street,
the first place you look for them is under the streetlight,
not because it's the most likely place for them to be,
but just because it's the only place you can actually get any sort of precise information,
which is in a sense base-optimal if you've simulated in this sort of setting.
This final example, we've decreased the precision or the inverse temperature
of the transition mapping.
So here, things transition much more randomly.
And you can see that in the actual environment.
So the agent believes it, but it's also true of the environment itself.
And you can see that this location is actually fixated much more often,
contrary to what's going on here.
So in this particular scenario, you can think of this heuristically as,
I haven't looked there in a while.
It's more likely to have changed than any of the others.
So there is more uncertainty to resolve there if I haven't looked there recently,
compared to the others which will remain relatively static
or at least relatively predictable even in the absence of me looking at it.
This particular location is likely to change and in an unpredictable way.
So I'd better look back there and check.
OK.
And the reason I put this up here is you can interpret these parameters
in terms of which components of this drive to explore have been altered.
So here we've got the conditional entropy is changed by changing this.
And you can see that as this entropy gets larger,
that becomes a less probable policy.
And similarly for the change here in the opposite direction.
OK.
So the focus there in that particular example was on the uncertainty I might have
about states of the world.
The next thing I'm going to talk about is the uncertainty I might have
in the actual mappings between things,
in the probabilistic structure of the world,
and the parameters that govern that rather than the states themselves.
So again, we have the same network here.
But now we're going to parameterize these mappings,
the A mappings that go from states to outcomes.
And we're going to say that this is something that can be learned
through the course of the trial or across several trials.
And this is parameterized essentially using a Dirichlet distribution.
And here is an example of the kinds of updating that you might expect
from a Dirichlet distribution in this scenario.
So again, this is if I'm in a particular state,
these are all the different observations I could make while there.
If I make this sequence of observations here,
then every time I see one, I update that element essentially
of that probability distribution so I'm more likely to see this in future.
The same thing is true here.
And the difference between each of these is the choice
of the initial Dirichlet parameters.
So I can actually say how certain am I in this mapping
a priori, both of them start with the belief that the mapping is itself uniform,
but I can be incredibly certain that it's uniform
or I can be relatively unconfident that it's uniform.
And as an analogy, if you imagine you flip a coin 100 times
and it comes up heads half the time,
then you're relatively confident it's a fair coin
and a new observation is not going to change your beliefs a great deal.
Whereas if I've only flipped it once or twice before,
I've got heads once and tails once,
I'm going to start with the same initial expectation
of it being fair based upon those data.
But a new observation will actually cause a much bigger change in my beliefs.
And that becomes very important in the sense of information gain about parameters
because if I'm going to behave in such a way that I minimize my expected free energy
or maximize the difference between my current beliefs about parameters
and my beliefs in the future,
then it makes sense that I should look to those regions
that have a profile more like this compared to those like this
because here I'm going to change my beliefs a great deal more
than if I make an observation in a region like this.
And I'll come back to that in the example.
So the example I have here is we have a state space that is just a grid world.
So I can look at any location in this world.
It's just an 8x8 grid and the state is where am I currently looking in it?
I can then transition between those and at each state,
I can observe one of three things.
I can observe nothing.
I can observe a black dot or I can observe a red dot.
And the way that this is set up is that every time I look at a black dot,
it will change color, so it will change from black to red.
This isn't explicitly in the generative model.
This is just a feature of the environment.
And I can then simulate the same sort of thing
by using those same gradient descents on the variational free energy.
