Case Western Reserve University's Institute for the Science of Origins proudly presents
the Origins Science Scholars Program.
The Institute advances the scientific understanding and application of origins and evolution of
human and natural systems.
The Origins Science Scholars Lectures are presented with the assistance of Case Western
Reserve University's Segal Lifelong Learning Program, College of Arts and Sciences, and
Media Vision.
Tonight, it's my pleasure to introduce Professor Michael Hincheski, who is the Warren E. Rupp
Assistant Professor in the Department of Physics here at Case Western Reserve University.
As Dr. Hincheski writes, random fluctuations pervade cellular biology from the level of
individual biochemical reactions to the intricate machinery responsible for transport and signaling.
And yet, despite the noise at all scales, collective order emerges, a bewildering hierarchy
of interconnected processes carefully arranged throughout the volume of the cell.
Tonight, he will tell us where that order might come from as he talks about thermodynamics
and the origin of life.
Please join me in welcoming him.
What I want to talk today, in some ways, the entire topic of the talk, is a field of physics
which maybe a lot of people have not heard of because it's not quite always in the news.
It's known as non-equilibrium thermodynamics.
It has kind of an unwieldy name, but the questions it asks are, in some ways, some of the most
fundamental questions we can ask about nature.
So in particular, one of the central aspects of it is that physical systems tend to disorder.
This is the thing, the famous concept of entropy, which is famous, in particular, for being
extremely confusing, not just to non-physicists, but to physicists themselves.
And so what I'm going to try to do in this talk, in some ways, the central theme of it,
is to hopefully make that concept a little bit less confusing, or if that fails, at least
confuse you in a way you haven't been confused before.
So I study biological things, so biological things are not entirely disordered.
So one of the fascinating aspects of this topic is also, despite the fact this tendency
toward disorder, why does complexity arise?
And this leads into very interesting questions, people debate whether how the second law influences
the origin of life.
So we're going to try to kind of disentangle some of the confusion in that area.
And most importantly, as you'll see, this is what I'm going to do with the themes of
the later part of the talk, at what cost.
So we're going to show, and of course this is obvious because we exist, that indeed complexity
does exist in nature, but it comes at an inherent cost, a thermodynamic cost, which
will explicate.
And kind of putting all these ideas together, what can they tell us about how life arose?
Can they actually give us some clues as to the fundamental mechanisms four billion years
ago where these living things first arose?
Now these are all kind of extremely weighty questions, very complex systems and topics,
and we're going to start somewhere seemingly completely ridiculously simple.
But I think it's a nice illustration of some of the basic ideas.
We're going to play a game.
This game is called Billiards on a Bunimovic stadium.
This is a shape shown here on the left that's named after Russian mathematician, and hence
it kind of looks like a hockey stadium.
And the game is going to be simple.
We're going to start with one particle initially.
And what I'm going to do is you're going to pay attention to the kind of position space.
The particle is going to basically move in that position space, bounce off the walls,
and it never loses velocity.
The energy is always constant.
And we're going to keep track of its position here on the left, or on your left also, and
the direction here on the right.
So let's show what a one particle looks like, and you can see just bouncing off, and every
time it bounces, the direction changes.
Now we're going to play now this same game, but we're going to make 200 copies of that
particle.
Now these particles don't bounce off of each other, they don't interact, we're basically
making 200 copies and all superimposing them on the same graph, and we're just going to
watch how they behave.
And the main difference is we're going to put all the particles initially at the same
spot shown here, but we're going to make their directions ever so slightly different, maybe
plus or minus a degree here, and then we're going to watch how the system evolves in time.
So you can see the particles start kind of as a compact group, but every time they hit
a curved surface, the groups begin to spread apart, and this continues, and we're going
to watch this for a few minutes.
And what you can notice here, this is actually one of the most famous effects in dynamical
systems, known as the butterfly effect.
You may have heard this from who talks about chaos theory, where if you have this complex
system, and this doesn't seem particularly complex, but it's complex enough, and you
start it with slightly different initial conditions, and you watch it later on in time, those trajectories
will eventually diverge quite dramatically.
And the famous example of this is a butterfly flapping its wings in Hong Kong might end
up weeks later with a hurricane off the coast of Florida, whereas if it didn't flap its
wings, that wouldn't have happened.
That's kind of the popular science version of this, but in general, it just means this
kind of very sensitive dependence on initial conditions.
And what you can see here is from that initial point where all the particles were in the
same spot with more or less the same direction, in other words, kind of an ordered system
and we'll define that a little bit more quantitatively later on, we go to a situation where all
the particles are now spread almost uniformly throughout the entire volume of the stadium
and almost all possible directions are uniformly exhibited.
So this is going to be crucial, but we want to kind of somehow quantify this, put this
into, give it a number.
So this is kind of going from an ordered to a disordered way, can we actually translate
that into like a quantity.
So to do that, we're going to add one more element to our game.
We're going to create basically a grid of addresses.
Think of this almost like regions on a map, you have demarcating longitude and latitude,
but here we're going to have some addresses denoting position on the left and some addresses
denoting direction on the right.
Now here I've just divided everything into six sections for simplicity.
I could do this into a million sections.
The same concept still applies and at each time, while we're looking at the system, we
can be at a particular, or the system is going to be a particular address.
So for example here, the system at the initial time is in address 1D, because it's in region
1 direction region D and at some later time it's going to be in some address which we'll
call xy, but there's always going to be some address in which the system is at all times
and in order to basically quantify this a little bit better, we're going to now, we're
considering this ensemble, this 200 copies of the system, so we're going to basically
define the fractions of that ensemble that are in particular addresses at particular
times.
So we're going to say the probability to be at a certain address xy at time t is basically
going to be the fraction of all those systems at that address.
Now this is going somewhere, it's not just mathematics for mathematics sake, because
once we have that probability, we're going to, this is the final step, we're going to
use it to calculate a number, right?
And that's, so I'm going to show, there's going to be a few equations in the talk,
I apologize, understanding it hopefully won't be based on, you know, you don't need to follow
the equations too deeply, but this equation I really wanted to show because in some ways
I'm going to argue it's one of the most beautiful equations in physics.
And so what we've done here is we've taken that probability, we've summed, we've multiplied
it by the logarithm of itself and summed over all the addresses.
We multiply it by some unimportant constant out in front and that's the thing we call
entropy, this mystical, important quantity in physics, that's it, that's all it is.
It's just this looking at a system, dividing it up into addresses, calculating probabilities,
getting a number out of it.
And what we'll show later is that as a concept this is incredibly versatile because these
addresses here are position and direction addresses, but they could be anything, in
fact they could be chemical states and we'll show examples of that later.
The originator of this equation, the first one to write it down is J.W. Gibbs.
He was, someone called Einstein called him the greatest mind in American history.
That might be, I'm not qualified to say that directly, he definitely arguably is among
the or the greatest physicists in American history.
Part of the beauty of this, of his work on thermodynamics in general in this equation
in particular is it has such a long afterlife.
So 50 years roughly after he wrote it down, Claude Shannon came across the same equation
in information theory and more or less laid the foundations of our modern understanding
of how information is processed in computer systems.
So this is an equation with an extremely many, many different applications, but for us it's
a number, a positive number and it's going to have a very simple kind of interpretation.
So if all of our system is in one position on the map, so one region on the map, our
entropy is zero.
As the ensemble kind of covers more and more different regions, so we're going to color
those with different shadings of orange, entropy grows.
So initially entropy was zero, here now it's spread out a little bit, so entropy has grown
to about 1.9 in this unit of K, here it's now almost uniformly spread throughout both
in position and direction space, it's reached about 3.4.
So think of it as just a measure of how much this probability spreads out.
And we're going to now watch that same movie that we saw before, that ensemble kind of
diverging because of this butterfly effect, but we're now going to keep track of that
number as we're watching that.
So this is what it looks like.
So it's diverging initially, probability is more or less confined in a few regions, our
entropy is small and then as time progresses entropy increases.
Now it's a bit noisy here because we only did 200 copies, so if we had done this like
a million times, a million copies we'd actually see a smooth increasing curve and as it increases
it begins to kind of saturate and almost reach a maximum, and that maximum is also going
to be important because that maximum essentially is the place where the probability is more
or less uniformly distributed both in position and direction space.
So I'm not going to wait until it completely reaches it, but this is what it looks like
after you run it for a long time.
And that maximum is then the second, like three equations in the talk, this is the second
equation.
Again, one of the most fundamental equations in physics, which is that maximum of entropy
that it reaches is just K times the log of W, and W all it is is just a number of addresses
in our map.
So we had six position addresses, six direction addresses, so altogether 36 combinations, so
it's just a log of 36, which is about 3.4.
That's it.
Now this equation is so important that Boltzmann or rather Boltzmann's probably immediate family
or supporters had it carved into his gravestone.
So it is literally a physics equation carved in stone, and it's probably the most famous
version of entropy that people are familiar with, but keep in mind that it only applies
at really long times, right?
Once the system has basically uniformly distributed itself around all of position and direction
space, and that's what we call equilibrium.
But what I'm going to argue throughout the rest of the talk is somehow this initial part
where entropy is rising is also incredibly important, and in some sense the Gibbs version,
which gives us the entropy throughout, is the more fundamental thing to look at.
So at this stage, we can kind of summarize what this little game has shown us by making
a law, right?
So this is our version of the second law of thermodynamics, which says that for an isolated
system, so a system that does not interact with the environment, like our little Bunimovic
stadium, billiards game, entropy increases over time until it reaches this maximum, given
by the Boltzmann equation, all right?
And it's a law, right?
So you should definitely, definitely believe it, you know.
And this is how I as an undergraduate always took it.
People brought me laws, especially if they're literally carved in stone, right?
You just, you know, you take it as a given.
This is a law of nature.
So is this, in fact, universally true?
So I'm going to show you next, we'll perhaps shock you.
I will violate the second law of thermodynamics right in front of your eyes.
And I'm going to do this by just a very, very small change to our game, right?
We had our Russian hockey stadium with curved edges.
I'm going to make them straight, all right?
So I'm going to look at tennis court.
And let's see what happens.
So here's the same game played in a rectangle, all right?
Now, you can see the group of trajectories here, which started with slightly different
initial conditions, is kind of spreading out, but definitely not as much as before.
And in particular, look at direction space on the right.
Direction space, you see that it's not spreading out at all.
It's only really visiting four corners of that direction graph.
Our entropy is increasing a little bit, but not so much.
