try to emphasize about entropy increasing is that it means when entropy is increasing,
certain sequences of events are much more likely to happen than others.
So for example, that sequence when the, when the balls kind of diverge and bounce and become
more disordered, that seems much more likely to you to occur than a sequence where everything
converges.
Even though it's possible to have all those velocities flipped and everything to go back
to its initial part, it's just really, really hard to do that.
And the more particles you have, the more statistically difficult it is to flip everything
and have everything converge back to itself, right?
So when entropy increases, there seems to be a preferred order of things, right?
And we'll make that a quantitative in a little bit.
Now I want to show you now just a contrast, a second snapshot, which is at the very end
of the simulation when we're looking near equilibrium, all right?
So this is going to be, I'm going to do the same game.
So I'm going to play it for 20 seconds like that.
And then, okay, it's bouncing around like a gas.
I'm going to do the same thing, flip all the velocities, play it again.
And there, if you look at it, you won't be able to tell which one, if I just showed you
without telling you the difference, there's no way you can actually even tell statistically
which one is going forwards, which one is going backwards.
In fact, every sequence of events, when we've reached our maximum, when entropy no longer
changes, is equally likely as any other.
There's no preferred direction.
So in a very real sense, in equilibrium, there is no arrow of time, right?
This is the famous, if you may have heard of it in discussions of this topic, but this
is what it boils down to, right?
We don't see that arrow of time in an equilibrium system when entropy is not increasing.
Okay, so let's make this a little quantitative.
So this is kind of, I'm going to give you a little bit of flavor of some of the ways we
think about entropy now, kind of a modern physics context.
We can actually do this, play this game in terms of addresses.
So we have one take, which is, we have a system that visits, you know, four addresses as
it's evolving in time.
We can ask, take two, the system visits those same addresses, but in reverse order.
And we can play this, you know, we can run the simulations many times.
You can ask, how often does that occur versus take, take one occur versus take two?
And we can calculate the probabilities of those, of those sequences.
If we take the ratio of them and then take the log of it, we get a number, which I'm
going to call sigma.
This sigma is actually in some ways our modern, the modern version of what Gibbs was doing.
We actually now interpret that as essentially being an entropy of a sequence.
So before I was talking about an entropy of an ensemble of many copies of a system, here
we're actually defining entropy at the level of a sequence of states which we visit.
And we, we interpret that as basically the amount by which the entropy of the universe
has increased during that visiting the forward sequence of states.
So if the entropy is not increasing, right, the probability of the top is the same as
the bottom, that's one, right, the ratio is one, the log of one is zero, so entropy
is not increased, that's consistent.
And if one is much, much larger, much, much smaller than the other, then entropy is either
large, very much increasing or decreasing during the sequence.
So let me give you a concrete example of this, Humpty Dumpty, right?
Everybody knows Humpty.
This is a sequence of two events, Humpty is intact, Humpty falls, breaks, and according
to the nursery rhyme, all the king's horses and all the king's men could not put Humpty
back together again.
But they, you know, Humpty could have spontaneously reassembled, again, not violating the laws
of physics, highly, highly unlikely.
But we can calculate the ratio, so Humpty falling, Humpty breaking versus Humpty reassembling,
that number is like 10 to the 10 to the 20, that's a very, very large number.
And how does that, you know, by converting it to entropy in the, in using the formula
from the previous slide, that corresponds about, about 10 to the 20 K of entropy released
into the universe.
So the universe has crept up that much closer towards heat death by Humpty falling.
Now here's then the crucial part for the next several, you know, slides of the talk.
That's fine.
So that's kind of, and this is, this is this law, this, this idea that I've shown you applies
just as well for Humpty's subsystems, the living things, you know, it doesn't, it's
a law that applies to subsystems as well as the whole universe.
But how does the entropy of the universe increase when Humpty falls?
And it turns out that this increase is just heat release, right?
It's about 0.2 calories, so not a huge amount of heat, but that's, oh, are we, okay, yeah.
So it's not a huge amount of heat release, but it's some amount of heat has been released
into, into the, into the surroundings.
And why does the universal entropy kind of increase there?
Because when we give energy to the surroundings, the surroundings in some ways can explore
more addresses, and hence the entropy of the universe as a whole is increasing, right?
So that's going to be kind of crucial because this is how typically we pay our due to entropy
as, you know, subsystems of the universe, we tend to release heat, dissipate heat, right?
Okay.
Let me give you a kind of a more down-to-earth example of this, where we are releasing heat.
So this is a circuit, right?
We attach a battery to a circuit, and, you know, a motor to that, to that circuit.
There's some heat being dissipated.
Now what's going on here, right?
Why doesn't there need to be heat dissipated?
Well, the electrons in that wire, if we didn't have the battery attached, would just be more
or less moving in random directions, right?
There would be no preferred direction.
So one kind of sequence of electrons motion to the left would be equally probable to another
sequence of electron motions to the right.
But in when we have a current flowing through the system, one of those sequences becomes
much more likely than the other, right?
There's a preferred direction for the electrons to move through our wire.
Based on the argument that I just showed you, because there's now this preferred sequence
of states which the system visits, the entropy of the universe must increase.
How does it increase?
Well, heat has to be dissipated, right?
Okay, heat is a form of energy.
It's being released into the environment.
Where does that energy come from?
There must be an energy source, right?
That's why you have to have a battery.
So in a weird way, in this kind of inverted thermodynamic perspective, I argue the existence
of the battery by the fact that first there's a symmetry breaking in the motion of the electrons.
But for me, that heat, that we take it for granted, you know, we plug things in, you
know, our electronics are all heated up, things like that.
That kind of very mundane aspect of our daily lives is, in fact, all the objects around
us are paying to the second law of thermodynamics.
So this heat dissipation is essentially the entropy price because those electrons are
moving in a preferred direction.
Because there's this other law of thermodynamics which I have not emphasized, called the first
law, more or less the energy conservation, the power in minus the power out is equal
to the power dissipated, right?
So you have to have more power in than power out in order to have dissipated heat, right?
So you have to have an energy source that's larger than the power that the motor is using.
All right, this is a much more concise way of explaining this.
This is stupid.
Don't do this.
Those electrons in that wire are not going to spontaneously begin moving around in a circle
because there's no power source.
Okay.
So now let's now switch gears a little bit.
So everything that I've talked so far have been addresses that have lived, positions
and directions, positions and directions of electrons or particles and stadium.
The power of this concept is that address could be anything.
It could be a chemical, it could be a step in a chemical cycle.
So this is one of these famous cycles that if you have memories of high school biology,
probably brings you back some nightmares of memorization.
We're not going to go through the details of these kind of metabolic cycles, but note
that whenever the people draw these cycles, always the arrows primarily in one direction.
Why?
Because that's how it happens in our bodies.
It primarily goes in one direction rather than the other.
What does that mean?
It means that we have now a sequence of states more likely to go in one direction than the
other.
Well, we've got to pay our due to the second law.
That means heat has to be dissipated, right?
So there's going to be some necessary entropic price to pay.
So let me give you kind of one very famous example of such a cycle.
So this is the ATP synthase protein.
So this is a protein that essentially synthesizes this energy molecule called ATP, which then
runs a lot of the other biological processes in our body.
And those biological processes degrade that energy molecule back into ATP and phosphate.
And this protein actually spins around.
It's kind of this interesting rotor complex, and it spins around primarily in one direction
so long as you have an imbalance of ions on one side of a membrane versus the other.
In that previous work where you were bouncing the particles around the racetrack, just as
a problem, I would ask what would happen if the racetrack was elliptical and you started
out at one focus of the ellipse?
I don't think there would be chaos.
I have not explored all possible shapes, but I do think if ellipses were, I would have
heard about it in some ways.
Like it requires a particular combinations of shapes.
An array of light that goes through one focus will wind up at the other focus.
So that would argue against the fact that things are going to become disordered inside
that ellipse.
But I think it would not become disordered in the same way.
I may have missed this.
Excuse me.
What is the actual definition of the word entropy?
So used in this talk, and I wanted to be kind of very clear so that there isn't this confusion.
The definition of entropy is this thing here in the blue box.
So you take a system, you divide it up into a state space, addresses which denote the states
of the system.
You have probabilities associated with being at a particular address at a certain time.
You take those probabilities, you multiply them by the logarithm, you add them all up.
That's entropy.
Could you give me a sentence?
A sentence, okay.
So the less mathematical definition would be, it would be a measure of how spread out
that probability is, how disordered the system is.
How disordered the problem is.
How spread out the probability is, which you can interpret in a way as how disordered
the system is.
Because the more spread out you are, the more addresses you live in, the more different
combinations are possible that you see in the system.
So when entropy is zero, everything lives at one address.
When entropy is large, everything is, all addresses are equally likely to occur.
And there's a roof to help for how I...
Exactly.
Right?
So if this was a completely empty city and all of us sitting here in the audience were
in this particular room, and let's say we divided up the city into boxes of the side
of this room, the entropy of our population would be zero.
But then if you guys all went out and equally like uniformly spread out throughout all of
the Cleveland area so that there was a uniform population density of you everywhere, then
your entropy would be maximum.
That would be the most disordered that you could become.
And it's different for different systems.
It's completely different for different systems.
Even in the notion of what an address is, that's why I kind of use this word address.
I mean, in physics we call it phase space, but that's more technical.
But like what I'm claiming is that this idea of addresses works just as well in chemistry
as it does for actual physical positions.
We hope you've been enjoying the Origin Science Scholars Program with Professor Michael Hynchewski.
Professor Hynchewski is the Warren E. Ropp assistant professor in the Department of Physics
at Case Western Reserve University, specializing in theoretical biophysics.
In the second part of our talk, we learned how life is a cascade of energy conversion
and dissipation.
