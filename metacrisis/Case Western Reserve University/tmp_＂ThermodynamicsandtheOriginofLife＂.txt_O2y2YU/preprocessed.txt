Case Western Reserve University's Institute for the Science of Origins proudly presents
the Origins Science Scholars Program.
The Institute advances the scientific understanding and application of origins and evolution of
human and natural systems.
The Origins Science Scholars Lectures are presented with the assistance of Case Western
Reserve University's Segal Lifelong Learning Program, College of Arts and Sciences, and
Media Vision.
Tonight, it's my pleasure to introduce Professor Michael Hincheski, who is the Warren E. Rupp
Assistant Professor in the Department of Physics here at Case Western Reserve University.
As Dr. Hincheski writes, random fluctuations pervade cellular biology from the level of
individual biochemical reactions to the intricate machinery responsible for transport and signaling.
And yet, despite the noise at all scales, collective order emerges, a bewildering hierarchy
of interconnected processes carefully arranged throughout the volume of the cell.
Tonight, he will tell us where that order might come from as he talks about thermodynamics
and the origin of life.
Please join me in welcoming him.
What I want to talk today, in some ways, the entire topic of the talk, is a field of physics
which maybe a lot of people have not heard of because it's not quite always in the news.
It's known as non-equilibrium thermodynamics.
It has kind of an unwieldy name, but the questions it asks are, in some ways, some of the most
fundamental questions we can ask about nature.
So in particular, one of the central aspects of it is that physical systems tend to disorder.
This is the thing, the famous concept of entropy, which is famous, in particular, for being
extremely confusing, not just to non-physicists, but to physicists themselves.
And so what I'm going to try to do in this talk, in some ways, the central theme of it,
is to hopefully make that concept a little bit less confusing, or if that fails, at least
confuse you in a way you haven't been confused before.
So I study biological things, so biological things are not entirely disordered.
So one of the fascinating aspects of this topic is also, despite the fact this tendency
toward disorder, why does complexity arise?
And this leads into very interesting questions, people debate whether how the second law influences
the origin of life.
So we're going to try to kind of disentangle some of the confusion in that area.
And most importantly, as you'll see, this is what I'm going to do with the themes of
the later part of the talk, at what cost.
So we're going to show, and of course this is obvious because we exist, that indeed complexity
does exist in nature, but it comes at an inherent cost, a thermodynamic cost, which
will explicate.
And kind of putting all these ideas together, what can they tell us about how life arose?
Can they actually give us some clues as to the fundamental mechanisms four billion years
ago where these living things first arose?
Now these are all kind of extremely weighty questions, very complex systems and topics,
and we're going to start somewhere seemingly completely ridiculously simple.
But I think it's a nice illustration of some of the basic ideas.
We're going to play a game.
This game is called Billiards on a Bunimovic stadium.
This is a shape shown here on the left that's named after Russian mathematician, and hence
it kind of looks like a hockey stadium.
And the game is going to be simple.
We're going to start with one particle initially.
And what I'm going to do is you're going to pay attention to the kind of position space.
The particle is going to basically move in that position space, bounce off the walls,
and it never loses velocity.
The energy is always constant.
And we're going to keep track of its position here on the left, or on your left also, and
the direction here on the right.
So let's show what a one particle looks like, and you can see just bouncing off, and every
time it bounces, the direction changes.
Now we're going to play now this same game, but we're going to make 200 copies of that
particle.
Now these particles don't bounce off of each other, they don't interact, we're basically
making 200 copies and all superimposing them on the same graph, and we're just going to
watch how they behave.
And the main difference is we're going to put all the particles initially at the same
spot shown here, but we're going to make their directions ever so slightly different, maybe
plus or minus a degree here, and then we're going to watch how the system evolves in time.
So you can see the particles start kind of as a compact group, but every time they hit
a curved surface, the groups begin to spread apart, and this continues, and we're going
to watch this for a few minutes.
And what you can notice here, this is actually one of the most famous effects in dynamical
systems, known as the butterfly effect.
You may have heard this from who talks about chaos theory, where if you have this complex
system, and this doesn't seem particularly complex, but it's complex enough, and you
start it with slightly different initial conditions, and you watch it later on in time, those trajectories
will eventually diverge quite dramatically.
And the famous example of this is a butterfly flapping its wings in Hong Kong might end
up weeks later with a hurricane off the coast of Florida, whereas if it didn't flap its
wings, that wouldn't have happened.
That's kind of the popular science version of this, but in general, it just means this
kind of very sensitive dependence on initial conditions.
And what you can see here is from that initial point where all the particles were in the
same spot with more or less the same direction, in other words, kind of an ordered system
and we'll define that a little bit more quantitatively later on, we go to a situation where all
the particles are now spread almost uniformly throughout the entire volume of the stadium
and almost all possible directions are uniformly exhibited.
So this is going to be crucial, but we want to kind of somehow quantify this, put this
into, give it a number.
So this is kind of going from an ordered to a disordered way, can we actually translate
that into like a quantity.
So to do that, we're going to add one more element to our game.
We're going to create basically a grid of addresses.
Think of this almost like regions on a map, you have demarcating longitude and latitude,
but here we're going to have some addresses denoting position on the left and some addresses
denoting direction on the right.
Now here I've just divided everything into six sections for simplicity.
I could do this into a million sections.
The same concept still applies and at each time, while we're looking at the system, we
can be at a particular, or the system is going to be a particular address.
So for example here, the system at the initial time is in address 1D, because it's in region
1 direction region D and at some later time it's going to be in some address which we'll
call xy, but there's always going to be some address in which the system is at all times
and in order to basically quantify this a little bit better, we're going to now, we're
considering this ensemble, this 200 copies of the system, so we're going to basically
define the fractions of that ensemble that are in particular addresses at particular
times.
So we're going to say the probability to be at a certain address xy at time t is basically
going to be the fraction of all those systems at that address.
Now this is going somewhere, it's not just mathematics for mathematics sake, because
once we have that probability, we're going to, this is the final step, we're going to
use it to calculate a number, right?
And that's, so I'm going to show, there's going to be a few equations in the talk,
I apologize, understanding it hopefully won't be based on, you know, you don't need to follow
the equations too deeply, but this equation I really wanted to show because in some ways
I'm going to argue it's one of the most beautiful equations in physics.
And so what we've done here is we've taken that probability, we've summed, we've multiplied
it by the logarithm of itself and summed over all the addresses.
We multiply it by some unimportant constant out in front and that's the thing we call
entropy, this mystical, important quantity in physics, that's it, that's all it is.
It's just this looking at a system, dividing it up into addresses, calculating probabilities,
getting a number out of it.
And what we'll show later is that as a concept this is incredibly versatile because these
addresses here are position and direction addresses, but they could be anything, in
fact they could be chemical states and we'll show examples of that later.
The originator of this equation, the first one to write it down is J.W. Gibbs.
He was, someone called Einstein called him the greatest mind in American history.
That might be, I'm not qualified to say that directly, he definitely arguably is among
the or the greatest physicists in American history.
Part of the beauty of this, of his work on thermodynamics in general in this equation
in particular is it has such a long afterlife.
So 50 years roughly after he wrote it down, Claude Shannon came across the same equation
in information theory and more or less laid the foundations of our modern understanding
of how information is processed in computer systems.
So this is an equation with an extremely many, many different applications, but for us it's
a number, a positive number and it's going to have a very simple kind of interpretation.
So if all of our system is in one position on the map, so one region on the map, our
entropy is zero.
As the ensemble kind of covers more and more different regions, so we're going to color
those with different shadings of orange, entropy grows.
So initially entropy was zero, here now it's spread out a little bit, so entropy has grown
to about 1.9 in this unit of K, here it's now almost uniformly spread throughout both
in position and direction space, it's reached about 3.4.
So think of it as just a measure of how much this probability spreads out.
And we're going to now watch that same movie that we saw before, that ensemble kind of
diverging because of this butterfly effect, but we're now going to keep track of that
number as we're watching that.
So this is what it looks like.
So it's diverging initially, probability is more or less confined in a few regions, our
entropy is small and then as time progresses entropy increases.
Now it's a bit noisy here because we only did 200 copies, so if we had done this like
a million times, a million copies we'd actually see a smooth increasing curve and as it increases
it begins to kind of saturate and almost reach a maximum, and that maximum is also going
to be important because that maximum essentially is the place where the probability is more
or less uniformly distributed both in position and direction space.
So I'm not going to wait until it completely reaches it, but this is what it looks like
after you run it for a long time.
And that maximum is then the second, like three equations in the talk, this is the second
equation.
Again, one of the most fundamental equations in physics, which is that maximum of entropy
that it reaches is just K times the log of W, and W all it is is just a number of addresses
in our map.
So we had six position addresses, six direction addresses, so altogether 36 combinations, so
it's just a log of 36, which is about 3.4.
That's it.
Now this equation is so important that Boltzmann or rather Boltzmann's probably immediate family
or supporters had it carved into his gravestone.
So it is literally a physics equation carved in stone, and it's probably the most famous
version of entropy that people are familiar with, but keep in mind that it only applies
at really long times, right?
Once the system has basically uniformly distributed itself around all of position and direction
space, and that's what we call equilibrium.
But what I'm going to argue throughout the rest of the talk is somehow this initial part
where entropy is rising is also incredibly important, and in some sense the Gibbs version,
which gives us the entropy throughout, is the more fundamental thing to look at.
So at this stage, we can kind of summarize what this little game has shown us by making
a law, right?
So this is our version of the second law of thermodynamics, which says that for an isolated
system, so a system that does not interact with the environment, like our little Bunimovic
stadium, billiards game, entropy increases over time until it reaches this maximum, given
by the Boltzmann equation, all right?
And it's a law, right?
So you should definitely, definitely believe it, you know.
And this is how I as an undergraduate always took it.
People brought me laws, especially if they're literally carved in stone, right?
You just, you know, you take it as a given.
This is a law of nature.
So is this, in fact, universally true?
So I'm going to show you next, we'll perhaps shock you.
I will violate the second law of thermodynamics right in front of your eyes.
And I'm going to do this by just a very, very small change to our game, right?
We had our Russian hockey stadium with curved edges.
I'm going to make them straight, all right?
So I'm going to look at tennis court.
And let's see what happens.
So here's the same game played in a rectangle, all right?
Now, you can see the group of trajectories here, which started with slightly different
initial conditions, is kind of spreading out, but definitely not as much as before.
And in particular, look at direction space on the right.
Direction space, you see that it's not spreading out at all.
It's only really visiting four corners of that direction graph.
Our entropy is increasing a little bit, but not so much.
And in fact, if you run this forever, it will never, ever reach the Boltzmann equation.
So that thing carved in stone is clearly not universally true.
If you keep on running this a long time, it's going to kind of look like a 1980s screensaver.
It's kind of mesmerizing.
But it's surely not this kind of spreading out in all of position space that we saw before,
where it almost looked like a gas, right?
Spreading out through a volume.
Here it's much more ordered, right?
And that's clearly reflected by the fact that our entropy is much smaller, all right?
So I think we're going to need a revision on our law.
Terms and conditions may apply.
And in fact, this is actually a subtle and difficult problem because this is, well, from
the mathematician standpoint, this is enough to show that clearly this is not a universal
law, but can it be shown for any systems?
So it turns out that the only place where this law has been rigorously proven is for
these kind of simple mathematical games like billiards.
And Yakov Sinai, who was the first person to actually prove this for a billiards system,
actually won what's the equivalent of a Nobel Prize in math, the 2014 Abel Prize for this
proof.
It's ridiculously hard.
And that's for one particle bouncing around.
It becomes even more difficult.
Like, it's not even proven for a group of balls bouncing off of each other in a simple
two-dimensional surface.
So it's quite a difficult thing.
What do you mean by an isolated system?
So in this particular case, I mean a system that doesn't exchange energy or particles
with its surroundings, right?
So this is, I mean, later on we'll consider systems like biological systems where we're
clearly sitting here, you know, this room is a certain temperature.
We're constantly exchanging energy particles in the gas around us or bouncing off of us,
passing us and taking energy away from us.
So that's a more complicated scenario.
So in this particular case, I mean a system that has no exchange of energy or particles
or anything else with its surroundings.
For both of those shapes, does the size of the shape matter?
In other words, if the stadium was round or if it was higher and wider or in the square,
if it was higher and wider, does that have anything to do with it?
The absolute size of the shape doesn't matter except that circles are a bit different.
So it does have to have this combination of straight and curved edges, but the actual dimensions
of the shape don't matter too much.
It'll matter in terms of how long it may take to equilibrate, but beyond that, the second
law will hold for the curved stadium, yeah.
Be, you know, again in the shape of the arena.
For example, when you have a curved one and you've got objects ricocheting off of a curve,
they have, you know, a bigger range of direction they can travel in when you hit a straight
line or a flat surface.
The angle of incidence equals the angle of deflection, yeah.
So that's actually, you bring up a very interesting point because it turns out that in this particular
shape, the way the dynamics are designed, the angle of incidence always does equal the
angle of reflection, but you have to define the angle as kind of the tangent to the circle
when you're doing the, when you're doing the kind of curved circular part.
So it's basically the line that just touches, that just grazes the edge of the circle.
And then it bounces off of that line.
But what you bring up, which is actually is true, is that the behavior off curved surfaces
is much different than the behavior of straight lines.
And that's why in the rectangular case, you get much less divergence.
So a little extra kind of, you know, the slight mathematical difference there is enough to
create this divergence.
But it's actually a really, it's a really subtle point, and it's not something that
like you can, well, I'm gonna actually, I'm gonna say you can't generalize it, but I'm
gonna literally just do that in the next slide, but it's very difficult to prove rigorously.
And that's why it's kind of, you know, I mean, Yakov Sinai was one of these mathematicians
who sat in the back of physics lectures and looked at what the physicists were saying
and thought, like, this is just ridiculous.
They're making so many assumptions.
Let me go back to first principles and prove it, and then it took them like a career to
prove one simple thing.
What if the arena were shaped, say, like a hexagon or a pentagon?
I think from, I have to double check this.
I believe that any combination of straight edges would not give you chaos, but I would
have to double check that.
I haven't seen this game played in like polygons, but I think you do need to have these curved
surfaces for this to work.
Thank you for joining us.
You've been watching Professor Mike Hynchefsky introducing the basic concepts of non-equilibrium
thermodynamics.
For more information on the Origins Science Scholars Program, please visit the institute's
website at origins.case.edu.
In the next part of the talk, Professor Hynchefsky will discuss entropy and the chemistry underlying
life.
Now, back to the talk.
There is clearly something called the second law of thermodynamics, which we learn about
in textbooks and which does exist, but in some ways it's kind of a leap of faith, right?
What I want to kind of emphasize is, yes, we look at these shapes of the surfaces, but
what really was the distinguished thing was one led to this kind of chaotic dynamics which
was highly dependent on initial conditions, the other didn't.
We believe, again, this very, very rough rule of thumb that complicated systems, meaning
systems with lots and lots of particles like all of us and the universe around us, with
strong interactions where particles bouncing off, interacting with each other through fundamental
physical forces, that system taking an aggregate is indeed a chaotic system.
And we, of course, we know that for things like weather systems, we see chaos all the
time.
And in particular, we think the entire universe, observable universe, falls in this category.
And this is why we can talk about, at least as a kind of leap of faith, a second law for
the universe, even though we can't rigorously mathematically prove it.
Now one thing I wanted to do is an aside, and this is, again, I apologize to actual
cosmologists in the audience.
People love to talk about the entropy of the universe.
What actually is it and what does it look like?
So this is the best estimate that I could find, but it's going to, it looks like that.
This is based on a paper from Egan and Lineweaver published a few years ago, and I wanted to
convince you, it's a little bit different than our line, you know, entropy increasing
over time in the stadium, but still same pattern, things increase over time.
So this is, the time scale is a little bit different.
You can notice this is like 10 to the minus 40, 10 to the minus 21, 10 to the 20.
We're about 10 to the 17 seconds since the Big Bang.
But overall, that entropy has increased by fairly large, you know, it kind of stays constant
for a while and then has these huge increases over time, and we're right about here kind
of on this final plateau before an extrapolated value where eventually the universe will end
up in heat death and perfect equilibrium, right?
That's the maximum over there.
We're not there yet.
We still have time to go, happily.
Now what's really interesting about, I'm not going to talk about all the, what causes
all these individual bumps, just kind of an aside, what's really interesting is if you
have to guess what contributes the most to the entropy of the universe in the current
moment, it's actually quite interesting.
So here's our entropy.
So 10 to the 104 in these units of K, and these are the contributions.
So supermassive black holes.
So these are the black holes that can be billions of times the mass of our sun, 99.999% a lot.
What else contributes to the entropy?
Oh, there's also smaller black holes.
Those are in there about .0001%.
And there's a bunch of other things, photons, you've got to add a bunch of zeros.
And then there's a whole list in this paper, and eventually you'll get down to ordinary
matter, including all of us in the audience here, and the rest of the stars and other
matter in the universe.
And that's our fraction of the total entropy.
In a weird way, and again, something that's not the topic of this course, black holes
dominate the entropy budget of the universe to a ridiculous extent.
In fact, that recent black hole that you all saw in the news, that famous, the black hole
picture, that's about 10 to the 95 K in entropy.
If you, based on its extrapolated parameters, that has 100 trillion times more entropy.
In other words, 100 trillion times more addresses living on the surface of that black hole than
all the ordinary of the matter in the universe put together.
So one way of looking at the issue of complexity in terms of the entropy, this second law says
the entropy of the universe must always increase, is in some sense, all of us here are highly,
highly irrelevant to the overall entropy budget of the universe.
That cannot be said enough.
I mean, in some sense, the fact that you have these stars burning for a few billion years,
and you have some rocky planets circling those stars, and some chemical processes happening
on those planets that lead to some kind of self-aware life, I mean, that's kind of like
froth on a giant ocean, where the vast majority, the bulk of that ocean is living on the surface
of black holes.
So in some sense, what's happening with black holes dominates the entropy budget, and everything
else is kind of like, you know, you can't even call it a rounding error, because this
is like such a tiny, tiny fraction of it.
Now, that's not quite the end of the story, right?
Because in that sense, then we could say, well, anything goes, because, you know, at
some level, like, the fact that we have, we lower our entropy state a little bit here,
it doesn't really matter that much in terms of the universal entropy budget, but it's
not quite that, right?
Because in some sense, the second law doesn't just apply to the universe as a whole, it
also applies to subsystems of the universe, and that's where we pay kind of the dues
to the second law in everything that we do.
All right, so how does the second law play out on smaller scale?
So that was the end of our cosmology aside.
To understand this kind of, you know, its implications on our scale, let's take a look
at some snapshots.
So we're going to go back to our game, and now we're going to look at, let's say the
first 20 seconds, okay?
So we're going to look at that movie again, first 20 seconds of it, while the entropy
is increasing, let's watch, all right?
So the thing kind of diverges, and then the movie will stop, all right?
I'm going to show you a second version of that, and you can just shout out what you
think is going on.
So this is the second version of that.
It's converging.
What have I done?
I'm playing, okay, yes.
Obviously, it seems like I'm playing backwards.
The answer is no.
I'm almost doing that, and in fact, I'm getting the same result.
What I've actually done is I've started in this configuration here, that was the configuration
at 20 seconds, and all I've done is I've reversed all the velocities.
So I basically, these were the old velocities here in pink, I've now just moved them across
the other side of the circle, and just, you know, rearranged all the directions.
And when you do that, everything just converges back to the beginning.
It seems like it's going backwards in time, right?
To us, psychologically, that's what seems to be happening, and in fact, this is actually
true that for any physical system, you can actually reverse things, right?
So people have actually just a few days ago that was in the news that people have managed
to do this for several qubits.
They more or less were able to have qubits evolve in time for a certain amount of time,
do some complicated quantum mechanical manipulations, and those qubits, those little quantum particles,
would then evolve backwards, kind of, they called them Benjamin Button, you know, qubits.
Now, why did you think it was going backwards in time?
Because to us, you know, our experience of the world, we know that there's a certain
sequence of things that are more likely to happen versus the other.
For example, we know that little babies grow up to be teenagers, grow up to be middle-aged
people, grow up to eventually, you know, be older, and then eventually we die.
That's, we see that all the time.
We rarely see except in movies with Brad Pitt, you know, old people becoming infants,
you know, and reverting back.
Now it turns out that there's nothing inherently violating the laws of physics of having a
Benjamin Button.
It's just highly, highly unlikely.
And this is one of the things that we, that kind of a consequence that we're going to
try to emphasize about entropy increasing is that it means when entropy is increasing,
certain sequences of events are much more likely to happen than others.
So for example, that sequence when the, when the balls kind of diverge and bounce and become
more disordered, that seems much more likely to you to occur than a sequence where everything
converges.
Even though it's possible to have all those velocities flipped and everything to go back
to its initial part, it's just really, really hard to do that.
And the more particles you have, the more statistically difficult it is to flip everything
and have everything converge back to itself, right?
So when entropy increases, there seems to be a preferred order of things, right?
And we'll make that a quantitative in a little bit.
Now I want to show you now just a contrast, a second snapshot, which is at the very end
of the simulation when we're looking near equilibrium, all right?
So this is going to be, I'm going to do the same game.
So I'm going to play it for 20 seconds like that.
And then, okay, it's bouncing around like a gas.
I'm going to do the same thing, flip all the velocities, play it again.
And there, if you look at it, you won't be able to tell which one, if I just showed you
without telling you the difference, there's no way you can actually even tell statistically
which one is going forwards, which one is going backwards.
In fact, every sequence of events, when we've reached our maximum, when entropy no longer
changes, is equally likely as any other.
There's no preferred direction.
So in a very real sense, in equilibrium, there is no arrow of time, right?
This is the famous, if you may have heard of it in discussions of this topic, but this
is what it boils down to, right?
We don't see that arrow of time in an equilibrium system when entropy is not increasing.
Okay, so let's make this a little quantitative.
So this is kind of, I'm going to give you a little bit of flavor of some of the ways we
think about entropy now, kind of a modern physics context.
We can actually do this, play this game in terms of addresses.
So we have one take, which is, we have a system that visits, you know, four addresses as
it's evolving in time.
We can ask, take two, the system visits those same addresses, but in reverse order.
And we can play this, you know, we can run the simulations many times.
You can ask, how often does that occur versus take, take one occur versus take two?
And we can calculate the probabilities of those, of those sequences.
If we take the ratio of them and then take the log of it, we get a number, which I'm
going to call sigma.
This sigma is actually in some ways our modern, the modern version of what Gibbs was doing.
We actually now interpret that as essentially being an entropy of a sequence.
So before I was talking about an entropy of an ensemble of many copies of a system, here
we're actually defining entropy at the level of a sequence of states which we visit.
And we, we interpret that as basically the amount by which the entropy of the universe
has increased during that visiting the forward sequence of states.
So if the entropy is not increasing, right, the probability of the top is the same as
the bottom, that's one, right, the ratio is one, the log of one is zero, so entropy
is not increased, that's consistent.
And if one is much, much larger, much, much smaller than the other, then entropy is either
large, very much increasing or decreasing during the sequence.
So let me give you a concrete example of this, Humpty Dumpty, right?
Everybody knows Humpty.
This is a sequence of two events, Humpty is intact, Humpty falls, breaks, and according
to the nursery rhyme, all the king's horses and all the king's men could not put Humpty
back together again.
But they, you know, Humpty could have spontaneously reassembled, again, not violating the laws
of physics, highly, highly unlikely.
But we can calculate the ratio, so Humpty falling, Humpty breaking versus Humpty reassembling,
that number is like 10 to the 10 to the 20, that's a very, very large number.
And how does that, you know, by converting it to entropy in the, in using the formula
from the previous slide, that corresponds about, about 10 to the 20 K of entropy released
into the universe.
So the universe has crept up that much closer towards heat death by Humpty falling.
Now here's then the crucial part for the next several, you know, slides of the talk.
That's fine.
So that's kind of, and this is, this is this law, this, this idea that I've shown you applies
just as well for Humpty's subsystems, the living things, you know, it doesn't, it's
a law that applies to subsystems as well as the whole universe.
But how does the entropy of the universe increase when Humpty falls?
And it turns out that this increase is just heat release, right?
It's about 0.2 calories, so not a huge amount of heat, but that's, oh, are we, okay, yeah.
So it's not a huge amount of heat release, but it's some amount of heat has been released
into, into the, into the surroundings.
And why does the universal entropy kind of increase there?
Because when we give energy to the surroundings, the surroundings in some ways can explore
more addresses, and hence the entropy of the universe as a whole is increasing, right?
So that's going to be kind of crucial because this is how typically we pay our due to entropy
as, you know, subsystems of the universe, we tend to release heat, dissipate heat, right?
Okay.
Let me give you a kind of a more down-to-earth example of this, where we are releasing heat.
So this is a circuit, right?
We attach a battery to a circuit, and, you know, a motor to that, to that circuit.
There's some heat being dissipated.
Now what's going on here, right?
Why doesn't there need to be heat dissipated?
Well, the electrons in that wire, if we didn't have the battery attached, would just be more
or less moving in random directions, right?
There would be no preferred direction.
So one kind of sequence of electrons motion to the left would be equally probable to another
sequence of electron motions to the right.
But in when we have a current flowing through the system, one of those sequences becomes
much more likely than the other, right?
There's a preferred direction for the electrons to move through our wire.
Based on the argument that I just showed you, because there's now this preferred sequence
of states which the system visits, the entropy of the universe must increase.
How does it increase?
Well, heat has to be dissipated, right?
Okay, heat is a form of energy.
It's being released into the environment.
Where does that energy come from?
There must be an energy source, right?
That's why you have to have a battery.
So in a weird way, in this kind of inverted thermodynamic perspective, I argue the existence
of the battery by the fact that first there's a symmetry breaking in the motion of the electrons.
But for me, that heat, that we take it for granted, you know, we plug things in, you
know, our electronics are all heated up, things like that.
That kind of very mundane aspect of our daily lives is, in fact, all the objects around
us are paying to the second law of thermodynamics.
So this heat dissipation is essentially the entropy price because those electrons are
moving in a preferred direction.
Because there's this other law of thermodynamics which I have not emphasized, called the first
law, more or less the energy conservation, the power in minus the power out is equal
to the power dissipated, right?
So you have to have more power in than power out in order to have dissipated heat, right?
So you have to have an energy source that's larger than the power that the motor is using.
All right, this is a much more concise way of explaining this.
This is stupid.
Don't do this.
Those electrons in that wire are not going to spontaneously begin moving around in a circle
because there's no power source.
Okay.
So now let's now switch gears a little bit.
So everything that I've talked so far have been addresses that have lived, positions
and directions, positions and directions of electrons or particles and stadium.
The power of this concept is that address could be anything.
It could be a chemical, it could be a step in a chemical cycle.
So this is one of these famous cycles that if you have memories of high school biology,
probably brings you back some nightmares of memorization.
We're not going to go through the details of these kind of metabolic cycles, but note
that whenever the people draw these cycles, always the arrows primarily in one direction.
Why?
Because that's how it happens in our bodies.
It primarily goes in one direction rather than the other.
What does that mean?
It means that we have now a sequence of states more likely to go in one direction than the
other.
Well, we've got to pay our due to the second law.
That means heat has to be dissipated, right?
So there's going to be some necessary entropic price to pay.
So let me give you kind of one very famous example of such a cycle.
So this is the ATP synthase protein.
So this is a protein that essentially synthesizes this energy molecule called ATP, which then
runs a lot of the other biological processes in our body.
And those biological processes degrade that energy molecule back into ATP and phosphate.
And this protein actually spins around.
It's kind of this interesting rotor complex, and it spins around primarily in one direction
so long as you have an imbalance of ions on one side of a membrane versus the other.
In that previous work where you were bouncing the particles around the racetrack, just as
a problem, I would ask what would happen if the racetrack was elliptical and you started
out at one focus of the ellipse?
I don't think there would be chaos.
I have not explored all possible shapes, but I do think if ellipses were, I would have
heard about it in some ways.
Like it requires a particular combinations of shapes.
An array of light that goes through one focus will wind up at the other focus.
So that would argue against the fact that things are going to become disordered inside
that ellipse.
But I think it would not become disordered in the same way.
I may have missed this.
Excuse me.
What is the actual definition of the word entropy?
So used in this talk, and I wanted to be kind of very clear so that there isn't this confusion.
The definition of entropy is this thing here in the blue box.
So you take a system, you divide it up into a state space, addresses which denote the states
of the system.
You have probabilities associated with being at a particular address at a certain time.
You take those probabilities, you multiply them by the logarithm, you add them all up.
That's entropy.
Could you give me a sentence?
A sentence, okay.
So the less mathematical definition would be, it would be a measure of how spread out
that probability is, how disordered the system is.
How disordered the problem is.
How spread out the probability is, which you can interpret in a way as how disordered
the system is.
Because the more spread out you are, the more addresses you live in, the more different
combinations are possible that you see in the system.
So when entropy is zero, everything lives at one address.
When entropy is large, everything is, all addresses are equally likely to occur.
And there's a roof to help for how I...
Exactly.
Right?
So if this was a completely empty city and all of us sitting here in the audience were
in this particular room, and let's say we divided up the city into boxes of the side
of this room, the entropy of our population would be zero.
But then if you guys all went out and equally like uniformly spread out throughout all of
the Cleveland area so that there was a uniform population density of you everywhere, then
your entropy would be maximum.
That would be the most disordered that you could become.
And it's different for different systems.
It's completely different for different systems.
Even in the notion of what an address is, that's why I kind of use this word address.
I mean, in physics we call it phase space, but that's more technical.
But like what I'm claiming is that this idea of addresses works just as well in chemistry
as it does for actual physical positions.
We hope you've been enjoying the Origin Science Scholars Program with Professor Michael Hynchewski.
Professor Hynchewski is the Warren E. Ropp assistant professor in the Department of Physics
at Case Western Reserve University, specializing in theoretical biophysics.
In the second part of our talk, we learned how life is a cascade of energy conversion
and dissipation.
In our final segment, Professor Hynchewski will talk about thermodynamics and the origin
of life.
Now, back to our talk.
So if you take a, you know, again, comparing or going back to this comparison with a battery,
it seems like a very, very different system, right?
So we have, but when I argue it's not that much different, right?
So on one hand we have a system that's kind of, has some preferred direction, right?
Here it's the mode, it's the proteins, you know, preferably going in one direction.
Here it was the electrons preferably going in one direction around the wire.
We have some process that's going on where energy is being utilized.
Here it's the synthesis of ATP.
Here it's the motor turning.
We have some power source.
Here it's a battery.
This was just such a chemical potential energy.
Here that battery is in something, the imbalance of ions on one side of the membrane versus
the other.
And as a necessary sort of consequence of the second law of thermodynamics, heat has to
be released.
There has to be some amount of loss of energy through heat.
Now if you let both of these systems run indefinitely, what would happen?
Well, your battery would become depleted.
It would go to equilibrium, right?
The electrons would stop moving in a preferred direction.
Here if you kept on running this thing forever, eventually this, there would be the equal
number of ions on one side of the membrane versus the other.
And now your protein would begin to have, it would still kind of move, but in random
ways and not with a preferred direction.
So basically your battery would become depleted.
So what happens in nature is, well, in our case, sometimes you switch out the batteries,
maybe it's a rechargeable battery, we can recharge it.
In nature we pump out, we pump those ions continuously back out, right?
How that pumping occurs differs from organism to organism.
And in our case, it's we ingest food and some of that energy is being converted into pumping
out those ions.
Let me give you one kind of very simple example.
This is from a simple bacterium where that pumping occurs because of another protein called
bacteria rhodopsin on the membrane.
And it basically interacts with photons from sunlight and uses that energy from the photons
to basically drive its own cycle of pumping where it pumps those ions back out.
On the other side of the membrane, then you have an imbalance form.
That imbalance then is used to power this ATP synthase to basically produce the ATP,
then that drives other biological processes, right?
And this is one particular example, but in some ways everything that we know about life
is kind of built on kind of these nested cycles, right?
So I'm going to kind of draw it like this, right?
So you might have some fundamental energy source on top.
In this case, it was a photon.
That energy source powered a cycle which created a certain imbalance.
In this case, it was an ion gradient, so more ions on one side of the membrane to the other.
That ion gradient, that imbalance, in turn then powered a different imbalance, right?
Another cycle primarily in one direction of the ATP synthase protein, which then in turns
powers other cycles driven primarily in one direction, all of our metabolic cycles and
cell division, things like this.
At every step, because we are driving things primarily in one direction, so we're breaking
the symmetry, we have to pay our dues to the second law.
So we have to dissipate energy.
We have to, there has to be some heat release at every single step of the process.
So note that because of energy conservation, that means the energy available to all the
lower tiers becomes progressively lower and lower and lower.
We're wasting energy all throughout.
But that waste is not somehow a bad thing.
It's what makes us living, right?
Because without that waste, there wouldn't be this broken symmetry.
We wouldn't have our cycles going around primarily in one direction.
So in fact, you sitting here, all of us, are releasing about 100 watts of power, which
is the old, this is not an audience where I have to remind people of what incandescent
light bulbs are, but to my students I may have to do that.
But I mean, it's about the power of an old fashioned, now old fashioned light bulb.
So that 100 watts of power releasing into the atmosphere, that is in some sense the indicator
of all the non-equilibrium thermodynamic processes going on inside of our bodies, all the biochemical
cycles being primarily driven in one direction.
The net result of that is this 100 watts that we're releasing out into the atmosphere.
So that was one thing, one particular organism.
How does it happen for others?
Well, it turns out that from this kind of cascade, there are two parts that are really
universal to all known life that we see around us.
One is the ion gradient of some kind, the other is ATP as this energy molecule, as this
currency.
What drives the ion gradient is now, in modern living organisms, is not universal anymore.
In some cases here it was a photon, in us it's ingesting of other material.
For bacteria living near hydrothermal vents at the bottom of the ocean, might be some
chemical sources which drive this.
So that becomes that first step in the top part of the cascade is no longer universal.
And of course, the things that we do with ATP differ from different organisms, so that's
no longer universal.
But in some sense, if we're trying to think of this as these nested chemical cycles where
one thing, one imbalance drives another, dissipating heat along the way, ultimately speaking, tracing
this evolutionary history back to the beginning of time, there had to be some fundamental original
imbalance that drove the first glimmers, the first echoes of these kind of cycles forward.
Where things were driven out of an equilibrium state to primarily go into one direction.
And on earth it turns out that if you do this kind of detective work, going forensically,
going back in time, everything that we see around us, that's interesting in some sense.
All things that are not in equilibrium, so air currents, ocean currents, plate tectonics,
all the chemical cycles in life.
And I'm trying to argue that mathematically we can lump all of these things together as
being different versions of the symmetry is broken, where things are going around in currents.
All of these have to be traced back to two fundamental imbalances.
And those are the fact that we're sitting next to a star which is bombarding us with
photons, beautiful source, they're transient on the scale of billions of years of energy
that can drive imbalances.
The other is geological, the fact that there's heat released as the core of the earth kind
of solidifies.
There are radioactive elements in the mantle that are decaying which provides heat.
So kind of geological and solar are really our two sources.
And unsurprisingly, there are two major camps in the origins of life field.
People who think that maybe life emerged in kind of like shallow surface lakes exposed
to sunlight, or people who think that maybe life emerged in deep ocean hydrothermal events.
Where we kind of have this clear division between these two imbalances, then of course
there could be combinations of the two.
You can have shallow surface lakes at Yellowstone or Yellowstone like things where there's some
kind of geothermal aspects to it as well.
But the field kind of breaks into two camps and can we use thermodynamics in some sense
to kind of maybe give an edge to one camp versus the other.
And again, I'm not going to say anything definitive, so this is still an ongoing question and there
are mechanisms by which both origins might work.
But this is actually, this is a kind of complicated graph, but let me walk you through it.
On the horizontal axis, think of this as energy.
And this is energy now measured here in terms of units of milli-electron volts.
It doesn't really matter what the units are, but from low to high.
And here we also see the corresponding energies of photons, where visible photons, the light
from the visible spectrum part of light is about this range of energies.
Above that is the so-called ultraviolet part of the electromagnetic spectrum.
Here in this green range are energies typically associated with biochemical processes, like
for example ATP hydrolysis, things like this.
This is membrane potential energy.
So they live in this kind of infrared range of the spectrum.
On the bottom of this graph are ranges of energies available from certain sources.
So here in blue is water heated up to its critical temperature, beyond which it is no
longer liquid in some sense.
Here is like the energies available from like lava, like molten rock at really high temperatures.
This orange curve is the energy available from the sun, the solar spectrum.
And out here at really much higher things are basically intermittent strikes of lightning.
Now if we think about this kind of range of sources, you can say, well look, molten rock,
water temperatures, high water temperatures, they do kind of give us, get us within the
ballpark of what we see as biochemical energies.
But then if we consider now what I've tried to argue over the last few slides, the fact
that we do have to pay this due to a second law, there has to be dissipation.
So your energy input has to be larger than what you're using the energy for.
We would in principle want to gravitate to like the largest persistent source of energy
available to us.
And that's pretty much the UV part of the spectrum of photons from the sun.
Those are kind of the highest energy sources that are constantly available to us, bombarding
us all the time, and basically have the largest allowance, the largest gap between that amount
of energy and the energy that we're using for biological processes.
And this in some sense is then the thermodynamic argument for why a photon-based, a light-based
origin of life is more plausible.
Again, it's not the final word.
This is definitely not a closed book.
Also a very high energy source, but I would argue probably too intermittent unless somebody's
willing to argue that the early earth atmosphere was lightning all the time.
But photons certainly are there.
And the other part of the thing, which is actually kind of very exciting because the
field has actually really blossomed in the last two years, there is now developments
in chemistry that support this notion.
So in particular, there's been work by Jack Sostak at Harvard, and this is John Southerl
in Cambridge.
And what they were able to finally show in the last few years is a 2015 paper, is now
there is a plausible chemistry that we can associate to making the basic elements of
life.
So lipids, which are the containers, the things that make up cell membranes, amino acids,
which are the bases of our proteins, things like RNA, nucleotides for genetic material.
All of these can be synthesized under what the field considers plausible conditions similar
to early earth, and critically, the catalyst here is ultraviolet light.
So there seems to be in some ways this, again, this is a plausibility argument.
It's not a definitive thing, but we can say, UV light is then, thermodynamically speaking,
the most attractive option for an origin of life.
And now we have a plausible chemistry driven by UV light that can actually lead to the
synthesis of these types of living materials.
Again, is this the actual chemistry that occurred in earlier life?
We will probably never know that for certain, but these are different pieces of the puzzle
that we're putting together.
So we're coming towards the end of the talk.
So what I want to emphasize at the end is, again, we think of chemistry and all these
other things like hurricanes to be really two very, very different things.
What I want to argue is that they're really truly connected to each other in this deep
mathematical way through the notion of entropy, and that all these processes are all connected
through non-equilibrium thermodynamics.
And it's kind of the line of argument that I tried to make tonight is something along
the following.
So if you see a current, if you see this symmetry breaking where a certain sequence of events
is more likely than the reverse, that necessarily implies there has to be some entropy increase
in the universe.
That entropy increase typically occurs through dissipation of heat into the environment.
OK, if heat is dissipated into the environment, what does that mean?
That means it requires a persistent power input, because you're losing energy, so that
energy has to be coming from something, and this is more or less what the kind of conceptual
framework what it boils down to.
What's interesting is this will apply just as well to exoplanets as it does to Earth.
These are universal aspects of what we understand to be laws of physics, and so any mechanism
we think of life arising anywhere else still has to obey these basic thermodynamic ideas.
And I just to kind of wrap up, this particular notion is kind of interwoven cascades of chemical
cycles where you start with like a photon on top and then, you know, dissipate energy
throughout.
I was discussing this in one of my classes, and a colleague, Robin Snyder, was a theoretical
ecologist.
She pointed out this beautiful, beautiful quote from Primo Levy, which summarizes this much,
much better and more eloquently than I could.
This is from a short story of his called Carbon, which I highly recommend.
Just go Google it and read it.
And the way he described it is like this, such as life and inserting itself, a drawing
off to its advantage, a parasitizing of the downward course of energy from its noble solar
form to the degraded one of low temperature heat.
In this downward course, which leads to equilibrium and thus death, life draws a bend and nests
in it.
And I think that beautifully kind of encapsulates in some ways the transient nature of all this,
right?
Because remember, as, you know, we're arguing here that entropy is increasing.
All of this depends on this entropy increase that can only go on for so long, right?
So all these beautiful things that we see, all these beautiful patterns, these complicated
structures, inherent in their physics is, of course, their transience and their eventual
death through equilibrium.
So in some ways, like, you know, we're, you know, it's kind of a mixed blessing.
We have all these beautiful things, but they won't last forever.
Do you see ATP in viruses, in prions?
So viruses basically are just kind of protein capsids incorporating genetic material.
So in order to actually copy themselves, they have to insert themselves into a host like
a bacterium, for example, and they use the machinery, the metabolic machinery of the
host to make the copy.
Yeah.
Well, I mean, the virus in itself just sitting there is not, you know, you know, if there's
no active biochemical cycle there, I wouldn't call that life.
Once it's inserted there, it becomes a component of active biochemical cycles, and then in
some sense, it becomes a part of a living thing.
So do you come down on the metabolism first versus, you know, a nucleic acid or a brain?
Yes.
So part of, okay, so again, this is more kind of, you know, I'm a physicist, not a chemist,
but I think part of what this kind of work that I showed here is arguing is in some sense
the answer to the, you know, which came first, the chicken and the egg is both.
That you can have a chemistry where all the various elements that you need can arise more
or less simultaneously, and that in some sense seems the most plausible case because it's
really hard to think of, you know, containers containing nothing, you know, things that
need to be replicated like genetic material without the mechanisms, you know, the machinery
to replicate them, right?
So in some sense, you kind of would like as, you know, in something as an Occam's razor
kind of most simplest thing, that somehow these, all things more or less arose around
the same time.
Again, whether there could be more complicated mechanisms by which you have one thing existing
right before the other.
The Origins Science Scholars Lectures are presented by Case Western Reserve University's
Institute for the Science of Origins with the assistance of the Segal Lifelong Learning
Program, the College of Arts and Sciences, and MediaVision.
For more information on the Origins Science Scholars Program, including a full video
archive, please visit the Institute's website at origins.case.edu.
