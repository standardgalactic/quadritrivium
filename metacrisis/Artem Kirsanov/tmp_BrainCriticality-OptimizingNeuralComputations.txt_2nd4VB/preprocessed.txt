Understanding the brain in all of its complexity is a difficult challenge.
One of the holy grails of neuroscience is to build an elegant, yet comprehensive theory
describing the general roles of the nervous system, something similar to what a set of
Maxwell equations was to electromagnetism. In this video, we are going to talk about the
critical brain hypothesis, a theory which has been getting a lot of attention and experimental
evidence in recent years. It states that networks of neurons operate near a point of
phase transition. It is a special, critical state, similar to when water molecules coexist
in liquid and gaseous phases. But what does it even mean for the brain to undergo a phase transition?
There are surely no melting and boiling, right? And in what way is this critical point,
whatever it is, important to neural information processing. All of that and more come in right up.
To understand the concept of criticality, let's begin with the notion of phase transitions.
And one such transition that is probably very intuitive to you is when liquid turns into gas.
Consider a pot of water on a hot stove. At first, when the water is below 100 degrees,
the energy provided by the burning stove heats it up, accelerating individual molecules.
At 100 degrees, however, something interesting happens. Even though the heat is still being
pumped into the water, its temperature stays constant. Instead of accelerating individual
water molecules, the energy is now being spent on breaking the bonds between them,
which allows the molecules to break free from the lattice and fly away as a gas.
Notice that at the boiling point, we have seen a qualitative switch from liquid, which is incompressible,
has surface tension and can dissolve substances, to gas, which is compressible and doesn't really
dissolve anything. Even though the individual water molecules stayed the same, the macroscopic
properties of the system as a whole changed drastically. What we have just witnessed is
known as a phase transition. More generally, a phase transition is observed when a system moves
from existing in one well-defined state of organization or phase to another. Phase is
usually characterized by an order parameter, a macroscopic property that somehow quantifies
the organization of the system. For example, in the case of water, order parameter can be
entropy, the degree of disorder, volume, fluidity or surface tension. The phase transition is driven
by changes in control parameter, for example, temperature or pressure. Essentially, you can
think of the control parameter as an independent variable, something that we can freely vary in
an experiment. And the order parameter is a dependent variable, which changes in response to
altering the control parameter and quantifies the state of our system. Notice that if you plot the
order parameter as a function of control parameter, then in the case of water boiling, you'll see a
discontinuity, a sudden jump as we go from liquid to gas, accompanied by the absorbance of latent
energy. Such phase transitions are called discontinuous or first-order transitions.
These are what you normally see in everyday life. However, we will be more interested in another type,
so-called continuous or second-order transitions. As you might have guessed from the name, in this
type of transitions, the change in the order parameter is continuous, which means that you can
smoothly go from one phase to another. Now, the slope of this curve can be very steep, sure,
but it's always continuous. This allows the system to exist in a unique intermediate state,
called the critical point, right at the interface, when the boundaries between the two different
phases are blurred and new properties emerge. This is what makes the critical point so special,
as we'll see further. Water can actually undergo a second-order transition at specific values of
temperature and pressure. This is where latent heat disappears, and water can be continuously
transformed into gas existing in a special state known as supercritical fluid. But to develop intuition
about the properties of critical point, let's focus on a different, much simpler system,
where the second-order transition feels more natural and which actually has a lot of applications
in neuroscience. Meet the Ising model, which was first developed to explain the properties of magnets.
The model consists of a large lattice, where each site can be in one of two states, plus one or minus
one. These sites represent individual particles that the system is made of, characterized by their
spins, which you can think about as this sign of magnetic field generated by each particle.
When all spins are aligned, tiny magnetic fields of individual particles add up, resulting in the
magnetic properties on a macro scale. However, when the spins are pointing in different directions,
individual magnetic fields essentially cancel each other out, and the system has no macro scale
magnetization. Physicists have known for a long time that if you heat up a magnet past a certain
point, called curie temperature, it will suddenly lose its magnetic properties, so there must be
some sort of phase transition at play. Now, there are two types of interactions that govern the
dynamics of the system. First of all, neighboring spins will tend to line up because it is more
energetically favorable. We can write the energy of a single pairwise interaction between the two
sites as being proportional to the negative product of their spins, where the coefficient of
proportionality J is called the coupling constant, and it tells us how strongly the sites are coupled.
As you can see, when the spins are of the same signs, the energy of an interaction is negative.
While for a pair of opposite spins, the value of energy is positive. To find the energy of one
site, all we need to do is to sum the energies of the interactions between its four neighbors,
and summing together the energies for all the sites will give us the energy of the entire system.
Because it will try to minimize its energy, we can expect all these spins to perfectly line up.
So what can prevent this from happening? Notice that so far, we haven't included our control
parameter, temperature, in any of the equations. In reality, however, flipping of spins is a
stochastic process, which is subject to a random thermal movement. More formally,
the distribution of energy values subject to random fluctuations obeys the Boltzmann distribution,
which gets more broad at higher temperatures. But it's not that important to understand
phase transitions, so don't worry about it. Essentially, you can think of temperature as this
stochastic destabilizing force that will induce fluctuations, jostling the spins around and
preventing them from settling down into their most energetically favorable configuration.
The higher the temperature, the more disordered the system would be.
To quantify this, let's introduce an order parameter, for example, the macroscopic magnetization,
which is defined as the average value of spins. When the absolute value of magnetization is big,
it means that a large number of spins are aligned and our system will display magnetic properties.
Let's play with the temperature and see how macroscopic properties of the system change.
At low temperatures, local interactions dominate, and the system exists in this state
when a large portion of spins are lined up. In contrast, if you heat it up, the temperature
related fluctuations take over. All the spins are oriented randomly, and no macroscopic magnetic
properties are observed. Notice that as we turn the temperature knob, there is something that
looks like a very abrupt change in the organization of spins, from order to disorder.
Indeed, it turns out that the Ising model undergoes a continuous second-order phase transition.
In other words, it is possible to catch the system in an intermediate state,
right at the point of critical temperature, where order and disorder are perfectly matched,
and a number of interesting properties emerge. Before we move to the brain,
let's spend a little more time exploring the Ising model.
Similarly to how neurons communicate with each other across the entire brain,
there is a crosstalk between spins in our lattice. Notice that if you flip one spin,
this can make some of the neighboring spins to flip, which in turn affects spins two sites away
and so forth. So eventually, even though only nearest neighbor interactions are explicitly
defined, the information about flipping one spin has the potential to be transmitted across the
entire lattice. In other words, long-range communication emerges from purely local interactions.
A good way to measure this is through a quantity called correlation length.
For example, let's take two lattice sites some distance apart and look at their dynamic correlation,
which is a measure of how coordinated in time their behavior is. More formally,
for sites i and j, we can express the dynamic correlation as follows, where angular brackets
indicate averaging over time. The first term in the parenthesis represents the amount by which
the spin i fluctuates from its average value, and likewise for fluctuations of the spin j.
Notice that in order to maximize the value of dynamic correlation, the spins should fluctuate
in a coordinated fashion for the product of the parenthesis to remain positive.
Let's see what happens to the value of dynamic correlation for a given pair as we change the
control parameter. At low temperatures, the spins don't fluctuate much, so they are stuck
at their average values, yielding a low value of dynamic correlation. At high temperatures,
the spins fluctuate wildly, but they do so in a random, uncoordinated fashion. At one point in
time, the terms in the parenthesis might be of the same sign, while on another occasion,
they might be of different signs, so on average the value of dynamic correlation is low as well.
At the critical temperature, however, nearest neighbor interactions are balanced by the thermal
stochasticity. There is a coexistence of coordination and fluctuation, resulting in the
large value of dynamic correlation, as spins flip up and down in a coordinated way.
So far, we've only looked at the dynamic correlation for a single pair of spins,
and that of course will depend on the distance between them. Let's now plot the value of C
as the function of distance between the spins for different temperatures. Unsurprisingly,
it decreases with distance because we've only incorporated local interactions into the model,
and what's prominent is that the curve corresponding to the critical temperature
decays much more slowly than the two other ones. For example, we might find that it extends up to
20 sites before it falls to near zero. Let's term this distance at which the dynamic correlation
drops to zero as correlation length. Drawing correlation length as a function of temperature
reveals a sharp peak at the critical point. Right there at the intersection of order and
disorder is where large distance communication emerges. If we work to make an analogy, this is
the state in which neurons in networks communicate most effectively through the largest number of
synapses between them. Another important property that emerges at the critical point is that our
system becomes scale-free. To illustrate what this means, take a look at these three magnified
snapshots of the Ising model at three different temperatures, exactly critical, slightly below
and slightly above the critical temperature. Can you guess which is which? Well, this is not so
obvious because they all look kind of similar. But let's zoom out to see the whole lattice.
As we move further and further away, it becomes apparent that the leftmost snapshot corresponds
to a subcritical temperature because everything is ordered. Zooming out on the rightmost snapshot
reveals a great deal of disorder, so this must be the supercritical state. Now, what
would happen as we zoom out when the system is at the edge of the phase transition? No matter how
close or how far away we are, things look very similar. In the ideal case, right at the critical
point, this pattern will continue to infinity. In other words, there is no characteristic scale.
This self-similarity, when the system as a whole resembles some of its parts at any scale,
is a typical property of fractals. Such zooming animations are surely satisfying to look at,
but is there any way we can objectively prove that this is scale-free, while this is not?
To mathematically describe the scaling properties, let's consider the probability
distribution of the size of clusters, that is, connected domains of the same spin.
Here are the examples of a few relatively small clusters and a couple of larger ones.
Let's focus on the critical case and build the intuition for what this function F should look
like. Suppose a dark wizard has shrunken you to a random size so that you have no idea how
tall you are and trapped you in the icing model at the critical temperature. The only way for you
to get out is to correctly guess the probability distribution of cluster sizes. So you grab a
ruler and start walking around, measuring the size of every cluster you see. Let's say on average,
you find out that every hundredth cluster you encounter has the size of 4 square inches,
so you can write down F of 4 equals 1 over 100. Because the ruler has been shrunken with you,
the exact units are irrelevant. What would matter in this case are ratios. So you begin looking for
clusters that are 2 times as large and discover that the probability of observing an 8 square
inches cluster is about 1 over 500, which is 5 times as large compared to the probability of seeing
a 4 square inches one. After repeating the measurements for a few other cluster sizes,
you discover an interesting pattern. Every time you double the size of a cluster,
its probability decreases by a factor of 5. Now, here is the crucial point. Let's imagine
that you were shrunken to a completely different size. For example, what if you were actually 10
times taller? What would you see in this case? Well, remember, because the system is right at
the critical point. It is scale invariant. In other words, no matter how small or big you were,
you would see similar pictures everywhere, and hence similar cluster distributions at any scale.
So this ratio of the probability for observing the cluster of size 2x versus the cluster of
size x would equal to one-fifth at every scale, which means it doesn't depend on x. Of course,
there is nothing special about the factor of 2. We could have just as well looked at any other
ratio of cluster sizes. So let's substitute the number 2 with a factor k. The value of this ratio
will be also different for different factors, because for instance, the probability of observing
a cluster with a size 3x will be different than the probability of seeing a cluster of size 2x.
In general, we can write down the right-hand side to be some function j of k, since as we have already
established, it surely doesn't depend on x. This right there is the mathematical definition
of scale invariance. So whatever the distribution of cluster sizes is, it should satisfy this
constraint. It turns out that the only function for which this equation holds is of the form a
times x to the power of minus gamma, where the number gamma is called the exponent. In that case,
j of k is equal to k to the power of minus gamma. Plugging in our values for cluster sizes
reveals that gamma equals 2.32. This behavior is called a power law,
and it is central for the study of critical phenomena. You can easily see for yourself
that x to the power of minus gamma is indeed scale free. Proving it the other way around that any
scale free function should have this form is a bit more tricky and requires taking a few derivatives.
I won't go through the derivation in this video, but if you're interested, check out the references
in the description. Another way to see why power laws are scale free is to plot a power law function,
for example x to the power of minus 2.3, next to something similar, but which is not power law,
for example an exponential function. Notice that if we look at the graphs at different scales,
the shape of the power law curve stays the same. The only thing that changes are the exact units.
If axis labels were erased, we couldn't distinguish them from each other. However,
the exponential curve looks different at different scales, and hence is not scale free.
Usually, such relations are plotted in double logarithmic coordinates, known as log log plots,
where you take the logarithm of both sides. In log log coordinates, power laws look like
straight lines, and the slope of that line is related to the exponent value gamma.
Importantly, many other characteristics of the system, for example the value of dynamic correlation
we discussed earlier, all follow a power law distribution near a critical point. In fact,
when you see that a lot of things are power law distributed, it often suggests that the system
is near a point of a second order phase transition. Now, after we've developed an intuition for many
important concepts in criticality, let's finally move to the brain.
One of the first pieces of experimental observations, suggesting that the brain might be
operating near a critical point, came in 2003 when John Becks and Dietmar Plants published
their seminal paper titled Neuronal Avalanches in Neocortical Circuits. They grew cultures of
neurons taken from red somatosensory cortex and put them on an 8x8 grid of electrodes to record
spontaneous activity that arises in this network. And what they found was a characteristic pattern
of activity spreading across the network in space and time. Let's take a closer look at the data
collected by a single electrode. It records how extracellular voltage, known as local field
potential, changes with time. If you've seen my video on thetorythms, it is essentially the same
thing. What is usually observed in these cultures of neurons in a dish, however, is that baseline LFP
occasionally shows large deflections following a characteristic shape of a negative spike
superimposed on a positive envelope. This arises when a number of neurons in the
vicinity of electrode activate simultaneously. We are going to treat such waveforms as discrete
events that are either present or absent, simply by finding when the voltage trace
dips below a certain threshold and also keep track of the amplitude of every such event.
Thus, our data will contain both a time point and an amplitude value for every LFP event that
crossed the threshold. Repeating this procedure for all the electrodes gives us the pattern of
spontaneous electrical activity spreading in the network. Notice how periods of quiescence
are interspersed by what looks like chains of activity arising, reverberating in the network
and finally dying out. Such bouts of activity were termed neuronal avalanches because they
resemble cascades of activity propagating in systems like pile of sand or earthquakes.
To define an avalanche more formally, let's break up the entire time course into short beans
and for each bean count the number of electrodes that recorded the LFP event, for example it might
go like this. First there is no activity, then one electrode detects the super threshold event,
in the next time bin another electrode, then two simultaneously, then another one,
then three others at the same time and then activity dies out. This sequence of time beans
with some non-zero activity flanked by at least one time bin of quiescence at either side
is what we are going to call an avalanche and we can describe each avalanche by its duration
as well as its size defined as the number of electrodes that were activated during the cascade.
Note that in the alternative definition size can also take into account the amplitude values.
Surprisingly the authors found that all these features of neuronal avalanches
were distributed according to a power law revealing a prominent straight line in log log plots.
This suggested that the network activity had no characteristic scale and that the brain might
be operating near a point of a second order phase transition. Now all of this happened in 2003
and since then power law described avalanches of activity were demonstrated in awake behaving
animals of many species including worms, zebrafish, monkeys and humans and on a variety of scales
from single neuron recordings to electroencephalography suggesting that criticality can
indeed be a universal phenomenon which describes many aspects in neural systems.
At this point in the video I think a couple of pieces are still missing. First of all I've said
that the critical point is the intermediate stage of phase transition but what are the
distinct phases for the case of the brain? What would be neural analogues to temperature as the
control parameter and magnetization as the order parameter and secondly why operating near a critical
point would be useful to the brain at all? To make the following discussion more intuitive
let's switch from thinking about avalanches in terms of LFP events on the electrode grid
and view the data as activity of individual neurons that either fire an action potential or not.
Such switch is justified because of scale invariance. Remember we are going to see
similar behavior at any scale plus it has been established experimentally that there are indeed
such spike avalanches whose features are power law distributed. In this representation each circle
is an individual neuron rather than a single electrode. Now it becomes easier to interpret
the activity propagation in the network. Whenever a neuron reaches its threshold voltage it sends
a signal to its downstream partners increasing or decreasing their probability of subsequent firing
but because neurons connect to each other in very intricate ways unlike the icing model for
example it is hard to see who is connected to whom. Let's make a slight simplification to this network
and rearrange the neurons into a layered structure so that information will always flow from left
to right. Even though we don't allow any feedback connections or loops to a first approximation
it is a reasonable assumption and importantly this description known as branching model still
allows for avalanches and other critical phenomena to emerge. In the branching model each neuron is
randomly connected to some neurons in the downstream layer and each connection has a transmission
probability associated with it. Upon the spike of a sender neuron the connection will transmit a
spike with a certain probability and if this happens on the next time point the receiver neuron
will activate and send the information further. Additionally each neuron has a very small probability
of spontaneously activating even when it doesn't receive any input. This stochasticity provides
the network with some input drive to work with. As you may have guessed how activities
brands in the network will be mostly controlled by the distribution of transmission probabilities.
To tune this with a single parameter let's introduce a number sigma called a branching ratio
and set the sum of our going transmission probabilities for each neuron to be equal
to sigma. For example if the branching ratio is one this configuration of our going connections
is valid because the sum of probabilities is equal to one and this is not. By the way the branching
ratio doesn't really tell us the exact number of connections. For sigma equal to one a neuron can
have a hundred connections each with a probability of one hundredth or one connection with the
probability of one. The only thing that is constrained is the overall sum of transmission
probabilities. It turns out that this branching ratio is a control parameter for our system
and that a phase transition occurs when sigma is equal to one. To understand why this is the case
notice from the very definition of sigma it follows that it is actually equal to the
average number of descendants that are activated per a single active ancestor neuron.
Let's run the simulation for different values of branching ratio and see what happens.
As you can see when sigma is equal to 0.5 any activity that arises quickly dies out. This is
because on average it takes two upstream neurons to fire in order to activate a single downstream
neuron. In a real brain that would correspond to deep comatose state with very little activity.
Increasing the value of sigma to 2 causes the network to blow up with activity similar to what
you would see during epilepsy. This is because on average single neuron activates two others
which makes the activity amplify with time. Something interesting happens when sigma is equal
to one however. In this configuration one neuron on average activates one downstream descendant
allowing the network activity to be maintained on a relatively constant level without decaying
or amplifying. Now because this is a stochastic process avalanches will eventually die out
but their sizes and durations will be power law distributed. This is why the branching ratio
is a control parameter governing the transition from decaying to amplifying activity. And we can
use the average density of active neurons as the order parameter which is really low for
subcritical values of branching ratio but quickly grows as soon as we hit the critical point.
By the way in real neurons this control parameter which is equal to the average number of neurons
activated by a single upstream neuron is shaped by the balance between excitation and inhibition
which are the two counteracting forces in the brain dynamics. Importantly because this balance of
excitation and inhibition is something that we can control experimentally it can further illustrate
that there is indeed a phase transition at play. Namely adding compounds that block inhibitory
transmission disrupted the power law distribution of avalanches seen in the normal condition leading
to supercritical behavior. Likewise adding compounds that block excitatory synaptic
transmission leads to subcritical dynamics again with the disruption of power laws.
Okay by now the only question left is why would brain even evolve to operate near a critical
point? Is it actually useful? It turns out that at the critical point a lot of brain's capabilities
such as information processing and computational power are maximized. To begin understanding why
this is the case let's think of information transmission as a guessing game. Suppose someone
provides an input to the first layer of our branching model by activating a random set of neurons
and our job is to correctly guess the number of neurons that were activated
by observing only the output the activity of the rightmost layer.
For low values of branching ratio no matter how many neurons were activated in the beginning
the activity quickly vanishes before reaching the output layer. We are completely ignorant about
the input because there is no activity trace left. For high values of sigma activity will be
amplified causing the output layer to be fully activated even for the weakest input which makes
the guessing again very difficult. At the critical case when sigma is equal to 1 the connection
strengths are tuned to the optimal intermediate value because one neuron on average activates
only one descendant. Activity of the output layer has a high probability of resembling the
activity of the input layer in terms of the number of active units. In other words observing the
output successfully reduces our uncertainty about the input. If we introduce some measure of
information transmission that will tell us how accurately we can make this guess this quantity
will have a sharp peak when the branching ratio is 1. Similar to how dynamic correlation
peaks at the critical temperature in the Ising model this on a very simplified level
is what it means to optimize information transfer. Of course we have really just
scratched the surface of the topic of critical point and its relevance to neuroscience. I didn't
really talk about the concept of universality and the beautiful relationship between the
exponents. The idea of course a criticality and how the brain actually maintains this right
balance in the first place. But if you are interested to know more about criticality and
phase transitions in the neuroscience context I really encourage you to check out a wonderful book
published in 2022 titled The Cortex and the Critical Point by John Beggs who is one of the
pioneers of this field. The book is written in a really accessible language and introduces all
the concepts from the ground up. Before we move to the summary I have one important message.
In this video we have seen a few examples of how analyzing neural data can help us better
understand the brain. But have you ever wondered how computers can be programmed to process and
analyze such complex datasets? If you're interested in the fascinating world of computer science
you're definitely going to love our today's sponsor brilliant.org. Brilliant is an innovative
educational platform that takes a hands-on approach to learning. Whether you are a student or a
professional it is the best way to learn and excel in STEM fields. Brilliant's courses are designed
to be engaging and interactive packed with stunning demonstrations and exercises to help you gain
intuition and develop problem-solving skills in the context of real-world applications.
Brilliant has thousands of lessons for everyone from foundational math and logic to AI and quantum
mechanics with new ones being added monthly. If you're interested in getting started with
computational neuroscience take a look at the course on algorithm fundamentals introducing
essential array algorithms such as sorting and binary search that underlie the majority of
modern computing. And from there you can move on to programming with Python which is what I've
actually used to run simulations you've seen in this video. Don't hesitate to start your learning
journey today. Go to brilliant.org slash rtmkrsanov to get a 30-day free trial of everything Brilliant
has to offer. Additionally the first 200 people to use this link will get 20% off the premium subscription.
All right let's recap. In this video we have explored the properties that arise when the system
undergoes a second-order phase transition. Namely it becomes scale-free and there is an emergence
of long-distance communication between the components. In the brain such phase transition is
governed by the fine balance between exaltation and inhibition and by hovering near a critical
point our brains optimize information processing. Of course this area is still very new so there
is a lot of things we don't know. But it is now apparent that the applications of criticality research
are far-reaching and exciting from understanding the general principles of brain function to
developing new treatments and therapies for neurological disorders. As our understanding
of criticality in neural networks continues to grow it is sure to be an exciting
field of exploration and discovering in the years to come. If you liked the video share it with
the friends and colleagues, subscribe to the channel if you haven't already, press like button
and consider supporting me on Patreon. Stay tuned for more interesting topics coming up.
Goodbye and thank you for the interest in the brain.
