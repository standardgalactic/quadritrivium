dips below a certain threshold and also keep track of the amplitude of every such event.
Thus, our data will contain both a time point and an amplitude value for every LFP event that
crossed the threshold. Repeating this procedure for all the electrodes gives us the pattern of
spontaneous electrical activity spreading in the network. Notice how periods of quiescence
are interspersed by what looks like chains of activity arising, reverberating in the network
and finally dying out. Such bouts of activity were termed neuronal avalanches because they
resemble cascades of activity propagating in systems like pile of sand or earthquakes.
To define an avalanche more formally, let's break up the entire time course into short beans
and for each bean count the number of electrodes that recorded the LFP event, for example it might
go like this. First there is no activity, then one electrode detects the super threshold event,
in the next time bin another electrode, then two simultaneously, then another one,
then three others at the same time and then activity dies out. This sequence of time beans
with some non-zero activity flanked by at least one time bin of quiescence at either side
is what we are going to call an avalanche and we can describe each avalanche by its duration
as well as its size defined as the number of electrodes that were activated during the cascade.
Note that in the alternative definition size can also take into account the amplitude values.
Surprisingly the authors found that all these features of neuronal avalanches
were distributed according to a power law revealing a prominent straight line in log log plots.
This suggested that the network activity had no characteristic scale and that the brain might
be operating near a point of a second order phase transition. Now all of this happened in 2003
and since then power law described avalanches of activity were demonstrated in awake behaving
animals of many species including worms, zebrafish, monkeys and humans and on a variety of scales
from single neuron recordings to electroencephalography suggesting that criticality can
indeed be a universal phenomenon which describes many aspects in neural systems.
At this point in the video I think a couple of pieces are still missing. First of all I've said
that the critical point is the intermediate stage of phase transition but what are the
distinct phases for the case of the brain? What would be neural analogues to temperature as the
control parameter and magnetization as the order parameter and secondly why operating near a critical
point would be useful to the brain at all? To make the following discussion more intuitive
let's switch from thinking about avalanches in terms of LFP events on the electrode grid
and view the data as activity of individual neurons that either fire an action potential or not.
Such switch is justified because of scale invariance. Remember we are going to see
similar behavior at any scale plus it has been established experimentally that there are indeed
such spike avalanches whose features are power law distributed. In this representation each circle
is an individual neuron rather than a single electrode. Now it becomes easier to interpret
the activity propagation in the network. Whenever a neuron reaches its threshold voltage it sends
a signal to its downstream partners increasing or decreasing their probability of subsequent firing
but because neurons connect to each other in very intricate ways unlike the icing model for
example it is hard to see who is connected to whom. Let's make a slight simplification to this network
and rearrange the neurons into a layered structure so that information will always flow from left
to right. Even though we don't allow any feedback connections or loops to a first approximation
it is a reasonable assumption and importantly this description known as branching model still
allows for avalanches and other critical phenomena to emerge. In the branching model each neuron is
randomly connected to some neurons in the downstream layer and each connection has a transmission
probability associated with it. Upon the spike of a sender neuron the connection will transmit a
spike with a certain probability and if this happens on the next time point the receiver neuron
will activate and send the information further. Additionally each neuron has a very small probability
of spontaneously activating even when it doesn't receive any input. This stochasticity provides
the network with some input drive to work with. As you may have guessed how activities
brands in the network will be mostly controlled by the distribution of transmission probabilities.
To tune this with a single parameter let's introduce a number sigma called a branching ratio
and set the sum of our going transmission probabilities for each neuron to be equal
to sigma. For example if the branching ratio is one this configuration of our going connections
is valid because the sum of probabilities is equal to one and this is not. By the way the branching
ratio doesn't really tell us the exact number of connections. For sigma equal to one a neuron can
have a hundred connections each with a probability of one hundredth or one connection with the
probability of one. The only thing that is constrained is the overall sum of transmission
probabilities. It turns out that this branching ratio is a control parameter for our system
and that a phase transition occurs when sigma is equal to one. To understand why this is the case
notice from the very definition of sigma it follows that it is actually equal to the
average number of descendants that are activated per a single active ancestor neuron.
Let's run the simulation for different values of branching ratio and see what happens.
As you can see when sigma is equal to 0.5 any activity that arises quickly dies out. This is
because on average it takes two upstream neurons to fire in order to activate a single downstream
neuron. In a real brain that would correspond to deep comatose state with very little activity.
Increasing the value of sigma to 2 causes the network to blow up with activity similar to what
you would see during epilepsy. This is because on average single neuron activates two others
which makes the activity amplify with time. Something interesting happens when sigma is equal
to one however. In this configuration one neuron on average activates one downstream descendant
allowing the network activity to be maintained on a relatively constant level without decaying
or amplifying. Now because this is a stochastic process avalanches will eventually die out
but their sizes and durations will be power law distributed. This is why the branching ratio
is a control parameter governing the transition from decaying to amplifying activity. And we can
use the average density of active neurons as the order parameter which is really low for
subcritical values of branching ratio but quickly grows as soon as we hit the critical point.
By the way in real neurons this control parameter which is equal to the average number of neurons
activated by a single upstream neuron is shaped by the balance between excitation and inhibition
which are the two counteracting forces in the brain dynamics. Importantly because this balance of
excitation and inhibition is something that we can control experimentally it can further illustrate
that there is indeed a phase transition at play. Namely adding compounds that block inhibitory
transmission disrupted the power law distribution of avalanches seen in the normal condition leading
to supercritical behavior. Likewise adding compounds that block excitatory synaptic
transmission leads to subcritical dynamics again with the disruption of power laws.
Okay by now the only question left is why would brain even evolve to operate near a critical
point? Is it actually useful? It turns out that at the critical point a lot of brain's capabilities
such as information processing and computational power are maximized. To begin understanding why
this is the case let's think of information transmission as a guessing game. Suppose someone
provides an input to the first layer of our branching model by activating a random set of neurons
and our job is to correctly guess the number of neurons that were activated
by observing only the output the activity of the rightmost layer.
For low values of branching ratio no matter how many neurons were activated in the beginning
the activity quickly vanishes before reaching the output layer. We are completely ignorant about
the input because there is no activity trace left. For high values of sigma activity will be
amplified causing the output layer to be fully activated even for the weakest input which makes
the guessing again very difficult. At the critical case when sigma is equal to 1 the connection
strengths are tuned to the optimal intermediate value because one neuron on average activates
only one descendant. Activity of the output layer has a high probability of resembling the
activity of the input layer in terms of the number of active units. In other words observing the
output successfully reduces our uncertainty about the input. If we introduce some measure of
information transmission that will tell us how accurately we can make this guess this quantity
will have a sharp peak when the branching ratio is 1. Similar to how dynamic correlation
peaks at the critical temperature in the Ising model this on a very simplified level
is what it means to optimize information transfer. Of course we have really just
scratched the surface of the topic of critical point and its relevance to neuroscience. I didn't
really talk about the concept of universality and the beautiful relationship between the
exponents. The idea of course a criticality and how the brain actually maintains this right
balance in the first place. But if you are interested to know more about criticality and
phase transitions in the neuroscience context I really encourage you to check out a wonderful book
published in 2022 titled The Cortex and the Critical Point by John Beggs who is one of the
pioneers of this field. The book is written in a really accessible language and introduces all
the concepts from the ground up. Before we move to the summary I have one important message.
In this video we have seen a few examples of how analyzing neural data can help us better
understand the brain. But have you ever wondered how computers can be programmed to process and
analyze such complex datasets? If you're interested in the fascinating world of computer science
you're definitely going to love our today's sponsor brilliant.org. Brilliant is an innovative
educational platform that takes a hands-on approach to learning. Whether you are a student or a
professional it is the best way to learn and excel in STEM fields. Brilliant's courses are designed
to be engaging and interactive packed with stunning demonstrations and exercises to help you gain
intuition and develop problem-solving skills in the context of real-world applications.
Brilliant has thousands of lessons for everyone from foundational math and logic to AI and quantum
mechanics with new ones being added monthly. If you're interested in getting started with
computational neuroscience take a look at the course on algorithm fundamentals introducing
essential array algorithms such as sorting and binary search that underlie the majority of
modern computing. And from there you can move on to programming with Python which is what I've
actually used to run simulations you've seen in this video. Don't hesitate to start your learning
journey today. Go to brilliant.org slash rtmkrsanov to get a 30-day free trial of everything Brilliant
has to offer. Additionally the first 200 people to use this link will get 20% off the premium subscription.
All right let's recap. In this video we have explored the properties that arise when the system
undergoes a second-order phase transition. Namely it becomes scale-free and there is an emergence
of long-distance communication between the components. In the brain such phase transition is
governed by the fine balance between exaltation and inhibition and by hovering near a critical
point our brains optimize information processing. Of course this area is still very new so there
is a lot of things we don't know. But it is now apparent that the applications of criticality research
are far-reaching and exciting from understanding the general principles of brain function to
developing new treatments and therapies for neurological disorders. As our understanding
of criticality in neural networks continues to grow it is sure to be an exciting
field of exploration and discovering in the years to come. If you liked the video share it with
the friends and colleagues, subscribe to the channel if you haven't already, press like button
and consider supporting me on Patreon. Stay tuned for more interesting topics coming up.
Goodbye and thank you for the interest in the brain.
