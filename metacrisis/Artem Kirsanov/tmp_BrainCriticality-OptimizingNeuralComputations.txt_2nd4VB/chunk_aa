Understanding the brain in all of its complexity is a difficult challenge.
One of the holy grails of neuroscience is to build an elegant, yet comprehensive theory
describing the general roles of the nervous system, something similar to what a set of
Maxwell equations was to electromagnetism. In this video, we are going to talk about the
critical brain hypothesis, a theory which has been getting a lot of attention and experimental
evidence in recent years. It states that networks of neurons operate near a point of
phase transition. It is a special, critical state, similar to when water molecules coexist
in liquid and gaseous phases. But what does it even mean for the brain to undergo a phase transition?
There are surely no melting and boiling, right? And in what way is this critical point,
whatever it is, important to neural information processing. All of that and more come in right up.
To understand the concept of criticality, let's begin with the notion of phase transitions.
And one such transition that is probably very intuitive to you is when liquid turns into gas.
Consider a pot of water on a hot stove. At first, when the water is below 100 degrees,
the energy provided by the burning stove heats it up, accelerating individual molecules.
At 100 degrees, however, something interesting happens. Even though the heat is still being
pumped into the water, its temperature stays constant. Instead of accelerating individual
water molecules, the energy is now being spent on breaking the bonds between them,
which allows the molecules to break free from the lattice and fly away as a gas.
Notice that at the boiling point, we have seen a qualitative switch from liquid, which is incompressible,
has surface tension and can dissolve substances, to gas, which is compressible and doesn't really
dissolve anything. Even though the individual water molecules stayed the same, the macroscopic
properties of the system as a whole changed drastically. What we have just witnessed is
known as a phase transition. More generally, a phase transition is observed when a system moves
from existing in one well-defined state of organization or phase to another. Phase is
usually characterized by an order parameter, a macroscopic property that somehow quantifies
the organization of the system. For example, in the case of water, order parameter can be
entropy, the degree of disorder, volume, fluidity or surface tension. The phase transition is driven
by changes in control parameter, for example, temperature or pressure. Essentially, you can
think of the control parameter as an independent variable, something that we can freely vary in
an experiment. And the order parameter is a dependent variable, which changes in response to
altering the control parameter and quantifies the state of our system. Notice that if you plot the
order parameter as a function of control parameter, then in the case of water boiling, you'll see a
discontinuity, a sudden jump as we go from liquid to gas, accompanied by the absorbance of latent
energy. Such phase transitions are called discontinuous or first-order transitions.
These are what you normally see in everyday life. However, we will be more interested in another type,
so-called continuous or second-order transitions. As you might have guessed from the name, in this
type of transitions, the change in the order parameter is continuous, which means that you can
smoothly go from one phase to another. Now, the slope of this curve can be very steep, sure,
but it's always continuous. This allows the system to exist in a unique intermediate state,
called the critical point, right at the interface, when the boundaries between the two different
phases are blurred and new properties emerge. This is what makes the critical point so special,
as we'll see further. Water can actually undergo a second-order transition at specific values of
temperature and pressure. This is where latent heat disappears, and water can be continuously
transformed into gas existing in a special state known as supercritical fluid. But to develop intuition
about the properties of critical point, let's focus on a different, much simpler system,
where the second-order transition feels more natural and which actually has a lot of applications
in neuroscience. Meet the Ising model, which was first developed to explain the properties of magnets.
The model consists of a large lattice, where each site can be in one of two states, plus one or minus
one. These sites represent individual particles that the system is made of, characterized by their
spins, which you can think about as this sign of magnetic field generated by each particle.
When all spins are aligned, tiny magnetic fields of individual particles add up, resulting in the
magnetic properties on a macro scale. However, when the spins are pointing in different directions,
individual magnetic fields essentially cancel each other out, and the system has no macro scale
magnetization. Physicists have known for a long time that if you heat up a magnet past a certain
point, called curie temperature, it will suddenly lose its magnetic properties, so there must be
some sort of phase transition at play. Now, there are two types of interactions that govern the
dynamics of the system. First of all, neighboring spins will tend to line up because it is more
energetically favorable. We can write the energy of a single pairwise interaction between the two
sites as being proportional to the negative product of their spins, where the coefficient of
proportionality J is called the coupling constant, and it tells us how strongly the sites are coupled.
As you can see, when the spins are of the same signs, the energy of an interaction is negative.
While for a pair of opposite spins, the value of energy is positive. To find the energy of one
site, all we need to do is to sum the energies of the interactions between its four neighbors,
and summing together the energies for all the sites will give us the energy of the entire system.
Because it will try to minimize its energy, we can expect all these spins to perfectly line up.
So what can prevent this from happening? Notice that so far, we haven't included our control
parameter, temperature, in any of the equations. In reality, however, flipping of spins is a
stochastic process, which is subject to a random thermal movement. More formally,
the distribution of energy values subject to random fluctuations obeys the Boltzmann distribution,
which gets more broad at higher temperatures. But it's not that important to understand
phase transitions, so don't worry about it. Essentially, you can think of temperature as this
stochastic destabilizing force that will induce fluctuations, jostling the spins around and
preventing them from settling down into their most energetically favorable configuration.
The higher the temperature, the more disordered the system would be.
To quantify this, let's introduce an order parameter, for example, the macroscopic magnetization,
which is defined as the average value of spins. When the absolute value of magnetization is big,
it means that a large number of spins are aligned and our system will display magnetic properties.
Let's play with the temperature and see how macroscopic properties of the system change.
At low temperatures, local interactions dominate, and the system exists in this state
when a large portion of spins are lined up. In contrast, if you heat it up, the temperature
related fluctuations take over. All the spins are oriented randomly, and no macroscopic magnetic
properties are observed. Notice that as we turn the temperature knob, there is something that
looks like a very abrupt change in the organization of spins, from order to disorder.
Indeed, it turns out that the Ising model undergoes a continuous second-order phase transition.
In other words, it is possible to catch the system in an intermediate state,
right at the point of critical temperature, where order and disorder are perfectly matched,
and a number of interesting properties emerge. Before we move to the brain,
let's spend a little more time exploring the Ising model.
Similarly to how neurons communicate with each other across the entire brain,
there is a crosstalk between spins in our lattice. Notice that if you flip one spin,
this can make some of the neighboring spins to flip, which in turn affects spins two sites away
and so forth. So eventually, even though only nearest neighbor interactions are explicitly
defined, the information about flipping one spin has the potential to be transmitted across the
entire lattice. In other words, long-range communication emerges from purely local interactions.
A good way to measure this is through a quantity called correlation length.
For example, let's take two lattice sites some distance apart and look at their dynamic correlation,
which is a measure of how coordinated in time their behavior is. More formally,
for sites i and j, we can express the dynamic correlation as follows, where angular brackets
indicate averaging over time. The first term in the parenthesis represents the amount by which
the spin i fluctuates from its average value, and likewise for fluctuations of the spin j.
Notice that in order to maximize the value of dynamic correlation, the spins should fluctuate
in a coordinated fashion for the product of the parenthesis to remain positive.
Let's see what happens to the value of dynamic correlation for a given pair as we change the
control parameter. At low temperatures, the spins don't fluctuate much, so they are stuck
at their average values, yielding a low value of dynamic correlation. At high temperatures,
the spins fluctuate wildly, but they do so in a random, uncoordinated fashion. At one point in
time, the terms in the parenthesis might be of the same sign, while on another occasion,
they might be of different signs, so on average the value of dynamic correlation is low as well.
At the critical temperature, however, nearest neighbor interactions are balanced by the thermal
stochasticity. There is a coexistence of coordination and fluctuation, resulting in the
large value of dynamic correlation, as spins flip up and down in a coordinated way.
So far, we've only looked at the dynamic correlation for a single pair of spins,
and that of course will depend on the distance between them. Let's now plot the value of C
as the function of distance between the spins for different temperatures. Unsurprisingly,
it decreases with distance because we've only incorporated local interactions into the model,
and what's prominent is that the curve corresponding to the critical temperature
decays much more slowly than the two other ones. For example, we might find that it extends up to
20 sites before it falls to near zero. Let's term this distance at which the dynamic correlation
drops to zero as correlation length. Drawing correlation length as a function of temperature
reveals a sharp peak at the critical point. Right there at the intersection of order and
disorder is where large distance communication emerges. If we work to make an analogy, this is
the state in which neurons in networks communicate most effectively through the largest number of
synapses between them. Another important property that emerges at the critical point is that our
system becomes scale-free. To illustrate what this means, take a look at these three magnified
snapshots of the Ising model at three different temperatures, exactly critical, slightly below
and slightly above the critical temperature. Can you guess which is which? Well, this is not so
obvious because they all look kind of similar. But let's zoom out to see the whole lattice.
As we move further and further away, it becomes apparent that the leftmost snapshot corresponds
to a subcritical temperature because everything is ordered. Zooming out on the rightmost snapshot
reveals a great deal of disorder, so this must be the supercritical state. Now, what
would happen as we zoom out when the system is at the edge of the phase transition? No matter how
close or how far away we are, things look very similar. In the ideal case, right at the critical
point, this pattern will continue to infinity. In other words, there is no characteristic scale.
This self-similarity, when the system as a whole resembles some of its parts at any scale,
is a typical property of fractals. Such zooming animations are surely satisfying to look at,
but is there any way we can objectively prove that this is scale-free, while this is not?
To mathematically describe the scaling properties, let's consider the probability
distribution of the size of clusters, that is, connected domains of the same spin.
Here are the examples of a few relatively small clusters and a couple of larger ones.
Let's focus on the critical case and build the intuition for what this function F should look
like. Suppose a dark wizard has shrunken you to a random size so that you have no idea how
tall you are and trapped you in the icing model at the critical temperature. The only way for you
to get out is to correctly guess the probability distribution of cluster sizes. So you grab a
ruler and start walking around, measuring the size of every cluster you see. Let's say on average,
you find out that every hundredth cluster you encounter has the size of 4 square inches,
so you can write down F of 4 equals 1 over 100. Because the ruler has been shrunken with you,
the exact units are irrelevant. What would matter in this case are ratios. So you begin looking for
clusters that are 2 times as large and discover that the probability of observing an 8 square
inches cluster is about 1 over 500, which is 5 times as large compared to the probability of seeing
a 4 square inches one. After repeating the measurements for a few other cluster sizes,
you discover an interesting pattern. Every time you double the size of a cluster,
its probability decreases by a factor of 5. Now, here is the crucial point. Let's imagine
that you were shrunken to a completely different size. For example, what if you were actually 10
times taller? What would you see in this case? Well, remember, because the system is right at
the critical point. It is scale invariant. In other words, no matter how small or big you were,
you would see similar pictures everywhere, and hence similar cluster distributions at any scale.
So this ratio of the probability for observing the cluster of size 2x versus the cluster of
size x would equal to one-fifth at every scale, which means it doesn't depend on x. Of course,
there is nothing special about the factor of 2. We could have just as well looked at any other
ratio of cluster sizes. So let's substitute the number 2 with a factor k. The value of this ratio
will be also different for different factors, because for instance, the probability of observing
a cluster with a size 3x will be different than the probability of seeing a cluster of size 2x.
In general, we can write down the right-hand side to be some function j of k, since as we have already
established, it surely doesn't depend on x. This right there is the mathematical definition
of scale invariance. So whatever the distribution of cluster sizes is, it should satisfy this
constraint. It turns out that the only function for which this equation holds is of the form a
times x to the power of minus gamma, where the number gamma is called the exponent. In that case,
j of k is equal to k to the power of minus gamma. Plugging in our values for cluster sizes
reveals that gamma equals 2.32. This behavior is called a power law,
and it is central for the study of critical phenomena. You can easily see for yourself
that x to the power of minus gamma is indeed scale free. Proving it the other way around that any
scale free function should have this form is a bit more tricky and requires taking a few derivatives.
I won't go through the derivation in this video, but if you're interested, check out the references
in the description. Another way to see why power laws are scale free is to plot a power law function,
for example x to the power of minus 2.3, next to something similar, but which is not power law,
for example an exponential function. Notice that if we look at the graphs at different scales,
the shape of the power law curve stays the same. The only thing that changes are the exact units.
If axis labels were erased, we couldn't distinguish them from each other. However,
the exponential curve looks different at different scales, and hence is not scale free.
Usually, such relations are plotted in double logarithmic coordinates, known as log log plots,
where you take the logarithm of both sides. In log log coordinates, power laws look like
straight lines, and the slope of that line is related to the exponent value gamma.
Importantly, many other characteristics of the system, for example the value of dynamic correlation
we discussed earlier, all follow a power law distribution near a critical point. In fact,
when you see that a lot of things are power law distributed, it often suggests that the system
is near a point of a second order phase transition. Now, after we've developed an intuition for many
important concepts in criticality, let's finally move to the brain.
One of the first pieces of experimental observations, suggesting that the brain might be
operating near a critical point, came in 2003 when John Becks and Dietmar Plants published
their seminal paper titled Neuronal Avalanches in Neocortical Circuits. They grew cultures of
neurons taken from red somatosensory cortex and put them on an 8x8 grid of electrodes to record
spontaneous activity that arises in this network. And what they found was a characteristic pattern
of activity spreading across the network in space and time. Let's take a closer look at the data
collected by a single electrode. It records how extracellular voltage, known as local field
potential, changes with time. If you've seen my video on thetorythms, it is essentially the same
thing. What is usually observed in these cultures of neurons in a dish, however, is that baseline LFP
occasionally shows large deflections following a characteristic shape of a negative spike
superimposed on a positive envelope. This arises when a number of neurons in the
vicinity of electrode activate simultaneously. We are going to treat such waveforms as discrete
events that are either present or absent, simply by finding when the voltage trace
