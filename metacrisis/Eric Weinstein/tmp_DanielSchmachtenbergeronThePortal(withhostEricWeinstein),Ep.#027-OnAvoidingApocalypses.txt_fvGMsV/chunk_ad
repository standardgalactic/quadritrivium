but then we come up with some new chemical that we say is less toxic just because the
long-term studies on its toxicity haven't come out yet and um mostly they don't look as visible
as thalidomide this is the thing we sometimes i say this as um we keep searching for our own
blind spot so that we can do business there right and so what it is that we can't measure
very often becomes a place to do business because almost everything produces negative
externalities and so it has to be disguised now this is a concept i wanted to bring up in terms
of what you were saying about your brother and so there's this idea of a multipolar trap which is
some scenario where some agent in the system whether the agent is a person or a nation or
a tribe or a corporation does something that is really bad for the whole over the long term
but it's actually really advantageous to them over the near term and if they do that they will get so
far ahead of and use that power against everybody else that now everybody else has to race to do
that thing and you get a race to the cliff so an arms race is a classic example right we really
don't want a world with AI weapons it's a much less good world with AI weapons but we would have
to ensure that nobody makes it we'd have to make an agreement where if anyone doesn't join the
agreement and says no we're going to make the AI weapon then everybody has to not only make them
but also make the countermeasures and race to make them faster and even if we make the agreement
we're pretty sure the other guy is secretly defecting on the agreement in the basement
so we make the agreement while secretly defecting on it in the basement while spying on them and
trying to confuse their spies and it's like how the fuck do we get out of that and the same is
true of the tragedy of the commons if I figure out how to externalize some of my cost and so my
margins go up all of my competitors have to figure out how to externalize some cost or we have to
try and figure out how to make some law to bind it but it's very hard to make laws to bind these
things and so the same is true with ethical issues right if I am perfectly ethical I'm going to lose
in politics because I won't be able to get anybody to support me whatever so I make certain kinds
of compromises and so one thing I would look at is if we want to look at catastrophic risk or at
large we can look at multipolar traps drive as a general game theoretic phenomena driving lots of
different ways that catastrophic risk can happen so one thing game B has to do is solve multipolar
traps so in other words look at the universal class of such things rather than begin then
getting bewitched by any particular instance correct but we have to solve for the class of
multipolar traps are at large because it's not just we don't want an arms race on AI weapons we
also don't want an arms race on bio weapons or nanotech weapons I think it's well phrased does
it buy us anything does it buy us anything no sorry I want to be clear not buy us B I A S but
buy B U Y us does it buy us anything in the sense of does that avoid catastrophic risk by itself
or can we actually do it formulating deciding to work over the class yeah of really dangerous
to use your language multipolar traps does that actually point us to some game
theoretic thing that all all of our friends who did the game theory of the Cold War might have missed
I'm not aware of any major game theoretic advance in what you're calling multipolar traps
it's not a major game theoretic advance what we're saying is that
we have that phenomena creates a lot of different race to the bottom type scenarios
but with exponentially more power the race to the bottoms are much deeper bottoms so in the
past we've had boom and bust cycles associated with those right and so everybody starts doing
the pollution thing until you get a bust that's associated which creates some new market advantage
to clean the pollution up at a certain point because people are willing to pay for that and so
you'll get these kind of boom and bust cycles that's kind of the best that a market can give you
with regard to multipolar traps but the level of bust that we get with exponential technologies
and this many people basically is unviable and so we've never figured out how to solve for that
class and I'm saying that that's one of the things you're asking is there a game b that I
believe in it would have to solve for a number of things it would have to actually remove rival
risk dynamics which is which would solve for multipolar traps because multipolar traps are a
situation where the well-being of each agent can be optimized independently of and even at the
expense of the other agents in the comments as long as that's the case we have an incentive to do
fucked up stuff with increasing power that is one way of thinking about an underlying generator
of all the catastrophic risks we face. Well let's assume that I buy in because quite honestly this
has been always been my problem with game b is that I look at the game theory and I look at our
history in terms of what has brought us to this point and I would say that if you believe any
version of the theory of natural and sexual selection you'd have to say that we are the
product of an arms race and the idea that we would be wise enough to stop the arms race
when I can hear in my mind's ear all these people saying wow chicken little's at it again
everything's great you know Stephen Pinker tells us that we were in a much more peaceful world
uh we won the cold war what are you guys going on about I mean things are a little bit screwy
in politics and suddenly it's all gloom and doom I I just I hear that voice and then I hear this
other voice which is this optimism about maybe we could become wise maybe we could become the people
wise enough to have synthetic biology and nuclear weapons and instant communication and
data warfare and all these things and survive and thrive and I don't see how help me out where's
the hope here I I think that you and I probably don't need to talk about this much but that
what I would call a naive techno optimism is bananas cherry picking data is easy from a big
data set well and the great negative externality is potential violence as long as you don't see
potential violence as having increased um now I you know I I then you don't see the problem
one argument you could make is that sub lethal technology
has increased our ability to shoot beanbags at protesters means that you don't actually have
to kill protesters you know that there there are some weird arguments but what you you never lost
the ability to kill them you just may have not outfitted riot police with lethal technology
in their first in the first wave of things that you send against an unruly crowd and
the most awesome thing like about the current system is we don't even have to deal with protesters
with tear gas or beanbags or whatever mostly because mostly um addiction and student debt
and information overwhelm and those things deal with the people adequately um so they
they don't actually understand enough or care enough or have the capacity to organize very
meaningfully we just legalize weed and make porn free and everyone's demotivated those are a
couple examples okay um I think we need to talk about you make it sound like a good thing
like maybe maybe it's keeping this no we're going critical no no no um I'm saying that
every dominant system has an intrinsic propensity to figure out how to stay being
the dominant system which means that it has a intrinsic propensity to get better at being able
to deal with dissent and um we can look at different kinds of conflict theory you and
peter we're talking about gerardian conflict we can look at this as kind of a marxist class
type conflict but I think it's it's deeper than that the system that is self-perpetuating is inanimate
the system which is self-perpetuating is not itself animate or sentient but it is auto poetic
but it is auto poetic and this is the fascinating thing to get is it's like
let's take let's take nick bostrom's paperclip maximizer as an analogy so I know you know this
but for the people who don't um when looking at concerning ai risk scenarios one of them is this
you know kind of funny idea of a paperclip maximizer paperclip is representative of any
widget so make an ai that basically can do two things it can optimize the production of something
here a paperclip and so it can use its intelligence to do that so it can make more efficient supply
chains whatever and it can use its intelligence to increase its own intelligence so you'll get
a exponential curve on intelligence which also then means an exponential curve on its capacity
to optimize whatever narrow metric it's optimizing and so of course after it just makes increases
in efficiency which are awesome then it starts making so many paperclips that it needs new
substrate to make paperclips out of and it eventually turns the whole world into paperclips
because it can it can grow its intelligence to out compete whoever's competing for those paperclips
faster than they can that's a very short version of it you're gonna say something well just
I guess what I find very bizarre about all of this is that I live in multiple social worlds
and intellectual worlds and in some of my worlds this stuff strikes people as loopy
oh here comes the stuff about the agi the robots are going to kill us all and in some other portion
of my world it's like well clearly we're on the verge of agi and that's going to be the existential
risk and this is in part to go back to your original point something that you and I share
a failure a catastrophic failure of communal sense making right so what I've claimed is
is that the revolution that we're in is is based around the idea that we don't have what I've called
semi-reliable communal sense making we can't all agree now even if it's slightly wrong or maybe even
deeply wrong as to what it is that we're seeing where we are in human history what our issues are
and so the first part of this decision tree that goes really wrong is that a lot of people think
that we're in great shape okay so this is I I'm actually going to come back to the
paperclip maximizer because it explains why we don't have communal sense making
if instead of thinking about an artificial intelligence that can increase its capacity
while optimizing something we think of a collective intelligence that can some way that humans are
processing information together in a group a market is a kind of collective intelligence
right the whole idea of what the invisible hand of the market the the market will figure out what
stuff people really want it's expressed as demand and then which version of the various supplies is
best I wouldn't say it's an intelligence it's not a central intelligence but the idea is that
there is kind of an emergent intelligence there is an emergent property of this thing
and you know it computes things like prices and allocations and if that's what you mean by
intelligence then I that's what I mean yeah so it's it is a bottom-up coordination system that
does end up having new information emerge as the result of the bottom-up coordination okay
and I can take a market as a as kind of at the center I can take capitalism at the center of
the more general class of what I would call rival risk dynamics as a whole as a kind of
collective intelligence okay because the thing that wins at the game of rivalry gets selected
for and so there is this kind of learning of how to get better at rival risk games
learning across the system as a whole which which things win in war which things keep more people
believing the thing which keep people from a tritting out of the thing like that that makes
sense yes and so I would say that we have if we just take the capitalism part capitalism is a
paperclip maximizer that is converting the natural world and human resources into capital
while getting better at doing so so it goes from barter to currency to fiat currency to fractional
reserve process to complex financial instruments to high-speed trading on those those are like
the increase in its capacity to do that but specifically now it is a incentive for all
the humans to do certain things so if leaving the whale alive in the ocean confers no economic
advantage on me but killing it and selling it as meat is a million dollars of economic
advantage and if I don't kill it the whale still won't be alive because somebody else
is going to kill it anyways and then they might actually even use that economic power against
me now I've got I have an incentive system that is encouraging all the humans to behave in certain
kinds of ways and now not only do we need to kill the whales we need to race at getting better to
do it and making better militaries and extracting all the resources and so I see this as a kind of
so if I'm understanding where you're headed what you're saying is is that the market is kind of a
precursor to an AGI it is a collective intelligence that is
eventually self-terminating in the same way that a cancer is right the cancer cells are
self-replicating and they're growing faster than normal cells but they end up killing the host which
kills themselves and so the the reason I'm bringing this up in terms of collective sense-making
is those who do the will of capitalism like those who do the will of the paperclip maximizer
mo-locks or on whatever kind of analogy we want to use here those who do well at the game of power
get more power and then they use that legislative power media power capital power to make systems
that to modify the systems in ways that help them more right those who oppose the system of power
also oppose those who are doing well at it so even though the system is inanimate the people
who are doing well at it are animate so then they take those people out which is we see how
Martin Luther King and Gandhi and Jesus and etc died people who actually oppose to the system of
power right and so you end up having a system that is selecting for or is conferring more power to
those who are good at getting more power which ends up meaning who are selecting for conferring
power to sociopathy yeah I don't find this part of the argument well maybe I'm just stuck somewhere
let me be I mean I think I'm on your side so I want to help make a different part of this case
I think a lot of this comes down to magical thinking because of the non-use of nuclear weapons
against humans since 1945 I think that one thing if 9-11 had been a nuclear attack rather than a
weird conventional attack we would know where we were in human history and by virtue of our luck
and our luck alone we are completely confused as to how perilous the present moment is because
our luck has been amazing and if you believe surprising yeah if you believe that somehow
it can't be luck because it's this good then you believe that there's some unknown principle
keeping us safe and you don't know what the name of that principle is maybe it's human ingenuity
maybe it's some sort of secret collective that keeps the world sensible maybe it's that markets
have tied us all together I don't know what your story is yeah but whatever your story is it's wrong
and it's it's obviously wrong right the the idea that we didn't have anything like 9-11 and then
we had a sudden 9-11 kind of attack is itself paradigmatic that these things with which you
have no data familiarity I mean look there was no suicide bombing in the modern world before the
1980s and I think this is the point is that it's generally more advantageous within a market to
believe that markets are good and the world is healthy and things are awesome right I'll usually
do better in academia if I say academia is good right which is a point that you make if I really
criticize it heavily I'm gonna get less 10 years Peter's point more than my point okay yeah I will
usually do better in markets if I say they're awesome and do better in a corporation if I say
it's awesome and so there is kind of an incentive for optimism about the dominant system if I want
to do well in the dominant system and if I have critiques of the dominant system I'm usually
gonna do less well in it which means less power will get conferred to those ideas and so there's
kind of a memetic selection right like the memes that that do well end up being the memes that
propagate but do well within a current system well that look this is why I've called for
a return to above-ground nuclear testing because my belief is is that we you laugh but I'm not
kidding right I mean if we don't get our amygdala is really engaged with where we are this magical
thinking which by the way I suffer from this magical thing I'm not it's not something I'm
claiming everyone else has I have the idea that nothing too bad can can come that you know I always
ask this this weird question which is how many foreign nuclear devices are currently on US soil
people always think about that nukes will have to come through an ICBM I'm not at all convinced
that that would have to be true people just don't think about these things because we've been in such
a rare period of time that these things haven't like everybody who's talked in these terms
sort of to me like there's a part of me that sounds like okay well that's the kind of a
conversation you have on on a dorm floor during a bull session like grown-ups realize that something
is keeping the world together which is funny right because it's basically saying
grown-ups have bought into magical thinking exactly yeah and so and by the way a lot of the people
that I think of as being the smartest most interesting people have not bought into this magical
thinking right what has happened is is that those people have been pushed out of the institutions
that form this sort of weird conversation that I refer to as the gated institutional narrative
in the depopulation of dissenters like really serious dissenters from inside the institutional
complex is one of the defining features of our age to me but something that you can't get any
commentary on because the commentary you're really looking for is is that conversation so
what I just try to do is to show people that no matter what you do the gated institutional
narrative cannot look at certain very basic facts right so you get a very dangerous kind of group
think there and even if someone disagrees with it they have a lot more incentive to not publicly
disagree with it within that group thing what's the hell with those people I mean this is why we're
doing the portal podcast which is um let's be honest this is pirate radio yeah and I don't know why
