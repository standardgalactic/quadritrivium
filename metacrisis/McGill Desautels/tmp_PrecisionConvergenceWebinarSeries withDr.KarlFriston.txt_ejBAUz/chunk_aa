For those who are joining, I am Laura Dub√©, the co-chair of this webinar.
We have two more minutes before we formally start.
We will be officially starting at 11.03.
There's Lauren and Alain.
Is Alain here yet?
I forgot about Alain.
Yes, he's there. I see him. He's online.
Okay, good.
Yeah, we're up to 80 participants.
Dr. Dagger, it's good to see you.
See you in a while.
Yes, same here.
We all meet in suburb space those days.
Okay, we will be starting just now, I think it's close to 11.03.
Good morning, everyone.
Welcome to this webinar that is part of the Precision Convergence Capacity Building Webinar Series.
This series is co-hosted by McGill Centre for the Convergence of Health and Economics
and the Pittsburgh Supercomputing Centre.
The series was started about a year ago.
Following up the HPCI-performing computer, COVID-19 consortium that had been engaging
PSE, another research computing institution around the country, with IBM and many of the other data business,
for accelerating our ability to do research on health and health and health and health
and health and health and health and health and health and health and health and health
and health and health and health data business for accelerating our ability to bring solutions to COVID-19
This consortium was created for seeing, to what extent we can bridge the omics and all the digital capacity to accelerate solution to COVID-19
profiling, characterizing of the virus, vaccine and so on and so forth, and we said, what if we were to take as a silver lining of the COVID-19, continue building of this capacity of AI and digital technology and others to see to what extent we can bridge the various
science that are needed if we are to bring in a timely manner a solution to real world problem, whether it is pandemics or whether it is climate change and so on.
And a lot of it is tied to the fact that in the last 250 years, we have changed a lot the context within which the human machine has evolved.
The first industrial revolution was 250 years ago. It's essentially a few seconds in the journey of mankind. So this series is really to bring the scientists, the action leader, data operating on multiple scale in saying can we go beyond than what we are doing so far.
So before I introduce more, what is it we'll do together during this next two hours and the keynote.
I would like to ask my co-chair to introduce himself and then I will come back for introducing the keynote. Go ahead, Sean.
Hi, my name is Sean Brown. I'm the director of the Pittsburgh Supercomputing Center, a joint computational research center between Carnegie Mellon University and the University of Pittsburgh.
And I'm also the vice chancellor for research computing at the University of Pittsburgh. And I'm very excited about our talk today and our panel, follow up panel discussion. So I will take less, I will not take any more time. So thank you all for joining us.
Thank you, Sean. Very briefly in a nutshell, the way we spend the next two hours. I have been for those of you who were on the series before, we always start by saying it's not a webinar.
It's a capacity building webinar where we convene a keynote of significance in advancing science in one of the three domains, the human machine, the man-made system or the computer.
And we build a panel where everyone is carefully selected to not only be aware and complimentary to the keynote, but each of the panelists, as you can see, are also bringing those, their own perspective in advancing this precision convergence program.
So we have the first 30, 40 minutes of the keynote presentation. And then we have a first round of panel where each of the panelists is bringing a three to five minutes commentary from their perspective in response to the keynote presentation, but also from their own, their own knowledge and so on.
The keynote has about two or three minutes for each of them to answer in the first round. And then in a second part, then if there are questions from the audience, Sean will be managing them from the chat room to address them after the first round of the panelists.
And then we have a second round where everyone, including the keynote, is invited to kind of do the one minute commentary of where can we go as a world community if we are to really reach scale into the transformation that we are all wanting to achieve.
Okay. So, this is, this is kind of the logistic aspects. I would like before introducing Dr. Carl Friston to position a bit is presentation within the continuum of the webinar we have been having over the last year.
On the psychology self side, we have been having as keynote, Roy Bormester, Daniel Schachter, the last one on which Carl was on the panel was by Konstantin Stedikides.
All of them really, one can arguably say the most influential living scientists in the study of psychology and the mind and so on.
And we have been, and in the last webinar where Carl was presented was on the panel, the discussion was to see to what extent the cell, to what extent the cell, the self, and the end the psychology could be placed on the same platform of the neurobiology when we look at human being being
evolving, not only for survival, but also for achievement. And this is within that context that we have been discussing and that's why the work of the work that will be presented today on the cell,
is not to be dancing for individual and collective agency. To what extent can this substantive knowledge of human biology, psychology, and real world can be brought together in some degree of formalism that helped bridging all of the above so that is why, without further introduction,
I am extremely pleased to welcome Carl Freestone and as we, as Sean has said, we are doing introductions very short because there is a limited time with the richness of the intellectual and human capital we have together in the next two hours.
So, Carl Freestone is a theoretical neuroscientist and authority on brain imaging. I won't enumerate the various statistical methods that he has been developing.
His most, his main contribution to theoretical neurobiology is a free energy principle for action and perception, which is the focus of his talk today.
So Carl, if you excuse me, I will not give read the whole, the whole description of your achievement and I will go with you, go to you directly for your presentation.
Thank you very much. It's a great pleasure to be back here. And it's now my turn to do a keynote of significance.
And I've got to do that with two constraints. First of all, get everything that's important on the table within half an hour, and also use not my favorite words, but the favorite words of my panelists.
I think we all met last week just to sort of cover the key themes that we want to bring out in discussion. So my job is to try and situate those within this keynote and this keynote is going to address a first principle account of sentient behavior.
There's a story about how I behave and how we behave that can be elaborated from the first principles of self organization and the approach I'm going to take is from the point of view of self evidencing so I'm going to introduce the notion of self evidencing as simply as possible.
I'm going to give a why this might be an interesting approach in relation to current perspectives on artificial intelligence and machine learning, and then try to ground it in the neurobiology, as was, you know, my brief or our brief in the introduction.
I'm going to take a special focus on the mechanics of bleep propagation. And in particular, how we organize our behavior to solicit the right kind of data that supplies evidence for our own existence and sell that notion that imperative as an important drive for active engagement with the world.
And then finally, talking to the collective aspect and the importance of collective agency and shared narratives. So self evidencing what do I mean by that. And I simply mean that everything that we think or do can be understood or described in terms of trying to maximize model evidence.
So what's model evidence. It's simply the probability of some observable outcomes, given my model of how those outcomes were generated.
And on this view we can associate belief updating with perception where I'm denoting beliefs here as with queue of some hidden states or causes latent states of the world that are generating these outcomes.
In a similar vein, I also am going to assume that we have beliefs about what we're going to do about our actions, and prior to selecting a particular action, we have to form beliefs about what is the most likely action you that I'm going to commit to.
Another is that both of these these kinds of belief updating are driven by an imperative to maximize model evidence. So that has a degree of constructivity in relation to establish normative accounts of behavior.
And if you hear, if I read this log evidence as the value of an outcome in the sense that if I prefer these kinds of outcomes or these outcomes are characteristic of me, then I should behave to secure those kinds of outcomes as valuable outcomes and from that we can spin off.
If I was in psychology reinforcement learning that was an engineer, we could cast this about as optimal control theory. And indeed, if I was an economist, it would just simply be one simple specification of expected utility theory.
And if on the other hand, I was an information theorist, I would read this quantity or the negative this one to this negative log probability of an event or an outcome conditioned upon a model as a self information, sometimes called surprise or more simply surprise.
And it is this quantity that is I'm going to associate with a tractable measure of the value or the negative value or the surprise, namely a free energy and this quantity suggests that everything we do is in the service of minimizing surprise.
We can read that as the principle of maximum mutual information on the max principle principles of minimum redundancy or maximum efficiency that can be attributed to Horace Barlow and indeed the free energy principle, which is a formalism I'm going to use.
This is nice because the expected self information, the expected surprise before an event has happened is entropy. So this also says that we perceive an act in a way to minimize on average entropy.
And of course that's the holy grail things like self organization of physics energetics and if I was a physiologist, it would simply be a statement of homeostasis that I am in the game of keeping my outcomes within some viable bounds minimizing their dispersion or the dissipation.
And if I were a statistician, I would treat this quantity as the model evidence and from that we can spin off things like the Bayesian brain hypothesis evidence accumulation and various implementations of that such as predictive coding.
So,
I'm going to unpack this quantity, this surprise in terms of value in terms of a free energy functional and try and give you a number of different perspectives that on this quantity.
So the first perspective is that we can regard this statement as a first principle statement about the imperatives for self organization that inherits directly from James's maximum entropy principle.
So in maximizing this free energy functional here, we are effectively increasing the dispersion of our beliefs for keeping our options open to the greatest extent possible under some constraints.
I'll talk about constraints later the particular constraints here are afforded by something called a generative model is usually decomposed into a likelihood and a prior the likelihood of some outcomes given their causes latent states in the world, and some prior beliefs or preferences about the states that I am likely to be exposed to in
terms of generating my observable outcomes. There's another way of writing down this quantity which provides a complementary perspective and that's the kind of perspective you get if you're a statistician.
So I'm just rearranging the terms in that free energy functional and to express it as a mixture of accuracy and complexity.
So the accuracy here is it's simply the expected log likely to some outcomes given my beliefs about how those outcomes were formed.
And whilst the complexity is a very interesting term in this setting it's just the difference between my beliefs after and before seeing some observations or evidence for beliefs about particular states of the world.
It scores the degree I change my mind when I see an outcome or observe something. It scores the cost of belief updating. And the story that I'm going to tell for the remainder of this talk is that if I have to choose what to do that will get some more
information, get some more observations, then I can cast behaviour of a sentient sword in terms of choosing those actions that maximise the accuracy and minimise the complexity I expect following that action.
It turns out that the expected accuracy complies exactly with the principles of optimal Bayesian design while the optimal the expected complexity turns out to be risk and is exactly the quantity that underwrites optimal decision theory optimal Bayesian decision theory.
What I'm saying is that putting these two things together gives us self evidencing in the spirit of selecting those behaviours or actions that maximise this expected free energy and implicitly increase the expected evidence.
So just to make this slightly more heuristic and to compare and contrast with alternative approaches, imagine you're an owl and you're hungry. And if I was there with you, I would be asking somebody on the front row, what are you going to do?
And invariably they give me the correct answer. Well, I'm going to look for my food. I'm going to search for my prey. And that simple answer tells us something quite fundamental about the way that you would write down a calculus for sentient behaviour.
And the two kinds of calculus that I'm going to compare and contrast and then bring together later on rests upon the notion of a value function of states of the world.
So, I could say, okay, if I had some function that scored the goodness the value of a state that ensued following at the next time point, following a chosen action you, I could then just choose for every given state of the world, the best or the value function maximising action to give
myself a state action policy. Is this kind of calculus apt for describing behaviour? Well, the answer I'm searching for my food tells you no, it can't be, because searching is an act that reduces your uncertainty about the location of your prey.
But uncertainty is not a function of states of the world, it's a function of the beliefs about states of the world. So, we have an alternative way of writing down good behaviour.
That is something that maximises a function, a function of a function. In this instance, that function is beliefs about the states of the world that would ensue if I committed to a particular action.
So, a very different kind of functional form. The other aspect of searching for food tells you something also about the nature of the best way to describe or the best policies.
And that's it matters whether I search for my food and eat it or try to eat it and then search for my food. And that tells one immediately that what we're chasing here is a way of describing a sequence of actions or actions over time.
And this functional then becomes technically a path integral, a sum of goodnesses over time. And that gives you a very different functional form for the optimal policies.
Now a sequential policy that maximises this path integral of an energy functional. We'll see in a moment, it's a free energy functional.
And the reason I'm using those words is just to refer to the two kinds of principles that we could appeal to in underwriting these different approaches to good behaviour.
So this state action policy is predicated on Bellman's optimality principle. We'll find that in optimal control theory dynamic programming deep reinforcement learning basic decision theory and so on.
And this on the other hand, has a slightly deeper and simpler heritage it's it's basically a variational principle of station reaction.
The free energy principle being one instance of that.
And we'll see later that this is more apt to describe things like artificial curiosity or in robotics intrinsic motivation, and is more concerned with data gathering optimal Bayesian design than Bayesian decision theory.
And I repeat, is concerned with optimizing sequences of policies under uncertainty, using things like partially observed Markov decision processes.
Principles of least action.
This is the apologize for the equations, but there's a beautiful symmetry between the functional forms of this way of writing down evidence in terms of complexity or accuracy.
And the things the equivalent things that you are optimizing, if you are selecting those behaviors that you think will maximize the expected free energy.
So, just writing down again the expression for the free energy here, in terms of accuracy and complexity, and rearranging it in a particular way here that people in machine learning will recognize.
So, changing these terms around.
We can also express this as the log of the evidence, the probability of some outcomes given a model, plus something, a k r divergence, something that is never less than zero, because this thing is never less than zero.
This quantity this free energy now becomes an evidence bound known with by the acronym and elder in evidence low bound machine learning.
So there's two ways of carving up this objective function. And what would they look like in expectation. Well, what happens is that the expected complexity becomes risk, the expected inaccuracy becomes ambiguity.
The expected log log evidence or negative log evidence becomes expected cost, and the expected divergence becomes expected information gain.
And this is beautiful, but it's also quite unremarkable because most people in one field or another already know about this. So for example, if we ignore the expected value of the negative expected costs then we're just left with this expected divergence.
What is this. Well, it's simply the difference between my beliefs about states of the world in the future, following a particular policy, if I were able to observe the consequences of that behavior relative to if I were not.
So it scores the information gain, or the reduction of uncertainty about hidden states or latent states of the world, afforded by this particular policies as the epistemic affordance of this policy.
And the visual search literature this will be known as expected Bayesian surprise more simply it's just the, the mutual information or the expected mutual information conditioned on a, on a policy.
I'm going to successfully remove sources of uncertainty from the table and just see what emerges and what's going to happen is we'll get back to the Bellman optimality principle and expected utility theory that on route.
Let's assume that we live in a universe where there's no ambiguity states or outcomes and outcomes and states I can see everything directly.
And this means that we can ignore the ambiguity term and supplant the SS for OS and vice versa and we're just left with this which is another KL divergence, and it just scores the difference between what I anticipate will happen.
I pursue this policy, and what a priori, I prefer to happen, given I am me, this is the expected the value of the negative, the negative cost expresses a probability.
So this minimizing this difference this divergence between what I anticipate and what I prefer is known in engineering as KL control and economics it's really sensitive control.
And it's simply a description of behavior in which you choose those things to do, which minimize the divergence of the outcomes that you expect under a generative model and mixture of likelihood and prize.
Namely, your prior preferences. And then finally, if we remove the kind of uncertainty that is reducible by my epistemic foraging or responding to epistemic affordances or the intrinsic motivation for finding out what would happen if I did that.
And we can ignore the expected information game we just left now with the expected custom that clearly can be read now as expected utility theory if you're in economics.
In summary, what we're saying is that there's a simple way of writing down self evidencing that speaks to engagement with the future and the way that we choose what to do next, and we can decompose maximization of expected free energy,
as we refer to as active inference, in terms of to dual aspect Bayesian or dual aspect Bayesian optimality one in terms of maximizing expected value of minimizing expected cost in accord with Bayesian decision theory base of making base optimal decisions.
At the same time, we're maximizing expected information gain, and this inherits from the work of people like Dennis Lindley on optimal Bayesian design basically designing your experiments and the way that you solicit those data that resolve the greatest amount of uncertainty about your
hypotheses or models of how data are generated.
So to close, I'm just going to race through things you can do with this first principle like account or method.
The first thing that one can do obviously is to simulate and in silica reproduces what agents and artifacts and see how they behave, but to do that you have to commit to a generative model these are the constraints that when supplied to James is maximum
entry principle gives you the free energy principle. So one key thing is that if you can write down a generative model, you can always express it in terms of a probabilistic graphical model.
What this slide is meant to convey is that then there always exists something called a factor graph. So why is this important. Well, the factor graph is just if you like a dual or an adjunct way of expressing your generative model in a way that reveals
the nature of the messages that need to be passed amongst the different belief structures that are over the random variables the hidden causes that generally are organized according to this Markov decision process so that we have outcomes that are generated by states of some
universe. Those states generate outcomes for a likelihood mapping. That's likely part of my generative model, but the states evolve over time through transition matrices that constitute my prior beliefs about the dynamics and the contingencies and the laws that denote the causal structure
of the universe. And it is these transitions that I'm in charge of in terms of the policies that I select and I select my policies to maximize the expected free energy here.
So that would be a generic graphical model description of an active engagement with the world. And this would be the equivalent factor graph that tells me the message passing that I would need to do to do the belief updating to realize and integrate the system to simulate
an increasing or active inference and all sorts of great games you can play here in terms of once you know the message passing the belief, the form, the functional forms and schemes of belief propagation that in some way must be under the hood for any given
model you wanted to commit to. And you can start to think about how this would, what this would look like in various systems such as the brain. I won't go into this but there's a, again, a very beautiful and very simple mapping between the time scales of different
updates during say belief updating perceptual inference, slower updating of the parameters of my generative model that we associate with learning and the action selection that ensues from this notion of planning as inference in terms of updating beliefs about what I'm going to do next.
And we can map this, these functions onto different brain areas and functional brain architectures and make predictions about the nature of those architectures which I'm sure we'll be talking about later.
We can simulate behavior with through one example just to sort of demystify and just pick out the most important aspect of this, of this formulation, which is basically this, this balance, this dual aspect, optimal Bayesian design versus optimal Bayesian decision
theory, and show that in fact that balance is systematically deterministically specified by how much I know about my environment. Effectively, it dissolves exploration exploitation dilemma.
And to illustrate that there's a little game we've created here we're simulating this mouse and the game has for this mouse is it has to, in just two moves for secure its reward that could be on the left or the right of the teammates.
And so it could either take a chance and find out where the reward is it has to stay in one of the upper arms.
If it chooses one so you're half the time it may be right and spend two bits moves with its reward the other half the time, it will be wrong, and has to stay with the non rewarded in the non rewarded location for its two moons.
So this is the choice and this is the important choice from the point of view of this little demo.
We've also equipped this universe with adiante queue and instructional queue that if it uses one of its moves to actually go and see the color of the queue down here, this queue tells it where the reward is.
It can choose to use the first move to explore to resolve uncertainty about where the reward is, and then the second move to go with a degree of confidence to secure its reward.
So from an expected utility point of view these are equally viable options but from the point of view of self evidencing with this epistemic affordance in play, it's going to be much more attractive for this little mouse to go and resolve uncertainty by going straight down to the informative
to simulate these this mouse under a particular giant in model using those update equations I showed you a couple of slides ago. That's exactly what happens.
So we start off with the little mouse going down finding where the reward isn't then sitting on its reward for the final move.
But as time progresses, these details are unimportant. The only thing that we're trying to convey here is, as time goes on.
The only thing that we're trying to convey here is leave the reward always on the left hand side. So that this mouse now accumulates the belief that the reward is always in the same place.
And as it gets more and more confident accumulates more and more evidence that the reward is always in the same place. The epistemic affordance or attractiveness of the instructional queue starts to weigh.
The epistemic value, the epistemic affordance will at some point become smaller than the expected utility or the expected cost of continuing to forage epistemically.
And at which point the behavior reverts to explore it to explore your behavior where it will go straight to where it now knows the reward is and stay there for two moves and we can see that occurring after about 20 trials in this particular example.
So that switch in behavior affords the opportunity now to do all sorts of interesting in silico experiments and make predictions about either neuronal responses to be chosen versus chosen option.
Look at the encoding of behaviors and locations in terms of place selectivity. And indeed we can simulate mismatch negativity paradigms or ball paradigms by just comparing trials where at the beginning, there was lots of uncertainty.
So everything is novel. Everything is an audible with trials with exactly the same behaviors and sensory stimuli or sensory outcomes.
But now after the mouse has become familiar and was bored with its environment. So everything now becomes a standard and again we can reproduce with some market remarkable fidelity the timing and the the forms of simulated
electrical physiological responses. So I'm giving these examples very very quickly just to show that they are both grounded in and to a certain extent inspired inherit from the neuro biological implementation of these this sort of belief updating scheme.
So I'll finish now with a with an important nod, but it'll be no more than a nod to what does this kind of formulation bring to the table when we're thinking about lots of agents acting together, all succumbing to the same kinds of imperative.
And one really interesting sort of thing that emerges technically from this is that we move from a world in which we're trying to think and explain the behavior of a creature or a particle that's making influences about it, the states of the world out there the environment through observable things and
over which it has control through active things and we refer to the these two kinds of variables as a Markov blanket. We move from a structure where there's a Markov blanket separating the internal states that do the brief updating
from the outside world that is supplying sensory evidence and is subject to our actions upon which we act. If we now think about two of these agents putting together, then if the external states disappear because you become my external states and I become your external
states, so there are no more external states are just lots of internal states, my brain states your brain states mutually trying to infer each other, and to cut a long story short, the, the, the natural consequence of this in terms of both of us trying to minimize our
surprise is that we will learn to make ourselves more predictable, and we will query each other in a way that we will resolve uncertainty about you are I will resolve uncertainty about you.
But I can only do that if we have the same shared narrative model.
I'm going to close with an example where we use the intrinsic motivation the epistemic affordance to compel two agents, one of which can see a scene and the other one can't, but they can talk to each other.
And because there's this epistemic affordance or incentive to resolve uncertainty, one agent knows because it shares the same language with the other agent that she knows what questions to ask to resolve uncertainty about the scene that she cannot see but the other agent can.
And it's quite simple just to write down the generative models and the equivalent graphical factor graphs for a very simple world where two people are basically playing, well, they're playing 20 questions and one person asked is, is, is something to is something like that.
They will say yes or no, and they will slowly share their beliefs. So in this example, the, the blue woman can see two shapes, two blocks as a red square below and there's a green square above this lady the tea lady cannot, but she can ask questions you can ask this lady is there a triangle
this lady says no there isn't. And then this lady knows that there is a square, but does not know the color. And through this process of making the right moves, engaging in policies that resolve the most uncertainty to buy maximizing the expected free energy.
She can ask the right question about the color of square below, and fans that it's not green so it has to be read and then she can drill down on the, the nature of the upper object until after about four questions.
She knows exactly what the other person's looking at it but they can switch roles and the other person can check that the first person has got the right idea.
And so the reason that I bring that to the table is to fold, want to emphasize the importance of this epistemic foraging in the context of interpersonal interactions and
the mechanics of exchanging beliefs. And the other thing is what underwrites this, which is the shared generative model that is necessary to actually communicate beliefs.
Things that are very necessary if you want to simulate belief updating in a distributed way that has to go across a Markov blanket in some ensemble or collective.
Now, what's on the other side of the Markov blanket doesn't always have to be the same, it could actually be, you know, an eco niche, but still there's going to be an exchange of information and querying when lifted to the cultural level.
And then we could actually talk about these, this formulation as a one way of understanding cultural niche construction. And then finally, if we now apply these principles over different time scales, we can also now think about sort of optimizing model
evidence through model selection as a mathematical image of Bayesian of natural selection and evolution.
So with that, I will close just to the final world to Einstein, everything should be as simple as possible, but not simpler with a not this important notion of complexity.
There is an important part of model evidence. So with that, I just want to thank those people whose ideas have been talking about the course. Thank you for your attention. Thank you very much indeed.
Thank you very much, Carl, for this brilliant presentation that get as simple as this candy presenting that complexity. And I really also appreciate that you squeeze our webinar series into your busy schedule after the one we have with
Konstantin. I'd like to ask a twin question before we get to the panel.
And that question is trying to my, I could say, let's say sophisticated lay woman or even just lay woman on understanding that I think I'm getting to get to of your Mark Marko blanket that is kind of a core aspects, which I believe is kind of the statistical
space that come that combine or differentiate what is in and out system. If I understand correctly, my two questions are the following. As you most likely don't know I was trying 30 years ago at Cornell at the time that be a
kind of mix was starting with with Dversky kind of man and the Taylor that was very descriptive vision of the non rational side of human behavior in moving toward precision convergence, I progressively get much more around along the work of
Herb Simon that is defining essentially bonded rationality as being resource rational within the context of your system. So my first question is to I'm intrigued to hear you connect your work with herb Simon view of bonded rationality and his approach and within that context also on the
panel that we were having on on one consenting was presenting is only a static view of self promotion and there was this debate between this model whether the psychological self promotion promotion and the the biological protection for
immunity and no one was trying to seize their cause, a common causal mechanism. So I would be intrigued in my question, my second question, if you could briefly comment on advancing the discussion we were having between the psychological, the mind and the self and the
biological basis within the context. And the challenge is answering this in three minutes.
I'll spend the first two on the first questions excellent questions so there's your Simon's and bounded rationality I love that because of course.
There's a nice semantic clash with an evidence bound. So if you go to machine learning and ask how does a variation auto encoder work. It optimizes its weights with respect to this elbow.
It's all in the bound so bounded rationality approximate Bayesian inference defined operationally as optimizing a variation free energy literally is bounded optimality.
The other thing that's nice about it, you know, is this notion of constraints, and I'm using that in the in the sort of James in the sense, but of course an important constraint is the uncertainty under which you're operating.
So, you know, that's why I was so keen to emphasize that you can tease apart your objective function into two bits. But remember, one bit is contextualized by the other so the base optimal rational decision making the unexpected
is always contextualized by a degree of uncertainty so there's a value of information that provides if you like a space or a constraint or a context for and that you know I'm reading that in the spirit of boundary rationality, where this is a computational
resource or an information resource. With respect to the homeostasis yeah I mean I love constant in stuff because you know it's just lifting this basic imperative for generalized homeostasis a generalized synchrony with your environment.
And from the physiology to the self modeling to the things that underwrite our behavior and speak to this really important notion that if it is the case that we behave in order to secure or realize the kinds of outcomes that we prefer.
And just to be base optimal in this generalized sense is basically to have that sampling bias and just to be to be intrinsically biased and if you weren't you wouldn't exist you know if you bring about the things that you a priori or expect.
Then that is a sufficient explanation for why you can serve yourself over time and of course that's just a statement of that in spaces.
Thank you so much. And so I would like to ask the panelists to all open their videos shifting to Sean, my co chair who can ask his privilege question also and stop the panel go ahead Sean.
No I'm not I'm not going to take that privilege because I'd like to hear what our panelists I'm very interested here with our panelists have to say so we'll go ahead and get it kicked off with our first round commentary.
I'd like to introduce Mark Bailey who is the vice president for vice president for research at CIFAR and a professor in the department of computer, the departments of computer science statistics and actual science.
