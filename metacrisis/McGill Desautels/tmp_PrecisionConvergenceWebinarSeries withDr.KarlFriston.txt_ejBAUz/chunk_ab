Electrical and computer engineering mathematics epidemiology and biostatistics and biology at Western University so welcome Mark and looking forward to your commentary.
Thanks Sean and thank you Carl for an absolutely beautiful presentation. So you asked in your opening slide is there some information theoretic imperative that renders inference and decision making two sides of the same coin.
And then you've given us that this elegant and complex model of how precisely that that might be implemented.
And I'm a simple person so I needed a simpler questions I said I'm going to step back and ask how might such a mechanism arise. Why might we end up in this place.
And I suspect most of us here in the audience have had the experience of you're using your laptop, and it's doing a computation or programs crashing and suddenly it gets very hot.
And you know what why is that why is your laptop getting hot when it's working harder it's not making toast it's just moving bits around.
And so there's a fundamental disconnect because we still tend to think of the physical world in terms of mass and energy.
But, you know, since the time of John Archibald Wheeler, we know it's actually mass energy and information. So information is a fundamental, fundamental core object in our physical universe.
And so Wheeler had a tagline it from bit, you know from a bit of information, we can create something physical and in fact that bit of information is something physical.
And then in the 1960s roll flounder at IBM Watson labs, conjectured that erasing information is thermodynamically dissipative.
So if you take a bit of information and you erase it you clear that bit, that will release energy. This is controversial at the time. There's literature both supporting it and not supporting it but there's been a spate of experimental results recently over say the past two
decades that have really strongly confirmed that yes erasing of information is dissipative. So information processing and thermodynamics are linked.
So if you take this framework and what we know in theoretical computer science, I'm going to say something is not quite true so forgive me. The harder the computation, the more deletion is involved in that computation, and the more energy you need.
We could spend hours talking about what I mean by hard what I mean by needing deletion but just trust me on this one for now.
So the question I come back to is, would evolution select against unbounded computational complexity, which is to say unbounded energy requirements.
And I think that that that's reasonable to say would be parsimonious for for evolution to not prefer systems that require unbounded amounts of energy.
And so this question has been asked in the literature, Martin Novak, who is a famous evolutionary game theorist has a nice publication and PNAS about 10 years ago on the computational complexity of evolutionary dynamics and more recently and on topic for
Kazanichiv at Oxford as a beautiful paper in genetics with title computational complexity as the ultimate constraint on evolution.
Love it. Definitely worth reading. So back to the brain and to the presentation we the keynote we've just had.
The brain is Turing complete, of course, any computable function I can compute with my brain, give me a pen and and this big enough stack of paper, I can simulate a Turing machine.
But the real question is, during my my everyday life, whether I'm a mouse in a maze or or me giving a response to a keynote here. Do I spend most of my time in that regime of super linear recursive recursively innumerable functions.
Am I really doing hard computations most of the time. And the answer is, I'm not right we've all had that experience of driving on autopilot, and there you know there's a lot of systems that are working with very little energy there's there's not a lot of
of of hard computation going on. And so what if the strongest constraint, as Kednichiv suggests, on the evolution of brains is computational complexity.
Where does that get us well to push it I'm going to take an extreme absurd position. What if what psychologists call attention. What if Carl sentient calculus I love that term by the way.
What if consciousness, what we perceive as consciousness are actually primarily mechanisms for controlling complexity.
They're mechanisms for controlling computational complexity, and everything else, all of the richness I experience in my conscious life is a happy side effect of that imperative that is selected for through evolution.
And you know, I think those are actually pretty good mechanisms attention is a great mechanism for for controlling computational complexity. Of course that you know that was an absurd position to propose but I did it to invite conversation.
In reality there's going to need to be a balance between this need to keep computational complexity low, while also providing a robust behavioral repertoire because you do exist in an environment and you must behave in that environment so computational complexity is not the only
mechanism on which evolution is performing selection, but I think it's an important one. I think the role of thermodynamic complexity of neural computation has been understudied, and I, I can't prove this, but I'm going to think about it harder.
I think if you pull on this thread, you end up with a model that looks very similar to to what Carl has just presented here. Thanks john.
Thanks. Yeah, Carl, do you have a response? Just maybe a minute, a minute response.
A minute. I'd like to respond for about half an hour. There's so many great points there. Okay, in one minute.
Right. First of all, selection, your natural selection, the evolutionary pressure, attentional selection, all one theme, I agree entirely. It's complex. I mean, it's only half the story because you've got the accuracy, but under, under constraints that the accuracy about the same, it's all about the complexity.
So, you know, you can see that in statistics through Bayesian model selection, that you're trying to get the simplest architectures that will generalize so you don't want to over parameterize so even the pragmatic agenda of evolution and machine learning is driven by getting the simplest explanation,
possible. So, you know, there are all sorts of ways in which one could celebrate that. You know, I'd like to talk about Commonwealth complexity and universal computation, Janinsky Equalities, you know, so many right things have been said there but I've only got one minute so I'll shut up.
Thank you.
I didn't mean to stifle you I just we have a lot of other perspectives to get to.
I'll turn it back over to Laura.
Yes, and I'm very pleased to introduce Conrad.
Conrad, I apologize in advance for my awful French accent, according, I discovered Conrad's work in a paper on foraging with the idea of looking at dopamine mechanism that are finding where to go and then where to have to have your, your choice.
So that's, I'm extremely pleased to have him with us with his lifetime work on sensory motor and various aspects of nomads in model in Bayesian statistic and other is a German neuroscientist and professor at the University of Pennsylvania.
Yeah, so I will keep it here Conrad to leave you more time. Go ahead.
Three to five minutes. I know you have a lot to say.
So let's, let's zoom out just a little bit no like there's a broader endeavor in our field which is called normative modeling.
It is, it is used in important parts of neuroscience. It's also the basis of machine learning arguably what we do in that field as we say, let's try and derive the optimal solution to an existing product.
It usually works like that we where we look at the world at the problem maybe that the brain is solving or at the problem that we want to solve as as machine learners.
And we write down what our goals are it's usually like a cost function this is what we want to optimize not like get it to be precise most of the time, whatever your definition is.
And then we will usually have a set of constraints those things that we cannot deviate from so my category example compute is one of them there's many of her many others of them.
And then there's prior knowledge that we have about the world aspects of the world I think Carl, Carl related to pump the piece earlier.
And then what we do is we take this prompt statement and we kind of derive the optimal solution and if we're neuroscientist we compare behavior maybe to that we look for representations in the brain.
If we're machine learning people we basically just see how good we are at solving the technical problem that we want to solve so in that sense I think we're all sitting in the same boat.
Now, where I think there's a rather large disconnect between the way the keynote was thinking about it in the way I think about that field is from me.
Most of what happens is in the details of that interface so when I say go.
It's not that simple get it right. It's more like on which data set will we get it want to get it right under which circumstances, what will be the future tasks that we will have to solve with that.
It's not strange it's not just like you only have x computations, it's complex, or but the world not like it's not just a pp world it's a world with structure and causality and like all the different pieces.
So, from my perspective, all the interesting things kind of live within these components that we put into that. So for example, if we want to talk about about curiosity, you can say well, what are we curious about what we curious about causal
things because we believe that in this specific world in which we are causal things will generalize to future problems. We will talk about which things are relevant for us but I'm not interested in information generally.
I'm just interested in the kind of information with which I can give a great talk.
So, so, basically, for an hour and curiosity within machine learning which is a huge field with like hundreds of thousands people of people working on is all built based on different people making different assumptions that that interface.
So, here comes kind of like my difficulty at some level with the keynote know like at some level we've seen an ansatz and was it talk about ansatzology.
Now the answers isn't different to like how anyone in machine learning users said this, we can write the same problem in a few different ways, for example, it's been known since the 60s that we can convert optimal control problems into equivalent inference problems and vice versa.
Interesting thing. So that's just like the same answer it's just written slightly differently.
So, so, so from my perspective, it's not the answers per se that is important because we're utility maximizers expected value for the future. We all agree on the answers.
We agree on the pieces what can we put in what are efficient algorithms and so forth. So, at some level the thing that is missing for me in the approach is pushing these things at the interface what are the assumptions about the world what are the assumptions about the cost functions.
What is what exactly can I say about computation and bits and so forth. And that then in a way like no in that talk.
Dr. Fresden kind of relates to kind of anything everything that is close to my heart now let's let's let's let's let's be positive about that now like curiosity, sentence planning self evidence the shared narratives it has like all these things that mean so much to me.
We've only ever seen the answers for all of them we haven't seen. Okay, how much does it bias to make this assumption versus this assumption so everything that computer scientists are concerned with is in a way hidden because we only ever get to see the answers.
And I'm sure that that there will be at least a minute maybe more than that of rebuttal to what I just said. Thank you for letting me comment on it.
Back to you.
In two minutes.
You can take three.
Okay.
I'll split it one and two. So I think I think there are two key points here.
The first one is your the status of normative theories I think there's a very general important point there.
You, for me and normative theory would be your being able to write down an objective function. So, from an engineer's point of view, your if you can write down the objective function, then the game starts and then you try and find the best way to satisfy that under constraints and the the perspective of the free energy principle.
So that sort of backstories the active inference implementations takes is slightly different in fact fundamentally different it takes a point of view of a physicist and say, if this system exists, what is this objective function.
You know, if I was a, you know, a classical physicist I'll be looking for the Lagrangian, and what's the way of articulating the behavior systems that optimize, say, their action or the pathological Lagrangian well that those are just principles of least action.
It's a mistake that the free energy principle is sown as a principle of least action, because what he's saying is that, given the system exists, then it must possess this Lagrangian that has an interpretation given the Markov blanket as, as a sort of
computational free energy that you'd find in, you'd find in, say machine learning statistics generally. So, I think that's a really interesting philosophical issue I don't leave that and not say anything else.
The last minute will be the second I think, I think, pragmatically more important point that Conor was making he was talking about getting the details right, getting the, getting the questions right in terms of an actual application of something like the free
energy principle to a particular problem. I think it's absolutely crucial. And for me, that really is distinction between a generic application of the free energy principle to what, well, it's the generative model.
That's where all the difficult questions lie. The structure of that generative model, the complexity, the ways in which you prune it, is it hierarchical, what kind of message passing scheme is going to be in play after this kind of generative model continues.
So I'm really acquiescing to Conrad's sort of observation that there is something quintessentially vacuous about the free energy principle because it is just a method. It does not give you the answer in the sense that this generative model is fit for
other factors that satisfies exactly marks complexity constraints. Yeah, I was just thinking, you know, Conrad, you're talking about so simplifying making this more efficient, which is exactly, you know, the complexity part that you would use and say basic model selection.
But the point I'm making here, the free energy principle is just a method. It's not a solution. The solution is actually identifying the generative model that is appropriate for this situation.
That's the area of study.
Thank you, Carl. So I think you're the one to introduce a lot.
Yeah, it's my pleasure to introduce Dr. Lauren Ross, who is an associate professor in logic and philosophy of science at UC Irvine. So we look forward to your commentary, Lauren.
Great. Thank you very much.
This was a great talk and it's a pleasure to be here.
I have two questions and the first one has to do with causation. The second one has to do with constraints.
The first question is about the type of causality that's most relevant to this work, in particular the type of causality that's present in the generative model itself, and perhaps also the type that they suggest that humans have in mind when they're
reasoning or an organism has in mind when they're reasoning about the world.
So I'm a philosopher of science and of course there are heated debates about how to understand causality and the account of causation that is relevant to, in particular, these generative models.
There are at least two accounts of causation that are often discussed in the context of generative models. One of them are process accounts and the other are dependence accounts.
A process account of causation is viewing causation in terms of continuity.
Typically, that there's some kind of continuity from cause to effect. This is sometimes specified in terms of a spatiotemporally connected process, and in other cases the transmission of something from cause to effect.
Sometimes it's referred to as a mark, mark transmission. The entity that's being transmitted can be material or it can be energy.
So this kind of flexibility with what is being transmitted, the emphasis in this account of causation is that there's the transmission of something, and often that there's a kind of physical connection through the causal process or from the cause to the effect.
Now the second type of causation is a dependence theory. And in this framework.
The second type of causation is understood in terms of dependency relationships. So effects are outcomes or factors that depend on their causes.
And in this sense, changes in an effect depend on changes that you would make, do make or could make to the cause. And you can be completely ignorant of or indifferent to the transmission of anything from cause to effect.
Physical connection or you don't need a transmission of any kind of material, or perhaps anything else besides causal influence.
The dependency accounts are also related to control in a number of cases, the sense in which we think causes control their effects, or that causes are kind of switch that you could manipulate or intervene on.
So that when you do that you get control over the, over the effect. So the question here is, do these generative models have implications or suggestions for the account of causation that we as humans or that an organism is using when they're reasoning about the world.
Additionally, and distinctly, what type of causality is involved in the model itself. And then the second question is about constraints. I'm very interested in and I very much like the role of constraints in this framework.
There is right dimension of constraints comes up in a number of places.
And they are.
It sounds like often cited as things that are limiting a kind of space of plausible brain architectures. That's how they sometimes come up, and they can include things like real neuro anatomy, maybe anatomical connectivity.
Maybe these are empirical constraints, but then the generative model is also cited as a constraint. So philosophers of science are very interested in, they've become very interested in constraints.
They are interested in clarifying what they are in particular what role they play and explanation and understanding. And part of their interest is motivated by the fact that they seem like a unique explanatory factor.
They are not always causal, it seems like, maybe, in some cases, they are mathematical, or they involve a kind of law of nature, their thought to explain limitations and a possibility space which is different from how we standardly think of causes or kind of run of the mill explanatory factor.
So the second question is about constraints in your framework and in this model, if you consider them to be causal non causal, or if they have some other character, and just for you to say a bit more about the role that they play in the model and perhaps also an explanation and understanding.
Or however much time you want.
That's dangerous.
To the three to five minutes no more.
So it was a brilliant question and also artfully described to the majority of us who probably don't know what philosophically cause and constraint means but that was very limiting thank you.
So, while it's fresh in my mind, the notion of constraint in this instance, I would read that in, you know, with my physicist hat on as exactly the way you described which is a way of describing disallowed or highly unlikely states of being in some space possible
states of being that just is the generative model.
So, I've been talking about sort of active inputs and self evidencing with it with a strong theological license and cheek, basically.
But from the physics point of view that teleology is just not there. You're all you're trying to describe is the conservation laws or the implications of certain conservation laws that lead to principles of usually similar principles like principles of least action.
And what that basically means for describing something.
So, to say, you know, demarcate the thing from everything else and that induces Markov blanket but once you've done that, then the shape of that thing and the nature of that thing is just basically described by the sets that are the kinds of states that that that thing can occupy the things that he can't
So the whole thing is predicated stipulatively on a constraint specification of the thing that's and then that from that you then interpret put a theological interpretation on that constraint as a generative model.
And if you've got a generative model you've got model evidence and then you can tell a story about self evidencing, but in reality, it's, you know, it's just a mathematical as a calculus just to write down the shape of things.
So the shape is the constraint, and you can describe that probabilistically in terms of a generative model is literally the probability of causes and effects joint distribution over causes and effects, where you're very careful to specify what you mean by cause and effect.
And that's why you need the Markov blanket, because, you know, what's causing things in me, well it has to be stuff that's not me. So then you have to have a separation so we know that.
I'm just saying, introducing that distinction between cause and effect as something which is quite a big move that you've made as a physicist.
Because you have to actually write down how would you discriminate to in a cause and effect, you know, then you get into the game of the Markov blanket.
And the second thing was really interesting, I'd love to hear more about it.
From my perspective, it is a sort of local and possibly limited perspective, it sounds as if the two different kinds of causality are essentially those that are of the kind that a control theorist or dynamicist or dynamical systems theory or a physicist
can talk about where it's not so much important about space that set the temporal continuity aspect that is the, if you like the defining feature of dynamics of dynamics is holds causality.
So, as soon as you write down a launch of an equation or a state space model or a Markov decision process or a hidden Markov model, anything that involves time, any mathematical process that involves time.
You've got causality baited in exactly the way that you describe it. I have in terms of Mark transitions or transition probability matrices or sort of, you know, flows implicit in state space models as differential equations, all of them.
At the end of the day, our core causal structures in the, in the control theoretic sense, simply because you have described the system of interest in terms of either discrete or continuous time equations that you're talking about dynamics.
There's no other, you know, causality.
You know, your geratin model, if it's a geratin model of a process that unfolds in time, it is by definition, a causal model. In version of that model, basically it makes inferences, you know, of a causal sort, selecting the right model to account for that causal
model that you're talking about in terms of evolution or Conrad finding, you know, getting the, getting the, doing it under the right kinds of constraints. That is just basically using this principle of least free energy as a way, a method of getting out
of a causal architecture by inverting a causal model, which begs the question, what's the other kind of dependency causality. I can certainly see a sort of a glimpse of a connection with sort of pearl like causality for static models.
But that kind of do calculus and that kind of model that you'd find in things like structural equation modeling and, you know, his interventional calculus.
That's unashamedly about static systems and unashamedly about usually directed a psychic grasp that don't exist in the, in the, you know, in the real world.
And I'm not quite sure about that dependency status. And of course, if you do use that dependency sort of causality, which we all do when we report a correlation coefficient as scientists.
I think you're a very shaky ground from the point of view of the physicists who just deals with the processes that are not background free because they have time in them.
If you give a physicist time, then you've also given him causality. So that would be my take on that.
I'm not sure about those answers. It'd be great to discuss that.
Thank you, Carl. Before introducing Randy.
Just a quick comment on the, on the, the first three commentary, I was just finishing a reading a book last weekend. It's an historical novel from the Turing Alan Turing.
He is, he was a theoretical mathematician physician, then moved to do whatever he did in his life in his young life.
And one of his regret that the engineer had taken over the basic mathematician in moving forward in understanding decision making.
So hearing Mark saying that everybody, not everybody, but solving the Turing puzzle by computer is, is doable.
You are saying that you, as soon as you have a production, a production function, you can go ahead and optimize.
What I'm asking to everyone is what if you were having the same at the same time your heart of philosopher, mathematician and physician and understanding trying to understand this real world behavior in real world context.
So that's kind of my commentary as we get to the second round of this in mind. And also that commentary allow me to introduce very directly my Canadian colleague Randy Macintosh who is the director of the Institute of neuroscience and the whole technology and
at Simon Fraser University and very much as a neuro computational neuroscientist very much moving in that direction of trying to see how we can bring this precision convergence. Randy, go ahead.
Thank you.
Hi Carl.
This is almost like a PhD dissertation defense I'm thinking.
So we have to decide at the end whether you pass or fail, or there's a heavy revision.
I'm appreciating discussion because I'm getting different perspectives on the whole collective of optimization and objective functions and generative models, and how we can think about using them to address different phenomena.
And what I want to try and maybe throw out there for for discussion is, and you just mentioned this Carl about time being a factor in building these generative models so let's take into consideration for instance,
different time scales over which, which events that affect us can happen. So there's this event now which is happening over this the time scale of seconds.
And then our interaction which you illustrate at the end of your talk.
So our slower time scales that evolve that are sort of circadian rhythms we have very, you know, salient events, for example, we're all sort of sitting in the middle of this pandemic, which has a scale over several years and probably going to have a cascade
effect that happens over subsequent years.
So there are social cultural factors that obviously impact our own interactions impact our physical wellness but also our social dependencies and
this gets into the notion of multi scale models and how to envision these dependencies in some sort of generative model which could I guess use free energy as one of the methods.
I guess the question is whether you can build in that time scale interaction where free energy has to span both within temporal scales, but also between temporal scales.
And whether we start getting into this issue kind of like it's turtles all the way down then because we're kind of looking at using exactly the same objective function for different types of generative models.
But the general model we're talking about one case is more biological, whereas another case might be more sort of psychosocial. And I'm trying to sort of reconcile how to think about these various time scales in the framework that we're sort of
evolving over the course of our discussion today. So I'll stop there and we'll let the candidates reply.
Sorry.
He can say that. I don't actually I haven't actually got my PhD yet.
Ah, seriously, I haven't quite gone round to it. So, so I get one for the physical version so that will be quite nice. Yeah, you have time for all your time.
That's an interesting question. I can see, you know, that that's that that question is also going to come up in the context of sort of natural selection and, you know, later on.
So I think a really key point, and I'm sure you know the answer that the so the importance of the deal.
The commitment to a first principle account is that very much like most of these principles that they're fairly scale free or scale invariant, which does not mean to say the same free energy, or whatever other functional you want to use is
to optimize at all scales. What we're saying is the functional form, the method, you know what it looks like in terms of, you know, what is a function of what and the shape of those functions that strictly speaking Lagrangian is actually conserved over different scales.
Another lovely example of that is statistics is the fact that if you're trying to optimize, let's take a, let's take, yeah, let's take a say a variation or to encode it from machine learning that, you know, is built to change its weights to optimize the elbow the
free energy. So it's a canonical state of the art 21st century free energy optimizing machine.
But now say I come at this, I've got a choice of four or 400 very short encoded some of them with six levels some with different kinds of rectilinear, you know, activation functions, you know, different kinds of parameters.
So this was a model selection or a selective or sort of, you know, using the differential evolution or genetic algorithm. So what I would then do is score the quality of each of these variation auto encoders, and then I would apply genetic algorithm and do some sort of your selection.
So that's a simulated evolution natural selection. The key thing is I'll be using the same objective function in the sense that I will be adding together the elbow or taking the path integral over the time that I've the temple scale that I was doing my selection.
So that's a Bayesian model selection in statistics. So the key point here is not the same quantity that's being optimized in the moment as I'm inferring, or I am learning.
And, but by taking the path integral or you're lifting that to the next scale, necessarily has the same functional form, and therefore gives the scale freeness to the next level of optimization which in this instance, could, you know, I framed as Bayesian model
destructivism. If I was a biologist, this would look like natural selection, where I'd now be reading the free energy is adaptive fitness is simply self evidencing maximizing the likelihood of getting this kind of phenotype this structure in this eco niche, you know, doing this, doing this kind of
self evidencing or
behavior.
So that
formally speaking, what that requires you really to write down is a renormalization group and practically speaking I think I think the question has the answer in it that
whatever principle you bring to the table has to be composable with a separation of temple scales in the sense that, you know, I can do, I can be an artifact that optimizes, extremizes my free energy over the next 200 milliseconds and I could be doing sort of deception.
It could be over the next week as I learned to ride a bicycle I could do procedural learning it could be over the next three years and I could be doing neuro development or it could be transgenerational I would be doing, I would be doing natural selection, but at every point.
Each of the objective function has has if you like a mapping, via the notion of this scale freeness.
So that the optimization at one level is essential for the optimization of the level below and vice versa. So if I was like George Ellis and be talking about top down and bottom up causation now, which means I have to have the right
heritage as a phenotype in the right species in order to have the right kind of brain in order to do the right kind of learning in order to make the right kinds of inferences.
But on the other hand, I need to do the right kind of inferences to learn the right things in order to do the right development so that so I have about adaptive fitness so I can be selected.
So there's a circular causality that couples each of these levels.
Now your question is, can you usually write that down in silico and start to actually grow intelligent artifacts or simulate and model systems that are subject to these mutual constraints and the circular causality.
That's a really difficult problem, particularly when any one level starts to show things like curiosity and the like, you know, the computational complexity is is is, you know, has to be met but it is great.
This is a challenge and then we come back to complexity minimization keeping things as simple as possible.
Right.
